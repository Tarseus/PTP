{"generation": 0, "index": 0, "ir": {"name": "Adaptive Margin Sigmoid Loss", "intuition": "This loss uses a sigmoid function to compare model preferences, similar to standard preference losses. However, the 'steepness' and 'center' of the sigmoid curve are dynamically adapted based on the magnitude of the cost difference between the two solutions. When the cost difference is large, the loss function creates a stronger preference gradient and a wider margin, pushing the model more forcefully to agree with the ground truth. When the cost difference is small (i.e., the solutions are of similar quality), the gradient is gentler, preventing the model from overfitting to minor, potentially noisy, cost variations. The use of `tanh` and `softplus` ensures that scaling factors remain bounded and positive, guaranteeing numerical stability.", "pseudocode": "1. Calculate the cost difference `delta_cost = cost(b) - cost(a)` and log-probability difference `delta_logp = logp(a) - logp(b)`.\n2. Normalize the cost difference using a `tanh` function to create a bounded scaling factor `cost_scale` between 0 and 1. This represents the 'importance' of the preference.\n3. Create an adaptive margin `margin` and an adaptive temperature `temperature` that both increase as `cost_scale` increases. Use `softplus` to ensure they are positive and non-zero.\n4. Apply the adaptive temperature to the log-probability difference: `scaled_delta_logp = delta_logp / temperature`.\n5. Compute the final loss using a margin-based sigmoid formula: `loss = logsigmoid(-(scaled_delta_logp - margin))`.\n6. The result is a loss that is small when the model's preference aligns with the (cost-scaled) ground truth preference, and large otherwise.", "hyperparams": {"beta": 1.0, "gamma": 0.5, "tau_min": 0.1, "margin_min": 0.0, "margin_scale": 2.0}, "operators_used": ["tanh", "softplus", "logsigmoid"], "implementation_hint": {"expects": ["cost_a", "cost_b", "logp_a", "logp_b"], "returns": "scalar"}, "code": "import torch\nimport torch.nn.functional as F\n\ndef generated_loss(batch, model_output, extra):\n    \"\"\"\n    Adaptive Margin Sigmoid Loss.\n\n    This loss function compares pairs of solutions (winner, loser) and penalizes the model\n    if it assigns a higher log-probability to the loser.\n\n    The key idea is that the margin and temperature of the loss are dynamically adjusted\n    based on the magnitude of the cost difference between the winner and loser.\n    A larger cost difference leads to a wider margin and a more aggressive gradient.\n\n    Args:\n        batch (dict): A dictionary containing tensors from the data loader.\n                      Expected keys:\n                      - 'cost_a': Cost of the preferred solution (winner), shape [N].\n                      - 'cost_b': Cost of the non-preferred solution (loser), shape [N].\n                      - 'weight' (optional): Per-sample loss weights, shape [N].\n        model_output (dict): A dictionary containing tensors from the model.\n                             Expected keys:\n                             - 'log_prob_w': Log-probability of the winner, shape [N].\n                             - 'log_prob_l': Log-probability of the loser, shape [N].\n        extra (dict): A dictionary for hyperparameters.\n                      Expected keys:\n                      - 'beta': Controls the saturation of the cost difference scaling.\n                      - 'gamma': Controls the sensitivity of temperature to cost difference.\n                      - 'tau_min': Minimum temperature to prevent division by zero.\n                      - 'margin_min': Minimum margin.\n                      - 'margin_scale': Controls the maximum size of the adaptive margin.\n\n    Returns:\n        torch.Tensor: A scalar tensor representing the mean loss.\n    \"\"\"\n    # Unpack hyperparameters\n    beta = extra.get('beta', 1.0)\n    gamma = extra.get('gamma', 0.5)\n    tau_min = extra.get('tau_min', 0.1)\n    margin_min = extra.get('margin_min', 0.0)\n    margin_scale = extra.get('margin_scale', 2.0)\n\n    # Unpack inputs, assuming cost_a < cost_b\n    cost_w, cost_l = batch['cost_a'], batch['cost_b']\n    logp_w, logp_l = model_output['log_prob_w'], model_output['log_prob_l']\n\n    # 1. Calculate cost and logp differences\n    delta_cost = cost_l - cost_w\n    delta_logp = logp_w - logp_l\n\n    # 2. Create a bounded, non-linear scaling factor from the cost difference.\n    # tanh maps the normalized cost difference to [-1, 1]. We scale and shift it to [0, 1].\n    # This factor represents the 'confidence' in the preference.\n    cost_scale = 0.5 * (1 + torch.tanh(beta * delta_cost))\n\n    # 3. Define adaptive margin and temperature based on cost_scale.\n    # Use softplus to ensure they are positive and numerically stable.\n    # Margin increases with cost difference, demanding a larger logp gap for confident preferences.\n    adaptive_margin = margin_min + margin_scale * F.softplus(cost_scale - 0.5)\n\n    # Temperature decreases as cost difference grows, making the loss steeper for confident preferences.\n    # Adding tau_min ensures temperature is always positive and avoids division by zero.\n    adaptive_temperature = tau_min + F.softplus(-gamma * cost_scale)\n\n    # 4. Compute the core loss term\n    # The loss is a logistic loss on the logp difference, scaled by temperature and shifted by the margin.\n    # We want (delta_logp - adaptive_margin) to be large and positive.\n    loss_tensor = -F.logsigmoid((delta_logp - adaptive_margin) / adaptive_temperature)\n\n    # 5. Apply optional sample weights\n    weights = batch.get('weight', None)\n    if weights is not None:\n        loss_tensor = loss_tensor * weights\n\n    # 6. Return the mean loss\n    return loss_tensor.mean()"}, "static_ok": true, "static_reason": "", "dynamic_ok": false, "dynamic_reason": "forward_error: 'log_prob_w'", "loss_value": null, "grad_norm": null}
{"generation": 0, "index": 1, "ir": {"name": "Adaptive Margin Sigmoid Loss", "intuition": "This loss uses a sigmoid function to create a smooth, bounded loss surface, similar to logistic loss. However, it introduces an adaptive margin that scales with the normalized cost difference between the two solutions. When the cost difference is large, the margin increases, demanding a larger log-probability gap to satisfy the preference. When the cost difference is small, the margin shrinks, tolerating smaller log-probability differences and preventing overfitting to noisy or insignificant preferences. A temperature parameter controls the steepness of the sigmoid, and a gain parameter scales the overall loss magnitude.", "pseudocode": "1. Calculate the difference in log-probabilities: logp_diff = logp_winner - logp_loser.\n2. Calculate the difference in costs: cost_diff = cost_loser - cost_winner.\n3. Normalize the cost difference across the batch (e.g., using z-score) to make it scale-invariant. Clamp the result for stability.\n4. Create an adaptive margin by scaling the normalized cost difference by a hyperparameter 'alpha'.\n5. The core of the loss is log(1 + exp(-(logp_diff - margin) / temperature)), which is equivalent to softplus(-(logp_diff - margin) / temperature).\n6. Multiply the result by a 'gain' factor to control the overall loss scale.\n7. If sample weights are provided, apply them before taking the mean.", "hyperparams": {"alpha": 1.0, "temperature": 1.0, "gain": 1.0, "clamp_val": 5.0}, "operators_used": ["softplus", "zscore", "clamp"], "implementation_hint": {"expects": ["cost_a", "cost_b", "logp_a", "logp_b"], "returns": "scalar"}, "code": "import torch\nimport torch.nn.functional as F\n\ndef zscore(x, eps=1e-8):\n    \"\"\"Custom z-score implementation to be part of the function body.\"\"\"\n    if x.numel() <= 1:\n        return torch.zeros_like(x)\n    return (x - x.mean()) / (x.std() + eps)\n\ndef generated_loss(batch, model_output, extra):\n    \"\"\"Calculates the Adaptive Margin Sigmoid Loss.\"\"\"\n    # 1. Unpack inputs\n    cost_w = batch['cost_w']  # Cost of the winner (lower is better)\n    cost_l = batch['cost_l']  # Cost of the loser\n    logp_w = model_output['log_prob_w']\n    logp_l = model_output['log_prob_l']\n    weights = batch.get('weight', None)\n\n    # 2. Get hyperparameters\n    alpha = extra.get('alpha', 1.0)\n    temperature = extra.get('temperature', 1.0)\n    gain = extra.get('gain', 1.0)\n    clamp_val = extra.get('clamp_val', 5.0)\n\n    # 3. Calculate core differences\n    # logp_diff > 0 means the model prefers the winner, which is good.\n    logp_diff = logp_w - logp_l\n    # cost_diff > 0 by definition (winner has lower cost).\n    cost_diff = cost_l - cost_w\n\n    # 4. Create the adaptive margin\n    # Normalize cost differences across the batch to make the margin scale-invariant.\n    # Using a custom zscore function to adhere to the whitelist.\n    # We only normalize non-zero cost differences to avoid division by zero if all costs are equal.\n    cost_mask = cost_diff > 1e-8\n    normalized_cost_diff = torch.zeros_like(cost_diff)\n    if cost_mask.any():\n        normalized_cost_diff[cost_mask] = zscore(cost_diff[cost_mask])\n\n    # Clamp the normalized values to prevent extreme margins from single outliers.\n    clamped_norm_cost_diff = torch.clamp(normalized_cost_diff, -clamp_val, clamp_val)\n\n    # The margin is proportional to the normalized cost gap.\n    # It's positive, encouraging logp_w to be greater than logp_l.\n    margin = alpha * F.softplus(clamped_norm_cost_diff)\n\n    # 5. Calculate the loss using softplus for numerical stability\n    # This is equivalent to log(1 + exp(-x)) where x = (logp_diff - margin) / temperature\n    # Loss is high if logp_diff < margin.\n    # Loss is low if logp_diff > margin.\n    loss_tensor = F.softplus(-(logp_diff - margin) / temperature)\n\n    # 6. Apply gain and sample weights\n    loss_tensor = gain * loss_tensor\n    if weights is not None:\n        loss = (loss_tensor * weights).mean()\n    else:\n        loss = loss_tensor.mean()\n\n    return loss"}, "static_ok": true, "static_reason": "", "dynamic_ok": false, "dynamic_reason": "forward_error: 'cost_w'", "loss_value": null, "grad_norm": null}
{"generation": 0, "index": 2, "ir": {"name": "Rank-Gap Adaptive Margin Loss", "intuition": "This loss function adapts the learning signal based on how much better one solution is than another. It uses a margin that grows with the normalized rank-gap of the cost difference, but saturates to prevent extreme values from dominating. The loss is a softplus function of the log-probability difference minus this adaptive margin. This ensures that pairs with a large, meaningful cost difference are pushed apart more strongly by the model, while pairs with a tiny, noisy cost difference are treated more gently, promoting robust learning.", "pseudocode": "1. For each pair (a, b) where cost(a) < cost(b), calculate the cost difference Δc = cost(b) - cost(a).\n2. Normalize the cost differences across the batch using a z-score to get a standardized measure of how significant each difference is.\n3. Transform the normalized cost difference into an adaptive margin 'm' using a scaled and clamped tanh function. This makes the margin grow with the cost gap but keeps it bounded.\n4. Calculate the log-probability difference Δlp = logp(b) - logp(a). The loss should push this value to be negative.\n5. The final loss for the pair is softplus(Δlp + m). This is a smoothed version of relu(Δlp + m), which penalizes the model if logp(a) is not sufficiently larger than logp(b) by the margin 'm'.\n6. The total loss is the weighted mean of these individual pair losses.", "hyperparams": {"margin_scale": 1.0, "margin_clamp_max": 5.0}, "operators_used": ["softplus", "tanh", "clamp", "zscore"], "implementation_hint": {"expects": ["cost_a", "cost_b", "logp_a", "logp_b"], "returns": "scalar"}, "code": "import torch\nimport torch.nn.functional as F\n\ndef zscore(x, eps=1e-8):\n    \"\"\"Calculates z-score for a tensor.\"\"\"\n    if x.numel() <= 1:\n        return torch.zeros_like(x)\n    mean = x.mean()\n    std = x.std()\n    return (x - mean) / (std + eps)\n\ndef generated_loss(batch, model_output, extra):\n    \"\"\"\n    Calculates the Rank-Gap Adaptive Margin Loss.\n\n    Args:\n        batch (dict): A dictionary containing tensors from the data loader.\n                      Expected keys: 'cost_a', 'cost_b', 'log_prob_w', 'log_prob_l'.\n                      'cost_a' and 'log_prob_w' correspond to the preferred solution.\n                      'cost_b' and 'log_prob_l' correspond to the less preferred solution.\n                      An optional 'weight' tensor may be present.\n        model_output (dict): Not used in this loss formulation, but part of the signature.\n        extra (dict): A dictionary for hyperparameters. Expected keys: 'margin_scale', 'margin_clamp_max'.\n\n    Returns:\n        torch.Tensor: A scalar tensor representing the mean loss.\n    \"\"\"\n    # By convention, 'a' is the winner (lower cost), 'b' is the loser (higher cost).\n    cost_w, cost_l = batch['cost_a'], batch['cost_b']\n    logp_w, logp_l = batch['log_prob_w'], batch['log_prob_l']\n\n    # Hyperparameters from the 'extra' dictionary\n    margin_scale = extra.get('margin_scale', 1.0)\n    margin_clamp_max = extra.get('margin_clamp_max', 5.0)\n\n    # 1. Calculate cost difference (always positive by problem definition)\n    cost_diff = cost_l - cost_w\n\n    # 2. Normalize the cost difference using z-score for stability and scale-invariance\n    # This gives a relative measure of how significant the cost gap is within the batch.\n    normalized_cost_diff = zscore(cost_diff)\n\n    # 3. Create an adaptive margin 'm' based on the normalized cost difference.\n    # tanh ensures the margin is bounded, preventing instability from extreme cost differences.\n    # We scale it and clamp it to control the magnitude of the margin.\n    adaptive_margin = torch.clamp(\n        margin_scale * torch.tanh(normalized_cost_diff),\n        min=0.0,  # Margin should be non-negative\n        max=margin_clamp_max\n    )\n\n    # 4. Calculate the log-probability difference.\n    # We want logp_w > logp_l, which means logp_l - logp_w should be negative.\n    logp_diff = logp_l - logp_w\n\n    # 5. Calculate the core loss: softplus(logp_diff + margin).\n    # This penalizes cases where logp_w is not sufficiently larger than logp_l\n    # by the adaptive margin. It's a smooth, non-negative loss.\n    loss_per_item = F.softplus(logp_diff + adaptive_margin)\n\n    # 6. Apply optional weights and compute the mean loss.\n    weights = batch.get('weight', None)\n    if weights is not None:\n        # Ensure weights are broadcastable if they are not the same shape\n        loss = (loss_per_item * weights).sum() / weights.sum().clamp(min=1e-8)\n    else:\n        loss = loss_per_item.mean()\n\n    return loss"}, "static_ok": true, "static_reason": "", "dynamic_ok": false, "dynamic_reason": "forward_error: name 'zscore' is not defined", "loss_value": null, "grad_norm": null}
{"generation": 0, "index": 3, "ir": {"name": "Adaptive Margin Sigmoid Loss", "intuition": "This loss function uses a sigmoid activation on the difference in log probabilities, similar to standard preference losses. However, it introduces an adaptive, cost-dependent margin. When the cost difference between two solutions is large, the margin increases, demanding a stronger preference signal from the model. Conversely, when solutions are very close in cost, the margin shrinks, relaxing the pressure and preventing the model from overfitting to minor, noisy cost differences. The margin is shaped by a clamped and scaled cost difference, which is then passed through a softplus function to ensure it is always positive and smooth. This makes the learning process more robust to the scale of cost differences and focuses the model's capacity on learning meaningful preferences.", "pseudocode": "1. Calculate the difference in log probabilities: `logp_diff = logp_winner - logp_loser`.\n2. Calculate the normalized cost difference: `cost_diff_norm = normalize(abs(cost_winner - cost_loser))`.\n3. Create an adaptive margin: `margin = softplus(alpha * clamp(cost_diff_norm, 0, M))`, where `alpha` scales the margin's sensitivity and `M` caps its maximum influence.\n4. Compute the final loss using a logistic loss structure with the adaptive margin: `loss = -logsigmoid(logp_diff - margin)`.\n5. Take the weighted mean of the loss over the batch.", "hyperparams": {"alpha": 1.0, "margin_cap": 5.0}, "operators_used": ["logsigmoid", "softplus", "clamp", "normalize"], "implementation_hint": {"expects": ["cost_a", "cost_b", "log_prob_w", "log_prob_l"], "returns": "scalar"}, "code": "import torch\nimport torch.nn.functional as F\n\ndef generated_loss(batch, model_output, extra):\n    \"\"\"\n    Computes the Adaptive Margin Sigmoid Loss.\n\n    Args:\n        batch (dict): A dictionary containing tensors.\n            'cost_a' (Tensor [N]): Cost of solution 'a'.\n            'cost_b' (Tensor [N]): Cost of solution 'b'.\n            'log_prob_w' (Tensor [N]): Log probability of the winning solution.\n            'log_prob_l' (Tensor [N]): Log probability of the losing solution.\n            'weight' (Tensor [N], optional): Per-pair loss weights. Defaults to 1.\n        model_output (dict): Model outputs (not used in this loss).\n        extra (dict): A dictionary for hyperparameters.\n            'alpha' (float): Scales the sensitivity of the margin to cost differences.\n            'margin_cap' (float): The maximum value for the unscaled margin input.\n\n    Returns:\n        torch.Tensor: A scalar tensor representing the mean loss.\n    \"\"\"\n\n    # Whitelisted operators implemented as helper functions for clarity\n    def zscore(x, eps=1e-8):\n        # N is the number of elements in the batch\n        # Normalize across the batch dimension\n        if x.numel() <= 1:\n            return x - x.mean() # Avoid division by zero for single-element batch\n        return (x - x.mean()) / (x.std() + eps)\n\n    # Retrieve hyperparameters\n    alpha = extra.get('alpha', 1.0)\n    margin_cap = extra.get('margin_cap', 5.0)\n\n    # Unpack tensors from the batch dictionary\n    cost_a = batch['cost_a']\n    cost_b = batch['cost_b']\n    log_prob_w = batch['log_prob_w']\n    log_prob_l = batch['log_prob_l']\n    weights = batch.get('weight', None)\n\n    # 1. Calculate the difference in log probabilities for the (winner, loser) pair\n    # We want logp(winner) to be greater than logp(loser)\n    logp_diff = log_prob_w - log_prob_l\n\n    # 2. Calculate the absolute cost difference\n    cost_diff = torch.abs(cost_a - cost_b)\n\n    # 3. Create an adaptive margin based on the cost difference\n    # Normalize the cost difference to make it robust to different cost scales\n    # Using z-score normalization as a robust `normalize` operator\n    normalized_cost_diff = zscore(cost_diff)\n\n    # Clamp the normalized difference to prevent extreme values from creating huge margins\n    clamped_diff = torch.clamp(normalized_cost_diff, min=0.0, max=margin_cap)\n\n    # Use softplus to create a smooth, non-negative margin\n    # The margin increases with the significance of the cost difference\n    adaptive_margin = F.softplus(alpha * clamped_diff)\n\n    # 4. Compute the core loss: a logistic loss with the adaptive margin\n    # The loss is -log(sigmoid(logp_diff - margin)).\n    # We want logp_diff > margin, so we push logp_diff to be larger.\n    # A larger cost difference results in a larger margin, demanding a stronger signal.\n    loss_per_item = -F.logsigmoid(logp_diff - adaptive_margin)\n\n    # 5. Apply optional weights and compute the mean loss\n    if weights is not None:\n        # Ensure weights don't have a different shape or cause broadcasting issues\n        loss = (loss_per_item * weights).sum() / weights.sum().clamp(min=1e-8)\n    else:\n        loss = loss_per_item.mean()\n\n    return loss"}, "static_ok": true, "static_reason": "", "dynamic_ok": true, "dynamic_reason": "ok", "loss_value": 1.0986123085021973, "grad_norm": 0.0}
{"generation": 0, "index": 4, "ir": {"name": "Adaptive Margin Rank-Gap Loss", "intuition": "This loss function compares the model's preference (log-probability difference) against a target preference derived from the cost difference. It uses an adaptive margin, where the required 'win' in log-probability space grows with the 'win' in cost space, but this growth is saturated by a tanh function to prevent extreme targets. The core loss is a softplus (smooth relu) on the difference between the target and the model's log-probability gap, which penalizes the model only when it fails to meet the adaptive target. The cost difference is normalized using rank-gaps to make the target scale invariant and robust to outlier costs.", "pseudocode": "1. For a batch of solution pairs (a, b) where a is preferred to b (cost(a) < cost(b)), calculate the cost difference delta_cost = cost(b) - cost(a).\n2. Normalize these cost differences across the batch by computing their rank-gap, which maps them to a stable range like [-1, 1].\n3. Define an adaptive margin as a target for the log-probability difference. This margin is calculated by scaling the normalized rank-gap with a tanh function, controlled by a temperature hyperparameter 'tau'. A higher 'tau' means a steeper target for larger cost gaps.\n4. Calculate the model's actual log-probability difference, delta_logp = logp(a) - logp(b).\n5. The loss is computed as softplus(margin - delta_logp). This penalizes the model if delta_logp is smaller than the required margin, pushing the model to increase the probability of the better solution 'a'.\n6. The final loss is the weighted mean of these individual losses over the batch.", "hyperparams": {"tau": {"value": 1.0, "description": "Temperature parameter controlling the steepness of the adaptive margin. Higher tau demands a larger logp gap for a given cost gap."}, "beta": {"value": 1.0, "description": "Scaling factor for the final loss value. Does not affect gradients, only the magnitude."}}, "operators_used": ["rank_gap", "tanh", "softplus"], "implementation_hint": {"expects": ["cost_a", "cost_b", "logp_a", "logp_b"], "returns": "scalar"}, "code": "import torch\nimport torch.nn.functional as F\n\ndef rank_gap(x):\n    \"\"\"Computes the rank-gap normalization of a 1D tensor x.\"\"\"\n    if x.numel() <= 1:\n        return torch.zeros_like(x)\n    \n    # Get the rank of each element\n    sorter = torch.argsort(x)\n    rank = torch.empty_like(sorter, dtype=x.dtype)\n    rank[sorter] = torch.arange(x.numel(), device=x.device, dtype=x.dtype)\n    \n    # Normalize rank to [-1, 1]\n    # Formula: 2 * (rank / (N - 1)) - 1\n    n_minus_1 = x.numel() - 1\n    normalized_rank = 2.0 * (rank / n_minus_1) - 1.0\n    return normalized_rank\n\ndef generated_loss(batch, model_output, extra):\n    \"\"\"\n    Computes the Adaptive Margin Rank-Gap Loss.\n    Assumes cost_a < cost_b for all pairs (a,b).\n    \"\"\"\n    # Read hyperparameters\n    # 'extra' can override hyperparams for dynamic scheduling\n    tau = extra.get('tau', 1.0)\n    beta = extra.get('beta', 1.0)\n\n    # Unpack tensors from batch and model_output\n    # In preference learning, we often have a 'winner' and 'loser' solution\n    # Let's assume 'w' is the winner (better solution) and 'l' is the loser\n    cost_w, cost_l = batch['cost_a'], batch['cost_b']\n    logp_w, logp_l = model_output['log_prob_a'], model_output['log_prob_b']\n    weights = batch.get('weight', None)\n\n    # 1. Calculate cost difference (always positive as cost_w < cost_l)\n    delta_cost = cost_l - cost_w\n\n    # 2. Normalize cost differences using rank-gap for stability\n    # This maps cost differences to a robust [-1, 1] range\n    normalized_cost_gap = rank_gap(delta_cost)\n\n    # 3. Define the adaptive margin (target for logp difference)\n    # The target grows with the normalized cost gap, but is saturated by tanh\n    # The result is in [-tau, tau], providing a bounded target\n    target_margin = tau * torch.tanh(normalized_cost_gap)\n\n    # 4. Calculate the model's log-probability difference\n    delta_logp = logp_w - logp_l\n\n    # 5. Compute the core loss using softplus (a smooth ReLU)\n    # The loss is positive only if delta_logp < target_margin\n    # This penalizes the model for not preferring the winner enough\n    loss_per_item = F.softplus(target_margin - delta_logp, beta=beta)\n\n    # 6. Apply optional weights and compute the mean loss\n    if weights is not None:\n        loss = (loss_per_item * weights).sum() / weights.sum().clamp(min=1e-8)\n    else:\n        loss = loss_per_item.mean()\n\n    return loss"}, "static_ok": true, "static_reason": "", "dynamic_ok": false, "dynamic_reason": "forward_error: 'log_prob_a'", "loss_value": null, "grad_norm": null}
{"generation": 0, "index": 5, "ir": {"name": "Adaptive Margin Sigmoid Loss", "intuition": "This loss uses a sigmoid function to create a smooth, bounded loss similar to logistic loss. However, it introduces an adaptive margin that depends on the normalized cost difference between the two solutions. If the cost difference is large, the margin increases, demanding a stronger preference from the model (a larger log-probability gap). If the cost difference is small, the margin shrinks, tolerating model uncertainty. This prevents overconfident penalization for pairs with negligible cost differences and focuses learning on clear-cut cases. The cost difference is normalized using a z-score to make the margin's scale invariant to the absolute cost values of the problem instance, and a softplus function ensures the margin is always positive and smooth.", "pseudocode": "1. For each pair (a, b) where cost(a) < cost(b), calculate the cost difference: delta_cost = cost(b) - cost(a).\n2. Normalize the delta_cost values across the batch using a z-score to get z_cost. Clamp the result for stability.\n3. Create an adaptive margin: margin = alpha * softplus(beta * z_cost). Alpha controls the base margin size, and beta controls its sensitivity to cost differences.\n4. Calculate the log-probability difference: delta_logp = logp(a) - logp(b).\n5. The core loss for the pair is log(1 + exp(-(delta_logp - margin))), which is equivalent to softplus(-(delta_logp - margin)). This penalizes the model if delta_logp is not greater than the adaptive margin.\n6. Compute the mean of this loss over the batch, applying optional weights if provided.", "hyperparams": {"alpha": {"value": 0.5, "description": "Base scaling factor for the adaptive margin. Controls the overall magnitude of the desired log-probability gap."}, "beta": {"value": 1.0, "description": "Sensitivity of the margin to the normalized cost difference. Higher values make the margin react more strongly to cost gaps."}, "clamp_val": {"value": 5.0, "description": "Maximum absolute value for the normalized z-score of cost differences to prevent extreme margin values."}}, "operators_used": ["softplus", "clamp", "zscore"], "implementation_hint": {"expects": ["cost_a", "cost_b", "logp_a", "logp_b"], "returns": "scalar"}, "code": "import torch\nimport torch.nn.functional as F\n\ndef generated_loss(batch, model_output, extra):\n    \"\"\"\n    Adaptive Margin Sigmoid Loss.\n\n    The loss aims to enforce logp_l > logp_w with a margin that adapts to the cost difference.\n    - cost_w, cost_l: Costs of the winning (better) and losing (worse) solutions.\n    - log_prob_w, log_prob_l: Log probabilities of the winning and losing solutions.\n    \"\"\"\n    # Whitelisted operators implemented as helper functions for clarity and strictness\n    def zscore(x, eps=1e-8):\n        # Custom zscore implementation to handle single-element tensors\n        if x.numel() <= 1:\n            return torch.zeros_like(x)\n        mean = x.mean()\n        std = x.std()\n        return (x - mean) / (std + eps)\n\n    # Unpack batch data. By convention, 'a' is the winner (lower cost) and 'b' is the loser.\n    cost_w = batch['cost_a']\n    cost_l = batch['cost_b']\n    log_prob_w = model_output['log_prob_a']\n    log_prob_l = model_output['log_prob_b']\n    weight = batch.get('weight', None)\n\n    # Get hyperparameters\n    alpha = extra.get('alpha', 0.5)\n    beta = extra.get('beta', 1.0)\n    clamp_val = extra.get('clamp_val', 5.0)\n\n    # Ensure winner has lower cost\n    assert torch.all(cost_w <= cost_l), \"Assumption failed: cost_a should be the winner (lower cost)\"\n\n    # 1. Calculate cost difference (always non-negative)\n    delta_cost = cost_l - cost_w\n\n    # 2. Normalize cost difference using z-score and clamp for stability\n    # This makes the margin robust to the scale of costs in different problems.\n    with torch.no_grad():\n        z_cost = zscore(delta_cost)\n        z_cost_clamped = torch.clamp(z_cost, -clamp_val, clamp_val)\n\n    # 3. Compute the adaptive margin\n    # softplus ensures the margin is always positive and smooth.\n    # The margin increases as the relative cost difference (z_cost) increases.\n    margin = alpha * F.softplus(beta * z_cost_clamped)\n\n    # 4. Calculate the log-probability difference the model produced\n    # We want log_prob_w to be much larger than log_prob_l.\n    delta_logp = log_prob_w - log_prob_l\n\n    # 5. Calculate the core loss using softplus (numerically stable log(1+exp(x)))\n    # This is equivalent to a logistic loss with a dynamic margin.\n    # The loss is -(delta_logp - margin) inside the softplus.\n    # We want delta_logp > margin, so (delta_logp - margin) should be positive.\n    # If it's negative, softplus(-(negative)) = softplus(positive) -> high loss.\n    # If it's positive, softplus(-(positive)) = softplus(negative) -> low loss.\n    loss_per_item = F.softplus(-(delta_logp - margin))\n\n    # 6. Apply weights and compute the mean loss\n    if weight is not None:\n        loss = (loss_per_item * weight).mean()\n    else:\n        loss = loss_per_item.mean()\n\n    return loss"}, "static_ok": true, "static_reason": "", "dynamic_ok": false, "dynamic_reason": "forward_error: 'log_prob_a'", "loss_value": null, "grad_norm": null}
{"generation": 0, "index": 6, "ir": {"name": "Normalized Margin Shift Loss", "intuition": "This loss function compares the model's preference (difference in log probabilities) against a target preference derived from the normalized cost difference. The cost difference is first normalized using a z-score to make it robust to scale variations across different instances and batches. This normalized value is then passed through a tanh function to create a bounded target preference between -1 and 1. The loss is a softplus of the difference between the model's preference and this target, plus a margin. This encourages the model's preference to align with the magnitude of the cost difference in a stable, bounded manner, penalizing misalignment more strongly when the cost gap is large.", "pseudocode": "1. For a batch of solution pairs (a, b), calculate the cost difference Δcost = cost(a) - cost(b) and log-probability difference Δlogp = logp(a) - logp(b).\n2. Normalize the cost differences across the batch using z-score normalization to get z_Δcost.\n3. Compute a target preference score by passing the normalized cost difference through a scaled tanh function: target = -tanh(beta * z_Δcost). The negative sign ensures that a lower cost for 'a' (negative Δcost) results in a positive target preference for 'a'.\n4. Calculate the loss as a softplus function of the difference between the model's preference and the target, plus a margin: loss = softplus(margin - Δlogp * target).\n5. The loss is high if the model's preference (sign of Δlogp) is opposite to the target's preference (sign of target).\n6. The final loss is the weighted mean over the batch.", "hyperparams": {"margin": 0.1, "beta": 1.0, "eps": 1e-08}, "operators_used": ["zscore", "tanh", "softplus"], "implementation_hint": {"expects": ["cost_a", "cost_b", "logp_a", "logp_b"], "returns": "scalar"}, "code": "import torch\nimport torch.nn.functional as F\n\ndef generated_loss(batch, model_output, extra):\n    \"\"\"\n    Implements the Normalized Margin Shift Loss.\n\n    The loss encourages the model's log-probability difference to align with a \n    target preference derived from the z-scored cost difference.\n    \"\"\"\n    # 1. Extract inputs from the batch dictionary\n    # We assume 'w' (winner) is 'a' and 'l' (loser) is 'b', so cost(a) < cost(b)\n    cost_a = batch['cost_w']\n    cost_b = batch['cost_l']\n    logp_a = model_output['log_prob_w']\n    logp_b = model_output['log_prob_l']\n    weights = batch.get('weight', None)\n\n    # 2. Retrieve hyperparameters\n    # Use 'extra' for dynamic hyperparams, or a fixed dict for static ones\n    hyperparams = extra.get('hyperparams', {'margin': 0.1, 'beta': 1.0, 'eps': 1e-8})\n    margin = hyperparams.get('margin', 0.1)\n    beta = hyperparams.get('beta', 1.0)\n    eps = hyperparams.get('eps', 1e-8)\n\n    # 3. Calculate differences\n    # Δcost is negative since cost_a < cost_b\n    delta_cost = cost_a - cost_b \n    # Δlogp should be positive for correct preference\n    delta_logp = logp_a - logp_b\n\n    # 4. Normalize cost differences (z-score)\n    # zscore(x) = (x - mean(x)) / (std(x) + eps)\n    if delta_cost.numel() > 1:\n        mean_cost_diff = delta_cost.mean()\n        std_cost_diff = delta_cost.std()\n        normalized_delta_cost = (delta_cost - mean_cost_diff) / (std_cost_diff + eps)\n    else:\n        # Handle batch size of 1 to avoid NaN from std=0\n        normalized_delta_cost = torch.zeros_like(delta_cost)\n\n    # 5. Compute the bounded target preference\n    # target is positive, as delta_cost is negative. Magnitude depends on cost gap.\n    # tanh squashes the normalized cost difference to be between -1 and 1.\n    target = -torch.tanh(beta * normalized_delta_cost)\n\n    # 6. Calculate the core loss term\n    # We want delta_logp to be positive and aligned with the positive target.\n    # The term 'delta_logp * target' is high and positive if they are aligned.\n    # loss = softplus(margin - alignment_score)\n    # This penalizes cases where the alignment score is low or negative.\n    loss_per_item = F.softplus(margin - delta_logp * target)\n\n    # 7. Apply optional weights and compute the final scalar loss\n    if weights is not None:\n        loss = (loss_per_item * weights).mean()\n    else:\n        loss = loss_per_item.mean()\n\n    return loss"}, "static_ok": true, "static_reason": "", "dynamic_ok": false, "dynamic_reason": "forward_error: 'cost_w'", "loss_value": null, "grad_norm": null}
{"generation": 0, "index": 7, "ir": {"name": "Adaptive Margin Sigmoid Loss", "intuition": "This loss function uses a sigmoid to create a smooth, bounded loss surface. The key idea is to dynamically adjust the 'margin' based on the magnitude of the cost difference between two solutions. If the cost difference is large, the model is penalized more heavily for getting the preference wrong, effectively demanding a larger log-probability gap. If the cost difference is small, the model is given more slack. This is achieved by scaling the log-probability difference by a softplus-transformed and normalized cost difference, preventing instability while still being sensitive to the quality gap.", "pseudocode": "1. Calculate the difference in log-probabilities, delta_logp = logp(winner) - logp(loser).\n2. Calculate the absolute difference in costs, delta_cost = abs(cost(winner) - cost(loser)).\n3. Transform delta_cost into a stable, non-negative scaling factor. Use softplus to ensure non-negativity and then normalize it to prevent extreme values from dominating the loss. This scale factor, 'cost_scale', will be larger for larger cost differences.\n4. Apply a temperature scaling to delta_logp for better gradient control.\n5. The core of the loss is log(sigmoid( -cost_scale * temp_scaled_delta_logp )). This is a stable binary cross-entropy formulation. When delta_logp is negative (incorrect preference), the argument to sigmoid is positive, yielding a high loss. When delta_logp is positive (correct preference), the argument is negative, yielding a low loss.\n6. The 'cost_scale' acts as an adaptive margin: a larger cost difference requires a larger delta_logp to achieve the same low loss value.\n7. Return the negative of this value, averaged over the batch.", "hyperparams": {"temperature": 1.0, "cost_scale_strength": 5.0, "cost_normalization_eps": 1e-08}, "operators_used": ["logsigmoid", "softplus", "normalize"], "implementation_hint": {"expects": ["cost_a", "cost_b", "log_prob_w", "log_prob_l"], "returns": "scalar"}, "code": "import torch\nimport torch.nn.functional as F\n\ndef generated_loss(batch, model_output, extra):\n    \"\"\"\n    Calculates the Adaptive Margin Sigmoid Loss.\n\n    The loss encourages the model to assign higher log-probability to the better solution\n    (lower cost). The margin of this preference is scaled by the normalized difference\n    in costs, making the loss more sensitive to large quality gaps.\n\n    Args:\n        batch (dict): A dictionary containing tensors from the data loader.\n                      Expected keys: 'cost_a', 'cost_b' (costs of the pair),\n                      'log_prob_w', 'log_prob_l' (model's log-probabilities for the\n                      winner and loser solutions respectively), and optional 'weight'.\n        model_output: Not used in this loss implementation.\n        extra (dict): A dictionary for hyperparameters. Expected keys:\n                      'temperature', 'cost_scale_strength', 'cost_normalization_eps'.\n\n    Returns:\n        torch.Tensor: A scalar tensor representing the mean loss over the batch.\n    \"\"\"\n    # Read hyperparameters from the 'extra' dictionary\n    temp = extra.get('temperature', 1.0)\n    cost_scale_strength = extra.get('cost_scale_strength', 5.0)\n    eps = extra.get('cost_normalization_eps', 1e-8)\n\n    # Retrieve costs and log probabilities from the batch\n    # The dataset provides costs for solutions a and b, and the model's log probabilities\n    # for the true winner (w) and loser (l) solutions.\n    cost_a = batch['cost_a']\n    cost_b = batch['cost_b']\n    log_prob_w = batch['log_prob_w']\n    log_prob_l = batch['log_prob_l']\n\n    # 1. Calculate the difference in log-probabilities\n    # We want log_prob_w to be greater than log_prob_l.\n    delta_logp = log_prob_w - log_prob_l\n\n    # 2. Calculate the absolute difference in costs\n    delta_cost_abs = torch.abs(cost_a - cost_b)\n\n    # 3. Transform delta_cost into a stable, non-negative scaling factor\n    # softplus ensures the scale is non-negative and smooth.\n    cost_signal = F.softplus(delta_cost_abs)\n\n    # Normalize the cost signal to prevent extreme values from dominating.\n    # We normalize over the batch. The mean and std are scalars.\n    # Using torch.normalize utility function.\n    if cost_signal.numel() > 1:\n        # F.normalize requires a float tensor\n        cost_signal_normalized = F.normalize(cost_signal.float(), p=2, dim=0)\n    else:\n        # Handle batch size of 1 to avoid division by zero in normalization\n        cost_signal_normalized = cost_signal / (torch.sum(cost_signal) + eps)\n\n    # The final adaptive scale. The strength parameter controls its influence.\n    # A baseline of 1.0 is added so the scale is always at least 1.\n    adaptive_scale = 1.0 + cost_scale_strength * cost_signal_normalized\n\n    # 4. Apply temperature scaling to the log-probability difference\n    scaled_delta_logp = delta_logp / temp\n\n    # 5. Calculate the core loss using log_sigmoid for numerical stability\n    # This is equivalent to -log(sigmoid(arg)), but more stable.\n    # We want to maximize `adaptive_scale * scaled_delta_logp`.\n    # So we minimize its negative.\n    loss_per_item = -F.logsigmoid(adaptive_scale * scaled_delta_logp)\n\n    # 6. Apply optional sample weights\n    weights = batch.get('weight', None)\n    if weights is not None:\n        loss_per_item = loss_per_item * weights\n\n    # 7. Return the mean loss over the batch\n    return loss_per_item.mean()"}, "static_ok": true, "static_reason": "", "dynamic_ok": true, "dynamic_reason": "ok", "loss_value": 0.6931471824645996, "grad_norm": 0.0}
{"generation": 1, "index": 0, "ir": {"name": "Rank-Gapped Sigmoid Loss with Cost-Weighted Logits", "intuition": "This loss function combines an additive margin with a multiplicative scaling of log-probabilities to create a robust preference learning objective. \n\nInherited ideas:\n- From Parent 0, we inherit the concept of an **additive margin** within the sigmoid: `logsigmoid(logit - margin)`. This sets a minimum required separation between the winner and loser probabilities, which adapts based on the cost difference.\n- From Parent 1, we inherit the idea of **multiplicatively scaling** the log-probability difference by a factor derived from the cost difference. This makes the loss more sensitive to preference errors when the cost gap is large.\n\nNew coupling ideas:\n1. **Rank-Gap Normalization for Margin**: Instead of using z-score or L2 normalization on the raw cost differences, we use a `rank_gap` normalization. This transforms the costs into a uniform distribution based on their rank order, making the margin's scale invariant to the distribution of costs and robust to outliers. The margin is then `softplus(beta * rank_gap(cost_diff))`, creating a smooth, positive, and predictably scaled margin.\n2. **Tanh-Weighted Logits**: The log-probability difference (`logp_diff`) is scaled by `tanh(alpha * rank_gap(cost_diff))`. The `tanh` function acts as a smooth switch, applying a gentle weight for small cost differences and saturating towards a full weight for large cost differences. This prevents extremely large cost differences from creating unbounded gradients while still emphasizing significant preference pairs.\n\nBy coupling rank-gapped cost signals with both an additive margin and a multiplicative logit weight, the loss becomes highly stable and focuses learning on the relative importance of preferences, as defined by the rank order of their cost differences.", "pseudocode": "1. Calculate the raw log-probability difference: `logp_diff = logp_winner - logp_loser`.\n2. Calculate the absolute cost difference: `cost_diff = abs(cost_winner - cost_loser)`.\n3. Compute a rank-based, normalized version of the cost difference: `ranked_cost_diff = rank_gap(cost_diff)`. This value will be between 0 and 1.\n4. **(New Coupling 1)** Create a multiplicative weight for the logits using the ranked cost difference. The weight is `logit_weight = tanh(alpha * ranked_cost_diff)`. `alpha` controls the steepness of the weighting.\n5. Apply the weight to the log-probability difference: `weighted_logp_diff = logit_weight * logp_diff`.\n6. **(New Coupling 2)** Create an additive margin, also based on the ranked cost difference. The margin is `margin = softplus(beta * ranked_cost_diff)`. `beta` controls the magnitude of the margin.\n7. Compute the final loss using a sigmoid structure that incorporates both the weighted logit and the adaptive margin: `loss = -logsigmoid(weighted_logp_diff - margin)`.\n8. Return the weighted mean of the loss over the batch.", "hyperparams": {"alpha": 3.0, "beta": 1.0}, "operators_used": ["logsigmoid", "tanh", "softplus", "rank_gap"], "implementation_hint": {"expects": ["cost_a", "cost_b", "log_prob_w", "log_prob_l", "weight"], "returns": "scalar"}, "code": "import torch\nimport torch.nn.functional as F\n\ndef generated_loss(batch, model_output, extra):\n    \"\"\"\n    Computes the Rank-Gapped Sigmoid Loss with Cost-Weighted Logits.\n\n    This loss combines an additive margin and a multiplicative logit weight, both\n    derived from the rank-normalized cost difference between solution pairs.\n\n    Args:\n        batch (dict): A dictionary containing tensors.\n            'cost_a' (Tensor [N]): Cost of solution 'a'.\n            'cost_b' (Tensor [N]): Cost of solution 'b'.\n            'log_prob_w' (Tensor [N]): Log probability of the winning solution.\n            'log_prob_l' (Tensor [N]): Log probability of the losing solution.\n            'weight' (Tensor [N], optional): Per-pair loss weights. Defaults to 1.\n        model_output (dict): Model outputs (not used in this loss).\n        extra (dict): A dictionary for hyperparameters.\n            'alpha' (float): Controls the steepness of the tanh logit weight.\n            'beta' (float): Controls the magnitude of the softplus additive margin.\n\n    Returns:\n        torch.Tensor: A scalar tensor representing the mean loss.\n    \"\"\"\n\n    # Whitelisted operators implemented as helper functions\n    def rank_gap(x, eps=1e-8):\n        if x.numel() <= 1:\n            return torch.zeros_like(x)\n        # argsort gives the indices that would sort the tensor.\n        # The rank of each element is its position in the sorted list.\n        ranks = torch.empty_like(x, dtype=torch.float)\n        # We use scatter_ to place the rank (0, 1, ..., N-1) at the original index.\n        ranks.scatter_(0, x.argsort(), torch.arange(x.numel(), device=x.device, dtype=x.dtype))\n        # Normalize ranks to be in [0, 1]\n        normalized_ranks = ranks / (x.numel() - 1 + eps)\n        return normalized_ranks\n\n    # Retrieve hyperparameters\n    alpha = extra.get('alpha', 3.0)\n    beta = extra.get('beta', 1.0)\n\n    # Unpack tensors from the batch dictionary\n    cost_a = batch['cost_a']\n    cost_b = batch['cost_b']\n    log_prob_w = batch['log_prob_w']\n    log_prob_l = batch['log_prob_l']\n    weights = batch.get('weight', None)\n\n    # 1. Calculate the raw log-probability difference\n    logp_diff = log_prob_w - log_prob_l\n\n    # 2. Calculate the absolute cost difference\n    cost_diff = torch.abs(cost_a - cost_b)\n\n    # 3. Compute a rank-based, normalized version of the cost difference\n    # This is robust to outliers and cost scale.\n    ranked_cost_diff = rank_gap(cost_diff)\n\n    # 4. Create a multiplicative weight for the logits using tanh\n    # Inherited idea: Multiplicative scaling from Parent 1\n    # New coupling: Tanh activation on rank-gapped costs\n    logit_weight = torch.tanh(alpha * ranked_cost_diff)\n\n    # 5. Apply the weight to the log-probability difference\n    weighted_logp_diff = logit_weight * logp_diff\n\n    # 6. Create an additive margin using softplus\n    # Inherited idea: Additive margin from Parent 0\n    # New coupling: Softplus on rank-gapped costs\n    margin = F.softplus(beta * ranked_cost_diff)\n\n    # 7. Compute the final loss\n    # The structure combines the additive margin and the multiplicatively scaled logp_diff.\n    loss_per_item = -F.logsigmoid(weighted_logp_diff - margin)\n\n    # 8. Apply optional weights and compute the mean loss\n    if weights is not None:\n        loss = (loss_per_item * weights).sum() / weights.sum().clamp(min=1e-8)\n    else:\n        loss = loss_per_item.mean()\n\n    return loss"}, "static_ok": true, "static_reason": "", "dynamic_ok": true, "dynamic_reason": "ok", "loss_value": 1.3059937953948975, "grad_norm": 0.0}
{"generation": 1, "index": 1, "ir": {"name": "Rank-Gapped Adaptive Sigmoid Loss", "intuition": "This loss function synthesizes an adaptive margin with an adaptive scaling mechanism, tempered by the relative ranking of cost differences within the batch. \n\nInherited ideas:\n- From Parent 0 (`Adaptive Margin Sigmoid Loss`), it inherits the core structure `loss = -logsigmoid(logp_diff - margin)`, where a positive margin is subtracted from the log-probability difference. This explicitly pushes the model's log-probability gap for the winning pair to be greater than a certain threshold.\n- From Parent 1 (`Adaptive Margin Sigmoid Loss`), it inherits the idea of adaptively *scaling* the log-probability difference based on the cost gap. This makes the loss gradient more sensitive to pairs with larger cost differences.\n\nNew coupling ideas:\n1. **Rank-Gap Normalization:** Instead of using z-score or L2 normalization on the raw cost differences, we use a `rank_gap` operator. This transforms the cost differences into a robust, rank-based percentile space (0 to 1), making the margin and scaling factors invariant to the absolute magnitude and distribution of costs in a batch. It focuses on relative importance, e.g., 'is this cost difference in the top 10% of the batch?'.\n2. **Hybrid Margin and Scaling:** The loss combines both an additive margin and a multiplicative scale, both derived from the same rank-gapped cost difference. The final form is `-logsigmoid(scale * logp_diff - margin)`. The `scale` term steepens the loss for important pairs, while the `margin` term sets a minimum required log-probability gap. This dual mechanism provides a more nuanced control over the learning signal.", "pseudocode": "1. Calculate the log-probability difference: `logp_diff = logp_winner - logp_loser`.\n2. Calculate the absolute cost difference for each pair in the batch: `cost_diff = abs(cost_winner - cost_loser)`.\n3. Compute a robust, rank-based signal from the cost differences using the `rank_gap` operator: `rank_signal = rank_gap(cost_diff)`. This maps each cost difference to its percentile rank in the batch, resulting in a value between 0 and 1.\n4. Create an adaptive margin using the rank signal: `margin = margin_strength * tanh(rank_signal)`. The `tanh` function shapes the margin to be more sensitive to changes in rank for mid-ranked pairs and saturates for extremely high or low ranks, providing stability.\n5. Create an adaptive scale factor using the same rank signal: `scale = 1.0 + scale_strength * rank_signal`. This ensures the scaling factor is always at least 1, increasing the gradient for pairs with higher-ranked cost differences.\n6. Combine the margin and scale in a unified loss expression: `loss = -logsigmoid(scale * logp_diff - margin)`.\n7. Compute the weighted mean of the loss over the batch.", "hyperparams": {"margin_strength": 1.0, "scale_strength": 2.0}, "operators_used": ["logsigmoid", "tanh", "rank_gap"], "implementation_hint": {"expects": ["cost_a", "cost_b", "log_prob_w", "log_prob_l", "weight"], "returns": "scalar"}, "code": "import torch\nimport torch.nn.functional as F\n\ndef generated_loss(batch, model_output, extra):\n    \"\"\"\n    Computes the Rank-Gapped Adaptive Sigmoid Loss.\n\n    This loss combines an adaptive margin and an adaptive scaling factor, both\n    derived from the percentile rank of the cost difference within the batch. This\n    makes the loss robust to the scale of costs and focuses on relative importance.\n\n    Args:\n        batch (dict): A dictionary containing tensors.\n            'cost_a' (Tensor [N]): Cost of solution 'a'.\n            'cost_b' (Tensor [N]): Cost of solution 'b'.\n            'log_prob_w' (Tensor [N]): Log probability of the winning solution.\n            'log_prob_l' (Tensor [N]): Log probability of the losing solution.\n            'weight' (Tensor [N], optional): Per-pair loss weights. Defaults to 1.\n        model_output (dict): Model outputs (not used in this loss).\n        extra (dict): A dictionary for hyperparameters.\n            'margin_strength' (float): Controls the magnitude of the adaptive margin.\n            'scale_strength' (float): Controls the magnitude of the adaptive scaling.\n\n    Returns:\n        torch.Tensor: A scalar tensor representing the mean loss.\n    \"\"\"\n\n    # Whitelisted operator implemented as a helper function\n    def rank_gap(x, eps=1e-8):\n        \"\"\"Maps values in a tensor to their percentile rank (0 to 1).\"\"\"\n        if x.numel() <= 1:\n            return torch.zeros_like(x)\n        # stable_rank_per_channel from https://github.com/google-deepmind/optax/blob/master/optax/_src/transform.py\n        ranks = x.argsort().argsort().float()\n        # Normalize ranks to [0, 1]\n        return ranks / (x.numel() - 1 + eps)\n\n    # Retrieve hyperparameters\n    margin_strength = extra.get('margin_strength', 1.0)\n    scale_strength = extra.get('scale_strength', 2.0)\n\n    # Unpack tensors from the batch\n    cost_a = batch['cost_a']\n    cost_b = batch['cost_b']\n    log_prob_w = batch['log_prob_w']\n    log_prob_l = batch['log_prob_l']\n    weights = batch.get('weight', None)\n\n    # 1. Calculate the log-probability difference\n    logp_diff = log_prob_w - log_prob_l\n\n    # 2. Calculate the absolute cost difference\n    cost_diff = torch.abs(cost_a - cost_b)\n\n    # 3. Compute a robust, rank-based signal from cost differences (New Coupling Idea)\n    # This is invariant to the scale of costs\n    rank_signal = rank_gap(cost_diff)\n\n    # 4. Create an adaptive margin (Inherited from Parent 0, modified with new coupling)\n    # tanh provides a smooth, bounded shaping of the rank signal\n    adaptive_margin = margin_strength * torch.tanh(rank_signal)\n\n    # 5. Create an adaptive scale (Inherited from Parent 1, modified with new coupling)\n    # The scale is >= 1 and increases with the rank of the cost difference\n    adaptive_scale = 1.0 + scale_strength * rank_signal\n\n    # 6. Combine margin and scale in a unified loss (New Coupling Idea)\n    # The argument to logsigmoid is `scale * logp_diff - margin`.\n    # We want this argument to be large and positive.\n    loss_per_item = -F.logsigmoid(adaptive_scale * logp_diff - adaptive_margin)\n\n    # 7. Apply optional weights and compute the mean loss\n    if weights is not None:\n        # Ensure weights don't have a different shape or cause broadcasting issues\n        loss = (loss_per_item * weights).sum() / weights.sum().clamp(min=1e-8)\n    else:\n        loss = loss_per_item.mean()\n\n    return loss"}, "static_ok": true, "static_reason": "", "dynamic_ok": true, "dynamic_reason": "ok", "loss_value": 0.9380261898040771, "grad_norm": 0.0}
{"generation": 1, "index": 2, "ir": {"name": "Rank-Gapped Adaptive Sigmoid Loss", "intuition": "This loss function synthesizes an additive margin approach with a multiplicative scaling mechanism, both of which are adaptively determined by the cost difference between solutions. The goal is to create a robust loss that is sensitive to both the absolute and relative differences in solution quality.\n\nInherited Ideas:\n- From Parent 0 (Adaptive Margin Sigmoid Loss), we inherit the core structure of an additive margin: `loss = -logsigmoid(logp_diff - margin)`. This sets a minimum required log-probability gap that the model must achieve, and this gap increases with the cost difference.\n- From Parent 1 (Adaptive Margin Sigmoid Loss), we inherit the concept of multiplicatively scaling the log-probability difference: `loss = -logsigmoid(scale * logp_diff)`. This makes the gradient steeper for pairs with larger cost differences, effectively prioritizing learning on more distinct pairs.\n\nNew Coupling Ideas:\n1.  **Dual Adaptive Mechanism**: We combine both the additive margin and the multiplicative scaling into a single loss: `loss = -logsigmoid(scale * logp_diff - margin)`. Both `scale` and `margin` are functions of the cost difference, creating a dual-pronged adaptive pressure on the model.\n2.  **Rank-Gap Normalization**: Instead of using standard z-score or L2 normalization on the raw cost differences, which can be sensitive to outliers, we introduce `rank_gap` normalization. This operator converts the cost differences into their fractional ranks within the batch. This makes the adaptive components of the loss invariant to the scale and distribution of costs, providing greater stability and predictable behavior across different problems and even different stages of training.", "pseudocode": "1. Calculate the log-probability difference: `logp_diff = logp_winner - logp_loser`.\n2. Calculate the absolute cost difference for each pair in the batch: `cost_diff = abs(cost_winner - cost_loser)`.\n3. Normalize the cost differences using rank-gap normalization: `rank_gapped_cost = rank_gap(cost_diff)`. This produces values in the [0, 1] range based on their rank in the batch.\n4. Create the adaptive multiplicative scale, `scale`, from the rank-gapped costs. A `softplus` function ensures it's a smooth, positive value that increases with the cost rank: `scale = 1.0 + softplus(beta * (rank_gapped_cost - 0.5))`, where `beta` controls sensitivity.\n5. Create the adaptive additive margin, `margin`, also from the rank-gapped costs. This ensures the margin is also smooth and grows with the cost rank: `margin = softplus(alpha * rank_gapped_cost)`.\n6. Combine these components into a single loss term: `argument = scale * logp_diff - margin`.\n7. Compute the final loss using a numerically stable logistic loss: `loss = -logsigmoid(argument)`.\n8. Return the weighted mean of the loss over the batch.", "hyperparams": {"alpha": 2.0, "beta": 1.0}, "operators_used": ["logsigmoid", "softplus", "rank_gap"], "implementation_hint": {"expects": ["cost_a", "cost_b", "log_prob_w", "log_prob_l", "weight"], "returns": "scalar"}, "code": "import torch\nimport torch.nn.functional as F\n\ndef generated_loss(batch, model_output, extra):\n    \"\"\"\n    Computes the Rank-Gapped Adaptive Sigmoid Loss.\n\n    This loss combines an adaptive multiplicative scale and an adaptive additive margin,\n    both derived from the rank-normalized cost difference between solution pairs.\n\n    Args:\n        batch (dict): A dictionary containing tensors.\n            'cost_a' (Tensor [N]): Cost of solution 'a'.\n            'cost_b' (Tensor [N]): Cost of solution 'b'.\n            'log_prob_w' (Tensor [N]): Log probability of the winning solution.\n            'log_prob_l' (Tensor [N]): Log probability of the losing solution.\n            'weight' (Tensor [N], optional): Per-pair loss weights. Defaults to 1.\n        model_output (dict): Model outputs (not used in this loss).\n        extra (dict): A dictionary for hyperparameters.\n            'alpha' (float): Controls the magnitude of the additive margin.\n            'beta' (float): Controls the sensitivity of the multiplicative scale.\n\n    Returns:\n        torch.Tensor: A scalar tensor representing the mean loss.\n    \"\"\"\n\n    # Whitelisted operators implemented as helper functions for clarity\n    def rank_gap(x, eps=1e-8):\n        \"\"\"Converts values to their fractional ranks. Output is in [0, 1].\"\"\"\n        if x.numel() <= 1:\n            return torch.ones_like(x) * 0.5 # Mid-point for single item\n        # Get ranks (0-indexed), convert to float, and normalize to [0, N-1]\n        ranks = x.argsort().argsort().float()\n        # Normalize to [0, 1]\n        normalized_ranks = ranks / (x.numel() - 1 + eps)\n        return normalized_ranks\n\n    # Retrieve hyperparameters\n    alpha = extra.get('alpha', 2.0)\n    beta = extra.get('beta', 1.0)\n\n    # Unpack tensors from the batch dictionary\n    cost_a = batch['cost_a']\n    cost_b = batch['cost_b']\n    log_prob_w = batch['log_prob_w']\n    log_prob_l = batch['log_prob_l']\n    weights = batch.get('weight', None)\n\n    # 1. Calculate the log-probability difference\n    logp_diff = log_prob_w - log_prob_l\n\n    # 2. Calculate the absolute cost difference\n    cost_diff = torch.abs(cost_a - cost_b)\n\n    # 3. Normalize cost differences using the new rank_gap operator for stability\n    rank_gapped_cost = rank_gap(cost_diff)\n\n    # 4. Create an adaptive multiplicative scale based on the rank-gapped cost\n    # This inherits the idea of multiplicative scaling from Parent 1.\n    # The scale is centered around 1.0 using `rank_gapped_cost - 0.5`.\n    # softplus ensures the scale is smooth and positive.\n    scale = 1.0 + F.softplus(beta * (rank_gapped_cost - 0.5))\n\n    # 5. Create an adaptive additive margin based on the rank-gapped cost\n    # This inherits the additive margin structure from Parent 0.\n    # softplus ensures the margin is smooth and non-negative.\n    margin = F.softplus(alpha * rank_gapped_cost)\n\n    # 6. Combine the scale and margin into a single loss argument.\n    # This is the core coupling idea: applying both pressures simultaneously.\n    argument = scale * logp_diff - margin\n\n    # 7. Compute the final loss using a stable logistic loss function\n    loss_per_item = -F.logsigmoid(argument)\n\n    # 8. Apply optional weights and compute the mean loss\n    if weights is not None:\n        loss = (loss_per_item * weights).sum() / weights.sum().clamp(min=1e-8)\n    else:\n        loss = loss_per_item.mean()\n\n    return loss"}, "static_ok": true, "static_reason": "", "dynamic_ok": true, "dynamic_reason": "ok", "loss_value": 1.5964173078536987, "grad_norm": 0.0}
{"generation": 1, "index": 3, "ir": {"name": "Rank-Gapped Adaptive Sigmoid Loss", "intuition": "This loss function synergizes an additive margin with a multiplicative scaling factor, both derived from cost information, to create a highly adaptive preference learning signal. \n\nIt inherits two key ideas from its parents:\n1. From Parent 0, it adopts the concept of an **additive, cost-dependent margin**. The loss aims to ensure the log-probability difference (`logp_diff`) surpasses this margin. The margin is calculated using a `softplus` transformation of the normalized cost difference, making it a smooth and positive value that grows with the cost gap.\n2. From Parent 1, it incorporates the idea of **multiplicatively scaling** the log-probability difference. This scaling factor also depends on the cost difference, effectively increasing the gradient and penalty for mis-ranking pairs with a large quality gap.\n\nThis child loss introduces two new coupling ideas for enhanced stability and control:\n1. **Rank-Gap Normalization**: Instead of normalizing the raw cost values, we first convert the cost differences into ranks within the batch (`rank_gap`). This makes the loss invariant to the absolute scale of costs and robust to outliers, focusing purely on the relative ordering of cost differences.\n2. **Tanh Activation for Scaling**: The rank-gapped signal is passed through a `tanh` function before being used as the multiplicative scaler. This bounds the scaler between 0 and 1, preventing it from causing numerical instability while still providing a smooth, monotonic mapping from the rank gap to the scaling factor.", "pseudocode": "1. Calculate the difference in log probabilities: `logp_diff = logp_winner - logp_loser`.\n2. Calculate the absolute difference in costs: `cost_diff = abs(cost_winner - cost_loser)`.\n3. **(New Coupling)** Convert the cost differences into normalized ranks within the batch using the `rank_gap` operator. This produces a stable signal between -0.5 and 0.5.\n4. **(Inherited from Parent 0)** Create an additive margin. Apply a `softplus` function to the rank-gapped signal, scaled by a hyperparameter `alpha`, to ensure the margin is smooth and non-negative: `margin = softplus(alpha * rank_gap_signal)`.\n5. **(New Coupling & Inherited from Parent 1)** Create a multiplicative scaler. Apply a `tanh` activation to the rank-gapped signal, scaled by a hyperparameter `beta`, to create a stable, bounded scaler: `scaler = tanh(beta * rank_gap_signal)`.\n6. Combine the components into the final loss argument: `argument = scaler * logp_diff - margin`. This structure ensures that both the required log probability gap (margin) and the penalty for incorrect preferences (scaler) increase with the significance of the cost difference.\n7. Compute the final loss using a numerically stable logistic loss: `loss = -logsigmoid(argument)`.\n8. Return the weighted mean of the loss over the batch.", "hyperparams": {"alpha": 2.0, "beta": 1.0}, "operators_used": ["logsigmoid", "softplus", "tanh", "rank_gap"], "implementation_hint": {"expects": ["cost_a", "cost_b", "log_prob_w", "log_prob_l", "weight"], "returns": "scalar"}, "code": "import torch\nimport torch.nn.functional as F\n\ndef generated_loss(batch, model_output, extra):\n    \"\"\"\n    Computes the Rank-Gapped Adaptive Sigmoid Loss.\n\n    This loss combines an additive margin and a multiplicative scaler, both derived from\n    the rank-normalized cost difference between solution pairs.\n\n    Args:\n        batch (dict): A dictionary containing tensors.\n            'cost_a' (Tensor [N]): Cost of solution 'a'.\n            'cost_b' (Tensor [N]): Cost of solution 'b'.\n            'log_prob_w' (Tensor [N]): Log probability of the winning solution.\n            'log_prob_l' (Tensor [N]): Log probability of the losing solution.\n            'weight' (Tensor [N], optional): Per-pair loss weights.\n        model_output (dict): Model outputs (not used).\n        extra (dict): A dictionary for hyperparameters.\n            'alpha' (float): Controls the magnitude of the additive margin.\n            'beta' (float): Controls the magnitude of the multiplicative scaler.\n\n    Returns:\n        torch.Tensor: A scalar tensor representing the mean loss.\n    \"\"\"\n\n    # Whitelisted operators implemented as helper functions\n    def rank_gap(x, eps=1e-9):\n        \"\"\"Converts a tensor to its normalized ranks within the range [-0.5, 0.5].\"\"\"\n        if x.numel() <= 1:\n            return torch.zeros_like(x)\n        ranks = x.to(torch.float32).argsort().argsort().to(x.dtype)\n        # Normalize ranks to [0, 1]\n        normalized_ranks = ranks / (x.numel() - 1 + eps)\n        # Shift to [-0.5, 0.5]\n        return normalized_ranks - 0.5\n\n    # Retrieve hyperparameters\n    alpha = extra.get('alpha', 2.0)\n    beta = extra.get('beta', 1.0)\n\n    # Unpack tensors from the batch\n    cost_a = batch['cost_a']\n    cost_b = batch['cost_b']\n    log_prob_w = batch['log_prob_w']\n    log_prob_l = batch['log_prob_l']\n    weights = batch.get('weight', None)\n\n    # 1. Calculate the difference in log probabilities\n    logp_diff = log_prob_w - log_prob_l\n\n    # 2. Calculate absolute cost difference\n    cost_diff = torch.abs(cost_a - cost_b)\n\n    # 3. (New Coupling) Use rank_gap for stable, scale-invariant cost signal\n    # This is robust to cost outliers and focuses on relative preference strength.\n    rank_gap_signal = rank_gap(cost_diff)\n\n    # 4. (Inherited from Parent 0) Create an additive margin\n    # softplus ensures the margin is always non-negative and smooth.\n    # The margin increases as the cost difference rank increases.\n    additive_margin = F.softplus(alpha * rank_gap_signal)\n\n    # 5. (Inherited from Parent 1 & New Coupling) Create a multiplicative scaler\n    # tanh bounds the scaler, preventing instability while providing a smooth signal.\n    # The scaler approaches 1 for high-rank cost differences and is lower for low-rank ones.\n    multiplicative_scaler = torch.tanh(beta * rank_gap_signal).clamp(min=0)\n\n    # 6. Combine the components into the final loss argument\n    # We want scaler * logp_diff > margin.\n    argument = multiplicative_scaler * logp_diff - additive_margin\n\n    # 7. Compute the final loss using numerically stable logsigmoid\n    # This is equivalent to -log(sigmoid(argument)).\n    loss_per_item = -F.logsigmoid(argument)\n\n    # 8. Apply optional weights and compute the mean loss\n    if weights is not None:\n        loss = (loss_per_item * weights).sum() / weights.sum().clamp(min=1e-8)\n    else:\n        loss = loss_per_item.mean()\n\n    return loss"}, "static_ok": true, "static_reason": "", "dynamic_ok": true, "dynamic_reason": "ok", "loss_value": 1.1398158073425293, "grad_norm": 0.0}
{"generation": 1, "index": 4, "ir": {"name": "Adaptive Rank-Gap Sigmoid Loss", "intuition": "This loss function merges the concept of a cost-dependent margin with a rank-based scaling mechanism to create a stable and robust preference learning objective.\n\nInherited Ideas:\n- From Parent 0, it inherits the core structure of a logistic loss with an explicit, additive margin: `loss = -logsigmoid(logp_diff - margin)`. This structure clearly separates the model's output difference from the target preference margin.\n- From Parent 1, it inherits the idea of using the cost difference as a primary signal to scale the loss's intensity, but applies it in a different way. The cost difference is used to construct the margin itself, rather than scaling the log-probability difference directly.\n\nNew Coupling Ideas:\n1.  **Rank-Gap Normalization**: Instead of using z-score or L2 normalization on the raw cost differences, which can be sensitive to outliers, this loss uses a rank-based normalization (`rank_gap`). This operator transforms the cost differences into their fractional ranks, mapping them to a stable `[0, 1]` range. This makes the margin's construction robust to the absolute scale and distribution of costs, focusing only on their relative ordering.\n2.  **Tanh-Shaped Margin**: The rank-gapped cost difference is passed through a `tanh` function, scaled by a `beta` hyperparameter. This creates a smooth, S-shaped margin that is near zero for very small cost differences (giving the model slack) and saturates at a maximum value for large cost differences (preventing excessively large margins from causing instability). This provides more granular control over the margin's shape compared to the unbounded `softplus` function.", "pseudocode": "1. Calculate the difference in log probabilities: `logp_diff = logp_winner - logp_loser`.\n2. Calculate the absolute difference in costs: `cost_diff = abs(cost_winner - cost_loser)`.\n3. Normalize the cost differences using rank-gap normalization to map them to a stable [0, 1] range: `ranked_cost_diff = rank_gap(cost_diff)`.\n4. Construct a smooth, bounded margin using the hyperbolic tangent function. The margin increases with the ranked cost difference but saturates: `margin = alpha * tanh(beta * ranked_cost_diff)`.\n5. Compute the final loss using the logistic loss structure with the new margin: `loss = -logsigmoid(logp_diff - margin)`.\n6. Return the weighted mean of the loss over the batch.", "hyperparams": {"alpha": 1.0, "beta": 3.0}, "operators_used": ["logsigmoid", "tanh", "rank_gap"], "implementation_hint": {"expects": ["cost_a", "cost_b", "log_prob_w", "log_prob_l", "weight"], "returns": "scalar"}, "code": "import torch\nimport torch.nn.functional as F\n\ndef generated_loss(batch, model_output, extra):\n    \"\"\"\n    Computes the Adaptive Rank-Gap Sigmoid Loss.\n\n    This loss combines an explicit margin from Parent 0 with a novel rank-based\n    normalization scheme. The margin is shaped by a tanh function, creating a smooth,\n    bounded target for the log-probability difference that is robust to cost outliers.\n\n    Args:\n        batch (dict): A dictionary containing tensors:\n            'cost_a' (Tensor [N]): Cost of the first solution.\n            'cost_b' (Tensor [N]): Cost of the second solution.\n            'log_prob_w' (Tensor [N]): Log probability of the winning solution.\n            'log_prob_l' (Tensor [N]): Log probability of the losing solution.\n            'weight' (Tensor [N], optional): Per-pair loss weights.\n        model_output (dict): Model outputs (not used).\n        extra (dict): A dictionary for hyperparameters:\n            'alpha' (float): The maximum value of the margin (scales tanh output).\n            'beta' (float): Controls the steepness of the margin's growth with cost difference.\n\n    Returns:\n        torch.Tensor: A scalar tensor representing the mean loss.\n    \"\"\"\n\n    # Whitelisted operator for rank-based normalization\n    def rank_gap(x, eps=1e-8):\n        \"\"\"Transforms values to their fractional rank in [0, 1].\"\"\"\n        if x.numel() <= 1:\n            return torch.zeros_like(x)\n        # stable_sort returns values and indices\n        _, ranks = torch.stable_sort(x)\n        # Convert ranks to a float tensor for division\n        ranks_float = torch.empty_like(ranks, dtype=torch.float32)\n        ranks_float[ranks] = torch.arange(x.numel(), device=x.device, dtype=torch.float32)\n        # Normalize to [0, 1]\n        normalized_ranks = ranks_float / (x.numel() - 1 + eps)\n        return normalized_ranks\n\n    # Retrieve hyperparameters from the 'extra' dictionary\n    alpha = extra.get('alpha', 1.0)\n    beta = extra.get('beta', 3.0)\n\n    # Unpack tensors from the batch dictionary\n    cost_a = batch['cost_a']\n    cost_b = batch['cost_b']\n    log_prob_w = batch['log_prob_w']\n    log_prob_l = batch['log_prob_l']\n    weights = batch.get('weight', None)\n\n    # 1. Calculate the difference in log probabilities (inherited structure)\n    logp_diff = log_prob_w - log_prob_l\n\n    # 2. Calculate the absolute cost difference\n    cost_diff = torch.abs(cost_a - cost_b)\n\n    # 3. Normalize cost difference using rank-gap (new coupling idea)\n    # This makes the margin robust to the scale and distribution of costs\n    ranked_cost_diff = rank_gap(cost_diff)\n\n    # 4. Construct a smooth, bounded margin using tanh (new coupling idea)\n    # The margin is near 0 for small cost differences and saturates towards `alpha`.\n    # `beta` controls how quickly the margin saturates.\n    margin = alpha * torch.tanh(beta * ranked_cost_diff)\n\n    # 5. Compute the core loss using a logistic structure (inherited from Parent 0)\n    # We want logp_diff > margin.\n    loss_per_item = -F.logsigmoid(logp_diff - margin)\n\n    # 6. Apply optional weights and compute the mean loss\n    if weights is not None:\n        # Guard against sum of weights being zero\n        sum_weights = weights.sum().clamp(min=1e-8)\n        loss = (loss_per_item * weights).sum() / sum_weights\n    else:\n        loss = loss_per_item.mean()\n\n    return loss"}, "static_ok": true, "static_reason": "", "dynamic_ok": false, "dynamic_reason": "forward_error: module 'torch' has no attribute 'stable_sort'", "loss_value": null, "grad_norm": null}
{"generation": 1, "index": 5, "ir": {"name": "Soft-Clipped Adaptive Margin Loss", "intuition": "This loss function synthesizes an additive margin from Parent 0 with a multiplicative scaling factor from Parent 1, coupled with a novel clipping mechanism to enhance stability and focus.\n\nInherited Ideas:\n- From Parent 0 (`Adaptive Margin Sigmoid Loss`): We inherit the core structure of an additive, cost-dependent margin, `loss = -logsigmoid(logp_diff - margin)`. The margin is designed to be larger for pairs with a greater cost difference, demanding a stronger preference signal from the model for clear-cut cases.\n- From Parent 1 (`Adaptive Margin Sigmoid Loss`): We borrow the idea of using `softplus` on the cost difference to create a smooth, non-negative signal that acts as the basis for the margin. This ensures the margin is always positive and differentiable.\n\nNew Coupling Ideas & Modifications:\n1.  **Dual-Component Margin:** Instead of just an additive margin, we construct it from two parts: a base margin (`beta`) and a cost-adaptive component. The adaptive part uses a `zscore` normalized absolute cost difference, similar to Parent 0, but is scaled by a hyperparameter `alpha`.\n2.  **Soft-Clipping with `tanh`:** The key innovation is to 'soft-clip' the log-probability difference (`logp_diff`) before applying the loss. We use `tanh(logp_diff)` to bound the difference within `[-1, 1]`. This prevents extremely confident (and potentially incorrect) predictions from generating massive, destabilizing gradients. It encourages the model to learn the correct preference direction without over-investing in making the log-probability gap infinitely large, leading to more stable training.\n3.  **Rank-Gap Normalization:** We introduce a `rank_gap` based normalization for the cost difference. This non-parametric approach is robust to the scale and distribution of costs, unlike standard normalization which is sensitive to outliers. The rank gap provides a stable, ordinal measure of how significant the cost difference is within the batch.", "pseudocode": "1. Calculate the log-probability difference: `logp_diff = logp_winner - logp_loser`.\n2. Soft-clip the log-probability difference using the hyperbolic tangent function: `clipped_logp_diff = tanh(logp_diff)`. This bounds the value to the range [-1, 1] for stability.\n3. Calculate the absolute cost difference: `cost_diff = abs(cost_winner - cost_loser)`.\n4. Normalize the cost difference using a rank-gap transformation to get a stable, ordinal signal `rank_gap_cost`.\n5. Construct an adaptive margin. Start with a base margin `beta`. Add a cost-sensitive component by scaling the rank-gapped cost difference with a hyperparameter `alpha`. Ensure the result is non-negative and smooth by applying `softplus`: `margin = softplus(beta + alpha * rank_gap_cost)`.\n6. Compute the final loss using a logistic loss structure on the clipped logp difference with the adaptive margin: `loss = -logsigmoid(clipped_logp_diff - margin)`.\n7. Return the weighted mean of the loss over the batch.", "hyperparams": {"alpha": 2.0, "beta": 0.5}, "operators_used": ["logsigmoid", "softplus", "tanh", "rank_gap"], "implementation_hint": {"expects": ["cost_a", "cost_b", "logp_w", "logp_l"], "returns": "scalar"}, "code": "import torch\nimport torch.nn.functional as F\n\ndef generated_loss(batch, model_output, extra):\n    \"\"\"\n    Computes the Soft-Clipped Adaptive Margin Loss.\n\n    This loss combines an additive, cost-adaptive margin with a tanh-based soft clipping\n    of the log-probability difference to improve training stability.\n\n    Args:\n        batch (dict): A dictionary containing tensors.\n            'cost_a' (Tensor [N]): Cost of solution 'a'.\n            'cost_b' (Tensor [N]): Cost of solution 'b'.\n            'log_prob_w' (Tensor [N]): Log probability of the winning solution.\n            'log_prob_l' (Tensor [N]): Log probability of the losing solution.\n            'weight' (Tensor [N], optional): Per-pair loss weights. Defaults to 1.\n        model_output (dict): Model outputs (not used in this loss).\n        extra (dict): A dictionary for hyperparameters.\n            'alpha' (float): Scales the sensitivity of the margin to the cost rank gap.\n            'beta' (float): A fixed base margin.\n\n    Returns:\n        torch.Tensor: A scalar tensor representing the mean loss.\n    \"\"\"\n\n    # Whitelisted operators implemented as helper functions\n    def rank_gap(x, eps=1e-8):\n        \"\"\"Non-parametric normalization based on rank statistics.\"\"\"\n        if x.numel() <= 1:\n            return torch.zeros_like(x)\n        # Get the rank of each element\n        ranks = x.argsort().argsort().float()\n        # Normalize ranks to [0, 1]\n        normalized_ranks = ranks / (x.numel() - 1 + eps)\n        return normalized_ranks\n\n    # Retrieve hyperparameters from the 'extra' dictionary\n    alpha = extra.get('alpha', 2.0)\n    beta = extra.get('beta', 0.5)\n\n    # Unpack tensors from the batch dictionary\n    cost_a = batch['cost_a']\n    cost_b = batch['cost_b']\n    log_prob_w = batch['log_prob_w']\n    log_prob_l = batch['log_prob_l']\n    weights = batch.get('weight', None)\n\n    # 1. Calculate the log-probability difference\n    logp_diff = log_prob_w - log_prob_l\n\n    # 2. (New Coupling) Soft-clip the logp difference with tanh for stability\n    # This bounds the input to the loss, preventing extreme gradients.\n    clipped_logp_diff = torch.tanh(logp_diff)\n\n    # 3. Calculate the absolute cost difference\n    cost_diff = torch.abs(cost_a - cost_b)\n\n    # 4. (New Coupling) Normalize cost difference using rank_gap for robustness\n    # This provides a stable, ordinal measure of preference strength.\n    rank_gap_cost = rank_gap(cost_diff)\n\n    # 5. (Inherited) Construct an adaptive margin using softplus\n    # Combines a base margin (beta) with a cost-sensitive part (alpha * rank_gap_cost).\n    # Inherits softplus from Parent 1 and additive margin structure from Parent 0.\n    adaptive_margin = F.softplus(beta + alpha * rank_gap_cost)\n\n    # 6. (Inherited) Compute the core loss with the additive margin\n    # We want clipped_logp_diff to be larger than the adaptive_margin.\n    loss_per_item = -F.logsigmoid(clipped_logp_diff - adaptive_margin)\n\n    # 7. Apply optional weights and compute the mean loss\n    if weights is not None:\n        # Guard against sum of weights being zero\n        weighted_sum = (loss_per_item * weights).sum()\n        weight_sum = weights.sum().clamp(min=1e-8)\n        loss = weighted_sum / weight_sum\n    else:\n        loss = loss_per_item.mean()\n\n    return loss"}, "static_ok": true, "static_reason": "", "dynamic_ok": true, "dynamic_reason": "ok", "loss_value": 1.9086496829986572, "grad_norm": 0.0}
{"generation": 1, "index": 6, "ir": {"name": "Rank-Gapped Adaptive Sigmoid Loss", "intuition": "This loss function combines an adaptive margin with a rank-based cost signal, introducing a schedule to control the margin's influence over time. \n\nInherited Ideas:\n- From Parent 0, we inherit the core structure `loss = -logsigmoid(logp_diff - margin)`, where a positive, adaptive margin is subtracted from the log-probability difference. This forces the model to create a larger probability gap for pairs with a larger margin.\n- From Parent 1, we adopt the idea of using `softplus` on the cost difference to create a smooth, non-negative signal that drives the margin's size. This ensures the margin is always positive and differentiable.\n\nNew Coupling Ideas:\n1.  **Rank-Gap Cost Signal**: Instead of using the raw or z-scored cost difference, which can be noisy or sensitive to outliers, we use `rank_gap`. This operator computes the difference in the ranks of the winner's and loser's costs within the batch. This creates a robust, bounded, and relative measure of cost difference, making the margin less sensitive to the absolute scale of costs and more focused on the relative ordering.\n2.  **Margin Schedule**: A dynamic hyperparameter, `margin_strength`, is introduced. It can be scheduled (e.g., annealed from 0 to a target value) during training. This allows the model to first learn basic preferences without a margin (when `margin_strength=0`) and then gradually introduces the margin to refine its predictions and focus on more significant cost differences. This stabilizes early training and provides a curriculum for the model.", "pseudocode": "1. Calculate the difference in log probabilities: `logp_diff = logp_winner - logp_loser`.\n2. Calculate the rank-gap of the costs: `cost_rank_gap = rank_gap(cost_winner, cost_loser)`. This provides a robust, batch-relative measure of cost separation.\n3. Transform the rank-gap into a smooth, non-negative base margin using softplus: `base_margin = softplus(cost_rank_gap)`.\n4. Introduce a schedulable `margin_strength` hyperparameter. Scale the base margin by this strength: `adaptive_margin = margin_strength * base_margin`.\n5. Compute the final loss using a logistic loss structure with the scheduled adaptive margin: `loss = -logsigmoid(logp_diff - adaptive_margin)`.\n6. Take the weighted mean of the loss over the batch.", "hyperparams": {"margin_strength": 1.0}, "operators_used": ["logsigmoid", "softplus", "rank_gap"], "implementation_hint": {"expects": ["cost_a", "cost_b", "logp_chosen", "logp_rejected", "weight"], "returns": "scalar"}, "code": "import torch\nimport torch.nn.functional as F\n\ndef generated_loss(batch, model_output, extra):\n    \"\"\"\n    Computes the Rank-Gapped Adaptive Sigmoid Loss.\n\n    This loss uses a sigmoid structure where the required margin of preference is\n    dynamically set by the rank difference of the costs within the batch. A margin\n    schedule is introduced via the 'margin_strength' hyperparameter.\n\n    Args:\n        batch (dict): A dictionary containing tensors:\n            'cost_a' (Tensor [N]): Cost of solution 'a'.\n            'cost_b' (Tensor [N]): Cost of solution 'b'.\n            'log_prob_w' (Tensor [N]): Log probability of the winning solution.\n            'log_prob_l' (Tensor [N]): Log probability of the losing solution.\n            'weight' (Tensor [N], optional): Per-pair loss weights.\n        model_output (dict): Model outputs (not used).\n        extra (dict): A dictionary for hyperparameters, including:\n            'margin_strength' (float): A schedulable parameter to scale the margin.\n\n    Returns:\n        torch.Tensor: A scalar tensor representing the mean loss.\n    \"\"\"\n\n    # Whitelisted operator implemented as a helper function\n    def rank_gap(cost_w, cost_l):\n        \"\"\"Calculates the difference in ranks between winner and loser costs.\"\"\"\n        # Concatenate costs to rank them together in the batch context\n        # cost_w is lower (better), cost_l is higher (worse)\n        all_costs = torch.cat([cost_w, cost_l])\n\n        # Get the ranks. `descending=True` means higher cost gets lower rank index (e.g., rank 0)\n        ranks = torch.argsort(torch.argsort(all_costs, descending=True))\n\n        # Split the ranks back into winner and loser components\n        rank_w, rank_l = torch.chunk(ranks, 2)\n\n        # The gap should be positive since winner has higher rank index (lower cost)\n        # We want to reward the model for correctly identifying pairs with a large rank gap\n        return (rank_w - rank_l).float()\n\n    # Retrieve hyperparameter(s) from the 'extra' dictionary\n    margin_strength = extra.get('margin_strength', 1.0)\n\n    # Unpack tensors from the batch\n    cost_a = batch['cost_a']\n    cost_b = batch['cost_b']\n    log_prob_w = batch['log_prob_w']\n    log_prob_l = batch['log_prob_l']\n    weights = batch.get('weight', None)\n\n    # Identify winner/loser costs based on which solution has the lower cost\n    is_a_winner = cost_a < cost_b\n    cost_w = torch.where(is_a_winner, cost_a, cost_b)\n    cost_l = torch.where(is_a_winner, cost_b, cost_a)\n\n    # 1. Calculate the difference in log probabilities\n    # We want logp(winner) > logp(loser)\n    logp_diff = log_prob_w - log_prob_l\n\n    # 2. Calculate the rank-gap of the costs (New Coupling Idea 1)\n    # This is a robust, batch-relative measure of cost difference\n    cost_rank_gap = rank_gap(cost_w, cost_l)\n\n    # 3. Create a smooth, non-negative base margin (Inherited from Parent 1)\n    base_margin = F.softplus(cost_rank_gap)\n\n    # 4. Scale the margin with a schedulable strength (New Coupling Idea 2)\n    adaptive_margin = margin_strength * base_margin\n\n    # 5. Compute the core loss using the adaptive margin structure (Inherited from Parent 0)\n    # The loss is -log(sigmoid(logp_diff - margin)).\n    # A larger margin (from a larger rank gap) demands a larger logp_diff.\n    loss_per_item = -F.logsigmoid(logp_diff - adaptive_margin)\n\n    # 6. Apply optional weights and compute the mean loss\n    if weights is not None and weights.numel() > 0:\n        loss = (loss_per_item * weights).sum() / weights.sum().clamp(min=1e-8)\n    else:\n        loss = loss_per_item.mean()\n\n    return loss"}, "static_ok": true, "static_reason": "", "dynamic_ok": false, "dynamic_reason": "loss 16.0000 outside soft range [-5.0, 5.0]", "loss_value": 16.0, "grad_norm": 0.0}
{"generation": 1, "index": 7, "ir": {"name": "Rank-Gated Adaptive Margin Loss", "intuition": "This loss function synergizes an additive margin with a multiplicative scaling factor, both of which are adaptive to the cost difference. \n\nInherited Ideas:\n- From Parent 0, we inherit the concept of an *additive* margin, where the loss is formulated as `-logsigmoid(logp_diff - margin)`. This provides a clear target for the log-probability difference that is directly related to the cost difference.\n- From Parent 1, we inherit the idea of a *multiplicative* scaling factor applied to the log-probability difference, `scale * logp_diff`. This adaptively penalizes incorrect preferences based on the magnitude of the cost gap.\n\nNew Coupling Ideas:\n1. **Combined Additive and Multiplicative Adaptation:** The child loss uses both an additive margin and a multiplicative scale simultaneously. The core term becomes `-logsigmoid(scale * logp_diff - margin)`. This creates a dual-pressure system: the scale amplifies the gradient for large errors, while the margin sets a minimum required separation, preventing complacency even when the preference is correct but weak.\n2. **Rank-Gap Normalization:** Instead of normalizing raw cost differences (which can be sensitive to outliers), we introduce `rank_gap` normalization. The cost differences within a batch are converted to their ranks. This makes the adaptive margin and scale robust to the absolute magnitude and distribution of costs, focusing only on the relative ordering of cost differences. For example, the pair with the largest cost difference will always have the highest rank-gap, regardless of whether that difference is 0.1 or 1000. This is combined with `zscore` on the ranks to produce a stable, standardized signal for adaptation.", "pseudocode": "1. Calculate the log-probability difference: `logp_diff = logp_winner - logp_loser`.\n2. Calculate the absolute cost difference for each pair in the batch: `cost_diff = abs(cost_winner - cost_loser)`.\n3. Create a stable adaptive signal using rank-gap normalization: `adaptive_signal = zscore(rank_gap(cost_diff))`. This converts cost differences into a standardized, rank-based measure.\n4. Inherit from Parent 0: Create an additive margin from this signal: `margin = softplus(alpha * adaptive_signal)`.\n5. Inherit from Parent 1: Create a multiplicative scale from this signal: `scale = 1.0 + beta * relu(adaptive_signal)`. ReLU ensures the scale only increases for above-average cost differences and never drops below 1.\n6. Combine the inherited ideas and new coupling: The loss argument is `scale * logp_diff - margin`.\n7. Compute the final loss using a numerically stable logistic loss form: `loss = -logsigmoid(scale * logp_diff - margin)`.\n8. Return the weighted mean of the loss over the batch.", "hyperparams": {"alpha": 1.0, "beta": 1.0}, "operators_used": ["logsigmoid", "softplus", "relu", "zscore", "rank_gap"], "implementation_hint": {"expects": ["cost_a", "cost_b", "log_prob_w", "log_prob_l", "weight"], "returns": "scalar"}, "code": "import torch\nimport torch.nn.functional as F\n\ndef generated_loss(batch, model_output, extra):\n    \"\"\"\n    Computes the Rank-Gated Adaptive Margin Loss.\n\n    This loss combines an adaptive additive margin and an adaptive multiplicative scale,\n    both derived from a rank-normalized cost difference signal.\n\n    Args:\n        batch (dict): A dictionary containing tensors.\n            'cost_a' (Tensor [N]): Cost of solution 'a'.\n            'cost_b' (Tensor [N]): Cost of solution 'b'.\n            'log_prob_w' (Tensor [N]): Log probability of the winning solution.\n            'log_prob_l' (Tensor [N]): Log probability of the losing solution.\n            'weight' (Tensor [N], optional): Per-pair loss weights. Defaults to 1.\n        model_output (dict): Model outputs (not used in this loss).\n        extra (dict): A dictionary for hyperparameters.\n            'alpha' (float): Controls the strength of the additive margin.\n            'beta' (float): Controls the strength of the multiplicative scale.\n\n    Returns:\n        torch.Tensor: A scalar tensor representing the mean loss.\n    \"\"\"\n    \n    # Whitelisted operators implemented as helper functions\n    def rank_gap(x):\n        if x.numel() <= 1:\n            return torch.zeros_like(x)\n        # Convert values to their ranks. The smallest value gets rank 0.\n        return x.argsort().argsort().float()\n\n    def zscore(x, eps=1e-8):\n        if x.numel() <= 1:\n            return x - x.mean()\n        return (x - x.mean()) / (x.std() + eps)\n\n    # Retrieve hyperparameters\n    alpha = extra.get('alpha', 1.0)\n    beta = extra.get('beta', 1.0)\n\n    # Unpack tensors from the batch dictionary\n    cost_a = batch['cost_a']\n    cost_b = batch['cost_b']\n    log_prob_w = batch['log_prob_w']\n    log_prob_l = batch['log_prob_l']\n    weights = batch.get('weight', None)\n\n    # 1. Calculate log-probability difference\n    logp_diff = log_prob_w - log_prob_l\n\n    # 2. Calculate absolute cost difference\n    cost_diff = torch.abs(cost_a - cost_b)\n\n    # 3. Create a stable adaptive signal using Rank-Gap Normalization\n    # This makes the signal robust to cost scale and outliers.\n    ranked_diff = rank_gap(cost_diff)\n    adaptive_signal = zscore(ranked_diff)\n\n    # 4. Create an additive margin (inherited from Parent 0)\n    # Use softplus for a smooth, non-negative margin.\n    margin = F.softplus(alpha * adaptive_signal)\n\n    # 5. Create a multiplicative scale (inherited from Parent 1)\n    # Use relu to ensure scale is >= 1, only increasing for pairs with\n    # an above-average cost difference (where adaptive_signal > 0).\n    scale = 1.0 + beta * F.relu(adaptive_signal)\n\n    # 6. Combine mechanisms into the loss argument\n    # The model must produce a logp_diff that overcomes the margin, and this\n    # requirement is amplified by the scale for large cost differences.\n    loss_arg = scale * logp_diff - margin\n\n    # 7. Compute the final loss using a stable logistic loss formulation\n    loss_per_item = -F.logsigmoid(loss_arg)\n\n    # 8. Apply optional weights and compute the mean loss\n    if weights is not None:\n        # Guard against sum of weights being zero\n        weighted_sum = (loss_per_item * weights).sum()\n        sum_of_weights = weights.sum().clamp(min=1e-8)\n        loss = weighted_sum / sum_of_weights\n    else:\n        loss = loss_per_item.mean()\n\n    return loss"}, "static_ok": true, "static_reason": "", "dynamic_ok": true, "dynamic_reason": "ok", "loss_value": 1.1981852054595947, "grad_norm": 0.0}
{"generation": 2, "index": 0, "ir": {"name": "Soft-Clipped Rank-Gapped Dual-Component Loss", "intuition": "This loss function creates a highly adaptive learning signal by synergizing an additive margin with a multiplicative scaler, while also ensuring stability through soft-clipping.\n\nInherited Ideas:\n- From Parent 0, we inherit the concept of a **multiplicative scaler** that amplifies the gradient for pairs with a large cost difference. The loss argument is scaled by this factor, making the model pay more attention to significant mis-rankings: `scaler * (logp_diff - margin)`.\n- From Parent 1, we inherit the idea of **soft-clipping the log-probability difference** using `tanh`. This bounds the `logp_diff` to `[-1, 1]`, preventing extremely confident but incorrect predictions from generating massive, destabilizing gradients. It focuses the model on getting the preference direction right rather than maximizing the log-probability gap.\n\nNew Coupling Ideas & Modifications:\n1. **Rank-Gap Driven Components**: Both the additive margin and the multiplicative scaler are driven by a `rank_gap` normalization of the cost difference. This non-parametric approach is robust to cost outliers and scale, providing a stable ordinal signal of preference strength within the batch. This is an idea present in both parents, but we use it as the sole driver for both components.\n2. **Gated Multiplicative Scaler**: The multiplicative scaler is constructed as `1.0 + relu(tanh(beta * rank_gap_signal))`. This design ensures the scaler is always `≥ 1`. The `tanh` provides a smooth, bounded signal from the rank-gap, and the `relu` ensures that only pairs with a positive rank-gap (i.e., a cost difference greater than the median) receive an amplified gradient. This focuses the amplification on the more important half of the batch preferences.", "pseudocode": "1. Calculate the difference in log probabilities: `logp_diff = logp_winner - logp_loser`.\n2. (Inherited from Parent 1) Soft-clip the log-probability difference using `tanh` for stability: `clipped_logp_diff = tanh(logp_diff)`.\n3. Calculate the absolute difference in costs: `cost_diff = abs(cost_winner - cost_loser)`.\n4. (New Coupling) Convert cost differences into a normalized rank signal between -0.5 and 0.5 using the `rank_gap` operator. This provides a stable, scale-invariant signal: `rank_gap_signal`.\n5. Construct an additive margin using `softplus` on the scaled rank-gap signal: `margin = softplus(alpha * rank_gap_signal)`. This ensures the margin is smooth, non-negative, and grows with the significance of the cost difference.\n6. (Inherited from Parent 0 & New Coupling) Construct a gated multiplicative scaler. The rank-gap signal is passed through `tanh`, then `relu`, and added to a base of 1.0: `scaler = 1.0 + relu(tanh(beta * rank_gap_signal))`. This ensures the scaler is always >= 1 and only amplifies gradients for pairs with above-median cost differences.\n7. Combine the components. The final loss argument is `scaler * (clipped_logp_diff - margin)`.\n8. Compute the final loss using a numerically stable logistic loss: `loss = -logsigmoid(argument)`.\n9. Return the weighted mean of the loss over the batch.", "hyperparams": {"alpha": 1.5, "beta": 2.0}, "operators_used": ["logsigmoid", "softplus", "tanh", "relu", "rank_gap"], "implementation_hint": {"expects": ["cost_a", "cost_b", "logp_w", "logp_l", "weight"], "returns": "scalar"}, "code": "import torch\nimport torch.nn.functional as F\n\ndef generated_loss(batch, model_output, extra):\n    \"\"\"\n    Computes the Soft-Clipped Rank-Gapped Dual-Component Loss.\n\n    This loss combines a multiplicative scaler (Parent 0) with a soft-clipped logp_diff\n    (Parent 1). It introduces a gated scaler and uses rank-gap normalization for both\n    the additive margin and the scaler to enhance stability and adaptivity.\n\n    Args:\n        batch (dict): A dictionary containing tensors.\n            'cost_a' (Tensor [N]): Cost of solution 'a'.\n            'cost_b' (Tensor [N]): Cost of solution 'b'.\n            'log_prob_w' (Tensor [N]): Log probability of the winning solution.\n            'log_prob_l' (Tensor [N]): Log probability of the losing solution.\n            'weight' (Tensor [N], optional): Per-pair loss weights.\n        model_output (dict): Model outputs (not used).\n        extra (dict): A dictionary for hyperparameters.\n            'alpha' (float): Controls the magnitude of the additive margin.\n            'beta' (float): Controls the sensitivity of the multiplicative scaler.\n\n    Returns:\n        torch.Tensor: A scalar tensor representing the mean loss.\n    \"\"\"\n\n    # Whitelisted operators implemented as helper functions\n    def rank_gap(x, eps=1e-9):\n        \"\"\"Converts a tensor to its normalized ranks within the range [-0.5, 0.5].\"\"\"\n        if x.numel() <= 1:\n            return torch.zeros_like(x)\n        ranks = x.to(torch.float32).argsort().argsort().to(x.dtype)\n        normalized_ranks = ranks / (x.numel() - 1 + eps)\n        return normalized_ranks - 0.5\n\n    # Retrieve hyperparameters\n    alpha = extra.get('alpha', 1.5)\n    beta = extra.get('beta', 2.0)\n\n    # Unpack tensors from the batch\n    cost_a = batch['cost_a']\n    cost_b = batch['cost_b']\n    log_prob_w = batch['log_prob_w']\n    log_prob_l = batch['log_prob_l']\n    weights = batch.get('weight', None)\n\n    # 1. Calculate the difference in log probabilities\n    logp_diff = log_prob_w - log_prob_l\n\n    # 2. (Inherited from Parent 1) Soft-clip logp_diff for stability\n    clipped_logp_diff = torch.tanh(logp_diff)\n\n    # 3. Calculate absolute cost difference\n    cost_diff = torch.abs(cost_a - cost_b)\n\n    # 4. (New Coupling) Use rank_gap for stable, scale-invariant cost signal\n    rank_gap_signal = rank_gap(cost_diff)\n\n    # 5. Construct an adaptive additive margin\n    # softplus ensures the margin is always non-negative and smooth.\n    margin = F.softplus(alpha * rank_gap_signal)\n\n    # 6. (Inherited from Parent 0 & New Coupling) Construct a gated multiplicative scaler\n    # The scaler is >= 1 and only > 1 for pairs with above-median cost differences.\n    scaler = 1.0 + F.relu(torch.tanh(beta * rank_gap_signal))\n\n    # 7. Combine the components into the final loss argument\n    # We want clipped_logp_diff > margin, and we scale the penalty by the scaler.\n    argument = scaler * (clipped_logp_diff - margin)\n\n    # 8. Compute the final loss using numerically stable logsigmoid\n    loss_per_item = -F.logsigmoid(argument)\n\n    # 9. Apply optional weights and compute the mean loss\n    if weights is not None:\n        loss = (loss_per_item * weights).sum() / weights.sum().clamp(min=1e-8)\n    else:\n        loss = loss_per_item.mean()\n\n    return loss"}, "static_ok": true, "static_reason": "", "dynamic_ok": true, "dynamic_reason": "ok", "loss_value": 1.2981805801391602, "grad_norm": 0.0}
{"generation": 2, "index": 1, "ir": {"name": "Soft-Clipped Rank-Adaptive Loss", "intuition": "This loss function creates a dynamic learning signal by combining a soft-clipped log-probability difference with both an adaptive margin and an adaptive scaler, which are themselves coupled through the cost difference.\n\nInherited Ideas:\n- From Parent 0 (`Rank-Gapped Adaptive Sigmoid Loss`), we inherit the idea of using a **multiplicative scaler** that modulates the log-probability difference. This scaler, `scaler = tanh(beta * rank_gap)`, increases the penalty for mis-ranking pairs with a large cost difference, focusing the model's attention where it matters most.\n- From Parent 1 (`Soft-Clipped Adaptive Margin Loss`), we inherit two key concepts: \n  1. The use of an **additive, cost-dependent margin**, `margin = softplus(base_margin + alpha * rank_gap)`, which sets a minimum required log-probability gap that grows with the cost difference.\n  2. The **soft-clipping of the log-probability difference** using `tanh(logp_diff)`. This stabilizes training by preventing extremely large log-probability differences from creating explosive gradients, encouraging the model to learn the correct preference direction without over-investing in any single pair.\n\nNew Coupling Ideas:\n1. **Unified Rank-Gap Signal:** Both the adaptive margin (from Parent 1) and the adaptive scaler (from Parent 0) are driven by the same `rank_gap` normalized cost difference. This creates a strong, unified coupling where both the required preference strength (margin) and the penalty for failure (scaler) increase in unison based on the relative importance of a pair within the batch. The `rank_gap` operator ensures this signal is robust to cost outliers and scale.\n2. **Dynamic Margin-Scaler Interaction:** The final loss argument is `scaler * clipped_logp_diff - margin`. By having both `scaler` and `margin` grow with the cost rank, we create a challenging learning objective for high-importance pairs. The model must not only overcome a larger `margin` but must do so while its `logp_diff` is being amplified by a larger `scaler`, leading to a more nuanced and powerful gradient signal.", "pseudocode": "1. Calculate the difference in log probabilities: `logp_diff = logp_winner - logp_loser`.\n2. (Inherited from Parent 1) Soft-clip the log-probability difference for stability: `clipped_logp_diff = tanh(logp_diff)`.\n3. Calculate the absolute difference in costs: `cost_diff = abs(cost_winner - cost_loser)`.\n4. (New Coupling) Create a unified, robust signal by converting cost differences into normalized ranks within the batch using the `rank_gap` operator: `rank_gap_signal`.\n5. (Inherited from Parent 1) Construct an adaptive additive margin. Apply a `softplus` function to a base margin (`gamma`) plus the `rank_gap_signal` scaled by a hyperparameter `alpha`. This creates a smooth, non-negative margin that increases with the cost rank: `margin = softplus(gamma + alpha * rank_gap_signal)`.\n6. (Inherited from Parent 0) Construct an adaptive multiplicative scaler. Apply a `tanh` activation to the `rank_gap_signal` scaled by a hyperparameter `beta`. This creates a bounded scaler between 0 and 1: `scaler = tanh(beta * rank_gap_signal)`.\n7. Combine the components. The final argument for the loss is `scaler * clipped_logp_diff - margin`.\n8. Compute the final loss using a numerically stable logistic loss: `loss = -logsigmoid(argument)`.\n9. Return the weighted mean of the loss over the batch.", "hyperparams": {"alpha": 2.0, "beta": 2.0, "gamma": 0.1}, "operators_used": ["logsigmoid", "softplus", "tanh", "rank_gap"], "implementation_hint": {"expects": ["cost_a", "cost_b", "logp_w", "logp_l", "weight"], "returns": "scalar"}, "code": "import torch\nimport torch.nn.functional as F\n\ndef generated_loss(batch, model_output, extra):\n    \"\"\"\n    Computes the Soft-Clipped Rank-Adaptive Loss.\n\n    This loss combines a tanh-clipped log-probability difference with both an\n    additive margin and a multiplicative scaler, which are jointly driven by\n    a unified rank-normalized cost signal.\n\n    Args:\n        batch (dict): A dictionary containing tensors.\n            'cost_a' (Tensor [N]): Cost of solution 'a'.\n            'cost_b' (Tensor [N]): Cost of solution 'b'.\n            'log_prob_w' (Tensor [N]): Log probability of the winning solution.\n            'log_prob_l' (Tensor [N]): Log probability of the losing solution.\n            'weight' (Tensor [N], optional): Per-pair loss weights.\n        model_output (dict): Model outputs (not used).\n        extra (dict): A dictionary for hyperparameters.\n            'alpha' (float): Controls the sensitivity of the additive margin.\n            'beta' (float): Controls the sensitivity of the multiplicative scaler.\n            'gamma' (float): A fixed base for the additive margin.\n\n    Returns:\n        torch.Tensor: A scalar tensor representing the mean loss.\n    \"\"\"\n\n    # Whitelisted operators implemented as helper functions\n    def rank_gap(x, eps=1e-9):\n        \"\"\"Converts a tensor to its normalized ranks within the range [-0.5, 0.5].\"\"\"\n        if x.numel() <= 1:\n            return torch.zeros_like(x)\n        ranks = x.to(torch.float32).argsort().argsort().to(x.dtype)\n        normalized_ranks = ranks / (x.numel() - 1 + eps)\n        return normalized_ranks - 0.5\n\n    # Retrieve hyperparameters\n    alpha = extra.get('alpha', 2.0)\n    beta = extra.get('beta', 2.0)\n    gamma = extra.get('gamma', 0.1)\n\n    # Unpack tensors from the batch\n    cost_a = batch['cost_a']\n    cost_b = batch['cost_b']\n    log_prob_w = batch['log_prob_w']\n    log_prob_l = batch['log_prob_l']\n    weights = batch.get('weight', None)\n\n    # 1. Calculate the difference in log probabilities\n    logp_diff = log_prob_w - log_prob_l\n\n    # 2. (Inherited from Parent 1) Soft-clip logp_diff with tanh for stability\n    clipped_logp_diff = torch.tanh(logp_diff)\n\n    # 3. Calculate absolute cost difference\n    cost_diff = torch.abs(cost_a - cost_b)\n\n    # 4. (New Coupling) Use rank_gap for a unified, robust cost signal\n    rank_gap_signal = rank_gap(cost_diff)\n\n    # 5. (Inherited from Parent 1) Construct an adaptive additive margin\n    # The margin increases as the cost difference rank increases.\n    additive_margin = F.softplus(gamma + alpha * rank_gap_signal)\n\n    # 6. (Inherited from Parent 0) Construct an adaptive multiplicative scaler\n    # The scaler, bounded by tanh, increases penalty with cost rank.\n    # Clamp(min=0) ensures the scaler is non-negative, as rank_gap can be negative.\n    multiplicative_scaler = torch.tanh(beta * rank_gap_signal).clamp(min=0)\n\n    # 7. Combine components into the final loss argument\n    # We want scaler * clipped_logp_diff > margin\n    argument = multiplicative_scaler * clipped_logp_diff - additive_margin\n\n    # 8. Compute the final loss using numerically stable logsigmoid\n    loss_per_item = -F.logsigmoid(argument)\n\n    # 9. Apply optional weights and compute the mean loss\n    if weights is not None:\n        loss = (loss_per_item * weights).sum() / weights.sum().clamp(min=1e-8)\n    else:\n        loss = loss_per_item.mean()\n\n    return loss"}, "static_ok": true, "static_reason": "", "dynamic_ok": true, "dynamic_reason": "ok", "loss_value": 1.175480604171753, "grad_norm": 0.0}
{"generation": 2, "index": 2, "ir": {"name": "Soft-Clipped Rank-Modulated Loss", "intuition": "This loss function creates a dynamic learning signal by combining a stable, soft-clipped log-probability difference with a dual-purpose modulation factor derived from cost ranks. It aims to be robust to outliers in both cost and model probabilities while dynamically adjusting the learning objective based on the clarity of preference.\n\nInherited Ideas:\n- From Parent 1 (`Soft-Clipped Adaptive Margin Loss`): It inherits the idea of **soft-clipping the log-probability difference** using `tanh(logp_diff)`. This bounds the logp difference to `[-1, 1]`, preventing extremely confident predictions from causing numerical instability and dominating the gradient.\n- From Parent 0 (`Rank-Gapped Adaptive Sigmoid Loss`): It adopts the concept of using a **multiplicative scaler** that modulates the log-probability difference. This scaler, `1 + cost_signal`, amplifies the importance of getting pairs with large cost differences correct.\n- From both parents: The use of `rank_gap` to create a stable, non-parametric, and scale-invariant signal from the cost differences is a core shared feature that is also inherited.\n\nNew Coupling Ideas & Modifications:\n1.  **Dual-Purpose Cost Modulation**: The key innovation is a single, rank-derived signal that acts as both a multiplicative scaler and an additive margin. We compute `cost_signal = tanh(alpha * rank_gap(cost_diff))`, which is a bounded value between `[-1, 1]`. The final loss argument becomes `(1 + cost_signal) * clipped_logp_diff - cost_signal`. When the cost difference is large, `cost_signal` approaches 1, pushing the loss towards `2 * clipped_logp_diff - 1`, demanding a strong, positive logp difference. When the cost difference is small, `cost_signal` approaches 0, and the loss simplifies to `clipped_logp_diff`, a standard hinge-like objective on the clipped logp difference.\n2.  **Simplified Hyperparameterization**: By coupling the scaler and margin through a single `cost_signal` term, the design is simplified to a single hyperparameter, `alpha`, which controls the sensitivity to the cost rank. This makes the loss easier to tune and understand.", "pseudocode": "1. Calculate the difference in log probabilities: `logp_diff = logp_winner - logp_loser`.\n2. (Inherited from Parent 1) Soft-clip the log-probability difference using `tanh` for stability: `clipped_logp_diff = tanh(logp_diff)`.\n3. Calculate the absolute difference in costs: `cost_diff = abs(cost_winner - cost_loser)`.\n4. (Inherited from both parents) Convert the cost differences into a normalized rank signal within the batch using the `rank_gap` operator to ensure robustness to cost scale and outliers.\n5. (New Coupling) Create a single, bounded cost modulation signal. Apply a scaled `tanh` function to the rank-gapped signal: `cost_signal = tanh(alpha * rank_gap_signal)`. This signal will range from approximately 0 for small cost differences to 1 for large ones.\n6. (Inherited from Parent 0 & New Coupling) Construct the loss argument by using `cost_signal` as both a multiplicative scaler for `clipped_logp_diff` and as an additive margin. The argument is `(1 + cost_signal) * clipped_logp_diff - cost_signal`.\n7. Compute the final loss using a numerically stable logistic loss function: `loss = -logsigmoid(argument)`.\n8. Return the weighted mean of the loss over the batch.", "hyperparams": {"alpha": 4.0}, "operators_used": ["logsigmoid", "tanh", "rank_gap"], "implementation_hint": {"expects": ["cost_a", "cost_b", "log_prob_w", "log_prob_l", "weight"], "returns": "scalar"}, "code": "import torch\nimport torch.nn.functional as F\n\ndef generated_loss(batch, model_output, extra):\n    \"\"\"\n    Computes the Soft-Clipped Rank-Modulated Loss.\n\n    This loss combines a tanh-clipped log-probability difference with a single,\n    rank-derived signal that acts as both a multiplicative scaler and an additive margin,\n    creating a dynamic and stable learning objective.\n\n    Args:\n        batch (dict): A dictionary containing tensors.\n            'cost_a' (Tensor [N]): Cost of solution 'a'.\n            'cost_b' (Tensor [N]): Cost of solution 'b'.\n            'log_prob_w' (Tensor [N]): Log probability of the winning solution.\n            'log_prob_l' (Tensor [N]): Log probability of the losing solution.\n            'weight' (Tensor [N], optional): Per-pair loss weights.\n        model_output (dict): Model outputs (not used).\n        extra (dict): A dictionary for hyperparameters.\n            'alpha' (float): Controls the sensitivity of the modulation to the cost rank.\n\n    Returns:\n        torch.Tensor: A scalar tensor representing the mean loss.\n    \"\"\"\n\n    # Whitelisted operators implemented as helper functions\n    def rank_gap(x, eps=1e-9):\n        \"\"\"Converts a tensor to its normalized ranks within the range [-0.5, 0.5].\"\"\"\n        if x.numel() <= 1:\n            return torch.zeros_like(x)\n        ranks = x.to(torch.float32).argsort().argsort().to(x.dtype)\n        # Normalize ranks to [0, 1]\n        normalized_ranks = ranks / (x.numel() - 1 + eps)\n        # Shift to [-0.5, 0.5]\n        return normalized_ranks - 0.5\n\n    # Retrieve hyperparameters\n    alpha = extra.get('alpha', 4.0)\n\n    # Unpack tensors from the batch\n    cost_a = batch['cost_a']\n    cost_b = batch['cost_b']\n    log_prob_w = batch['log_prob_w']\n    log_prob_l = batch['log_prob_l']\n    weights = batch.get('weight', None)\n\n    # 1. Calculate the difference in log probabilities\n    logp_diff = log_prob_w - log_prob_l\n\n    # 2. (Inherited from Parent 1) Soft-clip for stability\n    clipped_logp_diff = torch.tanh(logp_diff)\n\n    # 3. Calculate absolute cost difference\n    cost_diff = torch.abs(cost_a - cost_b)\n\n    # 4. (Inherited from both) Use rank_gap for a stable cost signal\n    rank_gap_signal = rank_gap(cost_diff)\n\n    # 5. (New Coupling) Create a single, bounded cost modulation signal\n    # The signal is clamped at 0 to focus on positive cost differences.\n    # It approaches 1 for high-rank differences and is near 0 for low-rank ones.\n    cost_signal = torch.tanh(alpha * rank_gap_signal).clamp(min=0)\n\n    # 6. (Inherited & New Coupling) Construct the argument for the loss function\n    # The cost_signal acts as both a multiplicative scaler and an additive margin.\n    # We want (1 + cost_signal) * clipped_logp_diff > cost_signal.\n    argument = (1 + cost_signal) * clipped_logp_diff - cost_signal\n\n    # 7. Compute the final loss using numerically stable logsigmoid\n    loss_per_item = -F.logsigmoid(argument)\n\n    # 8. Apply optional weights and compute the mean loss\n    if weights is not None:\n        loss = (loss_per_item * weights).sum() / weights.sum().clamp(min=1e-8)\n    else:\n        loss = loss_per_item.mean()\n\n    return loss"}, "static_ok": true, "static_reason": "", "dynamic_ok": true, "dynamic_reason": "ok", "loss_value": 0.8968955874443054, "grad_norm": 0.0}
{"generation": 2, "index": 3, "ir": {"name": "Soft-Clipped Rank-Adaptive Loss", "intuition": "This loss function creates a highly adaptive learning signal by combining a multiplicative scaler and a soft-clipping mechanism, both driven by the relative rank of cost differences within a batch.\n\nInherited Ideas:\n- From Parent 0 (`Rank-Gapped Adaptive Sigmoid Loss`), we inherit the concept of a **multiplicative scaler** that modulates the log-probability difference. The intuition is that pairs with a larger cost difference should contribute more to the gradient, and this scaler, `scaler = tanh(beta * rank_gap_signal)`, achieves this. The use of `tanh` is also borrowed from Parent 0's scaler design for its bounding properties.\n- From Parent 1 (`Soft-Clipped Adaptive Margin Loss`), we inherit the idea of **soft-clipping the log-probability difference** using `tanh(logp_diff)`. This is a stability trick that prevents extremely large `logp_diff` values from creating explosive gradients, encouraging the model to focus on getting the preference direction right rather than maximizing the log-probability gap indefinitely.\n\nNew Coupling Ideas & Modifications:\n1.  **Direct Scaler and Clipper Fusion:** The core innovation is how these two inherited ideas are coupled. Instead of using an additive margin, we directly fuse the scaler and the clipped log-probability difference: `argument = scaler * tanh(logp_diff)`. This creates a dynamic target: for pairs with a small cost difference (low rank), the scaler is small, so the loss is satisfied even if `tanh(logp_diff)` is small. For pairs with a large cost difference (high rank), the scaler approaches 1, pushing `tanh(logp_diff)` towards its maximum value of 1. This effectively sets a dynamic, rank-dependent target for the clipped log-probability difference.\n2.  **Rank-Gap Normalization as the Sole Driver:** Both parents use `rank_gap` to process cost information. This child simplifies the design by making the rank-gapped cost difference the *sole* driver for adapting the loss. It is used to create the scaler, which in turn defines the dynamic target for the clipped `logp_diff`. This creates a clean, non-parametric, and robust loss function, entirely driven by the ordinal relationship of cost differences in the batch.", "pseudocode": "1. Calculate the difference in log probabilities: `logp_diff = logp_winner - logp_loser`.\n2. (Inherited from Parent 1) Apply a soft-clipping function to the log-probability difference for stability: `clipped_logp_diff = tanh(logp_diff)`.\n3. Calculate the absolute difference in costs: `cost_diff = abs(cost_winner - cost_loser)`.\n4. (New Coupling & Inherited from Parents) Convert the cost differences into a normalized rank signal between -0.5 and 0.5 using the `rank_gap` operator. This provides a robust, scale-invariant measure of preference significance.\n5. (Inherited from Parent 0) Create a multiplicative scaler from the rank-gapped signal. The scaler is bounded and smooth: `scaler = tanh(beta * rank_gap_signal)`. We clamp it at zero to ensure it's non-negative.\n6. (New Coupling) Fuse the scaler and the clipped log-probability difference. The goal is to make the product `scaler * clipped_logp_diff` as large as possible. This is achieved by minimizing `-logsigmoid(scaler * clipped_logp_diff)`.\n7. Compute the final loss using a numerically stable logistic loss: `loss = -logsigmoid(scaler * clipped_logp_diff)`.\n8. Return the weighted mean of the loss over the batch.", "hyperparams": {"beta": 4.0}, "operators_used": ["logsigmoid", "tanh", "rank_gap"], "implementation_hint": {"expects": ["cost_a", "cost_b", "log_prob_w", "log_prob_l", "weight"], "returns": "scalar"}, "code": "import torch\nimport torch.nn.functional as F\n\ndef generated_loss(batch, model_output, extra):\n    \"\"\"\n    Computes the Soft-Clipped Rank-Adaptive Loss.\n\n    This loss combines a tanh-based soft clipping of the log-probability difference\n    with a multiplicative scaler derived from the rank-normalized cost difference.\n\n    Args:\n        batch (dict): A dictionary containing tensors.\n            'cost_a' (Tensor [N]): Cost of solution 'a'.\n            'cost_b' (Tensor [N]): Cost of solution 'b'.\n            'log_prob_w' (Tensor [N]): Log probability of the winning solution.\n            'log_prob_l' (Tensor [N]): Log probability of the losing solution.\n            'weight' (Tensor [N], optional): Per-pair loss weights.\n        model_output (dict): Model outputs (not used).\n        extra (dict): A dictionary for hyperparameters.\n            'beta' (float): Controls the magnitude of the rank-based multiplicative scaler.\n\n    Returns:\n        torch.Tensor: A scalar tensor representing the mean loss.\n    \"\"\"\n\n    # Whitelisted operator implemented as a helper function\n    def rank_gap(x, eps=1e-9):\n        \"\"\"Converts a tensor to its normalized ranks within the range [-0.5, 0.5].\"\"\"\n        if x.numel() <= 1:\n            return torch.zeros_like(x)\n        ranks = x.to(torch.float32).argsort().argsort().to(x.dtype)\n        # Normalize ranks to [0, 1]\n        normalized_ranks = ranks / (x.numel() - 1 + eps)\n        # Shift to [-0.5, 0.5]\n        return normalized_ranks - 0.5\n\n    # Retrieve hyperparameters\n    beta = extra.get('beta', 4.0)\n\n    # Unpack tensors from the batch\n    cost_a = batch['cost_a']\n    cost_b = batch['cost_b']\n    log_prob_w = batch['log_prob_w']\n    log_prob_l = batch['log_prob_l']\n    weights = batch.get('weight', None)\n\n    # 1. Calculate the difference in log probabilities\n    logp_diff = log_prob_w - log_prob_l\n\n    # 2. (Inherited from Parent 1) Soft-clip logp_diff for stability\n    clipped_logp_diff = torch.tanh(logp_diff)\n\n    # 3. Calculate absolute cost difference\n    cost_diff = torch.abs(cost_a - cost_b)\n\n    # 4. (Inherited from Parents) Use rank_gap for a stable, scale-invariant cost signal\n    rank_gap_signal = rank_gap(cost_diff)\n\n    # 5. (Inherited from Parent 0) Create a multiplicative scaler from the rank signal\n    # The scaler increases with the rank of the cost difference, demanding a stronger preference.\n    # Clamping at 0 ensures the scaler is non-negative.\n    scaler = torch.tanh(beta * rank_gap_signal).clamp(min=0)\n\n    # 6. (New Coupling) Fuse the scaler and the clipped log-probability difference\n    # The loss pushes this product to be positive and large.\n    # The effective target for clipped_logp_diff becomes dependent on the scaler.\n    argument = scaler * clipped_logp_diff\n\n    # 7. Compute the final loss using numerically stable logsigmoid\n    loss_per_item = -F.logsigmoid(argument)\n\n    # 8. Apply optional weights and compute the mean loss\n    if weights is not None:\n        loss = (loss_per_item * weights).sum() / weights.sum().clamp(min=1e-8)\n    else:\n        loss = loss_per_item.mean()\n\n    return loss"}, "static_ok": true, "static_reason": "", "dynamic_ok": true, "dynamic_reason": "ok", "loss_value": 0.6931471824645996, "grad_norm": 0.0}
{"generation": 2, "index": 4, "ir": {"name": "Rank-Biased Clipped Margin Loss", "intuition": "This loss function creates a stable and adaptive preference learning signal by combining a soft-clipped log-probability difference with a margin that is sensitive to the relative rank of cost differences within a batch.\n\nInherited Ideas:\n- From Parent 1 (`Soft-Clipped Adaptive Margin Loss`), we inherit the idea of **soft-clipping the log-probability difference** using `tanh(logp_diff)`. This bounds the input to the loss function, preventing extremely confident (and potentially incorrect) predictions from generating massive, destabilizing gradients. It focuses the model on getting the preference direction right rather than maximizing the log-probability gap.\n- From both parents, we inherit the use of **rank-gap normalization** for the cost difference (`cost_diff`). This non-parametric approach is robust to the scale and distribution of costs, unlike standard normalization which is sensitive to outliers. The rank gap provides a stable, ordinal measure of how significant the cost difference is within the batch.\n- From both parents, we also inherit the fundamental structure of a **sigmoid-based loss with an additive margin**: `loss = -logsigmoid(input - margin)`.\n\nNew Coupling Ideas & Modifications:\n1. **Dual-Component Rank-Based Margin:** We introduce a new way to construct the margin. It has two parts: a fixed base margin (`beta`) and an adaptive component derived from the rank-gapped cost difference. Crucially, the adaptive part is scaled by `(1 - tanh(logp_diff))`. This couples the margin's sensitivity to the model's own confidence. When the model is very confident (i.e., `tanh(logp_diff)` is close to 1), the adaptive part of the margin shrinks, effectively telling the model \"you've learned this pair well enough, focus elsewhere\". This prevents the model from wasting capacity on already-learned pairs.\n2. **Margin Smoothing with Softplus:** The entire constructed margin is passed through a `softplus` function. This ensures the final margin is always non-negative and smooth, contributing to numerical stability and well-behaved gradients, an idea present in both parents but applied here to our novel margin formulation.", "pseudocode": "1. Calculate the difference in log probabilities: `logp_diff = logp_winner - logp_loser`.\n2. (Inherited from Parent 1) Soft-clip the log-probability difference using the `tanh` function for stability: `clipped_logp_diff = tanh(logp_diff)`.\n3. Calculate the absolute difference in costs: `cost_diff = abs(cost_winner - cost_loser)`.\n4. (Inherited from both) Normalize the cost difference using the `rank_gap` operator to get a stable, ordinal signal `rank_gap_signal` between -0.5 and 0.5.\n5. (New Coupling) Create a confidence-aware adaptive margin component. Scale the rank-gap signal by a hyperparameter `alpha` and by the model's uncertainty `(1 - clipped_logp_diff)`. This makes the margin larger for uncertain, high-cost-difference pairs: `adaptive_part = alpha * rank_gap_signal * (1 - clipped_logp_diff)`.\n6. (New Coupling & Inherited) Construct the full margin by adding a base margin `beta` to the adaptive part, and then applying `softplus` to ensure it is smooth and non-negative: `margin = softplus(beta + adaptive_part)`.\n7. Combine the components into the final loss argument. We want the clipped logp difference to exceed the margin: `argument = clipped_logp_diff - margin`.\n8. Compute the final loss using a numerically stable logistic loss: `loss = -logsigmoid(argument)`.\n9. Return the weighted mean of the loss over the batch.", "hyperparams": {"alpha": 2.0, "beta": 0.1}, "operators_used": ["logsigmoid", "softplus", "tanh", "rank_gap"], "implementation_hint": {"expects": ["cost_a", "cost_b", "log_prob_w", "log_prob_l", "weight"], "returns": "scalar"}, "code": "import torch\nimport torch.nn.functional as F\n\ndef generated_loss(batch, model_output, extra):\n    \"\"\"\n    Computes the Rank-Biased Clipped Margin Loss.\n\n    This loss combines a tanh-clipped log-probability difference with a novel margin\n    that is adaptive to both the rank of the cost difference and the model's confidence.\n\n    Args:\n        batch (dict): A dictionary containing tensors.\n            'cost_a' (Tensor [N]): Cost of solution 'a'.\n            'cost_b' (Tensor [N]): Cost of solution 'b'.\n            'log_prob_w' (Tensor [N]): Log probability of the winning solution.\n            'log_prob_l' (Tensor [N]): Log probability of the losing solution.\n            'weight' (Tensor [N], optional): Per-pair loss weights.\n        model_output (dict): Model outputs (not used).\n        extra (dict): A dictionary for hyperparameters.\n            'alpha' (float): Controls the sensitivity of the margin to cost rank and model confidence.\n            'beta' (float): A fixed base margin.\n\n    Returns:\n        torch.Tensor: A scalar tensor representing the mean loss.\n    \"\"\"\n\n    # Whitelisted operators implemented as helper functions\n    def rank_gap(x, eps=1e-9):\n        \"\"\"Converts a tensor to its normalized ranks within the range [-0.5, 0.5].\"\"\"\n        if x.numel() <= 1:\n            return torch.zeros_like(x)\n        ranks = x.to(torch.float32).argsort().argsort().to(x.dtype)\n        # Normalize ranks to [0, 1]\n        normalized_ranks = ranks / (x.numel() - 1 + eps)\n        # Shift to [-0.5, 0.5]\n        return normalized_ranks - 0.5\n\n    # Retrieve hyperparameters\n    alpha = extra.get('alpha', 2.0)\n    beta = extra.get('beta', 0.1)\n\n    # Unpack tensors from the batch\n    cost_a = batch['cost_a']\n    cost_b = batch['cost_b']\n    log_prob_w = batch['log_prob_w']\n    log_prob_l = batch['log_prob_l']\n    weights = batch.get('weight', None)\n\n    # 1. Calculate the difference in log probabilities\n    logp_diff = log_prob_w - log_prob_l\n\n    # 2. (Inherited from Parent 1) Soft-clip logp_diff for stability\n    clipped_logp_diff = torch.tanh(logp_diff)\n\n    # 3. Calculate absolute cost difference\n    cost_diff = torch.abs(cost_a - cost_b)\n\n    # 4. (Inherited from both) Use rank_gap for stable, scale-invariant cost signal\n    rank_gap_signal = rank_gap(cost_diff)\n\n    # 5. (New Coupling) Create a confidence-aware adaptive margin component\n    # The margin's sensitivity to cost_rank is reduced as the model becomes more confident (clipped_logp_diff -> 1).\n    # This helps the model focus on harder examples.\n    model_uncertainty_factor = (1.0 - clipped_logp_diff).detach() # Detach to prevent double backprop path\n    adaptive_part = alpha * rank_gap_signal * model_uncertainty_factor\n\n    # 6. (New Coupling & Inherited) Construct the full margin and smooth with softplus\n    # Combines a base margin (beta) with the new confidence-aware adaptive part.\n    # softplus ensures the final margin is non-negative and smooth.\n    margin = F.softplus(beta + adaptive_part)\n\n    # 7. Combine components into the final loss argument\n    # The goal is for the clipped logp difference to be greater than the margin.\n    argument = clipped_logp_diff - margin\n\n    # 8. Compute the final loss using numerically stable logsigmoid\n    loss_per_item = -F.logsigmoid(argument)\n\n    # 9. Apply optional weights and compute the mean loss\n    if weights is not None:\n        loss = (loss_per_item * weights).sum() / weights.sum().clamp(min=1e-8)\n    else:\n        loss = loss_per_item.mean()\n\n    return loss"}, "static_ok": true, "static_reason": "", "dynamic_ok": true, "dynamic_reason": "ok", "loss_value": 1.175480604171753, "grad_norm": 0.0}
{"generation": 2, "index": 5, "ir": {"name": "Soft-Clipped Rank-Modulated Loss", "intuition": "This loss function creates a highly adaptive learning signal by dynamically modulating both the preference target (margin) and the penalty for mis-ranking (scaler), while ensuring numerical stability through soft-clipping.\n\nInherited Ideas:\n- From Parent 0 (`Rank-Gapped Adaptive Sigmoid Loss`), we inherit the concept of a **multiplicative scaler** that amplifies the gradient for pairs with a large cost difference. The loss structure `scaler * logp_diff` is a key inherited element.\n- From Parent 1 (`Soft-Clipped Adaptive Margin Loss`), we adopt the use of `tanh` to **soft-clip the log-probability difference** (`logp_diff`). This is a crucial stability trick that prevents extreme gradients from destabilizing training by bounding the effective log-probability difference.\n\nNew Coupling Ideas:\n1. **Unified Rank-Gap Signal:** Both parents use `rank_gap` on the cost difference to create a stable, ordinal signal. We unify this by creating a single `rank_gap_signal` that serves as the input for both the margin and the scaler. This ensures both components are driven by the same robust, non-parametric measure of preference importance.\n2. **Dynamic Margin via Clamped Softplus:** We introduce a new way to form the additive margin. We apply `softplus` to the rank-gap signal, but then clamp its maximum value using `max_margin`. This creates a responsive margin for most pairs but prevents it from growing uncontrollably for extreme outliers in the rank-gap distribution, further enhancing stability.", "pseudocode": "1. Calculate the difference in log probabilities: `logp_diff = logp_winner - logp_loser`.\n2. (Inherited from Parent 1) Apply a soft-clipping function to the log-probability difference for stability: `clipped_logp_diff = tanh(logp_diff)`.\n3. Calculate the absolute difference in costs: `cost_diff = abs(cost_winner - cost_loser)`.\n4. (New Coupling) Create a unified, robust signal by applying the `rank_gap` operator to the cost differences. This produces a `rank_gap_signal` that is invariant to the scale of costs.\n5. (New Coupling) Create a dynamic additive margin. Apply a `softplus` function to the `rank_gap_signal` (scaled by `alpha`), then clamp the result to a maximum value `max_margin`. This creates a smooth, bounded margin: `margin = clamp(softplus(alpha * rank_gap_signal), max=max_margin)`.\n6. (Inherited from Parent 0) Create a multiplicative scaler. Apply a `softplus` function to the `rank_gap_signal` (scaled by `beta`) and add 1. This ensures the scaler is always >= 1, increasing the penalty for mis-ranking pairs with a higher rank-gap: `scaler = 1.0 + softplus(beta * rank_gap_signal)`.\n7. Combine the components into the final loss argument: `argument = scaler * clipped_logp_diff - margin`. The goal is to make the scaled, clipped log-probability difference greater than the dynamic margin.\n8. Compute the final loss using a numerically stable logistic loss: `loss = -logsigmoid(argument)`.\n9. Return the weighted mean of the loss over the batch.", "hyperparams": {"alpha": 3.0, "beta": 1.5, "max_margin": 1.0}, "operators_used": ["logsigmoid", "softplus", "tanh", "clamp", "rank_gap"], "implementation_hint": {"expects": ["cost_a", "cost_b", "logp_w", "logp_l", "weight"], "returns": "scalar"}, "code": "import torch\nimport torch.nn.functional as F\n\ndef generated_loss(batch, model_output, extra):\n    \"\"\"\n    Computes the Soft-Clipped Rank-Modulated Loss.\n\n    This loss combines a soft-clipped log-probability difference with a multiplicative scaler\n    and a clamped additive margin, both derived from a unified rank-gap signal.\n\n    Args:\n        batch (dict): A dictionary containing tensors.\n            'cost_a' (Tensor [N]): Cost of solution 'a'.\n            'cost_b' (Tensor [N]): Cost of solution 'b'.\n            'log_prob_w' (Tensor [N]): Log probability of the winning solution.\n            'log_prob_l' (Tensor [N]): Log probability of the losing solution.\n            'weight' (Tensor [N], optional): Per-pair loss weights.\n        model_output (dict): Model outputs (not used).\n        extra (dict): A dictionary for hyperparameters.\n            'alpha' (float): Controls the magnitude of the additive margin.\n            'beta' (float): Controls the magnitude of the multiplicative scaler.\n            'max_margin' (float): The maximum value for the additive margin.\n\n    Returns:\n        torch.Tensor: A scalar tensor representing the mean loss.\n    \"\"\"\n\n    # Whitelisted operators implemented as helper functions\n    def rank_gap(x, eps=1e-9):\n        \"\"\"Converts a tensor to its normalized ranks within the range [-0.5, 0.5].\"\"\"\n        if x.numel() <= 1:\n            return torch.zeros_like(x)\n        ranks = x.to(torch.float32).argsort().argsort().to(x.dtype)\n        # Normalize ranks to [0, 1]\n        normalized_ranks = ranks / (x.numel() - 1 + eps)\n        # Shift to [-0.5, 0.5]\n        return normalized_ranks - 0.5\n\n    # Retrieve hyperparameters\n    alpha = extra.get('alpha', 3.0)\n    beta = extra.get('beta', 1.5)\n    max_margin = extra.get('max_margin', 1.0)\n\n    # Unpack tensors from the batch\n    cost_a = batch['cost_a']\n    cost_b = batch['cost_b']\n    log_prob_w = batch['log_prob_w']\n    log_prob_l = batch['log_prob_l']\n    weights = batch.get('weight', None)\n\n    # 1. Calculate the difference in log probabilities\n    logp_diff = log_prob_w - log_prob_l\n\n    # 2. (Inherited from Parent 1) Soft-clip logp_diff for stability\n    clipped_logp_diff = torch.tanh(logp_diff)\n\n    # 3. Calculate absolute cost difference\n    cost_diff = torch.abs(cost_a - cost_b)\n\n    # 4. (New Coupling) Create a unified rank-gap signal for robustness\n    rank_gap_signal = rank_gap(cost_diff)\n\n    # 5. (New Coupling) Create a dynamic, clamped additive margin\n    # softplus ensures a smooth, non-negative value. clamp prevents it from growing too large.\n    margin = torch.clamp(F.softplus(alpha * rank_gap_signal), max=max_margin)\n\n    # 6. (Inherited from Parent 0) Create a multiplicative scaler\n    # The scaler is >= 1 and increases the penalty for pairs with a higher cost rank.\n    scaler = 1.0 + F.softplus(beta * rank_gap_signal)\n\n    # 7. Combine components into the final loss argument\n    # We want scaler * clipped_logp_diff > margin.\n    argument = scaler * clipped_logp_diff - margin\n\n    # 8. Compute the final loss using numerically stable logsigmoid\n    loss_per_item = -F.logsigmoid(argument)\n\n    # 9. Apply optional weights and compute the mean loss\n    if weights is not None:\n        loss = (loss_per_item * weights).sum() / weights.sum().clamp(min=1e-8)\n    else:\n        loss = loss_per_item.mean()\n\n    return loss"}, "static_ok": true, "static_reason": "", "dynamic_ok": true, "dynamic_reason": "ok", "loss_value": 1.093460202217102, "grad_norm": 0.0}
{"generation": 2, "index": 6, "ir": {"name": "Soft-Clipped Scaled Margin Loss", "intuition": "This loss function creates a dynamic and stable preference learning signal by blending a soft-clipped probability difference with a margin that is both additively and multiplicatively scaled by the cost difference.\n\nInherited Ideas:\n- From Parent 0 (`Rank-Gapped Adaptive Sigmoid Loss`): It inherits the idea of multiplicatively scaling the log-probability difference (`logp_diff`) by a cost-dependent factor. This `scaler` amplifies the loss signal for pairs with a large cost discrepancy, encouraging the model to prioritize learning these clear-cut preferences.\n- From Parent 1 (`Soft-Clipped Adaptive Margin Loss`): It inherits the concept of 'soft-clipping' the log-probability difference using a `tanh` activation. This bounds the `logp_diff` to the range [-1, 1], which prevents extremely confident (but potentially wrong) predictions from generating excessively large gradients and destabilizing training.\n\nNew Coupling Ideas & Modifications:\n1. **Dual-Role Cost Signal**: A single, robust cost signal, derived from the `rank_gap` of the cost difference, is used in two distinct roles. This signal simultaneously determines both the multiplicative `scaler` (inherited from Parent 0) and the additive `margin`.\n2. **Scaled Additive Margin**: Instead of a simple additive margin, the margin itself is scaled by the same cost-derived `scaler`. The margin is `scaler * softplus(beta + alpha * rank_gap_signal)`. This coupling ensures that as the cost difference becomes more significant (higher rank), not only does the required log-probability difference increase (additive margin), but the penalty for failing to meet that margin is also amplified (multiplicative scaling). This creates a highly responsive loss where the target gap and the penalty for missing it grow in unison.", "pseudocode": "1. Calculate the difference in log probabilities: `logp_diff = logp_winner - logp_loser`.\n2. **(Inherited from Parent 1)** Soft-clip the `logp_diff` using a `tanh` function for stability: `clipped_logp_diff = tanh(logp_diff)`.\n3. Calculate the absolute difference in costs: `cost_diff = abs(cost_winner - cost_loser)`.\n4. **(New Coupling)** Create a unified, robust cost signal by applying the `rank_gap` operator to `cost_diff`. This produces `rank_gap_signal`, a value between -0.5 and 0.5 that is insensitive to the scale of costs.\n5. **(Inherited from Parent 0)** Create a multiplicative scaler from the `rank_gap_signal`. We clamp it to be non-negative: `scaler = clamp(rank_gap_signal + 0.5, min=0)`. This ranges from 0 to 1.\n6. **(New Coupling)** Create a scaled additive margin. First, compute a base margin using `softplus` on a combination of a fixed term `beta` and the cost-sensitive `rank_gap_signal` scaled by `alpha`. Then, multiply this base margin by the `scaler` from the previous step: `margin = scaler * softplus(beta + alpha * rank_gap_signal)`.\n7. Combine the components into the final loss argument: `argument = clipped_logp_diff - margin`. The goal is for the clipped log-probability difference to exceed the scaled margin.\n8. Compute the final loss using a numerically stable logistic loss: `loss = -logsigmoid(argument)`.\n9. Return the weighted mean of the loss over the batch.", "hyperparams": {"alpha": 1.5, "beta": 0.1}, "operators_used": ["logsigmoid", "softplus", "tanh", "clamp", "rank_gap"], "implementation_hint": {"expects": ["cost_a", "cost_b", "log_prob_w", "log_prob_l"], "returns": "scalar"}, "code": "import torch\nimport torch.nn.functional as F\n\ndef generated_loss(batch, model_output, extra):\n    \"\"\"\n    Computes the Soft-Clipped Scaled Margin Loss.\n\n    This loss combines a tanh-clipped log-probability difference with a margin that is\n    both additively and multiplicatively dependent on the rank-normalized cost difference.\n\n    Args:\n        batch (dict): A dictionary containing tensors.\n            'cost_a' (Tensor [N]): Cost of solution 'a'.\n            'cost_b' (Tensor [N]): Cost of solution 'b'.\n            'log_prob_w' (Tensor [N]): Log probability of the winning solution.\n            'log_prob_l' (Tensor [N]): Log probability of the losing solution.\n            'weight' (Tensor [N], optional): Per-pair loss weights.\n        model_output (dict): Model outputs (not used).\n        extra (dict): A dictionary for hyperparameters.\n            'alpha' (float): Controls the sensitivity of the margin to the cost rank.\n            'beta' (float): A fixed base component for the margin.\n\n    Returns:\n        torch.Tensor: A scalar tensor representing the mean loss.\n    \"\"\"\n\n    # Whitelisted operators implemented as helper functions\n    def rank_gap(x, eps=1e-9):\n        \"\"\"Converts a tensor to its normalized ranks within the range [-0.5, 0.5].\"\"\"\n        if x.numel() <= 1:\n            return torch.zeros_like(x)\n        ranks = x.to(torch.float32).argsort().argsort().to(x.dtype)\n        normalized_ranks = ranks / (x.numel() - 1 + eps)\n        return normalized_ranks - 0.5\n\n    # Retrieve hyperparameters\n    alpha = extra.get('alpha', 1.5)\n    beta = extra.get('beta', 0.1)\n\n    # Unpack tensors from the batch\n    cost_a = batch['cost_a']\n    cost_b = batch['cost_b']\n    log_prob_w = batch['log_prob_w']\n    log_prob_l = batch['log_prob_l']\n    weights = batch.get('weight', None)\n\n    # 1. Calculate the difference in log probabilities\n    logp_diff = log_prob_w - log_prob_l\n\n    # 2. (Inherited from Parent 1) Soft-clip logp_diff for stability\n    clipped_logp_diff = torch.tanh(logp_diff)\n\n    # 3. Calculate absolute cost difference\n    cost_diff = torch.abs(cost_a - cost_b)\n\n    # 4. (New Coupling) Create a unified, robust cost signal via rank_gap\n    rank_gap_signal = rank_gap(cost_diff)\n\n    # 5. (Inherited from Parent 0) Create a multiplicative scaler from the cost signal\n    # The scaler is the rank-gap signal shifted to be in [0, 1]\n    scaler = torch.clamp(rank_gap_signal + 0.5, min=0.0)\n\n    # 6. (New Coupling) Create a scaled additive margin\n    # The margin's base is adaptive, and the whole margin is scaled by the cost rank.\n    base_margin = F.softplus(beta + alpha * rank_gap_signal)\n    margin = scaler * base_margin\n\n    # 7. Combine components into the final loss argument\n    # The structure is loss = -logsigmoid(clipped_logp_diff - margin)\n    argument = clipped_logp_diff - margin\n\n    # 8. Compute the final loss using numerically stable logsigmoid\n    loss_per_item = -F.logsigmoid(argument)\n\n    # 9. Apply optional weights and compute the mean loss\n    if weights is not None:\n        loss = (loss_per_item * weights).sum() / weights.sum().clamp(min=1e-8)\n    else:\n        loss = loss_per_item.mean()\n\n    return loss"}, "static_ok": true, "static_reason": "", "dynamic_ok": true, "dynamic_reason": "ok", "loss_value": 0.965088427066803, "grad_norm": 0.0}
{"generation": 2, "index": 7, "ir": {"name": "Soft-Clipped Rank-Modulated Loss", "intuition": "This loss function creates a dynamic learning signal by blending a stable, soft-clipped log-probability difference with a margin that is modulated by both a multiplicative scaler and an additive component. The design prioritizes numerical stability and robustness to cost scales.\n\nInherited Ideas:\n- From Parent 1 (`Soft-Clipped Adaptive Margin Loss`): It inherits the core idea of **soft-clipping the log-probability difference** using `tanh`. This bounds the `logp_diff` to the `[-1, 1]` range, preventing extreme gradients from destabilizing training and focusing the model on getting the preference direction right rather than maximizing the log-probability gap.\n- From Parent 0 (`Rank-Gapped Adaptive Sigmoid Loss`): It inherits the structure of using a **multiplicative scaler** applied to the log-probability difference. This scaler, derived from the cost difference, amplifies the learning signal for pairs with a larger quality gap.\n\nNew Coupling Ideas & Modifications:\n1.  **Unified Rank-Based Modulation**: Instead of having separate margin and scaler terms, this loss introduces a unified `rank_modulator`. This modulator is derived from a `rank_gap` normalized cost difference, ensuring it is robust to cost outliers and scale. It's calculated as `softplus(alpha * rank_gap_signal)`, creating a smooth, non-negative, and monotonically increasing signal based on the relative importance of the preference pair within the batch.\n2.  **Dual-Role Modulator (Scaler & Margin)**: The `rank_modulator` is coupled to the loss in two ways simultaneously. It acts as a multiplicative `scaler` on the `tanh(logp_diff)` and also as an additive `margin`. The final loss argument becomes `scaler * tanh(logp_diff) - margin`, which simplifies to `rank_modulator * (tanh(logp_diff) - 1)`. This elegantly couples the scaling and margin effects, demanding that for high-rank pairs (large modulator), the `tanh(logp_diff)` must be very close to its maximum value of 1 to achieve low loss.", "pseudocode": "1. Calculate the difference in log probabilities: `logp_diff = logp_winner - logp_loser`.\n2. (Inherited from Parent 1) Soft-clip the log-probability difference using `tanh` for stability: `clipped_logp_diff = tanh(logp_diff)`.\n3. Calculate the absolute difference in costs: `cost_diff = abs(cost_winner - cost_loser)`.\n4. (New Coupling) Convert the cost differences into a normalized rank signal between -0.5 and 0.5 using the `rank_gap` operator. This makes the signal robust to cost scale and outliers.\n5. (New Coupling) Create a single, unified `rank_modulator` by applying `softplus` to the scaled rank signal: `rank_modulator = softplus(alpha * rank_gap_signal)`. This results in a smooth, non-negative value that increases with the rank of the cost difference.\n6. (Inherited from Parent 0 & New Coupling) Combine the components. Use the `rank_modulator` as both a multiplicative scaler and an additive margin. The target is `scaler * clipped_logp_diff > margin`, which becomes `rank_modulator * clipped_logp_diff > rank_modulator`. This simplifies to `rank_modulator * (clipped_logp_diff - 1)`.\n7. Compute the final loss using a numerically stable logistic loss: `loss = -logsigmoid(rank_modulator * (clipped_logp_diff - 1))`.\n8. Return the weighted mean of the loss over the batch.", "hyperparams": {"alpha": 4.0}, "operators_used": ["logsigmoid", "softplus", "tanh", "rank_gap"], "implementation_hint": {"expects": ["cost_a", "cost_b", "log_prob_w", "log_prob_l", "weight"], "returns": "scalar"}, "code": "import torch\nimport torch.nn.functional as F\n\ndef generated_loss(batch, model_output, extra):\n    \"\"\"\n    Computes the Soft-Clipped Rank-Modulated Loss.\n\n    This loss uses a unified, rank-based modulator that acts as both a multiplicative\n    scaler and an additive margin on a tanh-clipped log-probability difference.\n\n    Args:\n        batch (dict): A dictionary containing tensors.\n            'cost_a' (Tensor [N]): Cost of solution 'a'.\n            'cost_b' (Tensor [N]): Cost of solution 'b'.\n            'log_prob_w' (Tensor [N]): Log probability of the winning solution.\n            'log_prob_l' (Tensor [N]): Log probability of the losing solution.\n            'weight' (Tensor [N], optional): Per-pair loss weights.\n        model_output (dict): Model outputs (not used).\n        extra (dict): A dictionary for hyperparameters.\n            'alpha' (float): Controls the magnitude of the rank modulator.\n\n    Returns:\n        torch.Tensor: A scalar tensor representing the mean loss.\n    \"\"\"\n\n    # Whitelisted operator implemented as a helper function\n    def rank_gap(x, eps=1e-9):\n        \"\"\"Converts a tensor to its normalized ranks within the range [-0.5, 0.5].\"\"\"\n        if x.numel() <= 1:\n            return torch.zeros_like(x)\n        ranks = x.to(torch.float32).argsort().argsort().to(x.dtype)\n        # Normalize ranks to [0, 1]\n        normalized_ranks = ranks / (x.numel() - 1 + eps)\n        # Shift to [-0.5, 0.5]\n        return normalized_ranks - 0.5\n\n    # Retrieve hyperparameters\n    alpha = extra.get('alpha', 4.0)\n\n    # Unpack tensors from the batch\n    cost_a = batch['cost_a']\n    cost_b = batch['cost_b']\n    log_prob_w = batch['log_prob_w']\n    log_prob_l = batch['log_prob_l']\n    weights = batch.get('weight', None)\n\n    # 1. Calculate the difference in log probabilities\n    logp_diff = log_prob_w - log_prob_l\n\n    # 2. (Inherited from Parent 1) Soft-clip for stability\n    clipped_logp_diff = torch.tanh(logp_diff)\n\n    # 3. Calculate absolute cost difference\n    cost_diff = torch.abs(cost_a - cost_b)\n\n    # 4. (New Coupling) Use rank_gap for a stable, scale-invariant cost signal\n    rank_gap_signal = rank_gap(cost_diff)\n\n    # 5. (New Coupling) Create a unified rank modulator\n    # softplus ensures the modulator is non-negative and smooth.\n    rank_modulator = F.softplus(alpha * rank_gap_signal)\n\n    # 6. (Inherited from Parent 0 & New Coupling) Combine components\n    # The modulator serves as both a scaler and a margin, i.e.,\n    # we want modulator * clipped_logp_diff > modulator.\n    # This is equivalent to modulator * (clipped_logp_diff - 1).\n    argument = rank_modulator * (clipped_logp_diff - 1)\n\n    # 7. Compute the final loss using numerically stable logsigmoid\n    loss_per_item = -F.logsigmoid(argument)\n\n    # 8. Apply optional weights and compute the mean loss\n    if weights is not None:\n        loss = (loss_per_item * weights).sum() / weights.sum().clamp(min=1e-8)\n    else:\n        loss = loss_per_item.mean()\n\n    return loss"}, "static_ok": true, "static_reason": "", "dynamic_ok": true, "dynamic_reason": "ok", "loss_value": 1.2550013065338135, "grad_norm": 0.0}
{"generation": 3, "index": 0, "ir": {"name": "Soft-Margin Rank-Gapped Loss", "intuition": "This loss function creates an adaptive learning signal by combining a stable, soft-clipped log-probability difference with a dual-purpose margin derived from cost ranks. The design prioritizes numerical stability and robustness to cost scales while introducing a novel way to handle the margin.\n\nInherited Ideas:\n- From `Soft-Clipped Rank-Modulated Loss` (Parent 0), it inherits the core idea of **soft-clipping the log-probability difference** using `tanh`. This bounds the log-probability difference to `[-1, 1]`, preventing extreme gradients and focusing the model on establishing the correct preference direction.\n- From `Rank-Gapped Adaptive Sigmoid Loss` (Parent 1), it inherits the concept of using an **additive, cost-dependent margin**. The loss aims to ensure the log-probability difference surpasses this margin, which is derived from the relative quality gap between pairs in a batch.\n\nNew Coupling Ideas & Modifications:\n1.  **Unified Rank-Based Margin Source**: Both parents use a `rank_gap` signal derived from cost differences for stability. This child loss continues this practice, using `rank_gap(abs(cost_winner - cost_loser))` as the sole source of cost information. This ensures the loss is invariant to the absolute scale of costs and robust to outliers.\n2.  **Softplus-Coupled Margin**: The key innovation is how the margin is constructed and used. The `rank_gap` signal is passed through a `softplus` function to create a smooth, non-negative `base_margin`. This `base_margin` is then coupled with the `tanh(logp_diff)` inside another `softplus` function: `softplus(tanh(logp_diff) - base_margin)`. This structure creates a 'soft margin': when `tanh(logp_diff)` is much larger than the `base_margin`, the loss approaches zero. When it is smaller, the loss increases smoothly, providing a gentler penalty compared to a hard margin inside a `logsigmoid` function. The final loss is simply the negative log of this 'soft margin' term, encouraging `tanh(logp_diff)` to exceed the `base_margin`.", "pseudocode": "1. Calculate the difference in log probabilities: `logp_diff = logp_winner - logp_loser`.\n2. (Inherited from Parent 0) Soft-clip the log-probability difference using `tanh` for stability: `clipped_logp_diff = tanh(logp_diff)`.\n3. Calculate the absolute difference in costs: `cost_diff = abs(cost_winner - cost_loser)`.\n4. (New Coupling) Convert the cost differences into a normalized rank signal between -0.5 and 0.5 using the `rank_gap` operator. This makes the signal robust to cost scale and outliers.\n5. (Inherited from Parent 1) Create a smooth, non-negative `base_margin` by applying `softplus` to the scaled rank signal: `base_margin = softplus(alpha * rank_gap_signal)`. This margin increases with the rank of the cost difference.\n6. (New Coupling) Couple the clipped log-probability difference and the base margin using a `softplus` function to create a 'soft margin' effect: `soft_margin_term = softplus(clipped_logp_diff - base_margin)`.\n7. Compute the final loss by taking the negative logarithm of the soft margin term: `loss = -log(soft_margin_term)`. This penalizes cases where `clipped_logp_diff` is not sufficiently larger than `base_margin`.\n8. Return the weighted mean of the loss over the batch.", "hyperparams": {"alpha": 2.0}, "operators_used": ["log", "softplus", "tanh", "rank_gap"], "implementation_hint": {"expects": ["cost_a", "cost_b", "log_prob_w", "log_prob_l", "weight"], "returns": "scalar"}, "code": "import torch\nimport torch.nn.functional as F\n\ndef generated_loss(batch, model_output, extra):\n    \"\"\"\n    Computes the Soft-Margin Rank-Gapped Loss.\n\n    This loss uses a softplus-coupled margin on a tanh-clipped log-probability\n    difference, with the margin's magnitude determined by the rank of the cost gap.\n\n    Args:\n        batch (dict): A dictionary containing tensors.\n            'cost_a' (Tensor [N]): Cost of solution 'a'.\n            'cost_b' (Tensor [N]): Cost of solution 'b'.\n            'log_prob_w' (Tensor [N]): Log probability of the winning solution.\n            'log_prob_l' (Tensor [N]): Log probability of the losing solution.\n            'weight' (Tensor [N], optional): Per-pair loss weights.\n        model_output (dict): Model outputs (not used).\n        extra (dict): A dictionary for hyperparameters.\n            'alpha' (float): Controls the magnitude of the rank-based margin.\n\n    Returns:\n        torch.Tensor: A scalar tensor representing the mean loss.\n    \"\"\"\n\n    # Whitelisted operator implemented as a helper function\n    def rank_gap(x, eps=1e-9):\n        \"\"\"Converts a tensor to its normalized ranks within the range [-0.5, 0.5].\"\"\"\n        if x.numel() <= 1:\n            return torch.zeros_like(x)\n        ranks = x.to(torch.float32).argsort().argsort().to(x.dtype)\n        # Normalize ranks to [0, 1]\n        normalized_ranks = ranks / (x.numel() - 1 + eps)\n        # Shift to [-0.5, 0.5]\n        return normalized_ranks - 0.5\n\n    # Retrieve hyperparameters\n    alpha = extra.get('alpha', 2.0)\n    log_eps = extra.get('log_eps', 1e-9)\n\n    # Unpack tensors from the batch\n    cost_a = batch['cost_a']\n    cost_b = batch['cost_b']\n    log_prob_w = batch['log_prob_w']\n    log_prob_l = batch['log_prob_l']\n    weights = batch.get('weight', None)\n\n    # 1. Calculate the difference in log probabilities\n    logp_diff = log_prob_w - log_prob_l\n\n    # 2. (Inherited from Parent 0) Soft-clip for stability\n    clipped_logp_diff = torch.tanh(logp_diff)\n\n    # 3. Calculate absolute cost difference\n    cost_diff = torch.abs(cost_a - cost_b)\n\n    # 4. (New Coupling) Use rank_gap for a stable, scale-invariant cost signal\n    rank_gap_signal = rank_gap(cost_diff)\n\n    # 5. (Inherited from Parent 1) Create a base margin from the rank signal\n    # softplus ensures the margin is non-negative and smooth.\n    base_margin = F.softplus(alpha * rank_gap_signal)\n\n    # 6. (New Coupling) Couple the clipped log-prob diff and margin inside softplus\n    # This creates a 'soft margin' that smoothly increases the loss as the\n    # clipped_logp_diff falls below the required base_margin.\n    soft_margin_term = F.softplus(base_margin - clipped_logp_diff)\n\n    # 7. Compute the final loss using log for stability\n    # This is equivalent to -log(sigmoid(clipped_logp_diff - base_margin)) but uses softplus.\n    # The goal is to make soft_margin_term as small as possible (close to 0).\n    loss_per_item = soft_margin_term\n\n    # 8. Apply optional weights and compute the mean loss\n    if weights is not None:\n        loss = (loss_per_item * weights).sum() / weights.sum().clamp(min=1e-8)\n    else:\n        loss = loss_per_item.mean()\n\n    return loss"}, "static_ok": true, "static_reason": "", "dynamic_ok": true, "dynamic_reason": "ok", "loss_value": 1.1398158073425293, "grad_norm": 0.0}
{"generation": 3, "index": 1, "ir": {"name": "Adaptive Rank-Gated Loss", "intuition": "This loss function creates a highly adaptive learning signal by gating the influence of the log-probability difference based on the relative importance of the preference pair. It is designed for stability and to focus learning on meaningful preference distinctions.\n\nInherited Ideas:\n- From Parent 0 (`Soft-Clipped Rank-Modulated Loss`), it inherits the idea of **soft-clipping the log-probability difference** using `tanh`. This bounds the `logp_diff` to `[-1, 1]`, preventing extreme gradients and ensuring numerical stability.\n- From Parent 1 (`Rank-Gapped Adaptive Sigmoid Loss`), it inherits the structure of combining a **multiplicative scaler and an additive margin**, both derived from the cost difference, to shape the loss landscape. The core structure `scaler * logp_diff - margin` is maintained.\n\nNew Coupling Ideas & Modifications:\n1. **Rank-Gapped Sigmoid Gate:** A new coupling is introduced by creating a `gate` signal using `sigmoid(beta * rank_gap_signal)`. This gate, derived from the rank-normalized cost difference, smoothly transitions from 0.5 (for low-rank pairs) to 1.0 (for high-rank pairs). This gate is used as the multiplicative scaler, effectively 'gating' the influence of the `logp_diff`. It ensures that pairs with a small cost difference have their `logp_diff` scaled down, reducing their impact on the gradient, while pairs with a large cost difference have their `logp_diff` fully considered.\n2. **Gated Additive Margin:** The same `gate` signal is coupled to the margin calculation. The margin is defined as `gate * softplus(alpha * rank_gap_signal)`. This means the required log-probability margin is also gated by the importance of the pair. For low-rank pairs where the gate is ~0.5, the margin is smaller, setting a less demanding target. For high-rank pairs, the gate approaches 1, allowing the `softplus` term to define a larger, more challenging margin. This dynamic coupling ensures the model is not unduly penalized for failing to meet a large margin on unimportant pairs.", "pseudocode": "1. Calculate the difference in log probabilities: `logp_diff = logp_winner - logp_loser`.\n2. (Inherited from Parent 0) Soft-clip the log-probability difference using `tanh` for stability: `clipped_logp_diff = tanh(logp_diff)`.\n3. Calculate the absolute difference in costs: `cost_diff = abs(cost_winner - cost_loser)`.\n4. (Inherited from Parents) Convert cost differences into a normalized rank signal between -0.5 and 0.5 using the `rank_gap` operator. This makes the signal robust to cost scale and outliers.\n5. (New Coupling) Create a smooth `gate` signal using the `sigmoid` function on the scaled rank signal: `gate = sigmoid(beta * rank_gap_signal)`. This gate ranges from approximately 0.5 to 1.0.\n6. (Inherited from Parent 1 & New Coupling) Use this `gate` as the multiplicative scaler on the clipped log-probability difference.\n7. (Inherited from Parent 1 & New Coupling) Create a gated additive margin. The margin is the product of the `gate` and a `softplus` transformation of the rank signal: `margin = gate * softplus(alpha * rank_gap_signal)`.\n8. Combine the components into the final loss argument: `argument = gate * clipped_logp_diff - margin`.\n9. Compute the final loss using a numerically stable logistic loss: `loss = -logsigmoid(argument)`.\n10. Return the weighted mean of the loss over the batch.", "hyperparams": {"alpha": 2.0, "beta": 4.0}, "operators_used": ["logsigmoid", "sigmoid", "softplus", "tanh", "rank_gap"], "implementation_hint": {"expects": ["cost_a", "cost_b", "log_prob_w", "log_prob_l", "weight"], "returns": "scalar"}, "code": "import torch\nimport torch.nn.functional as F\n\ndef generated_loss(batch, model_output, extra):\n    \"\"\"\n    Computes the Adaptive Rank-Gated Loss.\n\n    This loss uses a sigmoid gate based on rank-normalized cost difference to adaptively\n    scale both the log-probability difference and the additive margin.\n\n    Args:\n        batch (dict): A dictionary containing tensors.\n            'cost_a' (Tensor [N]): Cost of solution 'a'.\n            'cost_b' (Tensor [N]): Cost of solution 'b'.\n            'log_prob_w' (Tensor [N]): Log probability of the winning solution.\n            'log_prob_l' (Tensor [N]): Log probability of the losing solution.\n            'weight' (Tensor [N], optional): Per-pair loss weights.\n        model_output (dict): Model outputs (not used).\n        extra (dict): A dictionary for hyperparameters.\n            'alpha' (float): Controls the magnitude of the margin component.\n            'beta' (float): Controls the steepness of the sigmoid gate.\n\n    Returns:\n        torch.Tensor: A scalar tensor representing the mean loss.\n    \"\"\"\n\n    # Whitelisted operator implemented as a helper function\n    def rank_gap(x, eps=1e-9):\n        \"\"\"Converts a tensor to its normalized ranks within the range [-0.5, 0.5].\"\"\"\n        if x.numel() <= 1:\n            return torch.zeros_like(x)\n        ranks = x.to(torch.float32).argsort().argsort().to(x.dtype)\n        # Normalize ranks to [0, 1]\n        normalized_ranks = ranks / (x.numel() - 1 + eps)\n        # Shift to [-0.5, 0.5]\n        return normalized_ranks - 0.5\n\n    # Retrieve hyperparameters\n    alpha = extra.get('alpha', 2.0)\n    beta = extra.get('beta', 4.0)\n\n    # Unpack tensors from the batch\n    cost_a = batch['cost_a']\n    cost_b = batch['cost_b']\n    log_prob_w = batch['log_prob_w']\n    log_prob_l = batch['log_prob_l']\n    weights = batch.get('weight', None)\n\n    # 1. Calculate the difference in log probabilities\n    logp_diff = log_prob_w - log_prob_l\n\n    # 2. (Inherited from Parent 0) Soft-clip for stability\n    clipped_logp_diff = torch.tanh(logp_diff)\n\n    # 3. Calculate absolute cost difference\n    cost_diff = torch.abs(cost_a - cost_b)\n\n    # 4. (Inherited from Parents) Use rank_gap for stable, scale-invariant cost signal\n    rank_gap_signal = rank_gap(cost_diff)\n\n    # 5. (New Coupling) Create a sigmoid gate from the rank signal\n    # This smoothly transitions from ~0.5 to 1.0 based on preference importance.\n    gate = torch.sigmoid(beta * rank_gap_signal)\n\n    # 6. (Inherited from Parent 1 & New Coupling) The gate acts as the multiplicative scaler\n    gated_clipped_logp_diff = gate * clipped_logp_diff\n\n    # 7. (Inherited from Parent 1 & New Coupling) Create a gated additive margin\n    # The margin is also scaled by the gate, making it adaptive.\n    gated_margin = gate * F.softplus(alpha * rank_gap_signal)\n\n    # 8. Combine the components. The structure is scaler * logp - margin.\n    argument = gated_clipped_logp_diff - gated_margin\n\n    # 9. Compute the final loss using numerically stable logsigmoid\n    loss_per_item = -F.logsigmoid(argument)\n\n    # 10. Apply optional weights and compute the mean loss\n    if weights is not None:\n        loss = (loss_per_item * weights).sum() / weights.sum().clamp(min=1e-8)\n    else:\n        loss = loss_per_item.mean()\n\n    return loss"}, "static_ok": true, "static_reason": "", "dynamic_ok": true, "dynamic_reason": "ok", "loss_value": 0.9569697380065918, "grad_norm": 0.0}
{"generation": 3, "index": 2, "ir": {"name": "Hybrid Rank-Modulated Loss", "intuition": "This loss function creates a highly adaptive preference learning signal by combining the strengths of its parents and introducing a novel coupling mechanism. The core objective is to ensure the log-probability difference not only surpasses a margin but is also amplified based on the relative importance of the preference pair.\n\nInherited Ideas:\n- From `Soft-Clipped Rank-Modulated Loss` (Parent 0), it inherits the idea of using a **unified modulator derived from a rank-gapped cost signal**. This modulator, created via `softplus(alpha * rank_gap(cost_diff))`, serves as a single, scale-invariant signal of preference importance. This design is robust to cost outliers and focuses on the relative ranking of cost differences within a batch.\n- From `Rank-Gapped Adaptive Sigmoid Loss` (Parent 1), it inherits the structure of having **separate multiplicative and additive components** in the loss formulation (`scaler * logp_diff - margin`). This allows for independent control over the amplification of the log-probability difference and the target margin it must exceed.\n\nNew Coupling Ideas & Modifications:\n1.  **Dual-Purpose Modulator as a Source**: The unified `rank_modulator` from Parent 0 is used as a common source to generate both the `scaler` and the `margin`. This elegantly couples their behavior: a pair with a higher-ranked cost difference will simultaneously have a larger target margin and a stronger gradient signal if ranked incorrectly.\n2.  **Bounded Scaler via Sigmoid**: The `scaler` is derived by applying a `sigmoid` function to the `rank_modulator`. This bounds the scaler to the `[0.5, 1]` range, ensuring it's always positive and prevents it from causing numerical instability by becoming too large or zero. This differs from Parent 0's direct use of the modulator and Parent 1's use of `tanh`.\n3.  **Direct Margin Inheritance**: The `margin` is set directly to be the `rank_modulator` itself (`softplus(alpha * rank_gap_signal)`). This provides a smooth, non-negative margin that grows with the importance of the preference pair, directly linking the required log-probability gap to the rank of the cost difference.", "pseudocode": "1. Calculate the difference in log probabilities: `logp_diff = logp_winner - logp_loser`.\n2. Calculate the absolute difference in costs: `cost_diff = abs(cost_winner - cost_loser)`.\n3. (Inherited from Parent 0) Convert the cost differences into a stable, normalized rank signal between -0.5 and 0.5 using the `rank_gap` operator.\n4. (Inherited from Parent 0) Create a unified, non-negative `rank_modulator` that increases with the importance of the preference pair: `rank_modulator = softplus(alpha * rank_gap_signal)`.\n5. (New Coupling & Inherited from Parent 1) Create the multiplicative `scaler`. Pass the `rank_modulator` through a `sigmoid` function to create a stable, bounded scaler between 0.5 and 1: `scaler = sigmoid(rank_modulator)`.\n6. (New Coupling & Inherited from Parent 1) Create the additive `margin`. Set the margin directly to the `rank_modulator`: `margin = rank_modulator`.\n7. Combine the components into the final loss argument, following the structure from Parent 1: `argument = scaler * logp_diff - margin`.\n8. Compute the final loss using a numerically stable logistic loss: `loss = -logsigmoid(argument)`.\n9. Return the weighted mean of the loss over the batch.", "hyperparams": {"alpha": 3.0}, "operators_used": ["logsigmoid", "softplus", "sigmoid", "rank_gap"], "implementation_hint": {"expects": ["cost_a", "cost_b", "log_prob_w", "log_prob_l", "weight"], "returns": "scalar"}, "code": "import torch\nimport torch.nn.functional as F\n\ndef generated_loss(batch, model_output, extra):\n    \"\"\"\n    Computes the Hybrid Rank-Modulated Loss.\n\n    This loss uses a unified, rank-based modulator as a source to generate both a\n    multiplicative scaler (via sigmoid) and an additive margin (directly), creating\n    a tightly coupled and adaptive learning signal.\n\n    Args:\n        batch (dict): A dictionary containing tensors.\n            'cost_a' (Tensor [N]): Cost of solution 'a'.\n            'cost_b' (Tensor [N]): Cost of solution 'b'.\n            'log_prob_w' (Tensor [N]): Log probability of the winning solution.\n            'log_prob_l' (Tensor [N]): Log probability of the losing solution.\n            'weight' (Tensor [N], optional): Per-pair loss weights.\n        model_output (dict): Model outputs (not used).\n        extra (dict): A dictionary for hyperparameters.\n            'alpha' (float): Controls the magnitude of the rank modulator.\n\n    Returns:\n        torch.Tensor: A scalar tensor representing the mean loss.\n    \"\"\"\n\n    # Whitelisted operator implemented as a helper function\n    def rank_gap(x, eps=1e-9):\n        \"\"\"Converts a tensor to its normalized ranks within the range [-0.5, 0.5].\"\"\"\n        if x.numel() <= 1:\n            return torch.zeros_like(x)\n        ranks = x.to(torch.float32).argsort().argsort().to(x.dtype)\n        # Normalize ranks to [0, 1]\n        normalized_ranks = ranks / (x.numel() - 1 + eps)\n        # Shift to [-0.5, 0.5]\n        return normalized_ranks - 0.5\n\n    # Retrieve hyperparameters\n    alpha = extra.get('alpha', 3.0)\n\n    # Unpack tensors from the batch\n    cost_a = batch['cost_a']\n    cost_b = batch['cost_b']\n    log_prob_w = batch['log_prob_w']\n    log_prob_l = batch['log_prob_l']\n    weights = batch.get('weight', None)\n\n    # 1. Calculate the difference in log probabilities\n    logp_diff = log_prob_w - log_prob_l\n\n    # 2. Calculate absolute cost difference\n    cost_diff = torch.abs(cost_a - cost_b)\n\n    # 3. (Inherited from Parent 0) Use rank_gap for a stable, scale-invariant cost signal\n    rank_gap_signal = rank_gap(cost_diff)\n\n    # 4. (Inherited from Parent 0) Create a unified, non-negative rank modulator\n    rank_modulator = F.softplus(alpha * rank_gap_signal)\n\n    # 5. (New Coupling & Inherited from Parent 1) Create a bounded multiplicative scaler\n    # The sigmoid ensures the scaler is always positive and bounded between [0.5, 1].\n    scaler = torch.sigmoid(rank_modulator)\n\n    # 6. (New Coupling & Inherited from Parent 1) Create the additive margin\n    # The margin is the modulator itself, providing a smooth, non-negative target.\n    margin = rank_modulator\n\n    # 7. Combine components into the final loss argument\n    # The structure is `scaler * logp_diff - margin`.\n    argument = scaler * logp_diff - margin\n\n    # 8. Compute the final loss using numerically stable logsigmoid\n    loss_per_item = -F.logsigmoid(argument)\n\n    # 9. Apply optional weights and compute the mean loss\n    if weights is not None and weights.sum() > 0:\n        loss = (loss_per_item * weights).sum() / weights.sum()\n    else:\n        loss = loss_per_item.mean()\n\n    return loss"}, "static_ok": true, "static_reason": "", "dynamic_ok": true, "dynamic_reason": "ok", "loss_value": 1.1892598867416382, "grad_norm": 0.0}
{"generation": 3, "index": 3, "ir": {"name": "Adaptive Rank-Gapped Margin Loss", "intuition": "This loss function creates a highly adaptive learning signal by combining a stable, soft-clipped log-probability difference with a dynamic, rank-based margin. It is designed to be robust to cost scales and prevent gradient explosion.\n\nInherited Ideas:\n- From `Soft-Clipped Rank-Modulated Loss` (Parent 0), it inherits the idea of **soft-clipping the log-probability difference using `tanh`**. This bounds the `logp_diff` to the `[-1, 1]` range, preventing extreme gradients and focusing the learning signal on getting the preference direction correct rather than chasing an infinite log-probability gap.\n- From `Rank-Gapped Adaptive Sigmoid Loss` (Parent 1), it inherits the concept of an **additive, cost-dependent margin**. The core of the loss is ensuring the (clipped) log-probability difference surpasses a margin that adapts based on the significance of the preference pair.\n\nNew Coupling Ideas & Modifications:\n1. **Unified Rank-Based Margin**: Both parents use a `rank_gap` signal derived from cost differences for stability. This child loss simplifies the formulation by using this `rank_gap` signal to construct a single, adaptive margin. The margin is calculated as `softplus(alpha * rank_gap_signal)`, creating a smooth, non-negative, and monotonically increasing margin based on the relative importance of the preference pair. This directly inherits the margin structure from Parent 1 but applies it to the clipped logp_diff from Parent 0.\n2. **Margin as a Target for Tanh**: The core coupling idea is to treat the `softplus`-derived margin as the target value for the `tanh(logp_diff)`. The `tanh` output is bounded by `[-1, 1]`, while the `softplus` margin is non-negative. The loss becomes `softplus(margin - tanh(logp_diff))`. This structure has a desirable property: for pairs with a small cost difference (low rank), the margin is small, and the model only needs a slightly positive `logp_diff` to achieve low loss. For pairs with a large cost difference (high rank), the margin grows, pushing `tanh(logp_diff)` towards its maximum value of 1. This creates a stable, bounded, and adaptive learning objective.", "pseudocode": "1. Calculate the difference in log probabilities: `logp_diff = logp_winner - logp_loser`.\n2. (Inherited from Parent 0) Soft-clip the log-probability difference using `tanh` for stability: `clipped_logp_diff = tanh(logp_diff)`.\n3. Calculate the absolute difference in costs: `cost_diff = abs(cost_winner - cost_loser)`.\n4. (Inherited from Parent 1) Convert the cost differences into a normalized rank signal between -0.5 and 0.5 using the `rank_gap` operator. This makes the signal robust to cost scale and outliers.\n5. (New Coupling) Create a single, unified, and adaptive margin by applying `softplus` to the scaled rank signal: `margin = softplus(alpha * rank_gap_signal)`. This results in a smooth, non-negative value that increases with the rank of the cost difference.\n6. (New Coupling) Formulate the loss argument as the difference between the target margin and the clipped log-probability difference: `argument = margin - clipped_logp_diff`. The goal is to make `clipped_logp_diff` greater than or equal to `margin`.\n7. Compute the final loss using `softplus` on the argument. This is equivalent to `-logsigmoid(-(argument))`, providing a numerically stable hinge-like loss that penalizes cases where `clipped_logp_diff < margin`.\n8. Return the weighted mean of the loss over the batch.", "hyperparams": {"alpha": 4.0}, "operators_used": ["softplus", "tanh", "rank_gap"], "implementation_hint": {"expects": ["cost_a", "cost_b", "log_prob_w", "log_prob_l", "weight"], "returns": "scalar"}, "code": "import torch\nimport torch.nn.functional as F\n\ndef generated_loss(batch, model_output, extra):\n    \"\"\"\n    Computes the Adaptive Rank-Gapped Margin Loss.\n\n    This loss penalizes the model if the tanh-clipped log-probability difference\n    does not exceed a dynamic margin derived from the rank of the cost difference.\n\n    Args:\n        batch (dict): A dictionary containing tensors.\n            'cost_a' (Tensor [N]): Cost of solution 'a'.\n            'cost_b' (Tensor [N]): Cost of solution 'b'.\n            'log_prob_w' (Tensor [N]): Log probability of the winning solution.\n            'log_prob_l' (Tensor [N]): Log probability of the losing solution.\n            'weight' (Tensor [N], optional): Per-pair loss weights.\n        model_output (dict): Model outputs (not used).\n        extra (dict): A dictionary for hyperparameters.\n            'alpha' (float): Controls the steepness and scale of the adaptive margin.\n\n    Returns:\n        torch.Tensor: A scalar tensor representing the mean loss.\n    \"\"\"\n\n    # Whitelisted operator implemented as a helper function\n    def rank_gap(x, eps=1e-9):\n        \"\"\"Converts a tensor to its normalized ranks within the range [-0.5, 0.5].\"\"\"\n        if x.numel() <= 1:\n            return torch.zeros_like(x)\n        ranks = x.to(torch.float32).argsort().argsort().to(x.dtype)\n        # Normalize ranks to [0, 1]\n        normalized_ranks = ranks / (x.numel() - 1 + eps)\n        # Shift to [-0.5, 0.5]\n        return normalized_ranks - 0.5\n\n    # Retrieve hyperparameters\n    alpha = extra.get('alpha', 4.0)\n\n    # Unpack tensors from the batch\n    cost_a = batch['cost_a']\n    cost_b = batch['cost_b']\n    log_prob_w = batch['log_prob_w']\n    log_prob_l = batch['log_prob_l']\n    weights = batch.get('weight', None)\n\n    # 1. Calculate the difference in log probabilities\n    logp_diff = log_prob_w - log_prob_l\n\n    # 2. (Inherited from Parent 0) Soft-clip for stability\n    clipped_logp_diff = torch.tanh(logp_diff)\n\n    # 3. Calculate absolute cost difference\n    cost_diff = torch.abs(cost_a - cost_b)\n\n    # 4. (Inherited from Parent 1) Use rank_gap for a stable, scale-invariant cost signal\n    rank_gap_signal = rank_gap(cost_diff)\n\n    # 5. (New Coupling) Create a single, unified adaptive margin\n    # The margin is smooth, non-negative, and increases with the rank of the cost difference.\n    margin = F.softplus(alpha * rank_gap_signal)\n\n    # 6. (New Coupling) Formulate the loss argument\n    # We want clipped_logp_diff > margin. The loss penalizes margin - clipped_logp_diff > 0.\n    argument = margin - clipped_logp_diff\n\n    # 7. Compute the final loss using softplus for a stable hinge-like penalty\n    # F.softplus(x) is a smooth approximation of relu(x) and is numerically stable.\n    loss_per_item = F.softplus(argument)\n\n    # 8. Apply optional weights and compute the mean loss\n    if weights is not None:\n        loss = (loss_per_item * weights).sum() / weights.sum().clamp(min=1e-8)\n    else:\n        loss = loss_per_item.mean()\n\n    return loss"}, "static_ok": true, "static_reason": "", "dynamic_ok": true, "dynamic_reason": "ok", "loss_value": 1.2550013065338135, "grad_norm": 0.0}
{"generation": 3, "index": 4, "ir": {"name": "Adaptive Rank-Modulated DPO Loss", "intuition": "This loss function adapts the principles of Direct Preference Optimization (DPO) by introducing a dynamic, rank-based modulation of the implicit reward margin. It aims to combine the stability of a clipped log-probability difference with the adaptiveness of a cost-aware margin, while introducing a new coupling mechanism inspired by the DPO formulation.\n\nInherited Ideas:\n- From `Soft-Clipped Rank-Modulated Loss` (Parent 0): It inherits the idea of using a **`rank_gap` normalized cost difference** to create a stable, scale-invariant signal representing the relative importance of a preference pair within a batch. This signal is transformed via `softplus` into a smooth, non-negative modulator.\n- From `Rank-Gapped Adaptive Sigmoid Loss` (Parent 1): It inherits the structure of combining a **multiplicative scaler and an additive margin**, both derived from the same underlying rank-based signal. This allows the loss to simultaneously adjust the gradient magnitude and the required log-probability gap based on the cost difference.\n\nNew Coupling Ideas & Modifications:\n1. **DPO-Style Formulation**: Instead of directly using `logp_diff` (log_prob_w - log_prob_l), this loss adopts a structure inspired by DPO, which uses `logsigmoid(logp_diff - logp_ref_diff)`. Here, we treat the rank-based modulator as an implicit reward margin, creating the term `logp_diff - rank_modulator`. This reframes the problem as ensuring the log-probability difference exceeds a dynamic, cost-sensitive margin.\n2. **Gated Log-Probability Difference**: The `logp_diff` term is gated by a `tanh` activation, `tanh(logp_diff)`. This is a new coupling that serves two purposes: it stabilizes the `logp_diff` by clipping its contribution to `[-1, 1]`, preventing extreme values from causing instability, and it focuses the loss on getting the sign of the preference correct rather than driving the log-probability gap to infinity. The final argument inside the `logsigmoid` becomes `tanh(logp_diff) * rank_modulator`, which can be interpreted as scaling the effective margin by the model's current confidence.", "pseudocode": "1. Calculate the difference in log probabilities: `logp_diff = logp_winner - logp_loser`.\n2. Calculate the absolute difference in costs: `cost_diff = abs(cost_winner - cost_loser)`.\n3. (Inherited from Parent 0) Convert cost differences into a normalized rank signal between -0.5 and 0.5 using the `rank_gap` operator. This makes the signal robust to cost scale and outliers.\n4. (Inherited from Parent 0 & 1) Create a single, unified `rank_modulator` by applying `softplus` to the scaled rank signal: `rank_modulator = softplus(beta * rank_gap_signal)`. This serves as a dynamic, cost-aware margin.\n5. (New Coupling) Gate the log-probability difference for stability using `tanh`: `gated_logp_diff = tanh(logp_diff)`.\n6. (New Coupling) Formulate the loss argument in a DPO-inspired style. The goal is to maximize the log-sigmoid of the difference between the model's output and an implicit reward margin. Here, we use `gated_logp_diff` as the model's output and `rank_modulator` as the margin, but we multiply them: `argument = gated_logp_diff * rank_modulator`. This structure encourages `gated_logp_diff` to be large and positive when the `rank_modulator` is large.\n7. Compute the final loss using a numerically stable logistic loss: `loss = -logsigmoid(argument)`.\n8. Return the weighted mean of the loss over the batch.", "hyperparams": {"beta": 4.0}, "operators_used": ["logsigmoid", "softplus", "tanh", "rank_gap"], "implementation_hint": {"expects": ["cost_a", "cost_b", "logp_w", "logp_l"], "returns": "scalar"}, "code": "import torch\nimport torch.nn.functional as F\n\ndef generated_loss(batch, model_output, extra):\n    \"\"\"\n    Computes the Adaptive Rank-Modulated DPO Loss.\n\n    This loss adapts a DPO-like structure by using a rank-based, cost-aware modulator\n    as a dynamic margin, applied to a tanh-gated log-probability difference.\n\n    Args:\n        batch (dict): A dictionary containing tensors.\n            'cost_a' (Tensor [N]): Cost of solution 'a'.\n            'cost_b' (Tensor [N]): Cost of solution 'b'.\n            'log_prob_w' (Tensor [N]): Log probability of the winning solution.\n            'log_prob_l' (Tensor [N]): Log probability of the losing solution.\n            'weight' (Tensor [N], optional): Per-pair loss weights.\n        model_output (dict): Model outputs (not used).\n        extra (dict): A dictionary for hyperparameters.\n            'beta' (float): Controls the magnitude of the rank modulator (implicit margin).\n\n    Returns:\n        torch.Tensor: A scalar tensor representing the mean loss.\n    \"\"\"\n\n    # Whitelisted operator implemented as a helper function\n    def rank_gap(x, eps=1e-9):\n        \"\"\"Converts a tensor to its normalized ranks within the range [-0.5, 0.5].\"\"\"\n        if x.numel() <= 1:\n            return torch.zeros_like(x)\n        ranks = x.to(torch.float32).argsort().argsort().to(x.dtype)\n        # Normalize ranks to [0, 1]\n        normalized_ranks = ranks / (x.numel() - 1 + eps)\n        # Shift to [-0.5, 0.5]\n        return normalized_ranks - 0.5\n\n    # Retrieve hyperparameters\n    beta = extra.get('beta', 4.0)\n\n    # Unpack tensors from the batch\n    cost_a = batch['cost_a']\n    cost_b = batch['cost_b']\n    log_prob_w = batch['log_prob_w']\n    log_prob_l = batch['log_prob_l']\n    weights = batch.get('weight', None)\n\n    # 1. Calculate the difference in log probabilities\n    logp_diff = log_prob_w - log_prob_l\n\n    # 2. Calculate absolute cost difference\n    cost_diff = torch.abs(cost_a - cost_b)\n\n    # 3. (Inherited from Parent 0) Use rank_gap for a stable, scale-invariant cost signal\n    rank_gap_signal = rank_gap(cost_diff)\n\n    # 4. (Inherited from Parents 0 & 1) Create a unified rank modulator as a dynamic margin\n    # softplus ensures the modulator is non-negative and smooth.\n    rank_modulator = F.softplus(beta * rank_gap_signal)\n\n    # 5. (New Coupling) Gate the logp_diff for stability\n    gated_logp_diff = torch.tanh(logp_diff)\n\n    # 6. (New Coupling) Formulate the loss argument in a DPO-inspired style\n    # We want to maximize the sigmoid of the gated difference scaled by the modulator.\n    argument = gated_logp_diff * rank_modulator\n\n    # 7. Compute the final loss using numerically stable logsigmoid\n    loss_per_item = -F.logsigmoid(argument)\n\n    # 8. Apply optional weights and compute the mean loss\n    if weights is not None and weights.sum() > 0:\n        loss = (loss_per_item * weights).sum() / weights.sum()\n    else:\n        loss = loss_per_item.mean()\n\n    return loss"}, "static_ok": true, "static_reason": "", "dynamic_ok": true, "dynamic_reason": "ok", "loss_value": 0.6931471824645996, "grad_norm": 0.0}
{"generation": 3, "index": 5, "ir": {"name": "Rank-Gated Clipped Loss", "intuition": "This loss function introduces a dynamic gating mechanism to selectively apply a stable, clipped learning signal. It aims to focus training on meaningful preference pairs by using cost information to modulate the loss.\n\nInherited Ideas:\n- From `Soft-Clipped Rank-Modulated Loss`: It inherits the core concept of **soft-clipping the log-probability difference** using `tanh(logp_diff)`. This bounds the gradient source, preventing numerical instability and focusing the model on learning the correct preference direction rather than an arbitrarily large log-probability gap.\n- From `Rank-Gapped Adaptive Sigmoid Loss`: It inherits the idea of using the **rank-normalized cost difference (`rank_gap`) as a robust, scale-invariant signal** for the importance of a preference pair. This makes the loss insensitive to the absolute scale of costs and robust to outliers.\n\nNew Coupling Ideas:\n1.  **Rank-Based Gating Mechanism**: A new coupling is introduced where the `rank_gap` signal is transformed into a `gate` using a `sigmoid` function. This gate, controlled by a hyperparameter `beta`, smoothly transitions from 0 to 1 based on the relative importance (rank) of the cost difference within the batch. For low-rank pairs (small cost difference), the gate is close to 0, effectively nullifying the loss and gradient for that pair. For high-rank pairs, the gate is close to 1, fully enabling the learning signal. This focuses model updates on the most significant preference examples.\n2.  **Gated and Clipped Signal Combination**: The `gate` is coupled with the inherited `tanh(logp_diff)` by simple multiplication: `gate * tanh(logp_diff)`. This elegantly combines the ideas: the `tanh` provides a stable, bounded signal, and the `gate` determines how much of that signal is actually used in the loss calculation. The final loss is computed using `softplus` on the negative of this gated signal, which is equivalent to a standard logistic loss but written to be stable.", "pseudocode": "1. Calculate the difference in log probabilities: `logp_diff = logp_winner - logp_loser`.\n2. (Inherited from `Soft-Clipped Rank-Modulated Loss`) Soft-clip the log-probability difference for stability: `clipped_logp_diff = tanh(logp_diff)`.\n3. Calculate the absolute difference in costs: `cost_diff = abs(cost_winner - cost_loser)`.\n4. (Inherited from `Rank-Gapped Adaptive Sigmoid Loss`) Convert the cost differences into a normalized, scale-invariant signal between -0.5 and 0.5 using the `rank_gap` operator.\n5. (New Coupling) Create a smooth gating signal from the rank signal. Apply a `sigmoid` function to the scaled rank signal: `gate = sigmoid(beta * rank_gap_signal)`. This gate will be close to 0 for low-rank pairs and close to 1 for high-rank pairs.\n6. (New Coupling) Combine the gate and the clipped log-probability difference. Multiply them to create a gated signal: `gated_signal = gate * clipped_logp_diff`. This ensures that the learning signal is only active for pairs with a sufficiently high cost difference rank.\n7. Compute the final loss using a `softplus` formulation, which is a stable way to implement a logistic loss: `loss = softplus(-gated_signal)`.\n8. Return the weighted mean of the loss over the batch.", "hyperparams": {"beta": 10.0}, "operators_used": ["softplus", "tanh", "sigmoid", "rank_gap"], "implementation_hint": {"expects": ["cost_a", "cost_b", "logp_a", "logp_b"], "returns": "scalar"}, "code": "import torch\nimport torch.nn.functional as F\n\ndef generated_loss(batch, model_output, extra):\n    \"\"\"\n    Computes the Rank-Gated Clipped Loss.\n\n    This loss uses a rank-based gating mechanism to selectively apply a stable, \n    tanh-clipped learning signal, focusing updates on more significant preference pairs.\n\n    Args:\n        batch (dict): A dictionary containing tensors.\n            'cost_a' (Tensor [N]): Cost of solution 'a'.\n            'cost_b' (Tensor [N]): Cost of solution 'b'.\n            'log_prob_w' (Tensor [N]): Log probability of the winning solution.\n            'log_prob_l' (Tensor [N]): Log probability of the losing solution.\n            'weight' (Tensor [N], optional): Per-pair loss weights.\n        model_output (dict): Model outputs (not used).\n        extra (dict): A dictionary for hyperparameters.\n            'beta' (float): Controls the steepness and midpoint of the sigmoid gate.\n\n    Returns:\n        torch.Tensor: A scalar tensor representing the mean loss.\n    \"\"\"\n\n    # Whitelisted operator implemented as a helper function\n    def rank_gap(x, eps=1e-9):\n        \"\"\"Converts a tensor to its normalized ranks within the range [-0.5, 0.5].\"\"\"\n        if x.numel() <= 1:\n            return torch.zeros_like(x)\n        ranks = x.to(torch.float32).argsort().argsort().to(x.dtype)\n        # Normalize ranks to [0, 1]\n        normalized_ranks = ranks / (x.numel() - 1 + eps)\n        # Shift to [-0.5, 0.5]\n        return normalized_ranks - 0.5\n\n    # Retrieve hyperparameters\n    beta = extra.get('beta', 10.0)\n\n    # Unpack tensors from the batch\n    cost_a = batch['cost_a']\n    cost_b = batch['cost_b']\n    log_prob_w = batch['log_prob_w']\n    log_prob_l = batch['log_prob_l']\n    weights = batch.get('weight', None)\n\n    # 1. Calculate the difference in log probabilities\n    logp_diff = log_prob_w - log_prob_l\n\n    # 2. (Inherited) Soft-clip the log-probability difference for stability\n    clipped_logp_diff = torch.tanh(logp_diff)\n\n    # 3. Calculate absolute cost difference\n    cost_diff = torch.abs(cost_a - cost_b)\n\n    # 4. (Inherited) Use rank_gap for a stable, scale-invariant cost signal\n    rank_gap_signal = rank_gap(cost_diff)\n\n    # 5. (New Coupling) Create a smooth gating signal from the rank signal\n    # The gate is close to 0 for low-rank pairs and 1 for high-rank pairs.\n    gate = torch.sigmoid(beta * rank_gap_signal)\n\n    # 6. (New Coupling) Combine the gate and the clipped log-probability difference\n    # This selectively applies the learning signal based on the cost rank.\n    gated_signal = gate * clipped_logp_diff\n\n    # 7. Compute the final loss using a stable softplus formulation (equivalent to -logsigmoid(x))\n    loss_per_item = F.softplus(-gated_signal)\n\n    # 8. Apply optional weights and compute the mean loss\n    if weights is not None:\n        loss = (loss_per_item * weights).sum() / weights.sum().clamp(min=1e-8)\n    else:\n        loss = loss_per_item.mean()\n\n    return loss"}, "static_ok": true, "static_reason": "", "dynamic_ok": true, "dynamic_reason": "ok", "loss_value": 0.6931471824645996, "grad_norm": 0.0}
{"generation": 3, "index": 6, "ir": {"name": "Adaptive Rank-Modulated Hinge Loss", "intuition": "This loss function combines the stability of soft-clipping with an adaptive, rank-based hinge margin to create a robust and focused learning signal. The goal is to ensure the log-probability difference not only exceeds a margin but that the margin itself is dynamically scaled by the relative importance of the preference pair within a batch.\n\nInherited Ideas:\n- From `Soft-Clipped Rank-Modulated Loss` (Parent 0), it inherits the idea of **soft-clipping the log-probability difference** using `tanh(logp_diff)`. This bounds the log-probability term, preventing extreme gradients and improving numerical stability.\n- From `Rank-Gapped Adaptive Sigmoid Loss` (Parent 1), it inherits the concept of using a **cost-dependent additive margin**. The model is penalized if the log-probability difference does not exceed this margin, which is larger for pairs with a more significant cost difference.\n\nNew Coupling Ideas & Modifications:\n1.  **Unified Rank-Based Modulator**: Both parents use a `rank_gap` signal derived from the cost difference. This child unifies this concept by creating a single `rank_modulator` calculated as `softplus(alpha * rank_gap(cost_diff))`. This modulator is a smooth, non-negative value that represents the 'importance' of a preference pair, and it is used to adaptively set the hinge margin.\n2.  **Adaptive Hinge Margin (`relu`)**: Instead of using a sigmoid-based loss (`logsigmoid`), this child introduces a hinge-like structure using the `relu` operator. The core of the loss is `relu(margin - clipped_logp_diff)`. The key coupling is that the `margin` is not a fixed hyperparameter but is set to the `rank_modulator`. This means pairs with a higher-ranked cost difference must achieve a larger `tanh(logp_diff)` to avoid incurring a loss, effectively focusing the model's capacity on learning the most important preferences.", "pseudocode": "1. Calculate the difference in log probabilities: `logp_diff = logp_winner - logp_loser`.\n2. (Inherited from Parent 0) Soft-clip the log-probability difference using `tanh` for stability: `clipped_logp_diff = tanh(logp_diff)`. This bounds the value to [-1, 1].\n3. Calculate the absolute difference in costs: `cost_diff = abs(cost_winner - cost_loser)`.\n4. (New Coupling) Convert the cost differences into a normalized rank signal between -0.5 and 0.5 using the `rank_gap` operator. This makes the signal robust to cost scale and outliers.\n5. (New Coupling) Create a single, unified `rank_modulator` by applying `softplus` to the scaled rank signal: `margin = softplus(alpha * rank_gap_signal)`. This results in a smooth, non-negative, and adaptive margin for each pair.\n6. (Inherited from Parent 1 & New Coupling) Construct the loss using an adaptive hinge mechanism. The loss for each item is the positive part of `margin - clipped_logp_diff`, calculated as `relu(margin - clipped_logp_diff)`. This penalizes the model only when the clipped log-probability difference fails to meet the dynamically set margin.\n7. Return the weighted mean of the loss over the batch.", "hyperparams": {"alpha": 3.0}, "operators_used": ["relu", "softplus", "tanh", "rank_gap"], "implementation_hint": {"expects": ["cost_a", "cost_b", "logp_w", "logp_l", "weight"], "returns": "scalar"}, "code": "import torch\nimport torch.nn.functional as F\n\ndef generated_loss(batch, model_output, extra):\n    \"\"\"\n    Computes the Adaptive Rank-Modulated Hinge Loss.\n\n    This loss uses a hinge-like structure where the margin is dynamically set\n    based on the rank of the cost difference within the batch. The log-probability\n    difference is soft-clipped with tanh for stability.\n\n    Args:\n        batch (dict): A dictionary containing tensors.\n            'cost_a' (Tensor [N]): Cost of solution 'a'.\n            'cost_b' (Tensor [N]): Cost of solution 'b'.\n            'log_prob_w' (Tensor [N]): Log probability of the winning solution.\n            'log_prob_l' (Tensor [N]): Log probability of the losing solution.\n            'weight' (Tensor [N], optional): Per-pair loss weights.\n        model_output (dict): Model outputs (not used).\n        extra (dict): A dictionary for hyperparameters.\n            'alpha' (float): Controls the magnitude of the adaptive margin.\n\n    Returns:\n        torch.Tensor: A scalar tensor representing the mean loss.\n    \"\"\"\n\n    # Whitelisted operator implemented as a helper function\n    def rank_gap(x, eps=1e-9):\n        \"\"\"Converts a tensor to its normalized ranks within the range [-0.5, 0.5].\"\"\"\n        if x.numel() <= 1:\n            return torch.zeros_like(x)\n        ranks = x.to(torch.float32).argsort().argsort().to(x.dtype)\n        # Normalize ranks to [0, 1]\n        normalized_ranks = ranks / (x.numel() - 1 + eps)\n        # Shift to [-0.5, 0.5]\n        return normalized_ranks - 0.5\n\n    # Retrieve hyperparameters\n    alpha = extra.get('alpha', 3.0)\n\n    # Unpack tensors from the batch\n    cost_a = batch['cost_a']\n    cost_b = batch['cost_b']\n    log_prob_w = batch['log_prob_w']\n    log_prob_l = batch['log_prob_l']\n    weights = batch.get('weight', None)\n\n    # 1. Calculate the difference in log probabilities\n    logp_diff = log_prob_w - log_prob_l\n\n    # 2. (Inherited from Parent 0) Soft-clip for stability\n    clipped_logp_diff = torch.tanh(logp_diff)\n\n    # 3. Calculate absolute cost difference\n    cost_diff = torch.abs(cost_a - cost_b)\n\n    # 4. (New Coupling) Use rank_gap for a stable, scale-invariant cost signal\n    rank_gap_signal = rank_gap(cost_diff)\n\n    # 5. (Inherited from Parent 1 & New Coupling) Create an adaptive margin\n    # softplus ensures the margin is non-negative and smoothly increases with rank.\n    adaptive_margin = F.softplus(alpha * rank_gap_signal)\n\n    # 6. (New Coupling) Construct the loss using an adaptive hinge mechanism (relu)\n    # The loss is incurred only if the clipped log-prob diff is less than the margin.\n    loss_per_item = F.relu(adaptive_margin - clipped_logp_diff)\n\n    # 7. Apply optional weights and compute the mean loss\n    if weights is not None:\n        loss = (loss_per_item * weights).sum() / weights.sum().clamp(min=1e-8)\n    else:\n        loss = loss_per_item.mean()\n\n    return loss"}, "static_ok": true, "static_reason": "", "dynamic_ok": true, "dynamic_reason": "ok", "loss_value": 0.7933657169342041, "grad_norm": 0.0}
{"generation": 3, "index": 7, "ir": {"name": "Adaptive Rank-Modulated Hinge Loss", "intuition": "This loss function creates a highly adaptive learning signal by combining a stable, bounded log-probability difference with a dynamic margin that is itself modulated by the rank of the cost difference. The design prioritizes stability and focuses learning on pairs with significant quality gaps.\n\nInherited Ideas:\n- From `Soft-Clipped Rank-Modulated Loss` (Parent 0): It inherits the use of a unified `rank_modulator` derived from the `rank_gap` of cost differences. This modulator, `softplus(alpha * rank_gap_signal)`, acts as a single, smooth, non-negative signal that increases with the relative importance of a preference pair within the batch. This makes the loss robust to cost scales and outliers.\n- From `Rank-Gapped Adaptive Sigmoid Loss` (Parent 1): It inherits the core structure of combining a multiplicative scaler and an additive margin. Specifically, it uses the idea that both the scaling and the margin should be functions of the cost difference to adapt the loss to the data.\n\nNew Coupling Ideas & Modifications:\n1.  **Dual-Purpose Rank Modulator**: This child loss couples the `rank_modulator` in a novel way. The `rank_modulator` itself becomes the additive margin. Simultaneously, a bounded version of this modulator, `tanh(beta * rank_modulator)`, is used as the multiplicative scaler for the log-probability difference. This creates a tight feedback loop: a larger cost difference leads to a larger margin, which in turn demands a stronger log-probability signal, and the penalty for failing to meet this demand is also increased via the `tanh`-scaled modulator.\n2.  **Stable Log-Probability Clipping**: Instead of directly using `tanh(logp_diff)` like Parent 0, this loss uses `tanh(logp_diff / tau)`. This introduces a temperature `tau` that controls the softness of the clipping. A higher `tau` makes the clipping gentler, allowing for larger gradients on pairs where `logp_diff` is already large, while a smaller `tau` makes the clipping sharper, focusing the model on just getting the sign correct. This provides an extra degree of control over gradient dynamics.", "pseudocode": "1. Calculate the difference in log probabilities: `logp_diff = logp_winner - logp_loser`.\n2. Calculate the absolute difference in costs: `cost_diff = abs(cost_winner - cost_loser)`.\n3. (Inherited from Parent 0 & 1) Convert cost differences into a stable, scale-invariant signal using the `rank_gap` operator, which normalizes ranks to the range [-0.5, 0.5].\n4. (Inherited from Parent 0) Create a unified `rank_modulator` using the rank signal: `rank_modulator = softplus(alpha * rank_gap_signal)`. This serves as the base for both the margin and the scaler.\n5. (New Coupling) Define the additive margin directly as the `rank_modulator`. This ensures the required log-probability gap increases smoothly with the rank of the cost difference.\n6. (New Coupling) Define the multiplicative scaler by passing the modulator through a `tanh` function: `scaler = tanh(beta * rank_modulator)`. This creates a bounded scaler (0 to ~1) that saturates as the cost difference becomes very significant, preventing instability.\n7. (New Coupling) Soft-clip the log-probability difference with a temperature `tau`: `clipped_logp_diff = tanh(logp_diff / tau)`.\n8. Combine the components into the loss argument, structured like a hinge loss: `argument = scaler * clipped_logp_diff - margin`.\n9. Compute the final loss using a numerically stable logistic loss function: `loss = -logsigmoid(argument)`.\n10. Return the weighted mean of the loss over the batch.", "hyperparams": {"alpha": 4.0, "beta": 1.0, "tau": 1.0}, "operators_used": ["logsigmoid", "softplus", "tanh", "rank_gap"], "implementation_hint": {"expects": ["cost_a", "cost_b", "log_prob_w", "log_prob_l", "weight"], "returns": "scalar"}, "code": "import torch\nimport torch.nn.functional as F\n\ndef generated_loss(batch, model_output, extra):\n    \"\"\"\n    Computes the Adaptive Rank-Modulated Hinge Loss.\n\n    This loss combines a rank-based margin with a tanh-bounded scaler derived from the same rank signal.\n    The log-probability difference is also soft-clipped with a temperature parameter.\n\n    Args:\n        batch (dict): A dictionary containing tensors.\n            'cost_a' (Tensor [N]): Cost of solution 'a'.\n            'cost_b' (Tensor [N]): Cost of solution 'b'.\n            'log_prob_w' (Tensor [N]): Log probability of the winning solution.\n            'log_prob_l' (Tensor [N]): Log probability of the losing solution.\n            'weight' (Tensor [N], optional): Per-pair loss weights.\n        model_output (dict): Model outputs (not used).\n        extra (dict): A dictionary for hyperparameters.\n            'alpha' (float): Controls the magnitude of the rank modulator and margin.\n            'beta' (float): Controls the saturation of the multiplicative scaler.\n            'tau' (float): Temperature for soft-clipping the logp_diff.\n\n    Returns:\n        torch.Tensor: A scalar tensor representing the mean loss.\n    \"\"\"\n\n    # Whitelisted operator implemented as a helper function\n    def rank_gap(x, eps=1e-9):\n        \"\"\"Converts a tensor to its normalized ranks within the range [-0.5, 0.5].\"\"\"\n        if x.numel() <= 1:\n            return torch.zeros_like(x)\n        ranks = x.to(torch.float32).argsort().argsort().to(x.dtype)\n        normalized_ranks = ranks / (x.numel() - 1 + eps)\n        return normalized_ranks - 0.5\n\n    # Retrieve hyperparameters\n    alpha = extra.get('alpha', 4.0)\n    beta = extra.get('beta', 1.0)\n    tau = extra.get('tau', 1.0)\n\n    # Unpack tensors from the batch\n    cost_a = batch['cost_a']\n    cost_b = batch['cost_b']\n    log_prob_w = batch['log_prob_w']\n    log_prob_l = batch['log_prob_l']\n    weights = batch.get('weight', None)\n\n    # 1. Calculate the difference in log probabilities\n    logp_diff = log_prob_w - log_prob_l\n\n    # 2. Calculate absolute cost difference\n    cost_diff = torch.abs(cost_a - cost_b)\n\n    # 3. (Inherited) Use rank_gap for stable, scale-invariant cost signal\n    rank_gap_signal = rank_gap(cost_diff)\n\n    # 4. (Inherited) Create a unified rank modulator\n    rank_modulator = F.softplus(alpha * rank_gap_signal)\n\n    # 5. (New Coupling) The rank modulator itself serves as the additive margin\n    margin = rank_modulator\n\n    # 6. (New Coupling) Create a bounded scaler from the modulator\n    scaler = torch.tanh(beta * rank_modulator)\n\n    # 7. (New Coupling) Soft-clip the log-probability difference with temperature tau\n    clipped_logp_diff = torch.tanh(logp_diff / tau)\n\n    # 8. Combine components into the final loss argument\n    # We want scaler * clipped_logp_diff > margin\n    argument = scaler * clipped_logp_diff - margin\n\n    # 9. Compute the final loss using numerically stable logsigmoid\n    loss_per_item = -F.logsigmoid(argument)\n\n    # 10. Apply optional weights and compute the mean loss\n    if weights is not None:\n        loss = (loss_per_item * weights).sum() / weights.sum().clamp(min=1e-8)\n    else:\n        loss = loss_per_item.mean()\n\n    return loss"}, "static_ok": true, "static_reason": "", "dynamic_ok": true, "dynamic_reason": "ok", "loss_value": 1.2550013065338135, "grad_norm": 0.0}
{"generation": 4, "index": 0, "ir": {"name": "Adaptive Rank-Gated Loss", "intuition": "This loss function creates a highly adaptive learning signal by dynamically gating the influence of the log-probability difference based on the relative importance of the preference pair. It is designed to be robust to outliers and numerically stable.\n\nInherited Ideas:\n- From `Soft-Clipped Rank-Modulated Loss` (Parent 0), it inherits the idea of using `rank_gap` on the cost difference to create a scale-invariant signal. This ensures the loss is robust to the magnitude and distribution of costs, focusing only on the relative importance of each preference pair within a batch.\n- From `Rank-Gapped Adaptive Sigmoid Loss` (Parent 1), it inherits the structure of having a multiplicative scaler and an additive margin, both derived from the cost difference, to create a loss of the form `scaler * logp_diff - margin`.\n\nNew Coupling Ideas & Modifications:\n1. **Sigmoid Gating Mechanism**: A new coupling is introduced where the `rank_gap` signal is passed through a `sigmoid` function to create a 'gate' value between 0 and 1. This gate acts as the multiplicative scaler. When the cost difference is insignificant (low rank), the gate is close to 0, effectively zeroing out the `logp_diff` term and focusing the loss purely on a small base margin. For significant cost differences (high rank), the gate approaches 1, allowing the `logp_diff` to be fully considered.\n2. **Softplus-Modulated Margin**: The additive margin is not just `softplus(signal)` but is now `softplus(gate - margin_offset)`. This couples the margin directly to the gate value. It creates a 'dead zone' where for pairs with a gate value below `margin_offset`, the margin is effectively zero. This prevents the model from being penalized for not distinguishing between pairs with very small cost differences, allowing it to focus its capacity on more important distinctions.", "pseudocode": "1. Calculate the difference in log probabilities: `logp_diff = logp_winner - logp_loser`.\n2. Calculate the absolute difference in costs: `cost_diff = abs(cost_winner - cost_loser)`.\n3. (Inherited from Parent 0) Convert the cost differences into a normalized rank signal between -0.5 and 0.5 using the `rank_gap` operator. This makes the signal robust to cost scale and outliers.\n4. (New Coupling) Create a gating value by applying a `sigmoid` function to the scaled rank signal: `gate = sigmoid(alpha * rank_gap_signal)`. This produces a smooth gate between 0 and 1.\n5. (Inherited from Parent 1 & New Coupling) Create an adaptive margin that is coupled to the gate. The margin is calculated as `softplus(gate - margin_offset)`. This creates a small margin that only becomes active when the gate value surpasses the `margin_offset` threshold.\n6. (Inherited from Parent 1 & New Coupling) Use the `gate` as the multiplicative scaler for the `logp_diff`.\n7. Combine the components into the final loss argument: `argument = gate * logp_diff - margin`. This structure uses the gate to control both the influence of the log-probability difference and the magnitude of the required margin.\n8. Compute the final loss using a numerically stable logistic loss: `loss = -logsigmoid(argument)`.\n9. Return the weighted mean of the loss over the batch.", "hyperparams": {"alpha": 5.0, "margin_offset": 0.5}, "operators_used": ["logsigmoid", "softplus", "sigmoid", "rank_gap"], "implementation_hint": {"expects": ["cost_a", "cost_b", "logp_a", "logp_b"], "returns": "scalar"}, "code": "import torch\nimport torch.nn.functional as F\n\ndef generated_loss(batch, model_output, extra):\n    \"\"\"\n    Computes the Adaptive Rank-Gated Loss.\n\n    This loss uses a sigmoid gate derived from rank-normalized cost differences to\n    modulate both a multiplicative scaler and an additive margin.\n\n    Args:\n        batch (dict): A dictionary containing tensors.\n            'cost_a' (Tensor [N]): Cost of solution 'a'.\n            'cost_b' (Tensor [N]): Cost of solution 'b'.\n            'log_prob_w' (Tensor [N]): Log probability of the winning solution.\n            'log_prob_l' (Tensor [N]): Log probability of the losing solution.\n            'weight' (Tensor [N], optional): Per-pair loss weights.\n        model_output (dict): Model outputs (not used).\n        extra (dict): A dictionary for hyperparameters.\n            'alpha' (float): Controls the steepness of the sigmoid gate.\n            'margin_offset' (float): The threshold for the gate before the margin activates.\n\n    Returns:\n        torch.Tensor: A scalar tensor representing the mean loss.\n    \"\"\"\n\n    # Whitelisted operator implemented as a helper function\n    def rank_gap(x, eps=1e-9):\n        \"\"\"Converts a tensor to its normalized ranks within the range [-0.5, 0.5].\"\"\"\n        if x.numel() <= 1:\n            return torch.zeros_like(x)\n        ranks = x.to(torch.float32).argsort().argsort().to(x.dtype)\n        normalized_ranks = ranks / (x.numel() - 1 + eps)\n        return normalized_ranks - 0.5\n\n    # Retrieve hyperparameters\n    alpha = extra.get('alpha', 5.0)\n    margin_offset = extra.get('margin_offset', 0.5)\n\n    # Unpack tensors from the batch\n    cost_a = batch['cost_a']\n    cost_b = batch['cost_b']\n    log_prob_w = batch['log_prob_w']\n    log_prob_l = batch['log_prob_l']\n    weights = batch.get('weight', None)\n\n    # 1. Calculate the difference in log probabilities\n    logp_diff = log_prob_w - log_prob_l\n\n    # 2. Calculate absolute cost difference\n    cost_diff = torch.abs(cost_a - cost_b)\n\n    # 3. (Inherited from Parent 0) Use rank_gap for stable, scale-invariant cost signal\n    rank_gap_signal = rank_gap(cost_diff)\n\n    # 4. (New Coupling) Create a sigmoid gate from the rank signal\n    # The gate is ~0 for low-rank pairs and ~1 for high-rank pairs.\n    gate = torch.sigmoid(alpha * rank_gap_signal)\n\n    # 5. (New Coupling) Create a margin coupled to the gate value\n    # The margin only becomes non-zero when the gate value is > margin_offset.\n    margin = F.softplus(gate - margin_offset)\n\n    # 6. (Inherited from Parent 1) Use the gate as a multiplicative scaler\n    # This effectively ignores logp_diff for pairs with insignificant cost differences.\n    scaled_logp_diff = gate * logp_diff\n\n    # 7. Combine components into the final loss argument\n    argument = scaled_logp_diff - margin\n\n    # 8. Compute the final loss using numerically stable logsigmoid\n    loss_per_item = -F.logsigmoid(argument)\n\n    # 9. Apply optional weights and compute the mean loss\n    if weights is not None and weights.sum() > 0:\n        loss = (loss_per_item * weights).sum() / weights.sum()\n    else:\n        loss = loss_per_item.mean()\n\n    return loss"}, "static_ok": true, "static_reason": "", "dynamic_ok": true, "dynamic_reason": "ok", "loss_value": 1.1082185506820679, "grad_norm": 0.0}
{"generation": 4, "index": 1, "ir": {"name": "Rank-Biased Sigmoid-Clipped Loss", "intuition": "This loss function integrates a stable, clipped log-probability difference with a dual-purpose, rank-based modulator that acts as both a margin and a dynamic clipping range. The goal is to create a robust learning signal that adapts to the relative importance of preference pairs within a batch.\n\nInherited Ideas:\n- From Parent 0 (`Soft-Clipped Rank-Modulated Loss`): It inherits the concept of using a **unified, rank-based modulator** derived from the cost difference. This modulator, `rank_bias`, is calculated using `softplus` on a `rank_gap` signal, making it a smooth, non-negative value that increases with the preference pair's importance.\n- From Parent 1 (`Rank-Gapped Adaptive Sigmoid Loss`): It inherits the core structure of a **multiplicatively scaled log-probability difference minus an additive margin**. The loss takes the form `logsigmoid(scaled_logp_diff - margin)`, where both the scaling and margin are influenced by the cost difference.\n\nNew Coupling Ideas & Modifications:\n1. **Sigmoid Clipping of Log-Probability Difference**: Instead of `tanh`, this loss uses a scaled `sigmoid` function, `2 * sigmoid(logp_diff) - 1`, to clip the log-probability difference into the `[-1, 1]` range. This provides a similar bounding effect to `tanh` for stability but offers a slightly different gradient profile.\n2. **Dynamic Clipping via Rank Bias**: The `rank_bias` modulator is coupled with the clipped `logp_diff` in a novel way. The final loss argument is `rank_bias * (clipped_logp_diff - 1)`. This formulation achieves two things simultaneously: it sets an implicit margin equal to `rank_bias` and scales the penalty by the same `rank_bias`. For a pair with a high `rank_bias`, the `clipped_logp_diff` must be extremely close to its maximum value of 1 to minimize the loss, effectively tightening the learning objective for more important pairs.", "pseudocode": "1. Calculate the difference in log probabilities: `logp_diff = logp_winner - logp_loser`.\n2. (New Coupling) Clip the `logp_diff` into the range `[-1, 1]` using a scaled sigmoid function for stability: `clipped_logp_diff = 2 * sigmoid(logp_diff) - 1`.\n3. Calculate the absolute difference in costs: `cost_diff = abs(cost_winner - cost_loser)`.\n4. Convert the cost differences into a normalized rank signal between -0.5 and 0.5 using the `rank_gap` operator. This makes the signal robust to cost scale and outliers.\n5. (Inherited from Parent 0) Create a unified `rank_bias` modulator by applying `softplus` to the scaled rank signal: `rank_bias = softplus(alpha * rank_gap_signal)`. This results in a smooth, non-negative value that increases with the rank of the cost difference.\n6. (Inherited from Parent 1 & New Coupling) Combine the components. Use the `rank_bias` as both a multiplicative scaler and an implicit additive margin. The loss argument becomes `rank_bias * (clipped_logp_diff - 1)`. This is equivalent to `rank_bias * clipped_logp_diff - rank_bias`, fitting the `scaler * logp_diff - margin` structure.\n7. Compute the final loss using a numerically stable logistic loss function: `loss = -logsigmoid(rank_bias * (clipped_logp_diff - 1))`.\n8. Return the weighted mean of the loss over the batch.", "hyperparams": {"alpha": 3.0}, "operators_used": ["logsigmoid", "softplus", "sigmoid", "rank_gap"], "implementation_hint": {"expects": ["cost_a", "cost_b", "logp_w", "logp_l", "weight"], "returns": "scalar"}, "code": "import torch\nimport torch.nn.functional as F\n\ndef generated_loss(batch, model_output, extra):\n    \"\"\"\n    Computes the Rank-Biased Sigmoid-Clipped Loss.\n\n    This loss uses a rank-based bias term derived from cost differences to dynamically\n    set both a margin and a scaling factor on a sigmoid-clipped log-probability difference.\n\n    Args:\n        batch (dict): A dictionary containing tensors.\n            'cost_a' (Tensor [N]): Cost of solution 'a'.\n            'cost_b' (Tensor [N]): Cost of solution 'b'.\n            'log_prob_w' (Tensor [N]): Log probability of the winning solution.\n            'log_prob_l' (Tensor [N]): Log probability of the losing solution.\n            'weight' (Tensor [N], optional): Per-pair loss weights.\n        model_output (dict): Model outputs (not used).\n        extra (dict): A dictionary for hyperparameters.\n            'alpha' (float): Controls the magnitude of the rank bias.\n\n    Returns:\n        torch.Tensor: A scalar tensor representing the mean loss.\n    \"\"\"\n\n    # Whitelisted operator implemented as a helper function\n    def rank_gap(x, eps=1e-9):\n        \"\"\"Converts a tensor to its normalized ranks within the range [-0.5, 0.5].\"\"\"\n        if x.numel() <= 1:\n            return torch.zeros_like(x)\n        ranks = x.to(torch.float32).argsort().argsort().to(x.dtype)\n        # Normalize ranks to [0, 1]\n        normalized_ranks = ranks / (x.numel() - 1 + eps)\n        # Shift to [-0.5, 0.5]\n        return normalized_ranks - 0.5\n\n    # Retrieve hyperparameters\n    alpha = extra.get('alpha', 3.0)\n\n    # Unpack tensors from the batch\n    cost_a = batch['cost_a']\n    cost_b = batch['cost_b']\n    log_prob_w = batch['log_prob_w']\n    log_prob_l = batch['log_prob_l']\n    weights = batch.get('weight', None)\n\n    # 1. Calculate the difference in log probabilities\n    logp_diff = log_prob_w - log_prob_l\n\n    # 2. (New Coupling) Clip logp_diff to [-1, 1] using sigmoid\n    clipped_logp_diff = 2 * torch.sigmoid(logp_diff) - 1\n\n    # 3. Calculate absolute cost difference\n    cost_diff = torch.abs(cost_a - cost_b)\n\n    # 4. Use rank_gap for a stable, scale-invariant cost signal\n    rank_gap_signal = rank_gap(cost_diff)\n\n    # 5. (Inherited from Parent 0) Create a unified rank bias modulator\n    # softplus ensures the bias is non-negative and smooth.\n    rank_bias = F.softplus(alpha * rank_gap_signal)\n\n    # 6. (Inherited from Parent 1 & New Coupling) Combine components\n    # The rank_bias serves as both a scaler and an implicit margin.\n    # The target is `rank_bias * clipped_logp_diff > rank_bias`.\n    argument = rank_bias * (clipped_logp_diff - 1)\n\n    # 7. Compute the final loss using numerically stable logsigmoid\n    loss_per_item = -F.logsigmoid(argument)\n\n    # 8. Apply optional weights and compute the mean loss\n    if weights is not None:\n        loss = (loss_per_item * weights).sum() / weights.sum().clamp(min=1e-8)\n    else:\n        loss = loss_per_item.mean()\n\n    return loss"}, "static_ok": true, "static_reason": "", "dynamic_ok": true, "dynamic_reason": "ok", "loss_value": 1.1892598867416382, "grad_norm": 0.0}
{"generation": 4, "index": 2, "ir": {"name": "Adaptive Rank-Gated Loss", "intuition": "This loss function creates a highly adaptive learning signal by dynamically gating the influence of the log-probability difference based on the relative quality of the preference pair. It aims to focus learning on 'easy' pairs (large cost gap) while regularizing the signal for 'hard' pairs (small cost gap).\n\nInherited Ideas:\n- From `Soft-Clipped Rank-Modulated Loss` (Parent 0), it inherits the idea of using a **`tanh` activation on the log-probability difference (`logp_diff`)**. This soft-clipping bounds the log-probability signal to `[-1, 1]`, preventing extreme gradients and ensuring numerical stability.\n- From `Rank-Gapped Adaptive Sigmoid Loss` (Parent 1), it inherits the concept of using a **`rank_gap` normalized cost difference** as the primary signal for modulating the loss. This makes the loss invariant to the absolute scale of costs and robust to outliers, focusing instead on the relative importance of preference pairs within a batch.\n\nNew Coupling Ideas:\n1.  **Sigmoid Gating Mechanism**: The core new idea is to use a `sigmoid` function, controlled by the `rank_gap` signal, to create a 'gate'. This gate, `gate = sigmoid(alpha * rank_gap_signal)`, smoothly transitions from approximately 0.5 (for low-rank pairs) to 1.0 (for high-rank pairs). It acts as a multiplicative scaler on the `tanh(logp_diff)`, effectively 'gating' its contribution. For high-rank pairs, the full `tanh(logp_diff)` signal is used; for low-rank pairs, its influence is halved, providing a form of regularization.\n2.  **Coupled Additive Margin**: The gate is also used to create a coupled additive margin. The margin is defined as `gate * beta`. This design ensures that the required log-probability gap (the margin) scales directly with the gate's confidence. For high-rank pairs where the gate is ~1, the margin is `beta`. For low-rank pairs where the gate is ~0.5, the margin is `beta/2`. This coupling elegantly ties the scaling of the learning signal and the target margin together through a single, interpretable gating value.", "pseudocode": "1. Calculate the difference in log probabilities: `logp_diff = logp_winner - logp_loser`.\n2. (Inherited from Parent 0) Soft-clip the log-probability difference for stability: `clipped_logp_diff = tanh(logp_diff)`.\n3. Calculate the absolute difference in costs: `cost_diff = abs(cost_winner - cost_loser)`.\n4. (Inherited from Parent 1) Convert cost differences into a stable, normalized rank signal between -0.5 and 0.5 using the `rank_gap` operator.\n5. (New Coupling) Create a sigmoid gate based on the rank signal: `gate = sigmoid(alpha * rank_gap_signal)`. This gate will be close to 1.0 for high-rank pairs and 0.5 for low-rank pairs.\n6. (New Coupling) Define an adaptive margin that is directly coupled to the gate: `margin = beta * gate`.\n7. Combine the components. The gate acts as a multiplicative scaler on the clipped log-probability difference. The final argument for the loss is `gate * clipped_logp_diff - margin`.\n8. Compute the final loss using the numerically stable logistic loss function: `loss = -logsigmoid(argument)`.\n9. Return the weighted mean of the loss over the batch.", "hyperparams": {"alpha": 4.0, "beta": 1.0}, "operators_used": ["logsigmoid", "sigmoid", "tanh", "rank_gap"], "implementation_hint": {"expects": ["cost_a", "cost_b", "logp_w", "logp_l"], "returns": "scalar"}, "code": "import torch\nimport torch.nn.functional as F\n\ndef generated_loss(batch, model_output, extra):\n    \"\"\"\n    Computes the Adaptive Rank-Gated Loss.\n\n    This loss uses a sigmoid gate derived from the rank-normalized cost difference\n    to both scale the log-probability difference and set a coupled margin.\n\n    Args:\n        batch (dict): A dictionary containing tensors.\n            'cost_a' (Tensor [N]): Cost of solution 'a'.\n            'cost_b' (Tensor [N]): Cost of solution 'b'.\n            'log_prob_w' (Tensor [N]): Log probability of the winning solution.\n            'log_prob_l' (Tensor [N]): Log probability of the losing solution.\n            'weight' (Tensor [N], optional): Per-pair loss weights.\n        model_output (dict): Model outputs (not used).\n        extra (dict): A dictionary for hyperparameters.\n            'alpha' (float): Controls the steepness of the sigmoid gate.\n            'beta' (float): The maximum margin, applied when the gate is 1.\n\n    Returns:\n        torch.Tensor: A scalar tensor representing the mean loss.\n    \"\"\"\n\n    # Whitelisted operator implemented as a helper function\n    def rank_gap(x, eps=1e-9):\n        \"\"\"Converts a tensor to its normalized ranks within the range [-0.5, 0.5].\"\"\"\n        if x.numel() <= 1:\n            return torch.zeros_like(x)\n        ranks = x.to(torch.float32).argsort().argsort().to(x.dtype)\n        # Normalize ranks to [0, 1]\n        normalized_ranks = ranks / (x.numel() - 1 + eps)\n        # Shift to [-0.5, 0.5]\n        return normalized_ranks - 0.5\n\n    # Retrieve hyperparameters\n    alpha = extra.get('alpha', 4.0)\n    beta = extra.get('beta', 1.0)\n\n    # Unpack tensors from the batch\n    cost_a = batch['cost_a']\n    cost_b = batch['cost_b']\n    log_prob_w = batch['log_prob_w']\n    log_prob_l = batch['log_prob_l']\n    weights = batch.get('weight', None)\n\n    # 1. Calculate the difference in log probabilities\n    logp_diff = log_prob_w - log_prob_l\n\n    # 2. (Inherited from Parent 0) Soft-clip for stability\n    clipped_logp_diff = torch.tanh(logp_diff)\n\n    # 3. Calculate absolute cost difference\n    cost_diff = torch.abs(cost_a - cost_b)\n\n    # 4. (Inherited from Parent 1) Use rank_gap for stable, scale-invariant signal\n    rank_gap_signal = rank_gap(cost_diff)\n\n    # 5. (New Coupling) Create a sigmoid gate based on the rank signal\n    # The gate smoothly transitions from ~0.5 to ~1.0 as rank increases.\n    gate = torch.sigmoid(alpha * rank_gap_signal)\n\n    # 6. (New Coupling) Create a margin coupled to the gate\n    # The margin scales with the gate's confidence.\n    margin = beta * gate\n\n    # 7. Combine components: the gate scales the logp_diff and sets the margin\n    # We want gate * clipped_logp_diff > margin\n    argument = gate * clipped_logp_diff - margin\n\n    # 8. Compute the final loss using numerically stable logsigmoid\n    loss_per_item = -F.logsigmoid(argument)\n\n    # 9. Apply optional weights and compute the mean loss\n    if weights is not None:\n        loss = (loss_per_item * weights).sum() / weights.sum().clamp(min=1e-8)\n    else:\n        loss = loss_per_item.mean()\n\n    return loss"}, "static_ok": true, "static_reason": "", "dynamic_ok": true, "dynamic_reason": "ok", "loss_value": 0.9817067384719849, "grad_norm": 0.0}
{"generation": 4, "index": 3, "ir": {"name": "Adaptive Rank-Gapped Margin Loss", "intuition": "This loss function creates a highly adaptive learning signal by dynamically setting a margin based on the relative quality of preferences within a batch, while also stabilizing the core learning signal.\n\nInherited Ideas:\n- From Parent 0 (`Soft-Clipped Rank-Modulated Loss`): It inherits the use of `tanh` to **soft-clip the log-probability difference** (`logp_diff`). This is a stability trick that bounds the log-probability signal to `[-1, 1]`, preventing extreme gradients and focusing the model on correctly ordering preferences rather than maximizing an unbounded difference.\n- From Parent 1 (`Rank-Gapped Adaptive Sigmoid Loss`): It inherits the core structure of using an **additive, cost-dependent margin**. The loss aims to ensure that the log-probability difference surpasses this margin, which is derived from the cost difference between the winning and losing candidates.\n\nNew Coupling Ideas & Modifications:\n1.  **Dual-Source Margin Coupling**: The margin is not derived from a single source. Instead, it couples two signals: a `rank_gap` signal (robust to cost scale and outliers) and a `log`-transformed raw cost difference signal. These are combined additively (`log(cost_diff) + alpha * rank_gap_signal`), allowing the margin to be sensitive to both the relative importance of a pair within the batch (via `rank_gap`) and its absolute magnitude (via `log(cost_diff)`). The `log` transformation dampens the effect of very large cost differences, improving stability.\n2.  **Adaptive Margin Scaling**: The entire computed margin is passed through a `softplus` function. This ensures the final margin is always non-negative, smooth, and differentiable. This creates a flexible, data-dependent margin that adapts to the specific characteristics of each preference pair, blending relative rank and absolute magnitude in a stable manner. The final structure `tanh(logp_diff) - margin` is then used, where the clipped `logp_diff` must overcome this adaptive margin.", "pseudocode": "1. Calculate the difference in log probabilities: `logp_diff = logp_winner - logp_loser`.\n2. (Inherited from Parent 0) Soft-clip the log-probability difference using `tanh` for stability: `clipped_logp_diff = tanh(logp_diff)`.\n3. Calculate the absolute difference in costs: `cost_diff = abs(cost_winner - cost_loser)`.\n4. (New Coupling) Create a raw magnitude signal by taking the logarithm of the cost difference, adding a small epsilon for stability: `log_cost_signal = log(cost_diff + epsilon)`.\n5. (New Coupling) Create a relative importance signal by converting cost differences into normalized ranks using the `rank_gap` operator: `rank_gap_signal = rank_gap(cost_diff)`.\n6. (Inherited from Parent 1 & New Coupling) Combine the two signals to form a unified, adaptive margin. First, add the scaled rank signal to the log-cost signal: `combined_signal = log_cost_signal + alpha * rank_gap_signal`. Then, apply `softplus` to ensure the final margin is non-negative and smooth: `margin = softplus(combined_signal)`.\n7. Construct the final loss argument by subtracting the adaptive margin from the clipped log-probability difference: `argument = clipped_logp_diff - margin`.\n8. Compute the final loss using a numerically stable logistic loss function: `loss = -logsigmoid(argument)`.\n9. Return the weighted mean of the loss over the batch.", "hyperparams": {"alpha": 3.0, "epsilon": 1e-06}, "operators_used": ["logsigmoid", "softplus", "tanh", "log", "rank_gap"], "implementation_hint": {"expects": ["cost_a", "cost_b", "logp_w", "logp_l", "weight"], "returns": "scalar"}, "code": "import torch\nimport torch.nn.functional as F\n\ndef generated_loss(batch, model_output, extra):\n    \"\"\"\n    Computes the Adaptive Rank-Gapped Margin Loss.\n\n    This loss combines a tanh-clipped log-probability difference with an adaptive margin.\n    The margin is a coupled signal derived from both the log-transformed raw cost\n    difference and its normalized rank within the batch.\n\n    Args:\n        batch (dict): A dictionary containing tensors.\n            'cost_a' (Tensor [N]): Cost of solution 'a'.\n            'cost_b' (Tensor [N]): Cost of solution 'b'.\n            'log_prob_w' (Tensor [N]): Log probability of the winning solution.\n            'log_prob_l' (Tensor [N]): Log probability of the losing solution.\n            'weight' (Tensor [N], optional): Per-pair loss weights.\n        model_output (dict): Model outputs (not used).\n        extra (dict): A dictionary for hyperparameters.\n            'alpha' (float): Controls the influence of the rank signal on the margin.\n            'epsilon' (float): Small constant for numerical stability in log.\n\n    Returns:\n        torch.Tensor: A scalar tensor representing the mean loss.\n    \"\"\"\n\n    # Whitelisted operator implemented as a helper function\n    def rank_gap(x, eps=1e-9):\n        \"\"\"Converts a tensor to its normalized ranks within the range [-0.5, 0.5].\"\"\"\n        if x.numel() <= 1:\n            return torch.zeros_like(x)\n        ranks = x.to(torch.float32).argsort().argsort().to(x.dtype)\n        # Normalize ranks to [0, 1]\n        normalized_ranks = ranks / (x.numel() - 1 + eps)\n        # Shift to [-0.5, 0.5]\n        return normalized_ranks - 0.5\n\n    # Retrieve hyperparameters\n    alpha = extra.get('alpha', 3.0)\n    epsilon = extra.get('epsilon', 1e-6)\n\n    # Unpack tensors from the batch\n    cost_a = batch['cost_a']\n    cost_b = batch['cost_b']\n    log_prob_w = batch['log_prob_w']\n    log_prob_l = batch['log_prob_l']\n    weights = batch.get('weight', None)\n\n    # 1. Calculate the difference in log probabilities\n    logp_diff = log_prob_w - log_prob_l\n\n    # 2. (Inherited from Parent 0) Soft-clip for stability\n    clipped_logp_diff = torch.tanh(logp_diff)\n\n    # 3. Calculate absolute cost difference\n    cost_diff = torch.abs(cost_a - cost_b)\n\n    # 4. (New Coupling) Create a raw magnitude signal from log-cost\n    log_cost_signal = torch.log(cost_diff + epsilon)\n\n    # 5. (New Coupling) Create a relative importance signal from rank\n    rank_gap_signal = rank_gap(cost_diff)\n\n    # 6. (Inherited from Parent 1 & New Coupling) Create the adaptive margin\n    # Couple the log-cost and rank signals, then use softplus for a smooth, non-negative margin.\n    combined_signal = log_cost_signal + alpha * rank_gap_signal\n    margin = F.softplus(combined_signal)\n\n    # 7. Construct the final loss argument\n    # The clipped logp_diff must overcome the adaptive margin.\n    argument = clipped_logp_diff - margin\n\n    # 8. Compute the final loss using numerically stable logsigmoid\n    loss_per_item = -F.logsigmoid(argument)\n\n    # 9. Apply optional weights and compute the mean loss\n    if weights is not None:\n        loss = (loss_per_item * weights).sum() / weights.sum().clamp(min=1e-8)\n    else:\n        loss = loss_per_item.mean()\n\n    return loss"}, "static_ok": true, "static_reason": "", "dynamic_ok": true, "dynamic_reason": "ok", "loss_value": 1.1892602443695068, "grad_norm": 0.0}
{"generation": 4, "index": 4, "ir": {"name": "Adaptive Log-Ratio Loss with Dual Rank-Modulation", "intuition": "This loss function creates a highly adaptive learning signal by combining the stability of a log-ratio formulation with a dynamic, rank-based margin and scaler. The core idea is to modulate the learning signal based on the relative importance of a preference pair within its batch, ensuring robustness and focusing learning on meaningful differences.\n\nInherited Ideas:\n- From `Soft-Clipped Rank-Modulated Loss` (Parent 0): It inherits the concept of using a single, unified `rank_modulator` derived from the `rank_gap` of cost differences. This modulator simultaneously dictates the learning signal's magnitude and target, creating a tightly coupled system where more significant preference pairs face a tougher learning objective.\n- From `Rank-Gapped Adaptive Sigmoid Loss` (Parent 1): It inherits the structure of combining a multiplicative scaler and an additive margin (`scaler * logp_diff - margin`). However, it adapts this by applying the logic to log-probabilities directly, rather than their difference, and using the unified modulator from Parent 0.\n\nNew Coupling Ideas & Modifications:\n1. **Log-Ratio Formulation**: Instead of using the log-probability difference (`logp_w - logp_l`), this loss uses a log-ratio formulation: `log(logsigmoid(logp_w)) - log(logsigmoid(logp_l))`. This is inspired by the Bradley-Terry model and is equivalent to `log(p_w / p_l)`, where `p` is the sigmoid-transformed log-probability. This structure can offer better numerical stability and a different gradient landscape compared to a simple difference.\n2. **Unified Dual Modulation**: The `rank_modulator` (inherited from Parent 0) is applied in two distinct but coupled ways. It acts as a multiplicative `scaler` on the log-ratio, amplifying its importance. It is also transformed via `softplus` to become a smooth, non-negative `margin`. The final loss argument becomes `scaler * log_ratio - margin`. This design ensures that as the rank of a pair's cost difference increases, the loss both scales up the log-ratio signal and demands that it exceeds a larger margin, creating a powerful and adaptive learning objective.", "pseudocode": "1. Calculate the absolute difference in costs: `cost_diff = abs(cost_winner - cost_loser)`.\n2. (Inherited from Parent 0 & New Coupling) Convert cost differences to a normalized rank signal using the `rank_gap` operator. Create a unified `rank_modulator` from this signal: `rank_modulator = 1.0 + tanh(alpha * rank_gap_signal)`. This bounds the modulator between 0 and 2, centered at 1.\n3. (New Coupling) Calculate the log-ratio of the sigmoid-transformed log probabilities: `log_ratio = logsigmoid(logp_winner) - logsigmoid(logp_loser)`.\n4. (Inherited from Parent 1) Use the `rank_modulator` as a multiplicative scaler on the log-ratio: `scaled_log_ratio = rank_modulator * log_ratio`.\n5. (Inherited from Parent 0) Create a smooth, non-negative additive margin by applying `softplus` to the scaled rank signal: `margin = softplus(beta * rank_gap_signal)`.\n6. Combine the components into the final loss argument. The goal is `scaled_log_ratio > margin`.\n7. Compute the per-item loss using the numerically stable logistic loss function: `loss = -logsigmoid(scaled_log_ratio - margin)`.\n8. Return the weighted mean of the loss over the batch.", "hyperparams": {"alpha": 2.0, "beta": 1.0}, "operators_used": ["logsigmoid", "softplus", "tanh", "rank_gap"], "implementation_hint": {"expects": ["cost_a", "cost_b", "log_prob_w", "log_prob_l", "weight"], "returns": "scalar"}, "code": "import torch\nimport torch.nn.functional as F\n\ndef generated_loss(batch, model_output, extra):\n    \"\"\"\n    Computes the Adaptive Log-Ratio Loss with Dual Rank-Modulation.\n\n    This loss uses a log-ratio of sigmoid probabilities and modulates the learning signal\n    using a rank-based scaler and margin derived from the same underlying rank signal.\n\n    Args:\n        batch (dict): A dictionary containing tensors.\n            'cost_a' (Tensor [N]): Cost of solution 'a'.\n            'cost_b' (Tensor [N]): Cost of solution 'b'.\n            'log_prob_w' (Tensor [N]): Log probability of the winning solution.\n            'log_prob_l' (Tensor [N]): Log probability of the losing solution.\n            'weight' (Tensor [N], optional): Per-pair loss weights.\n        model_output (dict): Model outputs (not used).\n        extra (dict): A dictionary for hyperparameters.\n            'alpha' (float): Controls the magnitude of the rank modulator/scaler.\n            'beta' (float): Controls the magnitude of the additive margin.\n\n    Returns:\n        torch.Tensor: A scalar tensor representing the mean loss.\n    \"\"\"\n\n    # Whitelisted operator implemented as a helper function\n    def rank_gap(x, eps=1e-9):\n        \"\"\"Converts a tensor to its normalized ranks within the range [-0.5, 0.5].\"\"\"\n        if x.numel() <= 1:\n            return torch.zeros_like(x)\n        ranks = x.to(torch.float32).argsort().argsort().to(x.dtype)\n        normalized_ranks = ranks / (x.numel() - 1 + eps)\n        return normalized_ranks - 0.5\n\n    # Retrieve hyperparameters\n    alpha = extra.get('alpha', 2.0)\n    beta = extra.get('beta', 1.0)\n\n    # Unpack tensors from the batch\n    cost_a = batch['cost_a']\n    cost_b = batch['cost_b']\n    log_prob_w = batch['log_prob_w']\n    log_prob_l = batch['log_prob_l']\n    weights = batch.get('weight', None)\n\n    # 1. Calculate absolute cost difference\n    cost_diff = torch.abs(cost_a - cost_b)\n\n    # 2. (Inherited & New Coupling) Create a rank signal and a bounded modulator\n    rank_gap_signal = rank_gap(cost_diff)\n    # tanh bounds the signal, 1.0+ shifts it to be centered at 1 and non-negative.\n    rank_modulator = 1.0 + torch.tanh(alpha * rank_gap_signal)\n\n    # 3. (New Coupling) Calculate the log-ratio of sigmoid probabilities\n    # This is equivalent to log(p_w) - log(p_l) where p = sigmoid(log_prob)\n    log_ratio = F.logsigmoid(log_prob_w) - F.logsigmoid(log_prob_l)\n\n    # 4. (Inherited) Use the rank_modulator as a multiplicative scaler\n    scaled_log_ratio = rank_modulator * log_ratio\n\n    # 5. (Inherited) Create a smooth, non-negative additive margin\n    margin = F.softplus(beta * rank_gap_signal)\n\n    # 6. Combine components: we want scaled_log_ratio > margin\n    argument = scaled_log_ratio - margin\n\n    # 7. Compute the final loss using numerically stable logsigmoid\n    loss_per_item = -F.logsigmoid(argument)\n\n    # 8. Apply optional weights and compute the mean loss\n    if weights is not None:\n        # Ensure weights sum is not zero to avoid division by zero\n        weights_sum = weights.sum().clamp(min=1e-8)\n        loss = (loss_per_item * weights).sum() / weights_sum\n    else:\n        loss = loss_per_item.mean()\n\n    return loss"}, "static_ok": true, "static_reason": "", "dynamic_ok": true, "dynamic_reason": "ok", "loss_value": 1.1090569496154785, "grad_norm": 0.0}
{"generation": 4, "index": 5, "ir": {"name": "Adaptive Rank-Gated Loss", "intuition": "This loss function creates a highly adaptive learning signal by dynamically gating the influence of the log-probability difference based on the relative importance of a preference pair. It is designed to be robust to cost scales and outliers while preventing gradient explosion from large log-probability differences.\n\nInherited Ideas:\n- From `Soft-Clipped Rank-Modulated Loss` (Parent 0), it inherits the use of `tanh(logp_diff)` to soft-clip the log-probability difference. This bounds the signal, enhancing numerical stability and focusing the model on establishing the correct preference order rather than maximizing the log-probability gap indefinitely.\n- From `Rank-Gapped Adaptive Sigmoid Loss` (Parent 1), it inherits the concept of using a multiplicative scaler derived from the cost difference to modulate the learning signal. This ensures that pairs with a larger quality gap contribute more significantly to the loss.\n\nNew Coupling Ideas & Modifications:\n1. **Sigmoid Gating Mechanism**: Instead of a simple multiplicative scaler, this loss introduces a `sigmoid` function to act as a 'gate'. The gate, controlled by the rank-normalized cost difference, determines how much of the clipped log-probability difference is passed through. A `sigmoid` gate is smooth, differentiable, and naturally bounded between 0 and 1, providing a probabilistic interpretation of the pair's importance.\n2. **Dynamic Margin via Gating**: The loss is structured as `-logsigmoid(gate * (tanh(logp_diff) - (1 - gamma)))`. This formulation elegantly couples the gating mechanism with a dynamic margin. When the gate is open (close to 1, for important pairs), the model is pushed to make `tanh(logp_diff)` close to `1 - gamma`. When the gate is closed (close to 0, for unimportant pairs), the argument to `logsigmoid` approaches 0, resulting in a near-zero loss, effectively ignoring these pairs. The `gamma` hyperparameter provides a small, constant buffer, preventing the target for `tanh(logp_diff)` from being exactly 1, which can be difficult to achieve.", "pseudocode": "1. Calculate the difference in log probabilities: `logp_diff = logp_winner - logp_loser`.\n2. (Inherited from Parent 0) Soft-clip the log-probability difference using `tanh` for stability: `clipped_logp_diff = tanh(logp_diff)`.\n3. Calculate the absolute difference in costs: `cost_diff = abs(cost_winner - cost_loser)`.\n4. (Inherited from Parents) Convert cost differences to a normalized rank signal between -0.5 and 0.5 using the `rank_gap` operator. This provides a scale-invariant and outlier-robust measure of preference importance.\n5. (New Coupling) Create a smooth, bounded 'gate' using a `sigmoid` function applied to the scaled rank signal: `gate = sigmoid(alpha * rank_gap_signal)`. This gate will be close to 1 for high-rank pairs and close to 0 for low-rank pairs.\n6. (New Coupling) Define the loss argument by coupling the gate with the clipped log-probability difference and a margin. The structure is `gate * (clipped_logp_diff - (1 - gamma))`. This pushes `clipped_logp_diff` towards `1 - gamma` for pairs with a high gate value.\n7. Compute the final loss using a numerically stable logistic loss: `loss = -logsigmoid(argument)`.\n8. Return the weighted mean of the loss over the batch.", "hyperparams": {"alpha": 5.0, "gamma": 0.05}, "operators_used": ["logsigmoid", "sigmoid", "tanh", "rank_gap"], "implementation_hint": {"expects": ["cost_a", "cost_b", "logp_a", "logp_b"], "returns": "scalar"}, "code": "import torch\nimport torch.nn.functional as F\n\ndef generated_loss(batch, model_output, extra):\n    \"\"\"\n    Computes the Adaptive Rank-Gated Loss.\n\n    This loss uses a sigmoid gate, derived from the rank-normalized cost difference,\n    to modulate the influence of the tanh-clipped log-probability difference.\n\n    Args:\n        batch (dict): A dictionary containing tensors.\n            'cost_a' (Tensor [N]): Cost of solution 'a'.\n            'cost_b' (Tensor [N]): Cost of solution 'b'.\n            'log_prob_w' (Tensor [N]): Log probability of the winning solution.\n            'log_prob_l' (Tensor [N]): Log probability of the losing solution.\n            'weight' (Tensor [N], optional): Per-pair loss weights.\n        model_output (dict): Model outputs (not used).\n        extra (dict): A dictionary for hyperparameters.\n            'alpha' (float): Controls the steepness of the sigmoid gate.\n            'gamma' (float): A small constant to set the target margin away from 1.\n\n    Returns:\n        torch.Tensor: A scalar tensor representing the mean loss.\n    \"\"\"\n\n    # Whitelisted operator implemented as a helper function\n    def rank_gap(x, eps=1e-9):\n        \"\"\"Converts a tensor to its normalized ranks within the range [-0.5, 0.5].\"\"\"\n        if x.numel() <= 1:\n            return torch.zeros_like(x)\n        ranks = x.to(torch.float32).argsort().argsort().to(x.dtype)\n        # Normalize ranks to [0, 1]\n        normalized_ranks = ranks / (x.numel() - 1 + eps)\n        # Shift to [-0.5, 0.5]\n        return normalized_ranks - 0.5\n\n    # Retrieve hyperparameters\n    alpha = extra.get('alpha', 5.0)\n    gamma = extra.get('gamma', 0.05)\n\n    # Unpack tensors from the batch\n    cost_a = batch['cost_a']\n    cost_b = batch['cost_b']\n    log_prob_w = batch['log_prob_w']\n    log_prob_l = batch['log_prob_l']\n    weights = batch.get('weight', None)\n\n    # 1. Calculate the difference in log probabilities\n    logp_diff = log_prob_w - log_prob_l\n\n    # 2. (Inherited from Parent 0) Soft-clip for stability\n    clipped_logp_diff = torch.tanh(logp_diff)\n\n    # 3. Calculate absolute cost difference\n    cost_diff = torch.abs(cost_a - cost_b)\n\n    # 4. (Inherited) Use rank_gap for a stable, scale-invariant cost signal\n    rank_gap_signal = rank_gap(cost_diff)\n\n    # 5. (New Coupling) Create a smooth sigmoid gate\n    # This gate is ~1 for important pairs (high rank_gap) and ~0 for unimportant ones.\n    gate = torch.sigmoid(alpha * rank_gap_signal)\n\n    # 6. (New Coupling) Define the loss argument\n    # The gate modulates the target: we want tanh(logp_diff) > 1 - gamma for important pairs.\n    # For pairs where the gate is ~0, the argument is ~0, resulting in minimal loss.\n    target_margin = 1.0 - gamma\n    argument = gate * (clipped_logp_diff - target_margin)\n\n    # 7. Compute the final loss using numerically stable logsigmoid\n    loss_per_item = -F.logsigmoid(argument)\n\n    # 8. Apply optional weights and compute the mean loss\n    if weights is not None:\n        loss = (loss_per_item * weights).sum() / weights.sum().clamp(min=1e-8)\n    else:\n        loss = loss_per_item.mean()\n\n    return loss"}, "static_ok": true, "static_reason": "", "dynamic_ok": true, "dynamic_reason": "ok", "loss_value": 0.9678080677986145, "grad_norm": 0.0}
{"generation": 4, "index": 6, "ir": {"name": "Soft-Clipped Exponentially Modulated Loss", "intuition": "This loss function creates a stable and adaptive learning signal by combining ideas from its parents with a new exponential modulation scheme. The goal is to create a loss where the required log-probability gap and the penalty for misclassification both grow smoothly and significantly with the relative importance of a preference pair.\n\nInherited Ideas:\n- From `Soft-Clipped Rank-Modulated Loss` (Parent 0), it inherits the use of `tanh(logp_diff)` to soft-clip the log-probability difference. This bounds the signal, preventing extreme gradients and enhancing numerical stability.\n- From `Rank-Gapped Adaptive Sigmoid Loss` (Parent 1), it inherits the general structure of combining a multiplicative scaler and an additive margin, `scaler * logp_diff - margin`. This allows for independent control over the gradient magnitude and the required preference gap.\n\nNew Coupling Ideas & Modifications:\n1. **Exponential Rank Modulation**: The core novelty is using the `exp` function to transform the rank-gapped cost signal. The `rank_gap` operator (also inherited) provides a stable, scale-invariant signal from [-0.5, 0.5]. Applying `exp(alpha * rank_gap_signal)` creates a smooth, positive, and exponentially growing modulator. This `exp_modulator` serves as the base for both the scaler and the margin, creating a strong emphasis on correctly classifying high-importance pairs (those with a large cost difference rank).\n2. **Coupled Scaler and Margin**: The `exp_modulator` is used to derive both the `scaler` and the `margin`. The `scaler` is the modulator itself, while the `margin` is its logarithm, `log(exp_modulator)`, scaled by a hyperparameter `beta`. This creates a direct, functional relationship: `margin = beta * alpha * rank_gap_signal`. This coupling ensures that as the scaling factor grows exponentially, the required log-probability margin grows linearly with the rank signal. This design forces the model to achieve a larger `logp_diff` for pairs that also receive a higher gradient scale, creating a focused and powerful learning signal for important preference pairs.", "pseudocode": "1. Calculate the difference in log probabilities: `logp_diff = logp_winner - logp_loser`.\n2. (Inherited from Parent 0) Soft-clip the log-probability difference using `tanh` for stability: `clipped_logp_diff = tanh(logp_diff)`.\n3. Calculate the absolute difference in costs: `cost_diff = abs(cost_winner - cost_loser)`.\n4. (Inherited) Convert cost differences into a normalized rank signal between -0.5 and 0.5 using the `rank_gap` operator. This makes the signal robust to cost scale and outliers.\n5. (New Coupling) Create an exponential modulator from the rank signal: `exp_modulator = exp(alpha * rank_gap_signal)`. This results in a smooth, positive modulator that grows exponentially with the rank.\n6. (Inherited from Parent 1 & New Coupling) Use the `exp_modulator` as the multiplicative scaler: `scaler = exp_modulator`.\n7. (New Coupling) Create the additive margin by taking the logarithm of the modulator, scaled by a hyperparameter `beta`: `margin = beta * log(scaler)`. This couples the margin directly to the scaler, ensuring it grows linearly with the rank signal.\n8. Combine the components into the final loss argument: `argument = scaler * clipped_logp_diff - margin`. The goal is to make `scaler * clipped_logp_diff` greater than `margin`.\n9. Compute the final loss using a numerically stable logistic loss function: `loss = -logsigmoid(argument)`.\n10. Return the weighted mean of the loss over the batch.", "hyperparams": {"alpha": 2.0, "beta": 1.0}, "operators_used": ["logsigmoid", "tanh", "exp", "log", "rank_gap"], "implementation_hint": {"expects": ["cost_a", "cost_b", "log_prob_w", "log_prob_l", "weight"], "returns": "scalar"}, "code": "import torch\nimport torch.nn.functional as F\n\ndef generated_loss(batch, model_output, extra):\n    \"\"\"\n    Computes the Soft-Clipped Exponentially Modulated Loss.\n\n    This loss combines a tanh-clipped log-probability difference with a multiplicative\n    scaler and an additive margin. Both the scaler and margin are derived from an\n    exponentially transformed rank-gap signal of the cost differences.\n\n    Args:\n        batch (dict): A dictionary containing tensors.\n            'cost_a' (Tensor [N]): Cost of solution 'a'.\n            'cost_b' (Tensor [N]): Cost of solution 'b'.\n            'log_prob_w' (Tensor [N]): Log probability of the winning solution.\n            'log_prob_l' (Tensor [N]): Log probability of the losing solution.\n            'weight' (Tensor [N], optional): Per-pair loss weights.\n        model_output (dict): Model outputs (not used).\n        extra (dict): A dictionary for hyperparameters.\n            'alpha' (float): Controls the exponential growth of the modulator.\n            'beta' (float): Controls the magnitude of the additive margin.\n\n    Returns:\n        torch.Tensor: A scalar tensor representing the mean loss.\n    \"\"\"\n\n    # Whitelisted operator implemented as a helper function\n    def rank_gap(x, eps=1e-9):\n        \"\"\"Converts a tensor to its normalized ranks within the range [-0.5, 0.5].\"\"\"\n        if x.numel() <= 1:\n            return torch.zeros_like(x)\n        ranks = x.to(torch.float32).argsort().argsort().to(x.dtype)\n        # Normalize ranks to [0, 1]\n        normalized_ranks = ranks / (x.numel() - 1 + eps)\n        # Shift to [-0.5, 0.5]\n        return normalized_ranks - 0.5\n\n    # Retrieve hyperparameters\n    alpha = extra.get('alpha', 2.0)\n    beta = extra.get('beta', 1.0)\n\n    # Unpack tensors from the batch\n    cost_a = batch['cost_a']\n    cost_b = batch['cost_b']\n    log_prob_w = batch['log_prob_w']\n    log_prob_l = batch['log_prob_l']\n    weights = batch.get('weight', None)\n\n    # 1. Calculate the difference in log probabilities\n    logp_diff = log_prob_w - log_prob_l\n\n    # 2. (Inherited from Parent 0) Soft-clip for stability\n    clipped_logp_diff = torch.tanh(logp_diff)\n\n    # 3. Calculate absolute cost difference\n    cost_diff = torch.abs(cost_a - cost_b)\n\n    # 4. (Inherited) Use rank_gap for a stable, scale-invariant cost signal\n    rank_gap_signal = rank_gap(cost_diff)\n\n    # 5. (New Coupling) Create an exponential modulator as the scaler\n    # This scaler grows exponentially with the rank of the cost difference.\n    scaler = torch.exp(alpha * rank_gap_signal)\n\n    # 6. (New Coupling) Create a coupled margin from the log of the scaler\n    # This creates a margin that grows linearly with the rank_gap_signal.\n    # Using log(scaler) is numerically equivalent to beta * alpha * rank_gap_signal.\n    margin = beta * torch.log(scaler.clamp(min=1e-9))\n\n    # 7. Combine components into the final loss argument\n    # We want scaler * clipped_logp_diff > margin\n    argument = scaler * clipped_logp_diff - margin\n\n    # 8. Compute the final loss using numerically stable logsigmoid\n    loss_per_item = -F.logsigmoid(argument)\n\n    # 9. Apply optional weights and compute the mean loss\n    if weights is not None:\n        loss = (loss_per_item * weights).sum() / weights.sum().clamp(min=1e-8)\n    else:\n        loss = loss_per_item.mean()\n\n    return loss"}, "static_ok": true, "static_reason": "", "dynamic_ok": true, "dynamic_reason": "ok", "loss_value": 0.7391051650047302, "grad_norm": 0.0}
{"generation": 4, "index": 7, "ir": {"name": "Rank-Gapped Modulated Log-Ratio Loss", "intuition": "This loss function constructs a dynamic preference signal by combining a rank-based margin with a log-ratio of probabilities, stabilized by a softplus transformation. It aims to be robust to cost scales and provide a strong learning signal for significant preference pairs.\n\nInherited Ideas:\n- From Parent 1 (`Rank-Gapped Adaptive Sigmoid Loss`): It inherits the use of `rank_gap` on the cost difference to create a scale-invariant and outlier-robust signal. This signal is then used to create an **additive, rank-based margin** via `softplus(alpha * rank_gap_signal)`, ensuring the required log-probability separation increases with the relative importance of the preference pair.\n- From Parent 0 (`Soft-Clipped Rank-Modulated Loss`): It inherits the concept of using a **single, unified modulator** derived from the cost difference to control the loss. However, instead of using it as a multiplicative scaler on a clipped `logp_diff`, we use it as an additive margin.\n\nNew Coupling Ideas & Modifications:\n1. **Log-Ratio Objective**: Instead of the standard `logp_winner - logp_loser` difference, this loss uses a log-ratio formulation: `log(sigmoid(logp_winner) / sigmoid(logp_loser))`. This is equivalent to `logp_winner - softplus(logp_winner) - (logp_loser - softplus(logp_loser))`. This formulation focuses on the ratio of the inferred probabilities (p_winner / p_loser), which can offer a different and potentially more stable learning dynamic compared to the unbounded log-probability difference.\n2. **Softplus Stabilization**: The log-ratio objective is wrapped in a `softplus` function. The final loss argument becomes `softplus(log_ratio_objective) - margin`. The `softplus` ensures the primary learning term is always non-negative and smooth, preventing large negative values from dominating the loss signal, and focuses the learning on ensuring the positive log-ratio exceeds the margin.", "pseudocode": "1. Calculate the difference in log probabilities: `logp_diff = logp_winner - logp_loser`.\n2. Calculate the absolute difference in costs: `cost_diff = abs(cost_winner - cost_loser)`.\n3. (Inherited from Parent 1) Convert the cost differences into a normalized rank signal between -0.5 and 0.5 using the `rank_gap` operator. This makes the signal robust to cost scale and outliers.\n4. (Inherited from Parent 0 & 1) Create an additive, rank-based margin by applying `softplus` to the scaled rank signal: `margin = softplus(alpha * rank_gap_signal)`. This margin is smooth, non-negative, and increases with the rank of the cost difference.\n5. (New Coupling) Calculate the log-ratio of probabilities. This can be expressed in a numerically stable way using log-probabilities as `logp_diff - (softplus(logp_winner) - softplus(logp_loser))`.\n6. (New Coupling) Stabilize the log-ratio objective by applying a `softplus` function: `stabilized_objective = softplus(log_ratio_objective)`.\n7. Combine the components into the final loss argument. The goal is for the stabilized objective to exceed the margin: `argument = stabilized_objective - margin`.\n8. Compute the final loss using a numerically stable logistic loss function: `loss = -logsigmoid(argument)`.\n9. Return the weighted mean of the loss over the batch.", "hyperparams": {"alpha": 3.0}, "operators_used": ["logsigmoid", "softplus", "rank_gap"], "implementation_hint": {"expects": ["cost_a", "cost_b", "log_prob_w", "log_prob_l"], "returns": "scalar"}, "code": "import torch\nimport torch.nn.functional as F\n\ndef generated_loss(batch, model_output, extra):\n    \"\"\"\n    Computes the Rank-Gapped Modulated Log-Ratio Loss.\n\n    This loss aims for the softplus-stabilized log-ratio of probabilities to exceed\n    a margin derived from the rank-normalized cost difference.\n\n    Args:\n        batch (dict): A dictionary containing tensors.\n            'cost_a' (Tensor [N]): Cost of solution 'a'.\n            'cost_b' (Tensor [N]): Cost of solution 'b'.\n            'log_prob_w' (Tensor [N]): Log probability of the winning solution.\n            'log_prob_l' (Tensor [N]): Log probability of the losing solution.\n            'weight' (Tensor [N], optional): Per-pair loss weights.\n        model_output (dict): Model outputs (not used).\n        extra (dict): A dictionary for hyperparameters.\n            'alpha' (float): Controls the magnitude of the rank-based margin.\n\n    Returns:\n        torch.Tensor: A scalar tensor representing the mean loss.\n    \"\"\"\n\n    # Whitelisted operator implemented as a helper function\n    def rank_gap(x, eps=1e-9):\n        \"\"\"Converts a tensor to its normalized ranks within the range [-0.5, 0.5].\"\"\"\n        if x.numel() <= 1:\n            return torch.zeros_like(x)\n        ranks = x.to(torch.float32).argsort().argsort().to(x.dtype)\n        # Normalize ranks to [0, 1]\n        normalized_ranks = ranks / (x.numel() - 1 + eps)\n        # Shift to [-0.5, 0.5]\n        return normalized_ranks - 0.5\n\n    # Retrieve hyperparameters\n    alpha = extra.get('alpha', 3.0)\n\n    # Unpack tensors from the batch\n    cost_a = batch['cost_a']\n    cost_b = batch['cost_b']\n    log_prob_w = batch['log_prob_w']\n    log_prob_l = batch['log_prob_l']\n    weights = batch.get('weight', None)\n\n    # 1. Calculate the difference in log probabilities\n    logp_diff = log_prob_w - log_prob_l\n\n    # 2. Calculate absolute cost difference\n    cost_diff = torch.abs(cost_a - cost_b)\n\n    # 3. (Inherited) Use rank_gap for stable, scale-invariant cost signal\n    rank_gap_signal = rank_gap(cost_diff)\n\n    # 4. (Inherited) Create an additive, rank-based margin\n    additive_margin = F.softplus(alpha * rank_gap_signal)\n\n    # 5. (New Coupling) Calculate log-ratio objective in a stable way\n    # log(sigmoid(x)/sigmoid(y)) = log(1/(1+exp(-x))) - log(1/(1+exp(-y)))\n    # = -log(1+exp(-x)) - (-log(1+exp(-y)))\n    # = log(1+exp(-y)) - log(1+exp(-x))\n    # Using softplus(z) = log(1+exp(z)), this is equivalent to:\n    # softplus(-y) - softplus(-x) = y - softplus(y) - (x - softplus(x))\n    # where x=logp_w, y=logp_l. But this simplifies to logp_diff - (softplus(logp_w) - softplus(logp_l))\n    log_ratio_objective = logp_diff - (F.softplus(log_prob_w) - F.softplus(log_prob_l))\n\n    # 6. (New Coupling) Stabilize the objective by applying softplus\n    stabilized_objective = F.softplus(log_ratio_objective)\n\n    # 7. Combine components into the final loss argument\n    # We want stabilized_objective > margin.\n    argument = stabilized_objective - additive_margin\n\n    # 8. Compute the final loss using numerically stable logsigmoid\n    loss_per_item = -F.logsigmoid(argument)\n\n    # 9. Apply optional weights and compute the mean loss\n    if weights is not None:\n        loss = (loss_per_item * weights).sum() / weights.sum().clamp(min=1e-8)\n    else:\n        loss = loss_per_item.mean()\n\n    return loss"}, "static_ok": true, "static_reason": "", "dynamic_ok": true, "dynamic_reason": "ok", "loss_value": 0.7713828682899475, "grad_norm": 0.0}
{"generation": 5, "index": 0, "ir": {"name": "Adaptive Rank-Gated Loss", "intuition": "This loss function creates a dynamic learning signal by gating the influence of the log-probability difference based on the relative importance of the preference pair. It is designed to be robust to outliers and focus learning on meaningful distinctions.\n\nInherited Ideas:\n- From `Soft-Clipped Rank-Modulated Loss` (Parent 0), it inherits the idea of using a **unified modulator derived from a rank-based signal**. This modulator, based on `rank_gap(cost_diff)`, ensures the loss is robust to the scale and distribution of costs, focusing on the relative importance of each pair within a batch.\n- From `Rank-Gapped Adaptive Sigmoid Loss` (Parent 1), it inherits the structure of an **additive margin**. The loss requires the log-probability difference to exceed a margin that grows with the significance of the cost difference, creating a more demanding target for more important pairs.\n\nNew Coupling Ideas & Modifications:\n1.  **Sigmoid Gating Mechanism**: A new coupling is introduced where the rank-based modulator is passed through a `sigmoid` function to create a 'gate' value between 0 and 1. This gate directly multiplies the `logp_diff`. This mechanism smoothly scales down the learning signal for pairs with small cost differences (low rank) and scales it up for pairs with large cost differences (high rank), effectively focusing the model's attention.\n2.  **Coupled Gate and Margin**: The same rank-based signal is used to derive both the `gate` (via `sigmoid`) and the `margin` (via `softplus`). This ensures a consistent relationship: as the cost difference becomes more significant, both the gate (allowing more gradient to flow) and the margin (the target to overcome) increase in a coupled manner. This prevents the margin from growing excessively for pairs where the gate is small, creating a more balanced and stable learning objective.", "pseudocode": "1. Calculate the difference in log probabilities: `logp_diff = logp_winner - logp_loser`.\n2. Calculate the absolute difference in costs: `cost_diff = abs(cost_winner - cost_loser)`.\n3. (Inherited from Parent 0) Convert the cost differences into a normalized rank signal between -0.5 and 0.5 using the `rank_gap` operator. This makes the signal robust to cost scale and outliers.\n4. (New Coupling) Create a 'gate' by applying a `sigmoid` function to the scaled rank signal: `gate = sigmoid(beta * rank_gap_signal)`. This produces a value between 0 and 1 that modulates the influence of `logp_diff`.\n5. (Inherited from Parent 1 & New Coupling) Create an additive margin using the same rank signal, but passed through a `softplus` function to ensure it is smooth and non-negative: `margin = softplus(alpha * rank_gap_signal)`. The margin and gate are thus coupled through the same underlying rank signal.\n6. Combine the components. The objective is for the gated log-probability difference to exceed the margin: `gate * logp_diff > margin`.\n7. Compute the final loss using a numerically stable logistic loss: `loss = -logsigmoid(gate * logp_diff - margin)`.\n8. Return the weighted mean of the loss over the batch.", "hyperparams": {"alpha": 2.0, "beta": 3.0}, "operators_used": ["logsigmoid", "softplus", "sigmoid", "rank_gap"], "implementation_hint": {"expects": ["cost_a", "cost_b", "logp_a", "logp_b"], "returns": "scalar"}, "code": "import torch\nimport torch.nn.functional as F\n\ndef generated_loss(batch, model_output, extra):\n    \"\"\"\n    Computes the Adaptive Rank-Gated Loss.\n\n    This loss uses a rank-based signal to create both a sigmoid gate that modulates\n    the log-probability difference and a softplus-derived additive margin.\n\n    Args:\n        batch (dict): A dictionary containing tensors.\n            'cost_a' (Tensor [N]): Cost of solution 'a'.\n            'cost_b' (Tensor [N]): Cost of solution 'b'.\n            'log_prob_w' (Tensor [N]): Log probability of the winning solution.\n            'log_prob_l' (Tensor [N]): Log probability of the losing solution.\n            'weight' (Tensor [N], optional): Per-pair loss weights.\n        model_output (dict): Model outputs (not used).\n        extra (dict): A dictionary for hyperparameters.\n            'alpha' (float): Controls the magnitude of the additive margin.\n            'beta' (float): Controls the steepness of the sigmoid gate.\n\n    Returns:\n        torch.Tensor: A scalar tensor representing the mean loss.\n    \"\"\"\n\n    # Whitelisted operator implemented as a helper function\n    def rank_gap(x, eps=1e-9):\n        \"\"\"Converts a tensor to its normalized ranks within the range [-0.5, 0.5].\"\"\"\n        if x.numel() <= 1:\n            return torch.zeros_like(x)\n        ranks = x.to(torch.float32).argsort().argsort().to(x.dtype)\n        # Normalize ranks to [0, 1]\n        normalized_ranks = ranks / (x.numel() - 1 + eps)\n        # Shift to [-0.5, 0.5]\n        return normalized_ranks - 0.5\n\n    # Retrieve hyperparameters\n    alpha = extra.get('alpha', 2.0)\n    beta = extra.get('beta', 3.0)\n\n    # Unpack tensors from the batch\n    cost_a = batch['cost_a']\n    cost_b = batch['cost_b']\n    log_prob_w = batch['log_prob_w']\n    log_prob_l = batch['log_prob_l']\n    weights = batch.get('weight', None)\n\n    # 1. Calculate the difference in log probabilities\n    logp_diff = log_prob_w - log_prob_l\n\n    # 2. Calculate absolute cost difference\n    cost_diff = torch.abs(cost_a - cost_b)\n\n    # 3. (Inherited from Parent 0) Use rank_gap for a stable, scale-invariant cost signal\n    rank_gap_signal = rank_gap(cost_diff)\n\n    # 4. (New Coupling) Create a sigmoid gate to modulate the logp_diff\n    # This gate is close to 0.5 for median-rank pairs and approaches 1 for high-rank pairs.\n    gate = torch.sigmoid(beta * rank_gap_signal)\n\n    # 5. (Inherited from Parent 1 & New Coupling) Create an additive margin from the same rank signal\n    # softplus ensures the margin is non-negative and smooth.\n    margin = F.softplus(alpha * rank_gap_signal)\n\n    # 6. Combine the components into the final loss argument\n    # We want gate * logp_diff > margin\n    argument = gate * logp_diff - margin\n\n    # 7. Compute the final loss using numerically stable logsigmoid\n    loss_per_item = -F.logsigmoid(argument)\n\n    # 8. Apply optional weights and compute the mean loss\n    if weights is not None:\n        loss = (loss_per_item * weights).sum() / weights.sum().clamp(min=1e-8)\n    else:\n        loss = loss_per_item.mean()\n\n    return loss"}, "static_ok": true, "static_reason": "", "dynamic_ok": true, "dynamic_reason": "ok", "loss_value": 1.1398158073425293, "grad_norm": 0.0}
{"generation": 5, "index": 1, "ir": {"name": "Rank-Modulated Dual-Clipping Loss", "intuition": "This loss function is designed for stable and adaptive preference learning by combining robust clipping mechanisms with a rank-based modulation signal. The core idea is to ensure the log-probability difference not only moves in the correct direction but also respects a margin that adapts to the relative importance of the preference pair.\n\nInherited Ideas:\n- From `Soft-Clipped Rank-Modulated Loss` (Parent 0), it inherits the idea of **soft-clipping the log-probability difference** using `tanh(logp_diff)`. This bounds the logp difference to `[-1, 1]`, preventing extreme gradients and focusing the learning signal on getting the preference direction correct.\n- From `Rank-Gapped Adaptive Sigmoid Loss` (Parent 1), it inherits the concept of using a **cost-derived additive margin**. The loss requires the model's preference score to exceed this margin, which is computed using `softplus` on a rank-based signal, ensuring it is smooth, non-negative, and grows with the cost difference's importance.\n\nNew Coupling Ideas & Modifications:\n1. **Unified Rank-Based Modulation Signal**: Both parents use a `rank_gap` signal. This child unifies this concept by creating a single, robust `rank_modulator` from the cost difference: `rank_modulator = softplus(alpha * rank_gap(cost_diff))`. This modulator serves as the basis for both the margin and a new clipping mechanism, ensuring that all adaptive components are derived from a single, scale-invariant source.\n2. **Dynamic Log-Probability Clipping**: Instead of using the raw `tanh(logp_diff)`, this loss introduces a new coupling where the `rank_modulator` itself defines the clipping bounds. The log-probability difference is clamped to `[-rank_modulator, rank_modulator]`. This means that for pairs with a small cost difference (low rank), the model is only required to produce a small logp difference, preventing it from wasting capacity on insignificant pairs. For important pairs (high rank), the allowed logp difference is larger, enabling a stronger learning signal. This is a form of dynamic regularization on the log-probabilities themselves.\n3. **Decoupled Margin and Clipping**: The final loss argument is `clipped_logp_diff - margin`. Here, `clipped_logp_diff` is the dynamically clamped `logp_diff`, and `margin` is the `rank_modulator` itself. This structure forces the clamped log probability difference to be greater than the margin, which is also the value used for clamping. To achieve low loss on high-rank pairs, the model must push `logp_diff` to the upper bound of its dynamic clamp.", "pseudocode": "1. Calculate the difference in log probabilities: `logp_diff = logp_winner - logp_loser`.\n2. Calculate the absolute difference in costs: `cost_diff = abs(cost_winner - cost_loser)`.\n3. (New Coupling) Convert the cost differences into a normalized rank signal between -0.5 and 0.5 using the `rank_gap` operator. This provides a stable, scale-invariant signal of preference importance.\n4. (New Coupling) Create a single, unified `rank_modulator` from the rank signal: `rank_modulator = softplus(alpha * rank_gap_signal)`. This creates a smooth, non-negative value that increases with the rank of the cost difference.\n5. (Inherited from Parent 0 & New Coupling) Dynamically clip the log probability difference. Instead of a fixed `tanh`, use the `rank_modulator` to define the clipping range: `clipped_logp_diff = clamp(logp_diff, min=-rank_modulator, max=rank_modulator)`. This focuses the model's effort proportionally to the importance of the pair.\n6. (Inherited from Parent 1) Define the additive margin to be the `rank_modulator` itself. This sets a target for the `clipped_logp_diff` to overcome.\n7. Combine the components into the loss argument: `argument = clipped_logp_diff - rank_modulator`. The goal is to push the (now dynamically clipped) logp difference beyond the margin.\n8. Compute the final loss using a numerically stable logistic loss function: `loss = -logsigmoid(argument)`.\n9. Return the weighted mean of the loss over the batch.", "hyperparams": {"alpha": 4.0}, "operators_used": ["logsigmoid", "softplus", "clamp", "rank_gap"], "implementation_hint": {"expects": ["cost_a", "cost_b", "logp_w", "logp_l", "weight"], "returns": "scalar"}, "code": "import torch\nimport torch.nn.functional as F\n\ndef generated_loss(batch, model_output, extra):\n    \"\"\"\n    Computes the Rank-Modulated Dual-Clipping Loss.\n\n    This loss uses a rank-based modulator to dynamically clip the log-probability\n    difference and to set an adaptive additive margin.\n\n    Args:\n        batch (dict): A dictionary containing tensors.\n            'cost_a' (Tensor [N]): Cost of solution 'a'.\n            'cost_b' (Tensor [N]): Cost of solution 'b'.\n            'log_prob_w' (Tensor [N]): Log probability of the winning solution.\n            'log_prob_l' (Tensor [N]): Log probability of the losing solution.\n            'weight' (Tensor [N], optional): Per-pair loss weights.\n        model_output (dict): Model outputs (not used).\n        extra (dict): A dictionary for hyperparameters.\n            'alpha' (float): Controls the magnitude of the rank modulator and margin.\n\n    Returns:\n        torch.Tensor: A scalar tensor representing the mean loss.\n    \"\"\"\n\n    # Whitelisted operator implemented as a helper function\n    def rank_gap(x, eps=1e-9):\n        \"\"\"Converts a tensor to its normalized ranks within the range [-0.5, 0.5].\"\"\"\n        if x.numel() <= 1:\n            return torch.zeros_like(x)\n        ranks = x.to(torch.float32).argsort().argsort().to(x.dtype)\n        normalized_ranks = ranks / (x.numel() - 1 + eps)\n        return normalized_ranks - 0.5\n\n    # Retrieve hyperparameters\n    alpha = extra.get('alpha', 4.0)\n\n    # Unpack tensors from the batch\n    cost_a = batch['cost_a']\n    cost_b = batch['cost_b']\n    log_prob_w = batch['log_prob_w']\n    log_prob_l = batch['log_prob_l']\n    weights = batch.get('weight', None)\n\n    # 1. Calculate the difference in log probabilities\n    logp_diff = log_prob_w - log_prob_l\n\n    # 2. Calculate absolute cost difference\n    cost_diff = torch.abs(cost_a - cost_b)\n\n    # 3. (New Coupling) Use rank_gap for a stable, scale-invariant cost signal\n    rank_gap_signal = rank_gap(cost_diff)\n\n    # 4. (New Coupling) Create a unified rank modulator\n    rank_modulator = F.softplus(alpha * rank_gap_signal)\n\n    # 5. (Inherited from Parent 0 & New Coupling) Dynamically clip logp_diff\n    # The clipping range [-M, M] is determined by the rank_modulator M.\n    clipped_logp_diff = torch.clamp(logp_diff, min=-rank_modulator, max=rank_modulator)\n\n    # 6. (Inherited from Parent 1) The margin is the rank_modulator itself\n    additive_margin = rank_modulator\n\n    # 7. Combine components. We want clipped_logp_diff > additive_margin.\n    argument = clipped_logp_diff - additive_margin\n\n    # 8. Compute the final loss using numerically stable logsigmoid\n    loss_per_item = -F.logsigmoid(argument)\n\n    # 9. Apply optional weights and compute the mean loss\n    if weights is not None:\n        loss = (loss_per_item * weights).sum() / weights.sum().clamp(min=1e-8)\n    else:\n        loss = loss_per_item.mean()\n\n    return loss"}, "static_ok": true, "static_reason": "", "dynamic_ok": true, "dynamic_reason": "ok", "loss_value": 1.2550013065338135, "grad_norm": 0.0}
{"generation": 5, "index": 2, "ir": {"name": "Rank-Gated Log-Ratio Loss", "intuition": "This loss function introduces a novel gating mechanism based on the relative rank of cost differences to modulate the learning signal. It aims to focus training on meaningful preference pairs while maintaining stability.\n\nInherited Ideas:\n- From Parent 0 (`Soft-Clipped Rank-Modulated Loss`): It inherits the use of `rank_gap` on the cost difference to create a scale-invariant, robust signal representing the relative importance of a preference pair within a batch. This makes the loss insensitive to the absolute scale of costs and robust to outliers.\n- From Parent 1 (`Rank-Gapped Adaptive Sigmoid Loss`): It inherits the core structure of using an additive, cost-dependent margin. The loss aims to ensure the log-probability difference surpasses this margin, which is computed via `softplus` on the rank-gapped cost signal.\n\nNew Coupling Ideas & Modifications:\n1. **Rank-Based Gating**: A new gating mechanism is introduced. The `rank_gap_signal` is passed through a `sigmoid` function, creating a 'gate' value between 0 and 1. This gate acts as a soft switch: for pairs with a low-ranked cost difference (gate ≈ 0.5), the loss is attenuated, effectively ignoring them. For high-ranked pairs (gate ≈ 1), the loss is fully applied. This focuses the model's capacity on learning from significant preferences.\n2. **Log-Ratio Formulation**: Instead of using the log-probability difference `logp_w - logp_l`, this loss uses the log of the probability ratio, `log(p_w / p_l)`, which is equivalent. The final loss term is `gate * (margin - logp_diff)`. This is then passed through `softplus` to compute the loss. This `softplus(gate * (margin - logp_diff))` structure is a stable alternative to the common `logsigmoid` formulation and ensures the loss is always non-negative. It penalizes cases where `logp_diff` is less than the `margin`, and this penalty is scaled by the `gate`.", "pseudocode": "1. Calculate the difference in log probabilities: `logp_diff = logp_winner - logp_loser`.\n2. Calculate the absolute difference in costs: `cost_diff = abs(cost_winner - cost_loser)`.\n3. (Inherited from Parent 0) Convert cost differences into a normalized rank signal between -0.5 and 0.5 using the `rank_gap` operator. This `rank_gap_signal` is robust to cost scale and outliers.\n4. (Inherited from Parent 1) Create an additive margin based on the rank signal. Apply `softplus` to the scaled rank signal: `margin = softplus(alpha * rank_gap_signal)`.\n5. (New Coupling) Create a 'gate' by applying a `sigmoid` function to the scaled rank signal: `gate = sigmoid(beta * rank_gap_signal)`. This gate value is close to 1 for high-rank pairs and close to 0.5 for low-rank pairs, effectively down-weighting less important comparisons.\n6. (New Coupling) Formulate the loss argument by combining the gate, margin, and log-probability difference: `argument = gate * (margin - logp_diff)`. The gate modulates the extent to which the model is penalized for not meeting the margin requirement.\n7. Compute the final per-item loss using `softplus` for stability and to ensure a non-negative loss value: `loss = softplus(argument)`.\n8. Return the weighted mean of the loss over the batch.", "hyperparams": {"alpha": 2.0, "beta": 5.0}, "operators_used": ["softplus", "sigmoid", "rank_gap", "log"], "implementation_hint": {"expects": ["cost_a", "cost_b", "log_prob_w", "log_prob_l", "weight"], "returns": "scalar"}, "code": "import torch\nimport torch.nn.functional as F\n\ndef generated_loss(batch, model_output, extra):\n    \"\"\"\n    Computes the Rank-Gated Log-Ratio Loss.\n\n    This loss uses a rank-based sigmoid gate to modulate the penalty for failing to meet\n    a rank-derived margin. It focuses learning on pairs with significant cost differences.\n\n    Args:\n        batch (dict): A dictionary containing tensors.\n            'cost_a' (Tensor [N]): Cost of solution 'a'.\n            'cost_b' (Tensor [N]): Cost of solution 'b'.\n            'log_prob_w' (Tensor [N]): Log probability of the winning solution.\n            'log_prob_l' (Tensor [N]): Log probability of the losing solution.\n            'weight' (Tensor [N], optional): Per-pair loss weights.\n        model_output (dict): Model outputs (not used).\n        extra (dict): A dictionary for hyperparameters.\n            'alpha' (float): Controls the magnitude of the adaptive margin.\n            'beta' (float): Controls the steepness of the rank-based gate.\n\n    Returns:\n        torch.Tensor: A scalar tensor representing the mean loss.\n    \"\"\"\n\n    # Whitelisted operator implemented as a helper function\n    def rank_gap(x, eps=1e-9):\n        \"\"\"Converts a tensor to its normalized ranks within the range [-0.5, 0.5].\"\"\"\n        if x.numel() <= 1:\n            return torch.zeros_like(x)\n        ranks = x.to(torch.float32).argsort().argsort().to(x.dtype)\n        normalized_ranks = ranks / (x.numel() - 1 + eps)\n        return normalized_ranks - 0.5\n\n    # Retrieve hyperparameters\n    alpha = extra.get('alpha', 2.0)\n    beta = extra.get('beta', 5.0)\n\n    # Unpack tensors from the batch\n    cost_a = batch['cost_a']\n    cost_b = batch['cost_b']\n    log_prob_w = batch['log_prob_w']\n    log_prob_l = batch['log_prob_l']\n    weights = batch.get('weight', None)\n\n    # 1. Calculate the difference in log probabilities (log of probability ratio)\n    logp_diff = log_prob_w - log_prob_l\n\n    # 2. Calculate absolute cost difference\n    cost_diff = torch.abs(cost_a - cost_b)\n\n    # 3. (Inherited from Parent 0) Use rank_gap for a stable, scale-invariant cost signal\n    rank_gap_signal = rank_gap(cost_diff)\n\n    # 4. (Inherited from Parent 1) Create an adaptive additive margin\n    margin = F.softplus(alpha * rank_gap_signal)\n\n    # 5. (New Coupling) Create a rank-based gate to modulate the loss\n    # The gate is close to 1 for high-rank pairs and ~0.5 for median-rank pairs.\n    gate = torch.sigmoid(beta * rank_gap_signal)\n\n    # 6. (New Coupling) Formulate the loss argument\n    # The loss is gate * (margin - logp_diff), penalizing when logp_diff < margin.\n    # The gate scales this penalty based on the rank of the cost difference.\n    argument = gate * (margin - logp_diff)\n\n    # 7. Compute the final loss using softplus for stability and non-negativity\n    loss_per_item = F.softplus(argument)\n\n    # 8. Apply optional weights and compute the mean loss\n    if weights is not None:\n        # Ensure weights sum is not zero to avoid division by zero\n        weights_sum = weights.sum().clamp(min=1e-8)\n        loss = (loss_per_item * weights).sum() / weights_sum\n    else:\n        loss = loss_per_item.mean()\n\n    return loss"}, "static_ok": true, "static_reason": "", "dynamic_ok": true, "dynamic_reason": "ok", "loss_value": 0.9667448997497559, "grad_norm": 0.0}
{"generation": 5, "index": 3, "ir": {"name": "Adaptive Rank-Modulated Bradley-Terry Loss", "intuition": "This loss function refines the standard Bradley-Terry model by introducing a dynamic, rank-based modulator that serves as both a multiplicative scaler and an additive margin. The goal is to create a stable and adaptive learning signal that is sensitive to the relative importance of preference pairs within a batch.\n\nInherited Ideas:\n- From `Soft-Clipped Rank-Modulated Loss`: It inherits the concept of using a single, unified `modulator` derived from cost information that acts as both a multiplicative scaler and an additive margin. This elegant coupling simplifies the loss structure while creating a strong learning signal.\n- From `Rank-Gapped Adaptive Sigmoid Loss`: It inherits the core principle of deriving a learning signal from the `rank_gap` of cost differences. This makes the loss invariant to the absolute scale of costs and robust to outliers, focusing on the relative ranking of preference strengths within a batch.\n\nNew Coupling Ideas & Modifications:\n1. **Bradley-Terry Core with Rank-Based Scaling**: Instead of directly using `logp_diff`, this loss uses `log(sigmoid(logp_diff))`, which is the log-probability of correctly classifying the winner according to the Bradley-Terry model. The inherited `rank_modulator` is then used to scale this log-probability. This couples the rank-based importance of a pair with the model's confidence, effectively saying: \"For important pairs (high rank), you must be very confident in your correct prediction.\"\n2. **Dynamic Margin via `softplus`**: The `rank_modulator` also functions as a dynamic, additive margin. The loss aims to make `modulator * log(sigmoid(logp_diff))` greater than `modulator`. This structure is then rearranged for numerical stability into `modulator * (log(sigmoid(logp_diff)) - 1)`. Since `log(sigmoid(x))` is always negative, this entire argument is always negative, and the loss is minimized as `logp_diff` becomes very large, pushing `log(sigmoid(logp_diff))` towards 0. The `softplus` ensures the modulator is smooth and non-negative.", "pseudocode": "1. Calculate the difference in log probabilities: `logp_diff = logp_winner - logp_loser`.\n2. Calculate the absolute difference in costs: `cost_diff = abs(cost_winner - cost_loser)`.\n3. (Inherited from `Rank-Gapped Adaptive Sigmoid Loss`) Convert cost differences into a stable, normalized rank signal between -0.5 and 0.5 using the `rank_gap` operator. This makes the signal robust to cost scale and outliers.\n4. (Inherited from `Soft-Clipped Rank-Modulated Loss`) Create a single, unified `rank_modulator` by applying `softplus` to the scaled rank signal: `rank_modulator = softplus(alpha * rank_gap_signal)`. This results in a smooth, non-negative value that increases with the rank of the cost difference.\n5. (New Coupling Idea 1) Calculate the log-probability of the winner's preference according to the Bradley-Terry model using `logsigmoid`: `bt_logp = logsigmoid(logp_diff)`.\n6. (New Coupling Idea 2) Combine the components. Use the `rank_modulator` as both a multiplicative scaler on the Bradley-Terry log-probability and as an additive margin. The target is `rank_modulator * bt_logp > rank_modulator`, which is rearranged to `rank_modulator * (bt_logp - 1)`.\n7. Compute the final loss using a numerically stable logistic loss on the combined argument: `loss = -logsigmoid(rank_modulator * (bt_logp - 1))`. Since the argument is always negative, this encourages it to move towards zero.\n8. Return the weighted mean of the loss over the batch.", "hyperparams": {"alpha": 4.0}, "operators_used": ["logsigmoid", "softplus", "rank_gap"], "implementation_hint": {"expects": ["cost_a", "cost_b", "log_prob_w", "log_prob_l", "weight"], "returns": "scalar"}, "code": "import torch\nimport torch.nn.functional as F\n\ndef generated_loss(batch, model_output, extra):\n    \"\"\"\n    Computes the Adaptive Rank-Modulated Bradley-Terry Loss.\n\n    This loss scales the log-probability from the Bradley-Terry model by a unified,\n    rank-based modulator that also serves as an additive margin.\n\n    Args:\n        batch (dict): A dictionary containing tensors.\n            'cost_a' (Tensor [N]): Cost of solution 'a'.\n            'cost_b' (Tensor [N]): Cost of solution 'b'.\n            'log_prob_w' (Tensor [N]): Log probability of the winning solution.\n            'log_prob_l' (Tensor [N]): Log probability of the losing solution.\n            'weight' (Tensor [N], optional): Per-pair loss weights.\n        model_output (dict): Model outputs (not used).\n        extra (dict): A dictionary for hyperparameters.\n            'alpha' (float): Controls the magnitude of the rank modulator.\n\n    Returns:\n        torch.Tensor: A scalar tensor representing the mean loss.\n    \"\"\"\n\n    # Whitelisted operator implemented as a helper function\n    def rank_gap(x, eps=1e-9):\n        \"\"\"Converts a tensor to its normalized ranks within the range [-0.5, 0.5].\"\"\"\n        if x.numel() <= 1:\n            return torch.zeros_like(x)\n        ranks = x.to(torch.float32).argsort().argsort().to(x.dtype)\n        # Normalize ranks to [0, 1]\n        normalized_ranks = ranks / (x.numel() - 1 + eps)\n        # Shift to [-0.5, 0.5]\n        return normalized_ranks - 0.5\n\n    # Retrieve hyperparameters\n    alpha = extra.get('alpha', 4.0)\n\n    # Unpack tensors from the batch\n    cost_a = batch['cost_a']\n    cost_b = batch['cost_b']\n    log_prob_w = batch['log_prob_w']\n    log_prob_l = batch['log_prob_l']\n    weights = batch.get('weight', None)\n\n    # 1. Calculate the difference in log probabilities\n    logp_diff = log_prob_w - log_prob_l\n\n    # 2. Calculate absolute cost difference\n    cost_diff = torch.abs(cost_a - cost_b)\n\n    # 3. (Inherited) Use rank_gap for a stable, scale-invariant cost signal\n    rank_gap_signal = rank_gap(cost_diff)\n\n    # 4. (Inherited) Create a unified rank modulator using softplus for a smooth, non-negative value\n    rank_modulator = F.softplus(alpha * rank_gap_signal)\n\n    # 5. (New Coupling) Calculate the Bradley-Terry log-probability of the winner\n    # This is log(sigmoid(logp_diff)), which is always negative.\n    bt_logp = F.logsigmoid(logp_diff)\n\n    # 6. (New Coupling) Combine components. The modulator scales the BT log-probability\n    # and also acts as the margin. The target is modulator * bt_logp > modulator,\n    # which we rearrange to modulator * (bt_logp - 1) for the loss argument.\n    # Since bt_logp is in (-inf, 0], (bt_logp - 1) is in (-inf, -1].\n    # The full argument is therefore always negative.\n    argument = rank_modulator * (bt_logp - 1)\n\n    # 7. Compute the final loss using numerically stable logsigmoid\n    loss_per_item = -F.logsigmoid(argument)\n\n    # 8. Apply optional weights and compute the mean loss\n    if weights is not None and weights.sum() > 0:\n        loss = (loss_per_item * weights).sum() / weights.sum()\n    else:\n        loss = loss_per_item.mean()\n\n    return loss"}, "static_ok": true, "static_reason": "", "dynamic_ok": true, "dynamic_reason": "ok", "loss_value": 1.750042200088501, "grad_norm": 0.0}
{"generation": 5, "index": 4, "ir": {"name": "Rank-Gapped Log-Ratio Loss with Adaptive Margin", "intuition": "This loss function creates an adaptive learning signal by combining a stable log-ratio of probabilities with a dynamic, rank-based margin. The goal is to ensure the ratio of winner to loser probabilities exceeds a certain threshold, where that threshold increases with the relative importance of the preference pair.\n\nInherited Ideas:\n- From Parent 0 (`Soft-Clipped Rank-Modulated Loss`): It inherits the idea of using a **unified, rank-based modulator** derived from the cost difference. This modulator, based on `rank_gap`, is robust to cost scale and outliers. It is used to create a dynamic margin that adapts to the significance of each preference pair within a batch.\n- From Parent 1 (`Rank-Gapped Adaptive Sigmoid Loss`): It inherits the concept of an **additive, cost-dependent margin**. The loss is structured to push a probability-based score (`score = logp_winner - logp_loser`) to be greater than a margin (`score > margin`).\n\nNew Coupling Ideas & Modifications:\n1. **Log-Ratio Scoring**: Instead of using the raw `logp_diff` (`logp_w - logp_l`), this loss uses a `log(sigmoid(logp_diff))` term. This transforms the unbounded `logp_diff` into a bounded log-probability score in the range `(-inf, 0)`. This score represents the log of the Bradley-Terry probability `P(w > l)`. This focuses the loss on the probability ratio `p_w / (p_w + p_l)` rather than the raw log-probability difference, which can be more stable.\n2. **Dynamic Margin Coupling**: The rank-based modulator, inherited from Parent 0, is transformed into a dynamic margin `m_rank`. This margin is then coupled with the log-ratio score. To make the target achievable, the margin is also transformed into the same log-space. The final loss aims to satisfy `log(sigmoid(logp_diff)) > log(sigmoid(m_rank))`. This is equivalent to `sigmoid(logp_diff) > sigmoid(m_rank)`, creating an elegant and stable comparison in probability space.", "pseudocode": "1. Calculate the difference in log probabilities: `logp_diff = logp_winner - logp_loser`.\n2. (New Coupling) Convert the log-probability difference into a log-ratio score using `log(sigmoid(logp_diff))`. This maps the score to `(-inf, 0)`.\n3. Calculate the absolute difference in costs: `cost_diff = abs(cost_winner - cost_loser)`.\n4. (Inherited from Parent 0 & 1) Convert cost differences into a stable, normalized rank signal between -0.5 and 0.5 using the `rank_gap` operator.\n5. (Inherited from Parent 0) Create a rank-based modulator by applying a scaled `softplus` to the rank signal: `rank_modulator = softplus(alpha * rank_gap_signal)`. This creates a smooth, non-negative value that increases with the cost difference's rank.\n6. (New Coupling) Transform the rank modulator into a target margin in the same log-space as the score. The target becomes `log_target_margin = log(sigmoid(rank_modulator))`. This ensures the comparison is between two values in the same domain.\n7. (Inherited from Parent 1) Formulate the loss argument as the difference between the score and the target margin: `argument = log_ratio_score - log_target_margin`. The goal is to make this argument positive.\n8. Compute the final loss using a numerically stable logistic loss function: `loss = -logsigmoid(argument)`.\n9. Return the weighted mean of the loss over the batch.", "hyperparams": {"alpha": 4.0}, "operators_used": ["logsigmoid", "sigmoid", "log", "softplus", "rank_gap"], "implementation_hint": {"expects": ["cost_a", "cost_b", "logp_w", "logp_l", "weight"], "returns": "scalar"}, "code": "import torch\nimport torch.nn.functional as F\n\ndef generated_loss(batch, model_output, extra):\n    \"\"\"\n    Computes the Rank-Gapped Log-Ratio Loss with Adaptive Margin.\n\n    This loss compares the log-sigmoid of the logp_diff against a dynamic margin\n    derived from the rank-normalized cost difference.\n\n    Args:\n        batch (dict): A dictionary containing tensors.\n            'cost_a' (Tensor [N]): Cost of solution 'a'.\n            'cost_b' (Tensor [N]): Cost of solution 'b'.\n            'log_prob_w' (Tensor [N]): Log probability of the winning solution.\n            'log_prob_l' (Tensor [N]): Log probability of the losing solution.\n            'weight' (Tensor [N], optional): Per-pair loss weights.\n        model_output (dict): Model outputs (not used).\n        extra (dict): A dictionary for hyperparameters.\n            'alpha' (float): Controls the magnitude of the rank-based margin.\n\n    Returns:\n        torch.Tensor: A scalar tensor representing the mean loss.\n    \"\"\"\n\n    # Whitelisted operator implemented as a helper function\n    def rank_gap(x, eps=1e-9):\n        \"\"\"Converts a tensor to its normalized ranks within the range [-0.5, 0.5].\"\"\"\n        if x.numel() <= 1:\n            return torch.zeros_like(x)\n        ranks = x.to(torch.float32).argsort().argsort().to(x.dtype)\n        normalized_ranks = ranks / (x.numel() - 1 + eps)\n        return normalized_ranks - 0.5\n\n    # Retrieve hyperparameters\n    alpha = extra.get('alpha', 4.0)\n    eps = 1e-9\n\n    # Unpack tensors from the batch\n    cost_a = batch['cost_a']\n    cost_b = batch['cost_b']\n    log_prob_w = batch['log_prob_w']\n    log_prob_l = batch['log_prob_l']\n    weights = batch.get('weight', None)\n\n    # 1. Calculate the difference in log probabilities\n    logp_diff = log_prob_w - log_prob_l\n\n    # 2. (New Coupling) Convert logp_diff into a log-ratio score\n    # This is equivalent to log(sigmoid(logp_diff))\n    log_ratio_score = F.logsigmoid(logp_diff)\n\n    # 3. Calculate absolute cost difference\n    cost_diff = torch.abs(cost_a - cost_b)\n\n    # 4. (Inherited from Parents) Use rank_gap for a stable, scale-invariant cost signal\n    rank_gap_signal = rank_gap(cost_diff)\n\n    # 5. (Inherited from Parent 0) Create a rank-based modulator\n    rank_modulator = F.softplus(alpha * rank_gap_signal)\n\n    # 6. (New Coupling) Transform the modulator into a target margin in the same log-space\n    # This is log(sigmoid(rank_modulator))\n    log_target_margin = F.logsigmoid(rank_modulator)\n\n    # 7. (Inherited from Parent 1) Formulate the loss argument as the difference\n    # We want log_ratio_score > log_target_margin\n    argument = log_ratio_score - log_target_margin\n\n    # 8. Compute the final loss using numerically stable logsigmoid\n    loss_per_item = -F.logsigmoid(argument)\n\n    # 9. Apply optional weights and compute the mean loss\n    if weights is not None:\n        loss = (loss_per_item * weights).sum() / weights.sum().clamp(min=1e-8)\n    else:\n        loss = loss_per_item.mean()\n\n    return loss"}, "static_ok": true, "static_reason": "", "dynamic_ok": true, "dynamic_reason": "ok", "loss_value": 0.859451174736023, "grad_norm": 0.0}
{"generation": 5, "index": 5, "ir": {"name": "Soft-Clipped Rank-Modulated Margin Loss", "intuition": "This loss function creates a highly adaptive preference signal by combining the stability of a soft-clipped log-probability difference with a dual-purpose, rank-based modulator that acts as both a multiplicative scaler and an additive margin.\n\nInherited Ideas:\n- From `Soft-Clipped Rank-Modulated Loss`: It inherits the core idea of **soft-clipping the log-probability difference** using `tanh`. This bounds the `logp_diff` to `[-1, 1]`, preventing extreme gradients and focusing the model on establishing the correct preference order rather than maximizing the log-probability gap.\n- From `Rank-Gapped Adaptive Sigmoid Loss`: It inherits the structure of having a **separate additive margin and a multiplicative scaler**, both derived from cost information. This allows for independent control over the required log-probability gap (margin) and the penalty for misclassifying pairs (scaler).\n\nNew Coupling Ideas & Modifications:\n1. **Shared Rank-Based Modulator**: Both the scaler and the margin are derived from the same underlying `rank_modulator`. This modulator is calculated as `softplus(alpha * rank_gap_signal)`, creating a smooth, non-negative signal based on the relative importance of the preference pair within the batch. This ensures that pairs with a larger cost difference have both a higher required margin and a stronger gradient signal.\n2. **Independent Control via Hyperparameters**: While both the scaler and margin originate from the same modulator, their magnitudes are controlled by separate hyperparameters, `beta` and `gamma`, respectively. The final loss argument is `(beta * rank_modulator) * tanh(logp_diff) - (gamma * rank_modulator)`. This coupling provides a robust, rank-based foundation while allowing fine-grained control over the relative influence of scaling versus the margin.", "pseudocode": "1. Calculate the difference in log probabilities: `logp_diff = logp_winner - logp_loser`.\n2. (Inherited from `Soft-Clipped Rank-Modulated Loss`) Soft-clip the log-probability difference using `tanh` for stability: `clipped_logp_diff = tanh(logp_diff)`.\n3. Calculate the absolute difference in costs: `cost_diff = abs(cost_winner - cost_loser)`.\n4. (New Coupling) Convert the cost differences into a normalized rank signal between -0.5 and 0.5 using the `rank_gap` operator. This makes the signal robust to cost scale and outliers.\n5. (New Coupling) Create a single, shared `rank_modulator` by applying `softplus` to the scaled rank signal: `rank_modulator = softplus(alpha * rank_gap_signal)`. This results in a smooth, non-negative value that increases with the rank of the cost difference.\n6. (Inherited from `Rank-Gapped Adaptive Sigmoid Loss`) Create a multiplicative scaler and an additive margin from the shared modulator, controlled by separate hyperparameters: `scaler = beta * rank_modulator` and `margin = gamma * rank_modulator`.\n7. Combine the components into the final loss argument: `argument = scaler * clipped_logp_diff - margin`. The goal is to make this value positive.\n8. Compute the final loss using a numerically stable logistic loss: `loss = -logsigmoid(argument)`.\n9. Return the weighted mean of the loss over the batch.", "hyperparams": {"alpha": 4.0, "beta": 1.0, "gamma": 1.0}, "operators_used": ["logsigmoid", "softplus", "tanh", "rank_gap"], "implementation_hint": {"expects": ["cost_a", "cost_b", "log_prob_w", "log_prob_l", "weight"], "returns": "scalar"}, "code": "import torch\nimport torch.nn.functional as F\n\ndef generated_loss(batch, model_output, extra):\n    \"\"\"\n    Computes the Soft-Clipped Rank-Modulated Margin Loss.\n\n    This loss combines a tanh-clipped log-probability difference with both a\n    multiplicative scaler and an additive margin. Both are derived from a shared,\n    rank-based modulator for stability and adaptivity.\n\n    Args:\n        batch (dict): A dictionary containing tensors.\n            'cost_a' (Tensor [N]): Cost of solution 'a'.\n            'cost_b' (Tensor [N]): Cost of solution 'b'.\n            'log_prob_w' (Tensor [N]): Log probability of the winning solution.\n            'log_prob_l' (Tensor [N]): Log probability of the losing solution.\n            'weight' (Tensor [N], optional): Per-pair loss weights.\n        model_output (dict): Model outputs (not used).\n        extra (dict): A dictionary for hyperparameters.\n            'alpha' (float): Controls the sensitivity of the rank modulator.\n            'beta' (float): Controls the magnitude of the multiplicative scaler.\n            'gamma' (float): Controls the magnitude of the additive margin.\n\n    Returns:\n        torch.Tensor: A scalar tensor representing the mean loss.\n    \"\"\"\n\n    # Whitelisted operator implemented as a helper function\n    def rank_gap(x, eps=1e-9):\n        \"\"\"Converts a tensor to its normalized ranks within the range [-0.5, 0.5].\"\"\"\n        if x.numel() <= 1:\n            return torch.zeros_like(x)\n        ranks = x.to(torch.float32).argsort().argsort().to(x.dtype)\n        normalized_ranks = ranks / (x.numel() - 1 + eps)\n        return normalized_ranks - 0.5\n\n    # Retrieve hyperparameters\n    alpha = extra.get('alpha', 4.0)\n    beta = extra.get('beta', 1.0)\n    gamma = extra.get('gamma', 1.0)\n\n    # Unpack tensors from the batch\n    cost_a = batch['cost_a']\n    cost_b = batch['cost_b']\n    log_prob_w = batch['log_prob_w']\n    log_prob_l = batch['log_prob_l']\n    weights = batch.get('weight', None)\n\n    # 1. Calculate the difference in log probabilities\n    logp_diff = log_prob_w - log_prob_l\n\n    # 2. (Inherited) Soft-clip the log-probability difference for stability\n    clipped_logp_diff = torch.tanh(logp_diff)\n\n    # 3. Calculate absolute cost difference\n    cost_diff = torch.abs(cost_a - cost_b)\n\n    # 4. (New Coupling) Use rank_gap for a stable, scale-invariant cost signal\n    rank_gap_signal = rank_gap(cost_diff)\n\n    # 5. (New Coupling) Create a shared, smooth, non-negative rank modulator\n    rank_modulator = F.softplus(alpha * rank_gap_signal)\n\n    # 6. (Inherited & New Coupling) Create scaler and margin from the shared modulator\n    # This allows independent control over their relative strengths.\n    scaler = beta * rank_modulator\n    margin = gamma * rank_modulator\n\n    # 7. Combine components: we want scaler * clipped_logp_diff > margin\n    argument = scaler * clipped_logp_diff - margin\n\n    # 8. Compute the final loss using numerically stable logsigmoid\n    loss_per_item = -F.logsigmoid(argument)\n\n    # 9. Apply optional weights and compute the mean loss\n    if weights is not None:\n        loss = (loss_per_item * weights).sum() / weights.sum().clamp(min=1e-8)\n    else:\n        loss = loss_per_item.mean()\n\n    return loss"}, "static_ok": true, "static_reason": "", "dynamic_ok": true, "dynamic_reason": "ok", "loss_value": 1.2550013065338135, "grad_norm": 0.0}
{"generation": 5, "index": 6, "ir": {"name": "Adaptive Rank-Gapped Margin Loss", "intuition": "This loss function creates a stable and adaptive learning signal by combining a rank-based margin with a soft-clipped log-probability difference. The design focuses on numerical stability and robustness to cost scales by using rank-based normalization.\n\nInherited Ideas:\n- From `Soft-Clipped Rank-Modulated Loss` (Parent 0): It inherits the use of **soft-clipping the log-probability difference** using `tanh`. This bounds the `logp_diff` to the `[-1, 1]` range, preventing extreme gradients and focusing the model on learning the correct preference direction rather than an arbitrarily large log-probability gap.\n- From `Rank-Gapped Adaptive Sigmoid Loss` (Parent 1): It inherits the core structure of using an **additive, cost-dependent margin**. The loss aims to ensure that the `logp_diff` surpasses this margin. The margin is derived from the cost difference, making the learning signal stronger for pairs with a larger quality gap.\n\nNew Coupling Ideas & Modifications:\n1. **Unified Rank-Based Margin**: Instead of normalizing raw costs or having separate scaling and margin terms, this loss computes a single additive margin based on the rank of the cost difference within the batch. The `rank_gap` operator converts cost differences into a scale-invariant signal from [-0.5, 0.5]. This signal is then transformed by `softplus` to create a smooth, non-negative margin (`rank_margin = softplus(alpha * rank_gap_signal)`). This makes the margin robust to outliers and independent of the absolute cost scale.\n2. **Coupled Clipping and Margin**: The soft-clipped log-probability difference is directly compared against the rank-based margin. The loss argument is `tanh(logp_diff) - rank_margin`. This coupling forces the model to achieve a `tanh(logp_diff)` value that exceeds the margin. For pairs with a high-rank cost difference (large margin), `tanh(logp_diff)` must be pushed very close to its maximum of 1, creating a strong learning signal where it matters most, while being gentle on pairs with small cost differences.", "pseudocode": "1. Calculate the difference in log probabilities: `logp_diff = logp_winner - logp_loser`.\n2. (Inherited from Parent 0) Soft-clip the log-probability difference using `tanh` for stability: `clipped_logp_diff = tanh(logp_diff)`.\n3. Calculate the absolute difference in costs: `cost_diff = abs(cost_winner - cost_loser)`.\n4. (New Coupling) Convert the cost differences into a normalized rank signal between -0.5 and 0.5 using the `rank_gap` operator. This makes the signal robust to cost scale and outliers.\n5. (Inherited from Parent 1 & New Coupling) Create a single, adaptive, rank-based margin. Apply `softplus` to the scaled rank signal to ensure it is smooth and non-negative: `rank_margin = softplus(alpha * rank_gap_signal)`.\n6. Combine the components. The objective is for the clipped log-probability difference to exceed the rank-based margin. The loss argument is `clipped_logp_diff - rank_margin`.\n7. Compute the final loss using a numerically stable logistic loss: `loss = -logsigmoid(clipped_logp_diff - rank_margin)`.\n8. Return the weighted mean of the loss over the batch.", "hyperparams": {"alpha": 3.0}, "operators_used": ["logsigmoid", "softplus", "tanh", "rank_gap"], "implementation_hint": {"expects": ["cost_a", "cost_b", "log_prob_w", "log_prob_l", "weight"], "returns": "scalar"}, "code": "import torch\nimport torch.nn.functional as F\n\ndef generated_loss(batch, model_output, extra):\n    \"\"\"\n    Computes the Adaptive Rank-Gapped Margin Loss.\n\n    This loss compares a tanh-clipped log-probability difference against an\n    additive margin derived from the rank-normalized cost difference.\n\n    Args:\n        batch (dict): A dictionary containing tensors.\n            'cost_a' (Tensor [N]): Cost of solution 'a'.\n            'cost_b' (Tensor [N]): Cost of solution 'b'.\n            'log_prob_w' (Tensor [N]): Log probability of the winning solution.\n            'log_prob_l' (Tensor [N]): Log probability of the losing solution.\n            'weight' (Tensor [N], optional): Per-pair loss weights.\n        model_output (dict): Model outputs (not used).\n        extra (dict): A dictionary for hyperparameters.\n            'alpha' (float): Controls the magnitude of the rank-based margin.\n\n    Returns:\n        torch.Tensor: A scalar tensor representing the mean loss.\n    \"\"\"\n\n    # Whitelisted operator implemented as a helper function\n    def rank_gap(x, eps=1e-9):\n        \"\"\"Converts a tensor to its normalized ranks within the range [-0.5, 0.5].\"\"\"\n        if x.numel() <= 1:\n            return torch.zeros_like(x)\n        ranks = x.to(torch.float32).argsort().argsort().to(x.dtype)\n        # Normalize ranks to [0, 1]\n        normalized_ranks = ranks / (x.numel() - 1 + eps)\n        # Shift to [-0.5, 0.5]\n        return normalized_ranks - 0.5\n\n    # Retrieve hyperparameters\n    alpha = extra.get('alpha', 3.0)\n\n    # Unpack tensors from the batch\n    cost_a = batch['cost_a']\n    cost_b = batch['cost_b']\n    log_prob_w = batch['log_prob_w']\n    log_prob_l = batch['log_prob_l']\n    weights = batch.get('weight', None)\n\n    # 1. Calculate the difference in log probabilities\n    logp_diff = log_prob_w - log_prob_l\n\n    # 2. (Inherited from Parent 0) Soft-clip for stability\n    clipped_logp_diff = torch.tanh(logp_diff)\n\n    # 3. Calculate absolute cost difference\n    cost_diff = torch.abs(cost_a - cost_b)\n\n    # 4. (New Coupling) Use rank_gap for a stable, scale-invariant cost signal\n    rank_gap_signal = rank_gap(cost_diff)\n\n    # 5. (Inherited from Parent 1 & New Coupling) Create an adaptive rank-based margin\n    # softplus ensures the margin is always non-negative and smooth.\n    # The margin increases as the rank of the cost difference increases.\n    rank_margin = F.softplus(alpha * rank_gap_signal)\n\n    # 6. Combine the components\n    # The objective is for the clipped log-prob difference to exceed the margin.\n    argument = clipped_logp_diff - rank_margin\n\n    # 7. Compute the final loss using numerically stable logsigmoid\n    loss_per_item = -F.logsigmoid(argument)\n\n    # 8. Apply optional weights and compute the mean loss\n    if weights is not None:\n        # Ensure weights sum is not zero to avoid division by zero\n        weights_sum = weights.sum().clamp(min=1e-8)\n        loss = (loss_per_item * weights).sum() / weights_sum\n    else:\n        loss = loss_per_item.mean()\n\n    return loss"}, "static_ok": true, "static_reason": "", "dynamic_ok": true, "dynamic_reason": "ok", "loss_value": 1.1892598867416382, "grad_norm": 0.0}
{"generation": 5, "index": 7, "ir": {"name": "Adaptive Rank-Modulated Sigmoid Loss", "intuition": "This loss function creates an adaptive learning signal by combining a multiplicative scaler and an additive margin, both derived from the relative rank of cost differences within a batch. The goal is to make the learning signal robust to cost scale and outliers, while dynamically adjusting the difficulty of the preference task.\n\nInherited Ideas:\n- From `Soft-Clipped Rank-Modulated Loss` (Parent 0): It inherits the core concept of using a **unified modulator derived from the rank-gapped cost difference**. This `rank_gap` operator provides a stable, scale-invariant signal of preference importance. The modulator is created using `softplus` to ensure it is smooth and non-negative.\n- From `Rank-Gapped Adaptive Sigmoid Loss` (Parent 1): It inherits the structural form of combining a **multiplicative scaler with an additive margin** (`scaler * logp_diff - margin`). This allows for independent control over how much the log-probability difference is amplified (scaler) and the minimum required gap (margin).\n\nNew Coupling Ideas & Modifications:\n1. **Dual-Role Modulator Decomposition**: The unified `rank_modulator` from Parent 0 is now decomposed to serve two distinct but coupled roles. It acts directly as the `additive_margin` (scaled by `alpha`), but it is also passed through a `tanh` activation to create a bounded `multiplicative_scaler` (scaled by `beta`). This coupling ensures that pairs with a higher cost-difference rank face both a larger required log-probability gap and a stronger gradient signal, while `tanh` prevents the scaler from causing numerical instability.\n2. **Dynamic Margin Scaling**: The additive margin (`rank_modulator`) is also used to scale the log-probability difference *before* it is multiplied by the `tanh`-based scaler. The loss argument becomes `scaler * (margin * logp_diff) - margin`. This can be simplified to `margin * (scaler * logp_diff - 1)`. This structure creates a strong incentive for the model: for pairs with a large margin, the scaled `logp_diff` must significantly exceed 1 to minimize the loss, making the learning task progressively harder for more important preference pairs.", "pseudocode": "1. Calculate the difference in log probabilities: `logp_diff = logp_winner - logp_loser`.\n2. Calculate the absolute difference in costs: `cost_diff = abs(cost_winner - cost_loser)`.\n3. (Inherited from Parent 0) Convert cost differences into a normalized rank signal between -0.5 and 0.5 using the `rank_gap` operator. This makes the signal robust to cost scale and outliers.\n4. (Inherited from Parent 0) Create a smooth, non-negative `rank_modulator` from the rank signal: `rank_modulator = softplus(alpha * rank_gap_signal)`.\n5. (Inherited from Parent 1 & New Coupling) Use the `rank_modulator` directly as the additive margin: `margin = rank_modulator`.\n6. (Inherited from Parent 1 & New Coupling) Create a bounded multiplicative scaler by passing the `rank_modulator` through a `tanh` function: `scaler = tanh(beta * rank_modulator)`.\n7. (New Coupling) Combine the components. The target is `scaler * (margin * logp_diff) > margin`, which is equivalent to `margin * (scaler * logp_diff - 1)`.\n8. Compute the final loss using a numerically stable logistic loss: `loss = -logsigmoid(margin * (scaler * logp_diff - 1))`.\n9. Return the weighted mean of the loss over the batch.", "hyperparams": {"alpha": 4.0, "beta": 1.0}, "operators_used": ["logsigmoid", "softplus", "tanh", "rank_gap"], "implementation_hint": {"expects": ["cost_a", "cost_b", "log_prob_w", "log_prob_l", "weight"], "returns": "scalar"}, "code": "import torch\nimport torch.nn.functional as F\n\ndef generated_loss(batch, model_output, extra):\n    \"\"\"\n    Computes the Adaptive Rank-Modulated Sigmoid Loss.\n\n    This loss combines a multiplicative scaler and an additive margin, both derived from a\n    unified, rank-based modulator. The modulator itself acts as the margin and is also used\n    to scale the logp_diff before the final scaling, creating a dynamic learning signal.\n\n    Args:\n        batch (dict): A dictionary containing tensors.\n            'cost_a' (Tensor [N]): Cost of solution 'a'.\n            'cost_b' (Tensor [N]): Cost of solution 'b'.\n            'log_prob_w' (Tensor [N]): Log probability of the winning solution.\n            'log_prob_l' (Tensor [N]): Log probability of the losing solution.\n            'weight' (Tensor [N], optional): Per-pair loss weights.\n        model_output (dict): Model outputs (not used).\n        extra (dict): A dictionary for hyperparameters.\n            'alpha' (float): Controls the magnitude of the rank modulator and margin.\n            'beta' (float): Controls the sensitivity of the multiplicative scaler.\n\n    Returns:\n        torch.Tensor: A scalar tensor representing the mean loss.\n    \"\"\"\n\n    # Whitelisted operator implemented as a helper function\n    def rank_gap(x, eps=1e-9):\n        \"\"\"Converts a tensor to its normalized ranks within the range [-0.5, 0.5].\"\"\"\n        if x.numel() <= 1:\n            return torch.zeros_like(x)\n        ranks = x.to(torch.float32).argsort().argsort().to(x.dtype)\n        # Normalize ranks to [0, 1]\n        normalized_ranks = ranks / (x.numel() - 1 + eps)\n        # Shift to [-0.5, 0.5]\n        return normalized_ranks - 0.5\n\n    # Retrieve hyperparameters\n    alpha = extra.get('alpha', 4.0)\n    beta = extra.get('beta', 1.0)\n\n    # Unpack tensors from the batch\n    cost_a = batch['cost_a']\n    cost_b = batch['cost_b']\n    log_prob_w = batch['log_prob_w']\n    log_prob_l = batch['log_prob_l']\n    weights = batch.get('weight', None)\n\n    # 1. Calculate the difference in log probabilities\n    logp_diff = log_prob_w - log_prob_l\n\n    # 2. Calculate absolute cost difference\n    cost_diff = torch.abs(cost_a - cost_b)\n\n    # 3. (Inherited) Use rank_gap for a stable, scale-invariant cost signal\n    rank_gap_signal = rank_gap(cost_diff)\n\n    # 4. (Inherited) Create a unified rank modulator\n    rank_modulator = F.softplus(alpha * rank_gap_signal)\n\n    # 5. (Inherited & New Coupling) Use the modulator directly as the additive margin\n    margin = rank_modulator\n\n    # 6. (Inherited & New Coupling) Create a bounded scaler from the modulator\n    scaler = torch.tanh(beta * rank_modulator)\n\n    # 7. (New Coupling) Combine components into the final loss argument\n    # The structure is margin * (scaler * logp_diff - 1)\n    argument = margin * (scaler * logp_diff - 1.0)\n\n    # 8. Compute the final loss using numerically stable logsigmoid\n    loss_per_item = -F.logsigmoid(argument)\n\n    # 9. Apply optional weights and compute the mean loss\n    if weights is not None and weights.sum() > 0:\n        loss = (loss_per_item * weights).sum() / weights.sum()\n    else:\n        loss = loss_per_item.mean()\n\n    return loss"}, "static_ok": true, "static_reason": "", "dynamic_ok": true, "dynamic_reason": "ok", "loss_value": 1.2550013065338135, "grad_norm": 0.0}
{"generation": 6, "index": 0, "ir": {"name": "Adaptive Rank-Gated Loss", "intuition": "This loss function introduces a dynamic gating mechanism that adaptively focuses the learning signal based on the relative importance of a preference pair. It ensures that the model's preference signal is both directionally correct and appropriately scaled, while maintaining numerical stability.\n\nInherited Ideas:\n- From `Soft-Clipped Rank-Modulated Loss` (Parent 0): It inherits the use of a **multiplicative scaler** derived from the cost difference. The loss is scaled by a factor that increases with the importance of the preference pair, amplifying the gradient for more significant pairs.\n- From `Rank-Modulated Dual-Clipping Loss` (Parent 1): It inherits the idea of an **adaptive additive margin**. The model's preference score must exceed a margin that is computed from a smooth, rank-based signal, making the learning target more demanding for higher-ranked pairs.\n\nNew Coupling Ideas & Modifications:\n1.  **Unified Rank-Based Modulator**: Both parents use `softplus(alpha * rank_gap(cost_diff))` to create a stable, scale-invariant signal. This child adopts this unified `rank_modulator` as the single source for all adaptive components, ensuring consistency.\n2.  **Adaptive Sigmoid Gating**: A new coupling mechanism is introduced where the `rank_modulator` is used to create a sigmoid gate: `gate = sigmoid(beta * (logp_diff - rank_modulator))`. This gate measures how well the log-probability difference (`logp_diff`) has satisfied the adaptive margin (`rank_modulator`). The gate's value approaches 1 when the margin is met and 0 otherwise. This provides a smooth, differentiable switch.\n3.  **Gated Log-Probability Difference**: The core of the loss is `gate * logp_diff`. The `logp_diff` is only allowed to contribute to the loss signal if the gate is 'open' (i.e., the margin has been met). This forces the model to first satisfy the margin requirement before it can receive further gradient to increase the `logp_diff`. The final loss `softplus(-gate * logp_diff)` encourages maximizing this gated quantity, creating a two-stage learning objective: first, cross the margin; second, maximize the difference.", "pseudocode": "1. Calculate the difference in log probabilities: `logp_diff = logp_winner - logp_loser`.\n2. Calculate the absolute difference in costs: `cost_diff = abs(cost_winner - cost_loser)`.\n3. Create a normalized rank signal from the cost difference using the `rank_gap` operator, making it robust to scale and outliers.\n4. (New Coupling) Create a single, unified `rank_modulator` from the rank signal: `rank_modulator = softplus(alpha * rank_gap_signal)`. This will serve as both a margin and a component of the loss scaler.\n5. (Inherited from Parent 1) Use the `rank_modulator` as an adaptive additive margin.\n6. (New Coupling) Create an adaptive sigmoid gate that activates when `logp_diff` surpasses the `rank_modulator` margin: `gate = sigmoid(beta * (logp_diff - rank_modulator))`.\n7. (Inherited from Parent 0 & New Coupling) Form the loss argument by gating the `logp_diff`: `argument = gate * logp_diff`. The `logp_diff` is effectively scaled by a factor between 0 and 1, depending on whether the margin has been satisfied.\n8. Compute the final loss using `softplus` for stability, which is equivalent to `-logsigmoid` but framed as maximizing the argument: `loss = softplus(-argument)`.\n9. Return the weighted mean of the loss over the batch.", "hyperparams": {"alpha": 4.0, "beta": 10.0}, "operators_used": ["softplus", "sigmoid", "rank_gap"], "implementation_hint": {"expects": ["cost_a", "cost_b", "log_prob_w", "log_prob_l", "weight"], "returns": "scalar"}, "code": "import torch\nimport torch.nn.functional as F\n\ndef generated_loss(batch, model_output, extra):\n    \"\"\"\n    Computes the Adaptive Rank-Gated Loss.\n\n    This loss uses a rank-based modulator to define an adaptive margin, which in turn\n    controls a sigmoid gate. The gate multiplies the log-probability difference,\n    focusing the learning signal on pairs that have met their margin requirement.\n\n    Args:\n        batch (dict): A dictionary containing tensors.\n            'cost_a' (Tensor [N]): Cost of solution 'a'.\n            'cost_b' (Tensor [N]): Cost of solution 'b'.\n            'log_prob_w' (Tensor [N]): Log probability of the winning solution.\n            'log_prob_l' (Tensor [N]): Log probability of the losing solution.\n            'weight' (Tensor [N], optional): Per-pair loss weights.\n        model_output (dict): Model outputs (not used).\n        extra (dict): A dictionary for hyperparameters.\n            'alpha' (float): Controls the magnitude of the rank-based margin.\n            'beta' (float): Controls the steepness of the sigmoid gate.\n\n    Returns:\n        torch.Tensor: A scalar tensor representing the mean loss.\n    \"\"\"\n\n    # Whitelisted operator implemented as a helper function\n    def rank_gap(x, eps=1e-9):\n        \"\"\"Converts a tensor to its normalized ranks within the range [-0.5, 0.5].\"\"\"\n        if x.numel() <= 1:\n            return torch.zeros_like(x)\n        ranks = x.to(torch.float32).argsort().argsort().to(x.dtype)\n        # Normalize ranks to [0, 1]\n        normalized_ranks = ranks / (x.numel() - 1 + eps)\n        # Shift to [-0.5, 0.5]\n        return normalized_ranks - 0.5\n\n    # Retrieve hyperparameters\n    alpha = extra.get('alpha', 4.0)\n    beta = extra.get('beta', 10.0)\n\n    # Unpack tensors from the batch\n    cost_a = batch['cost_a']\n    cost_b = batch['cost_b']\n    log_prob_w = batch['log_prob_w']\n    log_prob_l = batch['log_prob_l']\n    weights = batch.get('weight', None)\n\n    # 1. Calculate the difference in log probabilities\n    logp_diff = log_prob_w - log_prob_l\n\n    # 2. Calculate absolute cost difference\n    cost_diff = torch.abs(cost_a - cost_b)\n\n    # 3. Create a normalized rank signal\n    rank_gap_signal = rank_gap(cost_diff)\n\n    # 4. (New Coupling) Create a unified rank modulator to act as the margin\n    # This is inherited from Parent 1's margin concept.\n    adaptive_margin = F.softplus(alpha * rank_gap_signal)\n\n    # 5. (New Coupling) Create an adaptive sigmoid gate\n    # The gate 'opens' (approaches 1) as logp_diff surpasses the adaptive_margin.\n    gate = torch.sigmoid(beta * (logp_diff - adaptive_margin))\n\n    # 6. (Inherited from Parent 0 & New Coupling) Form the loss argument\n    # The logp_diff is scaled by the gate. This is a form of multiplicative scaling.\n    argument = gate * logp_diff\n\n    # 7. Compute the final loss. softplus(-x) is a stable way to compute -log(sigmoid(x))\n    # We want to maximize the 'argument', so we minimize its negative.\n    loss_per_item = F.softplus(-argument)\n\n    # 8. Apply optional weights and compute the mean loss\n    if weights is not None:\n        loss = (loss_per_item * weights).sum() / weights.sum().clamp(min=1e-8)\n    else:\n        loss = loss_per_item.mean()\n\n    return loss"}, "static_ok": true, "static_reason": "", "dynamic_ok": true, "dynamic_reason": "ok", "loss_value": 0.6931471824645996, "grad_norm": 0.0}
{"generation": 6, "index": 1, "ir": {"name": "Adaptive Log-Ratio Margin Loss", "intuition": "This loss function introduces a novel adaptive margin based on the logarithmic ratio of model probabilities, stabilized by a rank-based signal. It aims to create a dynamic target for the log-probability difference that is sensitive to both the model's current confidence and the intrinsic importance of the preference pair.\n\nInherited Ideas:\n- From `Soft-Clipped Rank-Modulated Loss` (Parent 0): It inherits the idea of **soft-clipping the log-probability difference** using `tanh`. This bounds the logp difference to `[-1, 1]`, preventing extreme gradients and focusing the learning signal on getting the preference direction correct rather than just maximizing the gap.\n- From `Rank-Modulated Dual-Clipping Loss` (Parent 1): It inherits the structure of using a **cost-derived additive margin**. The loss requires the model's preference score to exceed this margin, which is computed using `softplus` on a rank-based signal (`rank_gap`), ensuring it is smooth, non-negative, and grows with the cost difference's importance.\n\nNew Coupling Ideas & Modifications:\n1. **Log-Ratio Adaptive Margin**: The core novelty is an adaptive margin that is not just based on cost, but is a product of the cost-based rank signal and a term derived from the model's own probabilities. Specifically, the margin is `rank_modulator * log(1 + exp(-abs(logp_diff)))`. This term `log(1 + exp(-|x|))` is a smooth approximation of `max(0, -|x|)`. It is large when `logp_diff` is near zero (model is uncertain) and small when `logp_diff` is large (model is confident). This coupling means the margin is largest for important pairs (high `rank_modulator`) where the model is uncertain, creating a strong learning signal. The margin shrinks as the model becomes more confident, preventing over-optimization on already well-separated pairs.\n2. **Stabilized Log-Ratio**: Using `abs(logp_diff)` inside the `log(1 + exp(...))` term makes the margin symmetric and prevents it from becoming negative, ensuring a stable and well-behaved target for the `tanh(logp_diff)` to overcome.", "pseudocode": "1. Calculate the difference in log probabilities: `logp_diff = logp_winner - logp_loser`.\n2. (Inherited from Parent 0) Soft-clip the log-probability difference using `tanh` for stability: `clipped_logp_diff = tanh(logp_diff)`.\n3. Calculate the absolute difference in costs: `cost_diff = abs(cost_winner - cost_loser)`.\n4. (Inherited from Parent 1) Create a base rank-based signal from the cost difference: `rank_modulator = softplus(alpha * rank_gap(cost_diff))`. This provides a smooth, non-negative signal of the pair's importance.\n5. (New Coupling) Compute a model-confidence term using `log` and `exp`: `confidence_term = log(1 + exp(-abs(logp_diff)))`. This term is large when the model is uncertain (logp_diff near zero) and small when confident.\n6. (New Coupling) Create the final adaptive margin by coupling the rank modulator and the confidence term: `adaptive_margin = rank_modulator * confidence_term`.\n7. Formulate the loss argument by subtracting the adaptive margin from the clipped log-probability difference: `argument = clipped_logp_diff - adaptive_margin`.\n8. Compute the final loss using a numerically stable logistic loss: `loss = -logsigmoid(argument)`.\n9. Return the weighted mean of the loss over the batch.", "hyperparams": {"alpha": 4.0}, "operators_used": ["logsigmoid", "softplus", "tanh", "log", "exp", "rank_gap"], "implementation_hint": {"expects": ["cost_a", "cost_b", "logp_w", "logp_l", "weight"], "returns": "scalar"}, "code": "import torch\nimport torch.nn.functional as F\n\ndef generated_loss(batch, model_output, extra):\n    \"\"\"\n    Computes the Adaptive Log-Ratio Margin Loss.\n\n    This loss combines a tanh-clipped log-probability difference with a novel\n    adaptive margin. The margin is the product of a rank-based cost signal\n    and a term that reflects the model's current confidence on the pair.\n\n    Args:\n        batch (dict): A dictionary containing tensors.\n            'cost_a' (Tensor [N]): Cost of solution 'a'.\n            'cost_b' (Tensor [N]): Cost of solution 'b'.\n            'log_prob_w' (Tensor [N]): Log probability of the winning solution.\n            'log_prob_l' (Tensor [N]): Log probability of the losing solution.\n            'weight' (Tensor [N], optional): Per-pair loss weights.\n        model_output (dict): Model outputs (not used).\n        extra (dict): A dictionary for hyperparameters.\n            'alpha' (float): Controls the magnitude of the rank-based modulator.\n\n    Returns:\n        torch.Tensor: A scalar tensor representing the mean loss.\n    \"\"\"\n\n    # Whitelisted operator implemented as a helper function\n    def rank_gap(x, eps=1e-9):\n        \"\"\"Converts a tensor to its normalized ranks within the range [-0.5, 0.5].\"\"\"\n        if x.numel() <= 1:\n            return torch.zeros_like(x)\n        ranks = x.to(torch.float32).argsort().argsort().to(x.dtype)\n        # Normalize ranks to [0, 1]\n        normalized_ranks = ranks / (x.numel() - 1 + eps)\n        # Shift to [-0.5, 0.5]\n        return normalized_ranks - 0.5\n\n    # Retrieve hyperparameters\n    alpha = extra.get('alpha', 4.0)\n\n    # Unpack tensors from the batch\n    cost_a = batch['cost_a']\n    cost_b = batch['cost_b']\n    log_prob_w = batch['log_prob_w']\n    log_prob_l = batch['log_prob_l']\n    weights = batch.get('weight', None)\n\n    # 1. Calculate the difference in log probabilities\n    logp_diff = log_prob_w - log_prob_l\n\n    # 2. (Inherited from Parent 0) Soft-clip for stability\n    clipped_logp_diff = torch.tanh(logp_diff)\n\n    # 3. Calculate absolute cost difference\n    cost_diff = torch.abs(cost_a - cost_b)\n\n    # 4. (Inherited from Parent 1) Create a base rank modulator\n    rank_gap_signal = rank_gap(cost_diff)\n    rank_modulator = F.softplus(alpha * rank_gap_signal)\n\n    # 5. (New Coupling) Compute model-confidence term\n    # This is equivalent to F.softplus(-abs(logp_diff)) but uses the required operators.\n    confidence_term = torch.log(1 + torch.exp(-torch.abs(logp_diff)))\n\n    # 6. (New Coupling) Create the final adaptive margin\n    adaptive_margin = rank_modulator * confidence_term\n\n    # 7. Formulate the loss argument\n    # We want clipped_logp_diff > adaptive_margin\n    argument = clipped_logp_diff - adaptive_margin\n\n    # 8. Compute the final loss using numerically stable logsigmoid\n    loss_per_item = -F.logsigmoid(argument)\n\n    # 9. Apply optional weights and compute the mean loss\n    if weights is not None and weights.numel() > 0:\n        loss = (loss_per_item * weights).sum() / weights.sum().clamp(min=1e-8)\n    else:\n        loss = loss_per_item.mean()\n\n    return loss"}, "static_ok": true, "static_reason": "", "dynamic_ok": true, "dynamic_reason": "ok", "loss_value": 1.05809485912323, "grad_norm": 0.0}
{"generation": 6, "index": 2, "ir": {"name": "Rank-Normalized Dynamic Clipping Loss", "intuition": "This loss function creates a stable and adaptive learning signal by dynamically regularizing the log-probability difference based on the relative importance of a preference pair. It ensures the model focuses its capacity on learning significant preferences while not being overly penalized for minor ones.\n\nInherited Ideas:\n- From `Rank-Modulated Dual-Clipping Loss` (Parent 1), it inherits the core concept of **dynamically clipping the log-probability difference**. The clipping range is not fixed but is determined by a signal derived from the cost difference. This forces the model to produce a larger log-probability gap for more important pairs and allows a smaller gap for less important ones.\n- From `Soft-Clipped Rank-Modulated Loss` (Parent 0), it inherits the idea of using a **multiplicative scaler** that modulates the main loss term. This scaler amplifies the loss signal for pairs with a larger quality gap, making the training more efficient.\n\nNew Coupling Ideas & Modifications:\n1.  **Z-Score Normalized Signal**: Instead of using `rank_gap` which is batch-dependent, this loss uses `zscore` on the cost differences. This standardizes the cost differences to have a mean of 0 and a standard deviation of 1, providing a robust signal of how many standard deviations away from the mean a given cost difference is. This makes the signal less sensitive to the specific distribution of cost differences within a mini-batch.\n2.  **Coupled Dynamic Clipping and Scaling**: The loss introduces a tight coupling between the clipping mechanism and the final scaling. First, a smooth, non-negative `modulator` is created via `softplus(alpha * zscore(cost_diff))`. This modulator is then used to dynamically `clamp` the log-probability difference. The key innovation is that this same `modulator` is also used as a multiplicative `scaler` on the final loss argument. The loss becomes `scaler * (clipped_logp_diff - margin)`, which simplifies to `modulator * (clamp(logp_diff, -modulator, modulator) - modulator)`. This structure creates a strong incentive: for important pairs (large modulator), the model must push `logp_diff` to the very edge of its dynamic clamp to minimize the loss, as the penalty for not doing so is also scaled up by the same modulator.", "pseudocode": "1. Calculate the difference in log probabilities: `logp_diff = logp_winner - logp_loser`.\n2. Calculate the absolute difference in costs: `cost_diff = abs(cost_winner - cost_loser)`.\n3. (New Coupling) Normalize the cost differences using `zscore` to get a standardized signal robust to scale and batch distribution: `z_cost_diff = zscore(cost_diff)`.\n4. Create a smooth, non-negative `modulator` from the standardized signal: `modulator = softplus(alpha * z_cost_diff)`.\n5. (Inherited from Parent 1 & New Coupling) Dynamically clip the log probability difference using the `modulator` to define the clipping range: `clipped_logp_diff = clamp(logp_diff, min=-modulator, max=modulator)`.\n6. (Inherited from Parent 0 & New Coupling) Use the `modulator` as both a margin and a multiplicative scaler on the final loss term. The target is `clipped_logp_diff > modulator`, and the loss argument is scaled by the modulator itself.\n7. Construct the final loss argument: `argument = modulator * (clipped_logp_diff - modulator)`.\n8. Compute the final loss using a numerically stable logistic loss function: `loss = -logsigmoid(argument)`.\n9. Return the weighted mean of the loss over the batch.", "hyperparams": {"alpha": 1.0, "margin": 0.0}, "operators_used": ["logsigmoid", "softplus", "clamp", "zscore"], "implementation_hint": {"expects": ["cost_a", "cost_b", "logp_w", "logp_l", "weight"], "returns": "scalar"}, "code": "import torch\nimport torch.nn.functional as F\n\ndef generated_loss(batch, model_output, extra):\n    \"\"\"\n    Computes the Rank-Normalized Dynamic Clipping Loss.\n\n    This loss uses a z-score normalized cost difference to create a modulator that\n    simultaneously defines a dynamic clipping range for the log-probability difference\n    and acts as a multiplicative scaler on the final loss term.\n\n    Args:\n        batch (dict): A dictionary containing tensors.\n            'cost_a' (Tensor [N]): Cost of solution 'a'.\n            'cost_b' (Tensor [N]): Cost of solution 'b'.\n            'log_prob_w' (Tensor [N]): Log probability of the winning solution.\n            'log_prob_l' (Tensor [N]): Log probability of the losing solution.\n            'weight' (Tensor [N], optional): Per-pair loss weights.\n        model_output (dict): Model outputs (not used).\n        extra (dict): A dictionary for hyperparameters.\n            'alpha' (float): Controls the magnitude of the modulator.\n\n    Returns:\n        torch.Tensor: A scalar tensor representing the mean loss.\n    \"\"\"\n\n    # Whitelisted operator implemented as a helper function\n    def zscore(x, eps=1e-8):\n        \"\"\"Computes z-score normalization for a tensor.\"\"\"\n        if x.numel() <= 1:\n            return torch.zeros_like(x)\n        mean = x.mean()\n        std = x.std().clamp(min=eps)\n        return (x - mean) / std\n\n    # Retrieve hyperparameters\n    alpha = extra.get('alpha', 1.0)\n\n    # Unpack tensors from the batch\n    cost_a = batch['cost_a']\n    cost_b = batch['cost_b']\n    log_prob_w = batch['log_prob_w']\n    log_prob_l = batch['log_prob_l']\n    weights = batch.get('weight', None)\n\n    # 1. Calculate the difference in log probabilities\n    logp_diff = log_prob_w - log_prob_l\n\n    # 2. Calculate absolute cost difference\n    cost_diff = torch.abs(cost_a - cost_b)\n\n    # 3. (New Coupling) Normalize cost differences using z-score\n    with torch.no_grad():\n        z_cost_diff = zscore(cost_diff)\n\n    # 4. Create a smooth, non-negative modulator\n    modulator = F.softplus(alpha * z_cost_diff)\n\n    # 5. (Inherited from Parent 1 & New Coupling) Dynamically clip logp_diff\n    # The clipping range [-M, M] is determined by the modulator M.\n    clipped_logp_diff = torch.clamp(logp_diff, min=-modulator, max=modulator)\n\n    # 6. (Inherited from Parent 0 & New Coupling) The margin is the modulator itself\n    margin = modulator\n\n    # 7. Construct the final loss argument, scaled by the modulator\n    argument = modulator * (clipped_logp_diff - margin)\n\n    # 8. Compute the final loss using numerically stable logsigmoid\n    loss_per_item = -F.logsigmoid(argument)\n\n    # 9. Apply optional weights and compute the mean loss\n    if weights is not None:\n        loss = (loss_per_item * weights).sum() / weights.sum().clamp(min=1e-8)\n    else:\n        loss = loss_per_item.mean()\n\n    return loss"}, "static_ok": true, "static_reason": "", "dynamic_ok": true, "dynamic_reason": "ok", "loss_value": 0.961954653263092, "grad_norm": 0.0}
{"generation": 6, "index": 3, "ir": {"name": "Adaptive Dual-Modulation Loss", "intuition": "This loss function creates a highly adaptive learning signal by using a rank-based cost signal to modulate both the target preference margin and the scale of the log-probability difference itself. It aims to prevent the model from wasting capacity on easy or insignificant pairs while demanding a stronger signal for more important ones.\n\nInherited Ideas:\n- From `Soft-Clipped Rank-Modulated Loss` (Parent 0): It inherits the use of a **multiplicative scaler** applied to a transformed log-probability difference. The scaler, derived from a rank-based signal, amplifies the learning signal for pairs with a larger, more significant cost gap.\n- From `Rank-Modulated Dual-Clipping Loss` (Parent 1): It inherits the concept of an **additive margin** also derived from a rank-based signal. The model is encouraged to produce a log-probability difference that exceeds this margin, which adapts to the importance of the preference pair.\n\nNew Coupling Ideas & Modifications:\n1.  **Z-Score Normalization of Log-Probabilities**: Before any other operation, the `logp_diff` is normalized using batch-wise z-scoring (`zscore(logp_diff)`). This new coupling stabilizes the input to the loss function by centering it around zero with a standard deviation of one, making the subsequent scaling and margin operations more robust to the initial distribution of `logp_diff` values from the model.\n2.  **Decoupled Modulation with a Shared Base**: Both parents use a single `rank_modulator` for multiple roles. This child loss decouples the scaler and margin but derives them from the same underlying rank signal. A base `rank_signal = softplus(alpha * rank_gap(cost_diff))` is computed. The `scaler` is set to `beta * rank_signal`, and the `margin` is set to `rank_signal` itself. This allows for independent tuning of the scaling effect (`beta`) relative to the margin, providing more control over the loss landscape. The final structure is `scaler * normalized_logp_diff - margin`, which encourages the normalized log-probability difference to be greater than `margin / scaler`, or `1 / beta`.", "pseudocode": "1. Calculate the difference in log probabilities: `logp_diff = logp_winner - logp_loser`.\n2. (New Coupling) Normalize the log-probability difference using batch-wise z-scoring: `norm_logp_diff = zscore(logp_diff)`. This stabilizes the signal by centering it to mean 0 and scaling to std 1.\n3. Calculate the absolute difference in costs: `cost_diff = abs(cost_winner - cost_loser)`.\n4. Convert the cost differences into a normalized rank signal between -0.5 and 0.5 using the `rank_gap` operator for scale-invariance.\n5. Create a smooth, non-negative base signal from the rank: `rank_signal = softplus(alpha * rank_gap_signal)`.\n6. (Inherited from Parent 0 & New Coupling) Define a multiplicative scaler based on the rank signal: `scaler = beta * rank_signal`.\n7. (Inherited from Parent 1) Define an additive margin, which is the rank signal itself: `margin = rank_signal`.\n8. Combine the components. The target is `scaler * norm_logp_diff > margin`. The argument for the loss function becomes `scaler * norm_logp_diff - margin`.\n9. Compute the final loss using a numerically stable logistic loss: `loss = -logsigmoid(argument)`.\n10. Return the weighted mean of the loss over the batch.", "hyperparams": {"alpha": 4.0, "beta": 2.0}, "operators_used": ["logsigmoid", "softplus", "zscore", "rank_gap"], "implementation_hint": {"expects": ["cost_a", "cost_b", "logp_w", "logp_l", "weight"], "returns": "scalar"}, "code": "import torch\nimport torch.nn.functional as F\n\ndef generated_loss(batch, model_output, extra):\n    \"\"\"\n    Computes the Adaptive Dual-Modulation Loss.\n\n    This loss uses a rank-based signal to create a decoupled multiplicative scaler\n    and additive margin, applied to a z-scored log-probability difference.\n\n    Args:\n        batch (dict): A dictionary containing tensors.\n            'cost_a' (Tensor [N]): Cost of solution 'a'.\n            'cost_b' (Tensor [N]): Cost of solution 'b'.\n            'log_prob_w' (Tensor [N]): Log probability of the winning solution.\n            'log_prob_l' (Tensor [N]): Log probability of the losing solution.\n            'weight' (Tensor [N], optional): Per-pair loss weights.\n        model_output (dict): Model outputs (not used).\n        extra (dict): A dictionary for hyperparameters.\n            'alpha' (float): Controls the magnitude of the base rank signal.\n            'beta' (float): Controls the strength of the multiplicative scaler.\n\n    Returns:\n        torch.Tensor: A scalar tensor representing the mean loss.\n    \"\"\"\n\n    # Whitelisted operators implemented as helper functions\n    def rank_gap(x, eps=1e-9):\n        \"\"\"Converts a tensor to its normalized ranks within the range [-0.5, 0.5].\"\"\"\n        if x.numel() <= 1:\n            return torch.zeros_like(x)\n        ranks = x.to(torch.float32).argsort().argsort().to(x.dtype)\n        normalized_ranks = ranks / (x.numel() - 1 + eps)\n        return normalized_ranks - 0.5\n\n    def zscore(x, eps=1e-8):\n        \"\"\"Computes z-score normalization for a tensor.\"\"\"\n        if x.numel() <= 1:\n            return torch.zeros_like(x)\n        mean = x.mean()\n        std = x.std()\n        return (x - mean) / (std + eps)\n\n    # Retrieve hyperparameters\n    alpha = extra.get('alpha', 4.0)\n    beta = extra.get('beta', 2.0)\n\n    # Unpack tensors from the batch\n    cost_a = batch['cost_a']\n    cost_b = batch['cost_b']\n    log_prob_w = batch['log_prob_w']\n    log_prob_l = batch['log_prob_l']\n    weights = batch.get('weight', None)\n\n    # 1. Calculate the difference in log probabilities\n    logp_diff = log_prob_w - log_prob_l\n\n    # 2. (New Coupling) Normalize logp_diff for stability\n    norm_logp_diff = zscore(logp_diff)\n\n    # 3. Calculate absolute cost difference\n    cost_diff = torch.abs(cost_a - cost_b)\n\n    # 4. Use rank_gap for a stable, scale-invariant cost signal\n    rank_gap_signal = rank_gap(cost_diff)\n\n    # 5. Create a smooth, non-negative base signal\n    rank_signal = F.softplus(alpha * rank_gap_signal)\n\n    # 6. (Inherited from Parent 0 & New Coupling) Define a multiplicative scaler\n    scaler = beta * rank_signal\n\n    # 7. (Inherited from Parent 1) Define an additive margin\n    margin = rank_signal\n\n    # 8. Combine components: scaler * norm_logp_diff should be > margin\n    argument = scaler * norm_logp_diff - margin\n\n    # 9. Compute the final loss using numerically stable logsigmoid\n    loss_per_item = -F.logsigmoid(argument)\n\n    # 10. Apply optional weights and compute the mean loss\n    if weights is not None:\n        loss = (loss_per_item * weights).sum() / weights.sum().clamp(min=1e-8)\n    else:\n        loss = loss_per_item.mean()\n\n    return loss"}, "static_ok": true, "static_reason": "", "dynamic_ok": true, "dynamic_reason": "ok", "loss_value": 1.2550013065338135, "grad_norm": 0.0}
{"generation": 6, "index": 4, "ir": {"name": "Adaptive Dual-Signal Loss", "intuition": "This loss function creates a robust and adaptive learning signal by combining stable, clipped log-probabilities with a dynamic, rank-based margin. It is designed to be resilient to outliers in both log-probabilities and cost differences, while focusing the model's effort on preference pairs that are most informative.\n\nInherited Ideas:\n- From `Soft-Clipped Rank-Modulated Loss` (Parent 0): It inherits the use of **soft-clipping the log-probability difference with `tanh`**. This bounds the `logp_diff` to `[-1, 1]`, preventing extreme gradients and ensuring numerical stability.\n- From `Rank-Modulated Dual-Clipping Loss` (Parent 1): It inherits the idea of using a **cost-derived additive margin**. The loss requires the model's preference score to exceed this margin, which is calculated using `softplus` on a rank-based signal. This makes the margin smooth, non-negative, and adaptive to the relative importance of the preference pair.\n\nNew Coupling Ideas & Modifications:\n1. **Rank-based Margin Schedule (`beta` hyperparameter)**: A new hyperparameter `beta` is introduced to control the schedule of the adaptive margin. The margin is calculated as `softplus(beta * rank_gap_signal)`. A high `beta` creates a steep margin, aggressively pushing the model on high-rank pairs, while a low `beta` creates a gentler, more uniform margin across all pairs. This provides explicit control over how strongly the loss differentiates between pairs of varying importance.\n2. **Z-Scored Log-Probability Difference**: Before applying the `tanh` clipping, the `logp_diff` is normalized using batch-level z-scoring (`(x - mean(x)) / std(x)`). This new coupling makes the input to the `tanh` function independent of the absolute scale and shift of the model's log-probabilities in the current batch. It centers the distribution of `logp_diff` around zero and scales it, ensuring that the `tanh` function operates in its sensitive, non-saturating region for a wider range of model outputs, improving gradient flow and stability throughout training.", "pseudocode": "1. Calculate the difference in log probabilities: `logp_diff = logp_winner - logp_loser`.\n2. (New Coupling) Normalize the log-probability differences within the batch using z-scoring: `z_logp_diff = zscore(logp_diff)`. This stabilizes the input to the clipping function.\n3. (Inherited from Parent 0) Soft-clip the normalized log-probability difference using `tanh` for stability: `clipped_logp_diff = tanh(z_logp_diff)`.\n4. Calculate the absolute difference in costs: `cost_diff = abs(cost_winner - cost_loser)`.\n5. Convert the cost differences into a normalized rank signal between -0.5 and 0.5 using the `rank_gap` operator. This makes the signal robust to cost scale and outliers.\n6. (Inherited from Parent 1 & New Coupling) Create an adaptive additive margin using the rank signal and a new `beta` hyperparameter: `margin = softplus(beta * rank_gap_signal)`. This `beta` controls the steepness of the margin schedule.\n7. Combine the clipped signal and the margin. The learning objective is for the clipped, normalized logp difference to exceed the adaptive margin: `argument = clipped_logp_diff - margin`.\n8. Compute the final loss per item using a numerically stable logistic loss: `loss = -logsigmoid(argument)`.\n9. Return the weighted mean of the loss over the batch.", "hyperparams": {"beta": 5.0}, "operators_used": ["logsigmoid", "softplus", "tanh", "zscore", "rank_gap"], "implementation_hint": {"expects": ["cost_a", "cost_b", "log_prob_w", "log_prob_l"], "returns": "scalar"}, "code": "import torch\nimport torch.nn.functional as F\n\ndef generated_loss(batch, model_output, extra):\n    \"\"\"\n    Computes the Adaptive Dual-Signal Loss.\n\n    This loss combines a z-scored, tanh-clipped log-probability difference with a\n    rank-based adaptive margin. The z-scoring stabilizes the logp signal, while the\n    margin, controlled by a 'beta' hyperparameter, adapts to pair importance.\n\n    Args:\n        batch (dict): A dictionary containing tensors.\n            'cost_a' (Tensor [N]): Cost of solution 'a'.\n            'cost_b' (Tensor [N]): Cost of solution 'b'.\n            'log_prob_w' (Tensor [N]): Log probability of the winning solution.\n            'log_prob_l' (Tensor [N]): Log probability of the losing solution.\n            'weight' (Tensor [N], optional): Per-pair loss weights.\n        model_output (dict): Model outputs (not used).\n        extra (dict): A dictionary for hyperparameters.\n            'beta' (float): Controls the steepness of the adaptive margin schedule.\n\n    Returns:\n        torch.Tensor: A scalar tensor representing the mean loss.\n    \"\"\"\n\n    # Whitelisted operators implemented as helper functions\n    def rank_gap(x, eps=1e-9):\n        \"\"\"Converts a tensor to its normalized ranks within the range [-0.5, 0.5].\"\"\"\n        if x.numel() <= 1:\n            return torch.zeros_like(x)\n        ranks = x.to(torch.float32).argsort().argsort().to(x.dtype)\n        normalized_ranks = ranks / (x.numel() - 1 + eps)\n        return normalized_ranks - 0.5\n\n    def zscore(x, eps=1e-8):\n        \"\"\"Applies z-score normalization to a tensor.\"\"\"\n        if x.numel() <= 1:\n            return torch.zeros_like(x)\n        mean = x.mean()\n        std = x.std()\n        return (x - mean) / (std + eps)\n\n    # Retrieve hyperparameters\n    beta = extra.get('beta', 5.0)\n\n    # Unpack tensors from the batch\n    cost_a = batch['cost_a']\n    cost_b = batch['cost_b']\n    log_prob_w = batch['log_prob_w']\n    log_prob_l = batch['log_prob_l']\n    weights = batch.get('weight', None)\n\n    # 1. Calculate the difference in log probabilities\n    logp_diff = log_prob_w - log_prob_l\n\n    # 2. (New Coupling) Normalize logp_diff with z-scoring for stability\n    z_logp_diff = zscore(logp_diff)\n\n    # 3. (Inherited from Parent 0) Soft-clip the normalized difference\n    clipped_logp_diff = torch.tanh(z_logp_diff)\n\n    # 4. Calculate absolute cost difference\n    cost_diff = torch.abs(cost_a - cost_b)\n\n    # 5. Get a stable, scale-invariant rank signal from cost difference\n    rank_gap_signal = rank_gap(cost_diff)\n\n    # 6. (Inherited from Parent 1 & New Coupling) Create an adaptive margin\n    # The 'beta' hyperparameter controls the margin's sensitivity to rank.\n    margin = F.softplus(beta * rank_gap_signal)\n\n    # 7. Combine components. We want clipped_logp_diff > margin.\n    argument = clipped_logp_diff - margin\n\n    # 8. Compute the final loss using numerically stable logsigmoid\n    loss_per_item = -F.logsigmoid(argument)\n\n    # 9. Apply optional weights and compute the mean loss\n    if weights is not None and weights.sum() > 0:\n        loss = (loss_per_item * weights).sum() / weights.sum()\n    else:\n        loss = loss_per_item.mean()\n\n    return loss"}, "static_ok": true, "static_reason": "", "dynamic_ok": true, "dynamic_reason": "ok", "loss_value": 1.334313154220581, "grad_norm": 0.0}
{"generation": 6, "index": 5, "ir": {"name": "Adaptive Dual-Clip Loss with Scheduled Margin", "intuition": "This loss function creates a highly adaptive learning signal by dynamically adjusting both the log-probability clipping range and the target margin based on the relative importance of a preference pair. It is designed to be robust and to focus the model's capacity where it matters most.\n\nInherited Ideas:\n- From `Rank-Modulated Dual-Clipping Loss` (Parent 1): It inherits the core idea of **dynamic clipping**, where the log-probability difference (`logp_diff`) is clamped within a range determined by the preference pair's importance. This prevents the model from wasting capacity on insignificant pairs while allowing a strong signal for important ones. The clipping range is `[-clip_bound, clip_bound]`, where `clip_bound` is derived from the cost difference.\n- From `Soft-Clipped Rank-Modulated Loss` (Parent 0): It inherits the concept of using a **multiplicative scaler** on a transformed `logp_diff`. While the parent used `tanh`, this child uses the dynamically clipped `logp_diff` and scales it before applying the loss, amplifying the gradient for pairs where the model's preference is far from the target.\n\nNew Coupling Ideas & Modifications:\n1. **Decoupled Clipping and Margin Signals**: Instead of using a single `rank_modulator` for both clipping and the margin, this loss decouples them. A `clip_bound` is calculated using `softplus` on a scaled `rank_gap` signal, defining the dynamic clipping range. A separate `scheduled_margin` is calculated as `beta * sigmoid(alpha * rank_gap_signal)`. This allows independent control over the maximum allowed `logp_diff` (via `clip_bound`) and the target preference margin (via `scheduled_margin`). The `sigmoid` ensures the margin is a smooth value between 0 and `beta`, making the target achievable and stable.\n2. **Margin as a Scaled Target**: The final loss argument is structured as `clip_bound * (clipped_logp_diff / clip_bound - scheduled_margin)`. This reframes the objective: the normalized clipped log-probability difference (`clipped_logp_diff / clip_bound`, which is in `[-1, 1]`) must exceed the `scheduled_margin` (which is in `[0, beta]`). The outer `clip_bound` acts as a multiplicative scaler inherited from Parent 0, increasing the loss penalty for high-importance pairs that fail to meet their target margin.", "pseudocode": "1. Calculate the difference in log probabilities: `logp_diff = logp_winner - logp_loser`.\n2. Calculate the absolute difference in costs: `cost_diff = abs(cost_winner - cost_loser)`.\n3. Convert the cost differences into a normalized rank signal between -0.5 and 0.5 using the `rank_gap` operator. This provides a stable, scale-invariant signal of preference importance.\n4. (New Coupling) Calculate the dynamic clipping bound: `clip_bound = softplus(alpha * rank_gap_signal)`. This creates a smooth, non-negative value that increases with the rank of the cost difference.\n5. (Inherited from Parent 1) Dynamically clip the log probability difference using the calculated bound: `clipped_logp_diff = clamp(logp_diff, min=-clip_bound, max=clip_bound)`.\n6. (New Coupling) Calculate a separate, scheduled margin using a sigmoid function: `scheduled_margin = beta * sigmoid(alpha * rank_gap_signal)`. This creates a smooth margin target between 0 and `beta`.\n7. (Inherited from Parent 0 & New Coupling) Combine the components. The objective is that the normalized `logp_diff` should exceed the margin. The argument is formulated as `clip_bound * (clipped_logp_diff / clip_bound - scheduled_margin)`. The outer `clip_bound` acts as a scaler, and `clipped_logp_diff / clip_bound` normalizes the clipped logp difference to `[-1, 1]` before comparing it to the margin.\n8. Compute the final loss using a numerically stable logistic loss function: `loss = -logsigmoid(argument)`.\n9. Return the weighted mean of the loss over the batch.", "hyperparams": {"alpha": 4.0, "beta": 0.9}, "operators_used": ["logsigmoid", "softplus", "sigmoid", "clamp", "rank_gap"], "implementation_hint": {"expects": ["cost_a", "cost_b", "logp_w", "logp_l", "weight"], "returns": "scalar"}, "code": "import torch\nimport torch.nn.functional as F\n\ndef generated_loss(batch, model_output, extra):\n    \"\"\"\n    Computes the Adaptive Dual-Clip Loss with Scheduled Margin.\n\n    This loss dynamically clips the log-probability difference based on a rank-derived\n    signal and requires it to exceed a separate, sigmoid-scheduled margin.\n\n    Args:\n        batch (dict): A dictionary containing tensors.\n            'cost_a' (Tensor [N]): Cost of solution 'a'.\n            'cost_b' (Tensor [N]): Cost of solution 'b'.\n            'log_prob_w' (Tensor [N]): Log probability of the winning solution.\n            'log_prob_l' (Tensor [N]): Log probability of the losing solution.\n            'weight' (Tensor [N], optional): Per-pair loss weights.\n        model_output (dict): Model outputs (not used).\n        extra (dict): A dictionary for hyperparameters.\n            'alpha' (float): Controls the steepness of the clipping and margin schedules.\n            'beta' (float): Sets the maximum target margin (should be < 1).\n\n    Returns:\n        torch.Tensor: A scalar tensor representing the mean loss.\n    \"\"\"\n\n    # Whitelisted operator implemented as a helper function\n    def rank_gap(x, eps=1e-9):\n        \"\"\"Converts a tensor to its normalized ranks within the range [-0.5, 0.5].\"\"\"\n        if x.numel() <= 1:\n            return torch.zeros_like(x)\n        ranks = x.to(torch.float32).argsort().argsort().to(x.dtype)\n        normalized_ranks = ranks / (x.numel() - 1 + eps)\n        return normalized_ranks - 0.5\n\n    # Retrieve hyperparameters\n    alpha = extra.get('alpha', 4.0)\n    beta = extra.get('beta', 0.9)\n    eps = 1e-9\n\n    # Unpack tensors from the batch\n    cost_a = batch['cost_a']\n    cost_b = batch['cost_b']\n    log_prob_w = batch['log_prob_w']\n    log_prob_l = batch['log_prob_l']\n    weights = batch.get('weight', None)\n\n    # 1. Calculate the difference in log probabilities\n    logp_diff = log_prob_w - log_prob_l\n\n    # 2. Calculate absolute cost difference\n    cost_diff = torch.abs(cost_a - cost_b)\n\n    # 3. Use rank_gap for a stable, scale-invariant cost signal\n    rank_gap_signal = rank_gap(cost_diff)\n\n    # 4. (New Coupling) Calculate the dynamic clipping bound\n    clip_bound = F.softplus(alpha * rank_gap_signal)\n\n    # 5. (Inherited from Parent 1) Dynamically clip logp_diff\n    clipped_logp_diff = torch.clamp(logp_diff, min=-clip_bound, max=clip_bound)\n\n    # 6. (New Coupling) Calculate a separate, scheduled margin\n    scheduled_margin = beta * torch.sigmoid(alpha * rank_gap_signal)\n\n    # 7. (Inherited from Parent 0 & New Coupling) Combine components\n    # Normalize clipped logp_diff to [-1, 1] and compare to the margin.\n    # The outer clip_bound acts as a scaler.\n    # Add epsilon for numerical stability when clip_bound is near zero.\n    normalized_clipped_logp = clipped_logp_diff / (clip_bound + eps)\n    argument = clip_bound * (normalized_clipped_logp - scheduled_margin)\n\n    # 8. Compute the final loss using numerically stable logsigmoid\n    loss_per_item = -F.logsigmoid(argument)\n\n    # 9. Apply optional weights and compute the mean loss\n    if weights is not None:\n        loss = (loss_per_item * weights).sum() / weights.sum().clamp(min=1e-8)\n    else:\n        loss = loss_per_item.mean()\n\n    return loss"}, "static_ok": true, "static_reason": "", "dynamic_ok": true, "dynamic_reason": "ok", "loss_value": 1.0240994691848755, "grad_norm": 0.0}
{"generation": 6, "index": 6, "ir": {"name": "Adaptive Dual-Clip Rank Loss", "intuition": "This loss function creates a stable and adaptive learning signal by dynamically controlling both the target log-probability difference and the gradient scale based on the relative importance of a preference pair. It is designed to prevent the model from over-investing in easy-to-distinguish pairs while scaling up the learning signal for more significant pairs.\n\nInherited Ideas:\n- From `Soft-Clipped Rank-Modulated Loss` (Parent 0): It inherits the use of `tanh(logp_diff)` to soft-clip the log-probability difference. This provides a baseline level of stability by bounding the difference to `[-1, 1]`, preventing extreme gradients from any single pair.\n- From `Rank-Modulated Dual-Clipping Loss` (Parent 1): It inherits the concept of a `rank_modulator` derived from the cost difference (`softplus(alpha * rank_gap(cost_diff))`) that serves as an adaptive margin. The loss aims for the model's preference score to exceed this margin, which is larger for more important pairs.\n\nNew Coupling Ideas & Modifications:\n1.  **Dual-Clipping Mechanism**: This is the core novelty. The loss employs two nested clipping functions. First, the `logp_diff` is soft-clipped with `tanh` for general stability (inherited). Second, this `tanh(logp_diff)` is then hard-clipped using `clamp` with a dynamic upper bound defined by a new `target_clip_value`. This `target_clip_value` is `tanh(beta * rank_modulator)`. This means that for pairs with a small cost difference (low rank), the target for `tanh(logp_diff)` is also small, preventing the model from wasting capacity trying to achieve a large log-probability gap. For high-rank pairs, the target approaches 1, demanding a strong preference signal.\n2.  **Decoupled Gradient Scaling**: The final loss argument is `clipped_score - margin`. Instead of using the `rank_modulator` directly in the loss argument, which affects both the target and the gradient scale, this child uses the `rank_modulator` as a multiplicative scaler on the *entire* logistic loss argument. The argument becomes `rank_modulator * (clipped_score - margin)`. This decouples the target-setting (via `target_clip_value`) from the gradient scaling. The `rank_modulator` now acts purely as a 'learning rate' for each pair, amplifying the loss signal for more important pairs without changing the preference target itself.", "pseudocode": "1. Calculate the difference in log probabilities: `logp_diff = logp_winner - logp_loser`.\n2. Calculate the absolute difference in costs: `cost_diff = abs(cost_winner - cost_loser)`.\n3. Create a normalized, scale-invariant rank signal from the cost difference using the `rank_gap` operator.\n4. (Inherited from Parent 1) Compute a smooth, non-negative `rank_modulator` based on the rank signal: `rank_modulator = softplus(alpha * rank_gap_signal)`.\n5. (New Coupling) Define a dynamic `target_clip_value` using the `rank_modulator`: `target_clip_value = tanh(beta * rank_modulator)`. This value, between 0 and 1, represents the desired upper bound for the `tanh(logp_diff)`.\n6. (Inherited from Parent 0 & New Coupling) Apply a dual-clipping mechanism. First, soft-clip `logp_diff` with `tanh`. Then, hard-clip the result to be no greater than the `target_clip_value`: `clipped_score = clamp(tanh(logp_diff), max=target_clip_value)`.\n7. Define a fixed additive margin, `gamma`.\n8. (New Coupling) Construct the loss argument by scaling the difference between the clipped score and the margin by the `rank_modulator`: `argument = rank_modulator * (clipped_score - gamma)`.\n9. Compute the final loss using a numerically stable logistic loss function: `loss = -logsigmoid(argument)`.\n10. Return the weighted mean of the loss over the batch.", "hyperparams": {"alpha": 4.0, "beta": 1.0, "gamma": 0.5}, "operators_used": ["logsigmoid", "softplus", "tanh", "clamp", "rank_gap"], "implementation_hint": {"expects": ["cost_a", "cost_b", "log_prob_w", "log_prob_l", "weight"], "returns": "scalar"}, "code": "import torch\nimport torch.nn.functional as F\n\ndef generated_loss(batch, model_output, extra):\n    \"\"\"\n    Computes the Adaptive Dual-Clip Rank Loss.\n\n    This loss uses a rank-based modulator to dynamically set a clipping target\n    for the tanh-transformed logp difference, and separately uses the modulator\n    to scale the gradient of the final loss.\n\n    Args:\n        batch (dict): A dictionary containing tensors.\n            'cost_a' (Tensor [N]): Cost of solution 'a'.\n            'cost_b' (Tensor [N]): Cost of solution 'b'.\n            'log_prob_w' (Tensor [N]): Log probability of the winning solution.\n            'log_prob_l' (Tensor [N]): Log probability of the losing solution.\n            'weight' (Tensor [N], optional): Per-pair loss weights.\n        model_output (dict): Model outputs (not used).\n        extra (dict): A dictionary for hyperparameters.\n            'alpha' (float): Controls the magnitude of the rank modulator (gradient scaler).\n            'beta' (float): Controls the sensitivity of the dynamic clipping target.\n            'gamma' (float): A fixed additive margin.\n\n    Returns:\n        torch.Tensor: A scalar tensor representing the mean loss.\n    \"\"\"\n\n    # Whitelisted operator implemented as a helper function\n    def rank_gap(x, eps=1e-9):\n        \"\"\"Converts a tensor to its normalized ranks within the range [-0.5, 0.5].\"\"\"\n        if x.numel() <= 1:\n            return torch.zeros_like(x)\n        ranks = x.to(torch.float32).argsort().argsort().to(x.dtype)\n        normalized_ranks = ranks / (x.numel() - 1 + eps)\n        return normalized_ranks - 0.5\n\n    # Retrieve hyperparameters\n    alpha = extra.get('alpha', 4.0)\n    beta = extra.get('beta', 1.0)\n    gamma = extra.get('gamma', 0.5)\n\n    # Unpack tensors from the batch\n    cost_a = batch['cost_a']\n    cost_b = batch['cost_b']\n    log_prob_w = batch['log_prob_w']\n    log_prob_l = batch['log_prob_l']\n    weights = batch.get('weight', None)\n\n    # 1. Calculate the difference in log probabilities\n    logp_diff = log_prob_w - log_prob_l\n\n    # 2. Calculate absolute cost difference\n    cost_diff = torch.abs(cost_a - cost_b)\n\n    # 3. Create a normalized rank signal\n    rank_gap_signal = rank_gap(cost_diff)\n\n    # 4. (Inherited) Compute the rank modulator\n    rank_modulator = F.softplus(alpha * rank_gap_signal)\n\n    # 5. (New Coupling) Define a dynamic clipping target\n    target_clip_value = torch.tanh(beta * rank_modulator)\n\n    # 6. (Inherited & New Coupling) Apply dual-clipping\n    # First, soft-clip for stability\n    soft_clipped_score = torch.tanh(logp_diff)\n    # Second, hard-clip to the dynamic target\n    clipped_score = torch.clamp(soft_clipped_score, max=target_clip_value)\n\n    # 7. Define a fixed margin\n    margin = gamma\n\n    # 8. (New Coupling) Construct the argument, scaling by the rank modulator\n    argument = rank_modulator * (clipped_score - margin)\n\n    # 9. Compute the final loss\n    loss_per_item = -F.logsigmoid(argument)\n\n    # 10. Apply optional weights and compute the mean loss\n    if weights is not None:\n        # Ensure weights sum is not zero to avoid division by zero\n        weights_sum = weights.sum().clamp(min=1e-8)\n        loss = (loss_per_item * weights).sum() / weights_sum\n    else:\n        loss = loss_per_item.mean()\n\n    return loss"}, "static_ok": true, "static_reason": "", "dynamic_ok": true, "dynamic_reason": "ok", "loss_value": 0.9440773129463196, "grad_norm": 0.0}
{"generation": 6, "index": 7, "ir": {"name": "Adaptive Dual-Modulation Loss", "intuition": "This loss function creates a highly adaptive learning signal by using a unified, rank-based modulator to control both the scale of the log-probability difference and an additive margin. The goal is to dynamically adjust both the target preference strength and the learning rate based on the relative importance of each preference pair within a batch.\n\nInherited Ideas:\n- From `Soft-Clipped Rank-Modulated Loss` (Parent 0): It inherits the core structure of using a rank-based modulator as a **multiplicative scaler** for the log-probability difference. This idea, `scaler * logp_diff`, amplifies the learning signal for pairs with a larger, more significant cost difference.\n- From `Rank-Modulated Dual-Clipping Loss` (Parent 1): It inherits the concept of using a rank-based modulator to define an **additive margin**. The loss requires the scaled log-probability difference to exceed this margin, creating a clear target for the model to achieve.\n\nNew Coupling Ideas & Modifications:\n1.  **Dual-Modulator from a Single Source**: A single `rank_gap` signal derived from the cost difference is used to generate two distinct but coupled modulators. A `scale_modulator` is created using `softplus` to act as a non-negative scaler. A separate `margin_modulator` is created using `sigmoid` and a hyperparameter `beta` to act as a margin bounded between 0 and `beta`. This decouples the magnitude of the scaling from the magnitude of the margin, allowing for more nuanced control, while ensuring both are derived from the same underlying rank importance.\n2.  **Dynamic Margin Scaling**: The final loss argument is `scale_modulator * logp_diff - margin_modulator`. This structure means that for important pairs (high rank), the model must produce a large `logp_diff` which is further amplified by a large `scale_modulator` to overcome a large `margin_modulator`. For less important pairs, both the scaling effect and the margin are small, preventing the model from wasting capacity on fine-grained distinctions and providing a stable, near-zero gradient.", "pseudocode": "1. Calculate the difference in log probabilities: `logp_diff = logp_winner - logp_loser`.\n2. Calculate the absolute difference in costs: `cost_diff = abs(cost_winner - cost_loser)`.\n3. Create a normalized rank signal from the cost differences using the `rank_gap` operator. This signal, ranging from -0.5 to 0.5, is robust to the scale and distribution of costs.\n4. (New Coupling) Generate two distinct modulators from the single rank signal:\n   a. (Inherited from Parent 0) Create a `scale_modulator` using `softplus`: `scale_modulator = softplus(alpha * rank_gap_signal)`. This produces a smooth, non-negative scaler that increases with the pair's importance.\n   b. (Inherited from Parent 1) Create a `margin_modulator` using `sigmoid`: `margin_modulator = beta * sigmoid(alpha * rank_gap_signal)`. This produces a smooth margin bounded between 0 and `beta`.\n5. Construct the loss argument by combining the scaled log-probability difference and the adaptive margin: `argument = scale_modulator * logp_diff - margin_modulator`. The goal is for the scaled difference to exceed the margin.\n6. Compute the final loss using a numerically stable logistic loss: `loss = -logsigmoid(argument)`.\n7. Return the weighted mean of the loss over the batch.", "hyperparams": {"alpha": 4.0, "beta": 1.0}, "operators_used": ["logsigmoid", "softplus", "sigmoid", "rank_gap"], "implementation_hint": {"expects": ["cost_a", "cost_b", "logp_a", "logp_b"], "returns": "scalar"}, "code": "import torch\nimport torch.nn.functional as F\n\ndef generated_loss(batch, model_output, extra):\n    \"\"\"\n    Computes the Adaptive Dual-Modulation Loss.\n\n    This loss uses a single rank-based signal to generate two separate modulators:\n    one for scaling the log-probability difference and one for an adaptive margin.\n\n    Args:\n        batch (dict): A dictionary containing tensors.\n            'cost_a' (Tensor [N]): Cost of solution 'a'.\n            'cost_b' (Tensor [N]): Cost of solution 'b'.\n            'log_prob_w' (Tensor [N]): Log probability of the winning solution.\n            'log_prob_l' (Tensor [N]): Log probability of the losing solution.\n            'weight' (Tensor [N], optional): Per-pair loss weights.\n        model_output (dict): Model outputs (not used).\n        extra (dict): A dictionary for hyperparameters.\n            'alpha' (float): Controls the sensitivity of the modulators to rank.\n            'beta' (float): Sets the maximum value for the adaptive margin.\n\n    Returns:\n        torch.Tensor: A scalar tensor representing the mean loss.\n    \"\"\"\n\n    # Whitelisted operator implemented as a helper function\n    def rank_gap(x, eps=1e-9):\n        \"\"\"Converts a tensor to its normalized ranks within the range [-0.5, 0.5].\"\"\"\n        if x.numel() <= 1:\n            return torch.zeros_like(x)\n        ranks = x.to(torch.float32).argsort().argsort().to(x.dtype)\n        normalized_ranks = ranks / (x.numel() - 1 + eps)\n        return normalized_ranks - 0.5\n\n    # Retrieve hyperparameters\n    alpha = extra.get('alpha', 4.0)\n    beta = extra.get('beta', 1.0)\n\n    # Unpack tensors from the batch\n    cost_a = batch['cost_a']\n    cost_b = batch['cost_b']\n    log_prob_w = batch['log_prob_w']\n    log_prob_l = batch['log_prob_l']\n    weights = batch.get('weight', None)\n\n    # 1. Calculate the difference in log probabilities\n    logp_diff = log_prob_w - log_prob_l\n\n    # 2. Calculate absolute cost difference\n    cost_diff = torch.abs(cost_a - cost_b)\n\n    # 3. Create a stable, scale-invariant rank signal\n    rank_gap_signal = rank_gap(cost_diff)\n\n    # 4. (New Coupling) Generate two modulators from the rank signal\n    # (Inherited from Parent 0) A non-negative scaler for the logp_diff\n    scale_modulator = F.softplus(alpha * rank_gap_signal)\n    \n    # (Inherited from Parent 1) A bounded margin between 0 and beta\n    margin_modulator = beta * torch.sigmoid(alpha * rank_gap_signal)\n\n    # 5. Construct the loss argument: scaled_logp_diff > margin\n    argument = scale_modulator * logp_diff - margin_modulator\n\n    # 6. Compute the final loss using numerically stable logsigmoid\n    loss_per_item = -F.logsigmoid(argument)\n\n    # 7. Apply optional weights and compute the mean loss\n    if weights is not None and weights.sum() > 0:\n        loss = (loss_per_item * weights).sum() / weights.sum()\n    else:\n        loss = loss_per_item.mean()\n\n    return loss"}, "static_ok": true, "static_reason": "", "dynamic_ok": true, "dynamic_reason": "ok", "loss_value": 0.9817067384719849, "grad_norm": 0.0}
{"generation": 7, "index": 0, "ir": {"name": "Adaptive Dual-Clip Rank Loss", "intuition": "This loss function creates a stable and adaptive learning signal by dynamically controlling both the target margin and the gradient scale based on the relative importance of a preference pair. It is designed to prevent the model from over-investing in easy-to-distinguish pairs while scaling up the learning signal for pairs with a significant quality difference.\n\nInherited Ideas:\n- From `Soft-Clipped Rank-Modulated Loss` (Parent 0): It inherits the use of `tanh` to soft-clip a log-probability term. However, instead of clipping the raw `logp_diff`, it clips a margin-adjusted difference, `tanh((logp_diff - margin)/tau)`. This bounds the core argument of the loss, ensuring stable gradients regardless of how large the log-probability difference or margin become.\n- From `Rank-Modulated Dual-Clipping Loss` (Parent 1): It inherits the concept of using a dynamic, rank-based additive margin. The loss requires `logp_diff` to be greater than a `margin` derived from the `rank_gap` of the cost difference. This ensures the model's preference strength adapts to the pair's importance.\n\nNew Coupling Ideas & Modifications:\n1. **Z-Scored Rank Signal**: Instead of directly using the `rank_gap` signal, this child loss first z-scores it (`zscore(rank_gap(cost_diff))`). This centers the signal around zero with a standard deviation of one, making the hyperparameter `alpha` less sensitive to the batch size and the distribution of cost differences. It provides a more standardized measure of a pair's importance.\n2. **Dynamic Temperature Scaling**: A new coupling is introduced where a dynamic temperature `tau` is derived from the same z-scored rank signal: `tau = softplus(beta * z_rank_signal) + eps`. This temperature scales the `tanh` argument. For important pairs (high rank), `tau` is larger, which flattens the `tanh` curve and reduces the gradient, preventing the model from becoming overconfident. For less important pairs (low rank), `tau` is smaller, sharpening the `tanh` curve and providing a stronger gradient to enforce the margin. This acts as a form of dynamic gradient regularization.", "pseudocode": "1. Calculate the difference in log probabilities: `logp_diff = logp_winner - logp_loser`.\n2. Calculate the absolute difference in costs: `cost_diff = abs(cost_winner - cost_loser)`.\n3. (New Coupling) Convert cost differences to a normalized rank signal using `rank_gap`.\n4. (New Coupling) Standardize the rank signal by applying `zscore` to it, creating `z_rank_signal`. This centers the importance signal around 0 with a standard deviation of 1.\n5. (Inherited from Parent 1) Calculate an adaptive additive margin using the standardized rank signal: `margin = softplus(alpha * z_rank_signal)`.\n6. (New Coupling) Calculate a dynamic temperature `tau` from the same signal: `tau = softplus(beta * z_rank_signal) + eps`. This temperature will control the sharpness of the `tanh` function.\n7. (Inherited from Parent 0) Compute the margin-adjusted log-probability difference, scale it by the dynamic temperature `tau`, and apply a soft-clipping `tanh`: `clipped_argument = tanh((logp_diff - margin) / tau)`.\n8. Compute the final loss using a numerically stable logistic loss. The target is for `clipped_argument` to be positive, so the loss is based on `-logsigmoid(clipped_argument)`.\n9. Return the weighted mean of the loss over the batch.", "hyperparams": {"alpha": 1.0, "beta": 0.5}, "operators_used": ["logsigmoid", "softplus", "tanh", "zscore", "rank_gap"], "implementation_hint": {"expects": ["cost_a", "cost_b", "logp_a", "logp_b"], "returns": "scalar"}, "code": "import torch\nimport torch.nn.functional as F\n\ndef generated_loss(batch, model_output, extra):\n    \"\"\"\n    Computes the Adaptive Dual-Clip Rank Loss.\n\n    This loss uses a z-scored rank signal to create both an adaptive margin and a\n    dynamic temperature for scaling a tanh-clipped, margin-adjusted log-probability difference.\n\n    Args:\n        batch (dict): A dictionary containing tensors.\n            'cost_a' (Tensor [N]): Cost of solution 'a'.\n            'cost_b' (Tensor [N]): Cost of solution 'b'.\n            'log_prob_w' (Tensor [N]): Log probability of the winning solution.\n            'log_prob_l' (Tensor [N]): Log probability of the losing solution.\n            'weight' (Tensor [N], optional): Per-pair loss weights.\n        model_output (dict): Model outputs (not used).\n        extra (dict): A dictionary for hyperparameters.\n            'alpha' (float): Controls the magnitude of the adaptive margin.\n            'beta' (float): Controls the magnitude of the dynamic temperature.\n\n    Returns:\n        torch.Tensor: A scalar tensor representing the mean loss.\n    \"\"\"\n    # Whitelisted operators implemented as helper functions\n    def rank_gap(x, eps=1e-9):\n        \"\"\"Converts a tensor to its normalized ranks within the range [-0.5, 0.5].\"\"\"\n        if x.numel() <= 1:\n            return torch.zeros_like(x)\n        ranks = x.to(torch.float32).argsort().argsort().to(x.dtype)\n        normalized_ranks = ranks / (x.numel() - 1 + eps)\n        return normalized_ranks - 0.5\n\n    def zscore(x, eps=1e-8):\n        \"\"\"Applies z-score normalization to a tensor.\"\"\"\n        if x.numel() <= 1:\n            return torch.zeros_like(x)\n        mean = x.mean()\n        std = x.std().clamp(min=eps)\n        return (x - mean) / std\n\n    # Retrieve hyperparameters\n    alpha = extra.get('alpha', 1.0)\n    beta = extra.get('beta', 0.5)\n    eps = 1e-8\n\n    # Unpack tensors from the batch\n    cost_a = batch['cost_a']\n    cost_b = batch['cost_b']\n    log_prob_w = batch['log_prob_w']\n    log_prob_l = batch['log_prob_l']\n    weights = batch.get('weight', None)\n\n    # 1. Calculate the difference in log probabilities\n    logp_diff = log_prob_w - log_prob_l\n\n    # 2. Calculate absolute cost difference\n    cost_diff = torch.abs(cost_a - cost_b)\n\n    # 3. & 4. (New Coupling) Create a z-scored rank signal for standardized importance\n    rank_gap_signal = rank_gap(cost_diff)\n    z_rank_signal = zscore(rank_gap_signal)\n\n    # 5. (Inherited from Parent 1) Calculate adaptive margin from the standardized signal\n    margin = F.softplus(alpha * z_rank_signal)\n\n    # 6. (New Coupling) Calculate dynamic temperature for gradient scaling\n    # The temperature is larger for more important pairs (positive z_rank_signal)\n    tau = F.softplus(beta * z_rank_signal) + eps\n\n    # 7. (Inherited from Parent 0) Compute the margin-adjusted, temp-scaled, and soft-clipped argument\n    # We want logp_diff > margin. The tanh clips the error signal for stability.\n    clipped_argument = torch.tanh((logp_diff - margin) / tau)\n\n    # 8. Compute the final loss using numerically stable logsigmoid\n    # The argument to logsigmoid should be positive for low loss.\n    loss_per_item = -F.logsigmoid(clipped_argument)\n\n    # 9. Apply optional weights and compute the mean loss\n    if weights is not None:\n        loss = (loss_per_item * weights).sum() / weights.sum().clamp(min=1e-8)\n    else:\n        loss = loss_per_item.mean()\n\n    return loss"}, "static_ok": true, "static_reason": "", "dynamic_ok": true, "dynamic_reason": "ok", "loss_value": 1.1259186267852783, "grad_norm": 0.0}
{"generation": 7, "index": 1, "ir": {"name": "Adaptive Dual-Clip Log-Ratio Loss", "intuition": "This loss function introduces a stable, adaptive learning target by blending dynamic clipping with a rank-based log-ratio objective. It is designed to be robust to the scale of both costs and log-probabilities.\n\nInherited Ideas:\n- From Parent 1 (`Rank-Modulated Dual-Clipping Loss`): It inherits the concept of **dynamic clipping**, where the `logp_diff` is clamped within a range defined by a cost-derived signal. This focuses the model's effort proportionally to the importance of a preference pair, preventing it from wasting capacity on insignificant pairs while allowing a strong signal for important ones.\n- From Parent 0 (`Soft-Clipped Rank-Modulated Loss`): It inherits the idea of using a **multiplicative scaler** derived from a rank-based signal. This scaler amplifies the learning signal for pairs with a larger, more significant cost difference, demanding a higher level of confidence from the model.\n\nNew Coupling Ideas & Modifications:\n1.  **Z-Score Normalization for Log-Probabilities**: Before any other operation, the raw `logp_diff` is normalized using z-scoring (`(x - mean) / std`). This new coupling step stabilizes the input to the loss function, making it robust to shifts and scales in the model's output log-probabilities, which can vary significantly during training. This pre-processing ensures the subsequent clipping and scaling operations work on a consistent distribution.\n2.  **Log-Ratio Objective**: Instead of a simple difference `(clipped_logp_diff - margin)`, this loss uses a log-ratio formulation: `log(sigmoid(clipped_logp_diff))`. This reframes the objective to maximizing the logistic probability of the clipped difference. It has the desirable property of being asymmetric, heavily penalizing negative `clipped_logp_diff` while providing a diminishing but still positive gradient for positive values. The final loss argument becomes `scaler * log(sigmoid(clipped_logp_diff))`, where the scaler demands a higher logistic probability for more important pairs.", "pseudocode": "1. Calculate the difference in log probabilities: `logp_diff = logp_winner - logp_loser`.\n2. (New Coupling) Normalize the `logp_diff` across the batch using z-scoring to stabilize its distribution: `normalized_logp_diff = zscore(logp_diff)`.\n3. Calculate the absolute difference in costs: `cost_diff = abs(cost_winner - cost_loser)`.\n4. Create a rank-based signal from the cost difference using `rank_gap` for scale-invariance.\n5. Compute a smooth, non-negative `rank_modulator` using `softplus` on the scaled rank signal: `rank_modulator = softplus(alpha * rank_gap_signal)`.\n6. (Inherited from Parent 1) Dynamically clip the normalized log-probability difference using the `rank_modulator`: `clipped_logp_diff = clamp(normalized_logp_diff, min=-rank_modulator, max=rank_modulator)`.\n7. (Inherited from Parent 0 & New Coupling) Formulate the loss objective as a scaled log-ratio. The `rank_modulator` acts as a multiplicative scaler on the log-sigmoid of the clipped difference: `argument = rank_modulator * log(sigmoid(clipped_logp_diff))`.\n8. Compute the final loss using a numerically stable logistic loss function (since the goal is to maximize the argument, we use -logsigmoid): `loss = -logsigmoid(argument)`.\n9. Return the weighted mean of the loss over the batch.", "hyperparams": {"alpha": 4.0, "zscore_eps": 1e-08}, "operators_used": ["logsigmoid", "sigmoid", "log", "clamp", "softplus", "rank_gap", "zscore"], "implementation_hint": {"expects": ["cost_a", "cost_b", "logp_w", "logp_l"], "returns": "scalar"}, "code": "import torch\nimport torch.nn.functional as F\n\ndef generated_loss(batch, model_output, extra):\n    \"\"\"\n    Computes the Adaptive Dual-Clip Log-Ratio Loss.\n\n    This loss z-scores the logp_diff for stability, then uses a rank-based modulator\n    to both dynamically clip the normalized logp_diff and to scale a log-ratio objective.\n\n    Args:\n        batch (dict): A dictionary containing tensors.\n            'cost_a' (Tensor [N]): Cost of solution 'a'.\n            'cost_b' (Tensor [N]): Cost of solution 'b'.\n            'log_prob_w' (Tensor [N]): Log probability of the winning solution.\n            'log_prob_l' (Tensor [N]): Log probability of the losing solution.\n            'weight' (Tensor [N], optional): Per-pair loss weights.\n        model_output (dict): Model outputs (not used).\n        extra (dict): A dictionary for hyperparameters.\n            'alpha' (float): Controls the magnitude of the rank modulator.\n            'zscore_eps' (float): Epsilon for z-score normalization to prevent division by zero.\n\n    Returns:\n        torch.Tensor: A scalar tensor representing the mean loss.\n    \"\"\"\n\n    # Whitelisted operators implemented as helper functions\n    def rank_gap(x, eps=1e-9):\n        if x.numel() <= 1:\n            return torch.zeros_like(x)\n        ranks = x.to(torch.float32).argsort().argsort().to(x.dtype)\n        normalized_ranks = ranks / (x.numel() - 1 + eps)\n        return normalized_ranks - 0.5\n\n    def zscore(x, eps=1e-8):\n        if x.numel() <= 1:\n            return torch.zeros_like(x)\n        mean = x.mean()\n        std = x.std().clamp(min=eps)\n        return (x - mean) / std\n\n    # Retrieve hyperparameters\n    alpha = extra.get('alpha', 4.0)\n    zscore_eps = extra.get('zscore_eps', 1e-8)\n\n    # Unpack tensors from the batch\n    cost_a = batch['cost_a']\n    cost_b = batch['cost_b']\n    log_prob_w = batch['log_prob_w']\n    log_prob_l = batch['log_prob_l']\n    weights = batch.get('weight', None)\n\n    # 1. Calculate the difference in log probabilities\n    logp_diff = log_prob_w - log_prob_l\n\n    # 2. (New Coupling) Normalize logp_diff for stability\n    normalized_logp_diff = zscore(logp_diff, eps=zscore_eps)\n\n    # 3. Calculate absolute cost difference\n    cost_diff = torch.abs(cost_a - cost_b)\n\n    # 4. Create a rank-based signal\n    rank_gap_signal = rank_gap(cost_diff)\n\n    # 5. Compute the rank modulator\n    rank_modulator = F.softplus(alpha * rank_gap_signal)\n\n    # 6. (Inherited from Parent 1) Dynamically clip the normalized logp_diff\n    clipped_logp_diff = torch.clamp(normalized_logp_diff, min=-rank_modulator, max=rank_modulator)\n\n    # 7. (Inherited from Parent 0 & New Coupling) Formulate a scaled log-ratio objective\n    # We use log(sigmoid(x)) which is numerically stable as F.logsigmoid(x).\n    log_ratio = F.logsigmoid(clipped_logp_diff)\n    argument = rank_modulator * log_ratio\n\n    # 8. Compute the final loss using a numerically stable logistic loss\n    # We want to maximize the argument, so we use -logsigmoid.\n    loss_per_item = -F.logsigmoid(argument)\n\n    # 9. Apply optional weights and compute the mean loss\n    if weights is not None:\n        loss = (loss_per_item * weights).sum() / weights.sum().clamp(min=1e-8)\n    else:\n        loss = loss_per_item.mean()\n\n    return loss"}, "static_ok": true, "static_reason": "", "dynamic_ok": true, "dynamic_reason": "ok", "loss_value": 1.05809485912323, "grad_norm": 0.0}
{"generation": 7, "index": 2, "ir": {"name": "Adaptive Dual-Clip Rank Loss", "intuition": "This loss function creates a highly adaptive learning signal by dynamically controlling both the target margin and the gradient scale based on the relative importance of a preference pair. It combines the stability of soft-clipping with the adaptivity of dynamic hard-clipping.\n\nInherited Ideas:\n- From `Soft-Clipped Rank-Modulated Loss` (Parent 0): It inherits the use of a **soft-clipped log-probability difference** `tanh(logp_diff)`. This provides a stable, bounded signal in `[-1, 1]` that focuses the model on getting the preference direction correct, preventing extreme gradients.\n- From `Rank-Modulated Dual-Clipping Loss` (Parent 1): It inherits the concept of **dynamic hard-clipping** of the `logp_diff` using `clamp`. This mechanism prevents the model from wasting capacity on pairs with small cost differences by enforcing a tighter bound, while allowing a larger learning signal for more important pairs.\n\nNew Coupling Ideas & Modifications:\n1.  **Dual-Clipping Mechanism**: This is the core novelty. The loss uses *both* `tanh` and `clamp` on the `logp_diff`. The `tanh` provides a baseline stable signal, which is then further constrained by a dynamic `clamp`. The clamping bounds `[-M, M]` are determined by a rank-based modulator `M`, but they are applied to the *already bounded* `tanh(logp_diff)`. This creates a two-stage clipping that is both smooth and adaptive, preventing the model from pushing `logp_diff` to infinity while still responding to the importance of the pair.\n2.  **Decoupled Margin and Clipping Modulator**: A unified rank-based signal `rank_modulator = softplus(alpha * rank_gap(cost_diff))` is generated. However, this modulator is used in two distinct ways. It serves directly as the **additive margin** the model must overcome. A separate, scaled-down version `beta * rank_modulator` (with `beta < 1`) is used to define the dynamic clamping range. This decoupling ensures the clamping range is always strictly smaller than the target margin, forcing the model to push the `clipped_logp_diff` to its dynamic upper bound to satisfy the preference, creating a more focused learning objective.", "pseudocode": "1. Calculate the difference in log probabilities: `logp_diff = logp_winner - logp_loser`.\n2. Calculate the absolute difference in costs: `cost_diff = abs(cost_winner - cost_loser)`.\n3. Create a normalized, scale-invariant rank signal from the cost difference using the `rank_gap` operator.\n4. (Inherited & New Coupling) Generate a unified, smooth, non-negative `rank_modulator` from the rank signal: `rank_modulator = softplus(alpha * rank_gap_signal)`.\n5. (Inherited from Parent 0) First, apply a soft-clip to the log-probability difference using `tanh` for baseline stability: `soft_clipped_logp_diff = tanh(logp_diff)`.\n6. (Inherited from Parent 1 & New Coupling) Define a dynamic clipping range using a scaled version of the modulator: `clip_bound = beta * rank_modulator`. Then, apply a hard clip to the *soft-clipped* difference: `dual_clipped_logp_diff = clamp(soft_clipped_logp_diff, min=-clip_bound, max=clip_bound)`.\n7. Define the additive margin to be the original, unscaled `rank_modulator`.\n8. Formulate the loss argument by subtracting the margin from the dual-clipped log-probability difference: `argument = dual_clipped_logp_diff - rank_modulator`.\n9. Compute the final loss using a numerically stable logistic loss: `loss = -logsigmoid(argument)`.\n10. Return the weighted mean of the loss over the batch.", "hyperparams": {"alpha": 4.0, "beta": 0.9}, "operators_used": ["logsigmoid", "softplus", "tanh", "clamp", "rank_gap"], "implementation_hint": {"expects": ["cost_a", "cost_b", "logp_w", "logp_l", "weight"], "returns": "scalar"}, "code": "import torch\nimport torch.nn.functional as F\n\ndef generated_loss(batch, model_output, extra):\n    \"\"\"\n    Computes the Adaptive Dual-Clip Rank Loss.\n\n    This loss combines tanh soft-clipping with a dynamic hard-clipping mechanism.\n    A rank-based modulator defines both the additive margin and a (scaled) clamping bound,\n    creating a focused and stable learning objective.\n\n    Args:\n        batch (dict): A dictionary containing tensors.\n            'cost_a' (Tensor [N]): Cost of solution 'a'.\n            'cost_b' (Tensor [N]): Cost of solution 'b'.\n            'log_prob_w' (Tensor [N]): Log probability of the winning solution.\n            'log_prob_l' (Tensor [N]): Log probability of the losing solution.\n            'weight' (Tensor [N], optional): Per-pair loss weights.\n        model_output (dict): Model outputs (not used).\n        extra (dict): A dictionary for hyperparameters.\n            'alpha' (float): Controls the magnitude of the rank modulator and margin.\n            'beta' (float): Scales the modulator to define the clamp bound, must be < 1.0.\n\n    Returns:\n        torch.Tensor: A scalar tensor representing the mean loss.\n    \"\"\"\n\n    # Whitelisted operator implemented as a helper function\n    def rank_gap(x, eps=1e-9):\n        \"\"\"Converts a tensor to its normalized ranks within the range [-0.5, 0.5].\"\"\"\n        if x.numel() <= 1:\n            return torch.zeros_like(x)\n        ranks = x.to(torch.float32).argsort().argsort().to(x.dtype)\n        normalized_ranks = ranks / (x.numel() - 1 + eps)\n        return normalized_ranks - 0.5\n\n    # Retrieve hyperparameters\n    alpha = extra.get('alpha', 4.0)\n    beta = extra.get('beta', 0.9)\n\n    # Unpack tensors from the batch\n    cost_a = batch['cost_a']\n    cost_b = batch['cost_b']\n    log_prob_w = batch['log_prob_w']\n    log_prob_l = batch['log_prob_l']\n    weights = batch.get('weight', None)\n\n    # 1. Calculate the difference in log probabilities\n    logp_diff = log_prob_w - log_prob_l\n\n    # 2. Calculate absolute cost difference\n    cost_diff = torch.abs(cost_a - cost_b)\n\n    # 3. Create a normalized rank signal\n    rank_gap_signal = rank_gap(cost_diff)\n\n    # 4. Generate the unified rank modulator\n    rank_modulator = F.softplus(alpha * rank_gap_signal)\n\n    # 5. (Inherited from Parent 0) Apply baseline soft-clipping for stability\n    soft_clipped_logp_diff = torch.tanh(logp_diff)\n\n    # 6. (Inherited from Parent 1 & New Coupling) Apply dynamic hard-clipping\n    # The clipping bound is a scaled version of the modulator.\n    clip_bound = beta * rank_modulator\n    # The hard clip is applied to the already soft-clipped value.\n    dual_clipped_logp_diff = torch.clamp(soft_clipped_logp_diff, min=-clip_bound, max=clip_bound)\n\n    # 7. The margin is the original, unscaled modulator\n    additive_margin = rank_modulator\n\n    # 8. Formulate the loss argument\n    # We want dual_clipped_logp_diff > additive_margin.\n    argument = dual_clipped_logp_diff - additive_margin\n\n    # 9. Compute the final loss using numerically stable logsigmoid\n    loss_per_item = -F.logsigmoid(argument)\n\n    # 10. Apply optional weights and compute the mean loss\n    if weights is not None:\n        loss = (loss_per_item * weights).sum() / weights.sum().clamp(min=1e-8)\n    else:\n        loss = loss_per_item.mean()\n\n    return loss"}, "static_ok": true, "static_reason": "", "dynamic_ok": true, "dynamic_reason": "ok", "loss_value": 1.2550013065338135, "grad_norm": 0.0}
{"generation": 7, "index": 3, "ir": {"name": "Adaptive Dual-Modulated Loss", "intuition": "This loss function creates a highly adaptive learning signal by modulating both the log-probability difference and the target margin using a unified, rank-based signal. It combines the stability of soft-clipping with the adaptivity of dynamic margins and scaling.\n\nInherited Ideas:\n- From `Soft-Clipped Rank-Modulated Loss` (Parent 0): It inherits the use of `tanh` to **soft-clip the log-probability difference**. This provides inherent stability by bounding the log-probability term to `[-1, 1]`, preventing extreme gradients regardless of the raw log-probability values.\n- From `Rank-Modulated Dual-Clipping Loss` (Parent 1): It inherits the idea of using a rank-based signal to create an **additive margin**. The loss requires the model's preference score to exceed a margin `M`, where `M` is derived from the relative importance of the preference pair within the batch.\n\nNew Coupling Ideas & Modifications:\n1.  **Unified Rank Modulator**: Like its parents, this loss creates a single, unified `rank_modulator` from the cost difference: `rank_modulator = softplus(alpha * rank_gap(cost_diff))`. This modulator is a smooth, non-negative, and scale-invariant signal of the pair's importance.\n2.  **Coupled Scaling and Margin**: The `rank_modulator` is coupled to the loss in two distinct but related ways. It acts as a multiplicative `scaler` on the `tanh(logp_diff)` term, amplifying the learning signal for more important pairs. It also serves as the `additive_margin`. The final loss argument becomes `scaler * tanh(logp_diff) - additive_margin`, or `rank_modulator * tanh(logp_diff) - rank_modulator`. This simplifies to `rank_modulator * (tanh(logp_diff) - 1)`, creating a structure that demands the `tanh(logp_diff)` term get progressively closer to its maximum value of 1 as the importance of the pair (and thus the `rank_modulator` value) increases.", "pseudocode": "1. Calculate the difference in log probabilities: `logp_diff = logp_winner - logp_loser`.\n2. (Inherited from Parent 0) Soft-clip the log-probability difference using `tanh` for stability: `clipped_logp_diff = tanh(logp_diff)`.\n3. Calculate the absolute difference in costs: `cost_diff = abs(cost_winner - cost_loser)`.\n4. (New Coupling) Convert the cost differences into a normalized rank signal between -0.5 and 0.5 using the `rank_gap` operator. This makes the signal robust to cost scale and outliers.\n5. (New Coupling) Create a single, unified `rank_modulator` by applying `softplus` to the scaled rank signal: `rank_modulator = softplus(alpha * rank_gap_signal)`. This results in a smooth, non-negative value that increases with the rank of the cost difference.\n6. (Inherited from Parents & New Coupling) Use the `rank_modulator` for two roles simultaneously. It acts as a multiplicative scaler on the `clipped_logp_diff` and as the additive margin. The loss argument is `scaler * clipped_logp_diff - margin`, which becomes `rank_modulator * clipped_logp_diff - rank_modulator`.\n7. Simplify the argument to `rank_modulator * (clipped_logp_diff - 1)`.\n8. Compute the final loss using a numerically stable logistic loss: `loss = -logsigmoid(argument)`.\n9. Return the weighted mean of the loss over the batch.", "hyperparams": {"alpha": 4.0}, "operators_used": ["logsigmoid", "softplus", "tanh", "rank_gap"], "implementation_hint": {"expects": ["cost_a", "cost_b", "logp_w", "logp_l", "weight"], "returns": "scalar"}, "code": "import torch\nimport torch.nn.functional as F\n\ndef generated_loss(batch, model_output, extra):\n    \"\"\"\n    Computes the Adaptive Dual-Modulated Loss.\n\n    This loss combines tanh-clipping with a unified, rank-based modulator that\n    simultaneously acts as a multiplicative scaler and an additive margin.\n\n    Args:\n        batch (dict): A dictionary containing tensors.\n            'cost_a' (Tensor [N]): Cost of solution 'a'.\n            'cost_b' (Tensor [N]): Cost of solution 'b'.\n            'log_prob_w' (Tensor [N]): Log probability of the winning solution.\n            'log_prob_l' (Tensor [N]): Log probability of the losing solution.\n            'weight' (Tensor [N], optional): Per-pair loss weights.\n        model_output (dict): Model outputs (not used).\n        extra (dict): A dictionary for hyperparameters.\n            'alpha' (float): Controls the magnitude of the rank modulator.\n\n    Returns:\n        torch.Tensor: A scalar tensor representing the mean loss.\n    \"\"\"\n\n    # Whitelisted operator implemented as a helper function\n    def rank_gap(x, eps=1e-9):\n        \"\"\"Converts a tensor to its normalized ranks within the range [-0.5, 0.5].\"\"\"\n        if x.numel() <= 1:\n            return torch.zeros_like(x)\n        ranks = x.to(torch.float32).argsort().argsort().to(x.dtype)\n        # Normalize ranks to [0, 1]\n        normalized_ranks = ranks / (x.numel() - 1 + eps)\n        # Shift to [-0.5, 0.5]\n        return normalized_ranks - 0.5\n\n    # Retrieve hyperparameters\n    alpha = extra.get('alpha', 4.0)\n\n    # Unpack tensors from the batch\n    cost_a = batch['cost_a']\n    cost_b = batch['cost_b']\n    log_prob_w = batch['log_prob_w']\n    log_prob_l = batch['log_prob_l']\n    weights = batch.get('weight', None)\n\n    # 1. Calculate the difference in log probabilities\n    logp_diff = log_prob_w - log_prob_l\n\n    # 2. (Inherited from Parent 0) Soft-clip for stability\n    clipped_logp_diff = torch.tanh(logp_diff)\n\n    # 3. Calculate absolute cost difference\n    cost_diff = torch.abs(cost_a - cost_b)\n\n    # 4. (New Coupling) Use rank_gap for a stable, scale-invariant cost signal\n    rank_gap_signal = rank_gap(cost_diff)\n\n    # 5. (New Coupling) Create a unified rank modulator\n    rank_modulator = F.softplus(alpha * rank_gap_signal)\n\n    # 6. (Inherited & New Coupling) Use the modulator for dual roles:\n    # We want: scaler * clipped_logp_diff > margin\n    # where scaler = rank_modulator and margin = rank_modulator.\n    # This becomes: rank_modulator * clipped_logp_diff > rank_modulator\n    # which simplifies to the argument below.\n    argument = rank_modulator * (clipped_logp_diff - 1)\n\n    # 8. Compute the final loss using numerically stable logsigmoid\n    loss_per_item = -F.logsigmoid(argument)\n\n    # 9. Apply optional weights and compute the mean loss\n    if weights is not None and weights.numel() > 0:\n        loss = (loss_per_item * weights).sum() / weights.sum().clamp(min=1e-8)\n    else:\n        loss = loss_per_item.mean()\n\n    return loss"}, "static_ok": true, "static_reason": "", "dynamic_ok": true, "dynamic_reason": "ok", "loss_value": 1.2550013065338135, "grad_norm": 0.0}
{"generation": 7, "index": 4, "ir": {"name": "Adaptive Dual-Signal Loss", "intuition": "This loss function creates a robust preference learning signal by combining two distinct, adaptively weighted components based on the relative importance of a preference pair. It is designed to be stable across different cost scales and to provide a rich gradient signal.\n\nInherited Ideas:\n- From `Soft-Clipped Rank-Modulated Loss` (Parent 0): It inherits the use of `tanh` to **soft-clip the log-probability difference**. This bounds the `logp_diff` to `[-1, 1]`, ensuring numerical stability and preventing oversized gradients from dominating the learning process.\n- From `Rank-Modulated Dual-Clipping Loss` (Parent 1): It inherits the idea of using `rank_gap` on the cost difference to create a **smooth, non-negative, and scale-invariant modulator** via `softplus(alpha * rank_gap(cost_diff))`. This modulator, which we call `rank_weight`, reflects the relative importance of each preference pair within the batch.\n\nNew Coupling Ideas & Modifications:\n1.  **Dual-Signal Structure**: The core innovation is to create two separate loss signals and combine them using the `rank_weight` as a blending factor. \n    - The first signal, `loss_margin`, is a standard margin-based loss (`tanh(logp_diff) - 1`), which is most effective for high-importance pairs where a large log-probability gap is desirable. \n    - The second signal, `loss_ratio`, is a ratio-based loss (`logsigmoid(logp_diff)`) that focuses on getting the sign of the preference correct, which is crucial for all pairs, especially those with small cost differences.\n2.  **Rank-Weighted Blending**: The `rank_weight` is coupled with a `sigmoid` function to create a smooth blending coefficient `beta` that ranges from 0 to 1. The final loss is a convex combination: `beta * loss_margin + (1 - beta) * loss_ratio`. This means that for high-rank pairs (large cost difference), the loss is dominated by the `loss_margin` component, pushing for a large log-probability gap. For low-rank pairs (small cost difference), the loss is dominated by the `loss_ratio` component, focusing on simply getting the preference direction right without enforcing a large margin.", "pseudocode": "1. Calculate the difference in log probabilities: `logp_diff = logp_winner - logp_loser`.\n2. (Inherited from Parent 0) Soft-clip the log-probability difference using `tanh`: `clipped_logp_diff = tanh(logp_diff)`.\n3. Calculate the absolute difference in costs: `cost_diff = abs(cost_winner - cost_loser)`.\n4. (Inherited from Parent 1) Create a scale-invariant, non-negative `rank_weight` by applying `softplus` to the normalized rank of the cost difference: `rank_weight = softplus(alpha * rank_gap(cost_diff))`.\n5. (New Coupling) Create two distinct loss signals:\n   a. A margin-based signal, `loss_margin`, which encourages the `clipped_logp_diff` to be close to 1: `loss_margin = 1 - clipped_logp_diff`.\n   b. A ratio-based signal, `loss_ratio`, which encourages `logp_diff` to be positive: `loss_ratio = -logsigmoid(logp_diff)`.\n6. (New Coupling) Create a smooth blending coefficient `beta` by passing the `rank_weight` through a sigmoid function: `beta = sigmoid(rank_weight)`. Beta will be close to 1 for high-rank pairs and close to 0.5 for low-rank pairs.\n7. Combine the two loss signals using `beta` as the blending factor: `loss = beta * loss_margin + (1 - beta) * loss_ratio`.\n8. Return the weighted mean of the combined loss over the batch.", "hyperparams": {"alpha": 4.0}, "operators_used": ["logsigmoid", "softplus", "sigmoid", "tanh", "rank_gap"], "implementation_hint": {"expects": ["cost_a", "cost_b", "logp_w", "logp_l"], "returns": "scalar"}, "code": "import torch\nimport torch.nn.functional as F\n\ndef generated_loss(batch, model_output, extra):\n    \"\"\"\n    Computes the Adaptive Dual-Signal Loss.\n\n    This loss dynamically blends a margin-based signal and a ratio-based signal\n    using a rank-derived weight, allowing it to adapt its objective based on the\n    relative importance of the preference pair.\n\n    Args:\n        batch (dict): A dictionary containing tensors.\n            'cost_a' (Tensor [N]): Cost of solution 'a'.\n            'cost_b' (Tensor [N]): Cost of solution 'b'.\n            'log_prob_w' (Tensor [N]): Log probability of the winning solution.\n            'log_prob_l' (Tensor [N]): Log probability of the losing solution.\n            'weight' (Tensor [N], optional): Per-pair loss weights.\n        model_output (dict): Model outputs (not used).\n        extra (dict): A dictionary for hyperparameters.\n            'alpha' (float): Controls the sensitivity of the rank-based blending.\n\n    Returns:\n        torch.Tensor: A scalar tensor representing the mean loss.\n    \"\"\"\n\n    # Whitelisted operator implemented as a helper function\n    def rank_gap(x, eps=1e-9):\n        \"\"\"Converts a tensor to its normalized ranks within the range [-0.5, 0.5].\"\"\"\n        if x.numel() <= 1:\n            return torch.zeros_like(x)\n        ranks = x.to(torch.float32).argsort().argsort().to(x.dtype)\n        normalized_ranks = ranks / (x.numel() - 1 + eps)\n        return normalized_ranks - 0.5\n\n    # Retrieve hyperparameters\n    alpha = extra.get('alpha', 4.0)\n\n    # Unpack tensors from the batch\n    cost_a = batch['cost_a']\n    cost_b = batch['cost_b']\n    log_prob_w = batch['log_prob_w']\n    log_prob_l = batch['log_prob_l']\n    weights = batch.get('weight', None)\n\n    # 1. Calculate the difference in log probabilities\n    logp_diff = log_prob_w - log_prob_l\n\n    # 2. (Inherited from Parent 0) Soft-clip for stability in the margin component\n    clipped_logp_diff = torch.tanh(logp_diff)\n\n    # 3. Calculate absolute cost difference\n    cost_diff = torch.abs(cost_a - cost_b)\n\n    # 4. (Inherited from Parent 1) Create a scale-invariant rank weight\n    rank_gap_signal = rank_gap(cost_diff)\n    rank_weight = F.softplus(alpha * rank_gap_signal)\n\n    # 5. (New Coupling) Create two distinct loss signals\n    # loss_margin: Pushes for a large logp_diff gap, good for high-rank pairs.\n    # Using 1 - tanh() is equivalent to -logsigmoid(inf * (tanh() - 1))\n    loss_margin = 1.0 - clipped_logp_diff\n\n    # loss_ratio: Standard logistic loss, focuses on getting the sign right.\n    loss_ratio = -F.logsigmoid(logp_diff)\n\n    # 6. (New Coupling) Create a smooth blending coefficient from the rank weight\n    # sigmoid(softplus(x)) creates a smooth step-like function.\n    beta = torch.sigmoid(rank_weight)\n\n    # 7. Blend the two loss signals using beta\n    # High-rank pairs (beta -> 1) focus on the margin loss.\n    # Low-rank pairs (beta -> 0.5) focus more on the ratio loss.\n    loss_per_item = beta * loss_margin + (1.0 - beta) * loss_ratio\n\n    # 8. Apply optional weights and compute the mean loss\n    if weights is not None:\n        loss = (loss_per_item * weights).sum() / weights.sum().clamp(min=1e-8)\n    else:\n        loss = loss_per_item.mean()\n\n    return loss"}, "static_ok": true, "static_reason": "", "dynamic_ok": true, "dynamic_reason": "ok", "loss_value": 0.9039267301559448, "grad_norm": 0.0}
{"generation": 7, "index": 5, "ir": {"name": "Adaptive Dual-Clip Rank Loss", "intuition": "This loss function creates a stable and adaptive learning signal by dynamically adjusting both the target margin and the log-probability clipping based on the relative importance of a preference pair. It aims to prevent the model from over-investing in easy-to-distinguish pairs while demanding a stronger signal for pairs with a large quality gap.\n\nInherited Ideas:\n- From `Soft-Clipped Rank-Modulated Loss` (Parent 0): It inherits the use of a **multiplicative scaler** applied to a clipped log-probability difference. This scaler, derived from the cost difference, amplifies the learning signal for more important preference pairs.\n- From `Rank-Modulated Dual-Clipping Loss` (Parent 1): It inherits the concept of an **additive margin** that the model's preference score must overcome. This margin is also derived from the cost difference, setting a dynamic target for the model.\n\nNew Coupling Ideas & Modifications:\n1.  **Unified Rank-Based Modulator**: Both parents use a `rank_gap` signal to derive adaptive components. This child unifies this into a single `rank_modulator = softplus(alpha * rank_gap(cost_diff))`. This modulator becomes the single, scale-invariant source for both the scaler and the margin, ensuring they are coherently linked.\n2.  **Coupled Scaling and Clipping**: Instead of a fixed `tanh` clip (Parent 0) or a hard `clamp` (Parent 1), this loss introduces a new form of dynamic soft-clipping. The log-probability difference is scaled *before* being clipped: `tanh(logp_diff / rank_modulator)`. This means for important pairs (large `rank_modulator`), the `logp_diff` is down-scaled, making it harder to saturate the `tanh` and encouraging the model to produce a larger log-probability gap. For unimportant pairs (small `rank_modulator`), the `logp_diff` is amplified, pushing it towards saturation quickly, effectively ignoring it once the preference is correct. The result is then scaled back up by the modulator, preserving the signal's magnitude.", "pseudocode": "1. Calculate the difference in log probabilities: `logp_diff = logp_winner - logp_loser`.\n2. Calculate the absolute difference in costs: `cost_diff = abs(cost_winner - cost_loser)`.\n3. (New Coupling) Convert cost differences into a normalized rank signal between -0.5 and 0.5 using the `rank_gap` operator. This creates a stable, scale-invariant measure of preference importance.\n4. (New Coupling) Create a single, unified `rank_modulator` from the rank signal: `rank_modulator = softplus(alpha * rank_gap_signal)`. This value is smooth, non-negative, and increases with the importance of the pair.\n5. (Inherited from Parent 0 & New Coupling) Create a dynamically clipped and scaled log-probability signal. First, normalize the `logp_diff` by the `rank_modulator`, then apply `tanh`, and finally scale it back up: `scaled_clipped_logp = rank_modulator * tanh(logp_diff / (rank_modulator + epsilon))`. This focuses model effort on important pairs.\n6. (Inherited from Parent 1) Define the additive margin. For simplicity and strong coupling, the margin is set to be the `rank_modulator` itself.\n7. Combine the components into the loss argument: `argument = scaled_clipped_logp - rank_modulator`. The goal is for the adaptively scaled and clipped log-probability difference to exceed the adaptive margin.\n8. Compute the final loss using a numerically stable logistic loss: `loss = -logsigmoid(argument)`.\n9. Return the weighted mean of the loss over the batch.", "hyperparams": {"alpha": 4.0}, "operators_used": ["logsigmoid", "softplus", "tanh", "rank_gap"], "implementation_hint": {"expects": ["cost_a", "cost_b", "log_prob_w", "log_prob_l", "weight"], "returns": "scalar"}, "code": "import torch\nimport torch.nn.functional as F\n\ndef generated_loss(batch, model_output, extra):\n    \"\"\"\n    Computes the Adaptive Dual-Clip Rank Loss.\n\n    This loss uses a unified rank-based modulator to simultaneously define an\n    additive margin and dynamically scale the input to a tanh function, effectively\n    creating an adaptive soft-clip on the log-probability difference.\n\n    Args:\n        batch (dict): A dictionary containing tensors.\n            'cost_a' (Tensor [N]): Cost of solution 'a'.\n            'cost_b' (Tensor [N]): Cost of solution 'b'.\n            'log_prob_w' (Tensor [N]): Log probability of the winning solution.\n            'log_prob_l' (Tensor [N]): Log probability of the losing solution.\n            'weight' (Tensor [N], optional): Per-pair loss weights.\n        model_output (dict): Model outputs (not used).\n        extra (dict): A dictionary for hyperparameters.\n            'alpha' (float): Controls the magnitude of the rank modulator.\n\n    Returns:\n        torch.Tensor: A scalar tensor representing the mean loss.\n    \"\"\"\n\n    # Whitelisted operator implemented as a helper function\n    def rank_gap(x, eps=1e-9):\n        \"\"\"Converts a tensor to its normalized ranks within the range [-0.5, 0.5].\"\"\"\n        if x.numel() <= 1:\n            return torch.zeros_like(x)\n        ranks = x.to(torch.float32).argsort().argsort().to(x.dtype)\n        normalized_ranks = ranks / (x.numel() - 1 + eps)\n        return normalized_ranks - 0.5\n\n    # Retrieve hyperparameters\n    alpha = extra.get('alpha', 4.0)\n    epsilon = 1e-8\n\n    # Unpack tensors from the batch\n    cost_a = batch['cost_a']\n    cost_b = batch['cost_b']\n    log_prob_w = batch['log_prob_w']\n    log_prob_l = batch['log_prob_l']\n    weights = batch.get('weight', None)\n\n    # 1. Calculate the difference in log probabilities\n    logp_diff = log_prob_w - log_prob_l\n\n    # 2. Calculate absolute cost difference\n    cost_diff = torch.abs(cost_a - cost_b)\n\n    # 3. (New Coupling) Use rank_gap for a stable, scale-invariant cost signal\n    rank_gap_signal = rank_gap(cost_diff)\n\n    # 4. (New Coupling) Create a unified rank modulator\n    rank_modulator = F.softplus(alpha * rank_gap_signal)\n\n    # 5. (Inherited from Parent 0 & New Coupling) Create the dynamically clipped signal\n    # The logp_diff is normalized by the modulator before tanh, then scaled back up.\n    # This focuses effort on pairs where logp_diff is large relative to the modulator.\n    scaled_clipped_logp = rank_modulator * torch.tanh(logp_diff / (rank_modulator + epsilon))\n\n    # 6. (Inherited from Parent 1) The additive margin is the rank_modulator itself\n    additive_margin = rank_modulator\n\n    # 7. Combine components. We want scaled_clipped_logp > additive_margin.\n    argument = scaled_clipped_logp - additive_margin\n    # This simplifies to: rank_modulator * (tanh(logp_diff / modulator) - 1)\n\n    # 8. Compute the final loss using numerically stable logsigmoid\n    loss_per_item = -F.logsigmoid(argument)\n\n    # 9. Apply optional weights and compute the mean loss\n    if weights is not None:\n        loss = (loss_per_item * weights).sum() / weights.sum().clamp(min=1e-8)\n    else:\n        loss = loss_per_item.mean()\n\n    return loss"}, "static_ok": true, "static_reason": "", "dynamic_ok": true, "dynamic_reason": "ok", "loss_value": 1.2550013065338135, "grad_norm": 0.0}
{"generation": 7, "index": 6, "ir": {"name": "Adaptive Dual-Clip Log-Ratio Loss", "intuition": "This loss function introduces a stable, adaptive learning target by blending dynamic clipping with a rank-based log-ratio objective. It is designed to be robust to the scale of costs and log-probabilities, while focusing the model's learning capacity on preference pairs that matter most.\n\nInherited Ideas:\n- From `Rank-Modulated Dual-Clipping Loss` (Parent 1): It inherits the concept of **dynamic clipping of the log-probability difference**. The `logp_diff` is clamped within a range `[-M, M]`, where the bound `M` is derived from the rank of the cost difference. This prevents the model from wasting capacity on generating excessively large log-probability gaps for unimportant pairs.\n- From `Soft-Clipped Rank-Modulated Loss` (Parent 0): It inherits the structure of using a **multiplicative scaler** applied to a transformed `logp_diff`. This scaler amplifies the learning signal for pairs with a larger, more significant cost difference.\n\nNew Coupling Ideas & Modifications:\n1. **Log-Ratio Objective**: Instead of a simple difference (`logp_diff - margin`), this loss uses a log-ratio formulation, `log(sigmoid(clipped_logp_diff))`. This transforms the clipped log-probability difference into a probability space (0, 1), making the loss less sensitive to the absolute magnitude of `logp_diff` and more focused on how confidently the model prefers the winner. The `log` brings it back to a log-space, similar to a standard cross-entropy objective, which is numerically stable.\n2. **Unified Rank-Based Modulator for Clipping and Scaling**: A single, robust `rank_modulator` is created using `softplus` on a `rank_gap` signal derived from the cost difference. This modulator is then coupled to the loss in two distinct ways: first, it defines the dynamic clipping bounds for `logp_diff` (inherited); second, it acts as a multiplicative scaler on the final log-ratio objective. This dual role ensures that as the importance of a pair increases (higher rank), the model is both allowed a larger `logp_diff` and more heavily penalized if it fails to achieve a high preference probability.", "pseudocode": "1. Calculate the difference in log probabilities: `logp_diff = logp_winner - logp_loser`.\n2. Calculate the absolute difference in costs: `cost_diff = abs(cost_winner - cost_loser)`.\n3. Convert the cost differences into a normalized rank signal between -0.5 and 0.5 using the `rank_gap` operator for a scale-invariant importance signal.\n4. (New Coupling) Create a single, unified `rank_modulator` from the rank signal: `rank_modulator = softplus(alpha * rank_gap_signal)`. This produces a smooth, non-negative value that increases with the rank of the cost difference.\n5. (Inherited from Parent 1) Dynamically clip the log probability difference using the `rank_modulator` to define the clipping range: `clipped_logp_diff = clamp(logp_diff, min=-rank_modulator, max=rank_modulator)`.\n6. (New Coupling) Transform the clipped log-probability difference into a log-probability space using `log(sigmoid(clipped_logp_diff))`, which is numerically equivalent to `logsigmoid(clipped_logp_diff)`. This creates a stable log-ratio objective.\n7. (Inherited from Parent 0) Apply the `rank_modulator` as a multiplicative scaler to the log-ratio objective. The goal is to maximize this scaled value. The loss argument becomes `rank_modulator * logsigmoid(clipped_logp_diff)`.\n8. Compute the final loss by taking the negative of this value: `loss = -rank_modulator * logsigmoid(clipped_logp_diff)`.\n9. Return the weighted mean of the loss over the batch.", "hyperparams": {"alpha": 4.0}, "operators_used": ["logsigmoid", "softplus", "clamp", "rank_gap"], "implementation_hint": {"expects": ["cost_a", "cost_b", "logp_a", "logp_b"], "returns": "scalar"}, "code": "import torch\nimport torch.nn.functional as F\n\ndef generated_loss(batch, model_output, extra):\n    \"\"\"\n    Computes the Adaptive Dual-Clip Log-Ratio Loss.\n\n    This loss uses a rank-based modulator to both dynamically clip the log-probability\n    difference and to scale a final log-ratio objective.\n\n    Args:\n        batch (dict): A dictionary containing tensors.\n            'cost_a' (Tensor [N]): Cost of solution 'a'.\n            'cost_b' (Tensor [N]): Cost of solution 'b'.\n            'log_prob_w' (Tensor [N]): Log probability of the winning solution.\n            'log_prob_l' (Tensor [N]): Log probability of the losing solution.\n            'weight' (Tensor [N], optional): Per-pair loss weights.\n        model_output (dict): Model outputs (not used).\n        extra (dict): A dictionary for hyperparameters.\n            'alpha' (float): Controls the magnitude of the rank modulator.\n\n    Returns:\n        torch.Tensor: A scalar tensor representing the mean loss.\n    \"\"\"\n\n    # Whitelisted operator implemented as a helper function\n    def rank_gap(x, eps=1e-9):\n        \"\"\"Converts a tensor to its normalized ranks within the range [-0.5, 0.5].\"\"\"\n        if x.numel() <= 1:\n            return torch.zeros_like(x)\n        ranks = x.to(torch.float32).argsort().argsort().to(x.dtype)\n        # Normalize ranks to [0, 1]\n        normalized_ranks = ranks / (x.numel() - 1 + eps)\n        # Shift to [-0.5, 0.5]\n        return normalized_ranks - 0.5\n\n    # Retrieve hyperparameters\n    alpha = extra.get('alpha', 4.0)\n\n    # Unpack tensors from the batch\n    cost_a = batch['cost_a']\n    cost_b = batch['cost_b']\n    log_prob_w = batch['log_prob_w']\n    log_prob_l = batch['log_prob_l']\n    weights = batch.get('weight', None)\n\n    # 1. Calculate the difference in log probabilities\n    logp_diff = log_prob_w - log_prob_l\n\n    # 2. Calculate absolute cost difference\n    cost_diff = torch.abs(cost_a - cost_b)\n\n    # 3. Use rank_gap for a stable, scale-invariant cost signal\n    rank_gap_signal = rank_gap(cost_diff)\n\n    # 4. (New Coupling) Create a unified rank modulator\n    rank_modulator = F.softplus(alpha * rank_gap_signal)\n\n    # 5. (Inherited from Parent 1) Dynamically clip logp_diff\n    clipped_logp_diff = torch.clamp(logp_diff, min=-rank_modulator, max=rank_modulator)\n\n    # 6. (New Coupling) Transform to a log-ratio objective\n    # This is equivalent to log(sigmoid(clipped_logp_diff))\n    log_ratio_objective = F.logsigmoid(clipped_logp_diff)\n\n    # 7. (Inherited from Parent 0) Apply the modulator as a scaler\n    # We want to maximize this value, so the loss is its negative.\n    # 8. Compute the final loss\n    loss_per_item = -rank_modulator * log_ratio_objective\n\n    # 9. Apply optional weights and compute the mean loss\n    if weights is not None:\n        loss = (loss_per_item * weights).sum() / weights.sum().clamp(min=1e-8)\n    else:\n        loss = loss_per_item.mean()\n\n    return loss"}, "static_ok": true, "static_reason": "", "dynamic_ok": true, "dynamic_reason": "ok", "loss_value": 0.5991289615631104, "grad_norm": 0.0}
{"generation": 7, "index": 7, "ir": {"name": "Rank-Modulated Hybrid Clipping Loss", "intuition": "This loss function creates a highly adaptive learning signal by combining the stability of fixed soft-clipping with a dynamic, rank-based margin. It aims to prevent the model from over-investing in easy preference pairs while still demanding a significant log-probability gap for high-importance pairs.\n\nInherited Ideas:\n- From `Soft-Clipped Rank-Modulated Loss` (Parent 0): It inherits the use of `tanh(logp_diff)` to soft-clip the log-probability difference. This provides a baseline stability by bounding the core signal to [-1, 1], preventing extreme gradients regardless of the raw logp difference.\n- From `Rank-Modulated Dual-Clipping Loss` (Parent 1): It inherits the concept of an adaptive, rank-based additive margin. The margin is computed as `softplus(alpha * rank_gap(cost_diff))`, ensuring it is smooth, non-negative, and grows with the relative importance of the cost difference within the batch.\n\nNew Coupling Ideas & Modifications:\n1.  **Hybrid Clipping (Hard-then-Soft)**: Instead of choosing between static `tanh` clipping and dynamic `clamp` clipping, this loss uses both in sequence. First, the logp difference is hard-clipped to a range `[-beta, beta]`. This acts as a robust outer guard against pathologically large logp differences that could cause numerical issues or dominate the learning signal. Then, this clipped value is passed through `tanh`. This hybrid approach ensures the final signal is always smoothly bounded in [-1, 1] while also being insensitive to extreme initial logp values.\n2.  **Decoupled Multiplicative Scaling**: The final loss argument is `scaler * tanh(clipped_logp_diff) - margin`. The `scaler` is a constant hyperparameter `gamma`, while the `margin` is the dynamic, rank-based modulator. This decouples the scaling of the logp signal from the target margin. It allows for independent tuning of how much to amplify the `tanh` output (`gamma`) versus how large the target preference margin should be (`alpha`), providing more fine-grained control over the loss landscape.", "pseudocode": "1. Calculate the difference in log probabilities: `logp_diff = logp_winner - logp_loser`.\n2. (New Coupling) Apply a hard-clipping outer guard to the logp difference: `clipped_logp_diff = clamp(logp_diff, min=-beta, max=beta)`. This prevents extreme values from proceeding.\n3. (Inherited from Parent 0) Apply `tanh` to the hard-clipped difference for a smooth, bounded signal: `tanh_clipped_logp = tanh(clipped_logp_diff)`.\n4. Calculate the absolute difference in costs: `cost_diff = abs(cost_winner - cost_loser)`.\n5. Convert the cost differences into a normalized rank signal between -0.5 and 0.5 using the `rank_gap` operator for a scale-invariant importance measure.\n6. (Inherited from Parent 1) Create an adaptive, additive margin from the rank signal: `margin = softplus(alpha * rank_gap_signal)`.\n7. (New Coupling) Introduce a constant multiplicative scaler `gamma` to control the magnitude of the logp signal.\n8. Combine the components. The target is `gamma * tanh_clipped_logp > margin`. This forms the loss argument: `argument = gamma * tanh_clipped_logp - margin`.\n9. Compute the final loss using a numerically stable logistic loss function: `loss = -logsigmoid(argument)`.\n10. Return the weighted mean of the loss over the batch.", "hyperparams": {"alpha": 4.0, "beta": 5.0, "gamma": 2.0}, "operators_used": ["logsigmoid", "softplus", "tanh", "clamp", "rank_gap"], "implementation_hint": {"expects": ["cost_a", "cost_b", "log_prob_w", "log_prob_l", "weight"], "returns": "scalar"}, "code": "import torch\nimport torch.nn.functional as F\n\ndef generated_loss(batch, model_output, extra):\n    \"\"\"\n    Computes the Rank-Modulated Hybrid Clipping Loss.\n\n    This loss combines a fixed hard-then-soft clipping of the log-probability\n    difference with a dynamic, rank-based additive margin.\n\n    Args:\n        batch (dict): A dictionary containing tensors.\n            'cost_a' (Tensor [N]): Cost of solution 'a'.\n            'cost_b' (Tensor [N]): Cost of solution 'b'.\n            'log_prob_w' (Tensor [N]): Log probability of the winning solution.\n            'log_prob_l' (Tensor [N]): Log probability of the losing solution.\n            'weight' (Tensor [N], optional): Per-pair loss weights.\n        model_output (dict): Model outputs (not used).\n        extra (dict): A dictionary for hyperparameters.\n            'alpha' (float): Controls the magnitude of the rank-based margin.\n            'beta' (float): The hard-clipping bound for the logp difference.\n            'gamma' (float): A constant multiplicative scaler for the tanh signal.\n\n    Returns:\n        torch.Tensor: A scalar tensor representing the mean loss.\n    \"\"\"\n\n    # Whitelisted operator implemented as a helper function\n    def rank_gap(x, eps=1e-9):\n        \"\"\"Converts a tensor to its normalized ranks within the range [-0.5, 0.5].\"\"\"\n        if x.numel() <= 1:\n            return torch.zeros_like(x)\n        ranks = x.to(torch.float32).argsort().argsort().to(x.dtype)\n        normalized_ranks = ranks / (x.numel() - 1 + eps)\n        return normalized_ranks - 0.5\n\n    # Retrieve hyperparameters\n    alpha = extra.get('alpha', 4.0)\n    beta = extra.get('beta', 5.0)\n    gamma = extra.get('gamma', 2.0)\n\n    # Unpack tensors from the batch\n    cost_a = batch['cost_a']\n    cost_b = batch['cost_b']\n    log_prob_w = batch['log_prob_w']\n    log_prob_l = batch['log_prob_l']\n    weights = batch.get('weight', None)\n\n    # 1. Calculate the difference in log probabilities\n    logp_diff = log_prob_w - log_prob_l\n\n    # 2. (New Coupling) Apply a hard-clipping outer guard\n    hard_clipped_logp_diff = torch.clamp(logp_diff, min=-beta, max=beta)\n\n    # 3. (Inherited from Parent 0) Apply tanh for a smooth, bounded signal\n    tanh_clipped_logp = torch.tanh(hard_clipped_logp_diff)\n\n    # 4. Calculate absolute cost difference\n    cost_diff = torch.abs(cost_a - cost_b)\n\n    # 5. Use rank_gap for a stable, scale-invariant cost signal\n    rank_gap_signal = rank_gap(cost_diff)\n\n    # 6. (Inherited from Parent 1) Create an adaptive additive margin\n    margin = F.softplus(alpha * rank_gap_signal)\n\n    # 7. (New Coupling) Use a constant multiplicative scaler\n    scaled_tanh_logp = gamma * tanh_clipped_logp\n\n    # 8. Combine components into the final argument for the logistic loss\n    argument = scaled_tanh_logp - margin\n\n    # 9. Compute the final loss using numerically stable logsigmoid\n    loss_per_item = -F.logsigmoid(argument)\n\n    # 10. Apply optional weights and compute the mean loss\n    if weights is not None and weights.sum() > 0:\n        loss = (loss_per_item * weights).sum() / weights.sum()\n    else:\n        loss = loss_per_item.mean()\n\n    return loss"}, "static_ok": true, "static_reason": "", "dynamic_ok": true, "dynamic_reason": "ok", "loss_value": 1.2550013065338135, "grad_norm": 0.0}
{"generation": 8, "index": 0, "ir": {"name": "Adaptive Dual-Clip Log-Ratio Loss", "intuition": "This loss function synergizes dynamic, rank-based clipping with a stable, bounded log-ratio objective. It is designed to adapt the learning signal's strength and target based on the relative importance of each preference pair, while preventing the model from chasing infinitely large log-probability differences.\n\nInherited Ideas:\n- From `Rank-Modulated Dual-Clipping Loss` (Parent 1): It inherits the core concept of **dynamic, rank-based clipping**. A `rank_modulator` is derived from the cost difference, and this modulator is used to `clamp(logp_diff, min=-modulator, max=modulator)`. This forces the model to expend more effort on pairs with a larger cost gap (higher rank) and less on insignificant pairs, effectively regularizing the log-probabilities.\n- From `Soft-Clipped Rank-Modulated Loss` (Parent 0): It inherits the idea of using `tanh` to create a **bounded target**. Instead of using `tanh` on the raw `logp_diff` for stability, it is used on a scaled version of the `rank_modulator` to create a dynamic, bounded target value between 0 and 1. This prevents the loss from demanding an infinitely large log-probability gap.\n\nNew Coupling Ideas & Modifications:\n1.  **Log-Ratio Formulation**: The central idea is to reframe the objective from a difference (`logp_diff - margin`) to a ratio. The loss encourages `clipped_logp_diff` to be greater than a target derived from the `rank_modulator`. This is formulated as `clipped_logp_diff / rank_modulator > 1`. To make this numerically stable and bounded, we use `logsigmoid`. The argument becomes `log(clipped_logp_diff) - log(rank_modulator)`, which is `log(clipped_logp_diff / rank_modulator)`. This directly penalizes how far the ratio is from the desired value of 1.\n2.  **Adaptive Target with `tanh`**: Instead of having `clipped_logp_diff` chase the `rank_modulator` itself, which can grow unbounded, we introduce a bounded target `tanh(beta * rank_modulator)`. This couples the `tanh` operator from Parent 0 with the `rank_modulator` from Parent 1. For small cost differences, the target is near 0; for large differences, it approaches 1. The loss then becomes encouraging `clipped_logp_diff` to be greater than this new, stable target. The final argument for the loss is `log(clipped_logp_diff) - log(tanh(beta * rank_modulator))`, creating a stable log-ratio comparison.", "pseudocode": "1. Calculate the difference in log probabilities: `logp_diff = logp_winner - logp_loser`.\n2. Calculate the absolute difference in costs: `cost_diff = abs(cost_winner - cost_loser)`.\n3. Create a stable, scale-invariant rank signal from the cost difference using the `rank_gap` operator.\n4. Compute a smooth, non-negative `rank_modulator` from the rank signal: `rank_modulator = softplus(alpha * rank_gap_signal)`.\n5. (Inherited from Parent 1) Dynamically clip the log-probability difference using the modulator: `clipped_logp_diff = clamp(logp_diff, min=-rank_modulator, max=rank_modulator)`. This ensures `clipped_logp_diff` is always positive for correctly ranked pairs that have a non-zero loss.\n6. (Inherited from Parent 0 & New Coupling) Create a bounded, adaptive target using `tanh`: `adaptive_target = tanh(beta * rank_modulator)`. This target smoothly transitions from 0 to 1 based on the pair's importance.\n7. (New Coupling) Formulate the loss argument as a stable log-ratio. To encourage `clipped_logp_diff > adaptive_target`, we use `log(clipped_logp_diff) - log(adaptive_target)`. To ensure numerical stability for small values, we add a small epsilon before taking the log.\n8. Compute the final loss using a numerically stable logistic loss function: `loss = -logsigmoid(log_ratio_argument)`.\n9. Return the weighted mean of the loss over the batch.", "hyperparams": {"alpha": 4.0, "beta": 1.0, "eps": 1e-09}, "operators_used": ["logsigmoid", "softplus", "clamp", "rank_gap", "tanh", "log"], "implementation_hint": {"expects": ["cost_a", "cost_b", "log_prob_w", "log_prob_l", "weight"], "returns": "scalar"}, "code": "import torch\nimport torch.nn.functional as F\n\ndef generated_loss(batch, model_output, extra):\n    \"\"\"\n    Computes the Adaptive Dual-Clip Log-Ratio Loss.\n\n    This loss combines dynamic, rank-based clipping of the log-probability difference\n    with a log-ratio objective against a bounded, adaptive target created using tanh.\n\n    Args:\n        batch (dict): A dictionary containing tensors.\n            'cost_a' (Tensor [N]): Cost of solution 'a'.\n            'cost_b' (Tensor [N]): Cost of solution 'b'.\n            'log_prob_w' (Tensor [N]): Log probability of the winning solution.\n            'log_prob_l' (Tensor [N]): Log probability of the losing solution.\n            'weight' (Tensor [N], optional): Per-pair loss weights.\n        model_output (dict): Model outputs (not used).\n        extra (dict): A dictionary for hyperparameters.\n            'alpha' (float): Controls the magnitude of the rank modulator.\n            'beta' (float): Controls the saturation of the tanh adaptive target.\n            'eps' (float): Small constant for numerical stability.\n\n    Returns:\n        torch.Tensor: A scalar tensor representing the mean loss.\n    \"\"\"\n\n    # Whitelisted operator implemented as a helper function\n    def rank_gap(x, eps=1e-9):\n        \"\"\"Converts a tensor to its normalized ranks within the range [-0.5, 0.5].\"\"\"\n        if x.numel() <= 1:\n            return torch.zeros_like(x)\n        ranks = x.to(torch.float32).argsort().argsort().to(x.dtype)\n        normalized_ranks = ranks / (x.numel() - 1 + eps)\n        return normalized_ranks - 0.5\n\n    # Retrieve hyperparameters\n    alpha = extra.get('alpha', 4.0)\n    beta = extra.get('beta', 1.0)\n    eps = extra.get('eps', 1e-9)\n\n    # Unpack tensors from the batch\n    cost_a = batch['cost_a']\n    cost_b = batch['cost_b']\n    log_prob_w = batch['log_prob_w']\n    log_prob_l = batch['log_prob_l']\n    weights = batch.get('weight', None)\n\n    # 1. Calculate the difference in log probabilities\n    logp_diff = log_prob_w - log_prob_l\n\n    # 2. Calculate absolute cost difference\n    cost_diff = torch.abs(cost_a - cost_b)\n\n    # 3. Create a stable, scale-invariant rank signal\n    rank_gap_signal = rank_gap(cost_diff)\n\n    # 4. Compute the rank modulator\n    rank_modulator = F.softplus(alpha * rank_gap_signal)\n\n    # 5. (Inherited from Parent 1) Dynamically clip logp_diff\n    # The clamp ensures clipped_logp_diff is positive where loss is non-zero.\n    # We only care about positive logp_diff, so we clamp the min at 0.\n    clipped_logp_diff = torch.clamp(logp_diff, min=0.0, max=rank_modulator)\n\n    # 6. (Inherited from Parent 0 & New Coupling) Create a bounded, adaptive target\n    adaptive_target = torch.tanh(beta * rank_modulator)\n\n    # 7. (New Coupling) Formulate the loss argument as a stable log-ratio\n    # We want clipped_logp_diff > adaptive_target.\n    # This is equivalent to log(clipped_logp_diff) > log(adaptive_target).\n    # The eps ensures log is not taken on zero.\n    log_ratio_argument = torch.log(clipped_logp_diff + eps) - torch.log(adaptive_target + eps)\n    \n    # 8. Compute the final loss using numerically stable logsigmoid\n    loss_per_item = -F.logsigmoid(log_ratio_argument)\n    \n    # 9. Apply optional weights and compute the mean loss\n    if weights is not None:\n        loss = (loss_per_item * weights).sum() / weights.sum().clamp(min=1e-8)\n    else:\n        loss = loss_per_item.mean()\n\n    return loss"}, "static_ok": true, "static_reason": "", "dynamic_ok": false, "dynamic_reason": "forward_error: clamp() received an invalid combination of arguments - got (Tensor, max=Tensor, min=float), but expected one of:\n * (Tensor input, Tensor min = None, Tensor max = None, *, Tensor out = None)\n * (Tensor input, Number min = None, Number max = None, *, Tensor out = None)\n", "loss_value": null, "grad_norm": null}
{"generation": 8, "index": 1, "ir": {"name": "Adaptive Rank-Gated Loss", "intuition": "This loss function introduces a dynamic gating mechanism that adaptively chooses between a stable, soft-clipped learning signal for less important preference pairs and a more demanding, margin-based signal for more important pairs. The switching behavior is controlled by the relative rank of the cost difference within a batch, making the loss robust and focused.\n\nInherited Ideas:\n- From `Soft-Clipped Rank-Modulated Loss` (Parent 0): It inherits the use of a **soft-clipped log-probability difference** (`tanh(logp_diff)`). This provides a stable, bounded learning signal, which is particularly useful for pairs where the cost difference is small or noisy.\n- From `Rank-Modulated Dual-Clipping Loss` (Parent 1): It inherits the concept of an **adaptive additive margin** (`softplus(alpha * rank_gap_signal)`). This margin creates a more challenging objective for the model, requiring the log-probability difference to surpass a threshold that grows with the importance of the preference pair.\n\nNew Coupling Ideas & Modifications:\n1. **Rank-Based Gating Mechanism**: A new coupling is introduced via a `gate` signal, calculated as `sigmoid(beta * rank_gap_signal)`. This gate smoothly transitions from ~0 for low-rank (less important) pairs to ~1 for high-rank (more important) pairs. This allows the loss to dynamically interpolate between two different learning objectives based on the `rank_gap` of the cost difference.\n2. **Hybrid Loss Formulation**: The `gate` is used to create a convex combination of two distinct loss arguments. For low-rank pairs (gate ≈ 0), the loss is dominated by a simple, stable objective (`tanh(logp_diff)`). For high-rank pairs (gate ≈ 1), the loss transitions to a more aggressive margin-based objective (`logp_diff - margin`). This hybrid approach allows the model to focus on simply getting the direction right for easy pairs while enforcing a stronger separation for hard pairs, preventing it from wasting capacity on insignificant preferences.", "pseudocode": "1. Calculate the difference in log probabilities: `logp_diff = logp_winner - logp_loser`.\n2. Calculate the absolute difference in costs: `cost_diff = abs(cost_winner - cost_loser)`.\n3. Compute a normalized, scale-invariant rank signal from the cost difference using the `rank_gap` operator: `rank_gap_signal = rank_gap(cost_diff)`.\n4. (Inherited from Parent 1) Calculate an adaptive additive margin based on the rank signal: `margin = softplus(alpha * rank_gap_signal)`.\n5. (Inherited from Parent 0) Calculate a stable, soft-clipped log-probability difference: `clipped_logp_diff = tanh(logp_diff)`.\n6. (New Coupling) Create a smooth gating signal that transitions from 0 to 1 based on the rank signal: `gate = sigmoid(beta * rank_gap_signal)`.\n7. (New Coupling) Formulate a hybrid loss argument by using the gate to interpolate between two objectives. The first objective is the stable `clipped_logp_diff`. The second is the more demanding `logp_diff - margin`. The final argument is `(1 - gate) * clipped_logp_diff + gate * (logp_diff - margin)`.\n8. Compute the final loss using a numerically stable logistic loss function: `loss = -logsigmoid(hybrid_argument)`.\n9. Return the weighted mean of the loss over the batch.", "hyperparams": {"alpha": 4.0, "beta": 8.0}, "operators_used": ["logsigmoid", "softplus", "sigmoid", "tanh", "rank_gap"], "implementation_hint": {"expects": ["cost_a", "cost_b", "logp_w", "logp_l", "weight"], "returns": "scalar"}, "code": "import torch\nimport torch.nn.functional as F\n\ndef generated_loss(batch, model_output, extra):\n    \"\"\"\n    Computes the Adaptive Rank-Gated Loss.\n\n    This loss uses a rank-based gating mechanism to interpolate between a stable,\n    soft-clipped objective for low-rank pairs and a margin-based objective for\n    high-rank pairs.\n\n    Args:\n        batch (dict): A dictionary containing tensors.\n            'cost_a' (Tensor [N]): Cost of solution 'a'.\n            'cost_b' (Tensor [N]): Cost of solution 'b'.\n            'log_prob_w' (Tensor [N]): Log probability of the winning solution.\n            'log_prob_l' (Tensor [N]): Log probability of the losing solution.\n            'weight' (Tensor [N], optional): Per-pair loss weights.\n        model_output (dict): Model outputs (not used).\n        extra (dict): A dictionary for hyperparameters.\n            'alpha' (float): Controls the magnitude of the adaptive margin.\n            'beta' (float): Controls the steepness of the sigmoid gate.\n\n    Returns:\n        torch.Tensor: A scalar tensor representing the mean loss.\n    \"\"\"\n\n    # Whitelisted operator implemented as a helper function\n    def rank_gap(x, eps=1e-9):\n        \"\"\"Converts a tensor to its normalized ranks within the range [-0.5, 0.5].\"\"\"\n        if x.numel() <= 1:\n            return torch.zeros_like(x)\n        ranks = x.to(torch.float32).argsort().argsort().to(x.dtype)\n        normalized_ranks = ranks / (x.numel() - 1 + eps)\n        return normalized_ranks - 0.5\n\n    # Retrieve hyperparameters\n    alpha = extra.get('alpha', 4.0)\n    beta = extra.get('beta', 8.0)\n\n    # Unpack tensors from the batch\n    cost_a = batch['cost_a']\n    cost_b = batch['cost_b']\n    log_prob_w = batch['log_prob_w']\n    log_prob_l = batch['log_prob_l']\n    weights = batch.get('weight', None)\n\n    # 1. Calculate the difference in log probabilities\n    logp_diff = log_prob_w - log_prob_l\n\n    # 2. Calculate absolute cost difference\n    cost_diff = torch.abs(cost_a - cost_b)\n\n    # 3. Compute a normalized rank signal\n    rank_gap_signal = rank_gap(cost_diff)\n\n    # 4. (Inherited from Parent 1) Calculate the adaptive margin\n    margin = F.softplus(alpha * rank_gap_signal)\n\n    # 5. (Inherited from Parent 0) Calculate the stable, soft-clipped signal\n    clipped_logp_diff = torch.tanh(logp_diff)\n\n    # 6. (New Coupling) Create a smooth gating signal\n    # For low-rank pairs (rank_gap_signal -> -0.5), gate -> 0.\n    # For high-rank pairs (rank_gap_signal -> 0.5), gate -> 1.\n    gate = torch.sigmoid(beta * rank_gap_signal)\n\n    # 7. (New Coupling) Formulate a hybrid loss argument via gating\n    # For low-rank pairs, argument ≈ clipped_logp_diff\n    # For high-rank pairs, argument ≈ logp_diff - margin\n    stable_term = (1 - gate) * clipped_logp_diff\n    margin_term = gate * (logp_diff - margin)\n    hybrid_argument = stable_term + margin_term\n\n    # 8. Compute the final loss using numerically stable logsigmoid\n    loss_per_item = -F.logsigmoid(hybrid_argument)\n\n    # 9. Apply optional weights and compute the mean loss\n    if weights is not None:\n        loss = (loss_per_item * weights).sum() / weights.sum().clamp(min=1e-8)\n    else:\n        loss = loss_per_item.mean()\n\n    return loss"}, "static_ok": true, "static_reason": "", "dynamic_ok": true, "dynamic_reason": "ok", "loss_value": 1.1239627599716187, "grad_norm": 0.0}
{"generation": 8, "index": 2, "ir": {"name": "Adaptive Dual-Clip Log-Ratio Loss", "intuition": "This loss function synergizes stable clipping mechanisms from its parents with a novel formulation based on a log-ratio of probabilities, modulated by the preference pair's importance. It aims to provide a robust and adaptive learning signal that is sensitive to both the magnitude of preference and the cost difference.\n\nInherited Ideas:\n- From `Soft-Clipped Rank-Modulated Loss` (Parent 0): It inherits the use of `tanh` to **soft-clip a signal derived from log probabilities**. This bounds the signal, preventing extreme gradients and ensuring numerical stability.\n- From `Rank-Modulated Dual-Clipping Loss` (Parent 1): It inherits the concept of **dynamic, hard clipping using `clamp`**, where the clipping bounds are determined by a `rank_modulator`. This forces the model to scale its response according to the importance of the preference pair.\n\nNew Coupling Ideas & Modifications:\n1.  **Log-Ratio Formulation**: Instead of directly using the difference `logp_w - logp_l`, this loss uses the log of the ratio of sigmoids: `log(sigmoid(logp_w) / sigmoid(logp_l))`. This is equivalent to `logsigmoid(logp_w) - logsigmoid(logp_l)`, framing the preference in terms of the log-ratio of choice probabilities. This formulation is inherently bounded and focuses on the relative confidence of the model in each choice.\n2.  **Dual-Clipping Cascade**: The log-ratio signal is passed through a two-stage clipping process. First, it is hard-clipped using the `clamp` operator with bounds derived from a `rank_modulator` (idea from Parent 1). This enforces a dynamic range. Second, the result is passed through `tanh` (idea from Parent 0), which smooths the hard edges from the clamp and provides a final, stable signal bounded in `[-1, 1]`. This cascade ensures both adaptive range-finding and smooth, stable gradients.\n3.  **Adaptive Margin Scaling**: The final loss argument is `(clipped_signal - beta) * rank_modulator`. The `rank_modulator` (derived from `rank_gap` and `softplus`) acts as a multiplicative scaler on the loss, amplifying the penalty for misclassifying important pairs. The `beta` term acts as a fixed margin, requiring the doubly-clipped signal to exceed a certain threshold before the loss approaches zero. This couples the adaptive scaling with a consistent target margin.", "pseudocode": "1. Calculate the log-probability difference: `logp_diff = logp_winner - logp_loser`.\n2. (New Coupling) Transform the log-probability difference into a log-ratio of sigmoids. This can be computed stably as `log_ratio = logsigmoid(logp_winner) - logsigmoid(logp_loser)`.\n3. Calculate the absolute cost difference: `cost_diff = abs(cost_winner - cost_loser)`.\n4. Create a stable, scale-invariant signal of preference importance by applying the `rank_gap` operator to the cost difference.\n5. Create a smooth, non-negative `rank_modulator` from the rank signal: `rank_modulator = softplus(alpha * rank_gap_signal)`.\n6. (Inherited from Parent 1 & New Coupling) First clipping stage: Dynamically hard-clip the `log_ratio` using the `rank_modulator` as the bound: `hard_clipped_signal = clamp(log_ratio, min=-rank_modulator, max=rank_modulator)`.\n7. (Inherited from Parent 0 & New Coupling) Second clipping stage: Soft-clip the result from the previous step using `tanh` for smoothness and stability: `doubly_clipped_signal = tanh(hard_clipped_signal)`.\n8. Construct the loss argument. The `doubly_clipped_signal` must overcome a fixed margin `beta`. This difference is then scaled by the `rank_modulator`: `argument = (doubly_clipped_signal - beta) * rank_modulator`.\n9. Compute the final loss using a numerically stable logistic loss function: `loss = -logsigmoid(argument)`.\n10. Return the weighted mean of the loss over the batch.", "hyperparams": {"alpha": 5.0, "beta": 0.95}, "operators_used": ["logsigmoid", "softplus", "clamp", "tanh", "rank_gap"], "implementation_hint": {"expects": ["cost_a", "cost_b", "logp_w", "logp_l", "weight"], "returns": "scalar"}, "code": "import torch\nimport torch.nn.functional as F\n\ndef generated_loss(batch, model_output, extra):\n    \"\"\"\n    Computes the Adaptive Dual-Clip Log-Ratio Loss.\n\n    This loss uses a log-ratio of sigmoid probabilities, which is then passed\n    through a cascaded dynamic hard-clip and a soft-clip. The final loss is\n    scaled by a rank-based modulator.\n\n    Args:\n        batch (dict): A dictionary containing tensors.\n            'cost_a' (Tensor [N]): Cost of solution 'a'.\n            'cost_b' (Tensor [N]): Cost of solution 'b'.\n            'log_prob_w' (Tensor [N]): Log probability of the winning solution.\n            'log_prob_l' (Tensor [N]): Log probability of the losing solution.\n            'weight' (Tensor [N], optional): Per-pair loss weights.\n        model_output (dict): Model outputs (not used).\n        extra (dict): A dictionary for hyperparameters.\n            'alpha' (float): Controls the magnitude of the rank modulator.\n            'beta' (float): The target margin for the doubly-clipped signal.\n\n    Returns:\n        torch.Tensor: A scalar tensor representing the mean loss.\n    \"\"\"\n\n    # Whitelisted operator implemented as a helper function\n    def rank_gap(x, eps=1e-9):\n        \"\"\"Converts a tensor to its normalized ranks within the range [-0.5, 0.5].\"\"\"\n        if x.numel() <= 1:\n            return torch.zeros_like(x)\n        ranks = x.to(torch.float32).argsort().argsort().to(x.dtype)\n        normalized_ranks = ranks / (x.numel() - 1 + eps)\n        return normalized_ranks - 0.5\n\n    # Retrieve hyperparameters\n    alpha = extra.get('alpha', 5.0)\n    beta = extra.get('beta', 0.95)\n\n    # Unpack tensors from the batch\n    cost_a = batch['cost_a']\n    cost_b = batch['cost_b']\n    log_prob_w = batch['log_prob_w']\n    log_prob_l = batch['log_prob_l']\n    weights = batch.get('weight', None)\n\n    # 2. (New Coupling) Compute log-ratio of sigmoids\n    # This is equivalent to log(sigmoid(w)/sigmoid(l))\n    log_ratio = F.logsigmoid(log_prob_w) - F.logsigmoid(log_prob_l)\n\n    # 3. Calculate absolute cost difference\n    cost_diff = torch.abs(cost_a - cost_b)\n\n    # 4. Create a stable, scale-invariant importance signal\n    rank_gap_signal = rank_gap(cost_diff)\n\n    # 5. Create a smooth, non-negative rank modulator\n    rank_modulator = F.softplus(alpha * rank_gap_signal)\n\n    # 6. (Inherited from Parent 1 & New Coupling) Dynamic hard-clipping\n    hard_clipped_signal = torch.clamp(log_ratio, min=-rank_modulator, max=rank_modulator)\n\n    # 7. (Inherited from Parent 0 & New Coupling) Smooth soft-clipping\n    doubly_clipped_signal = torch.tanh(hard_clipped_signal)\n\n    # 8. Construct the loss argument with a margin and scaling\n    # We want the clipped signal to be > beta. The modulator scales the penalty.\n    argument = (doubly_clipped_signal - beta) * rank_modulator\n\n    # 9. Compute the final loss using numerically stable logsigmoid\n    loss_per_item = -F.logsigmoid(argument)\n\n    # 10. Apply optional weights and compute the mean loss\n    if weights is not None:\n        loss = (loss_per_item * weights).sum() / weights.sum().clamp(min=1e-8)\n    else:\n        loss = loss_per_item.mean()\n\n    return loss"}, "static_ok": true, "static_reason": "", "dynamic_ok": true, "dynamic_reason": "ok", "loss_value": 1.2960779666900635, "grad_norm": 0.0}
{"generation": 8, "index": 3, "ir": {"name": "Adaptive Dual-Clipping Loss", "intuition": "This loss function creates a highly adaptive learning signal by using a rank-based modulator to control two separate clipping mechanisms. It ensures that the model's effort is proportional to the importance of the preference pair, preventing overfitting on easy pairs and providing a strong signal for difficult ones.\n\nInherited Ideas:\n- From `Soft-Clipped Rank-Modulated Loss` (Parent 0): It inherits the idea of using `tanh` to **soft-clip the log-probability difference**. This provides a baseline level of stability by bounding the difference to `[-1, 1]`, preventing extreme gradients early in training.\n- From `Rank-Modulated Dual-Clipping Loss` (Parent 1): It inherits the concept of using a **cost-derived additive margin**. The loss requires the model's preference score to exceed a margin that adapts based on the relative importance of the preference pair within the batch. This margin is derived from a `rank_gap` signal, making it robust to cost scale.\n\nNew Coupling Ideas & Modifications:\n1.  **Dual-Clipping Mechanism**: This is the core novelty. The `logp_diff` is first soft-clipped with `tanh` for general stability. Then, it undergoes a second, dynamic clipping using `clamp`. The bounds of this hard clamp, `[-rank_modulator, rank_modulator]`, are determined by a `rank_modulator` derived from the cost difference. This forces the model's output to be both well-behaved globally (due to `tanh`) and adaptively constrained based on the pair's importance (due to `clamp`). For unimportant pairs, the effective `logp_diff` is squeezed towards zero, while for important pairs, it is allowed to be larger.\n2.  **Margin-Modulator Decoupling**: Unlike the parents where the modulator might be the margin itself, here the margin and the modulator for clipping are related but distinct. The `rank_modulator` for clipping is `softplus(alpha * rank_gap_signal)`, while the `additive_margin` is `softplus(beta * rank_gap_signal)`. By using separate hyperparameters (`alpha` and `beta`), we can independently tune the aggressiveness of the dynamic clipping and the magnitude of the target margin. This provides finer control over the learning dynamics.", "pseudocode": "1. Calculate the difference in log probabilities: `logp_diff = logp_winner - logp_loser`.\n2. Calculate the absolute difference in costs: `cost_diff = abs(cost_winner - cost_loser)`.\n3. (Inherited) Convert the cost differences into a normalized rank signal between -0.5 and 0.5 using the `rank_gap` operator for a scale-invariant importance signal.\n4. (New Coupling) Create a `rank_modulator` to control the clipping range: `rank_modulator = softplus(alpha * rank_gap_signal)`. This is a smooth, non-negative value that increases with the rank of the cost difference.\n5. (Inherited from Parent 0) Apply a first layer of clipping for baseline stability: `soft_clipped_logp_diff = tanh(logp_diff)`.\n6. (New Coupling) Apply a second, dynamic layer of clipping using the modulator: `dual_clipped_logp_diff = clamp(soft_clipped_logp_diff, min=-rank_modulator, max=rank_modulator)`. The effective logp difference is thus bounded by both `tanh` and the dynamic range.\n7. (Inherited from Parent 1 & New Coupling) Define a decoupled additive margin. The margin also uses the rank signal but with its own hyperparameter `beta`: `additive_margin = softplus(beta * rank_gap_signal)`.\n8. Combine the components. The objective is for the dual-clipped logp difference to exceed the adaptive margin: `argument = dual_clipped_logp_diff - additive_margin`.\n9. Compute the final loss using a numerically stable logistic loss: `loss = -logsigmoid(argument)`.\n10. Return the weighted mean of the loss over the batch.", "hyperparams": {"alpha": 1.0, "beta": 2.0}, "operators_used": ["logsigmoid", "softplus", "tanh", "clamp", "rank_gap"], "implementation_hint": {"expects": ["cost_a", "cost_b", "log_prob_w", "log_prob_l", "weight"], "returns": "scalar"}, "code": "import torch\nimport torch.nn.functional as F\n\ndef generated_loss(batch, model_output, extra):\n    \"\"\"\n    Computes the Adaptive Dual-Clipping Loss.\n\n    This loss uses a rank-based signal to both dynamically clip the log-probability\n    difference and set an adaptive additive margin, with decoupled hyperparameters for\n    fine-grained control.\n\n    Args:\n        batch (dict): A dictionary containing tensors.\n            'cost_a' (Tensor [N]): Cost of solution 'a'.\n            'cost_b' (Tensor [N]): Cost of solution 'b'.\n            'log_prob_w' (Tensor [N]): Log probability of the winning solution.\n            'log_prob_l' (Tensor [N]): Log probability of the losing solution.\n            'weight' (Tensor [N], optional): Per-pair loss weights.\n        model_output (dict): Model outputs (not used).\n        extra (dict): A dictionary for hyperparameters.\n            'alpha' (float): Controls the magnitude of the dynamic clipping range.\n            'beta' (float): Controls the magnitude of the adaptive margin.\n\n    Returns:\n        torch.Tensor: A scalar tensor representing the mean loss.\n    \"\"\"\n\n    # Whitelisted operator implemented as a helper function\n    def rank_gap(x, eps=1e-9):\n        \"\"\"Converts a tensor to its normalized ranks within the range [-0.5, 0.5].\"\"\"\n        if x.numel() <= 1:\n            return torch.zeros_like(x)\n        ranks = x.to(torch.float32).argsort().argsort().to(x.dtype)\n        normalized_ranks = ranks / (x.numel() - 1 + eps)\n        return normalized_ranks - 0.5\n\n    # Retrieve hyperparameters\n    alpha = extra.get('alpha', 1.0)\n    beta = extra.get('beta', 2.0)\n\n    # Unpack tensors from the batch\n    cost_a = batch['cost_a']\n    cost_b = batch['cost_b']\n    log_prob_w = batch['log_prob_w']\n    log_prob_l = batch['log_prob_l']\n    weights = batch.get('weight', None)\n\n    # 1. Calculate the difference in log probabilities\n    logp_diff = log_prob_w - log_prob_l\n\n    # 2. Calculate absolute cost difference\n    cost_diff = torch.abs(cost_a - cost_b)\n\n    # 3. Create a stable, scale-invariant rank signal\n    rank_gap_signal = rank_gap(cost_diff)\n\n    # 4. (New Coupling) Create a modulator for the dynamic clipping range\n    rank_modulator = F.softplus(alpha * rank_gap_signal)\n\n    # 5. (Inherited from Parent 0) First, soft-clip for global stability\n    soft_clipped_logp_diff = torch.tanh(logp_diff)\n\n    # 6. (New Coupling) Second, dynamically clip based on rank modulator\n    # The clamp is applied to the already-tanh'd value.\n    # Since rank_modulator >= 0 and tanh is in [-1, 1], this is an adaptive squeeze.\n    dual_clipped_logp_diff = torch.clamp(soft_clipped_logp_diff, min=-rank_modulator, max=rank_modulator)\n\n    # 7. (Inherited from Parent 1 & New Coupling) Define a decoupled adaptive margin\n    additive_margin = F.softplus(beta * rank_gap_signal)\n\n    # 8. Combine components. We want the clipped diff to exceed the margin.\n    argument = dual_clipped_logp_diff - additive_margin\n\n    # 9. Compute the final loss using numerically stable logsigmoid\n    loss_per_item = -F.logsigmoid(argument)\n\n    # 10. Apply optional weights and compute the mean loss\n    if weights is not None and weights.sum() > 0:\n        loss = (loss_per_item * weights).sum() / weights.sum()\n    else:\n        loss = loss_per_item.mean()\n\n    return loss"}, "static_ok": true, "static_reason": "", "dynamic_ok": true, "dynamic_reason": "ok", "loss_value": 1.1398158073425293, "grad_norm": 0.0}
{"generation": 8, "index": 4, "ir": {"name": "Adaptive Rank-Modulated Tangent Loss", "intuition": "This loss function creates a stable and adaptive learning signal by dynamically adjusting both the scale of the log-probability difference and the target margin based on the relative importance of a preference pair. It is designed to be robust to outliers in both cost and log-probability values.\n\nInherited Ideas:\n- From Parent 0 (`Soft-Clipped Rank-Modulated Loss`): It inherits the use of `tanh` to **soft-clip the log-probability difference** (`logp_diff`). This bounds the logp difference to `[-1, 1]`, preventing extreme gradients and focusing the learning signal on getting the preference direction correct rather than maximizing an unbounded difference.\n- From Parent 1 (`Rank-Modulated Dual-Clipping Loss`): It inherits the idea of using a **cost-derived additive margin**. The loss requires the model's preference score to exceed this margin, which is computed from a `rank_gap` signal, ensuring it is smooth, non-negative, and grows with the cost difference's importance.\n\nNew Coupling Ideas & Modifications:\n1.  **Unified Rank-Based Modulator**: Both parents use `rank_gap` to create a modulator. This child refines this by creating a single, robust `rank_modulator` from the cost difference: `rank_modulator = softplus(alpha * rank_gap(cost_diff))`. This modulator serves as the basis for both a new scaling mechanism and the adaptive margin, ensuring all adaptive components are derived from a single, scale-invariant source.\n2.  **Decoupled Scaling and Margin**: Unlike Parent 0, where the modulator acts as both scaler and margin, here they are decoupled. The `rank_modulator` serves as the `additive_margin`. A new, separate `scaler` is derived from it using a `tanh` transformation: `scaler = beta * tanh(rank_modulator)`. This bounds the scaler to a predictable range `[0, beta]`, preventing it from growing uncontrollably for high-rank pairs, which adds an extra layer of stability compared to using `softplus` directly as a scaler. The final loss argument becomes `scaler * tanh(logp_diff) - additive_margin`, creating a stable but challenging target where both the scaling factor and the margin adapt to the pair's importance.", "pseudocode": "1. Calculate the difference in log probabilities: `logp_diff = logp_winner - logp_loser`.\n2. Calculate the absolute difference in costs: `cost_diff = abs(cost_winner - cost_loser)`.\n3. (New Coupling) Convert the cost differences into a normalized rank signal between -0.5 and 0.5 using the `rank_gap` operator. This provides a stable, scale-invariant signal of preference importance.\n4. (New Coupling) Create a single, unified `rank_modulator` from the rank signal: `rank_modulator = softplus(alpha * rank_gap_signal)`. This creates a smooth, non-negative value that increases with the rank of the cost difference.\n5. (Inherited from Parent 1) The `additive_margin` is set to be the `rank_modulator` itself, creating an adaptive target for the model to overcome.\n6. (New Coupling) Create a bounded, adaptive `scaler` by applying `tanh` to the `rank_modulator`: `scaler = beta * tanh(rank_modulator)`. This ensures the scaling factor is smooth and bounded, preventing instability from very high-rank pairs.\n7. (Inherited from Parent 0) Soft-clip the log-probability difference using `tanh` for stability: `clipped_logp_diff = tanh(logp_diff)`.\n8. Combine the components into the loss argument: `argument = scaler * clipped_logp_diff - additive_margin`. The goal is to push the scaled and clipped logp difference beyond the adaptive margin.\n9. Compute the final loss using a numerically stable logistic loss function: `loss = -logsigmoid(argument)`.\n10. Return the weighted mean of the loss over the batch.", "hyperparams": {"alpha": 4.0, "beta": 2.0}, "operators_used": ["logsigmoid", "softplus", "tanh", "rank_gap"], "implementation_hint": {"expects": ["cost_a", "cost_b", "log_prob_w", "log_prob_l", "weight"], "returns": "scalar"}, "code": "import torch\nimport torch.nn.functional as F\n\ndef generated_loss(batch, model_output, extra):\n    \"\"\"\n    Computes the Adaptive Rank-Modulated Tangent Loss.\n\n    This loss combines a tanh-clipped log-probability difference with a rank-based\n    additive margin and a bounded, rank-based scaler.\n\n    Args:\n        batch (dict): A dictionary containing tensors.\n            'cost_a' (Tensor [N]): Cost of solution 'a'.\n            'cost_b' (Tensor [N]): Cost of solution 'b'.\n            'log_prob_w' (Tensor [N]): Log probability of the winning solution.\n            'log_prob_l' (Tensor [N]): Log probability of the losing solution.\n            'weight' (Tensor [N], optional): Per-pair loss weights.\n        model_output (dict): Model outputs (not used).\n        extra (dict): A dictionary for hyperparameters.\n            'alpha' (float): Controls the magnitude of the rank modulator and margin.\n            'beta' (float): Controls the maximum value of the scaler.\n\n    Returns:\n        torch.Tensor: A scalar tensor representing the mean loss.\n    \"\"\"\n\n    # Whitelisted operator implemented as a helper function\n    def rank_gap(x, eps=1e-9):\n        \"\"\"Converts a tensor to its normalized ranks within the range [-0.5, 0.5].\"\"\"\n        if x.numel() <= 1:\n            return torch.zeros_like(x)\n        ranks = x.to(torch.float32).argsort().argsort().to(x.dtype)\n        # Normalize ranks to [0, 1]\n        normalized_ranks = ranks / (x.numel() - 1 + eps)\n        # Shift to [-0.5, 0.5]\n        return normalized_ranks - 0.5\n\n    # Retrieve hyperparameters\n    alpha = extra.get('alpha', 4.0)\n    beta = extra.get('beta', 2.0)\n\n    # Unpack tensors from the batch\n    cost_a = batch['cost_a']\n    cost_b = batch['cost_b']\n    log_prob_w = batch['log_prob_w']\n    log_prob_l = batch['log_prob_l']\n    weights = batch.get('weight', None)\n\n    # 1. Calculate the difference in log probabilities\n    logp_diff = log_prob_w - log_prob_l\n\n    # 2. Calculate absolute cost difference\n    cost_diff = torch.abs(cost_a - cost_b)\n\n    # 3. (New Coupling) Use rank_gap for a stable, scale-invariant cost signal\n    rank_gap_signal = rank_gap(cost_diff)\n\n    # 4. (New Coupling) Create a unified rank modulator\n    rank_modulator = F.softplus(alpha * rank_gap_signal)\n\n    # 5. (Inherited from Parent 1) The margin is the rank_modulator itself\n    additive_margin = rank_modulator\n\n    # 6. (New Coupling) Create a bounded scaler from the modulator\n    scaler = beta * torch.tanh(rank_modulator)\n\n    # 7. (Inherited from Parent 0) Soft-clip logp_diff for stability\n    clipped_logp_diff = torch.tanh(logp_diff)\n\n    # 8. Combine components. We want scaler * clipped_logp_diff > additive_margin.\n    argument = scaler * clipped_logp_diff - additive_margin\n\n    # 9. Compute the final loss using numerically stable logsigmoid\n    loss_per_item = -F.logsigmoid(argument)\n\n    # 10. Apply optional weights and compute the mean loss\n    if weights is not None:\n        loss = (loss_per_item * weights).sum() / weights.sum().clamp(min=1e-8)\n    else:\n        loss = loss_per_item.mean()\n\n    return loss"}, "static_ok": true, "static_reason": "", "dynamic_ok": true, "dynamic_reason": "ok", "loss_value": 1.2550013065338135, "grad_norm": 0.0}
{"generation": 8, "index": 5, "ir": {"name": "Adaptive Dual-Modulation Loss", "intuition": "This loss function creates a highly adaptive learning signal by dynamically modulating both the scale and the target margin of the preference objective. It ensures that the model's effort is proportional to the significance of the preference pair, as determined by the cost difference.\n\nInherited Ideas:\n- From `Soft-Clipped Rank-Modulated Loss` (Parent 0): It inherits the idea of using `tanh(logp_diff)` to soft-clip the log-probability difference. This bounds the signal, preventing extreme gradients and promoting stable training.\n- From `Rank-Modulated Dual-Clipping Loss` (Parent 1): It inherits the concept of an adaptive additive margin. The loss requires the model's preference score to exceed a margin derived from the cost difference, making the learning target more demanding for more important pairs.\n\nNew Coupling Ideas & Modifications:\n1. **Dual Rank-Based Modulators**: Instead of a single modulator, this loss introduces two distinct modulators derived from the same underlying rank signal. A `scale_modulator` is created using `softplus`, providing a smooth, non-negative scaling factor. A `margin_modulator` is created using `sigmoid` and a hyperparameter `beta`, providing a bounded margin between 0 and `beta`. This separation allows for finer control over scaling and margin effects.\n2. **Z-Scored Log-Probability Difference**: Before applying any other operations, the `logp_diff` is z-scored across the batch (`zscore(logp_diff)`). This normalizes the distribution of log-probability differences, making the subsequent `tanh` clipping and scaling operations more consistent and robust to shifts in the model's output distribution during training.", "pseudocode": "1. Calculate the difference in log probabilities: `logp_diff = logp_winner - logp_loser`.\n2. (New Coupling) Normalize the log-probability differences across the batch using z-scoring to stabilize the signal: `normalized_logp_diff = zscore(logp_diff)`.\n3. (Inherited from Parent 0) Soft-clip the normalized log-probability difference using `tanh` for stability: `clipped_logp_diff = tanh(normalized_logp_diff)`.\n4. Calculate the absolute difference in costs: `cost_diff = abs(cost_winner - cost_loser)`.\n5. Convert the cost differences into a normalized rank signal between -0.5 and 0.5 using the `rank_gap` operator, making it robust to cost scale and outliers.\n6. (New Coupling) Create two distinct modulators from the rank signal:\n   a. A `scale_modulator` using `softplus`: `scale_modulator = softplus(alpha * rank_gap_signal)`.\n   b. A `margin_modulator` using `sigmoid`: `margin_modulator = beta * sigmoid(rank_gap_signal)`.\n7. (Inherited from Parent 1) Combine the components. The scaled, clipped logp difference must overcome the adaptive margin: `argument = scale_modulator * clipped_logp_diff - margin_modulator`.\n8. Compute the final loss using a numerically stable logistic loss: `loss = -logsigmoid(argument)`.\n9. Return the weighted mean of the loss over the batch.", "hyperparams": {"alpha": 4.0, "beta": 1.0}, "operators_used": ["logsigmoid", "softplus", "sigmoid", "tanh", "zscore", "rank_gap"], "implementation_hint": {"expects": ["cost_a", "cost_b", "logp_w", "logp_l", "weight"], "returns": "scalar"}, "code": "import torch\nimport torch.nn.functional as F\n\ndef generated_loss(batch, model_output, extra):\n    \"\"\"\n    Computes the Adaptive Dual-Modulation Loss.\n\n    This loss z-scores the log-probability difference and then applies a soft-clipped\n    signal against an adaptive margin, where both the signal's scale and the margin\n    are modulated by the rank of the cost difference.\n\n    Args:\n        batch (dict): A dictionary containing tensors.\n            'cost_a' (Tensor [N]): Cost of solution 'a'.\n            'cost_b' (Tensor [N]): Cost of solution 'b'.\n            'log_prob_w' (Tensor [N]): Log probability of the winning solution.\n            'log_prob_l' (Tensor [N]): Log probability of the losing solution.\n            'weight' (Tensor [N], optional): Per-pair loss weights.\n        model_output (dict): Model outputs (not used).\n        extra (dict): A dictionary for hyperparameters.\n            'alpha' (float): Controls the magnitude of the scaling modulator.\n            'beta' (float): Controls the maximum value of the margin modulator.\n\n    Returns:\n        torch.Tensor: A scalar tensor representing the mean loss.\n    \"\"\"\n\n    # Whitelisted operators implemented as helper functions\n    def rank_gap(x, eps=1e-9):\n        \"\"\"Converts a tensor to its normalized ranks within the range [-0.5, 0.5].\"\"\"\n        if x.numel() <= 1:\n            return torch.zeros_like(x)\n        ranks = x.to(torch.float32).argsort().argsort().to(x.dtype)\n        normalized_ranks = ranks / (x.numel() - 1 + eps)\n        return normalized_ranks - 0.5\n\n    def zscore(x, eps=1e-8):\n        \"\"\"Normalizes a tensor to have zero mean and unit variance.\"\"\"\n        if x.numel() <= 1:\n            return torch.zeros_like(x)\n        mean = x.mean()\n        std = x.std().clamp(min=eps)\n        return (x - mean) / std\n\n    # Retrieve hyperparameters\n    alpha = extra.get('alpha', 4.0)\n    beta = extra.get('beta', 1.0)\n\n    # Unpack tensors from the batch\n    cost_a = batch['cost_a']\n    cost_b = batch['cost_b']\n    log_prob_w = batch['log_prob_w']\n    log_prob_l = batch['log_prob_l']\n    weights = batch.get('weight', None)\n\n    # 1. Calculate the difference in log probabilities\n    logp_diff = log_prob_w - log_prob_l\n\n    # 2. (New Coupling) Normalize logp_diff with z-scoring for stability\n    normalized_logp_diff = zscore(logp_diff)\n\n    # 3. (Inherited from Parent 0) Soft-clip the normalized difference\n    clipped_logp_diff = torch.tanh(normalized_logp_diff)\n\n    # 4. Calculate absolute cost difference\n    cost_diff = torch.abs(cost_a - cost_b)\n\n    # 5. Get a stable, scale-invariant rank signal from the cost difference\n    rank_gap_signal = rank_gap(cost_diff)\n\n    # 6. (New Coupling) Create two separate modulators from the rank signal\n    scale_modulator = F.softplus(alpha * rank_gap_signal)\n    margin_modulator = beta * torch.sigmoid(rank_gap_signal)\n\n    # 7. (Inherited from Parent 1) Combine components into the loss argument\n    # The scaled preference signal must exceed the adaptive margin.\n    argument = scale_modulator * clipped_logp_diff - margin_modulator\n\n    # 8. Compute the final loss using numerically stable logsigmoid\n    loss_per_item = -F.logsigmoid(argument)\n\n    # 9. Apply optional weights and compute the mean loss\n    if weights is not None:\n        loss = (loss_per_item * weights).sum() / weights.sum().clamp(min=1e-8)\n    else:\n        loss = loss_per_item.mean()\n\n    return loss"}, "static_ok": true, "static_reason": "", "dynamic_ok": true, "dynamic_reason": "ok", "loss_value": 0.9747512340545654, "grad_norm": 0.0}
{"generation": 8, "index": 6, "ir": {"name": "Adaptive Rank-Modulated Tanh-Clipping Loss", "intuition": "This loss function creates a stable and adaptive learning signal by dynamically controlling the scale and target of the log-probability difference based on the relative importance of a preference pair. It ensures the model focuses its capacity on pairs with significant cost differences while not being overly penalized for minor or ambiguous preferences.\n\nInherited Ideas:\n- From `Soft-Clipped Rank-Modulated Loss` (Parent 0): It inherits the core structure of using a **multiplicative scaler on a soft-clipped log-probability difference**. Specifically, it uses a `tanh` function to bound the logp difference and a `rank_modulator` to scale it, like `scaler * tanh(...)`.\n- From `Rank-Modulated Dual-Clipping Loss` (Parent 1): It inherits the idea of using an **additive margin derived from a rank-based signal**. The loss requires the scaled log-probability difference to exceed a margin, `... - margin`, where the margin adapts based on the cost difference's rank within the batch.\n\nNew Coupling Ideas & Modifications:\n1.  **Dynamic Tanh Scaling**: Instead of applying `tanh` directly to the raw `logp_diff`, this child introduces a new coupling where the `logp_diff` is first scaled by a hyperparameter `beta` *before* the `tanh` is applied: `tanh(beta * logp_diff)`. This `beta` parameter acts as an inverse temperature, controlling the steepness of the `tanh` function. A smaller `beta` makes the transition smoother, allowing for a wider range of `logp_diff` values to provide a gradient, while a larger `beta` makes it act more like a step function, focusing only on getting the sign correct.\n2.  **Unified Rank-Modulator for Scaler and Margin**: Both parents use a `rank_gap` signal to create a modulator. This child refines this by creating a single, unified `rank_modulator = softplus(alpha * rank_gap(cost_diff))`. This modulator is then coupled to the loss in two distinct roles: it acts as the multiplicative `scaler` for the `tanh` term and also as the additive `margin` to be overcome. The final loss argument becomes `rank_modulator * tanh(beta * logp_diff) - rank_modulator`, which can be simplified to `rank_modulator * (tanh(beta * logp_diff) - 1)`. This elegant structure forces the model to push `tanh(beta * logp_diff)` towards its maximum value of 1 for high-importance pairs, with the required `logp_diff` magnitude being influenced by `beta`.", "pseudocode": "1. Calculate the difference in log probabilities: `logp_diff = logp_winner - logp_loser`.\n2. Calculate the absolute difference in costs: `cost_diff = abs(cost_winner - cost_loser)`.\n3. Create a normalized, scale-invariant signal from the cost difference using the `rank_gap` operator.\n4. (New Coupling) Create a single, unified `rank_modulator` by applying `softplus` to the scaled rank signal: `rank_modulator = softplus(alpha * rank_gap_signal)`. This value is non-negative and increases with the importance of the pair.\n5. (New Coupling) Scale the `logp_diff` by a hyperparameter `beta`: `scaled_logp_diff = beta * logp_diff`.\n6. (Inherited from Parent 0) Apply `tanh` to the scaled log-probability difference for stability: `clipped_logp_diff = tanh(scaled_logp_diff)`.\n7. (Inherited from Parent 0 & 1, New Coupling) Combine the components. Use the `rank_modulator` as both a multiplicative scaler and an additive margin. The loss argument is `scaler * clipped_logp_diff - margin`, which simplifies to `rank_modulator * (clipped_logp_diff - 1)`.\n8. Compute the final loss using a numerically stable logistic loss function: `loss = -logsigmoid(rank_modulator * (clipped_logp_diff - 1))`.\n9. Return the weighted mean of the loss over the batch.", "hyperparams": {"alpha": 4.0, "beta": 0.5}, "operators_used": ["logsigmoid", "softplus", "tanh", "rank_gap"], "implementation_hint": {"expects": ["cost_a", "cost_b", "log_prob_w", "log_prob_l", "weight"], "returns": "scalar"}, "code": "import torch\nimport torch.nn.functional as F\n\ndef generated_loss(batch, model_output, extra):\n    \"\"\"\n    Computes the Adaptive Rank-Modulated Tanh-Clipping Loss.\n\n    This loss combines a rank-based modulator, which serves as both a scaler and\n    a margin, with a tanh-clipped log-probability difference. The tanh clipping\n    is made adaptive via a `beta` scaling parameter.\n\n    Args:\n        batch (dict): A dictionary containing tensors.\n            'cost_a' (Tensor [N]): Cost of solution 'a'.\n            'cost_b' (Tensor [N]): Cost of solution 'b'.\n            'log_prob_w' (Tensor [N]): Log probability of the winning solution.\n            'log_prob_l' (Tensor [N]): Log probability of the losing solution.\n            'weight' (Tensor [N], optional): Per-pair loss weights.\n        model_output (dict): Model outputs (not used).\n        extra (dict): A dictionary for hyperparameters.\n            'alpha' (float): Controls the magnitude of the rank modulator.\n            'beta' (float): Scales the logp_diff before tanh, controlling steepness.\n\n    Returns:\n        torch.Tensor: A scalar tensor representing the mean loss.\n    \"\"\"\n\n    # Whitelisted operator implemented as a helper function\n    def rank_gap(x, eps=1e-9):\n        \"\"\"Converts a tensor to its normalized ranks within the range [-0.5, 0.5].\"\"\"\n        if x.numel() <= 1:\n            return torch.zeros_like(x)\n        ranks = x.to(torch.float32).argsort().argsort().to(x.dtype)\n        # Normalize ranks to [0, 1]\n        normalized_ranks = ranks / (x.numel() - 1 + eps)\n        # Shift to [-0.5, 0.5]\n        return normalized_ranks - 0.5\n\n    # Retrieve hyperparameters\n    alpha = extra.get('alpha', 4.0)\n    beta = extra.get('beta', 0.5)\n\n    # Unpack tensors from the batch\n    cost_a = batch['cost_a']\n    cost_b = batch['cost_b']\n    log_prob_w = batch['log_prob_w']\n    log_prob_l = batch['log_prob_l']\n    weights = batch.get('weight', None)\n\n    # 1. Calculate the difference in log probabilities\n    logp_diff = log_prob_w - log_prob_l\n\n    # 2. Calculate absolute cost difference\n    cost_diff = torch.abs(cost_a - cost_b)\n\n    # 3. Create a normalized, scale-invariant signal from cost_diff\n    rank_gap_signal = rank_gap(cost_diff)\n\n    # 4. (New Coupling) Create a unified rank modulator\n    rank_modulator = F.softplus(alpha * rank_gap_signal)\n\n    # 5. (New Coupling) Scale logp_diff by beta before clipping\n    scaled_logp_diff = beta * logp_diff\n\n    # 6. (Inherited from Parent 0) Soft-clip the scaled logp_diff\n    clipped_logp_diff = torch.tanh(scaled_logp_diff)\n\n    # 7. (Inherited from Parents & New Coupling) Combine components\n    # The rank_modulator is used as both a multiplicative scaler and an additive margin.\n    # The goal is for `modulator * clipped_logp_diff` to exceed `modulator`.\n    argument = rank_modulator * (clipped_logp_diff - 1)\n\n    # 8. Compute the final loss using numerically stable logsigmoid\n    loss_per_item = -F.logsigmoid(argument)\n\n    # 9. Apply optional weights and compute the mean loss\n    if weights is not None:\n        # Ensure weights sum is non-zero to avoid division by zero\n        loss = (loss_per_item * weights).sum() / weights.sum().clamp(min=1e-8)\n    else:\n        loss = loss_per_item.mean()\n\n    return loss"}, "static_ok": true, "static_reason": "", "dynamic_ok": true, "dynamic_reason": "ok", "loss_value": 1.2550013065338135, "grad_norm": 0.0}
{"generation": 8, "index": 7, "ir": {"name": "Adaptive Dual-Clip Rank Loss", "intuition": "This loss function creates a highly adaptive learning signal by dynamically adjusting both the target margin and the log-probability clipping range based on the relative importance of a preference pair. It is designed for stability and to focus model capacity effectively.\n\nInherited Ideas:\n- From Parent 0 (`Soft-Clipped Rank-Modulated Loss`): It inherits the idea of using a **multiplicative scaler** on a transformed log-probability difference. The scaler, derived from a rank-based signal, amplifies the loss for more important pairs.\n- From Parent 1 (`Rank-Modulated Dual-Clipping Loss`): It inherits the concept of **dynamically clipping the log-probability difference** using `clamp`. The clipping bounds adapt based on the cost difference, preventing the model from wasting capacity on insignificant pairs while allowing for a strong signal on important ones.\n\nNew Coupling Ideas & Modifications:\n1.  **Dual-Purpose Rank Modulator**: A single, unified `rank_modulator` is created from the rank-gapped cost difference: `rank_modulator = softplus(alpha * rank_gap(cost_diff))`. This modulator is coupled to the loss in two distinct ways: it defines the dynamic clipping range `[-rank_modulator, rank_modulator]` for `logp_diff`, and it also acts as a multiplicative scaler on the final loss argument. This dual role ensures that the learning signal's magnitude and the log-probability's allowed range are both governed by the same underlying importance signal.\n2.  **Normalized Log-Probability Difference**: Instead of directly using the clamped `logp_diff`, it is first normalized by its clipping bound (`rank_modulator`). The term becomes `clipped_logp_diff / rank_modulator`, which effectively maps the log-probability difference to a `[-1, 1]` range, similar to `tanh`. This normalization makes the subsequent margin comparison scale-invariant. The loss then encourages this normalized value to exceed a fixed margin `beta`, resulting in the final argument: `scaler * (normalized_clipped_logp_diff - beta)`.", "pseudocode": "1. Calculate the difference in log probabilities: `logp_diff = logp_winner - logp_loser`.\n2. Calculate the absolute difference in costs: `cost_diff = abs(cost_winner - cost_loser)`.\n3. Create a stable, scale-invariant importance signal by applying the `rank_gap` operator to the cost difference.\n4. (New Coupling) Create a single, unified `rank_modulator` from the rank signal: `rank_modulator = softplus(alpha * rank_gap_signal)`. This value is smooth, non-negative, and increases with the importance of the pair.\n5. (Inherited from Parent 1) Dynamically clip the log-probability difference using the modulator as the bound: `clipped_logp_diff = clamp(logp_diff, min=-rank_modulator, max=rank_modulator)`.\n6. (New Coupling) Normalize the clipped log-probability difference by its clipping bound (the modulator) to bring it into a `[-1, 1]` range: `normalized_logp = clipped_logp_diff / (rank_modulator + epsilon)`.\n7. (Inherited from Parent 0) The `rank_modulator` also acts as a multiplicative scaler for the final loss argument.\n8. Combine the components. The goal is for the `normalized_logp` to exceed a fixed margin `beta`. The final loss argument is `scaler * (normalized_logp - beta)` which is `rank_modulator * (normalized_logp - beta)`.\n9. Compute the final loss using a numerically stable logistic loss: `loss = -logsigmoid(argument)`.\n10. Return the weighted mean of the loss over the batch.", "hyperparams": {"alpha": 4.0, "beta": 0.5}, "operators_used": ["logsigmoid", "softplus", "clamp", "rank_gap"], "implementation_hint": {"expects": ["cost_a", "cost_b", "log_prob_w", "log_prob_l", "weight"], "returns": "scalar"}, "code": "import torch\nimport torch.nn.functional as F\n\ndef generated_loss(batch, model_output, extra):\n    \"\"\"\n    Computes the Adaptive Dual-Clip Rank Loss.\n\n    This loss uses a rank-based modulator to both dynamically clip the log-probability\n    difference and to scale the final loss term. The clipped logp_diff is normalized\n    before a margin is applied.\n\n    Args:\n        batch (dict): A dictionary containing tensors.\n            'cost_a' (Tensor [N]): Cost of solution 'a'.\n            'cost_b' (Tensor [N]): Cost of solution 'b'.\n            'log_prob_w' (Tensor [N]): Log probability of the winning solution.\n            'log_prob_l' (Tensor [N]): Log probability of the losing solution.\n            'weight' (Tensor [N], optional): Per-pair loss weights.\n        model_output (dict): Model outputs (not used).\n        extra (dict): A dictionary for hyperparameters.\n            'alpha' (float): Controls the magnitude of the rank modulator.\n            'beta' (float): A fixed margin for the normalized logp difference.\n\n    Returns:\n        torch.Tensor: A scalar tensor representing the mean loss.\n    \"\"\"\n\n    # Whitelisted operator implemented as a helper function\n    def rank_gap(x, eps=1e-9):\n        \"\"\"Converts a tensor to its normalized ranks within the range [-0.5, 0.5].\"\"\"\n        if x.numel() <= 1:\n            return torch.zeros_like(x)\n        ranks = x.to(torch.float32).argsort().argsort().to(x.dtype)\n        normalized_ranks = ranks / (x.numel() - 1 + eps)\n        return normalized_ranks - 0.5\n\n    # Retrieve hyperparameters\n    alpha = extra.get('alpha', 4.0)\n    beta = extra.get('beta', 0.5)\n    epsilon = 1e-9\n\n    # Unpack tensors from the batch\n    cost_a = batch['cost_a']\n    cost_b = batch['cost_b']\n    log_prob_w = batch['log_prob_w']\n    log_prob_l = batch['log_prob_l']\n    weights = batch.get('weight', None)\n\n    # 1. Calculate the difference in log probabilities\n    logp_diff = log_prob_w - log_prob_l\n\n    # 2. Calculate absolute cost difference\n    cost_diff = torch.abs(cost_a - cost_b)\n\n    # 3. Create a stable rank-based signal\n    rank_gap_signal = rank_gap(cost_diff)\n\n    # 4. (New Coupling) Create a unified rank modulator\n    rank_modulator = F.softplus(alpha * rank_gap_signal)\n\n    # 5. (Inherited from Parent 1) Dynamically clip logp_diff\n    clipped_logp_diff = torch.clamp(logp_diff, min=-rank_modulator, max=rank_modulator)\n\n    # 6. (New Coupling) Normalize the clipped logp_diff to [-1, 1]\n    normalized_logp = clipped_logp_diff / (rank_modulator + epsilon)\n\n    # 7. (Inherited from Parent 0) The modulator also acts as a scaler\n    scaler = rank_modulator\n\n    # 8. Combine components: scaler * (normalized_value - margin)\n    argument = scaler * (normalized_logp - beta)\n\n    # 9. Compute the final loss using numerically stable logsigmoid\n    loss_per_item = -F.logsigmoid(argument)\n\n    # 10. Apply optional weights and compute the mean loss\n    if weights is not None:\n        loss = (loss_per_item * weights).sum() / weights.sum().clamp(min=1e-8)\n    else:\n        loss = loss_per_item.mean()\n\n    return loss"}, "static_ok": true, "static_reason": "", "dynamic_ok": true, "dynamic_reason": "ok", "loss_value": 0.9440773129463196, "grad_norm": 0.0}
{"generation": 9, "index": 0, "ir": {"name": "Progressive Margin DPO-Style Loss", "intuition": "This loss function adapts the core structure of Direct Preference Optimization (DPO) by introducing a dynamic, cost-aware margin. It aims to balance the DPO objective of increasing the relative log-probability of the preferred response with a margin that grows based on the quality difference between the pair, preventing the model from pushing probabilities arbitrarily high for all pairs.\n\nInherited Ideas:\n- From `Adaptive Rank-Modulated Tanh-Clipping Loss` (Parent 0): It inherits the idea of scaling the log-probability difference by a hyperparameter `beta` (`beta * logp_diff`). This `beta` acts as an inverse temperature, controlling the sensitivity of the loss to the log-probability gap.\n- From both `Adaptive Rank-Modulated Tanh-Clipping Loss` (Parent 0) and `Soft-Clipped Rank-Modulated Loss` (Parent 1): It inherits the use of a `rank_gap` signal derived from the cost difference. This provides a robust, scale-invariant measure of a preference pair's importance within the batch, which is then used to create a dynamic margin.\n\nNew Coupling Ideas & Modifications:\n1. **DPO-Style Structure**: Instead of a generic `logsigmoid(f(logp_diff, cost_diff))` structure, this loss adopts the DPO formulation: `logsigmoid(beta * logp_diff - margin)`. This directly frames the problem as ensuring the scaled log-probability difference exceeds a certain margin.\n2. **Progressive Rank-Based Margin**: The key innovation is how the `margin` is constructed and coupled. It's not a fixed value but is progressively introduced based on the pair's importance. First, a `rank_gap` signal is computed from the cost difference. This signal is then scaled by a hyperparameter `alpha` and passed through `softplus` to create a smooth, non-negative `rank_margin_base`. Finally, this base is coupled with the DPO objective via a `log` transformation: `margin = log(1 + rank_margin_base)`. This `log` coupling ensures that for pairs with negligible cost differences (rank_margin_base ≈ 0), the margin is also close to zero, and the loss simplifies to standard DPO. As the cost difference becomes significant, the margin grows sub-linearly, demanding a larger `logp_diff` without causing instability from excessively large margin values.", "pseudocode": "1. Calculate the difference in log probabilities: `logp_diff = logp_winner - logp_loser`.\n2. (Inherited from Parent 0) Scale the log-probability difference by an inverse temperature hyperparameter `beta`: `scaled_logp_diff = beta * logp_diff`.\n3. Calculate the absolute difference in costs: `cost_diff = abs(cost_winner - cost_loser)`.\n4. (Inherited from Parents 0 & 1) Create a scale-invariant signal from the cost differences using the `rank_gap` operator, which normalizes ranks to the [-0.5, 0.5] range.\n5. (New Coupling) Create a non-negative base for the margin by applying `softplus` to the scaled rank signal: `rank_margin_base = softplus(alpha * rank_gap_signal)`.\n6. (New Coupling) Transform the margin base into the final margin using a `log` function for stable, progressive growth: `margin = log(1 + rank_margin_base)`. The `+1` ensures the argument to `log` is always positive.\n7. (New Coupling - DPO-Style) Combine the scaled log-probability difference and the margin into the final argument for the logistic loss: `argument = scaled_logp_diff - margin`.\n8. Compute the final loss using a numerically stable logistic loss function: `loss = -logsigmoid(argument)`.\n9. Return the weighted mean of the loss over the batch.", "hyperparams": {"alpha": 4.0, "beta": 0.1}, "operators_used": ["logsigmoid", "softplus", "log", "rank_gap"], "implementation_hint": {"expects": ["cost_a", "cost_b", "log_prob_w", "log_prob_l", "weight"], "returns": "scalar"}, "code": "import torch\nimport torch.nn.functional as F\n\ndef generated_loss(batch, model_output, extra):\n    \"\"\"\n    Computes the Progressive Margin DPO-Style Loss.\n\n    This loss adapts the DPO framework by subtracting a dynamic, rank-based margin\n    from the scaled log-probability difference. The margin grows sub-linearly with\n    the importance of the preference pair.\n\n    Args:\n        batch (dict): A dictionary containing tensors.\n            'cost_a' (Tensor [N]): Cost of solution 'a'.\n            'cost_b' (Tensor [N]): Cost of solution 'b'.\n            'log_prob_w' (Tensor [N]): Log probability of the winning solution.\n            'log_prob_l' (Tensor [N]): Log probability of the losing solution.\n            'weight' (Tensor [N], optional): Per-pair loss weights.\n        model_output (dict): Model outputs (not used).\n        extra (dict): A dictionary for hyperparameters.\n            'alpha' (float): Controls the magnitude of the rank-based margin.\n            'beta' (float): The inverse temperature for scaling logp_diff, as in DPO.\n\n    Returns:\n        torch.Tensor: A scalar tensor representing the mean loss.\n    \"\"\"\n\n    # Whitelisted operator implemented as a helper function\n    def rank_gap(x, eps=1e-9):\n        \"\"\"Converts a tensor to its normalized ranks within the range [-0.5, 0.5].\"\"\"\n        if x.numel() <= 1:\n            return torch.zeros_like(x)\n        ranks = x.to(torch.float32).argsort().argsort().to(x.dtype)\n        # Normalize ranks to [0, 1]\n        normalized_ranks = ranks / (x.numel() - 1 + eps)\n        # Shift to [-0.5, 0.5]\n        return normalized_ranks - 0.5\n\n    # Retrieve hyperparameters\n    alpha = extra.get('alpha', 4.0)\n    beta = extra.get('beta', 0.1)\n\n    # Unpack tensors from the batch\n    cost_a = batch['cost_a']\n    cost_b = batch['cost_b']\n    log_prob_w = batch['log_prob_w']\n    log_prob_l = batch['log_prob_l']\n    weights = batch.get('weight', None)\n\n    # 1. Calculate the difference in log probabilities\n    logp_diff = log_prob_w - log_prob_l\n\n    # 2. (Inherited from Parent 0) Scale logp_diff by beta\n    scaled_logp_diff = beta * logp_diff\n\n    # 3. Calculate absolute cost difference\n    cost_diff = torch.abs(cost_a - cost_b)\n\n    # 4. (Inherited from Parents) Create a scale-invariant rank signal\n    rank_gap_signal = rank_gap(cost_diff)\n\n    # 5. (New Coupling) Create a non-negative margin base via softplus\n    rank_margin_base = F.softplus(alpha * rank_gap_signal)\n\n    # 6. (New Coupling) Transform the base into the final margin using log\n    # This creates a margin that grows sub-linearly with pair importance.\n    margin = torch.log1p(rank_margin_base)\n\n    # 7. (New Coupling - DPO-Style) Combine into the final loss argument\n    # The goal is for the scaled logp_diff to exceed the dynamic margin.\n    argument = scaled_logp_diff - margin\n\n    # 8. Compute the final loss using numerically stable logsigmoid\n    loss_per_item = -F.logsigmoid(argument)\n\n    # 9. Apply optional weights and compute the mean loss\n    if weights is not None:\n        # Ensure weights sum is non-zero to avoid division by zero\n        loss = (loss_per_item * weights).sum() / weights.sum().clamp(min=1e-8)\n    else:\n        loss = loss_per_item.mean()\n\n    return loss"}, "static_ok": true, "static_reason": "", "dynamic_ok": true, "dynamic_reason": "ok", "loss_value": 1.029091477394104, "grad_norm": 0.0}
{"generation": 9, "index": 1, "ir": {"name": "Rank-Normalized Tanh-Margin Loss", "intuition": "This loss function creates a stable and adaptive learning signal by dynamically setting a margin based on the rank of the cost difference. The goal is to ensure the log-probability gap between the winner and loser exceeds this rank-aware margin.\n\nInherited Ideas:\n- From `Adaptive Rank-Modulated Tanh-Clipping Loss` (Parent 0) and `Soft-Clipped Rank-Modulated Loss` (Parent 1): It inherits the use of a **rank-based signal** derived from the cost difference (`rank_gap(cost_diff)`) to make the loss robust to the scale and distribution of costs. It also inherits the concept of using this signal to define a **dynamic margin** that the model must overcome.\n- From `Adaptive Rank-Modulated Tanh-Clipping Loss` (Parent 0): It inherits the idea of **scaling the log-probability difference** with a hyperparameter `beta` *before* applying a non-linearity (`tanh(beta * logp_diff)`). This `beta` acts as an inverse temperature, controlling the sensitivity of the loss to the magnitude of the log-probability difference.\n\nNew Coupling Ideas & Modifications:\n1. **Direct Rank-to-Margin Mapping**: Both parents use the rank signal to create a multiplicative scaler. This child simplifies this by directly using the normalized rank signal to define an additive margin. The rank signal `rank_gap(cost_diff)` is first shifted to be non-negative (`rank_gap + 0.5`) and then scaled by a hyperparameter `gamma`. This creates a margin `m = gamma * (rank_gap(cost_diff) + 0.5)` that is directly proportional to the rank of the cost difference within the batch, ranging from `0` to `gamma`.\n2. **Tanh-Stabilized Margin Objective**: The core objective is `logp_diff > margin`. However, to prevent extreme gradients when `logp_diff` is very large or very small, this objective is stabilized by coupling it with a `tanh` function. The final loss argument is `tanh(beta * logp_diff) - tanh(beta * margin)`. This structure bounds the loss argument, ensuring numerical stability. The `beta` hyperparameter controls the steepness of the `tanh` function for both the log-probability difference and the margin, creating a symmetric and stable comparison.", "pseudocode": "1. Calculate the difference in log probabilities: `logp_diff = logp_winner - logp_loser`.\n2. Calculate the absolute difference in costs: `cost_diff = abs(cost_winner - cost_loser)`.\n3. (Inherited from Parents) Create a normalized, scale-invariant signal from the cost difference using the `rank_gap` operator, which maps cost differences to ranks in [-0.5, 0.5].\n4. (New Coupling) Create a dynamic, rank-aware margin. Shift the rank signal to be non-negative (`rank_gap + 0.5`) and scale it by a hyperparameter `gamma`. This results in `margin = gamma * (rank_gap_signal + 0.5)`.\n5. (Inherited from Parent 0) Scale the `logp_diff` by a hyperparameter `beta`: `scaled_logp_diff = beta * logp_diff`.\n6. (New Coupling) To create a stable objective, apply `tanh` to both the scaled log-probability difference and the scaled margin. The loss argument becomes `tanh(scaled_logp_diff) - tanh(beta * margin)`.\n7. Compute the final loss using a numerically stable logistic loss function: `loss = -logsigmoid(tanh(beta * logp_diff) - tanh(beta * margin))`.\n8. Return the weighted mean of the loss over the batch.", "hyperparams": {"beta": 1.0, "gamma": 2.0}, "operators_used": ["logsigmoid", "tanh", "rank_gap"], "implementation_hint": {"expects": ["cost_a", "cost_b", "log_prob_w", "log_prob_l", "weight"], "returns": "scalar"}, "code": "import torch\nimport torch.nn.functional as F\n\ndef generated_loss(batch, model_output, extra):\n    \"\"\"\n    Computes the Rank-Normalized Tanh-Margin Loss.\n\n    This loss requires the log-probability difference to exceed a dynamic margin\n    derived from the rank of the cost difference. The comparison is stabilized\n    by applying a tanh function to both terms.\n\n    Args:\n        batch (dict): A dictionary containing tensors.\n            'cost_a' (Tensor [N]): Cost of solution 'a'.\n            'cost_b' (Tensor [N]): Cost of solution 'b'.\n            'log_prob_w' (Tensor [N]): Log probability of the winning solution.\n            'log_prob_l' (Tensor [N]): Log probability of the losing solution.\n            'weight' (Tensor [N], optional): Per-pair loss weights.\n        model_output (dict): Model outputs (not used).\n        extra (dict): A dictionary for hyperparameters.\n            'beta' (float): Scales logp_diff and margin before tanh (inverse temperature).\n            'gamma' (float): Scales the rank-based margin.\n\n    Returns:\n        torch.Tensor: A scalar tensor representing the mean loss.\n    \"\"\"\n\n    # Whitelisted operator implemented as a helper function\n    def rank_gap(x, eps=1e-9):\n        \"\"\"Converts a tensor to its normalized ranks within the range [-0.5, 0.5].\"\"\"\n        if x.numel() <= 1:\n            return torch.zeros_like(x)\n        ranks = x.to(torch.float32).argsort().argsort().to(x.dtype)\n        # Normalize ranks to [0, 1]\n        normalized_ranks = ranks / (x.numel() - 1 + eps)\n        # Shift to [-0.5, 0.5]\n        return normalized_ranks - 0.5\n\n    # Retrieve hyperparameters\n    beta = extra.get('beta', 1.0)\n    gamma = extra.get('gamma', 2.0)\n\n    # Unpack tensors from the batch\n    cost_a = batch['cost_a']\n    cost_b = batch['cost_b']\n    log_prob_w = batch['log_prob_w']\n    log_prob_l = batch['log_prob_l']\n    weights = batch.get('weight', None)\n\n    # 1. Calculate the difference in log probabilities\n    logp_diff = log_prob_w - log_prob_l\n\n    # 2. Calculate absolute cost difference\n    cost_diff = torch.abs(cost_a - cost_b)\n\n    # 3. (Inherited) Create a normalized rank signal from cost_diff\n    rank_gap_signal = rank_gap(cost_diff)\n\n    # 4. (New Coupling) Create a dynamic, rank-aware margin\n    # Shift rank_gap from [-0.5, 0.5] to [0, 1] and scale by gamma\n    margin = gamma * (rank_gap_signal + 0.5)\n\n    # 5. (Inherited) Scale logp_diff by beta\n    scaled_logp_diff = beta * logp_diff\n\n    # 6. (New Coupling) Create a tanh-stabilized objective\n    # The goal is logp_diff > margin, stabilized by tanh.\n    argument = torch.tanh(scaled_logp_diff) - torch.tanh(beta * margin)\n\n    # 7. Compute the final loss using numerically stable logsigmoid\n    loss_per_item = -F.logsigmoid(argument)\n\n    # 8. Apply optional weights and compute the mean loss\n    if weights is not None:\n        # Ensure weights sum is non-zero to avoid division by zero\n        loss = (loss_per_item * weights).sum() / weights.sum().clamp(min=1e-8)\n    else:\n        loss = loss_per_item.mean()\n\n    return loss"}, "static_ok": true, "static_reason": "", "dynamic_ok": true, "dynamic_reason": "ok", "loss_value": 1.0809324979782104, "grad_norm": 0.0}
{"generation": 9, "index": 2, "ir": {"name": "Adaptive Dual-Signal Tanh Loss", "intuition": "This loss function creates a stable and adaptive learning signal by separating the modulation of the log-probability difference into two distinct channels: a multiplicative scaler and an additive margin. The scaler focuses on the magnitude of the log-probability difference, while the margin sets a dynamic target based on the relative importance of the preference pair.\n\nInherited Ideas:\n- From `Adaptive Rank-Modulated Tanh-Clipping Loss` (Parent 0): It inherits the idea of scaling the log-probability difference *before* applying the tanh function: `tanh(beta * logp_diff)`. This `beta` acts as an inverse temperature, controlling the steepness of the tanh saturation and thus the range of logp differences that provide a useful gradient.\n- From both parents: It inherits the use of a rank-based signal (`rank_gap`) to derive a modulator from the cost difference. This makes the loss robust to the scale and distribution of costs by operating on relative rankings within the batch.\n\nNew Coupling Ideas & Modifications:\n1. **Dual-Signal Modulation**: Instead of using a single `rank_modulator` for both scaling and margin as the parents do (`modulator * (tanh(...) - 1)`), this child decouples them. It creates two separate signals from the rank-based cost difference: a `scaler = softplus(alpha * rank_gap_signal)` and a `margin = softplus(gamma * rank_gap_signal)`. This allows for independent control over how much to amplify the logp_diff gradient (via `scaler` and `alpha`) versus how large of a target margin to set (via `margin` and `gamma`).\n2. **Dynamic Logp Normalization**: The log-probability difference (`logp_diff`) is normalized using z-scoring (`zscore(logp_diff)`) before being scaled by `beta`. This new coupling stabilizes training by ensuring the input to the `tanh` function has a consistent, zero-mean, unit-variance distribution across different batches and training stages. It prevents the model from pushing `logp_diff` to extreme values where `tanh` would saturate and gradients would vanish, instead encouraging a well-distributed separation of probabilities.", "pseudocode": "1. Calculate the difference in log probabilities: `logp_diff = logp_winner - logp_loser`.\n2. (New Coupling) Normalize the logp difference using z-scoring for stability: `normalized_logp_diff = zscore(logp_diff)`.\n3. Calculate the absolute difference in costs: `cost_diff = abs(cost_winner - cost_loser)`.\n4. (Inherited from both) Create a normalized, scale-invariant signal from the cost difference using the `rank_gap` operator: `rank_gap_signal = rank_gap(cost_diff)`.\n5. (New Coupling) Create two distinct modulators from the rank signal:\n   a. A multiplicative scaler: `scaler = softplus(alpha * rank_gap_signal)`.\n   b. An additive margin: `margin = softplus(gamma * rank_gap_signal)`.\n6. (Inherited from Parent 0) Scale the normalized logp difference by `beta` and apply `tanh` for stable clipping: `clipped_term = tanh(beta * normalized_logp_diff)`.\n7. Combine the components into the final loss argument: `argument = scaler * clipped_term - margin`. The goal is to make the scaled, clipped logp difference exceed the dynamic, rank-based margin.\n8. Compute the final loss using a numerically stable logistic loss function: `loss = -logsigmoid(argument)`.\n9. Return the weighted mean of the loss over the batch.", "hyperparams": {"alpha": 2.0, "beta": 1.0, "gamma": 4.0}, "operators_used": ["logsigmoid", "softplus", "tanh", "zscore", "rank_gap"], "implementation_hint": {"expects": ["cost_a", "cost_b", "logp_a", "logp_b"], "returns": "scalar"}, "code": "import torch\nimport torch.nn.functional as F\n\ndef generated_loss(batch, model_output, extra):\n    \"\"\"\n    Computes the Adaptive Dual-Signal Tanh Loss.\n\n    This loss decouples the rank-based modulation into a multiplicative scaler and an\n    additive margin. It also normalizes the logp_diff using z-scoring before\n    applying a beta-scaled tanh function for enhanced stability.\n\n    Args:\n        batch (dict): A dictionary containing tensors.\n            'cost_a' (Tensor [N]): Cost of solution 'a'.\n            'cost_b' (Tensor [N]): Cost of solution 'b'.\n            'log_prob_w' (Tensor [N]): Log probability of the winning solution.\n            'log_prob_l' (Tensor [N]): Log probability of the losing solution.\n            'weight' (Tensor [N], optional): Per-pair loss weights.\n        model_output (dict): Model outputs (not used).\n        extra (dict): A dictionary for hyperparameters.\n            'alpha' (float): Controls the magnitude of the multiplicative scaler.\n            'beta' (float): Scales the logp_diff before tanh, controlling steepness.\n            'gamma' (float): Controls the magnitude of the additive margin.\n\n    Returns:\n        torch.Tensor: A scalar tensor representing the mean loss.\n    \"\"\"\n\n    # Whitelisted operators implemented as helper functions\n    def rank_gap(x, eps=1e-9):\n        \"\"\"Converts a tensor to its normalized ranks within the range [-0.5, 0.5].\"\"\"\n        if x.numel() <= 1:\n            return torch.zeros_like(x)\n        ranks = x.to(torch.float32).argsort().argsort().to(x.dtype)\n        normalized_ranks = ranks / (x.numel() - 1 + eps)\n        return normalized_ranks - 0.5\n\n    def zscore(x, eps=1e-8):\n        \"\"\"Computes z-score normalization for a tensor.\"\"\"\n        if x.numel() <= 1:\n            return torch.zeros_like(x)\n        return (x - x.mean()) / (x.std() + eps)\n\n    # Retrieve hyperparameters\n    alpha = extra.get('alpha', 2.0)\n    beta = extra.get('beta', 1.0)\n    gamma = extra.get('gamma', 4.0)\n\n    # Unpack tensors from the batch\n    cost_a = batch['cost_a']\n    cost_b = batch['cost_b']\n    log_prob_w = batch['log_prob_w']\n    log_prob_l = batch['log_prob_l']\n    weights = batch.get('weight', None)\n\n    # 1. Calculate the difference in log probabilities\n    logp_diff = log_prob_w - log_prob_l\n\n    # 2. (New Coupling) Normalize logp_diff with z-scoring for stability\n    normalized_logp_diff = zscore(logp_diff)\n\n    # 3. Calculate absolute cost difference\n    cost_diff = torch.abs(cost_a - cost_b)\n\n    # 4. (Inherited) Use rank_gap for a stable, scale-invariant cost signal\n    rank_gap_signal = rank_gap(cost_diff)\n\n    # 5. (New Coupling) Create decoupled scaler and margin\n    scaler = F.softplus(alpha * rank_gap_signal)\n    margin = F.softplus(gamma * rank_gap_signal)\n\n    # 6. (Inherited from Parent 0) Apply beta-scaled tanh to normalized logp_diff\n    clipped_term = torch.tanh(beta * normalized_logp_diff)\n\n    # 7. Combine components into the loss argument\n    argument = scaler * clipped_term - margin\n\n    # 8. Compute the final loss using numerically stable logsigmoid\n    loss_per_item = -F.logsigmoid(argument)\n\n    # 9. Apply optional weights and compute the mean loss\n    if weights is not None and weights.sum() > 0:\n        loss = (loss_per_item * weights).sum() / weights.sum().clamp(min=1e-8)\n    else:\n        loss = loss_per_item.mean()\n\n    return loss"}, "static_ok": true, "static_reason": "", "dynamic_ok": true, "dynamic_reason": "ok", "loss_value": 1.2550013065338135, "grad_norm": 0.0}
{"generation": 9, "index": 3, "ir": {"name": "Progressive Rank-Adaptive Tanh Loss", "intuition": "This loss function dynamically adjusts its learning signal based on the relative importance of preference pairs, while progressively increasing the difficulty for the model as training proceeds. The design focuses on stability and curriculum learning.\n\nInherited Ideas:\n- From `Adaptive Rank-Modulated Tanh-Clipping Loss` (Parent 0): It inherits the idea of scaling the log-probability difference *before* applying the `tanh` function: `tanh(beta * logp_diff)`. This `beta` parameter acts as an inverse temperature, controlling the sharpness of the `tanh` function and thus the gradient's sensitivity to the magnitude of `logp_diff`.\n- From `Soft-Clipped Rank-Modulated Loss` (Parent 1) and `Adaptive Rank-Modulated Tanh-Clipping Loss` (Parent 0): It inherits the use of a `rank_gap` signal derived from the cost difference to create a scale-invariant, robust modulator. This modulator, `softplus(alpha * rank_gap(...))`, amplifies the loss for pairs with a larger cost difference, focusing model capacity on more significant preferences.\n\nNew Coupling Ideas & Modifications:\n1.  **Decoupled Scaler and Margin**: Unlike the parents, which use a single `rank_modulator` for both scaling and margin, this child decouples them. The `rank_modulator` acts only as a multiplicative `scaler` on the `tanh` term. The margin is a separate, constant hyperparameter `gamma`. This allows for independent control over the amplification of the loss signal (via `alpha`) and the minimum required log-probability difference (via `gamma`). The loss argument becomes `scaler * tanh(...) - margin`.\n2.  **Progressive Difficulty via Beta Schedule**: A new coupling is introduced where the `beta` hyperparameter is not fixed but is scheduled to increase over time based on the training `step`. Specifically, `current_beta = beta_initial * exp(step / beta_schedule_tau)`. This creates a curriculum: early in training, a small `beta` results in a smooth `tanh`, allowing the model to learn the correct preference direction easily. As training progresses, `beta` increases, making the `tanh` function steeper and pushing the model to produce a larger `logp_diff` to satisfy the margin, refining the policy.", "pseudocode": "1. Calculate the difference in log probabilities: `logp_diff = logp_winner - logp_loser`.\n2. Calculate the absolute difference in costs: `cost_diff = abs(cost_winner - cost_loser)`.\n3. (Inherited from Parents) Create a normalized, scale-invariant signal from the cost difference using the `rank_gap` operator.\n4. (Inherited from Parents) Create a `rank_scaler` by applying `softplus` to the scaled rank signal: `rank_scaler = softplus(alpha * rank_gap_signal)`. This value acts as a multiplicative weight.\n5. (New Coupling) Implement a beta schedule. Calculate the current `beta` value based on the training step: `current_beta = beta_initial * exp(step / beta_schedule_tau)`.\n6. (Inherited from Parent 0) Scale the `logp_diff` by the scheduled `current_beta`: `scaled_logp_diff = current_beta * logp_diff`.\n7. Apply `tanh` to the scaled log-probability difference for stability: `clipped_logp_diff = tanh(scaled_logp_diff)`.\n8. (New Coupling) Combine the components with a decoupled scaler and margin. The loss argument is `rank_scaler * clipped_logp_diff - gamma`, where `gamma` is a fixed margin.\n9. Compute the final loss using a numerically stable logistic loss function: `loss = -logsigmoid(rank_scaler * clipped_logp_diff - gamma)`.\n10. Return the weighted mean of the loss over the batch.", "hyperparams": {"alpha": 4.0, "gamma": 0.5, "beta_initial": 0.1, "beta_schedule_tau": 20000.0}, "operators_used": ["logsigmoid", "softplus", "tanh", "exp", "rank_gap"], "implementation_hint": {"expects": ["cost_a", "cost_b", "log_prob_w", "log_prob_l", "weight", "step"], "returns": "scalar"}, "code": "import torch\nimport torch.nn.functional as F\n\ndef generated_loss(batch, model_output, extra):\n    \"\"\"\n    Computes the Progressive Rank-Adaptive Tanh Loss.\n\n    This loss combines a rank-based scaler with a tanh-clipped log-probability\n    difference. It introduces a curriculum by scheduling the `beta` parameter,\n    which controls the steepness of the tanh function, to increase with the\n    training step. The margin is a fixed hyperparameter.\n\n    Args:\n        batch (dict): A dictionary containing tensors.\n            'cost_a' (Tensor [N]): Cost of solution 'a'.\n            'cost_b' (Tensor [N]): Cost of solution 'b'.\n            'log_prob_w' (Tensor [N]): Log probability of the winning solution.\n            'log_prob_l' (Tensor [N]): Log probability of the losing solution.\n            'weight' (Tensor [N], optional): Per-pair loss weights.\n            'step' (int, optional): The current training step for scheduling.\n        model_output (dict): Model outputs (not used).\n        extra (dict): A dictionary for hyperparameters.\n            'alpha' (float): Controls the magnitude of the rank scaler.\n            'gamma' (float): A fixed margin to be overcome.\n            'beta_initial' (float): The initial value for the beta schedule.\n            'beta_schedule_tau' (float): The time constant for the beta schedule.\n\n    Returns:\n        torch.Tensor: A scalar tensor representing the mean loss.\n    \"\"\"\n\n    # Whitelisted operator implemented as a helper function\n    def rank_gap(x, eps=1e-9):\n        \"\"\"Converts a tensor to its normalized ranks within the range [-0.5, 0.5].\"\"\"\n        if x.numel() <= 1:\n            return torch.zeros_like(x)\n        ranks = x.to(torch.float32).argsort().argsort().to(x.dtype)\n        normalized_ranks = ranks / (x.numel() - 1 + eps)\n        return normalized_ranks - 0.5\n\n    # Retrieve hyperparameters\n    alpha = extra.get('alpha', 4.0)\n    gamma = extra.get('gamma', 0.5)\n    beta_initial = extra.get('beta_initial', 0.1)\n    beta_schedule_tau = extra.get('beta_schedule_tau', 20000.0)\n\n    # Unpack tensors from the batch\n    cost_a = batch['cost_a']\n    cost_b = batch['cost_b']\n    log_prob_w = batch['log_prob_w']\n    log_prob_l = batch['log_prob_l']\n    weights = batch.get('weight', None)\n    step = batch.get('step', 0) # Get current step, default to 0\n\n    # 1. Calculate the difference in log probabilities\n    logp_diff = log_prob_w - log_prob_l\n\n    # 2. Calculate absolute cost difference\n    cost_diff = torch.abs(cost_a - cost_b)\n\n    # 3. (Inherited) Create a normalized signal from cost_diff\n    rank_gap_signal = rank_gap(cost_diff)\n\n    # 4. (Inherited) Create a rank-based scaler\n    rank_scaler = F.softplus(alpha * rank_gap_signal)\n\n    # 5. (New Coupling) Implement beta schedule\n    current_beta = beta_initial * torch.exp(torch.tensor(step / beta_schedule_tau, device=logp_diff.device))\n\n    # 6. (Inherited from Parent 0) Scale logp_diff by the scheduled beta\n    scaled_logp_diff = current_beta * logp_diff\n\n    # 7. Apply tanh for stability\n    clipped_logp_diff = torch.tanh(scaled_logp_diff)\n\n    # 8. (New Coupling) Combine with decoupled scaler and margin\n    argument = rank_scaler * clipped_logp_diff - gamma\n\n    # 9. Compute the final loss using numerically stable logsigmoid\n    loss_per_item = -F.logsigmoid(argument)\n\n    # 10. Apply optional weights and compute the mean loss\n    if weights is not None:\n        loss = (loss_per_item * weights).sum() / weights.sum().clamp(min=1e-8)\n    else:\n        loss = loss_per_item.mean()\n\n    return loss"}, "static_ok": true, "static_reason": "", "dynamic_ok": true, "dynamic_reason": "ok", "loss_value": 0.9740771055221558, "grad_norm": 0.0}
{"generation": 9, "index": 4, "ir": {"name": "Adaptive Rank-Gapped Margin Loss", "intuition": "This loss function creates a stable and adaptive learning signal by framing the preference learning task as achieving a dynamic margin. The margin itself is derived from the relative importance of the preference pair within a batch, ensuring the model focuses its capacity on pairs with significant cost differences.\n\nInherited Ideas:\n- From both parents (`Adaptive Rank-Modulated Tanh-Clipping Loss` and `Soft-Clipped Rank-Modulated Loss`), it inherits the core concept of using a **rank-based modulator** derived from cost differences. This is achieved via `rank_gap(cost_diff)`, which provides a scale-invariant and robust signal of a pair's importance. The modulator is then shaped using `softplus` for smoothness and non-negativity.\n- From `Adaptive Rank-Modulated Tanh-Clipping Loss` (Parent 0), it inherits the idea of **scaling the log-probability difference** with a hyperparameter (`beta`). This `beta` acts as an inverse temperature, controlling the sensitivity of the loss to the magnitude of `logp_diff`.\n\nNew Coupling Ideas & Modifications:\n1.  **Direct Margin Formulation**: Instead of the `scaler * (clipped_term - 1)` structure seen in the parents, this child simplifies the objective to a direct margin loss: `scaled_logp_diff - margin > 0`. The `margin` is directly the `rank_modulator`. This makes the objective more interpretable: the scaled log-probability difference must exceed a margin whose value is determined by the pair's rank-based importance.\n2.  **Softplus Activation for the Loss Argument**: To ensure the loss is well-behaved and focuses only on violations of the margin (i.e., when `scaled_logp_diff < margin`), the entire argument `margin - scaled_logp_diff` is passed through a `softplus` function. This is a smooth approximation of the `ReLU` (hinge loss), providing a non-zero loss only when the margin is not met, and yielding smooth gradients. The final loss is `softplus(margin - beta * logp_diff)`, a structure that is both stable and clearly expresses the learning objective.", "pseudocode": "1. Calculate the difference in log probabilities: `logp_diff = logp_winner - logp_loser`.\n2. Calculate the absolute difference in costs: `cost_diff = abs(cost_winner - cost_loser)`.\n3. (Inherited from both parents) Create a normalized, scale-invariant signal from the cost difference using the `rank_gap` operator.\n4. (Inherited from both parents) Create a dynamic `margin` by applying `softplus` to the scaled rank signal: `margin = softplus(alpha * rank_gap_signal)`. This margin is non-negative and increases with the importance of the pair.\n5. (Inherited from Parent 0) Scale the `logp_diff` by a hyperparameter `beta`: `scaled_logp_diff = beta * logp_diff`.\n6. (New Coupling) Formulate the loss argument as the difference between the required margin and the achieved scaled log-probability difference: `argument = margin - scaled_logp_diff`.\n7. (New Coupling) Apply a `softplus` function to the argument. This acts like a smooth hinge loss, penalizing the model only when `scaled_logp_diff < margin`, i.e., when the argument is positive. `loss = softplus(argument)`.\n8. Return the weighted mean of the loss over the batch.", "hyperparams": {"alpha": 4.0, "beta": 0.5}, "operators_used": ["softplus", "rank_gap"], "implementation_hint": {"expects": ["cost_a", "cost_b", "logp_a", "logp_b"], "returns": "scalar"}, "code": "import torch\nimport torch.nn.functional as F\n\ndef generated_loss(batch, model_output, extra):\n    \"\"\"\n    Computes the Adaptive Rank-Gapped Margin Loss.\n\n    This loss requires the scaled log-probability difference to exceed a dynamic\n    margin derived from the rank of the cost difference. The loss is computed\n    using a smooth hinge-like function (softplus).\n\n    Args:\n        batch (dict): A dictionary containing tensors.\n            'cost_a' (Tensor [N]): Cost of solution 'a'.\n            'cost_b' (Tensor [N]): Cost of solution 'b'.\n            'log_prob_w' (Tensor [N]): Log probability of the winning solution.\n            'log_prob_l' (Tensor [N]): Log probability of the losing solution.\n            'weight' (Tensor [N], optional): Per-pair loss weights.\n        model_output (dict): Model outputs (not used).\n        extra (dict): A dictionary for hyperparameters.\n            'alpha' (float): Controls the magnitude of the rank-based margin.\n            'beta' (float): Scales the logp_diff, acting as an inverse temperature.\n\n    Returns:\n        torch.Tensor: A scalar tensor representing the mean loss.\n    \"\"\"\n\n    # Whitelisted operator implemented as a helper function\n    def rank_gap(x, eps=1e-9):\n        \"\"\"Converts a tensor to its normalized ranks within the range [-0.5, 0.5].\"\"\"\n        if x.numel() <= 1:\n            return torch.zeros_like(x)\n        ranks = x.to(torch.float32).argsort().argsort().to(x.dtype)\n        # Normalize ranks to [0, 1]\n        normalized_ranks = ranks / (x.numel() - 1 + eps)\n        # Shift to [-0.5, 0.5]\n        return normalized_ranks - 0.5\n\n    # Retrieve hyperparameters\n    alpha = extra.get('alpha', 4.0)\n    beta = extra.get('beta', 0.5)\n\n    # Unpack tensors from the batch\n    cost_a = batch['cost_a']\n    cost_b = batch['cost_b']\n    log_prob_w = batch['log_prob_w']\n    log_prob_l = batch['log_prob_l']\n    weights = batch.get('weight', None)\n\n    # 1. Calculate the difference in log probabilities\n    logp_diff = log_prob_w - log_prob_l\n\n    # 2. Calculate absolute cost difference\n    cost_diff = torch.abs(cost_a - cost_b)\n\n    # 3. (Inherited) Create a normalized, scale-invariant signal from cost_diff\n    rank_gap_signal = rank_gap(cost_diff)\n\n    # 4. (Inherited) Create a dynamic margin from the rank signal\n    margin = F.softplus(alpha * rank_gap_signal)\n\n    # 5. (Inherited) Scale logp_diff by beta\n    scaled_logp_diff = beta * logp_diff\n\n    # 6. (New Coupling) Formulate loss as a margin violation\n    # The objective is `scaled_logp_diff > margin`, so the loss is incurred\n    # on `margin - scaled_logp_diff` when this quantity is positive.\n    argument = margin - scaled_logp_diff\n\n    # 7. (New Coupling) Use softplus as a smooth hinge loss\n    # This penalizes violations where the margin is not met.\n    loss_per_item = F.softplus(argument)\n\n    # 8. Apply optional weights and compute the mean loss\n    if weights is not None:\n        # Ensure weights sum is non-zero to avoid division by zero\n        loss = (loss_per_item * weights).sum() / weights.sum().clamp(min=1e-8)\n    else:\n        loss = loss_per_item.mean()\n\n    return loss"}, "static_ok": true, "static_reason": "", "dynamic_ok": true, "dynamic_reason": "ok", "loss_value": 1.2550013065338135, "grad_norm": 0.0}
{"generation": 9, "index": 5, "ir": {"name": "Exponential-Scaled Tanh-Rank Loss", "intuition": "This loss function combines the stability of tanh-clipping with a dynamic, rank-based scaling mechanism that is further modulated by an exponential term. The goal is to create a loss that strongly enforces preference order for high-importance pairs while being sensitive to the magnitude of the log-probability difference itself.\n\nInherited Ideas:\n- From `Adaptive Rank-Modulated Tanh-Clipping Loss` (Parent 0): It inherits the idea of scaling the log-probability difference *before* applying the `tanh` function: `tanh(beta * logp_diff)`. This `beta` parameter controls the sharpness of the tanh curve, making the loss more or less sensitive to the magnitude of `logp_diff`.\n- From both parents (`Adaptive Rank-Modulated Tanh-Clipping Loss` and `Soft-Clipped Rank-Modulated Loss`): It inherits the use of a `rank_gap` signal on the cost difference, which is then passed through `softplus` to create a smooth, non-negative `rank_modulator`. This makes the loss's scaling robust to the absolute scale of costs and outliers.\n\nNew Coupling Ideas & Modifications:\n1. **Exponential Log-Probability Scaling**: A new exponential term, `exp(logp_diff / tau)`, is introduced. This term acts as a dynamic scaler. When the model is confident (`logp_diff` is large and positive), this term grows, amplifying the loss signal. When the model is unconfident or wrong (`logp_diff` is small or negative), this term shrinks towards zero, reducing the penalty. This encourages the model to not just get the sign right, but to also produce a confident `logp_diff`.\n2. **Coupled Exponential and Rank Modulation**: The new exponential scaler is multiplicatively coupled with the inherited `rank_modulator`. The final loss argument becomes `rank_modulator * exp(logp_diff / tau) * (tanh(beta * logp_diff) - 1)`. This creates a three-way interaction: the `rank_modulator` sets the baseline importance, the `tanh` term provides a stable, bounded target, and the `exp` term provides an additional incentive for the model to increase its confidence on correctly ordered pairs.", "pseudocode": "1. Calculate the difference in log probabilities: `logp_diff = logp_winner - logp_loser`.\n2. Calculate the absolute difference in costs: `cost_diff = abs(cost_winner - cost_loser)`.\n3. (Inherited from both parents) Create a scale-invariant rank signal from the cost difference using the `rank_gap` operator.\n4. (Inherited from both parents) Create a smooth, non-negative `rank_modulator` from the rank signal: `rank_modulator = softplus(alpha * rank_gap_signal)`.\n5. (Inherited from Parent 0) Scale the `logp_diff` by `beta` and apply `tanh` for a stable, clipped signal: `clipped_term = tanh(beta * logp_diff)`.\n6. (New Coupling) Create a new exponential scaling factor based on the raw `logp_diff`: `exp_scaler = exp(logp_diff / tau)`. The `tau` hyperparameter controls the sensitivity of this scaler.\n7. (New Coupling) Combine all components. The loss argument is formed by multiplying the `rank_modulator` and the new `exp_scaler` with the target-seeking term `(clipped_term - 1)`. The full argument is `rank_modulator * exp_scaler * (clipped_term - 1)`.\n8. Compute the final loss per item using a numerically stable logistic loss: `loss = -logsigmoid(argument)`.\n9. Return the weighted mean of the loss over the batch.", "hyperparams": {"alpha": 4.0, "beta": 0.5, "tau": 2.0}, "operators_used": ["logsigmoid", "softplus", "tanh", "rank_gap", "exp"], "implementation_hint": {"expects": ["cost_a", "cost_b", "log_prob_w", "log_prob_l", "weight"], "returns": "scalar"}, "code": "import torch\nimport torch.nn.functional as F\n\ndef generated_loss(batch, model_output, extra):\n    \"\"\"\n    Computes the Exponential-Scaled Tanh-Rank Loss.\n\n    This loss combines a rank-based modulator with an exponential scaler to modulate\n    a tanh-clipped log-probability difference. The goal is to create a strong signal\n    for high-importance pairs where the model is also confident.\n\n    Args:\n        batch (dict): A dictionary containing tensors.\n            'cost_a' (Tensor [N]): Cost of solution 'a'.\n            'cost_b' (Tensor [N]): Cost of solution 'b'.\n            'log_prob_w' (Tensor [N]): Log probability of the winning solution.\n            'log_prob_l' (Tensor [N]): Log probability of the losing solution.\n            'weight' (Tensor [N], optional): Per-pair loss weights.\n        model_output (dict): Model outputs (not used).\n        extra (dict): A dictionary for hyperparameters.\n            'alpha' (float): Controls the magnitude of the rank modulator.\n            'beta' (float): Scales the logp_diff before tanh, controlling steepness.\n            'tau' (float): Temperature for the exponential scaler.\n\n    Returns:\n        torch.Tensor: A scalar tensor representing the mean loss.\n    \"\"\"\n\n    # Whitelisted operator implemented as a helper function\n    def rank_gap(x, eps=1e-9):\n        \"\"\"Converts a tensor to its normalized ranks within the range [-0.5, 0.5].\"\"\"\n        if x.numel() <= 1:\n            return torch.zeros_like(x)\n        ranks = x.to(torch.float32).argsort().argsort().to(x.dtype)\n        # Normalize ranks to [0, 1]\n        normalized_ranks = ranks / (x.numel() - 1 + eps)\n        # Shift to [-0.5, 0.5]\n        return normalized_ranks - 0.5\n\n    # Retrieve hyperparameters\n    alpha = extra.get('alpha', 4.0)\n    beta = extra.get('beta', 0.5)\n    tau = extra.get('tau', 2.0)\n\n    # Unpack tensors from the batch\n    cost_a = batch['cost_a']\n    cost_b = batch['cost_b']\n    log_prob_w = batch['log_prob_w']\n    log_prob_l = batch['log_prob_l']\n    weights = batch.get('weight', None)\n\n    # 1. Calculate the difference in log probabilities\n    logp_diff = log_prob_w - log_prob_l\n\n    # 2. Calculate absolute cost difference\n    cost_diff = torch.abs(cost_a - cost_b)\n\n    # 3. (Inherited) Create a scale-invariant rank signal\n    rank_gap_signal = rank_gap(cost_diff)\n\n    # 4. (Inherited) Create a smooth, non-negative rank modulator\n    rank_modulator = F.softplus(alpha * rank_gap_signal)\n\n    # 5. (Inherited) Scale and clip the logp_diff\n    clipped_term = torch.tanh(beta * logp_diff)\n\n    # 6. (New Coupling) Create an exponential scaler sensitive to logp_diff magnitude\n    exp_scaler = torch.exp(logp_diff / tau)\n\n    # 7. (New Coupling) Combine the rank modulator, exp scaler, and clipped term\n    # The target is for tanh(beta * logp_diff) to approach 1.\n    # The loss is scaled by both the pair's importance (rank) and the model's confidence (exp).\n    argument = rank_modulator * exp_scaler * (clipped_term - 1)\n\n    # 8. Compute the final loss using numerically stable logsigmoid\n    loss_per_item = -F.logsigmoid(argument)\n\n    # 9. Apply optional weights and compute the mean loss\n    if weights is not None:\n        # Ensure weights sum is non-zero to avoid division by zero\n        loss = (loss_per_item * weights).sum() / weights.sum().clamp(min=1e-8)\n    else:\n        loss = loss_per_item.mean()\n\n    return loss"}, "static_ok": true, "static_reason": "", "dynamic_ok": true, "dynamic_reason": "ok", "loss_value": 1.2550013065338135, "grad_norm": 0.0}
{"generation": 9, "index": 6, "ir": {"name": "Rank-Gated Exponential Loss", "intuition": "This loss function dynamically adjusts the learning signal based on the relative importance of a preference pair, using a gating mechanism to prevent the model from over-optimizing on easy pairs. It encourages a large log-probability gap for important pairs while being lenient on ambiguous ones.\n\nInherited Ideas:\n- From `Adaptive Rank-Modulated Tanh-Clipping Loss` (Parent 0) and `Soft-Clipped Rank-Modulated Loss` (Parent 1): It inherits the core concept of using a **rank-based signal** (`rank_gap`) derived from the cost difference. This makes the loss robust to the scale and distribution of costs, focusing on the relative importance of pairs within a batch.\n- From `Adaptive Rank-Modulated Tanh-Clipping Loss` (Parent 0): It inherits the idea of applying a **scaling parameter (`beta`) to the log-probability difference** (`logp_diff`). This `beta` acts as an inverse temperature, controlling the sensitivity of the loss to the magnitude of `logp_diff`.\n\nNew Coupling Ideas & Modifications:\n1.  **Exponential Loss Core**: Instead of the `tanh`-based clipping and margin structure seen in the parents, this child uses an exponential loss core, `exp(-beta * logp_diff)`. This provides a strong, convex penalty that aggressively pushes `logp_diff` to be large and positive, similar to AdaBoost, but scaled by `beta`.\n2.  **Rank-Based Gating Mechanism**: The primary innovation is coupling the exponential loss with a rank-based gate. A `gate` is computed as `sigmoid(alpha * rank_gap(cost_diff))`. This gate smoothly transitions from approximately 0 for low-importance pairs (small cost difference) to 1 for high-importance pairs. The final loss is `gate * exp(-beta * logp_diff)`. This coupling ensures that the strong exponential penalty is only applied to pairs that are deemed important by their rank, effectively 'gating' or 'switching off' the loss for trivial pairs and preventing the model from wasting capacity on them.", "pseudocode": "1. Calculate the difference in log probabilities: `logp_diff = logp_winner - logp_loser`.\n2. Calculate the absolute difference in costs: `cost_diff = abs(cost_winner - cost_loser)`.\n3. (Inherited from Parents) Create a normalized, scale-invariant signal from the cost difference using the `rank_gap` operator, which maps cost differences to ranks in [-0.5, 0.5].\n4. (New Coupling) Create a smooth 'gate' based on the rank signal: `gate = sigmoid(alpha * rank_gap_signal)`. This gate value approaches 1 for high-rank pairs and 0 for low-rank pairs.\n5. (Inherited from Parent 0) Scale the log-probability difference by a hyperparameter `beta`: `scaled_logp_diff = beta * logp_diff`.\n6. (New Coupling) Compute the core exponential loss term: `exp_loss = exp(-scaled_logp_diff)`.\n7. (New Coupling) Apply the gate to the exponential loss. The loss for each pair is `gated_loss = gate * exp_loss`. This focuses learning on pairs with a significant cost difference.\n8. Return the weighted mean of the `gated_loss` over the batch.", "hyperparams": {"alpha": 5.0, "beta": 0.5}, "operators_used": ["sigmoid", "exp", "rank_gap"], "implementation_hint": {"expects": ["cost_a", "cost_b", "log_prob_w", "log_prob_l", "weight"], "returns": "scalar"}, "code": "import torch\nimport torch.nn.functional as F\n\ndef generated_loss(batch, model_output, extra):\n    \"\"\"\n    Computes the Rank-Gated Exponential Loss.\n\n    This loss applies an exponential penalty to the log-probability difference,\n    which is gated by a sigmoid function of the cost difference's rank.\n    This focuses the model on important pairs while ignoring trivial ones.\n\n    Args:\n        batch (dict): A dictionary containing tensors.\n            'cost_a' (Tensor [N]): Cost of solution 'a'.\n            'cost_b' (Tensor [N]): Cost of solution 'b'.\n            'log_prob_w' (Tensor [N]): Log probability of the winning solution.\n            'log_prob_l' (Tensor [N]): Log probability of the losing solution.\n            'weight' (Tensor [N], optional): Per-pair loss weights.\n        model_output (dict): Model outputs (not used).\n        extra (dict): A dictionary for hyperparameters.\n            'alpha' (float): Controls the steepness of the rank-based gate.\n            'beta' (float): Scales the logp_diff, acting as an inverse temperature.\n\n    Returns:\n        torch.Tensor: A scalar tensor representing the mean loss.\n    \"\"\"\n\n    # Whitelisted operator implemented as a helper function\n    def rank_gap(x, eps=1e-9):\n        \"\"\"Converts a tensor to its normalized ranks within the range [-0.5, 0.5].\"\"\"\n        if x.numel() <= 1:\n            return torch.zeros_like(x)\n        ranks = x.to(torch.float32).argsort().argsort().to(x.dtype)\n        # Normalize ranks to [0, 1]\n        normalized_ranks = ranks / (x.numel() - 1 + eps)\n        # Shift to [-0.5, 0.5]\n        return normalized_ranks - 0.5\n\n    # Retrieve hyperparameters\n    alpha = extra.get('alpha', 5.0)\n    beta = extra.get('beta', 0.5)\n\n    # Unpack tensors from the batch\n    cost_a = batch['cost_a']\n    cost_b = batch['cost_b']\n    log_prob_w = batch['log_prob_w']\n    log_prob_l = batch['log_prob_l']\n    weights = batch.get('weight', None)\n\n    # 1. Calculate the difference in log probabilities\n    logp_diff = log_prob_w - log_prob_l\n\n    # 2. Calculate absolute cost difference\n    cost_diff = torch.abs(cost_a - cost_b)\n\n    # 3. (Inherited) Create a normalized rank signal from cost_diff\n    rank_gap_signal = rank_gap(cost_diff)\n\n    # 4. (New Coupling) Create a smooth gate from the rank signal\n    gate = torch.sigmoid(alpha * rank_gap_signal)\n\n    # 5. (Inherited) Scale the log-probability difference\n    scaled_logp_diff = beta * logp_diff\n\n    # 6. (New Coupling) Compute the core exponential loss term\n    # Clamping the argument to exp prevents potential overflow if logp_diff is very negative\n    exp_loss = torch.exp(-scaled_logp_diff.clamp(max=80.0))\n\n    # 7. (New Coupling) Apply the gate to the exponential loss\n    loss_per_item = gate * exp_loss\n\n    # 8. Apply optional weights and compute the mean loss\n    if weights is not None:\n        # Ensure weights sum is non-zero to avoid division by zero\n        loss = (loss_per_item * weights).sum() / weights.sum().clamp(min=1e-8)\n    else:\n        loss = loss_per_item.mean()\n\n    return loss"}, "static_ok": true, "static_reason": "", "dynamic_ok": true, "dynamic_reason": "ok", "loss_value": 0.5, "grad_norm": 0.0}
{"generation": 9, "index": 7, "ir": {"name": "Adaptive Rank-Gated Loss", "intuition": "This loss function creates an adaptive learning signal by dynamically gating the gradient based on the relative importance of a preference pair. It ensures the model focuses on learning from pairs with a significant cost difference, while effectively ignoring pairs with negligible or ambiguous cost differences, preventing noise from corrupting the learning process.\n\nInherited Ideas:\n- From `Adaptive Rank-Modulated Tanh-Clipping Loss` (Parent 0): It inherits the idea of scaling the log-probability difference by a hyperparameter `beta` (`beta * logp_diff`). This acts as an inverse temperature, controlling the sensitivity of the loss to the magnitude of the log-probability difference.\n- From `Soft-Clipped Rank-Modulated Loss` (Parent 1): It inherits the use of `rank_gap` on the cost difference to create a scale-invariant signal of a pair's importance relative to others in the batch. This signal is then transformed via `softplus`.\n\nNew Coupling Ideas & Modifications:\n1.  **Rank-Based Gating Mechanism**: This is the core innovation. Instead of using the rank-based signal as a multiplicative scaler for the entire loss term (e.g., `scaler * tanh(...)`), this child uses it to create a 'gate'. The gate is computed as `sigmoid(alpha * rank_gap_signal - gate_bias)`. This sigmoid function acts as a soft switch: for low-rank pairs (small cost difference), the gate's value is close to 0, effectively zeroing out the loss and gradient for that pair. For high-rank pairs, the gate's value approaches 1, allowing the full loss to be computed. The `gate_bias` hyperparameter controls the threshold at which the gate 'opens'.\n2.  **Direct Log-Probability Difference with Gating**: Instead of clipping the log-probability difference with `tanh`, this loss applies the rank-based gate directly to the scaled log-probability difference. The final loss argument is `gate * (beta * logp_diff)`. This coupling means that the model is only penalized for `logp_diff` on pairs that are deemed important enough by the gating mechanism. This avoids forcing the model to push `logp_diff` towards an arbitrary boundary for all pairs, focusing its capacity on pairs that provide a clear signal.", "pseudocode": "1. Calculate the difference in log probabilities: `logp_diff = logp_winner - logp_loser`.\n2. Calculate the absolute difference in costs: `cost_diff = abs(cost_winner - cost_loser)`.\n3. (Inherited from Parent 1) Create a normalized, scale-invariant signal from the cost difference using the `rank_gap` operator. This signal ranges from -0.5 to 0.5.\n4. (New Coupling) Create a 'gate' value for each pair. This is done by scaling the rank signal, shifting it by a bias, and passing it through a `sigmoid` function: `gate = sigmoid(alpha * rank_gap_signal - gate_bias)`. This gate will be close to 0 for low-rank pairs and close to 1 for high-rank pairs.\n5. (Inherited from Parent 0) Scale the `logp_diff` by a hyperparameter `beta`: `scaled_logp_diff = beta * logp_diff`.\n6. (New Coupling) Apply the gate to the scaled log-probability difference. The argument for the loss function is `gate * scaled_logp_diff`. When the gate is near 0, this term is nullified, stopping gradients for unimportant pairs.\n7. Compute the final loss using a numerically stable logistic loss function: `loss = -logsigmoid(gate * scaled_logp_diff)`.\n8. Return the weighted mean of the loss over the batch.", "hyperparams": {"alpha": 8.0, "beta": 0.5, "gate_bias": 1.0}, "operators_used": ["logsigmoid", "sigmoid", "rank_gap"], "implementation_hint": {"expects": ["cost_a", "cost_b", "log_prob_w", "log_prob_l", "weight"], "returns": "scalar"}, "code": "import torch\nimport torch.nn.functional as F\n\ndef generated_loss(batch, model_output, extra):\n    \"\"\"\n    Computes the Adaptive Rank-Gated Loss.\n\n    This loss uses a rank-based gating mechanism to selectively apply the loss\n    only to preference pairs with a sufficiently large cost difference. This\n    prevents the model from being penalized for ambiguous or noisy pairs.\n\n    Args:\n        batch (dict): A dictionary containing tensors.\n            'cost_a' (Tensor [N]): Cost of solution 'a'.\n            'cost_b' (Tensor [N]): Cost of solution 'b'.\n            'log_prob_w' (Tensor [N]): Log probability of the winning solution.\n            'log_prob_l' (Tensor [N]): Log probability of the losing solution.\n            'weight' (Tensor [N], optional): Per-pair loss weights.\n        model_output (dict): Model outputs (not used).\n        extra (dict): A dictionary for hyperparameters.\n            'alpha' (float): Controls the steepness of the sigmoid gate.\n            'beta' (float): Scales the logp_diff, acting as an inverse temperature.\n            'gate_bias' (float): Shifts the activation point of the gate.\n\n    Returns:\n        torch.Tensor: A scalar tensor representing the mean loss.\n    \"\"\"\n\n    # Whitelisted operator implemented as a helper function\n    def rank_gap(x, eps=1e-9):\n        \"\"\"Converts a tensor to its normalized ranks within the range [-0.5, 0.5].\"\"\"\n        if x.numel() <= 1:\n            return torch.zeros_like(x)\n        ranks = x.to(torch.float32).argsort().argsort().to(x.dtype)\n        # Normalize ranks to [0, 1]\n        normalized_ranks = ranks / (x.numel() - 1 + eps)\n        # Shift to [-0.5, 0.5]\n        return normalized_ranks - 0.5\n\n    # Retrieve hyperparameters\n    alpha = extra.get('alpha', 8.0)\n    beta = extra.get('beta', 0.5)\n    gate_bias = extra.get('gate_bias', 1.0)\n\n    # Unpack tensors from the batch\n    cost_a = batch['cost_a']\n    cost_b = batch['cost_b']\n    log_prob_w = batch['log_prob_w']\n    log_prob_l = batch['log_prob_l']\n    weights = batch.get('weight', None)\n\n    # 1. Calculate the difference in log probabilities\n    logp_diff = log_prob_w - log_prob_l\n\n    # 2. Calculate absolute cost difference\n    cost_diff = torch.abs(cost_a - cost_b)\n\n    # 3. (Inherited from Parent 1) Create a scale-invariant rank signal\n    rank_gap_signal = rank_gap(cost_diff)\n\n    # 4. (New Coupling) Create a soft gate based on the rank signal\n    # The gate is close to 0 for low-rank pairs and 1 for high-rank pairs.\n    gate = torch.sigmoid(alpha * rank_gap_signal - gate_bias)\n\n    # 5. (Inherited from Parent 0) Scale the logp_diff\n    scaled_logp_diff = beta * logp_diff\n\n    # 6. (New Coupling) Apply the gate to the scaled log-probability difference\n    argument = gate * scaled_logp_diff\n\n    # 7. Compute the final loss using numerically stable logsigmoid\n    loss_per_item = -F.logsigmoid(argument)\n\n    # 8. Apply optional weights and compute the mean loss\n    if weights is not None:\n        # Ensure weights sum is non-zero to avoid division by zero\n        loss = (loss_per_item * weights).sum() / weights.sum().clamp(min=1e-8)\n    else:\n        loss = loss_per_item.mean()\n\n    return loss"}, "static_ok": true, "static_reason": "", "dynamic_ok": true, "dynamic_reason": "ok", "loss_value": 0.6931471824645996, "grad_norm": 0.0}
