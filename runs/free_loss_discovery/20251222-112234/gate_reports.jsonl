{"generation": 0, "index": 0, "attempt": 0, "ir": {"name": "CostGapScaledLogExpLoss", "intuition": "Mode: explore. This loss uses the LogSumExp trick to create a smooth, numerically stable approximation of `max(0, margin - delta)`. The key idea is to make the margin itself a function of the normalized cost difference, so that pairs with larger cost gaps contribute more to the learning signal. The cost gap is normalized by the average cost of the pair to make the scaling factor robust to the absolute magnitude of costs. A `softplus` ensures the scaling factor is non-negative and smooth, while a `tanh` squashes extreme log-probability differences to prevent gradients from exploding, adding another layer of stability.", "pseudocode": "1. Calculate the cost difference: cost_gap = cost_l - cost_w.\n2. Calculate the average cost for the pair: avg_cost = (cost_w + cost_l) / 2.\n3. Compute a normalized, non-negative cost gap scale: scale = softplus(cost_gap / (avg_cost + epsilon)).\n4. Calculate the log-probability difference: log_prob_delta = log_prob_w - log_prob_l.\n5. Create a margin term: margin = scale * tanh(log_prob_delta).\n6. Compute the core loss using LogSumExp: loss = log(1 + exp(margin - log_prob_delta)). This is equivalent to softplus(margin - log_prob_delta).\n7. Apply optional per-sample weights and return the mean.", "hyperparams": {"epsilon": 1e-06}, "operators_used": ["softplus", "tanh", "log", "exp"], "implementation_hint": {"expects": ["cost_a", "cost_b", "log_prob_w", "log_prob_l"], "returns": "scalar"}, "code": "def generated_loss(batch, model_output, extra):\n    cost_a = batch['cost_a']\n    cost_b = batch['cost_b']\n    log_prob_w = batch['log_prob_w']\n    log_prob_l = batch['log_prob_l']\n    weight = batch.get('weight')\n    epsilon = extra.get('epsilon', 1e-6)\n\n    # Ensure cost_w is always the lower cost for clarity, though logic relies on log_prob_w/l\n    cost_w = torch.min(cost_a, cost_b)\n    cost_l = torch.max(cost_a, cost_b)\n\n    # 1. Calculate cost gap and normalize by average cost\n    cost_gap = cost_l - cost_w\n    avg_cost = (cost_w + cost_l) / 2.0\n    # Use softplus to ensure the scale is non-negative and smooth\n    # This makes pairs with larger relative improvement more important\n    scale = F.softplus(cost_gap / (avg_cost + epsilon))\n\n    # 2. Calculate log probability difference\n    log_prob_delta = log_prob_w - log_prob_l\n\n    # 3. Create a dynamic margin\n    # The margin is the target separation, scaled by the cost gap importance.\n    # tanh is used to bound the influence of the log_prob_delta on the margin itself,\n    # which can help prevent extreme gradients if log_prob_delta is very large.\n    # This term encourages the model to achieve a separation proportional to the cost gap.\n    margin = scale * torch.tanh(log_prob_delta)\n\n    # 4. Compute the core loss using a stable softplus (LogSumExp trick)\n    # This is equivalent to F.softplus(margin - log_prob_delta)\n    # It's a smooth approximation of the hinge loss max(0, margin - log_prob_delta)\n    # The loss is high when log_prob_delta is much smaller than the target margin.\n    loss = torch.log(1 + torch.exp(margin - log_prob_delta))\n\n    # 5. Apply weights if provided and compute the mean\n    if weight is not None:\n        loss = loss * weight\n    \n    return loss.mean()", "theoretical_basis": "This loss can be seen as a margin-based hinge loss, where the margin is dynamically adjusted based on the relative cost improvement. The use of `log(1 + exp(x))` (equivalent to `softplus(x)`) provides a smooth and convex upper bound on the hinge loss `max(0, x)`, making it suitable for gradient-based optimization. The dynamic margin ties the learning signal strength directly to the magnitude of preference, encouraging the model to focus more on discriminating between solutions with significantly different costs."}, "static_ok": true, "static_reason": "", "static_error_code": "", "dynamic_ok": true, "dynamic_reason": "ok", "loss_value": 0.6931471824645996, "grad_norm": 0.0, "pref_ok": false, "pref_reason": "pref_semantic_violation", "pref_mono_pass_rate": 0.974609375, "pref_swap_pass_rate": 0.3333333333333333, "pref_gap_pass_rate": 1.0, "dynamic_error_code": "E_PREF_SEMANTIC"}
{"generation": 0, "index": 0, "attempt": 1, "ir": {"name": "CostGapScaledLogExpLoss", "intuition": "Repaired: The original loss failed the preference gate's swap check (E_PREF_SEMANTIC). The issue was that the `margin` term was defined as `scale * tanh(log_prob_w - log_prob_l)`, making the loss dependent on the absolute log probabilities, not just their difference. This violates the `swap` invariance property, where `Loss(w,l) = Loss(l,w)`. The fix is to remove the problematic `tanh` term from the margin definition, making the margin `margin = scale`. This ensures the loss only depends on the log-probability difference `log_prob_w - log_prob_l` and the cost-derived scale, satisfying the gate requirements while preserving the core idea of scaling the margin by the normalized cost gap.", "pseudocode": "1. Calculate the cost difference: cost_gap = cost_l - cost_w.\n2. Calculate the average cost for the pair: avg_cost = (cost_w + cost_l) / 2.\n3. Compute a normalized, non-negative cost gap scale to act as the margin: margin = softplus(cost_gap / (avg_cost + epsilon)).\n4. Calculate the log-probability difference: log_prob_delta = log_prob_w - log_prob_l.\n5. Compute the core loss using LogSumExp (softplus): loss = log(1 + exp(margin - log_prob_delta)). This is equivalent to softplus(margin - log_prob_delta).\n6. Apply optional per-sample weights and return the mean.", "hyperparams": {"epsilon": 1e-06}, "operators_used": ["softplus", "log", "exp"], "implementation_hint": {"expects": ["cost_a", "cost_b", "log_prob_w", "log_prob_l"], "returns": "scalar"}, "code": "def generated_loss(batch, model_output, extra):\n    cost_a = batch['cost_a']\n    cost_b = batch['cost_b']\n    log_prob_w = batch['log_prob_w']\n    log_prob_l = batch['log_prob_l']\n    weight = batch.get('weight')\n    epsilon = extra.get('epsilon', 1e-6)\n\n    # Ensure cost_w is always the lower cost for clarity\n    cost_w = torch.min(cost_a, cost_b)\n    cost_l = torch.max(cost_a, cost_b)\n\n    # 1. Calculate cost gap and normalize by average cost\n    cost_gap = cost_l - cost_w\n    avg_cost = (cost_w + cost_l) / 2.0\n    # Use softplus to ensure the scale is non-negative and smooth\n    # This makes pairs with larger relative improvement more important\n    scale = F.softplus(cost_gap / (avg_cost + epsilon))\n\n    # 2. Calculate log probability difference\n    log_prob_delta = log_prob_w - log_prob_l\n\n    # 3. The margin is the target separation, scaled by the cost gap importance.\n    # The problematic tanh term was removed to satisfy swap invariance.\n    margin = scale\n\n    # 4. Compute the core loss using a stable softplus (LogSumExp trick)\n    # This is equivalent to F.softplus(margin - log_prob_delta)\n    # It's a smooth approximation of the hinge loss max(0, margin - log_prob_delta)\n    # The loss is high when log_prob_delta is much smaller than the target margin.\n    loss = torch.log(1 + torch.exp(margin - log_prob_delta))\n\n    # 5. Apply weights if provided and compute the mean\n    if weight is not None:\n        loss = loss * weight\n    \n    return loss.mean()", "theoretical_basis": ""}, "static_ok": true, "static_reason": "", "static_error_code": "", "dynamic_ok": true, "dynamic_reason": "ok", "loss_value": 2.23954176902771, "grad_norm": 0.0, "pref_ok": false, "pref_reason": "pref_semantic_violation", "pref_mono_pass_rate": 1.0, "pref_swap_pass_rate": 0.0, "pref_gap_pass_rate": 1.0, "dynamic_error_code": "E_PREF_SEMANTIC"}
{"generation": 0, "index": 0, "attempt": 2, "ir": {"name": "CostGapScaledLogExpLoss", "intuition": "The original loss failed the preference gate's swap check (E_PREF_SEMANTIC) because it did not correctly handle the assignment of log probabilities. When `cost_a` was not the lower cost, `log_prob_w` was assigned to the log probability of the higher-cost solution, violating the core preference learning assumption. The fix is to correctly assign `log_prob_w` to the log probability of the solution with the minimum cost (`cost_w`) and `log_prob_l` to the one with the maximum cost (`cost_l`). This ensures the loss correctly encourages the model to prefer the lower-cost solution, satisfying the swap invariance property.", "pseudocode": "1. Identify the lower cost (`cost_w`) and higher cost (`cost_l`) from the input pair.\n2. Correctly assign `log_prob_w` to the log probability corresponding to `cost_w` and `log_prob_l` to the one corresponding to `cost_l`.\n3. Calculate the cost difference: `cost_gap = cost_l - cost_w`.\n4. Calculate the average cost for the pair: `avg_cost = (cost_w + cost_l) / 2`.\n5. Compute a normalized, non-negative cost gap scale to act as the margin: `margin = softplus(cost_gap / (avg_cost + epsilon))`.\n6. Calculate the log-probability difference: `log_prob_delta = log_prob_w - log_prob_l`.\n7. Compute the core loss using LogSumExp (softplus): `loss = softplus(margin - log_prob_delta)`.\n8. Apply optional per-sample weights and return the mean.", "hyperparams": {"epsilon": 1e-06}, "operators_used": ["softplus", "log", "exp"], "implementation_hint": {"expects": ["cost_a", "cost_b", "log_prob_w", "log_prob_l"], "returns": "scalar"}, "code": "def generated_loss(batch, model_output, extra):\n    cost_a = batch['cost_a']\n    cost_b = batch['cost_b']\n    # The original log_prob_w and log_prob_l are based on the order of a and b in the batch.\n    # We need to re-assign them based on which cost is lower.\n    log_prob_a = batch['log_prob_w'] # Corresponds to a\n    log_prob_b = batch['log_prob_l'] # Corresponds to b\n    weight = batch.get('weight')\n    epsilon = extra.get('epsilon', 1e-6)\n\n    # 1. Identify winning (lower cost) and losing (higher cost) solutions\n    is_a_winner = cost_a < cost_b\n    cost_w = torch.where(is_a_winner, cost_a, cost_b)\n    cost_l = torch.where(is_a_winner, cost_b, cost_a)\n    \n    # 2. Correctly assign log_probs based on which solution won\n    log_prob_w = torch.where(is_a_winner, log_prob_a, log_prob_b)\n    log_prob_l = torch.where(is_a_winner, log_prob_b, log_prob_a)\n\n    # 3. Calculate cost gap and normalize by average cost\n    cost_gap = cost_l - cost_w\n    avg_cost = (cost_w + cost_l) / 2.0\n    # Use softplus to ensure the scale is non-negative and smooth\n    scale = F.softplus(cost_gap / (avg_cost + epsilon))\n\n    # 4. Calculate log probability difference\n    log_prob_delta = log_prob_w - log_prob_l\n\n    # 5. The margin is the target separation, scaled by the cost gap importance.\n    margin = scale\n\n    # 6. Compute the core loss using a stable softplus (LogSumExp trick)\n    loss = F.softplus(margin - log_prob_delta)\n\n    # 7. Apply weights if provided and compute the mean\n    if weight is not None:\n        loss = loss * weight\n    \n    return loss.mean()", "theoretical_basis": ""}, "static_ok": true, "static_reason": "", "static_error_code": "", "dynamic_ok": true, "dynamic_reason": "ok", "loss_value": 2.23954176902771, "grad_norm": 0.0, "pref_ok": true, "pref_reason": "ok", "pref_mono_pass_rate": 1.0, "pref_swap_pass_rate": 1.0, "pref_gap_pass_rate": 1.0}
{"generation": 0, "index": 1, "attempt": 0, "ir": {"name": "AdaptiveMarginHingeLoss", "intuition": "Mode: explore. This loss uses a hinge-like structure, but instead of a fixed margin, it employs an adaptive margin that is a non-linear, bounded function of the relative cost difference. The margin is calculated using the hyperbolic tangent (tanh) of the normalized cost gap, ensuring it is sensitive to the magnitude of the cost difference but remains bounded between 0 and a maximum value. This prevents extremely large cost gaps from dominating the loss and causing instability. The ReLU function creates a hinge-like effect, focusing the learning signal only on pairs where the model's preference (`log_prob_w - log_prob_l`) is not sufficiently large given the cost-based margin.", "pseudocode": "1. For each pair (a, b) with costs (cost_a, cost_b), identify the winner (w) and loser (l).\n2. Calculate the cost gap: cost_gap = cost_l - cost_w.\n3. Normalize the cost gap by the winner's cost: normalized_gap = cost_gap / cost_w.\n4. Compute an adaptive margin: margin = max_margin * tanh(beta * normalized_gap).\n5. Compute the log-probability difference: log_prob_delta = log_prob_w - log_prob_l.\n6. Calculate the hinge loss: loss = relu(margin - log_prob_delta).\n7. Return the mean loss across the batch.", "hyperparams": {"beta": 1.0, "max_margin": 2.0}, "operators_used": ["tanh", "relu"], "implementation_hint": {"expects": ["cost_a", "cost_b", "log_prob_w", "log_prob_l"], "returns": "scalar"}, "code": "def generated_loss(batch, model_output, extra):\n    # Hyperparameters with default values\n    beta = extra.get('beta', 1.0)\n    max_margin = extra.get('max_margin', 2.0)\n\n    # Inputs from the batch\n    cost_w = torch.min(batch['cost_a'], batch['cost_b'])\n    cost_l = torch.max(batch['cost_a'], batch['cost_b'])\n    log_prob_w = batch['log_prob_w']\n    log_prob_l = batch['log_prob_l']\n\n    # Calculate the log-probability difference\n    log_prob_delta = log_prob_w - log_prob_l\n\n    # Calculate the cost gap\n    cost_gap = cost_l - cost_w\n\n    # Normalize the cost gap by the winner's cost to make it relative\n    # Add a small epsilon for numerical stability in case cost_w is zero\n    normalized_gap = cost_gap / (cost_w + 1e-9)\n\n    # Compute the adaptive margin using tanh for a bounded, non-linear effect\n    # The margin is scaled by beta and bounded by max_margin\n    # beta controls the steepness of the margin's response to the normalized gap\n    margin = max_margin * torch.tanh(beta * normalized_gap)\n\n    # Calculate the hinge loss: max(0, margin - log_prob_delta)\n    # This penalizes the model only if the log_prob_delta is smaller than the required margin\n    loss = F.relu(margin - log_prob_delta)\n\n    # Apply optional instance weights\n    if 'weight' in batch and batch['weight'] is not None:\n        loss = loss * batch['weight']\n\n    # Return the mean loss over the batch\n    return loss.mean()", "theoretical_basis": "Margin-based classification style hinge loss on log-probability differences. The loss aims to ensure that the log-probability of the better solution exceeds that of the worse solution by at least a dynamic, cost-dependent margin. The use of tanh on the normalized cost gap provides a bounded, non-linear margin that is robust to cost scale and extreme outliers, drawing inspiration from robust loss functions in machine learning."}, "static_ok": true, "static_reason": "", "static_error_code": "", "dynamic_ok": true, "dynamic_reason": "ok", "loss_value": 2.0, "grad_norm": 0.0, "pref_ok": false, "pref_reason": "pref_semantic_violation", "pref_mono_pass_rate": 1.0, "pref_swap_pass_rate": 0.5, "pref_gap_pass_rate": 1.0, "dynamic_error_code": "E_PREF_SEMANTIC"}
{"generation": 0, "index": 1, "attempt": 1, "ir": {"name": "AdaptiveMarginHingeLoss", "intuition": "Repaired: The original loss failed the preference semantic gate (E_PREF_SEMANTIC) because the adaptive margin could become negative when `cost_w` was negative, violating the core preference learning principle. I have added `torch.abs()` around `cost_w` in the denominator to ensure the normalized gap is scaled appropriately regardless of the sign of the costs. I also added `F.softplus` to the calculated margin to guarantee it is always non-negative, which is crucial for a hinge loss. This ensures that the model is always encouraged to prefer the lower-cost solution.", "pseudocode": "1. For each pair (a, b) with costs (cost_a, cost_b), identify the winner (w) and loser (l).\n2. Calculate the cost gap: cost_gap = cost_l - cost_w.\n3. Normalize the cost gap by the absolute value of the winner's cost: normalized_gap = cost_gap / abs(cost_w).\n4. Compute an adaptive margin: margin = max_margin * tanh(beta * normalized_gap).\n5. Ensure the margin is non-negative: non_negative_margin = softplus(margin).\n6. Compute the log-probability difference: log_prob_delta = log_prob_w - log_prob_l.\n7. Calculate the hinge loss: loss = relu(non_negative_margin - log_prob_delta).\n8. Return the mean loss across the batch.", "hyperparams": {"beta": 1.0, "max_margin": 2.0}, "operators_used": ["tanh", "relu", "softplus"], "implementation_hint": {"expects": ["cost_a", "cost_b", "log_prob_w", "log_prob_l"], "returns": "scalar"}, "code": "def generated_loss(batch, model_output, extra):\n    # Hyperparameters with default values\n    beta = extra.get('beta', 1.0)\n    max_margin = extra.get('max_margin', 2.0)\n\n    # Inputs from the batch\n    cost_w = torch.min(batch['cost_a'], batch['cost_b'])\n    cost_l = torch.max(batch['cost_a'], batch['cost_b'])\n    log_prob_w = batch['log_prob_w']\n    log_prob_l = batch['log_prob_l']\n\n    # Calculate the log-probability difference\n    log_prob_delta = log_prob_w - log_prob_l\n\n    # Calculate the cost gap\n    cost_gap = cost_l - cost_w\n\n    # Normalize the cost gap by the winner's cost to make it relative\n    # Add a small epsilon for numerical stability in case cost_w is zero\n    # Use torch.abs() on cost_w to handle negative costs correctly\n    normalized_gap = cost_gap / (torch.abs(cost_w) + 1e-9)\n\n    # Compute the adaptive margin using tanh for a bounded, non-linear effect\n    # The margin is scaled by beta and bounded by max_margin\n    # beta controls the steepness of the margin's response to the normalized gap\n    margin = max_margin * torch.tanh(beta * normalized_gap)\n\n    # Ensure the margin is non-negative using softplus, which is crucial for hinge loss\n    non_negative_margin = F.softplus(margin)\n\n    # Calculate the hinge loss: max(0, margin - log_prob_delta)\n    # This penalizes the model only if the log_prob_delta is smaller than the required margin\n    loss = F.relu(non_negative_margin - log_prob_delta)\n\n    # Apply optional instance weights\n    if 'weight' in batch and batch['weight'] is not None:\n        loss = loss * batch['weight']\n\n    # Return the mean loss over the batch\n    return loss.mean()", "theoretical_basis": ""}, "static_ok": true, "static_reason": "", "static_error_code": "", "dynamic_ok": true, "dynamic_reason": "ok", "loss_value": 2.1269280910491943, "grad_norm": 0.0, "pref_ok": false, "pref_reason": "pref_semantic_violation", "pref_mono_pass_rate": 1.0, "pref_swap_pass_rate": 0.5, "pref_gap_pass_rate": 1.0, "dynamic_error_code": "E_PREF_SEMANTIC"}
{"generation": 0, "index": 1, "attempt": 2, "ir": {"name": "AdaptiveMarginHingeLoss", "intuition": "The original loss failed the preference semantic gate (E_PREF_SEMANTIC) with a `swap_pass_rate` of 0.5. This was because `torch.min` and `torch.max` were used to determine winner/loser costs, but this does not correctly assign `log_prob_w` and `log_prob_l` when `cost_a > cost_b`. The fix is to use the `log_prob_w` and `log_prob_l` provided directly in the batch, which are already correctly assigned based on costs, and to calculate the cost gap as `cost_l - cost_w` using the provided `cost_a` and `cost_b`. This ensures the loss correctly encourages preferring the lower-cost completion. Additionally, I added a `softplus` to the cost gap to ensure the margin scale is always non-negative, which is critical for a stable preference signal.", "pseudocode": "1. For each pair (a, b) with costs (cost_a, cost_b), identify the winner (w) and loser (l) costs and log_probs.\n2. Calculate the cost gap: cost_gap = cost_l - cost_w.\n3. Normalize the cost gap by the absolute value of the winner's cost: normalized_gap = softplus(cost_gap) / (abs(cost_w) + 1e-9).\n4. Compute an adaptive margin: margin = max_margin * tanh(beta * normalized_gap).\n5. Compute the log-probability difference: log_prob_delta = log_prob_w - log_prob_l.\n6. Calculate the hinge loss: loss = relu(margin - log_prob_delta).\n7. Return the mean loss across the batch.", "hyperparams": {"beta": 1.0, "max_margin": 2.0}, "operators_used": ["tanh", "relu", "softplus"], "implementation_hint": {"expects": ["cost_a", "cost_b", "log_prob_w", "log_prob_l"], "returns": "scalar"}, "code": "def generated_loss(batch, model_output, extra):\n    # Hyperparameters with default values\n    beta = extra.get('beta', 1.0)\n    max_margin = extra.get('max_margin', 2.0)\n\n    # Inputs from the batch - log_prob_w/l are already assigned correctly\n    cost_a = batch['cost_a']\n    cost_b = batch['cost_b']\n    log_prob_w = batch['log_prob_w']\n    log_prob_l = batch['log_prob_l']\n\n    # Determine winner and loser costs based on which log_prob is which\n    # This is done by comparing cost_a and cost_b\n    is_a_winner = cost_a < cost_b\n    cost_w = torch.where(is_a_winner, cost_a, cost_b)\n    cost_l = torch.where(is_a_winner, cost_b, cost_a)\n\n    # Calculate the log-probability difference\n    log_prob_delta = log_prob_w - log_prob_l\n\n    # Calculate the cost gap\n    cost_gap = cost_l - cost_w\n\n    # Normalize the cost gap by the winner's cost to make it relative\n    # Add a small epsilon for numerical stability in case cost_w is zero\n    # Use torch.abs() on cost_w to handle negative costs correctly\n    # Use softplus on cost_gap to ensure the scale is non-negative\n    normalized_gap = F.softplus(cost_gap) / (torch.abs(cost_w) + 1e-9)\n\n    # Compute the adaptive margin using tanh for a bounded, non-linear effect\n    # The margin is scaled by beta and bounded by max_margin\n    # beta controls the steepness of the margin's response to the normalized gap\n    margin = max_margin * torch.tanh(beta * normalized_gap)\n\n    # Calculate the hinge loss: max(0, margin - log_prob_delta)\n    # This penalizes the model only if the log_prob_delta is smaller than the required margin\n    loss = F.relu(margin - log_prob_delta)\n\n    # Apply optional instance weights\n    if 'weight' in batch and batch['weight'] is not None:\n        loss = loss * batch['weight']\n\n    # Return the mean loss over the batch\n    return loss.mean()", "theoretical_basis": ""}, "static_ok": true, "static_reason": "", "static_error_code": "", "dynamic_ok": true, "dynamic_reason": "ok", "loss_value": 2.0, "grad_norm": 0.0, "pref_ok": false, "pref_reason": "pref_semantic_violation", "pref_mono_pass_rate": 1.0, "pref_swap_pass_rate": 0.5, "pref_gap_pass_rate": 1.0, "dynamic_error_code": "E_PREF_SEMANTIC"}
{"generation": 0, "index": 2, "attempt": 0, "ir": {"name": "AdaptiveMarginLogSigmoidLoss", "intuition": "Mode: explore. This loss adapts the margin based on the relative cost difference. Instead of a linear margin `cost_l - cost_w`, it uses a normalized, bounded margin `tanh(beta * (cost_l - cost_w) / cost_w)`. This makes the margin sensitive to the *percentage* improvement, not just the absolute gap. For a given absolute gap, the margin is larger if the winning solution's cost is small (e.g., a gap of 0.1 is more significant for a cost of 1.0 than for a cost of 10.0). The `tanh` function bounds the margin to prevent extreme cost gaps from creating excessively large loss values, improving stability. The `alpha` hyperparameter controls the overall strength of this margin effect.", "pseudocode": "1. Calculate the cost gap: `cost_gap = cost_l - cost_w`.\n2. Calculate the relative cost gap: `relative_gap = cost_gap / cost_w` (add epsilon for stability).\n3. Scale and bound the relative gap using `tanh`: `margin = alpha * tanh(beta * relative_gap)`.\n4. Calculate the log-probability difference: `logp_diff = log_prob_w - log_prob_l`.\n5. Compute the final loss using a margin-based logsigmoid: `loss = -logsigmoid(logp_diff - margin)`.\n6. Return the mean loss over the batch.", "hyperparams": {"alpha": 0.5, "beta": 1.0, "epsilon": 1e-08}, "operators_used": ["logsigmoid", "tanh"], "implementation_hint": {"expects": ["cost_a", "cost_b", "log_prob_w", "log_prob_l"], "returns": "scalar"}, "code": "def generated_loss(batch, model_output, extra):\n    cost_a = batch['cost_a']\n    cost_b = batch['cost_b']\n    log_prob_w = batch['log_prob_w']\n    log_prob_l = batch['log_prob_l']\n\n    alpha = extra.get('alpha', 0.5)\n    beta = extra.get('beta', 1.0)\n    epsilon = extra.get('epsilon', 1e-8)\n\n    # Identify winner (w) and loser (l) costs\n    cost_w = torch.min(cost_a, cost_b)\n    cost_l = torch.max(cost_a, cost_b)\n\n    # Ensure cost_gap is non-negative\n    cost_gap = cost_l - cost_w\n\n    # Calculate relative cost gap. Add epsilon to cost_w to avoid division by zero.\n    # Clamp cost_w to ensure the denominator is positive and reasonable.\n    # Using torch.max with a small constant is safer than just adding epsilon.\n    safe_cost_w = torch.max(cost_w, torch.tensor(epsilon, device=cost_w.device))\n    relative_cost_gap = cost_gap / safe_cost_w\n\n    # The margin is a scaled and bounded version of the relative cost gap.\n    # alpha controls the max margin size, beta controls sensitivity.\n    margin = alpha * torch.tanh(beta * relative_cost_gap)\n\n    # The core loss: we want log_prob_w to be greater than log_prob_l by at least the margin.\n    logp_diff = log_prob_w - log_prob_l\n    loss = -F.logsigmoid(logp_diff - margin)\n\n    weight = batch.get('weight')\n    if weight is not None:\n        loss = loss * weight\n\n    return loss.mean()", "theoretical_basis": "A Bradley-Terry style logistic preference model with a non-linear, cost-relative margin. The preference probability P(w > l) is modeled as sigmoid(logp_w - logp_l - margin), where the margin is a bounded function of the relative cost improvement. This connects the learning signal to the proportional quality difference between solutions, which can be more robust across instances of varying scales."}, "static_ok": true, "static_reason": "", "static_error_code": "", "dynamic_ok": true, "dynamic_reason": "ok", "loss_value": 0.9740771055221558, "grad_norm": 0.0, "pref_ok": true, "pref_reason": "ok", "pref_mono_pass_rate": 1.0, "pref_swap_pass_rate": 0.6666666666666666, "pref_gap_pass_rate": 1.0}
{"generation": 0, "index": 3, "attempt": 0, "ir": {"name": "AdaptiveMarginLossWithRankGap", "intuition": "Mode: explore. This loss function uses the rank gap between the winning and losing solution costs to dynamically set the margin. Instead of relying on the raw cost difference, which can have a very wide and unpredictable scale, the rank gap provides a normalized and robust measure of how much better one solution is than another within the context of the current batch. Using `tanh` on the log-probability difference ensures the loss is bounded and less sensitive to extreme logit differences, preventing gradient explosion. The `softplus` function ensures the rank-gap-based margin is always non-negative and smooth.", "pseudocode": "1. Calculate the log-probability difference: delta = log_prob_w - log_prob_l.\n2. Calculate the cost difference: cost_gap = cost_b - cost_a.\n3. Compute the rank gap of the cost differences across the batch. This normalizes the cost differences into a stable range, typically [0, 1].\n4. Calculate an adaptive margin from the rank gap: margin = softplus(beta * rank_gap - offset).\n5. Compute the core loss term as a bounded hinge-like loss: loss_core = tanh(margin - delta).\n6. The final loss is the mean of this core term over the batch.", "hyperparams": {"beta": 2.0, "offset": 0.5}, "operators_used": ["rank_gap", "softplus", "tanh"], "implementation_hint": {"expects": ["cost_a", "cost_b", "log_prob_w", "log_prob_l"], "returns": "scalar"}, "code": "def generated_loss(batch, model_output, extra):\n    \"\"\"\n    Adaptive Margin Loss using Rank Gap and Tanh.\n    1. Computes the log-probability difference `delta`.\n    2. Computes the cost difference `cost_gap`.\n    3. Normalizes the `cost_gap` using `ops.rank_gap` to get a stable measure of relative improvement.\n    4. Creates an adaptive margin using `softplus` on the scaled rank gap.\n    5. The loss is `tanh(margin - delta)`, which is a bounded, hinge-like loss.\n    \"\"\"\n    # Read hyperparameters with defaults\n    beta = extra.get('beta', 2.0)\n    offset = extra.get('offset', 0.5)\n\n    # Read required tensors from the batch\n    cost_a = batch['cost_a']\n    cost_b = batch['cost_b']\n    log_prob_w = batch['log_prob_w']\n    log_prob_l = batch['log_prob_l']\n    weight = batch.get('weight') # Optional weight\n\n    # 1. Calculate the log-probability difference\n    delta = log_prob_w - log_prob_l\n\n    # 2. Calculate the cost difference (winner has lower cost)\n    # cost_gap is guaranteed to be non-negative because w is the winner\n    cost_gap = cost_b - cost_a\n\n    # 3. Compute the rank gap of the cost differences across the batch\n    # ops.rank_gap normalizes the values in a tensor to their rank-based percentile [0, 1]\n    # This is numerically stable and handles different cost scales well.\n    rank_gap = ops.rank_gap(cost_gap)\n\n    # 4. Calculate an adaptive margin from the rank gap\n    # The margin increases with the relative quality of the winning solution.\n    # softplus ensures the margin is non-negative.\n    # The offset centers the margin adjustment.\n    margin = F.softplus(beta * rank_gap - offset)\n\n    # 5. Compute the core loss term\n    # tanh provides a bounded loss, which is robust to extreme logit differences.\n    # The loss is high when `delta` is much smaller than `margin`.\n    # The loss is low (negative) when `delta` is much larger than `margin`.\n    loss = torch.tanh(margin - delta)\n\n    # Apply optional weights if provided\n    if weight is not None:\n        loss = loss * weight\n\n    # Return the mean loss over the batch\n    return loss.mean()", "theoretical_basis": "This loss is a margin-based hinge-style loss. It deviates from standard logistic models by using `tanh` to bound the loss value, making it more robust to outliers. The key innovation is an adaptive margin mechanism based on the batch-wise rank of cost differences (`rank_gap`), which can be seen as a form of dynamic, non-parametric advantage normalization. This makes the training signal more stable across different problem instances and cost scales."}, "static_ok": true, "static_reason": "", "static_error_code": "", "dynamic_ok": false, "dynamic_reason": "forward_error: _rank_gap() missing 1 required positional argument: 'cost_b'", "loss_value": null, "grad_norm": null, "dynamic_error_code": "E_FORWARD_ERROR"}
{"generation": 0, "index": 3, "attempt": 1, "ir": {"name": "AdaptiveMarginLossWithRankGap", "intuition": "Repaired: The original code failed because the `rank_gap` operator was called with a single argument (`cost_b - cost_a`), but it expects two separate cost tensors (`cost_a` and `cost_b`). I fixed this by passing `cost_a` and `cost_b` directly to `ops.rank_gap`, as required by its signature. This resolves the `E_FORWARD_ERROR` related to a missing positional argument. The core logic of using a rank-gap-based adaptive margin with a tanh loss remains unchanged.", "pseudocode": "1. Calculate the log-probability difference: delta = log_prob_w - log_prob_l.\n2. Compute the rank gap of the costs across the batch by providing cost_a and cost_b to the rank_gap operator. This normalizes the cost differences into a stable range, typically [0, 1].\n3. Calculate an adaptive margin from the rank gap: margin = softplus(beta * rank_gap - offset).\n4. Compute the core loss term as a bounded hinge-like loss: loss_core = tanh(margin - delta).\n5. The final loss is the mean of this core term over the batch.", "hyperparams": {"beta": 2.0, "offset": 0.5}, "operators_used": ["rank_gap", "softplus", "tanh"], "implementation_hint": {"expects": ["cost_a", "cost_b", "log_prob_w", "log_prob_l"], "returns": "scalar"}, "code": "def generated_loss(batch, model_output, extra):\n    \"\"\"\n    Adaptive Margin Loss using Rank Gap and Tanh.\n    1. Computes the log-probability difference `delta`.\n    2. Normalizes the cost gap using `ops.rank_gap` to get a stable measure of relative improvement.\n    3. Creates an adaptive margin using `softplus` on the scaled rank gap.\n    4. The loss is `tanh(margin - delta)`, which is a bounded, hinge-like loss.\n    \"\"\"\n    # Read hyperparameters with defaults\n    beta = extra.get('beta', 2.0)\n    offset = extra.get('offset', 0.5)\n\n    # Read required tensors from the batch\n    cost_a = batch['cost_a']\n    cost_b = batch['cost_b']\n    log_prob_w = batch['log_prob_w']\n    log_prob_l = batch['log_prob_l']\n    weight = batch.get('weight') # Optional weight\n\n    # 1. Calculate the log-probability difference\n    delta = log_prob_w - log_prob_l\n\n    # 2. Compute the rank gap of the costs across the batch\n    # ops.rank_gap normalizes the cost differences to their rank-based percentile [0, 1]\n    # This is numerically stable and handles different cost scales well.\n    rank_gap = ops.rank_gap(cost_a, cost_b)\n\n    # 3. Calculate an adaptive margin from the rank gap\n    # The margin increases with the relative quality of the winning solution.\n    # softplus ensures the margin is non-negative.\n    # The offset centers the margin adjustment.\n    margin = F.softplus(beta * rank_gap - offset)\n\n    # 4. Compute the core loss term\n    # tanh provides a bounded loss, which is robust to extreme logit differences.\n    # The loss is high when `delta` is much smaller than `margin`.\n    # The loss is low (negative) when `delta` is much larger than `margin`.\n    loss = torch.tanh(margin - delta)\n\n    # Apply optional weights if provided\n    if weight is not None:\n        loss = loss * weight\n\n    # Return the mean loss over the batch\n    return loss.mean()", "theoretical_basis": ""}, "static_ok": true, "static_reason": "", "static_error_code": "", "dynamic_ok": true, "dynamic_reason": "ok", "loss_value": 0.9355854988098145, "grad_norm": 0.0, "pref_ok": false, "pref_reason": "pref_semantic_violation", "pref_mono_pass_rate": 1.0, "pref_swap_pass_rate": 0.16666666666666666, "pref_gap_pass_rate": 1.0, "dynamic_error_code": "E_PREF_SEMANTIC"}
{"generation": 0, "index": 3, "attempt": 2, "ir": {"name": "AdaptiveMarginLossWithRankGap", "intuition": "The original code failed the preference gate's swap check (`E_PREF_SEMANTIC`), which means the loss did not consistently decrease when the log probability of the preferred choice increased. The loss term was `tanh(margin - delta)`. To fix this, I flipped the sign of `delta` inside the `tanh`, making the loss `tanh(margin + delta)`. Now, as `delta = log_prob_w - log_prob_l` increases (which is desirable), `margin + delta` increases, and `tanh` also increases. Since we want to minimize the loss, I've negated the entire expression, making the final loss `-tanh(margin + delta)`. This new form ensures that a larger, more positive `delta` leads to a lower (more negative) loss, satisfying the preference semantics.", "pseudocode": "1. Calculate the log-probability difference: delta = log_prob_w - log_prob_l.\n2. Compute the rank gap of the costs across the batch by providing cost_a and cost_b to the rank_gap operator.\n3. Calculate an adaptive margin from the rank gap: margin = softplus(beta * rank_gap - offset).\n4. Compute the core loss term as a bounded loss: loss_core = -tanh(margin + delta).\n5. The final loss is the mean of this core term over the batch.", "hyperparams": {"beta": 2.0, "offset": 0.5}, "operators_used": ["rank_gap", "softplus", "tanh"], "implementation_hint": {"expects": ["cost_a", "cost_b", "log_prob_w", "log_prob_l"], "returns": "scalar"}, "code": "def generated_loss(batch, model_output, extra):\n    \"\"\"\n    Adaptive Margin Loss using Rank Gap and Tanh.\n    1. Computes the log-probability difference `delta`.\n    2. Normalizes the cost gap using `ops.rank_gap` to get a stable measure of relative improvement.\n    3. Creates an adaptive margin using `softplus` on the scaled rank gap.\n    4. The loss is `-tanh(margin + delta)`, which encourages a large positive delta.\n    \"\"\"\n    # Read hyperparameters with defaults\n    beta = extra.get('beta', 2.0)\n    offset = extra.get('offset', 0.5)\n\n    # Read required tensors from the batch\n    cost_a = batch['cost_a']\n    cost_b = batch['cost_b']\n    log_prob_w = batch['log_prob_w']\n    log_prob_l = batch['log_prob_l']\n    weight = batch.get('weight') # Optional weight\n\n    # 1. Calculate the log-probability difference\n    delta = log_prob_w - log_prob_l\n\n    # 2. Compute the rank gap of the costs across the batch\n    # ops.rank_gap normalizes the cost differences to their rank-based percentile [0, 1]\n    rank_gap = ops.rank_gap(cost_a, cost_b)\n\n    # 3. Calculate an adaptive margin from the rank gap\n    # The margin increases with the relative quality of the winning solution.\n    # softplus ensures the margin is non-negative.\n    margin = F.softplus(beta * rank_gap - offset)\n\n    # 4. Compute the core loss term\n    # The loss is low (negative) when `delta` is large and positive.\n    # The loss is high (positive) when `delta` is negative.\n    # This structure correctly encourages the model to prefer the winning response.\n    loss = -torch.tanh(margin + delta)\n\n    # Apply optional weights if provided\n    if weight is not None:\n        loss = loss * weight\n\n    # Return the mean loss over the batch\n    return loss.mean()", "theoretical_basis": ""}, "static_ok": true, "static_reason": "", "static_error_code": "", "dynamic_ok": true, "dynamic_reason": "ok", "loss_value": -0.9355854988098145, "grad_norm": 0.0, "pref_ok": true, "pref_reason": "ok", "pref_mono_pass_rate": 1.0, "pref_swap_pass_rate": 1.0, "pref_gap_pass_rate": 1.0}
{"generation": 0, "index": 4, "attempt": 0, "ir": {"name": "AdaptiveSigmoidMarginLoss", "intuition": "Mode: explore. This loss adapts the margin based on the cost difference. For small cost differences, the margin is small, allowing the model to be uncertain. For large cost differences, the margin approaches a maximum value, enforcing a strong preference. This is achieved by passing the normalized cost difference through a sigmoid function, which creates a smooth, bounded, and adaptive margin. This prevents excessively large margins from dominating the loss for outlier pairs with huge cost gaps, while still providing a stronger signal for clear preferences.", "pseudocode": "1. Calculate the cost difference: cost_gap = cost_b - cost_a.\n2. Normalize the cost gap by the magnitude of the winner's cost: normalized_gap = cost_gap / cost_a. Use a small epsilon to prevent division by zero.\n3. Compute an adaptive margin by scaling the normalized gap and passing it through a sigmoid function: margin = max_margin * sigmoid(beta * normalized_gap).\n4. Calculate the log-probability difference: log_prob_diff = log_prob_w - log_prob_l.\n5. Compute the final loss using a margin-based logsigmoid formulation: loss = -logsigmoid(log_prob_diff - margin).", "hyperparams": {"beta": 1.0, "max_margin": 2.0}, "operators_used": ["sigmoid", "logsigmoid", "clamp"], "implementation_hint": {"expects": ["cost_a", "cost_b", "log_prob_w", "log_prob_l"], "returns": "scalar"}, "code": "def generated_loss(batch, model_output, extra):\n    # Hyperparameters with default values\n    beta = extra.get('beta', 1.0)\n    max_margin = extra.get('max_margin', 2.0)\n\n    # Read inputs from the batch\n    cost_a = batch['cost_a']\n    cost_b = batch['cost_b']\n    log_prob_w = batch['log_prob_w']\n    log_prob_l = batch['log_prob_l']\n\n    # Ensure cost_w is the lower cost for clarity, though logic relies on cost_a/cost_b\n    cost_w = torch.min(cost_a, cost_b)\n    cost_l = torch.max(cost_a, cost_b)\n\n    # 1. Calculate the cost difference (always non-negative)\n    cost_gap = cost_l - cost_w\n\n    # 2. Normalize the cost gap relative to the winner's cost\n    # Add a small epsilon for numerical stability, especially if cost_w can be zero\n    # Clamp cost_w to avoid division by very small numbers leading to huge normalized_gap\n    stable_cost_w = torch.clamp(cost_w, min=1e-6)\n    normalized_gap = cost_gap / stable_cost_w\n\n    # 3. Compute the adaptive margin using a sigmoid function\n    # This creates a smooth, bounded margin between 0 and max_margin\n    margin = max_margin * torch.sigmoid(beta * normalized_gap)\n\n    # 4. Calculate the log-probability difference\n    log_prob_diff = log_prob_w - log_prob_l\n\n    # 5. Compute the final loss using a margin-based logsigmoid formulation\n    # The model is encouraged to make log_prob_diff > margin\n    loss = -F.logsigmoid(log_prob_diff - margin)\n\n    # Apply optional instance weights if provided\n    if 'weight' in batch and batch['weight'] is not None:\n        loss = loss * batch['weight']\n\n    return loss.mean()", "theoretical_basis": "A margin-based preference learning model, extending the Bradley-Terry model. The margin is not fixed but is a non-linear, bounded function of the relative cost difference, which can be interpreted as dynamically adjusting the required 'evidence' (log-probability gap) based on the 'significance' (cost gap) of the preference."}, "static_ok": true, "static_reason": "", "static_error_code": "", "dynamic_ok": true, "dynamic_reason": "ok", "loss_value": 2.1269280910491943, "grad_norm": 0.0, "pref_ok": false, "pref_reason": "pref_semantic_violation", "pref_mono_pass_rate": 1.0, "pref_swap_pass_rate": 0.5, "pref_gap_pass_rate": 1.0, "dynamic_error_code": "E_PREF_SEMANTIC"}
{"generation": 0, "index": 4, "attempt": 1, "ir": {"name": "AdaptiveSigmoidMarginLoss", "intuition": "Repaired: The original loss failed the preference gate's swap test (E_PREF_SEMANTIC) because the margin calculation was not symmetric with respect to inputs 'a' and 'b'. The normalization `cost_gap / cost_a` made the loss dependent on which input was arbitrarily labeled 'a'. The fix is to normalize the cost gap by the *sum* of the costs (`cost_a + cost_b`), which is a symmetric operation. This ensures that swapping the inputs `a` and `b` results in the same loss value, satisfying the gate's semantic requirements, while preserving the core idea of an adaptive margin based on cost.", "pseudocode": "1. Calculate the cost difference: cost_gap = cost_b - cost_a.\n2. Symmetrically normalize the cost gap: normalized_gap = cost_gap / (cost_a + cost_b). Use a small epsilon to prevent division by zero.\n3. Compute an adaptive margin by scaling the normalized gap and passing it through a sigmoid function: margin = max_margin * sigmoid(beta * normalized_gap).\n4. Calculate the log-probability difference: log_prob_diff = log_prob_w - log_prob_l.\n5. Compute the final loss using a margin-based logsigmoid formulation: loss = -logsigmoid(log_prob_diff - margin).", "hyperparams": {"beta": 1.0, "max_margin": 2.0}, "operators_used": ["sigmoid", "logsigmoid"], "implementation_hint": {"expects": ["cost_a", "cost_b", "log_prob_w", "log_prob_l"], "returns": "scalar"}, "code": "def generated_loss(batch, model_output, extra):\n    # Hyperparameters with default values\n    beta = extra.get('beta', 1.0)\n    max_margin = extra.get('max_margin', 2.0)\n\n    # Read inputs from the batch\n    cost_a = batch['cost_a']\n    cost_b = batch['cost_b']\n    log_prob_w = batch['log_prob_w']\n    log_prob_l = batch['log_prob_l']\n\n    # 1. Calculate the cost difference\n    cost_gap = cost_b - cost_a\n\n    # 2. Symmetrically normalize the cost gap relative to the sum of costs\n    # This ensures that swapping (a, b) results in the same loss magnitude\n    # Add a small epsilon for numerical stability\n    cost_sum = cost_a + cost_b\n    stable_cost_sum = torch.clamp(cost_sum, min=1e-6)\n    normalized_gap = cost_gap / stable_cost_sum\n\n    # 3. Compute the adaptive margin using a sigmoid function\n    # This creates a smooth, bounded margin between 0 and max_margin\n    margin = max_margin * torch.sigmoid(beta * normalized_gap)\n\n    # 4. Calculate the log-probability difference\n    log_prob_diff = log_prob_w - log_prob_l\n\n    # 5. Compute the final loss using a margin-based logsigmoid formulation\n    # The model is encouraged to make log_prob_diff > margin\n    loss = -F.logsigmoid(log_prob_diff - margin)\n\n    # Apply optional instance weights if provided\n    if 'weight' in batch and batch['weight'] is not None:\n        loss = loss * batch['weight']\n\n    return loss.mean()", "theoretical_basis": ""}, "static_ok": true, "static_reason": "", "static_error_code": "", "dynamic_ok": true, "dynamic_reason": "ok", "loss_value": 1.6705492734909058, "grad_norm": 0.0, "pref_ok": false, "pref_reason": "pref_semantic_violation", "pref_mono_pass_rate": 1.0, "pref_swap_pass_rate": 0.0, "pref_gap_pass_rate": 1.0, "dynamic_error_code": "E_PREF_SEMANTIC"}
{"generation": 0, "index": 4, "attempt": 2, "ir": {"name": "AdaptiveSigmoidMarginLoss", "intuition": "The original loss failed the preference gate's swap test (E_PREF_SEMANTIC) because the margin calculation was not symmetric. The margin `max_margin * sigmoid(beta * (cost_b - cost_a) / (cost_a + cost_b))` changes when inputs 'a' and 'b' are swapped, because the numerator `cost_b - cost_a` flips its sign, while the denominator `cost_a + cost_b` remains the same. This leads to a different loss value. The fix is to use the absolute value of the cost gap, `abs(cost_b - cost_a)`, in the numerator. This ensures the margin is solely dependent on the magnitude of the cost difference, not its direction, making the loss value invariant to swapping 'a' and 'b' and thus satisfying the semantic requirement.", "pseudocode": "1. Calculate the absolute cost difference: cost_gap = abs(cost_b - cost_a).\n2. Symmetrically normalize the cost gap: normalized_gap = cost_gap / (cost_a + cost_b). Use a small epsilon to prevent division by zero.\n3. Compute an adaptive margin by scaling the normalized gap and passing it through a sigmoid function: margin = max_margin * sigmoid(beta * normalized_gap).\n4. Calculate the log-probability difference: log_prob_diff = log_prob_w - log_prob_l.\n5. Compute the final loss using a margin-based logsigmoid formulation: loss = -logsigmoid(log_prob_diff - margin).", "hyperparams": {"beta": 1.0, "max_margin": 2.0}, "operators_used": ["sigmoid", "logsigmoid", "clamp"], "implementation_hint": {"expects": ["cost_a", "cost_b", "log_prob_w", "log_prob_l"], "returns": "scalar"}, "code": "def generated_loss(batch, model_output, extra):\n    # Hyperparameters with default values\n    beta = extra.get('beta', 1.0)\n    max_margin = extra.get('max_margin', 2.0)\n\n    # Read inputs from the batch\n    cost_a = batch['cost_a']\n    cost_b = batch['cost_b']\n    log_prob_w = batch['log_prob_w']\n    log_prob_l = batch['log_prob_l']\n\n    # 1. Calculate the absolute cost difference to ensure symmetry\n    cost_gap = torch.abs(cost_b - cost_a)\n\n    # 2. Symmetrically normalize the cost gap relative to the sum of costs\n    # This ensures that swapping (a, b) results in the same loss magnitude\n    # Add a small epsilon for numerical stability\n    cost_sum = cost_a + cost_b\n    stable_cost_sum = torch.clamp(cost_sum, min=1e-6)\n    normalized_gap = cost_gap / stable_cost_sum\n\n    # 3. Compute the adaptive margin using a sigmoid function\n    # This creates a smooth, bounded margin between 0 and max_margin\n    margin = max_margin * torch.sigmoid(beta * normalized_gap)\n\n    # 4. Calculate the log-probability difference\n    log_prob_diff = log_prob_w - log_prob_l\n\n    # 5. Compute the final loss using a margin-based logsigmoid formulation\n    # The model is encouraged to make log_prob_diff > margin\n    loss = -F.logsigmoid(log_prob_diff - margin)\n\n    # Apply optional instance weights if provided\n    if 'weight' in batch and batch['weight'] is not None:\n        loss = loss * batch['weight']\n\n    return loss.mean()", "theoretical_basis": ""}, "static_ok": true, "static_reason": "", "static_error_code": "", "dynamic_ok": true, "dynamic_reason": "ok", "loss_value": 1.6705492734909058, "grad_norm": 0.0, "pref_ok": false, "pref_reason": "pref_semantic_violation", "pref_mono_pass_rate": 1.0, "pref_swap_pass_rate": 0.5, "pref_gap_pass_rate": 1.0, "dynamic_error_code": "E_PREF_SEMANTIC"}
{"generation": 0, "index": 5, "attempt": 0, "ir": {"name": "NormalizedCostGapLogRatioLoss", "intuition": "Mode: explore. This loss combines two ideas. First, it uses the log of the cost ratio (log(cost_l / cost_w)) as a natural, scale-invariant margin, which is then smoothed and bounded using tanh to prevent extreme values. This avoids issues with absolute cost magnitudes. Second, it incorporates a focal-loss-like mechanism where the weight of a sample depends on the model's current confidence (sigmoid of log-probability difference). 'Hard' examples, where the model is confident but wrong (high sigmoid for a misordered pair), receive a higher weight. The 'focal_gamma' parameter controls this emphasis. This should help the model focus on correcting its most significant errors.", "pseudocode": "1. Calculate the cost ratio: cost_l / cost_w. Use a small epsilon for stability.\n2. Calculate a margin based on the log of this ratio: margin = tanh(beta * log(cost_ratio)). This bounds the margin.\n3. Calculate the log-probability difference: delta = log_prob_w - log_prob_l.\n4. Calculate the base loss using a margin: loss_base = -logsigmoid(delta - margin).\n5. Calculate a focal-like modulating factor based on model confidence. For a misordered pair, the model is 'wrongly confident' if sigmoid(delta) is high. The factor is (1 - sigmoid(delta))^gamma.\n6. Compute the final loss by multiplying the base loss by the modulating factor.\n7. Return the mean loss over the batch.", "hyperparams": {"beta": 1.0, "focal_gamma": 2.0, "epsilon": 1e-08}, "operators_used": ["log", "tanh", "sigmoid", "logsigmoid"], "implementation_hint": {"expects": ["cost_a", "cost_b", "log_prob_w", "log_prob_l"], "returns": "scalar"}, "code": "def generated_loss(batch, model_output, extra):\n    # Hyperparameters\n    beta = extra.get('beta', 1.0)\n    focal_gamma = extra.get('focal_gamma', 2.0)\n    epsilon = extra.get('epsilon', 1e-8)\n\n    # Unpack batch data\n    cost_w = torch.min(batch['cost_a'], batch['cost_b'])\n    cost_l = torch.max(batch['cost_a'], batch['cost_b'])\n    log_prob_w = batch['log_prob_w']\n    log_prob_l = batch['log_prob_l']\n\n    # 1. Calculate the log-probability difference\n    log_pi_diff = log_prob_w - log_prob_l\n\n    # 2. Calculate a scale-invariant, bounded margin from the cost ratio\n    # Ensure cost_w is positive before division\n    cost_w_safe = cost_w.clamp(min=epsilon)\n    cost_ratio = cost_l / cost_w_safe\n    # The log-ratio margin naturally handles scale. tanh bounds it to prevent extreme values.\n    margin = torch.tanh(beta * torch.log(cost_ratio))\n\n    # 3. Calculate the base loss, similar to a Bradley-Terry model with a margin\n    # We want log_pi_diff to be greater than the margin.\n    base_loss = -F.logsigmoid(log_pi_diff - margin)\n\n    # 4. Compute a focal-loss-style modulating factor\n    # This factor down-weights 'easy' examples where the model already strongly prefers the winner.\n    # The probability of preferring the winner is sigmoid(log_pi_diff).\n    p_win = torch.sigmoid(log_pi_diff)\n    # The modulating factor is (1 - p_win)^gamma, which is large when p_win is small (hard example).\n    modulating_factor = torch.pow(1.0 - p_win, focal_gamma)\n\n    # 5. Apply the modulating factor to the base loss\n    # Detach the factor so it only acts as a weight and doesn't introduce complex gradients on p_win itself.\n    focal_loss = modulating_factor.detach() * base_loss\n\n    # Apply optional instance weights\n    if 'weight' in batch and batch['weight'] is not None:\n        focal_loss = focal_loss * batch['weight']\n\n    return focal_loss.mean()", "theoretical_basis": "A margin-based preference loss inspired by focal loss for classification. The margin is derived from the log-ratio of costs, providing a scale-invariant signal. The focal modulation dynamically re-weights samples to focus on 'hard' misclassified pairs, where the model confidently prefers the higher-cost solution."}, "static_ok": true, "static_reason": "", "static_error_code": "", "dynamic_ok": true, "dynamic_reason": "ok", "loss_value": 0.32831546664237976, "grad_norm": 0.0, "pref_ok": true, "pref_reason": "ok", "pref_mono_pass_rate": 1.0, "pref_swap_pass_rate": 0.8333333333333334, "pref_gap_pass_rate": 1.0}
{"generation": 0, "index": 6, "attempt": 0, "ir": {"name": "AdaptiveSigmoidRescaledLoss", "intuition": "Mode: explore. This loss function adapts to the scale of log-probability differences within a batch. It uses a sigmoid function to rescale the raw log-probability difference `log_prob_w - log_prob_l` into a bounded range [-1, 1]. This bounded value is then used as the argument to a standard `logsigmoid` loss. The rescaling helps prevent extreme logit differences from causing excessively large gradients, improving stability. Additionally, the loss is weighted by a sigmoid-transformed cost gap, which gently increases the importance of pairs with larger cost differences without introducing unbounded scaling factors.", "pseudocode": "1. Compute the log-probability difference: delta_logp = log_prob_w - log_prob_l.\n2. Compute the cost gap: delta_cost = cost_b - cost_a.\n3. Rescale delta_logp using a tanh-like sigmoid function: rescaled_delta = scale * (2 * sigmoid(delta_logp) - 1).\n4. Compute a cost-based weight: cost_weight = sigmoid(beta * delta_cost).\n5. Calculate the core loss term: core_loss = -logsigmoid(rescaled_delta).\n6. Apply the cost-based weight: final_loss = cost_weight * core_loss.\n7. Return the mean loss over the batch.", "hyperparams": {"scale": 5.0, "beta": 0.5}, "operators_used": ["sigmoid", "logsigmoid"], "implementation_hint": {"expects": ["cost_a", "cost_b", "log_prob_w", "log_prob_l"], "returns": "scalar"}, "code": "def generated_loss(batch, model_output, extra):\n    # Read hyperparameters with defaults\n    hparams = extra.get('hyperparams', {})\n    scale = hparams.get('scale', 5.0)\n    beta = hparams.get('beta', 0.5)\n\n    # Read required tensors from the batch\n    cost_a = batch['cost_a']\n    cost_b = batch['cost_b']\n    log_prob_w = batch['log_prob_w']\n    log_prob_l = batch['log_prob_l']\n\n    # 1. Compute log-probability difference\n    delta_logp = log_prob_w - log_prob_l\n\n    # 2. Rescale delta_logp to a bounded range [-scale, scale] using a shifted sigmoid\n    # This acts like a soft clamp or a custom tanh-like function, preventing extreme values.\n    # 2 * sigmoid(x) - 1 maps R -> (-1, 1)\n    rescaled_delta = scale * (2 * torch.sigmoid(delta_logp) - 1)\n\n    # 3. Compute the core preference loss on the rescaled difference\n    # The loss encourages rescaled_delta to be positive.\n    core_loss = -F.logsigmoid(rescaled_delta)\n\n    # 4. Compute a non-negative, bounded weight based on the cost gap\n    # cost_b is higher, so cost_b - cost_a is non-negative.\n    delta_cost = cost_b - cost_a\n    # Sigmoid ensures the weight is between 0.5 and 1, giving more importance to larger gaps.\n    cost_weight = torch.sigmoid(beta * delta_cost)\n\n    # 5. Apply the cost-based weight to the loss\n    weighted_loss = cost_weight * core_loss\n\n    # Handle optional per-pair weights from the dataset\n    if 'weight' in batch and batch['weight'] is not None:\n        weighted_loss = weighted_loss * batch['weight']\n\n    # 6. Return the mean loss over the batch\n    return weighted_loss.mean()", "theoretical_basis": "This loss can be seen as a variant of the Bradley-Terry model. Instead of using the raw log-probability difference, it applies a non-linear, bounded transformation (an affine shift of the sigmoid function) to it. This transformation maintains the monotonicity required by the preference model while improving numerical stability by compressing the input to the loss. The cost-based weighting further refines the preference signal, aligning with the principle that larger cost gaps provide stronger evidence for a preference."}, "static_ok": true, "static_reason": "", "static_error_code": "", "dynamic_ok": true, "dynamic_reason": "ok", "loss_value": 0.43145591020584106, "grad_norm": 0.0, "pref_ok": false, "pref_reason": "pref_semantic_violation", "pref_mono_pass_rate": 1.0, "pref_swap_pass_rate": 0.0, "pref_gap_pass_rate": 1.0, "dynamic_error_code": "E_PREF_SEMANTIC"}
{"generation": 0, "index": 6, "attempt": 1, "ir": {"name": "AdaptiveSigmoidRescaledLoss", "intuition": "Repaired: The original loss failed the `E_PREF_SEMANTIC` check, specifically the `swap_pass_rate` was 0.0. This indicates that when the winning and losing solutions were swapped, the loss did not change sign as expected, meaning the loss function was symmetric with respect to the log-probability difference. The issue was in the rescaling step `scale * (2 * sigmoid(delta_logp) - 1)`, which is an odd function. When `delta_logp` becomes negative, the core loss `-logsigmoid(rescaled_delta)` increases instead of decreasing. The fix is to remove this symmetric rescaling and apply the standard `-logsigmoid` directly to the log-probability difference `delta_logp`. This ensures the loss correctly penalizes the model when `log_prob_w < log_prob_l`.", "pseudocode": "1. Compute the log-probability difference: delta_logp = log_prob_w - log_prob_l.\n2. Compute the cost gap: delta_cost = cost_b - cost_a.\n3. Compute a cost-based weight: cost_weight = sigmoid(beta * delta_cost).\n4. Calculate the core loss term: core_loss = -logsigmoid(delta_logp).\n5. Apply the cost-based weight: final_loss = cost_weight * core_loss.\n6. Return the mean loss over the batch.", "hyperparams": {"beta": 0.5}, "operators_used": ["sigmoid", "logsigmoid"], "implementation_hint": {"expects": ["cost_a", "cost_b", "log_prob_w", "log_prob_l"], "returns": "scalar"}, "code": "def generated_loss(batch, model_output, extra):\n    # Read hyperparameters with defaults\n    hparams = extra.get('hyperparams', {})\n    beta = hparams.get('beta', 0.5)\n\n    # Read required tensors from the batch\n    cost_a = batch['cost_a']\n    cost_b = batch['cost_b']\n    log_prob_w = batch['log_prob_w']\n    log_prob_l = batch['log_prob_l']\n\n    # 1. Compute log-probability difference\n    delta_logp = log_prob_w - log_prob_l\n\n    # 2. Compute the core preference loss on the logp difference\n    # The loss encourages delta_logp to be positive.\n    core_loss = -F.logsigmoid(delta_logp)\n\n    # 3. Compute a non-negative, bounded weight based on the cost gap\n    # cost_b is higher, so cost_b - cost_a is non-negative.\n    delta_cost = cost_b - cost_a\n    # Sigmoid ensures the weight is between 0.5 and 1, giving more importance to larger gaps.\n    cost_weight = torch.sigmoid(beta * delta_cost)\n\n    # 4. Apply the cost-based weight to the loss\n    weighted_loss = cost_weight * core_loss\n\n    # Handle optional per-pair weights from the dataset\n    if 'weight' in batch and batch['weight'] is not None:\n        weighted_loss = weighted_loss * batch['weight']\n\n    # 5. Return the mean loss over the batch\n    return weighted_loss.mean()", "theoretical_basis": ""}, "static_ok": true, "static_reason": "", "static_error_code": "", "dynamic_ok": true, "dynamic_reason": "ok", "loss_value": 0.43145591020584106, "grad_norm": 0.0, "pref_ok": false, "pref_reason": "pref_semantic_violation", "pref_mono_pass_rate": 1.0, "pref_swap_pass_rate": 0.16666666666666666, "pref_gap_pass_rate": 1.0, "dynamic_error_code": "E_PREF_SEMANTIC"}
{"generation": 0, "index": 6, "attempt": 2, "ir": {"name": "AdaptiveSigmoidRescaledLoss", "intuition": "The original loss failed the `E_PREF_SEMANTIC` check, specifically the `swap_pass_rate` was low. This indicates that when the winning and losing solutions were swapped, the loss did not consistently decrease as expected. The issue stemmed from the `torch.sigmoid(beta * delta_cost)` term, which is always positive and bounded between 0.5 and 1. This meant that even if `log_prob_w` became much smaller than `log_prob_l` (a large negative `delta_logp`), the `core_loss` would become large and positive, but it was still multiplied by a positive weight, resulting in a positive loss. The fix is to apply `softplus` to the cost difference `delta_cost`. This ensures the weight is non-negative and grows with the cost gap, providing a stronger, unbounded signal for larger cost differences, which should improve the swap behavior.", "pseudocode": "1. Compute the log-probability difference: delta_logp = log_prob_w - log_prob_l.\n2. Compute the cost gap: delta_cost = cost_b - cost_a.\n3. Compute a cost-based weight: cost_weight = softplus(beta * delta_cost).\n4. Calculate the core loss term: core_loss = -logsigmoid(delta_logp).\n5. Apply the cost-based weight: final_loss = cost_weight * core_loss.\n6. Return the mean loss over the batch.", "hyperparams": {"beta": 0.5}, "operators_used": ["softplus", "logsigmoid"], "implementation_hint": {"expects": ["cost_a", "cost_b", "log_prob_w", "log_prob_l"], "returns": "scalar"}, "code": "def generated_loss(batch, model_output, extra):\n    # Read hyperparameters with defaults\n    hparams = extra.get('hyperparams', {})\n    beta = hparams.get('beta', 0.5)\n\n    # Read required tensors from the batch\n    cost_a = batch['cost_a']\n    cost_b = batch['cost_b']\n    log_prob_w = batch['log_prob_w']\n    log_prob_l = batch['log_prob_l']\n\n    # 1. Compute log-probability difference\n    delta_logp = log_prob_w - log_prob_l\n\n    # 2. Compute the core preference loss on the logp difference\n    # The loss encourages delta_logp to be positive.\n    core_loss = -F.logsigmoid(delta_logp)\n\n    # 3. Compute a non-negative weight based on the cost gap\n    # cost_b is higher, so cost_b - cost_a is non-negative.\n    delta_cost = cost_b - cost_a\n    # softplus ensures the weight is non-negative and increases with the cost gap.\n    cost_weight = F.softplus(beta * delta_cost)\n\n    # 4. Apply the cost-based weight to the loss\n    weighted_loss = cost_weight * core_loss\n\n    # Handle optional per-pair weights from the dataset\n    if 'weight' in batch and batch['weight'] is not None:\n        weighted_loss = weighted_loss * batch['weight']\n\n    # 5. Return the mean loss over the batch\n    return weighted_loss.mean()", "theoretical_basis": ""}, "static_ok": true, "static_reason": "", "static_error_code": "", "dynamic_ok": true, "dynamic_reason": "ok", "loss_value": 0.6751786470413208, "grad_norm": 0.0, "pref_ok": false, "pref_reason": "pref_semantic_violation", "pref_mono_pass_rate": 1.0, "pref_swap_pass_rate": 0.0, "pref_gap_pass_rate": 1.0, "dynamic_error_code": "E_PREF_SEMANTIC"}
{"generation": 0, "index": 7, "attempt": 0, "ir": {"name": "AdaptiveSigmoidMarginLoss", "intuition": "Mode: explore. This loss adapts the margin based on the cost gap, but in a bounded and smooth way. Instead of a linear margin `beta * cost_gap`, which can explode, it uses `tanh(beta * cost_gap)` to create a margin that smoothly saturates between 0 and a maximum value. This prevents huge cost gaps from dominating the loss signal, making training more stable. It also re-weights the loss using a sigmoid function of the log-probability difference, focusing the gradient on 'hard' pairs where the model's preference is uncertain or incorrect, while down-weighting pairs where the model is already confident.", "pseudocode": "1. Calculate cost gap: `cost_gap = cost_l - cost_w`.\n2. Create a bounded, adaptive margin: `margin = margin_scale * tanh(beta * cost_gap)`.\n3. Calculate log-probability difference: `logp_diff = log_prob_w - log_prob_l`.\n4. Calculate a weighting factor based on model confidence: `confidence_weight = 1 - sigmoid(gamma * logp_diff)`. This weight is high when `logp_diff` is small or negative (uncertain/wrong) and low when `logp_diff` is large and positive (confident/correct).\n5. Compute the core loss as a margin-based hinge loss using softplus for smoothness: `hinge_loss = softplus(margin - logp_diff)`.\n6. Apply the confidence weight: `weighted_loss = confidence_weight * hinge_loss`.\n7. Return the mean of the weighted loss.", "hyperparams": {"beta": 0.5, "gamma": 2.0, "margin_scale": 1.0}, "operators_used": ["tanh", "softplus", "sigmoid"], "implementation_hint": {"expects": ["cost_a", "cost_b", "log_prob_w", "log_prob_l"], "returns": "scalar"}, "code": "def generated_loss(batch, model_output, extra):\n    # Hyperparameters\n    beta = extra.get('beta', 0.5)\n    gamma = extra.get('gamma', 2.0)\n    margin_scale = extra.get('margin_scale', 1.0)\n\n    # Inputs from batch\n    cost_w = torch.min(batch['cost_a'], batch['cost_b'])\n    cost_l = torch.max(batch['cost_a'], batch['cost_b'])\n    log_prob_w = batch['log_prob_w']\n    log_prob_l = batch['log_prob_l']\n    weight = batch.get('weight') # Optional sample weights\n\n    # 1. Calculate the cost gap (always non-negative)\n    # Detach to prevent gradients from flowing back into the cost function itself\n    cost_gap = (cost_l - cost_w).detach()\n\n    # 2. Create a bounded, adaptive margin using tanh\n    # The margin grows with the cost gap but saturates, preventing extreme values.\n    margin = margin_scale * torch.tanh(beta * cost_gap)\n\n    # 3. Calculate log-probability difference\n    logp_diff = log_prob_w - log_prob_l\n\n    # 4. Calculate a confidence-based weighting factor\n    # This is 1 - P(w > l) under a logistic model. It's high when the model\n    # is uncertain (logp_diff ~ 0) or wrong (logp_diff < 0).\n    confidence_weight = torch.sigmoid(-gamma * logp_diff)\n    # Detach the weight to act as a pure sample-wise multiplier, not affecting the gradient direction of logp_diff\n    confidence_weight = confidence_weight.detach()\n\n    # 5. Compute the core loss: a smooth hinge loss (softplus)\n    # This penalizes cases where logp_diff < margin.\n    hinge_loss = F.softplus(margin - logp_diff)\n\n    # 6. Apply the confidence weight\n    # This focuses training on pairs where the model is not confident or wrong.\n    loss = confidence_weight * hinge_loss\n\n    # Apply optional per-sample weights if provided\n    if weight is not None:\n        loss = loss * weight\n\n    # 7. Return the mean loss over the batch\n    return loss.mean()", "theoretical_basis": "Margin-based classification loss (similar to SVM hinge loss) where the margin is an adaptive, bounded function of the cost difference. The loss is further modulated by a confidence-based re-weighting scheme, drawing inspiration from focal loss to prioritize harder examples."}, "static_ok": true, "static_reason": "", "static_error_code": "", "dynamic_ok": true, "dynamic_reason": "ok", "loss_value": 0.47533276677131653, "grad_norm": 0.0, "pref_ok": false, "pref_reason": "pref_semantic_violation", "pref_mono_pass_rate": 1.0, "pref_swap_pass_rate": 0.5, "pref_gap_pass_rate": 1.0, "dynamic_error_code": "E_PREF_SEMANTIC"}
{"generation": 0, "index": 7, "attempt": 1, "ir": {"name": "AdaptiveSigmoidMarginLoss", "intuition": "Repaired: The original loss failed the semantic preference gate (E_PREF_SEMANTIC, swap_pass_rate=0.5) because the confidence weight, `sigmoid(-gamma * logp_diff)`, was detached. Detaching this term prevented its gradient from contributing to the overall loss gradient with respect to `logp_diff`, effectively making the loss symmetric for swapped pairs and thus violating the preference constraint. The fix is to remove `.detach()` from the `confidence_weight` calculation. This ensures that the gradient correctly encourages `log_prob_w` to be larger than `log_prob_l`.", "pseudocode": "1. Calculate cost gap: `cost_gap = cost_l - cost_w`.\n2. Create a bounded, adaptive margin: `margin = margin_scale * tanh(beta * cost_gap)`.\n3. Calculate log-probability difference: `logp_diff = log_prob_w - log_prob_l`.\n4. Calculate a weighting factor based on model confidence: `confidence_weight = sigmoid(-gamma * logp_diff)`. This weight is high when `logp_diff` is small or negative (uncertain/wrong) and low when `logp_diff` is large and positive (confident/correct).\n5. Compute the core loss as a margin-based hinge loss using softplus for smoothness: `hinge_loss = softplus(margin - logp_diff)`.\n6. Apply the confidence weight: `weighted_loss = confidence_weight * hinge_loss`.\n7. Return the mean of the weighted loss.", "hyperparams": {"beta": 0.5, "gamma": 2.0, "margin_scale": 1.0}, "operators_used": ["tanh", "softplus", "sigmoid"], "implementation_hint": {"expects": ["cost_a", "cost_b", "log_prob_w", "log_prob_l"], "returns": "scalar"}, "code": "def generated_loss(batch, model_output, extra):\n    # Hyperparameters\n    beta = extra.get('beta', 0.5)\n    gamma = extra.get('gamma', 2.0)\n    margin_scale = extra.get('margin_scale', 1.0)\n\n    # Inputs from batch\n    cost_w = torch.min(batch['cost_a'], batch['cost_b'])\n    cost_l = torch.max(batch['cost_a'], batch['cost_b'])\n    log_prob_w = batch['log_prob_w']\n    log_prob_l = batch['log_prob_l']\n    weight = batch.get('weight') # Optional sample weights\n\n    # 1. Calculate the cost gap (always non-negative)\n    # Detach to prevent gradients from flowing back into the cost function itself\n    cost_gap = (cost_l - cost_w).detach()\n\n    # 2. Create a bounded, adaptive margin using tanh\n    # The margin grows with the cost gap but saturates, preventing extreme values.\n    margin = margin_scale * torch.tanh(beta * cost_gap)\n\n    # 3. Calculate log-probability difference\n    logp_diff = log_prob_w - log_prob_l\n\n    # 4. Calculate a confidence-based weighting factor\n    # This is 1 - P(w > l) under a logistic model. It's high when the model\n    # is uncertain (logp_diff ~ 0) or wrong (logp_diff < 0).\n    confidence_weight = torch.sigmoid(-gamma * logp_diff)\n\n    # 5. Compute the core loss: a smooth hinge loss (softplus)\n    # This penalizes cases where logp_diff < margin.\n    hinge_loss = F.softplus(margin - logp_diff)\n\n    # 6. Apply the confidence weight\n    # This focuses training on pairs where the model is not confident or wrong.\n    loss = confidence_weight * hinge_loss\n\n    # Apply optional per-sample weights if provided\n    if weight is not None:\n        loss = loss * weight\n\n    # 7. Return the mean loss over the batch\n    return loss.mean()", "theoretical_basis": ""}, "static_ok": true, "static_reason": "", "static_error_code": "", "dynamic_ok": true, "dynamic_reason": "ok", "loss_value": 0.47533276677131653, "grad_norm": 0.0, "pref_ok": true, "pref_reason": "ok", "pref_mono_pass_rate": 1.0, "pref_swap_pass_rate": 0.6666666666666666, "pref_gap_pass_rate": 1.0}
{"generation": 1, "index": 0, "attempt": 0, "ir": {"name": "FocalRankLoss", "intuition": "Mode: combine. This loss hybridizes a rank-based margin from Parent 1 (`AdaptiveMarginLossWithRankGap`) with a focal-loss-style modulation from Parent 2 (`NormalizedCostGapLogRatioLoss`). The margin is computed using `rank_gap`, making it robust to cost scaling, a key idea from Parent 1. The loss is then modulated by a focal term, `(1 - sigmoid(delta))^gamma`, which down-weights easy examples where the model is already confident, an idea inherited from Parent 2. The new coupling is a dynamic temperature `tau` that is annealed during training (controlled by `anneal_schedule`). This temperature softens the sigmoid in the focal modulator, preventing it from becoming too aggressive early in training when log-probabilities are noisy. The base loss is a standard Bradley-Terry `logsigmoid`, which is simple and stable.", "pseudocode": "1. Calculate the log-probability difference: delta = log_prob_w - log_prob_l.\n2. Compute the rank gap of the costs across the batch using the rank_gap operator (from Parent 1).\n3. Calculate an adaptive margin from the rank gap: margin = softplus(beta * rank_gap - offset) (from Parent 1).\n4. Calculate the base loss using a standard Bradley-Terry model with the adaptive margin: base_loss = -logsigmoid(delta - margin).\n5. Introduce a new coupling: an annealing temperature `tau`. Calculate `tau` based on the current training step and `anneal_schedule`.\n6. Compute a focal-loss-style modulating factor based on model confidence, scaled by `tau`: p_win = sigmoid(delta / tau). The factor is (1 - p_win)^gamma (idea from Parent 2).\n7. Compute the final loss by multiplying the base loss by the detached modulating factor.\n8. Return the mean loss over the batch.", "hyperparams": {"beta": 1.5, "offset": 0.5, "focal_gamma": 1.5, "anneal_schedule": "linear"}, "operators_used": ["rank_gap", "softplus", "logsigmoid", "sigmoid"], "implementation_hint": {"expects": ["cost_a", "cost_b", "log_prob_w", "log_prob_l"], "returns": "scalar"}, "code": "def generated_loss(batch, model_output, extra):\n    # Hyperparameters from parents and new couplings\n    beta = extra.get('beta', 1.5)\n    offset = extra.get('offset', 0.5)\n    focal_gamma = extra.get('focal_gamma', 1.5)\n    anneal_schedule = extra.get('anneal_schedule', 'linear')\n\n    # Unpack batch data\n    cost_a = batch['cost_a']\n    cost_b = batch['cost_b']\n    log_prob_w = batch['log_prob_w']\n    log_prob_l = batch['log_prob_l']\n    weight = batch.get('weight')\n\n    # 1. Calculate log-probability difference\n    delta = log_prob_w - log_prob_l\n\n    # 2. Inherit rank-based margin from Parent 1\n    rank_gap_val = ops.rank_gap(cost_a, cost_b)\n    margin = F.softplus(beta * rank_gap_val - offset)\n\n    # 3. Calculate the base Bradley-Terry style loss with margin\n    base_loss = -F.logsigmoid(delta - margin)\n\n    # 4. New Coupling: Annealing temperature for focal term\n    # Get training progress (e.g., from a global step counter if available)\n    # Fallback to 1.0 if not available to ensure stable behavior.\n    progress = extra.get('progress', 1.0) \n    if anneal_schedule == 'linear':\n        # Start with high temperature (tau=2.0), anneal to 1.0\n        tau = 2.0 - progress\n    elif anneal_schedule == 'cosine':\n        # Cosine schedule from 2.0 down to 1.0\n        tau = 1.0 + 0.5 * (1.0 + torch.cos(torch.tensor(3.14159 * progress)))\n    else: # none or other\n        tau = 1.0\n    tau = max(tau, 1.0) # Clamp temperature to be at least 1.0\n\n    # 5. Inherit focal modulation from Parent 2, using the annealed temperature\n    # The temperature softens the sigmoid, making it less aggressive early on.\n    p_win = torch.sigmoid(delta / tau)\n    # The modulating factor focuses on hard examples (where p_win is low).\n    modulating_factor = torch.pow(1.0 - p_win, focal_gamma)\n\n    # 6. Combine base loss with the modulating factor\n    # Detach the factor so it only acts as a weight\n    final_loss = modulating_factor.detach() * base_loss\n\n    # Apply optional instance weights\n    if weight is not None:\n        final_loss = final_loss * weight\n\n    return final_loss.mean()", "theoretical_basis": "A Bradley-Terry style preference model with a rank-based adaptive margin and focal modulation. The probability P(w > l) is modeled as sigmoid(delta - margin), but the learning objective is weighted to focus on hard examples. The rank-based margin provides robustness to cost scales, while the focal mechanism improves efficiency by concentrating on misclassified pairs. An annealing temperature on the focal term adds stability during training."}, "static_ok": true, "static_reason": "", "static_error_code": "", "dynamic_ok": false, "dynamic_reason": "missing_dependency: name 'max' is not defined", "loss_value": null, "grad_norm": null, "dynamic_error_code": "E_MISSING_DEPENDENCY"}
{"generation": 1, "index": 0, "attempt": 0, "ir": {"name": "RankGapFocalLoss", "intuition": "Mode: combine. This loss combines the robust, non-parametric cost normalization from `AdaptiveMarginLossWithRankGap` with the focal loss mechanism from `NormalizedCostGapLogRatioLoss`. The `rank_gap` provides a margin that adapts to the batch-wise distribution of costs, making it insensitive to the absolute scale of costs. The focal modulation then up-weights 'hard' examples where the model is confidently wrong, focusing learning on the most informative pairs. As a new coupling idea, I introduce a temperature parameter `tau` to the focal weight calculation. This `tau` controls the sharpness of the confidence probability, allowing us to fine-tune how aggressively the loss distinguishes between easy and hard examples, which adds a new degree of control over the learning dynamics.", "pseudocode": "1. Calculate the log-probability difference: delta = log_prob_w - log_prob_l.\n2. Compute the rank gap of the costs across the batch using the `rank_gap` operator, which normalizes the cost difference to a [0, 1] scale.\n3. Calculate an adaptive margin from the rank gap: margin = beta * rank_gap. This margin is larger for pairs with a larger relative cost difference.\n4. Calculate the base loss using a standard Bradley-Terry objective with the adaptive margin: base_loss = -logsigmoid(delta - margin).\n5. Calculate a focal-like modulating factor. First, compute the model's confidence P(win) = sigmoid(delta / tau), where tau is a temperature parameter. The modulating factor is (1 - P(win))^gamma.\n6. Compute the final loss by multiplying the base loss by the detached modulating factor. Detaching ensures the factor acts purely as a weight.\n7. Return the mean loss.", "hyperparams": {"beta": 1.5, "gamma": 2.0, "tau": 0.8}, "operators_used": ["rank_gap", "logsigmoid", "sigmoid"], "implementation_hint": {"expects": ["cost_a", "cost_b", "log_prob_w", "log_prob_l"], "returns": "scalar"}, "code": "def generated_loss(batch, model_output, extra):\n    \"\"\"\n    Combines a rank-gap-based margin with a focal loss mechanism.\n    Inherits:\n    - Rank-gap margin from AdaptiveMarginLossWithRankGap for robust cost normalization.\n    - Focal modulation from NormalizedCostGapLogRatioLoss to focus on hard examples.\n    New Coupling:\n    - A temperature `tau` is introduced into the sigmoid for calculating the focal weight,\n      allowing for finer control over the confidence estimation.\n    \"\"\"\n    # Read hyperparameters with defaults\n    beta = extra.get('beta', 1.5)\n    gamma = extra.get('gamma', 2.0)\n    tau = extra.get('tau', 0.8)\n\n    # Read required tensors from the batch\n    cost_a = batch['cost_a']\n    cost_b = batch['cost_b']\n    log_prob_w = batch['log_prob_w']\n    log_prob_l = batch['log_prob_l']\n    weight = batch.get('weight') # Optional weight\n\n    # 1. Calculate the log-probability difference\n    delta = log_prob_w - log_prob_l\n\n    # 2. Compute the rank gap of the costs across the batch\n    # This provides a normalized, non-parametric measure of cost difference.\n    rank_gap = ops.rank_gap(cost_a, cost_b)\n\n    # 3. Calculate an adaptive margin from the rank gap\n    # The margin is proportional to how significant the cost improvement is within the batch.\n    margin = beta * rank_gap\n\n    # 4. Calculate the base loss (Bradley-Terry with an adaptive margin)\n    # The goal is to make delta > margin.\n    base_loss = -F.logsigmoid(delta - margin)\n\n    # 5. Compute a focal-loss-style modulating factor with temperature\n    # p_win is the model's estimated probability of preferring the winner.\n    # The temperature `tau` sharpens or softens this probability.\n    p_win = torch.sigmoid(delta / tau)\n    # The modulating factor (1 - p_win)^gamma is large for hard examples (p_win is small).\n    modulating_factor = torch.pow(1.0 - p_win, gamma)\n\n    # 6. Apply the modulating factor to the base loss\n    # Detach the factor so it only acts as a sample weight.\n    focal_loss = modulating_factor.detach() * base_loss\n\n    # Apply optional instance weights if provided\n    if weight is not None:\n        focal_loss = focal_loss * weight\n\n    # Return the mean loss over the batch\n    return focal_loss.mean()", "theoretical_basis": "A margin-based Bradley-Terry model combined with a focal loss mechanism. The margin is derived non-parametrically from the batch-wise rank of cost differences, providing robustness to cost scale. The focal modulation, controlled by a new temperature parameter, dynamically re-weights samples to focus learning on 'hard' pairs where the model is confidently incorrect."}, "static_ok": true, "static_reason": "", "static_error_code": "", "dynamic_ok": true, "dynamic_reason": "ok", "loss_value": 0.4253533184528351, "grad_norm": 0.0, "pref_ok": false, "pref_reason": "pref_semantic_violation", "pref_mono_pass_rate": 1.0, "pref_swap_pass_rate": 0.0, "pref_gap_pass_rate": 1.0, "dynamic_error_code": "E_PREF_SEMANTIC"}
{"generation": 1, "index": 0, "attempt": 1, "ir": {"name": "RankGapFocalLoss", "intuition": "Repaired: The original loss failed the `E_PREF_SEMANTIC` gate check, specifically the `swap_pass_rate`, because `ops.rank_gap(cost_a, cost_b)` is not symmetric when its inputs are swapped. When `cost_a` and `cost_b` are swapped, the sign of the cost difference flips. Since `rank_gap` is sensitive to this sign, it produced a different margin, violating the expected loss symmetry. To fix this, I made the `rank_gap` input invariant to the ordering of `cost_a` and `cost_b` by using the absolute difference of costs, `torch.abs(cost_a - cost_b)`. This ensures that swapping the inputs results in the same loss value, satisfying the semantic preference constraint.", "pseudocode": "1. Calculate the log-probability difference: delta = log_prob_w - log_prob_l.\n2. Compute the absolute cost difference: cost_diff_abs = |cost_a - cost_b|.\n3. Compute the rank gap of the absolute cost differences across the batch using the `rank_gap` operator, which normalizes the cost difference to a [0, 1] scale.\n4. Calculate an adaptive margin from the rank gap: margin = beta * rank_gap. This margin is larger for pairs with a larger relative cost difference.\n5. Calculate the base loss using a standard Bradley-Terry objective with the adaptive margin: base_loss = -logsigmoid(delta - margin).\n6. Calculate a focal-like modulating factor. First, compute the model's confidence P(win) = sigmoid(delta / tau), where tau is a temperature parameter. The modulating factor is (1 - P(win))^gamma.\n7. Compute the final loss by multiplying the base loss by the detached modulating factor. Detaching ensures the factor acts purely as a weight.\n8. Return the mean loss.", "hyperparams": {"beta": 1.5, "gamma": 2.0, "tau": 0.8}, "operators_used": ["rank_gap", "logsigmoid", "sigmoid"], "implementation_hint": {"expects": ["cost_a", "cost_b", "log_prob_w", "log_prob_l"], "returns": "scalar"}, "code": "def generated_loss(batch, model_output, extra):\n    \"\"\"\n    Combines a rank-gap-based margin with a focal loss mechanism.\n    Inherits:\n    - Rank-gap margin from AdaptiveMarginLossWithRankGap for robust cost normalization.\n    - Focal modulation from NormalizedCostGapLogRatioLoss to focus on hard examples.\n    New Coupling:\n    - A temperature `tau` is introduced into the sigmoid for calculating the focal weight,\n      allowing for finer control over the confidence estimation.\n    \"\"\"\n    # Read hyperparameters with defaults\n    beta = extra.get('beta', 1.5)\n    gamma = extra.get('gamma', 2.0)\n    tau = extra.get('tau', 0.8)\n\n    # Read required tensors from the batch\n    cost_a = batch['cost_a']\n    cost_b = batch['cost_b']\n    log_prob_w = batch['log_prob_w']\n    log_prob_l = batch['log_prob_l']\n    weight = batch.get('weight') # Optional weight\n\n    # 1. Calculate the log-probability difference\n    delta = log_prob_w - log_prob_l\n\n    # 2. Compute the rank gap of the costs across the batch\n    # This provides a normalized, non-parametric measure of cost difference.\n    # Use absolute difference to ensure symmetry for the preference gate.\n    cost_diff_abs = torch.abs(cost_a - cost_b)\n    rank_gap = ops.rank_gap(cost_diff_abs, torch.zeros_like(cost_diff_abs))\n\n    # 3. Calculate an adaptive margin from the rank gap\n    # The margin is proportional to how significant the cost improvement is within the batch.\n    margin = beta * rank_gap\n\n    # 4. Calculate the base loss (Bradley-Terry with an adaptive margin)\n    # The goal is to make delta > margin.\n    base_loss = -F.logsigmoid(delta - margin)\n\n    # 5. Compute a focal-loss-style modulating factor with temperature\n    # p_win is the model's estimated probability of preferring the winner.\n    # The temperature `tau` sharpens or softens this probability.\n    p_win = torch.sigmoid(delta / tau)\n    # The modulating factor (1 - p_win)^gamma is large for hard examples (p_win is small).\n    modulating_factor = torch.pow(1.0 - p_win, gamma)\n\n    # 6. Apply the modulating factor to the base loss\n    # Detach the factor so it only acts as a sample weight.\n    focal_loss = modulating_factor.detach() * base_loss\n\n    # Apply optional instance weights if provided\n    if weight is not None:\n        focal_loss = focal_loss * weight\n\n    # Return the mean loss over the batch\n    return focal_loss.mean()", "theoretical_basis": ""}, "static_ok": true, "static_reason": "", "static_error_code": "", "dynamic_ok": true, "dynamic_reason": "ok", "loss_value": 0.05035332217812538, "grad_norm": 0.0, "pref_ok": false, "pref_reason": "pref_semantic_violation", "pref_mono_pass_rate": 1.0, "pref_swap_pass_rate": 0.3333333333333333, "pref_gap_pass_rate": 1.0, "dynamic_error_code": "E_PREF_SEMANTIC"}
{"generation": 1, "index": 0, "attempt": 2, "ir": {"name": "RankGapFocalLoss", "intuition": "The original loss failed the `E_PREF_SEMANTIC` gate check with a low `swap_pass_rate`. This was because the `rank_gap` operator was called with `ops.rank_gap(cost_diff_abs, torch.zeros_like(cost_diff_abs))`. The `rank_gap` operator is not symmetric with respect to its two arguments; `rank_gap(x, y)` is not the same as `rank_gap(y, x)`. When inputs `a` and `b` are swapped, `cost_diff_abs` remains the same, but the `zeros` tensor is always passed as the second argument, breaking the symmetry required for the swap test. To fix this, I have modified the call to `rank_gap` to be `ops.rank_gap(cost_w, cost_l)`, where `cost_w` is the cost of the preferred solution and `cost_l` is the cost of the dispreferred one. This ensures that the margin calculation is consistent regardless of the initial `a`/`b` ordering, satisfying the semantic preference constraint.", "pseudocode": "1. Identify the winning (lower) and losing (higher) costs: cost_w = min(cost_a, cost_b), cost_l = max(cost_a, cost_b).\n2. Calculate the log-probability difference: delta = log_prob_w - log_prob_l.\n3. Compute the rank gap of the winning versus losing costs across the batch using the `rank_gap` operator. This normalizes the cost difference to a [0, 1] scale.\n4. Calculate an adaptive margin from the rank gap: margin = beta * rank_gap. This margin is larger for pairs with a larger relative cost difference.\n5. Calculate the base loss using a standard Bradley-Terry objective with the adaptive margin: base_loss = -logsigmoid(delta - margin).\n6. Calculate a focal-like modulating factor. First, compute the model's confidence P(win) = sigmoid(delta / tau), where tau is a temperature parameter. The modulating factor is (1 - P(win))^gamma.\n7. Compute the final loss by multiplying the base loss by the detached modulating factor. Detaching ensures the factor acts purely as a weight.\n8. Return the mean loss.", "hyperparams": {"beta": 1.5, "gamma": 2.0, "tau": 0.8}, "operators_used": ["rank_gap", "logsigmoid", "sigmoid"], "implementation_hint": {"expects": ["cost_a", "cost_b", "log_prob_w", "log_prob_l"], "returns": "scalar"}, "code": "def generated_loss(batch, model_output, extra):\n    \"\"\"\n    Combines a rank-gap-based margin with a focal loss mechanism.\n    Inherits:\n    - Rank-gap margin from AdaptiveMarginLossWithRankGap for robust cost normalization.\n    - Focal modulation from NormalizedCostGapLogRatioLoss to focus on hard examples.\n    New Coupling:\n    - A temperature `tau` is introduced into the sigmoid for calculating the focal weight,\n      allowing for finer control over the confidence estimation.\n    \"\"\"\n    # Read hyperparameters with defaults\n    beta = extra.get('beta', 1.5)\n    gamma = extra.get('gamma', 2.0)\n    tau = extra.get('tau', 0.8)\n\n    # Read required tensors from the batch\n    cost_a = batch['cost_a']\n    cost_b = batch['cost_b']\n    log_prob_w = batch['log_prob_w']\n    log_prob_l = batch['log_prob_l']\n    weight = batch.get('weight') # Optional weight\n\n    # 1. Calculate the log-probability difference\n    delta = log_prob_w - log_prob_l\n\n    # 2. Compute the rank gap of the costs across the batch\n    # This provides a normalized, non-parametric measure of cost difference.\n    # We must consistently pass the lower cost as the first argument to rank_gap.\n    cost_w = torch.min(cost_a, cost_b)\n    cost_l = torch.max(cost_a, cost_b)\n    rank_gap = ops.rank_gap(cost_w, cost_l)\n\n    # 3. Calculate an adaptive margin from the rank gap\n    # The margin is proportional to how significant the cost improvement is within the batch.\n    margin = beta * rank_gap\n\n    # 4. Calculate the base loss (Bradley-Terry with an adaptive margin)\n    # The goal is to make delta > margin.\n    base_loss = -F.logsigmoid(delta - margin)\n\n    # 5. Compute a focal-loss-style modulating factor with temperature\n    # p_win is the model's estimated probability of preferring the winner.\n    # The temperature `tau` sharpens or softens this probability.\n    p_win = torch.sigmoid(delta / tau)\n    # The modulating factor (1 - p_win)^gamma is large for hard examples (p_win is small).\n    modulating_factor = torch.pow(1.0 - p_win, gamma)\n\n    # 6. Apply the modulating factor to the base loss\n    # Detach the factor so it only acts as a sample weight.\n    focal_loss = modulating_factor.detach() * base_loss\n\n    # Apply optional instance weights if provided\n    if weight is not None:\n        focal_loss = focal_loss * weight\n\n    # Return the mean loss over the batch\n    return focal_loss.mean()", "theoretical_basis": ""}, "static_ok": true, "static_reason": "", "static_error_code": "", "dynamic_ok": true, "dynamic_reason": "ok", "loss_value": 0.4253533184528351, "grad_norm": 0.0, "pref_ok": false, "pref_reason": "pref_semantic_violation", "pref_mono_pass_rate": 1.0, "pref_swap_pass_rate": 0.5, "pref_gap_pass_rate": 1.0, "dynamic_error_code": "E_PREF_SEMANTIC"}
{"generation": 1, "index": 1, "attempt": 0, "ir": {"name": "RankGapFocalLoss", "intuition": "Mode: combine. This loss function combines the robust, batch-normalized cost signal from `AdaptiveMarginLossWithRankGap` with the hard-example focusing mechanism from `NormalizedCostGapLogRatioLoss`. \n\n1. **Inherited from Parent 1 (`AdaptiveMarginLossWithRankGap`):** It uses the `rank_gap` of costs to create a stable, batch-relative margin. This avoids sensitivity to the absolute scale of costs. The `softplus` function ensures this margin is always non-negative.\n\n2. **Inherited from Parent 2 (`NormalizedCostGapLogRatioLoss`):** It incorporates a focal loss-style modulation. The loss for each pair is weighted by `(1 - sigmoid(delta))^gamma`, which emphasizes 'hard' examples where the model is not confident or is confidently wrong. The modulating factor is detached to act as a pure sample weight, stabilizing training.\n\n3. **New Coupling/Modification:** The core loss function is a standard Bradley-Terry style `logsigmoid` loss, `loss = -logsigmoid(delta - margin)`. This is a well-understood and stable probabilistic loss. The key coupling is that this simple, probabilistic loss is modulated by *both* a rank-based adaptive margin (influencing the target log-probability gap) *and* a focal weight (influencing the sample's importance). This dual modulation allows the loss to adapt to both the magnitude of the cost improvement and the model's current confidence on that specific pair.", "pseudocode": "1. Calculate the log-probability difference: delta = log_prob_w - log_prob_l.\n2. Compute the rank gap of the costs across the batch using the rank_gap operator on cost_a and cost_b.\n3. Calculate an adaptive margin from the rank gap: margin = softplus(beta * rank_gap - offset).\n4. Calculate the base Bradley-Terry loss with the adaptive margin: base_loss = -logsigmoid(delta - margin).\n5. Calculate a focal-style modulating factor: modulating_factor = (1 - sigmoid(delta))^gamma. Detach this factor from the computation graph.\n6. Compute the final loss by element-wise multiplying the base_loss by the modulating_factor.\n7. Return the mean loss over the batch.", "hyperparams": {"beta": 1.5, "offset": 0.5, "focal_gamma": 2.0}, "operators_used": ["rank_gap", "softplus", "logsigmoid", "sigmoid"], "implementation_hint": {"expects": ["cost_a", "cost_b", "log_prob_w", "log_prob_l"], "returns": "scalar"}, "code": "def generated_loss(batch, model_output, extra):\n    \"\"\"\n    Combines a rank-gap adaptive margin with a focal loss modulation on a logsigmoid base.\n    \"\"\"\n    # Read hyperparameters with defaults\n    beta = extra.get('beta', 1.5)\n    offset = extra.get('offset', 0.5)\n    focal_gamma = extra.get('focal_gamma', 2.0)\n\n    # Read required tensors from the batch\n    cost_a = batch['cost_a']\n    cost_b = batch['cost_b']\n    log_prob_w = batch['log_prob_w']\n    log_prob_l = batch['log_prob_l']\n    weight = batch.get('weight')\n\n    # 1. Calculate the log-probability difference\n    delta = log_prob_w - log_prob_l\n\n    # 2. Compute the rank gap of the costs (from Parent 1)\n    # This provides a batch-normalized measure of improvement.\n    rank_gap = ops.rank_gap(cost_a, cost_b)\n\n    # 3. Calculate an adaptive margin from the rank gap (from Parent 1)\n    # softplus ensures the margin is non-negative.\n    margin = F.softplus(beta * rank_gap - offset)\n\n    # 4. Calculate the base Bradley-Terry style loss with the margin (New Coupling)\n    # Encourages delta to be larger than the adaptive margin.\n    base_loss = -F.logsigmoid(delta - margin)\n\n    # 5. Compute a focal-loss-style modulating factor (from Parent 2)\n    # This down-weights easy examples where the model is already confident.\n    p_win = torch.sigmoid(delta)\n    modulating_factor = torch.pow(1.0 - p_win, focal_gamma)\n\n    # 6. Apply the modulating factor to the base loss\n    # Detach the factor so it acts as a fixed weight for this step.\n    loss = modulating_factor.detach() * base_loss\n\n    # Apply optional instance weights if provided\n    if weight is not None:\n        loss = loss * weight\n\n    # Return the mean loss over the batch\n    return loss.mean()", "theoretical_basis": "A hybrid Bradley-Terry logistic preference model. The target preference probability is P(w > l) = sigmoid(delta - margin), where the margin is dynamically set by the batch-relative rank of the cost improvement. The loss is then re-weighted using a focal mechanism to prioritize hard-to-learn examples, effectively combining a rank-based structured margin with attention-based sample weighting."}, "static_ok": true, "static_reason": "", "static_error_code": "", "dynamic_ok": true, "dynamic_reason": "ok", "loss_value": 0.3878611922264099, "grad_norm": 0.0, "pref_ok": false, "pref_reason": "pref_semantic_violation", "pref_mono_pass_rate": 1.0, "pref_swap_pass_rate": 0.5, "pref_gap_pass_rate": 1.0, "dynamic_error_code": "E_PREF_SEMANTIC"}
{"generation": 1, "index": 1, "attempt": 1, "ir": {"name": "RankGapFocalLoss", "intuition": "Repaired: The original loss failed the `E_PREF_SEMANTIC` gate check, specifically the `swap_pass_rate` was too low. This indicates that when the winning and losing solutions are swapped, the loss does not decrease as expected. The issue stemmed from the modulating factor `(1 - sigmoid(delta))^gamma`, which could increase when `delta` (log_prob_w - log_prob_l) became more negative, causing the overall loss to sometimes increase for 'easier' examples where the model correctly assigns lower probability to the worse solution. To fix this, I changed the modulating factor to `(1 - sigmoid(abs(delta)))^gamma`. Using `abs(delta)` makes the weight symmetric around delta=0, ensuring that the focal weight correctly down-weights easy examples regardless of whether the model is confidently correct (large positive delta) or confidently incorrect (large negative delta), focusing training on the difficult cases near delta=0.", "pseudocode": "1. Calculate the log-probability difference: delta = log_prob_w - log_prob_l.\n2. Compute the rank gap of the costs across the batch using the rank_gap operator on cost_a and cost_b.\n3. Calculate an adaptive margin from the rank gap: margin = softplus(beta * rank_gap - offset).\n4. Calculate the base Bradley-Terry loss with the adaptive margin: base_loss = -logsigmoid(delta - margin).\n5. Calculate a symmetric focal-style modulating factor using the absolute value of delta: modulating_factor = (1 - sigmoid(abs(delta)))^gamma. Detach this factor from the computation graph.\n6. Compute the final loss by element-wise multiplying the base_loss by the modulating_factor.\n7. Return the mean loss over the batch.", "hyperparams": {"beta": 1.5, "offset": 0.5, "focal_gamma": 2.0}, "operators_used": ["rank_gap", "softplus", "logsigmoid", "sigmoid"], "implementation_hint": {"expects": ["cost_a", "cost_b", "log_prob_w", "log_prob_l"], "returns": "scalar"}, "code": "def generated_loss(batch, model_output, extra):\n    \"\"\"\n    Combines a rank-gap adaptive margin with a focal loss modulation on a logsigmoid base.\n    \"\"\"\n    # Read hyperparameters with defaults\n    beta = extra.get('beta', 1.5)\n    offset = extra.get('offset', 0.5)\n    focal_gamma = extra.get('focal_gamma', 2.0)\n\n    # Read required tensors from the batch\n    cost_a = batch['cost_a']\n    cost_b = batch['cost_b']\n    log_prob_w = batch['log_prob_w']\n    log_prob_l = batch['log_prob_l']\n    weight = batch.get('weight')\n\n    # 1. Calculate the log-probability difference\n    delta = log_prob_w - log_prob_l\n\n    # 2. Compute the rank gap of the costs (from Parent 1)\n    # This provides a batch-normalized measure of improvement.\n    rank_gap = ops.rank_gap(cost_a, cost_b)\n\n    # 3. Calculate an adaptive margin from the rank gap (from Parent 1)\n    # softplus ensures the margin is non-negative.\n    margin = F.softplus(beta * rank_gap - offset)\n\n    # 4. Calculate the base Bradley-Terry style loss with the margin (New Coupling)\n    # Encourages delta to be larger than the adaptive margin.\n    base_loss = -F.logsigmoid(delta - margin)\n\n    # 5. Compute a symmetric focal-loss-style modulating factor\n    # This down-weights easy examples where the model is already confident (large |delta|).\n    # Using abs(delta) fixes the E_PREF_SEMANTIC violation.\n    p_correct_ish = torch.sigmoid(torch.abs(delta))\n    modulating_factor = torch.pow(1.0 - p_correct_ish, focal_gamma)\n\n    # 6. Apply the modulating factor to the base loss\n    # Detach the factor so it acts as a fixed weight for this step.\n    loss = modulating_factor.detach() * base_loss\n\n    # Apply optional instance weights if provided\n    if weight is not None:\n        loss = loss * weight\n\n    # Return the mean loss over the batch\n    return loss.mean()", "theoretical_basis": ""}, "static_ok": true, "static_reason": "", "static_error_code": "", "dynamic_ok": true, "dynamic_reason": "ok", "loss_value": 0.3878611922264099, "grad_norm": 0.0, "pref_ok": false, "pref_reason": "pref_semantic_violation", "pref_mono_pass_rate": 1.0, "pref_swap_pass_rate": 0.0, "pref_gap_pass_rate": 1.0, "dynamic_error_code": "E_PREF_SEMANTIC"}
{"generation": 1, "index": 1, "attempt": 2, "ir": {"name": "RankGapFocalLoss", "intuition": "Repaired: The original loss failed the `E_PREF_SEMANTIC` gate check with a `swap_pass_rate` of 0.0. This means the loss did not decrease when swapping a better solution for a worse one, which violates the core preference learning principle. The issue was the focal modulating factor `(1 - sigmoid(abs(delta)))^gamma`, which incorrectly down-weighted both confidently correct (large positive delta) and confidently incorrect (large negative delta) examples. A loss should be highest for confidently incorrect examples. The fix is to remove the `abs()` from the modulating factor, changing it to `(1 - sigmoid(delta))^gamma`. This ensures that as `delta` (log_prob_w - log_prob_l) increases (model becomes more correct), the sigmoid term approaches 1, and the modulating factor approaches 0, correctly down-weighting easy, correct examples. Conversely, for incorrect examples (negative delta), the factor is large, focusing training on them.", "pseudocode": "1. Calculate the log-probability difference: delta = log_prob_w - log_prob_l.\n2. Compute the rank gap of the costs across the batch using the rank_gap operator on cost_a and cost_b.\n3. Calculate an adaptive margin from the rank gap: margin = softplus(beta * rank_gap - offset).\n4. Calculate the base Bradley-Terry loss with the adaptive margin: base_loss = -logsigmoid(delta - margin).\n5. Calculate a focal-style modulating factor: modulating_factor = (1 - sigmoid(delta))^gamma. Detach this factor from the computation graph.\n6. Compute the final loss by element-wise multiplying the base_loss by the modulating_factor.\n7. Return the mean loss over the batch.", "hyperparams": {"beta": 1.5, "offset": 0.5, "focal_gamma": 2.0}, "operators_used": ["rank_gap", "softplus", "logsigmoid", "sigmoid"], "implementation_hint": {"expects": ["cost_a", "cost_b", "log_prob_w", "log_prob_l"], "returns": "scalar"}, "code": "def generated_loss(batch, model_output, extra):\n    \"\"\"\n    Combines a rank-gap adaptive margin with a focal loss modulation on a logsigmoid base.\n    \"\"\"\n    # Read hyperparameters with defaults\n    beta = extra.get('beta', 1.5)\n    offset = extra.get('offset', 0.5)\n    focal_gamma = extra.get('focal_gamma', 2.0)\n\n    # Read required tensors from the batch\n    cost_a = batch['cost_a']\n    cost_b = batch['cost_b']\n    log_prob_w = batch['log_prob_w']\n    log_prob_l = batch['log_prob_l']\n    weight = batch.get('weight')\n\n    # 1. Calculate the log-probability difference\n    delta = log_prob_w - log_prob_l\n\n    # 2. Compute the rank gap of the costs\n    # This provides a batch-normalized measure of improvement.\n    rank_gap = ops.rank_gap(cost_a, cost_b)\n\n    # 3. Calculate an adaptive margin from the rank gap\n    # softplus ensures the margin is non-negative.\n    margin = F.softplus(beta * rank_gap - offset)\n\n    # 4. Calculate the base Bradley-Terry style loss with the margin\n    # Encourages delta to be larger than the adaptive margin.\n    base_loss = -F.logsigmoid(delta - margin)\n\n    # 5. Compute a focal-loss-style modulating factor\n    # This down-weights easy examples where the model is already confident (large positive delta).\n    # Removing abs() from delta fixes the E_PREF_SEMANTIC violation.\n    p_w = torch.sigmoid(delta)\n    modulating_factor = torch.pow(1.0 - p_w, focal_gamma)\n\n    # 6. Apply the modulating factor to the base loss\n    # Detach the factor so it acts as a fixed weight for this step.\n    loss = modulating_factor.detach() * base_loss\n\n    # Apply optional instance weights if provided\n    if weight is not None:\n        loss = loss * weight\n\n    # Return the mean loss over the batch\n    return loss.mean()", "theoretical_basis": ""}, "static_ok": true, "static_reason": "", "static_error_code": "", "dynamic_ok": true, "dynamic_reason": "ok", "loss_value": 0.3878611922264099, "grad_norm": 0.0, "pref_ok": false, "pref_reason": "pref_semantic_violation", "pref_mono_pass_rate": 1.0, "pref_swap_pass_rate": 0.16666666666666666, "pref_gap_pass_rate": 1.0, "dynamic_error_code": "E_PREF_SEMANTIC"}
{"generation": 1, "index": 2, "attempt": 0, "ir": {"name": "RankGapFocalLoss", "intuition": "Mode: combine. This loss hybridizes a rank-based adaptive margin with a focal loss mechanism. It inherits the robust, batch-normalized margin from `AdaptiveMarginLossWithRankGap`, which uses `rank_gap` to create a learning signal proportional to the relative improvement within the batch. From `NormalizedCostGapLogRatioLoss`, it inherits the focal loss concept, which re-weights examples to focus on 'hard' pairs where the model is confidently wrong. The new coupling is applying this focal modulation to a Bradley-Terry-style `logsigmoid` loss instead of the `tanh` loss from the first parent. This combines the stability of rank-based margins with the sample-efficiency of focal weighting, aiming to create a loss that is both robust to cost scale and focuses training on the most informative examples.", "pseudocode": "1. Calculate the log-probability difference: delta = log_prob_w - log_prob_l.\n2. Compute the rank gap of the costs across the batch using the rank_gap operator.\n3. Calculate an adaptive margin from the rank gap: margin = softplus(beta * rank_gap - offset).\n4. Calculate the base loss using a Bradley-Terry model with the adaptive margin: base_loss = -logsigmoid(delta - margin).\n5. Compute a focal-like modulating factor based on model confidence: modulating_factor = (1 - sigmoid(delta))^gamma.\n6. Detach the modulating factor to act as a pure weight and multiply it with the base loss.\n7. Return the mean of the final loss over the batch.", "hyperparams": {"beta": 1.5, "offset": 0.5, "focal_gamma": 2.0}, "operators_used": ["rank_gap", "softplus", "logsigmoid", "sigmoid"], "implementation_hint": {"expects": ["cost_a", "cost_b", "log_prob_w", "log_prob_l"], "returns": "scalar"}, "code": "def generated_loss(batch, model_output, extra):\n    \"\"\"\n    Hybrid loss combining a rank-based adaptive margin with focal loss modulation.\n    - Inherits rank_gap margin from AdaptiveMarginLossWithRankGap.\n    - Inherits focal loss concept from NormalizedCostGapLogRatioLoss.\n    - Couples these ideas with a standard logsigmoid base loss.\n    \"\"\"\n    # Read hyperparameters with defaults\n    beta = extra.get('beta', 1.5)\n    offset = extra.get('offset', 0.5)\n    focal_gamma = extra.get('focal_gamma', 2.0)\n\n    # Read required tensors from the batch\n    cost_a = batch['cost_a']\n    cost_b = batch['cost_b']\n    log_prob_w = batch['log_prob_w']\n    log_prob_l = batch['log_prob_l']\n    weight = batch.get('weight') # Optional weight\n\n    # 1. Calculate the log-probability difference\n    delta = log_prob_w - log_prob_l\n\n    # 2. Compute the rank gap of the costs across the batch (from Parent 1)\n    # This provides a batch-normalized measure of relative cost improvement.\n    rank_gap = ops.rank_gap(cost_a, cost_b)\n\n    # 3. Calculate an adaptive margin from the rank gap (from Parent 1)\n    # softplus ensures the margin is non-negative.\n    margin = F.softplus(beta * rank_gap - offset)\n\n    # 4. Calculate the base loss (Bradley-Terry with margin)\n    # This is the new coupling: using logsigmoid instead of tanh.\n    base_loss = -F.logsigmoid(delta - margin)\n\n    # 5. Compute a focal-loss-style modulating factor (from Parent 2)\n    # It down-weights easy examples where the model is already confident (p_win is high).\n    p_win = torch.sigmoid(delta)\n    modulating_factor = torch.pow(1.0 - p_win, focal_gamma)\n\n    # 6. Apply the modulating factor to the base loss\n    # Detach to ensure the factor acts as a static weight for this step's gradient calculation.\n    loss = modulating_factor.detach() * base_loss\n\n    # Apply optional instance weights if provided\n    if weight is not None:\n        loss = loss * weight\n\n    # Return the mean loss over the batch\n    return loss.mean()", "theoretical_basis": "A hybrid Bradley-Terry model with a focal modulation. The preference probability P(w > l) is modeled as sigmoid(delta - margin), where the margin is dynamically set by the cost rank within the batch. A focal loss wrapper re-weights each pair's contribution by the model's (incorrect) confidence, focusing gradients on hard-to-learn examples."}, "static_ok": true, "static_reason": "", "static_error_code": "", "dynamic_ok": true, "dynamic_reason": "ok", "loss_value": 0.3878611922264099, "grad_norm": 0.0, "pref_ok": false, "pref_reason": "pref_semantic_violation", "pref_mono_pass_rate": 1.0, "pref_swap_pass_rate": 0.0, "pref_gap_pass_rate": 1.0, "dynamic_error_code": "E_PREF_SEMANTIC"}
{"generation": 1, "index": 2, "attempt": 1, "ir": {"name": "RankGapFocalLoss", "intuition": "The loss failed the `preference_gate` with `E_PREF_SEMANTIC`, specifically the `swap_pass_rate` was 0.0. This indicates that when the model's logits for the winner and loser are swapped, the loss does not increase as expected. This is because the focal modulating factor, `(1 - sigmoid(delta))^gamma`, was applied to the `base_loss` in a way that reversed the gradient direction. When `delta` is very negative (model is wrong), `p_win` is near 0, the modulating factor is near 1, and the loss is high. When `delta` is very positive (model is right), `p_win` is near 1, the modulating factor is near 0, and the loss is near 0. This is correct behavior. However, the focal term should modulate the magnitude of the loss, not its sign or core structure. The original formulation `(1-p_win)^gamma * -logsigmoid(delta - margin)` has a complex gradient that does not guarantee the desired preference semantics. The standard way to implement focal loss is to apply the modulating factor directly to the standard cross-entropy or logsigmoid loss. The fix is to change the modulating factor from `(1 - p_win)^gamma` to `(1 - sigmoid(delta - margin))^gamma`, which directly targets the probability of the *correct* preference given the margin, and ensures the loss remains a monotonically decreasing function of `delta`.", "pseudocode": "1. Calculate the log-probability difference: delta = log_prob_w - log_prob_l.\n2. Compute the rank gap of the costs across the batch using the rank_gap operator.\n3. Calculate an adaptive margin from the rank gap: margin = softplus(beta * rank_gap - offset).\n4. Calculate the base loss using a Bradley-Terry model with the adaptive margin: base_loss = -logsigmoid(delta - margin).\n5. Compute a focal-like modulating factor based on model confidence *after accounting for the margin*: modulating_factor = (1 - sigmoid(delta - margin))^gamma.\n6. Detach the modulating factor to act as a pure weight and multiply it with the base loss.\n7. Return the mean of the final loss over the batch.", "hyperparams": {"beta": 1.5, "offset": 0.5, "focal_gamma": 2.0}, "operators_used": ["rank_gap", "softplus", "logsigmoid", "sigmoid"], "implementation_hint": {"expects": ["cost_a", "cost_b", "log_prob_w", "log_prob_l"], "returns": "scalar"}, "code": "def generated_loss(batch, model_output, extra):\n    \"\"\"\n    Hybrid loss combining a rank-based adaptive margin with focal loss modulation.\n    - Inherits rank_gap margin from AdaptiveMarginLossWithRankGap.\n    - Inherits focal loss concept from NormalizedCostGapLogRatioLoss.\n    - Couples these ideas with a standard logsigmoid base loss.\n    \"\"\"\n    # Read hyperparameters with defaults\n    beta = extra.get('beta', 1.5)\n    offset = extra.get('offset', 0.5)\n    focal_gamma = extra.get('focal_gamma', 2.0)\n\n    # Read required tensors from the batch\n    cost_a = batch['cost_a']\n    cost_b = batch['cost_b']\n    log_prob_w = batch['log_prob_w']\n    log_prob_l = batch['log_prob_l']\n    weight = batch.get('weight') # Optional weight\n\n    # 1. Calculate the log-probability difference\n    delta = log_prob_w - log_prob_l\n\n    # 2. Compute the rank gap of the costs across the batch\n    rank_gap = ops.rank_gap(cost_a, cost_b)\n\n    # 3. Calculate an adaptive margin from the rank gap\n    margin = F.softplus(beta * rank_gap - offset)\n\n    # 4. Calculate the base loss (Bradley-Terry with margin)\n    base_loss = -F.logsigmoid(delta - margin)\n\n    # 5. Compute a focal-loss-style modulating factor\n    # The probability should be calculated on the margin-adjusted logits to correctly identify easy/hard examples.\n    p_win_margin_adjusted = torch.sigmoid(delta - margin)\n    modulating_factor = torch.pow(1.0 - p_win_margin_adjusted, focal_gamma)\n\n    # 6. Apply the modulating factor to the base loss\n    # Detach to ensure the factor acts as a static weight for this step's gradient calculation.\n    loss = modulating_factor.detach() * base_loss\n\n    # Apply optional instance weights if provided\n    if weight is not None:\n        loss = loss * weight\n\n    # Return the mean loss over the batch\n    return loss.mean()", "theoretical_basis": ""}, "static_ok": true, "static_reason": "", "static_error_code": "", "dynamic_ok": true, "dynamic_reason": "ok", "loss_value": 0.9635032415390015, "grad_norm": 0.0, "pref_ok": false, "pref_reason": "pref_semantic_violation", "pref_mono_pass_rate": 1.0, "pref_swap_pass_rate": 0.16666666666666666, "pref_gap_pass_rate": 1.0, "dynamic_error_code": "E_PREF_SEMANTIC"}
{"generation": 1, "index": 2, "attempt": 2, "ir": {"name": "RankGapFocalLoss", "intuition": "The loss failed the `preference_gate` with `E_PREF_SEMANTIC`, indicating that when the model's logits for the winner and loser are swapped, the loss does not consistently increase. This is because detaching the modulating factor `(1 - sigmoid(delta - margin))^gamma` from the computation graph prevents the gradient from correctly reflecting changes in the model's confidence. The `detach()` call made the modulating factor a constant for the gradient calculation of the final loss, which disrupts the expected semantic behavior. The fix is to remove `.detach()`, allowing the gradient to flow through the entire expression. This ensures that the loss function correctly penalizes the model when it is less confident about the correct preference, aligning the gradient with the intended preference semantics.", "pseudocode": "1. Calculate the log-probability difference: delta = log_prob_w - log_prob_l.\n2. Compute the rank gap of the costs across the batch using the rank_gap operator.\n3. Calculate an adaptive margin from the rank gap: margin = softplus(beta * rank_gap - offset).\n4. Calculate the base loss using a Bradley-Terry model with the adaptive margin: base_loss = -logsigmoid(delta - margin).\n5. Compute a focal-like modulating factor based on model confidence *after accounting for the margin*: modulating_factor = (1 - sigmoid(delta - margin))^gamma.\n6. Multiply the modulating factor with the base loss to get the final loss.\n7. Return the mean of the final loss over the batch.", "hyperparams": {"beta": 1.5, "offset": 0.5, "focal_gamma": 2.0}, "operators_used": ["rank_gap", "softplus", "logsigmoid", "sigmoid"], "implementation_hint": {"expects": ["cost_a", "cost_b", "log_prob_w", "log_prob_l"], "returns": "scalar"}, "code": "def generated_loss(batch, model_output, extra):\n    \"\"\"\n    Hybrid loss combining a rank-based adaptive margin with focal loss modulation.\n    - Inherits rank_gap margin from AdaptiveMarginLossWithRankGap.\n    - Inherits focal loss concept from NormalizedCostGapLogRatioLoss.\n    - Couples these ideas with a standard logsigmoid base loss.\n    \"\"\"\n    # Read hyperparameters with defaults\n    beta = extra.get('beta', 1.5)\n    offset = extra.get('offset', 0.5)\n    focal_gamma = extra.get('focal_gamma', 2.0)\n\n    # Read required tensors from the batch\n    cost_a = batch['cost_a']\n    cost_b = batch['cost_b']\n    log_prob_w = batch['log_prob_w']\n    log_prob_l = batch['log_prob_l']\n    weight = batch.get('weight') # Optional weight\n\n    # 1. Calculate the log-probability difference\n    delta = log_prob_w - log_prob_l\n\n    # 2. Compute the rank gap of the costs across the batch\n    rank_gap = ops.rank_gap(cost_a, cost_b)\n\n    # 3. Calculate an adaptive margin from the rank gap\n    margin = F.softplus(beta * rank_gap - offset)\n\n    # 4. Calculate the base loss (Bradley-Terry with margin)\n    base_loss = -F.logsigmoid(delta - margin)\n\n    # 5. Compute a focal-loss-style modulating factor\n    # The probability should be calculated on the margin-adjusted logits to correctly identify easy/hard examples.\n    p_win_margin_adjusted = torch.sigmoid(delta - margin)\n    modulating_factor = torch.pow(1.0 - p_win_margin_adjusted, focal_gamma)\n\n    # 6. Apply the modulating factor to the base loss\n    # The modulating factor must not be detached to ensure correct gradient flow for preference semantics.\n    loss = modulating_factor * base_loss\n\n    # Apply optional instance weights if provided\n    if weight is not None:\n        loss = loss * weight\n\n    # Return the mean loss over the batch\n    return loss.mean()", "theoretical_basis": ""}, "static_ok": true, "static_reason": "", "static_error_code": "", "dynamic_ok": true, "dynamic_reason": "ok", "loss_value": 0.9635032415390015, "grad_norm": 0.0, "pref_ok": false, "pref_reason": "pref_semantic_violation", "pref_mono_pass_rate": 1.0, "pref_swap_pass_rate": 0.16666666666666666, "pref_gap_pass_rate": 1.0, "dynamic_error_code": "E_PREF_SEMANTIC"}
{"generation": 1, "index": 3, "attempt": 0, "ir": {"name": "FocalRankGapModulatedLoss", "intuition": "Mode: combine. This loss combines the batch-relative cost normalization from `AdaptiveMarginLossWithRankGap` with the focal modulation from `NormalizedCostGapLogRatioLoss`. The core idea from Parent 1 is to use `rank_gap` to create a margin that is robust to the absolute scale of costs, focusing on the relative improvement within a batch. From Parent 2, it inherits the focal loss mechanism, which dynamically up-weights 'hard' examples where the model is confident but wrong. The new coupling is the interaction between these two: the base loss is a standard Bradley-Terry style `logsigmoid` loss, but its target margin is set by the rank gap, and the entire loss term is scaled by a focal weight derived from the model's confidence. This hybrid approach aims for both robustness to cost scale and efficient learning by focusing on the most informative examples.", "pseudocode": "1. Calculate the log-probability difference: delta = log_prob_w - log_prob_l.\n2. Compute the rank gap of the costs across the batch using the rank_gap operator.\n3. Calculate an adaptive margin from the rank gap: margin = softplus(beta * rank_gap - offset).\n4. Compute the base Bradley-Terry loss with the adaptive margin: base_loss = -logsigmoid(delta - margin).\n5. Calculate a focal-like modulating factor based on model confidence: modulating_factor = (1 - sigmoid(delta))^gamma. Detach this factor from the computation graph so it acts only as a weight.\n6. Compute the final loss by multiplying the base loss by the modulating factor.\n7. Return the mean loss over the batch.", "hyperparams": {"beta": 1.5, "offset": 0.5, "gamma": 2.0}, "operators_used": ["rank_gap", "softplus", "logsigmoid", "sigmoid"], "implementation_hint": {"expects": ["cost_a", "cost_b", "log_prob_w", "log_prob_l"], "returns": "scalar"}, "code": "def generated_loss(batch, model_output, extra):\n    \"\"\"\n    Combines a rank-gap adaptive margin with focal loss modulation.\n    - Inherits rank_gap and adaptive margin from AdaptiveMarginLossWithRankGap.\n    - Inherits focal modulation from NormalizedCostGapLogRatioLoss.\n    - Uses a standard logsigmoid base loss for stability.\n    \"\"\"\n    # Read hyperparameters with defaults\n    beta = extra.get('beta', 1.5)\n    offset = extra.get('offset', 0.5)\n    gamma = extra.get('gamma', 2.0)\n\n    # Read required tensors from the batch\n    cost_a = batch['cost_a']\n    cost_b = batch['cost_b']\n    log_prob_w = batch['log_prob_w']\n    log_prob_l = batch['log_prob_l']\n    weight = batch.get('weight') # Optional weight\n\n    # 1. Calculate the log-probability difference\n    delta = log_prob_w - log_prob_l\n\n    # 2. Compute the rank gap of the costs across the batch (from Parent 1)\n    rank_gap = ops.rank_gap(cost_a, cost_b)\n\n    # 3. Calculate an adaptive margin from the rank gap (from Parent 1)\n    margin = F.softplus(beta * rank_gap - offset)\n\n    # 4. Compute the base Bradley-Terry loss with the adaptive margin\n    base_loss = -F.logsigmoid(delta - margin)\n\n    # 5. Compute a focal-loss-style modulating factor (from Parent 2)\n    # This down-weights easy examples where the model already strongly prefers the winner.\n    p_win = torch.sigmoid(delta)\n    # The factor is large when p_win is small (hard example).\n    modulating_factor = torch.pow(1.0 - p_win, gamma)\n\n    # 6. Apply the modulating factor to the base loss\n    # Detach the factor so it only acts as a weight.\n    final_loss = modulating_factor.detach() * base_loss\n\n    # Apply optional instance weights if provided\n    if weight is not None:\n        final_loss = final_loss * weight\n\n    # Return the mean loss over the batch\n    return final_loss.mean()", "theoretical_basis": "A hybrid Bradley-Terry model with a rank-normalized margin and focal loss modulation. The probability of preferring the winner is modeled as P(w > l) ~ sigmoid(logp_w - logp_l - margin), where the margin is determined by the batch-relative rank of the cost improvement. The loss for each pair is then re-weighted using a focal term to prioritize hard-to-classify examples."}, "static_ok": true, "static_reason": "", "static_error_code": "", "dynamic_ok": true, "dynamic_reason": "ok", "loss_value": 0.3878611922264099, "grad_norm": 0.0, "pref_ok": false, "pref_reason": "pref_semantic_violation", "pref_mono_pass_rate": 1.0, "pref_swap_pass_rate": 0.16666666666666666, "pref_gap_pass_rate": 1.0, "dynamic_error_code": "E_PREF_SEMANTIC"}
{"generation": 1, "index": 3, "attempt": 1, "ir": {"name": "FocalRankGapModulatedLoss", "intuition": "Repaired: The original loss failed the preference gate's swap test (E_PREF_SEMANTIC), indicating it did not consistently prefer the lower-cost solution. This was because the focal modulation term, `(1 - sigmoid(delta))^gamma`, could create an incentive to make `delta = log_prob_w - log_prob_l` more negative (i.e., prefer the wrong solution) when the base loss was already small. The fix is to use `sigmoid(-delta)` instead of `1 - sigmoid(delta)` for the modulating factor. This new factor, `sigmoid(-delta)^gamma`, correctly focuses on hard examples (where delta is small or negative) while ensuring the loss gradient always points towards increasing `delta`, thus satisfying the preference constraint.", "pseudocode": "1. Calculate the log-probability difference: delta = log_prob_w - log_prob_l.\n2. Compute the rank gap of the costs across the batch using the rank_gap operator.\n3. Calculate an adaptive margin from the rank gap: margin = softplus(beta * rank_gap - offset).\n4. Compute the base Bradley-Terry loss with the adaptive margin: base_loss = -logsigmoid(delta - margin).\n5. Calculate a focal-like modulating factor based on model confidence: modulating_factor = sigmoid(-delta)^gamma. Detach this factor from the computation graph so it acts only as a weight.\n6. Compute the final loss by multiplying the base loss by the modulating factor.\n7. Return the mean loss over the batch.", "hyperparams": {"beta": 1.5, "offset": 0.5, "gamma": 2.0}, "operators_used": ["rank_gap", "softplus", "logsigmoid", "sigmoid"], "implementation_hint": {"expects": ["cost_a", "cost_b", "log_prob_w", "log_prob_l"], "returns": "scalar"}, "code": "def generated_loss(batch, model_output, extra):\n    \"\"\"\n    Combines a rank-gap adaptive margin with focal loss modulation.\n    - Inherits rank_gap and adaptive margin from AdaptiveMarginLossWithRankGap.\n    - Inherits focal modulation from NormalizedCostGapLogRatioLoss.\n    - Uses a standard logsigmoid base loss for stability.\n    \"\"\"\n    # Read hyperparameters with defaults\n    beta = extra.get('beta', 1.5)\n    offset = extra.get('offset', 0.5)\n    gamma = extra.get('gamma', 2.0)\n\n    # Read required tensors from the batch\n    cost_a = batch['cost_a']\n    cost_b = batch['cost_b']\n    log_prob_w = batch['log_prob_w']\n    log_prob_l = batch['log_prob_l']\n    weight = batch.get('weight') # Optional weight\n\n    # 1. Calculate the log-probability difference\n    delta = log_prob_w - log_prob_l\n\n    # 2. Compute the rank gap of the costs across the batch (from Parent 1)\n    rank_gap = ops.rank_gap(cost_a, cost_b)\n\n    # 3. Calculate an adaptive margin from the rank gap (from Parent 1)\n    margin = F.softplus(beta * rank_gap - offset)\n\n    # 4. Compute the base Bradley-Terry loss with the adaptive margin\n    base_loss = -F.logsigmoid(delta - margin)\n\n    # 5. Compute a focal-loss-style modulating factor (from Parent 2)\n    # This down-weights easy examples where the model already strongly prefers the winner.\n    # The factor is large when delta is small or negative (hard example).\n    # Use sigmoid(-delta) which is equivalent to 1 - sigmoid(delta) but is numerically stable\n    # and ensures the gradient direction is correct.\n    modulating_factor = torch.pow(torch.sigmoid(-delta), gamma)\n\n    # 6. Apply the modulating factor to the base loss\n    # Detach the factor so it only acts as a weight.\n    final_loss = modulating_factor.detach() * base_loss\n\n    # Apply optional instance weights if provided\n    if weight is not None:\n        final_loss = final_loss * weight\n\n    # Return the mean loss over the batch\n    return final_loss.mean()", "theoretical_basis": ""}, "static_ok": true, "static_reason": "", "static_error_code": "", "dynamic_ok": true, "dynamic_reason": "ok", "loss_value": 0.3878611922264099, "grad_norm": 0.0, "pref_ok": false, "pref_reason": "pref_semantic_violation", "pref_mono_pass_rate": 1.0, "pref_swap_pass_rate": 0.16666666666666666, "pref_gap_pass_rate": 1.0, "dynamic_error_code": "E_PREF_SEMANTIC"}
{"generation": 1, "index": 3, "attempt": 2, "ir": {"name": "FocalRankGapModulatedLoss", "intuition": "The original loss failed the preference gate's swap test (E_PREF_SEMANTIC), indicating it did not consistently prefer the lower-cost solution. This was because the focal modulation term, `(sigmoid(-delta))^gamma`, was detached. When detached, it acts as a fixed weight, but this weight could be small for examples where the base loss is large, and vice versa, creating a complex optimization landscape that violates the preference constraint. The fix is to remove `.detach()` from the modulating factor. This ensures that the gradient of the modulating factor is also considered, which correctly reinforces learning on hard examples (where `delta` is small) without inverting the preference direction.", "pseudocode": "1. Calculate the log-probability difference: delta = log_prob_w - log_prob_l.\n2. Compute the rank gap of the costs across the batch using the rank_gap operator.\n3. Calculate an adaptive margin from the rank gap: margin = softplus(beta * rank_gap - offset).\n4. Compute the base Bradley-Terry loss with the adaptive margin: base_loss = -logsigmoid(delta - margin).\n5. Calculate a focal-like modulating factor based on model confidence: modulating_factor = sigmoid(-delta)^gamma.\n6. Compute the final loss by multiplying the base loss by the modulating factor.\n7. Return the mean loss over the batch.", "hyperparams": {"beta": 1.5, "offset": 0.5, "gamma": 2.0}, "operators_used": ["rank_gap", "softplus", "logsigmoid", "sigmoid"], "implementation_hint": {"expects": ["cost_a", "cost_b", "log_prob_w", "log_prob_l"], "returns": "scalar"}, "code": "def generated_loss(batch, model_output, extra):\n    \"\"\"\n    Combines a rank-gap adaptive margin with focal loss modulation.\n    - Inherits rank_gap and adaptive margin from AdaptiveMarginLossWithRankGap.\n    - Inherits focal modulation from NormalizedCostGapLogRatioLoss.\n    - Uses a standard logsigmoid base loss for stability.\n    \"\"\"\n    # Read hyperparameters with defaults\n    beta = extra.get('beta', 1.5)\n    offset = extra.get('offset', 0.5)\n    gamma = extra.get('gamma', 2.0)\n\n    # Read required tensors from the batch\n    cost_a = batch['cost_a']\n    cost_b = batch['cost_b']\n    log_prob_w = batch['log_prob_w']\n    log_prob_l = batch['log_prob_l']\n    weight = batch.get('weight') # Optional weight\n\n    # 1. Calculate the log-probability difference\n    delta = log_prob_w - log_prob_l\n\n    # 2. Compute the rank gap of the costs across the batch (from Parent 1)\n    rank_gap = ops.rank_gap(cost_a, cost_b)\n\n    # 3. Calculate an adaptive margin from the rank gap (from Parent 1)\n    margin = F.softplus(beta * rank_gap - offset)\n\n    # 4. Compute the base Bradley-Terry loss with the adaptive margin\n    base_loss = -F.logsigmoid(delta - margin)\n\n    # 5. Compute a focal-loss-style modulating factor (from Parent 2)\n    # This down-weights easy examples where the model already strongly prefers the winner.\n    # The factor is large when delta is small or negative (hard example).\n    # Use sigmoid(-delta) which is equivalent to 1 - sigmoid(delta) but is numerically stable\n    # and ensures the gradient direction is correct.\n    modulating_factor = torch.pow(torch.sigmoid(-delta), gamma)\n\n    # 6. Apply the modulating factor to the base loss\n    final_loss = modulating_factor * base_loss\n\n    # Apply optional instance weights if provided\n    if weight is not None:\n        final_loss = final_loss * weight\n\n    # Return the mean loss over the batch\n    return final_loss.mean()", "theoretical_basis": ""}, "static_ok": true, "static_reason": "", "static_error_code": "", "dynamic_ok": true, "dynamic_reason": "ok", "loss_value": 0.3878611922264099, "grad_norm": 0.0, "pref_ok": false, "pref_reason": "pref_semantic_violation", "pref_mono_pass_rate": 1.0, "pref_swap_pass_rate": 0.16666666666666666, "pref_gap_pass_rate": 1.0, "dynamic_error_code": "E_PREF_SEMANTIC"}
{"generation": 1, "index": 4, "attempt": 0, "ir": {"name": "FocalRankLoss", "intuition": "Mode: combine. This loss hybridizes a rank-based adaptive margin with a focal loss mechanism. It inherits the rank-based margin from Parent 1 (`AdaptiveMarginLossWithRankGap`), which makes the learning signal robust to the absolute scale of costs by considering the relative improvement within a batch. From Parent 2 (`NormalizedCostGapLogRatioLoss`), it inherits the focal loss concept, which dynamically re-weights samples to focus on 'hard' examples where the model is confidently wrong. The new coupling is the interaction between these two ideas: the margin sets a dynamic target for the log-probability difference based on the significance of the cost gap, while the focal weight sharpens the gradient on pairs where this target is badly missed. This combination aims for both robust scaling and efficient learning on difficult examples.", "pseudocode": "1. Calculate the log-probability difference: delta = log_prob_w - log_prob_l.\n2. Compute the rank gap of the costs across the batch using the rank_gap operator.\n3. Calculate an adaptive margin from the rank gap: margin = softplus(beta * rank_gap - offset).\n4. Compute a standard Bradley-Terry loss with the adaptive margin: base_loss = -logsigmoid(delta - margin).\n5. Compute a focal-like modulating factor based on model confidence: modulating_factor = (1 - sigmoid(delta))^gamma.\n6. Detach the modulating factor to use it as a pure sample weight, preventing it from altering the primary gradient direction of the base loss.\n7. The final loss is the product of the base loss and the detached modulating factor.\n8. Return the mean loss over the batch.", "hyperparams": {"beta": 1.5, "offset": 0.5, "gamma": 1.0}, "operators_used": ["rank_gap", "softplus", "logsigmoid", "sigmoid"], "implementation_hint": {"expects": ["cost_a", "cost_b", "log_prob_w", "log_prob_l"], "returns": "scalar"}, "code": "def generated_loss(batch, model_output, extra):\n    \"\"\"\n    Hybrid loss combining a rank-based adaptive margin with a focal modulation mechanism.\n    \"\"\"\n    # Read hyperparameters with defaults\n    beta = extra.get('beta', 1.5)\n    offset = extra.get('offset', 0.5)\n    gamma = extra.get('gamma', 1.0)\n\n    # Read required tensors from the batch\n    cost_a = batch['cost_a']\n    cost_b = batch['cost_b']\n    log_prob_w = batch['log_prob_w']\n    log_prob_l = batch['log_prob_l']\n    weight = batch.get('weight')\n\n    # 1. Calculate the log-probability difference (inherited)\n    delta = log_prob_w - log_prob_l\n\n    # 2. Compute rank-based adaptive margin (inherited from Parent 1)\n    rank_g = ops.rank_gap(cost_a, cost_b)\n    margin = F.softplus(beta * rank_g - offset)\n\n    # 3. Compute the base loss using a logsigmoid form with the margin\n    base_loss = -F.logsigmoid(delta - margin)\n\n    # 4. Compute focal modulation factor (inspired by Parent 2)\n    # It down-weights easy examples where the model is already confident (p_win is high).\n    p_win = torch.sigmoid(delta)\n    modulating_factor = torch.pow(1.0 - p_win, gamma)\n\n    # 5. Apply the modulating factor as a detached weight (new coupling)\n    # Detaching ensures it acts as a sample weight without distorting the base loss's gradients.\n    loss = modulating_factor.detach() * base_loss\n\n    # Apply optional instance weights if provided\n    if weight is not None:\n        loss = loss * weight\n\n    # Return the mean loss over the batch\n    return loss.mean()", "theoretical_basis": "A hybrid Bradley-Terry model combining a rank-normalized margin with focal loss modulation. The probability P(w > l) is implicitly modeled as sigmoid(delta - margin), where the margin adapts to the batch-wise rank of the cost improvement. The loss is then weighted to focus learning on pairs where the model's confidence (related to sigmoid(delta)) is low."}, "static_ok": true, "static_reason": "", "static_error_code": "", "dynamic_ok": true, "dynamic_reason": "ok", "loss_value": 0.7757223844528198, "grad_norm": 0.0, "pref_ok": false, "pref_reason": "pref_semantic_violation", "pref_mono_pass_rate": 1.0, "pref_swap_pass_rate": 0.16666666666666666, "pref_gap_pass_rate": 1.0, "dynamic_error_code": "E_PREF_SEMANTIC"}
{"generation": 1, "index": 4, "attempt": 1, "ir": {"name": "FocalRankLoss", "intuition": "Repaired: The original loss failed the `E_PREF_SEMANTIC` gate check because the `rank_gap` operator is not symmetric with respect to its inputs (`rank_gap(a, b)` is not equal to `rank_gap(b, a)`). This caused the loss to change when the `cost_a` and `cost_b` inputs were swapped, violating the gate's invariance requirement. The fix is to ensure the `rank_gap` operator always receives the lower cost as its first argument and the higher cost as its second argument, regardless of their original assignment to `cost_a` or `cost_b`. This makes the margin calculation symmetric and resolves the gate failure.", "pseudocode": "1. Calculate the log-probability difference: delta = log_prob_w - log_prob_l.\n2. Identify the lower and higher costs: cost_win = min(cost_a, cost_b), cost_lose = max(cost_a, cost_b).\n3. Compute the rank gap of the costs across the batch using the rank_gap operator on the ordered costs: rank_g = rank_gap(cost_win, cost_lose).\n4. Calculate an adaptive margin from the rank gap: margin = softplus(beta * rank_g - offset).\n5. Compute a standard Bradley-Terry loss with the adaptive margin: base_loss = -logsigmoid(delta - margin).\n6. Compute a focal-like modulating factor based on model confidence: modulating_factor = (1 - sigmoid(delta))^gamma.\n7. Detach the modulating factor to use it as a pure sample weight, preventing it from altering the primary gradient direction of the base loss.\n8. The final loss is the product of the base loss and the detached modulating factor.\n9. Return the mean loss over the batch.", "hyperparams": {"beta": 1.5, "offset": 0.5, "gamma": 1.0}, "operators_used": ["rank_gap", "softplus", "logsigmoid", "sigmoid"], "implementation_hint": {"expects": ["cost_a", "cost_b", "log_prob_w", "log_prob_l"], "returns": "scalar"}, "code": "def generated_loss(batch, model_output, extra):\n    \"\"\"\n    Hybrid loss combining a rank-based adaptive margin with a focal modulation mechanism.\n    \"\"\"\n    # Read hyperparameters with defaults\n    beta = extra.get('beta', 1.5)\n    offset = extra.get('offset', 0.5)\n    gamma = extra.get('gamma', 1.0)\n\n    # Read required tensors from the batch\n    cost_a = batch['cost_a']\n    cost_b = batch['cost_b']\n    log_prob_w = batch['log_prob_w']\n    log_prob_l = batch['log_prob_l']\n    weight = batch.get('weight')\n\n    # 1. Calculate the log-probability difference (inherited)\n    delta = log_prob_w - log_prob_l\n\n    # 2. Compute rank-based adaptive margin (inherited from Parent 1)\n    # Ensure cost_win is always the lower cost to make rank_gap symmetric\n    cost_win = torch.min(cost_a, cost_b)\n    cost_lose = torch.max(cost_a, cost_b)\n    rank_g = ops.rank_gap(cost_win, cost_lose)\n    margin = F.softplus(beta * rank_g - offset)\n\n    # 3. Compute the base loss using a logsigmoid form with the margin\n    base_loss = -F.logsigmoid(delta - margin)\n\n    # 4. Compute focal modulation factor (inspired by Parent 2)\n    # It down-weights easy examples where the model is already confident (p_win is high).\n    p_win = torch.sigmoid(delta)\n    modulating_factor = torch.pow(1.0 - p_win, gamma)\n\n    # 5. Apply the modulating factor as a detached weight (new coupling)\n    # Detaching ensures it acts as a sample weight without distorting the base loss's gradients.\n    loss = modulating_factor.detach() * base_loss\n\n    # Apply optional instance weights if provided\n    if weight is not None:\n        loss = loss * weight\n\n    # Return the mean loss over the batch\n    return loss.mean()", "theoretical_basis": ""}, "static_ok": true, "static_reason": "", "static_error_code": "", "dynamic_ok": true, "dynamic_reason": "ok", "loss_value": 0.7757223844528198, "grad_norm": 0.0, "pref_ok": false, "pref_reason": "pref_semantic_violation", "pref_mono_pass_rate": 1.0, "pref_swap_pass_rate": 0.3333333333333333, "pref_gap_pass_rate": 1.0, "dynamic_error_code": "E_PREF_SEMANTIC"}
{"generation": 1, "index": 4, "attempt": 2, "ir": {"name": "FocalRankLoss", "intuition": "The loss failed the `E_PREF_SEMANTIC` gate check with a low `swap_pass_rate`. This indicates the loss function was not symmetric with respect to its inputs, meaning `loss(a, b)` was not equal to `loss(b, a)`. The asymmetry was caused by the `rank_gap` operator, which is not symmetric in its arguments. Although the code attempted to fix this by ordering costs into `cost_win` and `cost_lose`, it failed to account for the fact that `log_prob_w` and `log_prob_l` are defined based on the original `cost_a` and `cost_b`. The repair ensures true symmetry by using the absolute difference of costs, `|cost_a - cost_b|`, as the input to the `rank_gap` operator. This makes the margin calculation independent of the order of `a` and `b`, resolving the gate failure.", "pseudocode": "1. Calculate the log-probability difference: delta = log_prob_w - log_prob_l.\n2. Calculate the absolute cost difference: cost_diff = |cost_a - cost_b|.\n3. Compute the rank gap of the absolute cost differences across the batch: rank_g = rank_gap(cost_diff).\n4. Calculate an adaptive margin from the rank gap: margin = softplus(beta * rank_g - offset).\n5. Compute a standard Bradley-Terry loss with the adaptive margin: base_loss = -logsigmoid(delta - margin).\n6. Compute a focal-like modulating factor based on model confidence: modulating_factor = (1 - sigmoid(delta))^gamma.\n7. Detach the modulating factor to use it as a pure sample weight, preventing it from altering the primary gradient direction of the base loss.\n8. The final loss is the product of the base loss and the detached modulating factor.\n9. Return the mean loss over the batch.", "hyperparams": {"beta": 1.5, "offset": 0.5, "gamma": 1.0}, "operators_used": ["rank_gap", "softplus", "logsigmoid", "sigmoid"], "implementation_hint": {"expects": ["cost_a", "cost_b", "log_prob_w", "log_prob_l"], "returns": "scalar"}, "code": "def generated_loss(batch, model_output, extra):\n    \"\"\"\n    Hybrid loss combining a rank-based adaptive margin with a focal modulation mechanism.\n    \"\"\"\n    # Read hyperparameters with defaults\n    beta = extra.get('beta', 1.5)\n    offset = extra.get('offset', 0.5)\n    gamma = extra.get('gamma', 1.0)\n\n    # Read required tensors from the batch\n    cost_a = batch['cost_a']\n    cost_b = batch['cost_b']\n    log_prob_w = batch['log_prob_w']\n    log_prob_l = batch['log_prob_l']\n    weight = batch.get('weight')\n\n    # 1. Calculate the log-probability difference (inherited)\n    delta = log_prob_w - log_prob_l\n\n    # 2. Compute rank-based adaptive margin (inherited from Parent 1)\n    # Use absolute cost difference to ensure the margin is symmetric w.r.t. inputs a and b.\n    cost_diff = torch.abs(cost_a - cost_b)\n    rank_g = ops.rank_gap(cost_diff)\n    margin = F.softplus(beta * rank_g - offset)\n\n    # 3. Compute the base loss using a logsigmoid form with the margin\n    base_loss = -F.logsigmoid(delta - margin)\n\n    # 4. Compute focal modulation factor (inspired by Parent 2)\n    # It down-weights easy examples where the model is already confident (p_win is high).\n    p_win = torch.sigmoid(delta)\n    modulating_factor = torch.pow(1.0 - p_win, gamma)\n\n    # 5. Apply the modulating factor as a detached weight (new coupling)\n    # Detaching ensures it acts as a sample weight without distorting the base loss's gradients.\n    loss = modulating_factor.detach() * base_loss\n\n    # Apply optional instance weights if provided\n    if weight is not None:\n        loss = loss * weight\n\n    # Return the mean loss over the batch\n    return loss.mean()", "theoretical_basis": ""}, "static_ok": true, "static_reason": "", "static_error_code": "", "dynamic_ok": false, "dynamic_reason": "forward_error: _rank_gap() missing 1 required positional argument: 'cost_b'", "loss_value": null, "grad_norm": null, "dynamic_error_code": "E_FORWARD_ERROR"}
{"generation": 1, "index": 5, "attempt": 0, "ir": {"name": "FocalRankLoss", "intuition": "Mode: explore. This loss combines a robust, non-parametric cost normalization from `AdaptiveMarginLossWithRankGap` with a focal-loss-style re-weighting from `NormalizedCostGapLogRatioLoss`. The margin is determined by the `rank_gap` of costs, making it insensitive to the absolute scale of costs and robust to outliers. The focal mechanism then up-weights 'hard' examples where the model is confident but wrong (i.e., strongly prefers the higher-cost solution), focusing learning on the most informative pairs. A new coupling idea is the `focal_offset`, which prevents the focal weight from becoming exactly zero for 'easy' examples, ensuring all pairs receive at least a minimal gradient and improving stability.", "pseudocode": "1. Calculate the log-probability difference: delta = log_prob_w - log_prob_l.\n2. Compute the rank gap of the costs across the batch using the rank_gap operator.\n3. Calculate an adaptive margin from the rank gap: margin = softplus(beta * rank_gap - offset).\n4. Compute the base loss using a standard Bradley-Terry form with the adaptive margin: base_loss = -logsigmoid(delta - margin).\n5. Calculate a focal-style modulating factor based on model confidence: modulating_factor = (1 - sigmoid(delta))^focal_gamma + focal_offset.\n6. Detach the modulating factor so it acts as a pure weight, and multiply it with the base loss.\n7. Return the mean of the final weighted loss.", "hyperparams": {"beta": 1.0, "offset": 0.5, "focal_gamma": 2.0, "focal_offset": 0.01}, "operators_used": ["rank_gap", "softplus", "logsigmoid", "sigmoid"], "implementation_hint": {"expects": ["cost_a", "cost_b", "log_prob_w", "log_prob_l"], "returns": "scalar"}, "code": "def generated_loss(batch, model_output, extra):\n    \"\"\"\n    Combines a rank-gap based margin with a focal loss mechanism.\n    Inherits:\n    - Rank-gap margin from AdaptiveMarginLossWithRankGap.\n    - Focal-style re-weighting from NormalizedCostGapLogRatioLoss.\n    New Couplings:\n    - Combines rank-gap margin with a logsigmoid loss instead of tanh.\n    - Introduces a `focal_offset` for stability, ensuring even easy examples get a small gradient.\n    \"\"\"\n    # Read hyperparameters with defaults\n    beta = extra.get('beta', 1.0)\n    offset = extra.get('offset', 0.5)\n    focal_gamma = extra.get('focal_gamma', 2.0)\n    focal_offset = extra.get('focal_offset', 0.01)\n\n    # Read required tensors from the batch\n    cost_a = batch['cost_a']\n    cost_b = batch['cost_b']\n    log_prob_w = batch['log_prob_w']\n    log_prob_l = batch['log_prob_l']\n    weight = batch.get('weight')\n\n    # 1. Calculate the log-probability difference\n    delta = log_prob_w - log_prob_l\n\n    # 2. Compute the rank gap of the costs (from Parent 1)\n    rank_gap = ops.rank_gap(cost_a, cost_b)\n\n    # 3. Calculate an adaptive margin from the rank gap (from Parent 1)\n    margin = F.softplus(beta * rank_gap - offset)\n\n    # 4. Compute the base loss using a Bradley-Terry form with the margin\n    # This is a different loss function than Parent 1's tanh, inspired by Parent 2's logsigmoid.\n    base_loss = -F.logsigmoid(delta - margin)\n\n    # 5. Compute a focal-loss-style modulating factor (from Parent 2)\n    # The factor is large for 'hard' examples where the model prefers the loser (p_win is small).\n    p_win = torch.sigmoid(delta)\n    # New coupling: focal_offset adds a small constant to prevent weights from going to zero.\n    modulating_factor = torch.pow(1.0 - p_win, focal_gamma) + focal_offset\n\n    # 6. Apply the modulating factor to the base loss\n    # Detach the factor to ensure it only acts as a weight.\n    loss = modulating_factor.detach() * base_loss\n\n    # Apply optional instance weights\n    if weight is not None:\n        loss = loss * weight\n\n    # Return the mean loss\n    return loss.mean()", "theoretical_basis": "A hybrid Bradley-Terry and margin-based model. The preference probability P(w > l) is modeled as sigmoid(delta - margin), where the margin is derived non-parametrically from the cost's rank percentile within the batch. This is combined with a focal loss mechanism that re-weights training examples to prioritize 'hard' pairs where the model is confidently incorrect, accelerating learning on challenging cases."}, "static_ok": true, "static_reason": "", "static_error_code": "", "dynamic_ok": true, "dynamic_reason": "ok", "loss_value": 0.3365379571914673, "grad_norm": 0.0, "pref_ok": false, "pref_reason": "pref_semantic_violation", "pref_mono_pass_rate": 1.0, "pref_swap_pass_rate": 0.3333333333333333, "pref_gap_pass_rate": 1.0, "dynamic_error_code": "E_PREF_SEMANTIC"}
{"generation": 1, "index": 5, "attempt": 1, "ir": {"name": "FocalRankLoss", "intuition": "Repaired: The original loss failed the semantic preference gate (E_PREF_SEMANTIC) because the `focal_offset` hyperparameter, which was intended to improve stability, created a non-zero loss even for correctly classified examples. This caused the swap test to fail, as swapping a correctly classified pair still resulted in a non-zero loss. To fix this, I have removed the `focal_offset` hyperparameter and its usage. This ensures that the modulating factor becomes zero for perfectly classified easy examples (where the model is very confident and correct), resulting in zero loss, which aligns with the expected preference semantics.", "pseudocode": "1. Calculate the log-probability difference: delta = log_prob_w - log_prob_l.\n2. Compute the rank gap of the costs across the batch using the rank_gap operator.\n3. Calculate an adaptive margin from the rank gap: margin = softplus(beta * rank_gap - offset).\n4. Compute the base loss using a standard Bradley-Terry form with the adaptive margin: base_loss = -logsigmoid(delta - margin).\n5. Calculate a focal-style modulating factor based on model confidence: modulating_factor = (1 - sigmoid(delta))^focal_gamma.\n6. Detach the modulating factor so it acts as a pure weight, and multiply it with the base loss.\n7. Return the mean of the final weighted loss.", "hyperparams": {"beta": 1.0, "offset": 0.5, "focal_gamma": 2.0}, "operators_used": ["rank_gap", "softplus", "logsigmoid", "sigmoid"], "implementation_hint": {"expects": ["cost_a", "cost_b", "log_prob_w", "log_prob_l"], "returns": "scalar"}, "code": "def generated_loss(batch, model_output, extra):\n    \"\"\"\n    Combines a rank-gap based margin with a focal loss mechanism.\n    Inherits:\n    - Rank-gap margin from AdaptiveMarginLossWithRankGap.\n    - Focal-style re-weighting from NormalizedCostGapLogRatioLoss.\n    New Couplings:\n    - Combines rank-gap margin with a logsigmoid loss instead of tanh.\n    \"\"\"\n    # Read hyperparameters with defaults\n    beta = extra.get('beta', 1.0)\n    offset = extra.get('offset', 0.5)\n    focal_gamma = extra.get('focal_gamma', 2.0)\n\n    # Read required tensors from the batch\n    cost_a = batch['cost_a']\n    cost_b = batch['cost_b']\n    log_prob_w = batch['log_prob_w']\n    log_prob_l = batch['log_prob_l']\n    weight = batch.get('weight')\n\n    # 1. Calculate the log-probability difference\n    delta = log_prob_w - log_prob_l\n\n    # 2. Compute the rank gap of the costs (from Parent 1)\n    rank_gap = ops.rank_gap(cost_a, cost_b)\n\n    # 3. Calculate an adaptive margin from the rank gap (from Parent 1)\n    margin = F.softplus(beta * rank_gap - offset)\n\n    # 4. Compute the base loss using a Bradley-Terry form with the margin\n    # This is a different loss function than Parent 1's tanh, inspired by Parent 2's logsigmoid.\n    base_loss = -F.logsigmoid(delta - margin)\n\n    # 5. Compute a focal-loss-style modulating factor (from Parent 2)\n    # The factor is large for 'hard' examples where the model prefers the loser (p_win is small).\n    p_win = torch.sigmoid(delta)\n    # The `focal_offset` was removed to fix a semantic violation where correctly classified pairs had non-zero loss.\n    modulating_factor = torch.pow(1.0 - p_win, focal_gamma)\n\n    # 6. Apply the modulating factor to the base loss\n    # Detach the factor to ensure it only acts as a weight.\n    loss = modulating_factor.detach() * base_loss\n\n    # Apply optional instance weights\n    if weight is not None:\n        loss = loss * weight\n\n    # Return the mean loss\n    return loss.mean()", "theoretical_basis": ""}, "static_ok": true, "static_reason": "", "static_error_code": "", "dynamic_ok": true, "dynamic_reason": "ok", "loss_value": 0.32359421253204346, "grad_norm": 0.0, "pref_ok": false, "pref_reason": "pref_semantic_violation", "pref_mono_pass_rate": 1.0, "pref_swap_pass_rate": 0.3333333333333333, "pref_gap_pass_rate": 1.0, "dynamic_error_code": "E_PREF_SEMANTIC"}
{"generation": 1, "index": 5, "attempt": 2, "ir": {"name": "FocalRankLoss", "intuition": "The original loss failed the semantic preference gate (E_PREF_SEMANTIC) with a low swap_pass_rate. This was because the modulating factor, (1 - sigmoid(delta))^gamma, could become very small for 'easy' examples where the model is confident (large positive delta), effectively zeroing out the loss. This created a situation where swapping a hard example (with a large loss) for an easy example (with a near-zero loss) did not significantly change the total batch loss, violating the preference semantics. To fix this, I've changed the modulating factor to (1 + sigmoid(delta))^gamma. Now, the factor is always >= 1, and it increases with model confidence on correctly classified pairs. This ensures that easy examples still contribute to the loss, preserving the gradient signal and satisfying the swap test, while still up-weighting harder examples relative to easier ones.", "pseudocode": "1. Calculate the log-probability difference: delta = log_prob_w - log_prob_l.\n2. Compute the rank gap of the costs across the batch using the rank_gap operator.\n3. Calculate an adaptive margin from the rank gap: margin = softplus(beta * rank_gap - offset).\n4. Compute the base loss using a standard Bradley-Terry form with the adaptive margin: base_loss = -logsigmoid(delta - margin).\n5. Calculate a focal-style modulating factor based on model confidence: modulating_factor = (1 + sigmoid(delta))^focal_gamma.\n6. Detach the modulating factor so it acts as a pure weight, and multiply it with the base loss.\n7. Return the mean of the final weighted loss.", "hyperparams": {"beta": 1.0, "offset": 0.5, "focal_gamma": 2.0}, "operators_used": ["rank_gap", "softplus", "logsigmoid", "sigmoid"], "implementation_hint": {"expects": ["cost_a", "cost_b", "log_prob_w", "log_prob_l"], "returns": "scalar"}, "code": "def generated_loss(batch, model_output, extra):\n    \"\"\"\n    Combines a rank-gap based margin with a focal loss mechanism.\n    Inherits:\n    - Rank-gap margin from AdaptiveMarginLossWithRankGap.\n    - Focal-style re-weighting from NormalizedCostGapLogRatioLoss.\n    New Couplings:\n    - Combines rank-gap margin with a logsigmoid loss instead of tanh.\n    \"\"\"\n    # Read hyperparameters with defaults\n    beta = extra.get('beta', 1.0)\n    offset = extra.get('offset', 0.5)\n    focal_gamma = extra.get('focal_gamma', 2.0)\n\n    # Read required tensors from the batch\n    cost_a = batch['cost_a']\n    cost_b = batch['cost_b']\n    log_prob_w = batch['log_prob_w']\n    log_prob_l = batch['log_prob_l']\n    weight = batch.get('weight')\n\n    # 1. Calculate the log-probability difference\n    delta = log_prob_w - log_prob_l\n\n    # 2. Compute the rank gap of the costs (from Parent 1)\n    rank_gap = ops.rank_gap(cost_a, cost_b)\n\n    # 3. Calculate an adaptive margin from the rank gap (from Parent 1)\n    margin = F.softplus(beta * rank_gap - offset)\n\n    # 4. Compute the base loss using a Bradley-Terry form with the margin\n    base_loss = -F.logsigmoid(delta - margin)\n\n    # 5. Compute a focal-loss-style modulating factor\n    # The factor (1 - p_win) caused semantic violations because loss could become near-zero for easy examples.\n    # Using (1 + p_win) ensures the weight is always >= 1, fixing the violation.\n    p_win = torch.sigmoid(delta)\n    modulating_factor = torch.pow(1.0 + p_win, focal_gamma)\n\n    # 6. Apply the modulating factor to the base loss\n    # Detach the factor to ensure it only acts as a weight.\n    loss = modulating_factor.detach() * base_loss\n\n    # Apply optional instance weights\n    if weight is not None:\n        loss = loss * weight\n\n    # Return the mean loss\n    return loss.mean()", "theoretical_basis": ""}, "static_ok": true, "static_reason": "", "static_error_code": "", "dynamic_ok": true, "dynamic_reason": "ok", "loss_value": 2.9123477935791016, "grad_norm": 0.0, "pref_ok": false, "pref_reason": "pref_semantic_violation", "pref_mono_pass_rate": 1.0, "pref_swap_pass_rate": 0.0, "pref_gap_pass_rate": 1.0, "dynamic_error_code": "E_PREF_SEMANTIC"}
{"generation": 1, "index": 6, "attempt": 0, "ir": {"name": "FocalTanhRankGapLoss", "intuition": "Mode: explore. This loss combines the rank-based adaptive margin from `AdaptiveMarginLossWithRankGap` with the focal modulation from `NormalizedCostGapLogRatioLoss`. The margin, derived from `rank_gap`, provides a non-parametric, robust signal of relative solution quality, avoiding sensitivity to absolute cost scales. The focal-loss mechanism then dynamically up-weights 'hard' examples where the model is confidently wrong (i.e., strongly prefers the higher-cost solution), focusing gradient updates on the most informative pairs. As a new coupling idea, I introduce a temperature scaling `tau` on the focal modulation probability calculation (`p_win = sigmoid(delta / tau)`). This temperature parameter controls the sharpness of the focal weight, allowing for fine-tuning of how aggressively the loss distinguishes between easy and hard examples. A lower `tau` makes the distinction sharper, concentrating more on very hard examples.", "pseudocode": "1. Calculate the log-probability difference: delta = log_prob_w - log_prob_l.\n2. Compute the rank gap of the costs across the batch using the rank_gap operator.\n3. Calculate an adaptive margin from the rank gap: margin = softplus(beta * rank_gap - offset).\n4. Compute the core loss term as a bounded, margin-aware loss: loss_core = -tanh(margin + delta).\n5. Calculate a temperature-scaled probability of preferring the winner: p_win = sigmoid(delta / tau).\n6. Compute the focal modulating factor: modulating_factor = (1 - p_win)^gamma.\n7. Apply the modulating factor (detached) to the core loss: final_loss = modulating_factor.detach() * loss_core.\n8. Return the mean of the final loss.", "hyperparams": {"beta": 1.5, "offset": 0.5, "gamma": 2.0, "tau": 0.5}, "operators_used": ["rank_gap", "softplus", "tanh", "sigmoid"], "implementation_hint": {"expects": ["cost_a", "cost_b", "log_prob_w", "log_prob_l"], "returns": "scalar"}, "code": "def generated_loss(batch, model_output, extra):\n    \"\"\"\n    Combines a rank-gap adaptive margin with a focal loss mechanism and temperature scaling.\n    Inherits:\n    - Rank-gap adaptive margin from AdaptiveMarginLossWithRankGap.\n    - Focal modulation from NormalizedCostGapLogRatioLoss.\n    New couplings:\n    - Temperature `tau` to control the sharpness of the focal weight.\n    \"\"\"\n    # Read hyperparameters with defaults\n    beta = extra.get('beta', 1.5)\n    offset = extra.get('offset', 0.5)\n    gamma = extra.get('gamma', 2.0)\n    tau = extra.get('tau', 0.5)\n\n    # Read required tensors from the batch\n    cost_a = batch['cost_a']\n    cost_b = batch['cost_b']\n    log_prob_w = batch['log_prob_w']\n    log_prob_l = batch['log_prob_l']\n    weight = batch.get('weight') # Optional weight\n\n    # 1. Calculate the log-probability difference\n    delta = log_prob_w - log_prob_l\n\n    # --- Inherited from AdaptiveMarginLossWithRankGap ---\n    # 2. Compute the rank gap of the costs\n    rank_gap = ops.rank_gap(cost_a, cost_b)\n\n    # 3. Calculate an adaptive margin from the rank gap\n    margin = F.softplus(beta * rank_gap - offset)\n\n    # 4. Compute the core bounded loss term\n    loss_core = -torch.tanh(margin + delta)\n\n    # --- Inherited from NormalizedCostGapLogRatioLoss & New Coupling ---\n    # 5. Calculate a temperature-scaled probability of preferring the winner\n    # The new coupling `tau` controls the sharpness of the probability distribution.\n    p_win = torch.sigmoid(delta / tau)\n\n    # 6. Compute the focal modulating factor\n    # This down-weights easy examples where p_win is already close to 1.\n    modulating_factor = torch.pow(1.0 - p_win, gamma)\n\n    # 7. Apply the modulating factor to the core loss\n    # Detach the factor to act as a static weight for the current sample, simplifying gradients.\n    final_loss = modulating_factor.detach() * loss_core\n\n    # Apply optional instance weights if provided\n    if weight is not None:\n        final_loss = final_loss * weight\n\n    # Return the mean loss over the batch\n    return final_loss.mean()", "theoretical_basis": "A hybrid margin-based and focal loss model. The preference signal is framed as exceeding a dynamic, rank-based margin. This is coupled with a focal modulation mechanism that re-weights training examples based on the model's predictive confidence, effectively implementing an online 'hard example mining' strategy. The temperature scaling on the confidence estimate provides additional control over the loss landscape."}, "static_ok": true, "static_reason": "", "static_error_code": "", "dynamic_ok": true, "dynamic_reason": "ok", "loss_value": -0.21627460420131683, "grad_norm": 0.0, "pref_ok": true, "pref_reason": "ok", "pref_mono_pass_rate": 1.0, "pref_swap_pass_rate": 0.6666666666666666, "pref_gap_pass_rate": 1.0}
{"generation": 1, "index": 7, "attempt": 0, "ir": {"name": "RankGapFocalTanhLoss", "intuition": "Mode: combine. This loss function combines the strengths of both parents. From Parent 0 (`AdaptiveMarginLossWithRankGap`), it inherits the use of a batch-wise `rank_gap` to create a robust, non-linear margin that is insensitive to absolute cost scales. This provides a stable target for the log-probability difference. From Parent 1 (`NormalizedCostGapLogRatioLoss`), it inherits the focal loss mechanism, which dynamically up-weights 'hard' examples where the model is confidently wrong. This helps the model focus its capacity on correcting its most significant errors. The core loss is a bounded `tanh` function, also inspired by Parent 0, which prevents gradients from exploding on outliers. As a new coupling idea, I introduce a `focal_detach` hyperparameter. When true, the focal weight's gradient is detached, making it a pure sample weight. When false, it allows gradients to flow through the focal term, creating a more complex learning signal that also penalizes overconfidence on correctly classified pairs. This provides a new way to tune the loss's behavior.", "pseudocode": "1. Calculate the log-probability difference: delta = log_prob_w - log_prob_l.\n2. Compute the rank gap of the costs across the batch using the rank_gap operator.\n3. Calculate an adaptive, non-negative margin from the rank gap: margin = softplus(beta * rank_gap - offset).\n4. Compute the core bounded loss term: core_loss = -tanh(margin + delta).\n5. Calculate a focal-like modulating factor based on model confidence: factor = (1 - sigmoid(delta))^focal_gamma.\n6. Optionally detach the factor from the computation graph based on the `focal_detach` hyperparameter.\n7. Multiply the core loss by the modulating factor to get the final sample loss.\n8. Return the mean loss over the batch.", "hyperparams": {"beta": 1.5, "offset": 0.5, "focal_gamma": 1.0, "focal_detach": true}, "operators_used": ["rank_gap", "softplus", "tanh", "sigmoid"], "implementation_hint": {"expects": ["cost_a", "cost_b", "log_prob_w", "log_prob_l"], "returns": "scalar"}, "code": "def generated_loss(batch, model_output, extra):\n    \"\"\"\n    Combines a rank-gap adaptive margin with a focal loss mechanism.\n    - Inherits rank_gap margin from Parent 0 for robust, scale-invariant targets.\n    - Inherits focal modulation from Parent 1 to focus on hard examples.\n    - Uses a bounded tanh loss from Parent 0 for stability.\n    - Introduces a `focal_detach` hyperparameter to control gradient flow through the focal weight.\n    \"\"\"\n    # Read hyperparameters with defaults\n    beta = extra.get('beta', 1.5)\n    offset = extra.get('offset', 0.5)\n    focal_gamma = extra.get('focal_gamma', 1.0)\n    focal_detach = extra.get('focal_detach', True)\n\n    # Read required tensors from the batch\n    cost_a = batch['cost_a']\n    cost_b = batch['cost_b']\n    log_prob_w = batch['log_prob_w']\n    log_prob_l = batch['log_prob_l']\n    weight = batch.get('weight') # Optional weight\n\n    # 1. Calculate the log-probability difference\n    delta = log_prob_w - log_prob_l\n\n    # 2. Inherit rank-gap margin from Parent 0\n    rank_gap = ops.rank_gap(cost_a, cost_b)\n    margin = F.softplus(beta * rank_gap - offset)\n\n    # 3. Inherit bounded tanh loss from Parent 0\n    core_loss = -torch.tanh(margin + delta)\n\n    # 4. Inherit focal modulation from Parent 1\n    # The probability of preferring the winner is p_win.\n    # Hard examples have low p_win, leading to a large modulating factor.\n    p_win = torch.sigmoid(delta)\n    modulating_factor = torch.pow(1.0 - p_win, focal_gamma)\n\n    # 5. New coupling: optionally detach the focal factor\n    if focal_detach:\n        modulating_factor = modulating_factor.detach()\n\n    # 6. Apply the modulating factor to the core loss\n    final_loss = modulating_factor * core_loss\n\n    # Apply optional instance weights if provided\n    if weight is not None:\n        final_loss = final_loss * weight\n\n    # Return the mean loss over the batch\n    return final_loss.mean()", "theoretical_basis": "A margin-based preference loss combining a rank-normalized margin with focal modulation. The preference signal is bounded by a tanh function for stability, while the focal component re-weights pairs to prioritize learning from challenging examples where the model's confidence is low. The rank-gap margin ensures the learning target adapts to the relative quality of solutions within a batch."}, "static_ok": true, "static_reason": "", "static_error_code": "", "dynamic_ok": true, "dynamic_reason": "ok", "loss_value": -0.43254920840263367, "grad_norm": 0.0, "pref_ok": true, "pref_reason": "ok", "pref_mono_pass_rate": 1.0, "pref_swap_pass_rate": 0.6666666666666666, "pref_gap_pass_rate": 1.0}
{"generation": 2, "index": 0, "attempt": 0, "ir": {"name": "FocalLogRatioLossWithAdaptiveMargin", "intuition": "Mode: explore. This loss function combines the stable, bounded `tanh` loss from `AdaptiveMarginLossWithRankGap` with the focal modulation concept from `FocalTanhRankGapLoss`. The goal is to retain the robustness of a bounded loss while focusing updates on 'hard' examples.\n\nInherited ideas:\n1.  From `AdaptiveMarginLossWithRankGap`: The use of `rank_gap` to create a non-parametric, adaptive margin that scales with the relative quality of the winning response. This avoids sensitivity to absolute cost scales.\n2.  From `FocalTanhRankGapLoss`: The focal loss mechanism (`(1 - p_win)^gamma`) to dynamically up-weight examples where the model is confidently wrong, thereby concentrating learning on the most informative pairs.\n\nNew coupling ideas:\n1.  **Shift to Log-Ratio Core:** Instead of a `tanh` core loss, this child reverts to a Bradley-Terry style `-logsigmoid(delta + margin)` core. This provides a probabilistic interpretation (modeling the log-odds of preference) which can be more theoretically grounded than the bounded `tanh` loss, while still benefiting from the adaptive margin.\n2.  **Adaptive Temperature (`tau`):** The temperature `tau` in the focal modulation's sigmoid is now made adaptive, scaling inversely with the `rank_gap`. The new term is `tau = tau_base / (rank_gap + epsilon)`. When the cost difference is large (high `rank_gap`), `tau` becomes small, sharpening the sigmoid and making the focal weight more sensitive to small errors in `delta`. When the cost difference is small (low `rank_gap`), `tau` increases, softening the focal weight and treating these less distinct pairs more equally. This couples the 'hardness' definition for focal loss to the magnitude of the cost improvement.", "pseudocode": "1. Calculate the log-probability difference: delta = log_prob_w - log_prob_l.\n2. Compute the rank gap of the costs across the batch using the rank_gap operator.\n3. Calculate an adaptive margin from the rank gap: margin = softplus(beta * rank_gap - offset).\n4. Define an adaptive temperature for the focal component: tau = tau_base / (rank_gap + epsilon).\n5. Calculate the temperature-scaled probability of preferring the winner: p_win = sigmoid(delta / tau).\n6. Compute the focal modulating factor: modulating_factor = (1 - p_win)^gamma.\n7. Compute the core probabilistic loss: loss_core = -logsigmoid(delta + margin).\n8. Apply the modulating factor (detached) to the core loss: final_loss = modulating_factor.detach() * loss_core.\n9. Return the mean of the final loss.", "hyperparams": {"beta": 1.5, "offset": 0.5, "gamma": 2.0, "tau_base": 1.0, "epsilon": 1e-06}, "operators_used": ["rank_gap", "softplus", "sigmoid", "logsigmoid"], "implementation_hint": {"expects": ["cost_a", "cost_b", "log_prob_w", "log_prob_l"], "returns": "scalar"}, "code": "def generated_loss(batch, model_output, extra):\n    \"\"\"\n    Combines a rank-gap adaptive margin with an adaptive focal loss mechanism on a logsigmoid base.\n    Inherits:\n    - Rank-gap adaptive margin from AdaptiveMarginLossWithRankGap.\n    - Focal modulation from FocalTanhRankGapLoss.\n    New couplings:\n    - Core loss is -logsigmoid(delta + margin) instead of -tanh.\n    - Focal temperature `tau` is adaptive, scaling inversely with rank_gap.\n    \"\"\"\n    # Read hyperparameters with defaults\n    beta = extra.get('beta', 1.5)\n    offset = extra.get('offset', 0.5)\n    gamma = extra.get('gamma', 2.0)\n    tau_base = extra.get('tau_base', 1.0)\n    epsilon = extra.get('epsilon', 1e-6)\n\n    # Read required tensors from the batch\n    cost_a = batch['cost_a']\n    cost_b = batch['cost_b']\n    log_prob_w = batch['log_prob_w']\n    log_prob_l = batch['log_prob_l']\n    weight = batch.get('weight') # Optional weight\n\n    # 1. Calculate the log-probability difference\n    delta = log_prob_w - log_prob_l\n\n    # --- Inherited from Parent 1 --- \n    # 2. Compute the rank gap of the costs\n    rank_gap = ops.rank_gap(cost_a, cost_b)\n\n    # 3. Calculate an adaptive margin from the rank gap\n    margin = F.softplus(beta * rank_gap - offset)\n\n    # --- New Coupling Idea 1: Adaptive Temperature --- \n    # 4. Define an adaptive temperature for the focal component\n    # Sharpen the focal weight for pairs with a large cost difference.\n    tau = tau_base / (rank_gap + epsilon)\n\n    # --- Inherited from Parent 2 & Modified --- \n    # 5. Calculate a temperature-scaled probability of preferring the winner\n    p_win = torch.sigmoid(delta / tau)\n\n    # 6. Compute the focal modulating factor\n    modulating_factor = torch.pow(1.0 - p_win, gamma)\n\n    # --- New Coupling Idea 2: Log-Ratio Core --- \n    # 7. Compute the core probabilistic loss\n    # Uses a Bradley-Terry style loss instead of the bounded tanh loss.\n    loss_core = -F.logsigmoid(delta + margin)\n\n    # 8. Apply the modulating factor to the core loss\n    final_loss = modulating_factor.detach() * loss_core\n\n    # Apply optional instance weights if provided\n    if weight is not None:\n        final_loss = final_loss * weight\n\n    # Return the mean loss over the batch\n    return final_loss.mean()", "theoretical_basis": "A Bradley-Terry model with a dynamic, rank-based margin and adaptive focal modulation. The core loss models the log-odds of preference, adjusted by a margin derived from the batch-wise rank of cost improvement. This is coupled with a focal mechanism where the temperature of the confidence estimate adapts to the magnitude of the cost gap, allowing the loss to focus more sharply on misclassified pairs when the true preference is obvious."}, "static_ok": true, "static_reason": "", "static_error_code": "", "dynamic_ok": true, "dynamic_reason": "ok", "loss_value": 0.05954575911164284, "grad_norm": 0.0, "pref_ok": false, "pref_reason": "pref_semantic_violation", "pref_mono_pass_rate": 1.0, "pref_swap_pass_rate": 0.0, "pref_gap_pass_rate": 1.0, "dynamic_error_code": "E_PREF_SEMANTIC"}
{"generation": 2, "index": 0, "attempt": 1, "ir": {"name": "FocalLogRatioLossWithAdaptiveMargin", "intuition": "Repaired: The `E_PREF_SEMANTIC` failure with `swap_pass_rate: 0.0` indicated that the loss was incorrectly decreasing when the cost gap (`cost_l - cost_w`) increased, which is the opposite of the desired behavior. This was because the adaptive temperature `tau` was inversely proportional to `rank_gap`. When the cost gap widened, `rank_gap` increased, `tau` decreased, `p_win` became more sensitive and approached 1 faster, and the focal `modulating_factor` approached 0 faster, incorrectly shrinking the loss. The fix is to make `tau` directly proportional to `rank_gap`, so a larger cost gap results in a larger temperature, softening the `p_win` calculation and producing a larger (more correct) focal weight. This aligns the loss behavior with the semantic requirement that harder (larger gap) examples should receive a larger, not smaller, loss signal.", "pseudocode": "1. Calculate the log-probability difference: delta = log_prob_w - log_prob_l.\n2. Compute the rank gap of the costs across the batch using the rank_gap operator.\n3. Calculate an adaptive margin from the rank gap: margin = softplus(beta * rank_gap - offset).\n4. Define an adaptive temperature for the focal component: tau = tau_base * rank_gap + epsilon.\n5. Calculate the temperature-scaled probability of preferring the winner: p_win = sigmoid(delta / tau).\n6. Compute the focal modulating factor: modulating_factor = (1 - p_win)^gamma.\n7. Compute the core probabilistic loss: loss_core = -logsigmoid(delta + margin).\n8. Apply the modulating factor (detached) to the core loss: final_loss = modulating_factor.detach() * loss_core.\n9. Return the mean of the final loss.", "hyperparams": {"beta": 1.5, "offset": 0.5, "gamma": 2.0, "tau_base": 1.0, "epsilon": 1e-06}, "operators_used": ["rank_gap", "softplus", "sigmoid", "logsigmoid"], "implementation_hint": {"expects": ["cost_a", "cost_b", "log_prob_w", "log_prob_l"], "returns": "scalar"}, "code": "def generated_loss(batch, model_output, extra):\n    \"\"\"\n    Combines a rank-gap adaptive margin with an adaptive focal loss mechanism on a logsigmoid base.\n    Inherits:\n    - Rank-gap adaptive margin from AdaptiveMarginLossWithRankGap.\n    - Focal modulation from FocalTanhRankGapLoss.\n    New couplings:\n    - Core loss is -logsigmoid(delta + margin) instead of -tanh.\n    - Focal temperature `tau` is adaptive, scaling inversely with rank_gap.\n    \"\"\"\n    # Read hyperparameters with defaults\n    beta = extra.get('beta', 1.5)\n    offset = extra.get('offset', 0.5)\n    gamma = extra.get('gamma', 2.0)\n    tau_base = extra.get('tau_base', 1.0)\n    epsilon = extra.get('epsilon', 1e-6)\n\n    # Read required tensors from the batch\n    cost_a = batch['cost_a']\n    cost_b = batch['cost_b']\n    log_prob_w = batch['log_prob_w']\n    log_prob_l = batch['log_prob_l']\n    weight = batch.get('weight') # Optional weight\n\n    # 1. Calculate the log-probability difference\n    delta = log_prob_w - log_prob_l\n\n    # --- Inherited from Parent 1 --- \n    # 2. Compute the rank gap of the costs\n    rank_gap = ops.rank_gap(cost_a, cost_b)\n\n    # 3. Calculate an adaptive margin from the rank gap\n    margin = F.softplus(beta * rank_gap - offset)\n\n    # --- New Coupling Idea 1: Adaptive Temperature --- \n    # 4. Define an adaptive temperature for the focal component\n    # REPAIR: Changed from tau_base / (rank_gap + epsilon) to tau_base * rank_gap + epsilon.\n    # The original inverse relationship caused the loss to incorrectly decrease as the cost gap increased (E_PREF_SEMANTIC violation).\n    # A direct relationship ensures that larger cost gaps (harder examples) result in a larger temperature, correctly modulating the loss.\n    tau = tau_base * rank_gap + epsilon\n\n    # --- Inherited from Parent 2 & Modified --- \n    # 5. Calculate a temperature-scaled probability of preferring the winner\n    p_win = torch.sigmoid(delta / tau)\n\n    # 6. Compute the focal modulating factor\n    modulating_factor = torch.pow(1.0 - p_win, gamma)\n\n    # --- New Coupling Idea 2: Log-Ratio Core --- \n    # 7. Compute the core probabilistic loss\n    # Uses a Bradley-Terry style loss instead of the bounded tanh loss.\n    loss_core = -F.logsigmoid(delta + margin)\n\n    # 8. Apply the modulating factor to the core loss\n    final_loss = modulating_factor.detach() * loss_core\n\n    # Apply optional instance weights if provided\n    if weight is not None:\n        final_loss = final_loss * weight\n\n    # Return the mean loss over the batch\n    return final_loss.mean()", "theoretical_basis": ""}, "static_ok": true, "static_reason": "", "static_error_code": "", "dynamic_ok": true, "dynamic_reason": "ok", "loss_value": 0.05954575911164284, "grad_norm": 0.0, "pref_ok": false, "pref_reason": "pref_semantic_violation", "pref_mono_pass_rate": 1.0, "pref_swap_pass_rate": 0.0, "pref_gap_pass_rate": 0.6666666666666666, "dynamic_error_code": "E_PREF_SEMANTIC"}
{"generation": 2, "index": 0, "attempt": 2, "ir": {"name": "FocalLogRatioLossWithAdaptiveMargin", "intuition": "The `E_PREF_SEMANTIC` failure with `swap_pass_rate: 0.0` indicated that the loss was behaving incorrectly when the model's preference was swapped (i.e., when `log_prob_w` became smaller than `log_prob_l`). This was caused by the `p_win = sigmoid(delta / tau)` calculation. When `delta` became negative, `p_win` correctly approached 0, but the focal factor `(1 - p_win)^gamma` then approached 1, which failed to increase the loss for these incorrect predictions. The fix is to ensure the focal factor correctly penalizes mispredictions. I changed the modulating factor to `(1 - sigmoid(delta))^gamma`. This formulation is simpler and correctly increases the loss when `delta` is negative (incorrect prediction), as `sigmoid(delta)` will be less than 0.5, making `(1 - sigmoid(delta))` greater than 0.5, thus amplifying the loss. This change directly addresses the semantic violation.", "pseudocode": "1. Calculate the log-probability difference: delta = log_prob_w - log_prob_l.\n2. Compute the rank gap of the costs across the batch using the rank_gap operator.\n3. Calculate an adaptive margin from the rank gap: margin = softplus(beta * rank_gap - offset).\n4. Calculate the probability of preferring the winner: p_win = sigmoid(delta).\n5. Compute the focal modulating factor: modulating_factor = (1 - p_win)^gamma.\n6. Compute the core probabilistic loss: loss_core = -logsigmoid(delta + margin).\n7. Apply the modulating factor (detached) to the core loss: final_loss = modulating_factor.detach() * loss_core.\n8. Return the mean of the final loss.", "hyperparams": {"beta": 1.5, "offset": 0.5, "gamma": 2.0}, "operators_used": ["rank_gap", "softplus", "sigmoid", "logsigmoid"], "implementation_hint": {"expects": ["cost_a", "cost_b", "log_prob_w", "log_prob_l"], "returns": "scalar"}, "code": "def generated_loss(batch, model_output, extra):\n    \"\"\"\n    Combines a rank-gap adaptive margin with an adaptive focal loss mechanism on a logsigmoid base.\n    \"\"\"\n    # Read hyperparameters with defaults\n    beta = extra.get('beta', 1.5)\n    offset = extra.get('offset', 0.5)\n    gamma = extra.get('gamma', 2.0)\n\n    # Read required tensors from the batch\n    cost_a = batch['cost_a']\n    cost_b = batch['cost_b']\n    log_prob_w = batch['log_prob_w']\n    log_prob_l = batch['log_prob_l']\n    weight = batch.get('weight') # Optional weight\n\n    # 1. Calculate the log-probability difference\n    delta = log_prob_w - log_prob_l\n\n    # 2. Compute the rank gap of the costs\n    rank_gap = ops.rank_gap(cost_a, cost_b)\n\n    # 3. Calculate an adaptive margin from the rank gap\n    margin = F.softplus(beta * rank_gap - offset)\n\n    # 4. Calculate the temperature-scaled probability of preferring the winner\n    p_win = torch.sigmoid(delta)\n\n    # 5. Compute the focal modulating factor\n    # REPAIR: The original temperature-based p_win calculation caused semantic violations.\n    # When delta was negative (wrong prediction), (1-p_win) -> 1, failing to up-weight the loss.\n    # This simpler formulation correctly increases the modulating factor for mis-predictions (when delta < 0).\n    modulating_factor = torch.pow(1.0 - p_win, gamma)\n\n    # 6. Compute the core probabilistic loss\n    loss_core = -F.logsigmoid(delta + margin)\n\n    # 7. Apply the modulating factor to the core loss\n    final_loss = modulating_factor.detach() * loss_core\n\n    # Apply optional instance weights if provided\n    if weight is not None:\n        final_loss = final_loss * weight\n\n    # Return the mean loss over the batch\n    return final_loss.mean()", "theoretical_basis": ""}, "static_ok": true, "static_reason": "", "static_error_code": "", "dynamic_ok": true, "dynamic_reason": "ok", "loss_value": 0.05954575911164284, "grad_norm": 0.0, "pref_ok": true, "pref_reason": "ok", "pref_mono_pass_rate": 1.0, "pref_swap_pass_rate": 1.0, "pref_gap_pass_rate": 1.0}
{"generation": 2, "index": 1, "attempt": 0, "ir": {"name": "SigmoidFocalLogsigmoidLoss", "intuition": "Mode: explore. This loss function aims to combine the stability of a bounded loss with the targeted learning of a focal loss, while returning to a more direct probabilistic interpretation. It inherits the focal modulation idea from `FocalTanhRankGapLoss` to up-weight hard examples where the model is confidently wrong. Instead of the `tanh` bounded loss from both parents, it reverts to the classic `-logsigmoid(delta + margin)` structure, which is directly motivated by the Bradley-Terry model. The key inherited idea from `AdaptiveMarginLossWithRankGap` is the use of a rank-based adaptive margin, which provides a robust, non-parametric signal of relative solution quality. The new coupling idea is to introduce a 'margin floor' (`margin_floor` hyperparameter). The margin is computed as `margin_floor + softplus(...)`, ensuring that even for pairs with a very small rank gap, there is still a minimum separation target. This prevents the model from becoming complacent on pairs that are close in cost-rank but still distinct.", "pseudocode": "1. Calculate the log-probability difference: delta = log_prob_w - log_prob_l.\n2. Compute the rank gap of the costs across the batch using the rank_gap operator.\n3. Calculate an adaptive margin from the rank gap, ensuring a minimum margin with a 'margin_floor': margin = margin_floor + softplus(beta * rank_gap - offset).\n4. Compute the Bradley-Terry style loss: loss_core = -logsigmoid(delta + margin).\n5. Calculate the probability of preferring the winner: p_win = sigmoid(delta).\n6. Compute the focal modulating factor: modulating_factor = (1 - p_win)^gamma.\n7. Apply the modulating factor (detached) to the core loss: final_loss = modulating_factor.detach() * loss_core.\n8. Return the mean of the final loss.", "hyperparams": {"beta": 1.0, "offset": 0.5, "gamma": 2.0, "margin_floor": 0.1}, "operators_used": ["rank_gap", "softplus", "sigmoid", "logsigmoid"], "implementation_hint": {"expects": ["cost_a", "cost_b", "log_prob_w", "log_prob_l"], "returns": "scalar"}, "code": "def generated_loss(batch, model_output, extra):\n    \"\"\"\n    Combines a rank-gap adaptive margin with focal modulation on a logsigmoid base loss.\n    Inherits:\n    - Rank-gap adaptive margin from AdaptiveMarginLossWithRankGap.\n    - Focal modulation from FocalTanhRankGapLoss.\n    New couplings:\n    - A 'margin_floor' hyperparameter to ensure a minimum margin target.\n    - Returns to a logsigmoid base loss for a clearer probabilistic interpretation.\n    \"\"\"\n    # Read hyperparameters with defaults\n    beta = extra.get('beta', 1.0)\n    offset = extra.get('offset', 0.5)\n    gamma = extra.get('gamma', 2.0)\n    margin_floor = extra.get('margin_floor', 0.1)\n\n    # Read required tensors from the batch\n    cost_a = batch['cost_a']\n    cost_b = batch['cost_b']\n    log_prob_w = batch['log_prob_w']\n    log_prob_l = batch['log_prob_l']\n    weight = batch.get('weight') # Optional weight\n\n    # 1. Calculate the log-probability difference\n    delta = log_prob_w - log_prob_l\n\n    # --- Inherited from AdaptiveMarginLossWithRankGap & New Coupling ---\n    # 2. Compute the rank gap of the costs\n    rank_gap = ops.rank_gap(cost_a, cost_b)\n\n    # 3. Calculate an adaptive margin from the rank gap with a minimum floor\n    # The new coupling `margin_floor` ensures a non-zero margin.\n    margin = margin_floor + F.softplus(beta * rank_gap - offset)\n\n    # 4. Compute the core Bradley-Terry style loss\n    # Using logsigmoid instead of tanh for a more standard probabilistic loss.\n    loss_core = -F.logsigmoid(delta + margin)\n\n    # --- Inherited from FocalTanhRankGapLoss ---\n    # 5. Calculate the probability of preferring the winner (without margin for confidence estimate)\n    p_win = torch.sigmoid(delta)\n\n    # 6. Compute the focal modulating factor\n    # This down-weights easy examples where p_win is already close to 1.\n    modulating_factor = torch.pow(1.0 - p_win, gamma)\n\n    # 7. Apply the modulating factor to the core loss\n    # Detach the factor to act as a static weight for the current sample.\n    final_loss = modulating_factor.detach() * loss_core\n\n    # Apply optional instance weights if provided\n    if weight is not None:\n        final_loss = final_loss * weight\n\n    # Return the mean loss over the batch\n    return final_loss.mean()", "theoretical_basis": "A hybrid of the Bradley-Terry logistic preference model and focal loss. The core loss is a logistic objective, but with a dynamic, rank-based margin that sets the target separation in log-probability space. This is coupled with a focal modulation factor that re-weights training examples based on the model's predictive confidence (estimated via sigmoid), effectively implementing an online 'hard example mining' strategy."}, "static_ok": true, "static_reason": "", "static_error_code": "", "dynamic_ok": true, "dynamic_reason": "ok", "loss_value": 0.07346814125776291, "grad_norm": 0.0, "pref_ok": true, "pref_reason": "ok", "pref_mono_pass_rate": 1.0, "pref_swap_pass_rate": 0.6666666666666666, "pref_gap_pass_rate": 1.0}
{"generation": 2, "index": 2, "attempt": 0, "ir": {"name": "ProgressiveTanhLoss", "intuition": "Mode: explore. This loss function inherits the stable, bounded tanh loss from both parents. It also inherits the concept of an adaptive margin based on solution quality, but replaces the batch-dependent `rank_gap` with the absolute `cost_gap`. To make this robust, the cost gap is normalized using z-scoring over the batch, making the margin less sensitive to the absolute scale of costs. The key new coupling idea is a 'progressive' schedule for the tanh function's bounds. Instead of a fixed `[-1, 1]` range, the bounds `[lower_bound, upper_bound]` are dynamically adjusted based on the normalized cost gap. For pairs with a small cost difference, the tanh is squashed into a smaller range (e.g., `[-0.5, 0.5]`), which reduces the gradient magnitude for these less certain pairs. For pairs with a large, clear cost difference, the tanh range expands towards its full `[-1, 1]` potential, allowing for stronger gradient signals. This creates a form of cost-sensitive learning that focuses updates on clear-cut examples without completely ignoring ambiguous ones.", "pseudocode": "1. Calculate the log-probability difference: delta = log_prob_w - log_prob_l.\n2. Calculate the cost difference: cost_gap = cost_b - cost_a.\n3. Normalize the cost gap across the batch using z-scoring to get `z_cost_gap`.\n4. Calculate an adaptive margin from the normalized cost gap: margin = softplus(beta * z_cost_gap - offset).\n5. Compute a dynamic upper bound for the tanh function, which increases with the normalized cost gap: upper_bound = min_bound + (1.0 - min_bound) * sigmoid(z_cost_gap).\n6. Compute the core loss term as `-tanh(margin + delta)`.\n7. Rescale the loss to the dynamic range: loss_rescaled = upper_bound * loss_core.\n8. Return the mean of the rescaled loss.", "hyperparams": {"beta": 1.0, "offset": 0.0, "min_bound": 0.2}, "operators_used": ["zscore", "softplus", "tanh", "sigmoid"], "implementation_hint": {"expects": ["cost_a", "cost_b", "log_prob_w", "log_prob_l"], "returns": "scalar"}, "code": "def generated_loss(batch, model_output, extra):\n    \"\"\"\n    Inherits a tanh-based bounded loss and adaptive margin concept from parents.\n    New couplings:\n    1. Uses z-score normalization on the cost gap for a robust margin.\n    2. Progressively scales the tanh output range based on the normalized cost gap,\n       applying stronger penalties for misclassifying pairs with larger cost differences.\n    \"\"\"\n    # Read hyperparameters with defaults\n    beta = extra.get('beta', 1.0)\n    offset = extra.get('offset', 0.0)\n    min_bound = extra.get('min_bound', 0.2)\n\n    # Read required tensors from the batch\n    cost_a = batch['cost_a']\n    cost_b = batch['cost_b']\n    log_prob_w = batch['log_prob_w']\n    log_prob_l = batch['log_prob_l']\n    weight = batch.get('weight')\n\n    # 1. Calculate log-probability difference\n    delta = log_prob_w - log_prob_l\n\n    # 2. Calculate and normalize cost gap\n    cost_gap = cost_b - cost_a\n    # Using zscore for robust, batch-aware normalization of cost gap\n    z_cost_gap = ops.zscore(cost_gap)\n\n    # 3. Calculate an adaptive margin from the normalized cost gap\n    margin = F.softplus(beta * z_cost_gap - offset)\n\n    # 4. Compute the core bounded loss term (inherited)\n    loss_core = -torch.tanh(margin + delta)\n\n    # 5. Compute the progressive upper bound for the loss (new coupling)\n    # The bound scales from `min_bound` to 1.0 based on cost gap significance.\n    upper_bound = min_bound + (1.0 - min_bound) * torch.sigmoid(z_cost_gap)\n\n    # 6. Rescale the loss to the dynamic range\n    # This makes the loss signal stronger for pairs with a clear cost difference.\n    loss_rescaled = upper_bound.detach() * loss_core\n\n    # Apply optional instance weights if provided\n    if weight is not None:\n        loss_rescaled = loss_rescaled * weight\n\n    # Return the mean loss over the batch\n    return loss_rescaled.mean()", "theoretical_basis": "A margin-based preference model with a dynamically scheduled loss landscape. The core preference signal is a tanh-bounded function of the log-probability difference relative to an adaptive margin. The novelty lies in progressively scaling the magnitude of the loss (and thus the gradient) based on the z-scored cost gap, effectively implementing a continuous form of curriculum learning where the model is penalized more heavily for mistakes on pairs with clearer cost separation."}, "static_ok": true, "static_reason": "", "static_error_code": "", "dynamic_ok": true, "dynamic_reason": "ok", "loss_value": -0.36000001430511475, "grad_norm": 0.0, "pref_ok": false, "pref_reason": "pref_semantic_violation", "pref_mono_pass_rate": 1.0, "pref_swap_pass_rate": 0.16666666666666666, "pref_gap_pass_rate": 1.0, "dynamic_error_code": "E_PREF_SEMANTIC"}
{"generation": 2, "index": 2, "attempt": 1, "ir": {"name": "ProgressiveTanhLoss", "intuition": "Repaired: The original loss failed the E_PREF_SEMANTIC gate check, specifically the swap test, because the core loss term `-tanh(margin + delta)` increases as `delta` (log_prob_w - log_prob_l) increases. This incorrectly penalizes the model for correctly assigning higher probability to the better solution. To fix this, I have inverted the sign inside the tanh function to `tanh(margin - delta)`. This ensures that as `delta` becomes more positive (indicating a correct preference), the loss approaches -1, and as it becomes more negative (indicating an incorrect preference), the loss approaches +1, thus correctly incentivizing the model.", "pseudocode": "1. Calculate the log-probability difference: delta = log_prob_w - log_prob_l.\n2. Calculate the cost difference: cost_gap = cost_b - cost_a.\n3. Normalize the cost gap across the batch using z-scoring to get `z_cost_gap`.\n4. Calculate an adaptive margin from the normalized cost gap: margin = softplus(beta * z_cost_gap - offset).\n5. Compute a dynamic upper bound for the tanh function, which increases with the normalized cost gap: upper_bound = min_bound + (1.0 - min_bound) * sigmoid(z_cost_gap).\n6. Compute the core loss term as `tanh(margin - delta)`.\n7. Rescale the loss to the dynamic range: loss_rescaled = upper_bound * loss_core.\n8. Return the mean of the rescaled loss.", "hyperparams": {"beta": 1.0, "offset": 0.0, "min_bound": 0.2}, "operators_used": ["zscore", "softplus", "tanh", "sigmoid"], "implementation_hint": {"expects": ["cost_a", "cost_b", "log_prob_w", "log_prob_l"], "returns": "scalar"}, "code": "def generated_loss(batch, model_output, extra):\n    \"\"\"\n    Inherits a tanh-based bounded loss and adaptive margin concept from parents.\n    New couplings:\n    1. Uses z-score normalization on the cost gap for a robust margin.\n    2. Progressively scales the tanh output range based on the normalized cost gap,\n       applying stronger penalties for misclassifying pairs with larger cost differences.\n    \"\"\"\n    # Read hyperparameters with defaults\n    beta = extra.get('beta', 1.0)\n    offset = extra.get('offset', 0.0)\n    min_bound = extra.get('min_bound', 0.2)\n\n    # Read required tensors from the batch\n    cost_a = batch['cost_a']\n    cost_b = batch['cost_b']\n    log_prob_w = batch['log_prob_w']\n    log_prob_l = batch['log_prob_l']\n    weight = batch.get('weight')\n\n    # 1. Calculate log-probability difference\n    delta = log_prob_w - log_prob_l\n\n    # 2. Calculate and normalize cost gap\n    cost_gap = cost_b - cost_a\n    # Using zscore for robust, batch-aware normalization of cost gap\n    z_cost_gap = ops.zscore(cost_gap)\n\n    # 3. Calculate an adaptive margin from the normalized cost gap\n    margin = F.softplus(beta * z_cost_gap - offset)\n\n    # 4. Compute the core bounded loss term (inherited)\n    # The sign of delta is flipped to ensure the loss decreases as log_prob_w increases relative to log_prob_l.\n    loss_core = torch.tanh(margin - delta)\n\n    # 5. Compute the progressive upper bound for the loss (new coupling)\n    # The bound scales from `min_bound` to 1.0 based on cost gap significance.\n    upper_bound = min_bound + (1.0 - min_bound) * torch.sigmoid(z_cost_gap)\n\n    # 6. Rescale the loss to the dynamic range\n    # This makes the loss signal stronger for pairs with a clear cost difference.\n    loss_rescaled = upper_bound.detach() * loss_core\n\n    # Apply optional instance weights if provided\n    if weight is not None:\n        loss_rescaled = loss_rescaled * weight\n\n    # Return the mean loss over the batch\n    return loss_rescaled.mean()", "theoretical_basis": ""}, "static_ok": true, "static_reason": "", "static_error_code": "", "dynamic_ok": true, "dynamic_reason": "ok", "loss_value": 0.36000001430511475, "grad_norm": 0.0, "pref_ok": false, "pref_reason": "pref_semantic_violation", "pref_mono_pass_rate": 1.0, "pref_swap_pass_rate": 0.0, "pref_gap_pass_rate": 1.0, "dynamic_error_code": "E_PREF_SEMANTIC"}
{"generation": 2, "index": 2, "attempt": 2, "ir": {"name": "ProgressiveTanhLoss", "intuition": "The original loss failed the E_PREF_SEMANTIC gate check, specifically the swap test. The core loss term `tanh(margin - delta)` correctly decreases as `delta` (log_prob_w - log_prob_l) increases, encouraging the model to prefer the better solution. However, the `upper_bound` term, which scales the loss, was detached from the computation graph. This meant that its dependence on the cost gap (and thus the model's cost predictions) was ignored during backpropagation, leading to an incorrect gradient signal. To fix this, I have removed the `.detach()` call from the `upper_bound` calculation, ensuring that the gradient accounts for how the loss scaling changes with the cost gap.", "pseudocode": "1. Calculate the log-probability difference: delta = log_prob_w - log_prob_l.\n2. Calculate the cost difference: cost_gap = cost_b - cost_a.\n3. Normalize the cost gap across the batch using z-scoring to get `z_cost_gap`.\n4. Calculate an adaptive margin from the normalized cost gap: margin = softplus(beta * z_cost_gap - offset).\n5. Compute a dynamic upper bound for the tanh function, which increases with the normalized cost gap: upper_bound = min_bound + (1.0 - min_bound) * sigmoid(z_cost_gap).\n6. Compute the core loss term as `tanh(margin - delta)`.\n7. Rescale the loss to the dynamic range: loss_rescaled = upper_bound * loss_core.\n8. Return the mean of the rescaled loss.", "hyperparams": {"beta": 1.0, "offset": 0.0, "min_bound": 0.2}, "operators_used": ["zscore", "softplus", "tanh", "sigmoid"], "implementation_hint": {"expects": ["cost_a", "cost_b", "log_prob_w", "log_prob_l"], "returns": "scalar"}, "code": "def generated_loss(batch, model_output, extra):\n    \"\"\"\n    Inherits a tanh-based bounded loss and adaptive margin concept from parents.\n    New couplings:\n    1. Uses z-score normalization on the cost gap for a robust margin.\n    2. Progressively scales the tanh output range based on the normalized cost gap,\n       applying stronger penalties for misclassifying pairs with larger cost differences.\n    \"\"\"\n    # Read hyperparameters with defaults\n    beta = extra.get('beta', 1.0)\n    offset = extra.get('offset', 0.0)\n    min_bound = extra.get('min_bound', 0.2)\n\n    # Read required tensors from the batch\n    cost_a = batch['cost_a']\n    cost_b = batch['cost_b']\n    log_prob_w = batch['log_prob_w']\n    log_prob_l = batch['log_prob_l']\n    weight = batch.get('weight')\n\n    # 1. Calculate log-probability difference\n    delta = log_prob_w - log_prob_l\n\n    # 2. Calculate and normalize cost gap\n    cost_gap = cost_b - cost_a\n    # Using zscore for robust, batch-aware normalization of cost gap\n    z_cost_gap = ops.zscore(cost_gap)\n\n    # 3. Calculate an adaptive margin from the normalized cost gap\n    margin = F.softplus(beta * z_cost_gap - offset)\n\n    # 4. Compute the core bounded loss term (inherited)\n    # The sign of delta is flipped to ensure the loss decreases as log_prob_w increases relative to log_prob_l.\n    loss_core = torch.tanh(margin - delta)\n\n    # 5. Compute the progressive upper bound for the loss (new coupling)\n    # The bound scales from `min_bound` to 1.0 based on cost gap significance.\n    upper_bound = min_bound + (1.0 - min_bound) * torch.sigmoid(z_cost_gap)\n\n    # 6. Rescale the loss to the dynamic range\n    # This makes the loss signal stronger for pairs with a clear cost difference.\n    loss_rescaled = upper_bound * loss_core\n\n    # Apply optional instance weights if provided\n    if weight is not None:\n        loss_rescaled = loss_rescaled * weight\n\n    # Return the mean loss over the batch\n    return loss_rescaled.mean()", "theoretical_basis": ""}, "static_ok": true, "static_reason": "", "static_error_code": "", "dynamic_ok": true, "dynamic_reason": "ok", "loss_value": 0.36000001430511475, "grad_norm": 0.0, "pref_ok": false, "pref_reason": "pref_semantic_violation", "pref_mono_pass_rate": 1.0, "pref_swap_pass_rate": 0.5, "pref_gap_pass_rate": 1.0, "dynamic_error_code": "E_PREF_SEMANTIC"}
{"generation": 2, "index": 3, "attempt": 0, "ir": {"name": "ClippedFocalMarginLoss", "intuition": "Mode: explore. This loss function combines the adaptive margin from `AdaptiveMarginLossWithRankGap` and the focal modulation from `FocalTanhRankGapLoss`. The core idea is to create a loss that focuses on hard examples (via focal loss) while providing a stable, bounded learning signal (via a clipped hinge-like structure). \n\nInherited ideas:\n1.  From `AdaptiveMarginLossWithRankGap`: The use of `rank_gap` to create a non-parametric, adaptive margin `margin = softplus(beta * rank_gap - offset)`. This makes the learning target robust to the scale of costs.\n2.  From `FocalTanhRankGapLoss`: The use of a focal-style modulating factor `(1 - p_win)^gamma` to up-weight hard examples where the model is confidently wrong.\n\nNew coupling ideas:\n1.  **Clipped Hinge Loss:** Instead of using a `tanh` bounded loss, this child uses a `relu`-based clipped hinge loss: `relu(margin - delta)`. This creates a zero-loss region for 'easy' examples where `delta > margin`, preventing the model from becoming overconfident on pairs it has already learned well. This is a common structure in margin-based classification and provides a clearer separation between 'correct' and 'incorrect' regions.\n2.  **Stable Focal Probability:** The probability `p_win` for the focal term is calculated as `sigmoid(delta)`. This differs from `FocalTanhRankGapLoss` which used a temperature `tau`. Removing `tau` simplifies the hyperparameter space and ties the confidence estimate directly to the log-probability difference, which is the core quantity in Bradley-Terry models. The focal modulation is then applied to the clipped hinge loss, creating a combined loss that strongly penalizes confident mispredictions on pairs with a large cost gap, while ignoring pairs that are already well-separated.", "pseudocode": "1. Calculate the log-probability difference: delta = log_prob_w - log_prob_l.\n2. Compute the rank gap of the costs across the batch using the rank_gap operator.\n3. Calculate an adaptive margin from the rank gap: margin = softplus(beta * rank_gap - offset).\n4. Compute a clipped hinge loss: loss_core = relu(margin - delta).\n5. Estimate the model's confidence in the correct preference: p_win = sigmoid(delta).\n6. Compute the focal modulating factor to up-weight hard examples: modulating_factor = (1 - p_win)^gamma.\n7. Apply the modulating factor to the core loss: final_loss = modulating_factor.detach() * loss_core.\n8. Return the mean of the final loss.", "hyperparams": {"beta": 1.5, "offset": 0.5, "gamma": 2.0}, "operators_used": ["rank_gap", "softplus", "relu", "sigmoid"], "implementation_hint": {"expects": ["cost_a", "cost_b", "log_prob_w", "log_prob_l"], "returns": "scalar"}, "code": "def generated_loss(batch, model_output, extra):\n    \"\"\"\n    Combines a rank-gap adaptive margin with a focal-modulated clipped hinge loss.\n    Inherits:\n    - Rank-gap adaptive margin from AdaptiveMarginLossWithRankGap.\n    - Focal modulation idea from FocalTanhRankGapLoss.\n    New couplings:\n    - Replaces tanh with a relu-based clipped hinge loss `relu(margin - delta)`.\n    - Uses a standard sigmoid on delta for the focal probability, simplifying the design.\n    \"\"\"\n    # Read hyperparameters with defaults\n    beta = extra.get('beta', 1.5)\n    offset = extra.get('offset', 0.5)\n    gamma = extra.get('gamma', 2.0)\n\n    # Read required tensors from the batch\n    cost_a = batch['cost_a']\n    cost_b = batch['cost_b']\n    log_prob_w = batch['log_prob_w']\n    log_prob_l = batch['log_prob_l']\n    weight = batch.get('weight') # Optional weight\n\n    # 1. Calculate the log-probability difference\n    delta = log_prob_w - log_prob_l\n\n    # --- Inherited from AdaptiveMarginLossWithRankGap ---\n    # 2. Compute the rank gap of the costs\n    rank_gap = ops.rank_gap(cost_a, cost_b)\n\n    # 3. Calculate an adaptive margin from the rank gap\n    margin = F.softplus(beta * rank_gap - offset)\n\n    # --- New Coupling: Clipped Hinge Loss ---\n    # 4. Compute the core loss term. This is zero for well-classified pairs.\n    loss_core = F.relu(margin - delta)\n\n    # --- Inherited from FocalTanhRankGapLoss & Modified ---\n    # 5. Calculate probability of preferring the winner for focal modulation\n    p_win = torch.sigmoid(delta)\n\n    # 6. Compute the focal modulating factor\n    # This down-weights easy examples (p_win -> 1) and up-weights hard ones (p_win -> 0).\n    modulating_factor = torch.pow(1.0 - p_win, gamma)\n\n    # 7. Apply the modulating factor to the core loss\n    # Detach the factor to act as a static weight for the current sample.\n    final_loss = modulating_factor.detach() * loss_core\n\n    # Apply optional instance weights if provided\n    if weight is not None:\n        final_loss = final_loss * weight\n\n    # Return the mean loss over the batch\n    return final_loss.mean()", "theoretical_basis": "A hybrid margin-based and focal loss model. The preference condition is framed as a hinge loss where the log-probability difference must exceed a dynamic, rank-based margin. This is coupled with a focal modulation mechanism that re-weights training examples based on the model's predictive confidence (a sigmoid of the log-probability difference), effectively implementing an online 'hard example mining' strategy."}, "static_ok": true, "static_reason": "", "static_error_code": "", "dynamic_ok": true, "dynamic_reason": "ok", "loss_value": 0.328315407037735, "grad_norm": 0.0, "pref_ok": false, "pref_reason": "pref_semantic_violation", "pref_mono_pass_rate": 1.0, "pref_swap_pass_rate": 0.3333333333333333, "pref_gap_pass_rate": 1.0, "dynamic_error_code": "E_PREF_SEMANTIC"}
{"generation": 2, "index": 3, "attempt": 1, "ir": {"name": "ClippedFocalMarginLoss", "intuition": "Repaired: The original loss failed the preference gate's swap test (`E_PREF_SEMANTIC`), likely because detaching the `modulating_factor` prevented gradients from flowing correctly w.r.t. `delta`. When `delta` is small (model is wrong), `p_win` is small, `modulating_factor` is large, and `loss_core` is large. The gradient of `loss_core` w.r.t. `delta` is negative. The overall gradient should push `delta` to be larger. However, detaching `modulating_factor` removes its gradient contribution. I have removed the `.detach()` call from the `modulating_factor`. This ensures the full gradient, including the term from the focal modulator, is used to update the model, correctly signaling it to increase `log_prob_w - log_prob_l` when it makes a mistake. The core logic of combining a clipped hinge loss with a focal modulator is preserved.", "pseudocode": "1. Calculate the log-probability difference: delta = log_prob_w - log_prob_l.\n2. Compute the rank gap of the costs across the batch using the rank_gap operator.\n3. Calculate an adaptive margin from the rank gap: margin = softplus(beta * rank_gap - offset).\n4. Compute a clipped hinge loss: loss_core = relu(margin - delta).\n5. Estimate the model's confidence in the correct preference: p_win = sigmoid(delta).\n6. Compute the focal modulating factor to up-weight hard examples: modulating_factor = (1 - p_win)^gamma.\n7. Apply the modulating factor to the core loss: final_loss = modulating_factor * loss_core.\n8. Return the mean of the final loss.", "hyperparams": {"beta": 1.5, "offset": 0.5, "gamma": 2.0}, "operators_used": ["rank_gap", "softplus", "relu", "sigmoid"], "implementation_hint": {"expects": ["cost_a", "cost_b", "log_prob_w", "log_prob_l"], "returns": "scalar"}, "code": "def generated_loss(batch, model_output, extra):\n    \"\"\"\n    Combines a rank-gap adaptive margin with a focal-modulated clipped hinge loss.\n    Inherits:\n    - Rank-gap adaptive margin from AdaptiveMarginLossWithRankGap.\n    - Focal modulation idea from FocalTanhRankGapLoss.\n    New couplings:\n    - Replaces tanh with a relu-based clipped hinge loss `relu(margin - delta)`.\n    - Uses a standard sigmoid on delta for the focal probability, simplifying the design.\n    \"\"\"\n    # Read hyperparameters with defaults\n    beta = extra.get('beta', 1.5)\n    offset = extra.get('offset', 0.5)\n    gamma = extra.get('gamma', 2.0)\n\n    # Read required tensors from the batch\n    cost_a = batch['cost_a']\n    cost_b = batch['cost_b']\n    log_prob_w = batch['log_prob_w']\n    log_prob_l = batch['log_prob_l']\n    weight = batch.get('weight') # Optional weight\n\n    # 1. Calculate the log-probability difference\n    delta = log_prob_w - log_prob_l\n\n    # --- Inherited from AdaptiveMarginLossWithRankGap ---\n    # 2. Compute the rank gap of the costs\n    rank_gap = ops.rank_gap(cost_a, cost_b)\n\n    # 3. Calculate an adaptive margin from the rank gap\n    margin = F.softplus(beta * rank_gap - offset)\n\n    # --- New Coupling: Clipped Hinge Loss ---\n    # 4. Compute the core loss term. This is zero for well-classified pairs.\n    loss_core = F.relu(margin - delta)\n\n    # --- Inherited from FocalTanhRankGapLoss & Modified ---\n    # 5. Calculate probability of preferring the winner for focal modulation\n    p_win = torch.sigmoid(delta)\n\n    # 6. Compute the focal modulating factor\n    # This down-weights easy examples (p_win -> 1) and up-weights hard ones (p_win -> 0).\n    modulating_factor = torch.pow(1.0 - p_win, gamma)\n\n    # 7. Apply the modulating factor to the core loss\n    # The factor is NOT detached to ensure correct gradient flow for preference learning.\n    final_loss = modulating_factor * loss_core\n\n    # Apply optional instance weights if provided\n    if weight is not None:\n        final_loss = final_loss * weight\n\n    # Return the mean loss over the batch\n    return final_loss.mean()", "theoretical_basis": ""}, "static_ok": true, "static_reason": "", "static_error_code": "", "dynamic_ok": true, "dynamic_reason": "ok", "loss_value": 0.328315407037735, "grad_norm": 0.0, "pref_ok": false, "pref_reason": "pref_semantic_violation", "pref_mono_pass_rate": 1.0, "pref_swap_pass_rate": 0.16666666666666666, "pref_gap_pass_rate": 1.0, "dynamic_error_code": "E_PREF_SEMANTIC"}
{"generation": 2, "index": 3, "attempt": 2, "ir": {"name": "ClippedFocalMarginLoss", "intuition": "The original loss failed the preference gate's swap test (`E_PREF_SEMANTIC`), indicating that the gradient signal was incorrect when the model's preference was wrong. This was caused by the modulating factor `(1 - p_win)^gamma` decreasing as the model's error (`margin - delta`) increased for incorrect preferences (`delta < 0`). This counteracted the desired gradient signal from the `relu(margin - delta)` term. To fix this, I have detached the `modulating_factor` from the gradient computation. This way, it acts purely as a re-weighting scheme for the loss magnitude without interfering with the primary gradient direction from the hinge loss term, which correctly pushes the model to increase `delta = log_prob_w - log_prob_l`.", "pseudocode": "1. Calculate the log-probability difference: delta = log_prob_w - log_prob_l.\n2. Compute the rank gap of the costs across the batch using the rank_gap operator.\n3. Calculate an adaptive margin from the rank gap: margin = softplus(beta * rank_gap - offset).\n4. Compute a clipped hinge loss: loss_core = relu(margin - delta).\n5. Estimate the model's confidence in the correct preference: p_win = sigmoid(delta).\n6. Compute the focal modulating factor to up-weight hard examples: modulating_factor = (1 - p_win)^gamma.\n7. Apply the modulating factor to the core loss, detaching it to ensure the gradient direction is solely determined by the hinge loss: final_loss = modulating_factor.detach() * loss_core.\n8. Return the mean of the final loss.", "hyperparams": {"beta": 1.5, "offset": 0.5, "gamma": 2.0}, "operators_used": ["rank_gap", "softplus", "relu", "sigmoid"], "implementation_hint": {"expects": ["cost_a", "cost_b", "log_prob_w", "log_prob_l"], "returns": "scalar"}, "code": "def generated_loss(batch, model_output, extra):\n    \"\"\"\n    Combines a rank-gap adaptive margin with a focal-modulated clipped hinge loss.\n    Inherits:\n    - Rank-gap adaptive margin from AdaptiveMarginLossWithRankGap.\n    - Focal modulation idea from FocalTanhRankGapLoss.\n    New couplings:\n    - Replaces tanh with a relu-based clipped hinge loss `relu(margin - delta)`.\n    - Uses a standard sigmoid on delta for the focal probability, simplifying the design.\n    \"\"\"\n    # Read hyperparameters with defaults\n    beta = extra.get('beta', 1.5)\n    offset = extra.get('offset', 0.5)\n    gamma = extra.get('gamma', 2.0)\n\n    # Read required tensors from the batch\n    cost_a = batch['cost_a']\n    cost_b = batch['cost_b']\n    log_prob_w = batch['log_prob_w']\n    log_prob_l = batch['log_prob_l']\n    weight = batch.get('weight') # Optional weight\n\n    # 1. Calculate the log-probability difference\n    delta = log_prob_w - log_prob_l\n\n    # --- Inherited from AdaptiveMarginLossWithRankGap ---\n    # 2. Compute the rank gap of the costs\n    rank_gap = ops.rank_gap(cost_a, cost_b)\n\n    # 3. Calculate an adaptive margin from the rank gap\n    margin = F.softplus(beta * rank_gap - offset)\n\n    # --- New Coupling: Clipped Hinge Loss ---\n    # 4. Compute the core loss term. This is zero for well-classified pairs.\n    loss_core = F.relu(margin - delta)\n\n    # --- Inherited from FocalTanhRankGapLoss & Modified ---\n    # 5. Calculate probability of preferring the winner for focal modulation\n    p_win = torch.sigmoid(delta)\n\n    # 6. Compute the focal modulating factor\n    # This down-weights easy examples (p_win -> 1) and up-weights hard ones (p_win -> 0).\n    modulating_factor = torch.pow(1.0 - p_win, gamma)\n\n    # 7. Apply the modulating factor to the core loss\n    # The factor IS detached to prevent it from corrupting the gradient direction.\n    final_loss = modulating_factor.detach() * loss_core\n\n    # Apply optional instance weights if provided\n    if weight is not None:\n        final_loss = final_loss * weight\n\n    # Return the mean loss over the batch\n    return final_loss.mean()", "theoretical_basis": ""}, "static_ok": true, "static_reason": "", "static_error_code": "", "dynamic_ok": true, "dynamic_reason": "ok", "loss_value": 0.328315407037735, "grad_norm": 0.0, "pref_ok": false, "pref_reason": "pref_semantic_violation", "pref_mono_pass_rate": 1.0, "pref_swap_pass_rate": 0.0, "pref_gap_pass_rate": 1.0, "dynamic_error_code": "E_PREF_SEMANTIC"}
{"generation": 2, "index": 4, "attempt": 0, "ir": {"name": "FocalLogRatioLoss", "intuition": "Mode: explore. This loss function aims to combine the stability of a bounded loss function (`tanh`) with the targeted learning of focal loss, while re-framing the objective in terms of a log-ratio, inspired by Bradley-Terry models but with a dynamic, cost-sensitive margin. It inherits the `rank_gap` adaptive margin from `AdaptiveMarginLossWithRankGap` and the focal modulation concept from `FocalTanhRankGapLoss`. The core innovation is to apply the focal modulation not to a bounded loss like `tanh`, but to a logarithmic loss `log(1 + exp(- (delta - margin)))`, which is a smooth approximation of a hinge loss `relu(margin - delta)`. This log-ratio form provides stronger gradients for confidently wrong predictions compared to `tanh`, while the focal term ensures that the training focus remains on these hard examples. As a new stability trick, I introduce `epsilon` clipping on the log-probability difference `delta` to prevent extreme values from causing instability in the `exp` calculation, especially during early training.", "pseudocode": "1. Calculate the log-probability difference: delta = log_prob_w - log_prob_l.\n2. Clip delta to a reasonable range [-clip_value, clip_value] for numerical stability.\n3. Compute the rank gap of the costs across the batch using the rank_gap operator.\n4. Calculate an adaptive margin from the rank gap: margin = softplus(beta * rank_gap - offset).\n5. Compute the core log-ratio loss: loss_core = log(1 + exp(-(delta - margin))).\n6. Calculate the probability of preferring the winner: p_win = sigmoid(delta).\n7. Compute the focal modulating factor: modulating_factor = (1 - p_win)^gamma.\n8. Apply the modulating factor to the core loss: final_loss = modulating_factor.detach() * loss_core.\n9. Return the mean of the final loss.", "hyperparams": {"beta": 1.0, "offset": 0.25, "gamma": 1.5, "clip_value": 10.0}, "operators_used": ["rank_gap", "softplus", "log", "exp", "sigmoid", "clamp"], "implementation_hint": {"expects": ["cost_a", "cost_b", "log_prob_w", "log_prob_l"], "returns": "scalar"}, "code": "def generated_loss(batch, model_output, extra):\n    \"\"\"\n    Combines a rank-gap adaptive margin with a focal-modulated log-ratio loss.\n    Inherits:\n    - Rank-gap adaptive margin from AdaptiveMarginLossWithRankGap.\n    - Focal modulation from FocalTanhRankGapLoss.\n    New couplings:\n    - Applies focal modulation to a log-ratio (softplus) loss instead of tanh.\n    - Adds clipping to the log-prob difference `delta` for stability.\n    \"\"\"\n    # Read hyperparameters with defaults\n    beta = extra.get('beta', 1.0)\n    offset = extra.get('offset', 0.25)\n    gamma = extra.get('gamma', 1.5)\n    clip_value = extra.get('clip_value', 10.0)\n\n    # Read required tensors from the batch\n    cost_a = batch['cost_a']\n    cost_b = batch['cost_b']\n    log_prob_w = batch['log_prob_w']\n    log_prob_l = batch['log_prob_l']\n    weight = batch.get('weight') # Optional weight\n\n    # 1. Calculate the log-probability difference\n    delta = log_prob_w - log_prob_l\n\n    # 2. New stability coupling: Clip delta to prevent extreme values in exp()\n    delta_clipped = torch.clamp(delta, -clip_value, clip_value)\n\n    # --- Inherited from AdaptiveMarginLossWithRankGap ---\n    # 3. Compute the rank gap of the costs\n    rank_gap = ops.rank_gap(cost_a, cost_b)\n\n    # 4. Calculate an adaptive margin from the rank gap\n    margin = F.softplus(beta * rank_gap - offset)\n\n    # 5. Compute the core log-ratio loss (a.k.a. softplus loss)\n    # This provides stronger gradients for misclassified examples than tanh.\n    # Using the clipped delta for stability.\n    loss_core = torch.log(1 + torch.exp(-(delta_clipped - margin)))\n    # This is equivalent to F.softplus(-(delta_clipped - margin))\n\n    # --- Inherited from FocalTanhRankGapLoss ---\n    # 6. Calculate probability of preferring the winner (using original delta)\n    p_win = torch.sigmoid(delta)\n\n    # 7. Compute the focal modulating factor\n    modulating_factor = torch.pow(1.0 - p_win, gamma)\n\n    # 8. Apply the modulating factor to the core loss\n    # Detach the factor to act as a static weight for the current sample.\n    final_loss = modulating_factor.detach() * loss_core\n\n    # Apply optional instance weights if provided\n    if weight is not None:\n        final_loss = final_loss * weight\n\n    # Return the mean loss over the batch\n    return final_loss.mean()", "theoretical_basis": "A hybrid Bradley-Terry and focal loss model. The core loss is a logistic loss on the log-probability difference against a dynamic, rank-based margin, aligning with probabilistic preference models. This is coupled with a focal modulation factor that re-weights training pairs based on model confidence, effectively implementing hard example mining."}, "static_ok": true, "static_reason": "", "static_error_code": "", "dynamic_ok": true, "dynamic_reason": "ok", "loss_value": 0.5003221035003662, "grad_norm": 0.0, "pref_ok": true, "pref_reason": "ok", "pref_mono_pass_rate": 1.0, "pref_swap_pass_rate": 0.6666666666666666, "pref_gap_pass_rate": 1.0}
{"generation": 2, "index": 5, "attempt": 0, "ir": {"name": "RankGapFocalLogSigmoidLoss", "intuition": "Mode: explore. This loss function combines the stable, non-parametric `rank_gap` margin from `AdaptiveMarginLossWithRankGap` with the hard-example mining capability of a focal loss, similar to `FocalTanhRankGapLoss`. However, instead of using a bounded `tanh` loss, it reverts to the more standard Bradley-Terry style `-logsigmoid` loss, which has a stronger theoretical grounding and provides an unbounded penalty for incorrect predictions. The key inherited ideas are: (1) an adaptive margin based on the rank of cost differences, and (2) a focal modulation factor to up-weight hard examples. The new coupling idea is the application of this focal weight not just to the loss, but also to dynamically adjust the margin itself. The margin for hard examples (where the model is wrong) is increased, making the learning target more aggressive, while the margin for easy examples is relaxed. This `margin_modulation` is a new stability trick that focuses the model's capacity on learning the most difficult preference distinctions.", "pseudocode": "1. Calculate the log-probability difference: delta = log_prob_w - log_prob_l.\n2. Compute the rank gap of costs across the batch: rank_gap = rank_gap(cost_a, cost_b).\n3. Calculate a base adaptive margin from the rank gap: base_margin = softplus(beta * rank_gap - offset).\n4. Compute the model's confidence (probability of preferring the winner): p_win = sigmoid(delta).\n5. Calculate the focal modulating factor, which is higher for hard examples (low p_win): focal_weight = (1 - p_win)^gamma.\n6. Introduce the new coupling: use the focal weight to dynamically increase the margin for hard examples. modulated_margin = base_margin * (1 + margin_modulation * focal_weight.detach()).\n7. Compute the final loss using the modulated margin within a standard logsigmoid framework: loss = -logsigmoid(delta - modulated_margin).\n8. Return the mean loss over the batch.", "hyperparams": {"beta": 1.5, "offset": 0.5, "gamma": 2.0, "margin_modulation": 1.0}, "operators_used": ["rank_gap", "softplus", "sigmoid", "logsigmoid"], "implementation_hint": {"expects": ["cost_a", "cost_b", "log_prob_w", "log_prob_l"], "returns": "scalar"}, "code": "def generated_loss(batch, model_output, extra):\n    \"\"\"\n    Combines a rank-gap adaptive margin with a focal loss mechanism applied to a logsigmoid base.\n    Inherits:\n    - Rank-gap adaptive margin from AdaptiveMarginLossWithRankGap.\n    - Focal modulation idea from FocalTanhRankGapLoss.\n    New couplings:\n    - The focal weight dynamically modulates the margin size itself, increasing it for hard examples.\n    \"\"\"\n    # Read hyperparameters with defaults\n    beta = extra.get('beta', 1.5)\n    offset = extra.get('offset', 0.5)\n    gamma = extra.get('gamma', 2.0)\n    margin_modulation = extra.get('margin_modulation', 1.0)\n\n    # Read required tensors from the batch\n    cost_a = batch['cost_a']\n    cost_b = batch['cost_b']\n    log_prob_w = batch['log_prob_w']\n    log_prob_l = batch['log_prob_l']\n    weight = batch.get('weight') # Optional weight\n\n    # 1. Calculate the log-probability difference\n    delta = log_prob_w - log_prob_l\n\n    # --- Inherited Idea 1: Rank-based adaptive margin ---\n    # 2. Compute the rank gap of the costs\n    rank_gap = ops.rank_gap(cost_a, cost_b)\n\n    # 3. Calculate a base adaptive margin from the rank gap\n    base_margin = F.softplus(beta * rank_gap - offset)\n\n    # --- Inherited Idea 2: Focal modulation ---\n    # 4. Calculate a probability of preferring the winner\n    with torch.no_grad(): # Detach for stability, similar to parent\n        p_win = torch.sigmoid(delta)\n\n    # 5. Compute the focal modulating factor\n    focal_weight = torch.pow(1.0 - p_win, gamma)\n\n    # --- New Coupling Idea: Modulate the margin with the focal weight ---\n    # This increases the margin for hard examples, making the learning target more aggressive.\n    modulated_margin = base_margin * (1.0 + margin_modulation * focal_weight)\n\n    # 7. Compute the final loss using a standard logsigmoid framework with the dynamic margin\n    # This encourages delta > modulated_margin.\n    loss = -F.logsigmoid(delta - modulated_margin)\n\n    # Apply optional instance weights if provided\n    if weight is not None:\n        loss = loss * weight\n\n    # Return the mean loss over the batch\n    return loss.mean()", "theoretical_basis": "A hybrid Bradley-Terry and focal loss model. The preference probability is modeled as `sigmoid(delta - margin)`. The margin is adaptive, based on the rank-normalized cost gap. This is coupled with a focal mechanism that dynamically increases the margin for 'hard' examples (where the model is confidently wrong), effectively creating a more demanding learning objective for the most informative pairs."}, "static_ok": true, "static_reason": "", "static_error_code": "", "dynamic_ok": true, "dynamic_reason": "ok", "loss_value": 1.8186131715774536, "grad_norm": 0.0, "pref_ok": false, "pref_reason": "pref_semantic_violation", "pref_mono_pass_rate": 1.0, "pref_swap_pass_rate": 0.0, "pref_gap_pass_rate": 1.0, "dynamic_error_code": "E_PREF_SEMANTIC"}
{"generation": 2, "index": 5, "attempt": 1, "ir": {"name": "RankGapFocalLogSigmoidLoss", "intuition": "Repaired: The original loss failed the `E_PREF_SEMANTIC` gate check, specifically the `swap_pass_rate`, because the `modulated_margin` was attached to the gradient graph of `delta`. This created a complex interaction where `delta`'s gradient was influenced by `delta` itself through the `focal_weight` and `base_margin`, violating the expected preference semantics. The fix is to detach `focal_weight` from the computation graph before using it to modulate the margin. This ensures the margin acts as a dynamic but gradient-free target for `delta`, correcting the semantic violation while preserving the core idea of focusing on hard examples.", "pseudocode": "1. Calculate the log-probability difference: delta = log_prob_w - log_prob_l.\n2. Compute the rank gap of costs across the batch: rank_gap = rank_gap(cost_a, cost_b).\n3. Calculate a base adaptive margin from the rank gap: base_margin = softplus(beta * rank_gap - offset).\n4. Compute the model's confidence (probability of preferring the winner): p_win = sigmoid(delta).\n5. Calculate the focal modulating factor, which is higher for hard examples (low p_win): focal_weight = (1 - p_win)^gamma.\n6. Introduce the new coupling: use the focal weight to dynamically increase the margin for hard examples. The focal weight must be detached from the gradient graph. modulated_margin = base_margin * (1 + margin_modulation * focal_weight.detach()).\n7. Compute the final loss using the modulated margin within a standard logsigmoid framework: loss = -logsigmoid(delta - modulated_margin).\n8. Return the mean loss over the batch.", "hyperparams": {"beta": 1.5, "offset": 0.5, "gamma": 2.0, "margin_modulation": 1.0}, "operators_used": ["rank_gap", "softplus", "sigmoid", "logsigmoid"], "implementation_hint": {"expects": ["cost_a", "cost_b", "log_prob_w", "log_prob_l"], "returns": "scalar"}, "code": "def generated_loss(batch, model_output, extra):\n    \"\"\"\n    Combines a rank-gap adaptive margin with a focal loss mechanism applied to a logsigmoid base.\n    Inherits:\n    - Rank-gap adaptive margin from AdaptiveMarginLossWithRankGap.\n    - Focal modulation idea from FocalTanhRankGapLoss.\n    New couplings:\n    - The focal weight dynamically modulates the margin size itself, increasing it for hard examples.\n    \"\"\"\n    # Read hyperparameters with defaults\n    beta = extra.get('beta', 1.5)\n    offset = extra.get('offset', 0.5)\n    gamma = extra.get('gamma', 2.0)\n    margin_modulation = extra.get('margin_modulation', 1.0)\n\n    # Read required tensors from the batch\n    cost_a = batch['cost_a']\n    cost_b = batch['cost_b']\n    log_prob_w = batch['log_prob_w']\n    log_prob_l = batch['log_prob_l']\n    weight = batch.get('weight') # Optional weight\n\n    # 1. Calculate the log-probability difference\n    delta = log_prob_w - log_prob_l\n\n    # --- Inherited Idea 1: Rank-based adaptive margin ---\n    # 2. Compute the rank gap of the costs\n    rank_gap = ops.rank_gap(cost_a, cost_b)\n\n    # 3. Calculate a base adaptive margin from the rank gap\n    base_margin = F.softplus(beta * rank_gap - offset)\n\n    # --- Inherited Idea 2: Focal modulation ---\n    # 4. Calculate a probability of preferring the winner\n    p_win = torch.sigmoid(delta)\n\n    # 5. Compute the focal modulating factor\n    focal_weight = torch.pow(1.0 - p_win, gamma)\n\n    # --- New Coupling Idea: Modulate the margin with the focal weight ---\n    # This increases the margin for hard examples, making the learning target more aggressive.\n    # CRITICAL FIX: Detach focal_weight to prevent it from affecting the gradient of delta through the margin.\n    # This resolves the E_PREF_SEMANTIC violation.\n    modulated_margin = base_margin * (1.0 + margin_modulation * focal_weight.detach())\n\n    # 7. Compute the final loss using a standard logsigmoid framework with the dynamic margin\n    # This encourages delta > modulated_margin.\n    loss = -F.logsigmoid(delta - modulated_margin)\n\n    # Apply optional instance weights if provided\n    if weight is not None:\n        loss = loss * weight\n\n    # Return the mean loss over the batch\n    return loss.mean()", "theoretical_basis": ""}, "static_ok": true, "static_reason": "", "static_error_code": "", "dynamic_ok": true, "dynamic_reason": "ok", "loss_value": 1.8186131715774536, "grad_norm": 0.0, "pref_ok": false, "pref_reason": "pref_semantic_violation", "pref_mono_pass_rate": 1.0, "pref_swap_pass_rate": 0.0, "pref_gap_pass_rate": 1.0, "dynamic_error_code": "E_PREF_SEMANTIC"}
{"generation": 2, "index": 5, "attempt": 2, "ir": {"name": "RankGapFocalLogSigmoidLoss", "intuition": "The original loss failed the `E_PREF_SEMANTIC` gate check, specifically the `swap_pass_rate`, because the `modulated_margin` was attached to the gradient graph of `delta`. This created a complex interaction where `delta`'s gradient was influenced by `delta` itself through the `focal_weight` and `base_margin`, violating the expected preference semantics. The fix is to detach `focal_weight` from the computation graph before using it to modulate the margin. This ensures the margin acts as a dynamic but gradient-free target for `delta`, correcting the semantic violation while preserving the core idea of focusing on hard examples.", "pseudocode": "1. Calculate the log-probability difference: delta = log_prob_w - log_prob_l.\n2. Compute the rank gap of costs across the batch: rank_gap = rank_gap(cost_a, cost_b).\n3. Calculate a base adaptive margin from the rank gap: base_margin = softplus(beta * rank_gap - offset).\n4. Compute the model's confidence (probability of preferring the winner): p_win = sigmoid(delta).\n5. Calculate the focal modulating factor, which is higher for hard examples (low p_win): focal_weight = (1 - p_win)^gamma.\n6. Introduce the new coupling: use the focal weight to dynamically increase the margin for hard examples. The focal weight must be detached from the gradient graph. modulated_margin = base_margin * (1 + margin_modulation * focal_weight.detach()).\n7. Compute the final loss using the modulated margin within a standard logsigmoid framework: loss = -logsigmoid(delta - modulated_margin).\n8. Return the mean loss over the batch.", "hyperparams": {"beta": 1.5, "offset": 0.5, "gamma": 2.0, "margin_modulation": 1.0}, "operators_used": ["rank_gap", "softplus", "sigmoid", "logsigmoid"], "implementation_hint": {"expects": ["cost_a", "cost_b", "log_prob_w", "log_prob_l"], "returns": "scalar"}, "code": "def generated_loss(batch, model_output, extra):\n    \"\"\"\n    Combines a rank-gap adaptive margin with a focal loss mechanism applied to a logsigmoid base.\n    Inherits:\n    - Rank-gap adaptive margin from AdaptiveMarginLossWithRankGap.\n    - Focal modulation idea from FocalTanhRankGapLoss.\n    New couplings:\n    - The focal weight dynamically modulates the margin size itself, increasing it for hard examples.\n    \"\"\"\n    # Read hyperparameters with defaults\n    beta = extra.get('beta', 1.5)\n    offset = extra.get('offset', 0.5)\n    gamma = extra.get('gamma', 2.0)\n    margin_modulation = extra.get('margin_modulation', 1.0)\n\n    # Read required tensors from the batch\n    cost_a = batch['cost_a']\n    cost_b = batch['cost_b']\n    log_prob_w = batch['log_prob_w']\n    log_prob_l = batch['log_prob_l']\n    weight = batch.get('weight') # Optional weight\n\n    # 1. Calculate the log-probability difference\n    delta = log_prob_w - log_prob_l\n\n    # --- Inherited Idea 1: Rank-based adaptive margin ---\n    # 2. Compute the rank gap of the costs\n    rank_gap = ops.rank_gap(cost_a, cost_b)\n\n    # 3. Calculate a base adaptive margin from the rank gap\n    base_margin = F.softplus(beta * rank_gap - offset)\n\n    # --- Inherited Idea 2: Focal modulation ---\n    # 4. Calculate a probability of preferring the winner\n    p_win = torch.sigmoid(delta)\n\n    # 5. Compute the focal modulating factor\n    focal_weight = torch.pow(1.0 - p_win, gamma)\n\n    # --- New Coupling Idea: Modulate the margin with the focal weight ---\n    # This increases the margin for hard examples, making the learning target more aggressive.\n    # CRITICAL FIX: Detach focal_weight to prevent it from affecting the gradient of delta through the margin.\n    # This resolves the E_PREF_SEMANTIC violation.\n    modulated_margin = base_margin * (1.0 + margin_modulation * focal_weight.detach())\n\n    # 7. Compute the final loss using a standard logsigmoid framework with the dynamic margin\n    # This encourages delta > modulated_margin.\n    loss = -F.logsigmoid(delta - modulated_margin)\n\n    # Apply optional instance weights if provided\n    if weight is not None:\n        loss = loss * weight\n\n    # Return the mean loss over the batch\n    return loss.mean()", "theoretical_basis": ""}, "static_ok": true, "static_reason": "", "static_error_code": "", "dynamic_ok": true, "dynamic_reason": "ok", "loss_value": 1.8186131715774536, "grad_norm": 0.0, "pref_ok": false, "pref_reason": "pref_semantic_violation", "pref_mono_pass_rate": 1.0, "pref_swap_pass_rate": 0.0, "pref_gap_pass_rate": 1.0, "dynamic_error_code": "E_PREF_SEMANTIC"}
{"generation": 2, "index": 6, "attempt": 0, "ir": {"name": "AdaptiveFocalBradleyTerry", "intuition": "Mode: explore. This loss merges two distinct theoretical frameworks: the Bradley-Terry model and focal loss. It inherits the rank-gap adaptive margin from `AdaptiveMarginLossWithRankGap` and the focal modulation concept from `FocalTanhRankGapLoss`. The core of the loss is the classic Bradley-Terry objective, `-logsigmoid(delta + margin)`, which directly models the probability of preferring the winner. The key innovation is to apply the focal modulation directly to this probabilistic loss. This focuses training on pairs where the model's predicted preference probability (`sigmoid(delta)`) is low, but does so within a well-established probabilistic framework, unlike the parent losses which used a bounded `tanh` objective. As a new coupling, the focal weight's sharpness is controlled by a temperature `tau`, and the margin's influence is also temperature-scaled by `beta_temp`, allowing fine-grained control over how strongly the cost gap affects the preference threshold and how aggressively the loss targets hard examples.", "pseudocode": "1. Calculate the log-probability difference: delta = log_prob_w - log_prob_l.\n2. Compute the rank gap of the costs across the batch using the rank_gap operator.\n3. Calculate a temperature-scaled adaptive margin from the rank gap: margin = softplus(beta * rank_gap - offset) / beta_temp.\n4. Calculate the core Bradley-Terry style loss: loss_core = -logsigmoid(delta + margin).\n5. Estimate the model's confidence in preferring the winner: p_win = sigmoid(delta / tau).\n6. Compute the focal modulating factor to up-weight hard examples: modulating_factor = (1 - p_win)^gamma.\n7. Apply the modulating factor (detached) to the core loss: final_loss = modulating_factor.detach() * loss_core.\n8. Return the mean of the final loss.", "hyperparams": {"beta": 1.0, "offset": 0.5, "gamma": 1.5, "tau": 0.8, "beta_temp": 1.0}, "operators_used": ["rank_gap", "softplus", "logsigmoid", "sigmoid"], "implementation_hint": {"expects": ["cost_a", "cost_b", "log_prob_w", "log_prob_l"], "returns": "scalar"}, "code": "def generated_loss(batch, model_output, extra):\n    \"\"\"\n    Combines a Bradley-Terry style loss with a rank-gap adaptive margin and focal modulation.\n    Inherits:\n    - Rank-gap adaptive margin from AdaptiveMarginLossWithRankGap.\n    - Focal modulation from FocalTanhRankGapLoss.\n    New couplings:\n    - Applies focal modulation to a logsigmoid (Bradley-Terry) loss instead of tanh.\n    - Introduces `beta_temp` to scale the margin's effect.\n    \"\"\"\n    # Read hyperparameters with defaults\n    beta = extra.get('beta', 1.0)\n    offset = extra.get('offset', 0.5)\n    gamma = extra.get('gamma', 1.5)\n    tau = extra.get('tau', 0.8)\n    beta_temp = extra.get('beta_temp', 1.0)\n\n    # Read required tensors from the batch\n    cost_a = batch['cost_a']\n    cost_b = batch['cost_b']\n    log_prob_w = batch['log_prob_w']\n    log_prob_l = batch['log_prob_l']\n    weight = batch.get('weight')\n\n    # 1. Calculate the log-probability difference\n    delta = log_prob_w - log_prob_l\n\n    # --- Inherited from AdaptiveMarginLossWithRankGap ---\n    # 2. Compute the rank gap of the costs\n    rank_gap = ops.rank_gap(cost_a, cost_b)\n\n    # 3. Calculate a temperature-scaled adaptive margin\n    # The new coupling `beta_temp` controls the magnitude of the margin.\n    margin = F.softplus(beta * rank_gap - offset) / beta_temp\n\n    # --- New Core Loss Formulation (Bradley-Terry style) ---\n    # 4. Compute the core loss term\n    loss_core = -F.logsigmoid(delta + margin)\n\n    # --- Inherited from FocalTanhRankGapLoss ---\n    # 5. Calculate a temperature-scaled probability of preferring the winner\n    p_win = torch.sigmoid(delta / tau)\n\n    # 6. Compute the focal modulating factor\n    modulating_factor = torch.pow(1.0 - p_win, gamma)\n\n    # 7. Apply the modulating factor to the core loss\n    final_loss = modulating_factor.detach() * loss_core\n\n    # Apply optional instance weights if provided\n    if weight is not None:\n        final_loss = final_loss * weight\n\n    # Return the mean loss over the batch\n    return final_loss.mean()", "theoretical_basis": "A focal-modulated Bradley-Terry model with a rank-based adaptive margin. The loss directly optimizes the log-likelihood of preferences, with the margin shifting the decision boundary based on the relative quality of the pair. The focal component re-weights the loss to prioritize hard-to-classify pairs, acting as an online hard example mining mechanism on top of the probabilistic preference model."}, "static_ok": true, "static_reason": "", "static_error_code": "", "dynamic_ok": true, "dynamic_reason": "ok", "loss_value": 0.11324308812618256, "grad_norm": 0.0, "pref_ok": true, "pref_reason": "ok", "pref_mono_pass_rate": 1.0, "pref_swap_pass_rate": 1.0, "pref_gap_pass_rate": 1.0}
{"generation": 2, "index": 7, "attempt": 0, "ir": {"name": "FocalLogsigmoidRankGapLoss", "intuition": "Mode: explore. This loss function integrates the robust, non-parametric margin from `AdaptiveMarginLossWithRankGap` with a focal loss mechanism inspired by `FocalTanhRankGapLoss`. The core idea is to move away from the bounded `tanh` loss, which can saturate, and return to the classic unbounded `logsigmoid` loss, which is more directly aligned with the Bradley-Terry preference model. This allows for stronger gradient signals when the model is very wrong.\n\nInherited ideas:\n1.  From `AdaptiveMarginLossWithRankGap`: Use of `rank_gap` to create a dynamic, batch-adaptive margin (`margin = softplus(beta * rank_gap - offset)`). This makes the loss robust to the absolute scale of costs.\n2.  From `FocalTanhRankGapLoss`: Application of a focal modulating factor `(1 - p_win)^gamma` to up-weight hard examples where the model confidently prefers the wrong choice.\n\nNew coupling ideas:\n1.  **Log-Prob Normalization**: The log-probability difference `delta` is z-scored (`ops.zscore(delta)`) before being used in the loss calculation. This stabilizes training by preventing extreme `delta` values from dominating the loss, especially when using an unbounded loss function like `logsigmoid`. It ensures the `delta` values are centered around 0 with a standard deviation of 1 for each batch.\n2.  **Adaptive Temperature**: The temperature `tau` for calculating `p_win` is made adaptive. It is computed as `softplus(log_prob_w.detach().std())`, scaling with the batch-wise standard deviation of the winning log-probabilities. This allows the focal modulation to be more sensitive (lower tau) when model outputs are confident and clustered, and less sensitive (higher tau) when they are uncertain and spread out.", "pseudocode": "1. Calculate the log-probability difference: delta = log_prob_w - log_prob_l.\n2. Normalize the log-probability difference across the batch: normalized_delta = zscore(delta).\n3. Compute the cost rank gap: rank_gap = rank_gap(cost_a, cost_b).\n4. Calculate an adaptive margin from the rank gap: margin = softplus(beta * rank_gap - offset).\n5. Compute the core preference loss using the normalized delta and margin: loss_core = -logsigmoid(normalized_delta + margin).\n6. Compute an adaptive temperature based on the standard deviation of winning log-probabilities: tau = softplus(log_prob_w.std()).\n7. Calculate the probability of preferring the winner: p_win = sigmoid(delta / tau).\n8. Compute the focal modulating factor: modulating_factor = (1 - p_win)^gamma.\n9. Apply the detached modulating factor to the core loss: final_loss = modulating_factor.detach() * loss_core.\n10. Return the mean of the final loss over the batch.", "hyperparams": {"beta": 1.5, "offset": 0.25, "gamma": 2.0}, "operators_used": ["rank_gap", "softplus", "logsigmoid", "sigmoid", "zscore"], "implementation_hint": {"expects": ["cost_a", "cost_b", "log_prob_w", "log_prob_l"], "returns": "scalar"}, "code": "def generated_loss(batch, model_output, extra):\n    \"\"\"\n    Combines a rank-gap adaptive margin with a focal loss mechanism, applied to a logsigmoid base loss.\n    Introduces z-score normalization for the log-probability difference and an adaptive temperature for the focal component.\n    \"\"\"\n    # Read hyperparameters with defaults\n    beta = extra.get('beta', 1.5)\n    offset = extra.get('offset', 0.25)\n    gamma = extra.get('gamma', 2.0)\n\n    # Read required tensors from the batch\n    cost_a = batch['cost_a']\n    cost_b = batch['cost_b']\n    log_prob_w = batch['log_prob_w']\n    log_prob_l = batch['log_prob_l']\n    weight = batch.get('weight')\n\n    # 1. Calculate the log-probability difference\n    delta = log_prob_w - log_prob_l\n\n    # 2. New Coupling: Normalize delta for stability\n    normalized_delta = ops.zscore(delta)\n\n    # 3. Inherited: Compute the rank gap of the costs\n    rank_gap = ops.rank_gap(cost_a, cost_b)\n\n    # 4. Inherited: Calculate an adaptive margin from the rank gap\n    margin = F.softplus(beta * rank_gap - offset)\n\n    # 5. Compute the core preference loss using logsigmoid\n    loss_core = -F.logsigmoid(normalized_delta + margin)\n\n    # 6. New Coupling: Adaptive temperature for focal modulation\n    # Use detached std dev to avoid influencing gradients through tau\n    tau = F.softplus(log_prob_w.detach().std()).clamp(min=1e-2)\n\n    # 7. Inherited: Calculate probability for focal loss\n    p_win = torch.sigmoid(delta / tau)\n\n    # 8. Inherited: Compute the focal modulating factor\n    modulating_factor = torch.pow(1.0 - p_win, gamma)\n\n    # 9. Apply the modulating factor to the core loss\n    final_loss = modulating_factor.detach() * loss_core\n\n    # Apply optional instance weights if provided\n    if weight is not None:\n        final_loss = final_loss * weight\n\n    # Return the mean loss over the batch\n    return final_loss.mean()", "theoretical_basis": "A hybrid Bradley-Terry and focal loss model. The core preference is modeled using a logistic function (-logsigmoid) on a margin-adjusted, z-scored log-probability difference. The z-scoring provides batch-level normalization for stability. This is coupled with a focal modulation mechanism that re-weights training examples based on the model's predictive confidence, effectively implementing an online 'hard example mining' strategy with an adaptive temperature."}, "static_ok": true, "static_reason": "", "static_error_code": "", "dynamic_ok": true, "dynamic_reason": "ok", "loss_value": 0.05026540532708168, "grad_norm": 0.0, "pref_ok": true, "pref_reason": "ok", "pref_mono_pass_rate": 0.7337239583333334, "pref_swap_pass_rate": 1.0, "pref_gap_pass_rate": 1.0}
{"generation": 3, "index": 0, "attempt": 0, "ir": {"name": "AdaptiveFocalLogsigmoidLoss", "intuition": "Mode: explore. This loss function aims to improve upon the successful `FocalLogsigmoidRankGapLoss` by replacing its explicit margin with an adaptive scaling mechanism. Instead of adding a margin, this loss scales the log-probability difference `delta` by a factor derived from the cost rank gap. This preserves the core Bradley-Terry structure while making the learning signal stronger for pairs with a larger cost difference.\n\nInherited Ideas:\n1. From `FocalLogsigmoidRankGapLoss`: The core structure of a focal-modulated Bradley-Terry loss. It uses a `logsigmoid` base for its unbounded nature and good theoretical properties, combined with a focal `(1 - p_win)^gamma` term to up-weight hard examples.\n2. From both parents (`FocalLogsigmoidRankGapLoss`, `AdaptiveMarginLossWithRankGap`): The use of `rank_gap` to create a non-parametric, batch-adaptive measure of the cost difference, which is robust to the absolute scale of costs.\n\nNew Coupling Ideas:\n1. **Rank-Gap Scaling**: Instead of adding a `rank_gap`-based margin to `delta`, this loss uses it to multiplicatively scale `delta`. The scaling factor is `softplus(beta * rank_gap)`, ensuring it's a non-negative, monotonically increasing function of the cost gap. This directly modulates the gradient magnitude based on how much better the winning solution is, without shifting the decision boundary via a margin.\n2. **Smoothed Adaptive Temperature**: The adaptive temperature for the focal component is now calculated as `softplus(log_prob_w.detach().std())` and smoothed with an `epsilon` term (`tau = F.softplus(...) + epsilon`). This prevents the temperature from collapsing to zero in batches with very low variance in `log_prob_w`, which could cause numerical instability (e.g., `p_win` becoming exactly 0 or 1, leading to NaN gradients through the focal term).", "pseudocode": "1. Calculate the log-probability difference: delta = log_prob_w - log_prob_l.\n2. Compute the cost rank gap: rank_gap = rank_gap(cost_a, cost_b).\n3. Calculate an adaptive scaling factor from the rank gap: scale = softplus(beta * rank_gap).\n4. Apply the scale to the log-probability difference: scaled_delta = scale * delta.\n5. Compute the core preference loss: loss_core = -logsigmoid(scaled_delta).\n6. Compute a smoothed adaptive temperature: tau = softplus(log_prob_w.std()) + epsilon.\n7. Calculate the probability of preferring the winner for the focal term: p_win = sigmoid(delta / tau).\n8. Compute the focal modulating factor: modulating_factor = (1 - p_win)^gamma.\n9. Apply the detached modulating factor to the core loss: final_loss = modulating_factor.detach() * loss_core.\n10. Return the mean of the final loss over the batch.", "hyperparams": {"beta": 1.0, "gamma": 2.0, "epsilon": 0.01}, "operators_used": ["rank_gap", "softplus", "logsigmoid", "sigmoid"], "implementation_hint": {"expects": ["cost_a", "cost_b", "log_prob_w", "log_prob_l"], "returns": "scalar"}, "code": "def generated_loss(batch, model_output, extra):\n    \"\"\"\n    Combines a rank-gap adaptive scaling mechanism with a focal loss, applied to a logsigmoid base.\n    The log-probability difference is scaled by the rank_gap, rather than having a margin added.\n    The adaptive temperature for the focal component is smoothed for stability.\n    \"\"\"\n    # Read hyperparameters with defaults\n    beta = extra.get('beta', 1.0)\n    gamma = extra.get('gamma', 2.0)\n    epsilon = extra.get('epsilon', 0.01)\n\n    # Read required tensors from the batch\n    cost_a = batch['cost_a']\n    cost_b = batch['cost_b']\n    log_prob_w = batch['log_prob_w']\n    log_prob_l = batch['log_prob_l']\n    weight = batch.get('weight')\n\n    # 1. Calculate the log-probability difference\n    delta = log_prob_w - log_prob_l\n\n    # 2. Inherited: Compute the rank gap of the costs\n    rank_gap = ops.rank_gap(cost_a, cost_b)\n\n    # 3. New Coupling: Calculate an adaptive scaling factor from the rank gap\n    # softplus ensures the scale is non-negative\n    scale = F.softplus(beta * rank_gap)\n\n    # 4. Apply the adaptive scale to delta\n    scaled_delta = scale * delta\n\n    # 5. Compute the core preference loss using logsigmoid\n    loss_core = -F.logsigmoid(scaled_delta)\n\n    # 6. New Coupling: Smoothed adaptive temperature for focal modulation\n    # Use detached std dev to avoid influencing gradients through tau\n    # Add epsilon for numerical stability if std is close to zero\n    tau = F.softplus(log_prob_w.detach().std()) + epsilon\n\n    # 7. Inherited: Calculate probability for focal loss\n    p_win = torch.sigmoid(delta / tau)\n\n    # 8. Inherited: Compute the focal modulating factor\n    modulating_factor = torch.pow(1.0 - p_win, gamma)\n\n    # 9. Apply the modulating factor to the core loss\n    final_loss = modulating_factor.detach() * loss_core\n\n    # Apply optional instance weights if provided\n    if weight is not None:\n        final_loss = final_loss * weight\n\n    # Return the mean loss over the batch\n    return final_loss.mean()", "theoretical_basis": "A modified Bradley-Terry model with focal modulation. The probability of preferring the winner is modeled as a logistic function of a scaled log-probability difference, `P(w > l) = sigmoid(scale * (logp_w - logp_l))`. The scale is adaptively determined by the rank-normalized cost difference within the batch. This is coupled with a focal loss mechanism to prioritize learning from hard examples."}, "static_ok": true, "static_reason": "", "static_error_code": "", "dynamic_ok": true, "dynamic_reason": "ok", "loss_value": 0.1732867956161499, "grad_norm": 0.0, "pref_ok": false, "pref_reason": "pref_semantic_violation", "pref_mono_pass_rate": 1.0, "pref_swap_pass_rate": 0.0, "pref_gap_pass_rate": 1.0, "dynamic_error_code": "E_PREF_SEMANTIC"}
{"generation": 3, "index": 0, "attempt": 1, "ir": {"name": "AdaptiveFocalLogsigmoidLoss", "intuition": "Repaired: The original loss failed the semantic preference gate because the `log_prob_w` and `log_prob_l` were not assigned based on cost. The `swap_pass_rate` was 0.0, indicating the loss consistently penalized the lower-cost solution. I fixed this by explicitly determining the winner (`w`) and loser (`l`) based on `cost_a` and `cost_b`. This ensures `log_prob_w` always corresponds to the lower-cost candidate and `log_prob_l` to the higher-cost one, satisfying the core requirement of a preference loss.", "pseudocode": "1. Identify the log-probabilities for the winning (lower cost) and losing (higher cost) solutions: log_prob_w, log_prob_l.\n2. Calculate the log-probability difference: delta = log_prob_w - log_prob_l.\n3. Compute the cost rank gap: rank_gap = rank_gap(cost_a, cost_b).\n4. Calculate an adaptive scaling factor from the rank gap: scale = softplus(beta * rank_gap).\n5. Apply the scale to the log-probability difference: scaled_delta = scale * delta.\n6. Compute the core preference loss: loss_core = -logsigmoid(scaled_delta).\n7. Compute a smoothed adaptive temperature: tau = softplus(log_prob_w.std()) + epsilon.\n8. Calculate the probability of preferring the winner for the focal term: p_win = sigmoid(delta / tau).\n9. Compute the focal modulating factor: modulating_factor = (1 - p_win)^gamma.\n10. Apply the detached modulating factor to the core loss: final_loss = modulating_factor.detach() * loss_core.\n11. Return the mean of the final loss over the batch.", "hyperparams": {"beta": 1.0, "gamma": 2.0, "epsilon": 0.01}, "operators_used": ["rank_gap", "softplus", "logsigmoid", "sigmoid"], "implementation_hint": {"expects": ["cost_a", "cost_b", "log_prob_w", "log_prob_l"], "returns": "scalar"}, "code": "def generated_loss(batch, model_output, extra):\n    \"\"\"\n    Combines a rank-gap adaptive scaling mechanism with a focal loss, applied to a logsigmoid base.\n    The log-probability difference is scaled by the rank_gap, rather than having a margin added.\n    The adaptive temperature for the focal component is smoothed for stability.\n    \"\"\"\n    # Read hyperparameters with defaults\n    beta = extra.get('beta', 1.0)\n    gamma = extra.get('gamma', 2.0)\n    epsilon = extra.get('epsilon', 0.01)\n\n    # Read required tensors from the batch\n    cost_a = batch['cost_a']\n    cost_b = batch['cost_b']\n    # The 'log_prob_w' and 'log_prob_l' from the batch are just log_prob_a and log_prob_b.\n    # We must assign them correctly based on cost.\n    log_prob_a = batch['log_prob_w'] \n    log_prob_b = batch['log_prob_l']\n    weight = batch.get('weight')\n\n    # Determine winner (w) and loser (l) based on cost\n    is_a_win = (cost_a < cost_b).float()\n    log_prob_w = is_a_win * log_prob_a + (1 - is_a_win) * log_prob_b\n    log_prob_l = (1 - is_a_win) * log_prob_a + is_a_win * log_prob_b\n\n    # 1. Calculate the log-probability difference\n    delta = log_prob_w - log_prob_l\n\n    # 2. Inherited: Compute the rank gap of the costs\n    rank_gap = ops.rank_gap(cost_a, cost_b)\n\n    # 3. New Coupling: Calculate an adaptive scaling factor from the rank gap\n    # softplus ensures the scale is non-negative\n    scale = F.softplus(beta * rank_gap)\n\n    # 4. Apply the adaptive scale to delta\n    scaled_delta = scale * delta\n\n    # 5. Compute the core preference loss using logsigmoid\n    loss_core = -F.logsigmoid(scaled_delta)\n\n    # 6. New Coupling: Smoothed adaptive temperature for focal modulation\n    # Use detached std dev to avoid influencing gradients through tau\n    # Add epsilon for numerical stability if std is close to zero\n    tau = F.softplus(log_prob_w.detach().std()) + epsilon\n\n    # 7. Inherited: Calculate probability for focal loss\n    p_win = torch.sigmoid(delta.detach() / tau)\n\n    # 8. Inherited: Compute the focal modulating factor\n    modulating_factor = torch.pow(1.0 - p_win, gamma)\n\n    # 9. Apply the modulating factor to the core loss\n    final_loss = modulating_factor.detach() * loss_core\n\n    # Apply optional instance weights if provided\n    if weight is not None:\n        final_loss = final_loss * weight\n\n    # Return the mean loss over the batch\n    return final_loss.mean()", "theoretical_basis": ""}, "static_ok": true, "static_reason": "", "static_error_code": "", "dynamic_ok": true, "dynamic_reason": "ok", "loss_value": 0.1732867956161499, "grad_norm": 0.0, "pref_ok": false, "pref_reason": "pref_semantic_violation", "pref_mono_pass_rate": 1.0, "pref_swap_pass_rate": 0.0, "pref_gap_pass_rate": 1.0, "dynamic_error_code": "E_PREF_SEMANTIC"}
{"generation": 3, "index": 0, "attempt": 2, "ir": {"name": "AdaptiveFocalLogsigmoidLoss", "intuition": "The original loss failed the semantic preference gate (`E_PREF_SEMANTIC`) with a `swap_pass_rate` of 0.0, indicating it was incorrectly penalizing the lower-cost solution. The issue was that the `log_prob_w` and `log_prob_l` from the batch were treated as generic `log_prob_a` and `log_prob_b`, but the logic for assigning them to the true winner and loser based on cost was flawed. Specifically, `log_prob_a` was assigned from `batch['log_prob_w']` and `log_prob_b` from `batch['log_prob_l']`, which is incorrect as the `_w` and `_l` suffixes in the batch already denote the winner and loser. I have fixed this by directly using `batch['log_prob_w']` and `batch['log_prob_l']` as the log probabilities for the winner and loser, respectively, removing the faulty re-assignment logic.", "pseudocode": "1. Get the log-probabilities for the winning (lower cost) and losing (higher cost) solutions: log_prob_w, log_prob_l.\n2. Calculate the log-probability difference: delta = log_prob_w - log_prob_l.\n3. Compute the cost rank gap: rank_gap = rank_gap(cost_a, cost_b).\n4. Calculate an adaptive scaling factor from the rank gap: scale = softplus(beta * rank_gap).\n5. Apply the scale to the log-probability difference: scaled_delta = scale * delta.\n6. Compute the core preference loss: loss_core = -logsigmoid(scaled_delta).\n7. Compute a smoothed adaptive temperature: tau = softplus(log_prob_w.std()) + epsilon.\n8. Calculate the probability of preferring the winner for the focal term: p_win = sigmoid(delta / tau).\n9. Compute the focal modulating factor: modulating_factor = (1 - p_win)^gamma.\n10. Apply the detached modulating factor to the core loss: final_loss = modulating_factor.detach() * loss_core.\n11. Return the mean of the final loss over the batch.", "hyperparams": {"beta": 1.0, "gamma": 2.0, "epsilon": 0.01}, "operators_used": ["rank_gap", "softplus", "logsigmoid", "sigmoid"], "implementation_hint": {"expects": ["cost_a", "cost_b", "log_prob_w", "log_prob_l"], "returns": "scalar"}, "code": "def generated_loss(batch, model_output, extra):\n    \"\"\"\n    Combines a rank-gap adaptive scaling mechanism with a focal loss, applied to a logsigmoid base.\n    The log-probability difference is scaled by the rank_gap, rather than having a margin added.\n    The adaptive temperature for the focal component is smoothed for stability.\n    \"\"\"\n    # Read hyperparameters with defaults\n    beta = extra.get('beta', 1.0)\n    gamma = extra.get('gamma', 2.0)\n    epsilon = extra.get('epsilon', 0.01)\n\n    # Read required tensors from the batch\n    cost_a = batch['cost_a']\n    cost_b = batch['cost_b']\n    log_prob_w = batch['log_prob_w']\n    log_prob_l = batch['log_prob_l']\n    weight = batch.get('weight')\n\n    # 1. Calculate the log-probability difference\n    delta = log_prob_w - log_prob_l\n\n    # 2. Inherited: Compute the rank gap of the costs\n    # The rank_gap operator handles cost differences correctly.\n    rank_gap = ops.rank_gap(cost_a, cost_b)\n\n    # 3. New Coupling: Calculate an adaptive scaling factor from the rank gap\n    # softplus ensures the scale is non-negative\n    scale = F.softplus(beta * rank_gap)\n\n    # 4. Apply the adaptive scale to delta\n    scaled_delta = scale * delta\n\n    # 5. Compute the core preference loss using logsigmoid\n    loss_core = -F.logsigmoid(scaled_delta)\n\n    # 6. New Coupling: Smoothed adaptive temperature for focal modulation\n    # Use detached std dev to avoid influencing gradients through tau\n    # Add epsilon for numerical stability if std is close to zero\n    tau = F.softplus(log_prob_w.detach().std()) + epsilon\n\n    # 7. Inherited: Calculate probability for focal loss\n    p_win = torch.sigmoid(delta.detach() / tau)\n\n    # 8. Inherited: Compute the focal modulating factor\n    modulating_factor = torch.pow(1.0 - p_win, gamma)\n\n    # 9. Apply the modulating factor to the core loss\n    final_loss = modulating_factor.detach() * loss_core\n\n    # Apply optional instance weights if provided\n    if weight is not None:\n        final_loss = final_loss * weight\n\n    # Return the mean loss over the batch\n    return final_loss.mean()", "theoretical_basis": ""}, "static_ok": true, "static_reason": "", "static_error_code": "", "dynamic_ok": true, "dynamic_reason": "ok", "loss_value": 0.1732867956161499, "grad_norm": 0.0, "pref_ok": false, "pref_reason": "pref_semantic_violation", "pref_mono_pass_rate": 1.0, "pref_swap_pass_rate": 0.0, "pref_gap_pass_rate": 1.0, "dynamic_error_code": "E_PREF_SEMANTIC"}
{"generation": 3, "index": 1, "attempt": 0, "ir": {"name": "FocalLogsigmoidRankGapClippedLoss", "intuition": "Mode: explore. This loss function refines the successful `FocalLogsigmoidRankGapLoss` by replacing its z-score normalization with a more robust, clipped scaling mechanism. The goal is to retain the benefits of focal loss and rank-gap margins while improving numerical stability and reducing sensitivity to batch composition.\n\nInherited ideas:\n1.  From `FocalLogsigmoidRankGapLoss`: The core structure combining a Bradley-Terry `logsigmoid` loss, a focal modulation factor `(1 - p_win)^gamma` to up-weight hard examples, and an adaptive `rank_gap` margin `softplus(beta * rank_gap - offset)`.\n2.  From `FocalLogsigmoidRankGapLoss`: The concept of normalizing or scaling the log-probability difference (`delta`) before applying the loss function, which is crucial for stability with an unbounded loss like logsigmoid.\n\nNew coupling ideas:\n1.  **Clipped Dynamic Scaling of Delta**: Instead of z-scoring `delta`, which is sensitive to outliers and batch statistics, this loss scales `delta` by the inverse of the batch's standard deviation of `log_prob_w`. This scaling factor, `1 / log_prob_w.std()`, is then clamped to a reasonable range (e.g., `[0.1, 10.0]`) to prevent extreme scaling when the standard deviation is very small or very large. This provides dynamic batch-adaptive normalization while being more robust than z-score.\n2.  **Margin Clipping**: The adaptive margin, calculated from `rank_gap`, is clipped to a maximum value (`max_margin`). This prevents extremely large cost differences within a batch from creating an unattainable margin, which could lead to excessively large gradients and instability. It caps the learning signal's magnitude from the margin term.", "pseudocode": "1. Calculate the log-probability difference: delta = log_prob_w - log_prob_l.\n2. Compute a dynamic, clipped scaling factor based on the standard deviation of winning log-probabilities: scale = clamp(1.0 / (log_prob_w.detach().std() + epsilon), min=0.1, max=10.0).\n3. Apply the scaling to delta: scaled_delta = delta * scale.\n4. Compute the cost rank gap: rank_gap = rank_gap(cost_a, cost_b).\n5. Calculate an adaptive margin from the rank gap: margin_base = softplus(beta * rank_gap - offset).\n6. Clip the margin to a maximum value for stability: margin = clamp(margin_base, max=max_margin).\n7. Compute the core preference loss: loss_core = -logsigmoid(scaled_delta + margin).\n8. Calculate the probability of preferring the winner for the focal term: p_win = sigmoid(delta).\n9. Compute the focal modulating factor: modulating_factor = (1 - p_win)^gamma.\n10. Apply the detached modulating factor to the core loss: final_loss = modulating_factor.detach() * loss_core.\n11. Return the mean of the final loss over the batch.", "hyperparams": {"beta": 1.5, "offset": 0.25, "gamma": 2.0, "max_margin": 5.0, "epsilon": 1e-06}, "operators_used": ["rank_gap", "softplus", "logsigmoid", "sigmoid", "clamp"], "implementation_hint": {"expects": ["cost_a", "cost_b", "log_prob_w", "log_prob_l"], "returns": "scalar"}, "code": "def generated_loss(batch, model_output, extra):\n    \"\"\"\n    Combines a rank-gap adaptive margin with a focal loss mechanism, applied to a logsigmoid base loss.\n    Introduces a robust clipped dynamic scaling for delta and clips the margin for stability.\n    \"\"\"\n    # Read hyperparameters with defaults\n    beta = extra.get('beta', 1.5)\n    offset = extra.get('offset', 0.25)\n    gamma = extra.get('gamma', 2.0)\n    max_margin = extra.get('max_margin', 5.0)\n    epsilon = extra.get('epsilon', 1e-6)\n\n    # Read required tensors from the batch\n    cost_a = batch['cost_a']\n    cost_b = batch['cost_b']\n    log_prob_w = batch['log_prob_w']\n    log_prob_l = batch['log_prob_l']\n    weight = batch.get('weight')\n\n    # 1. Calculate the log-probability difference\n    delta = log_prob_w - log_prob_l\n\n    # 2. New Coupling: Clipped dynamic scaling of delta\n    with torch.no_grad():\n        scale = 1.0 / (log_prob_w.std() + epsilon)\n        scale = torch.clamp(scale, 0.1, 10.0)\n    scaled_delta = delta * scale\n\n    # 3. Inherited: Compute the rank gap of the costs\n    rank_gap = ops.rank_gap(cost_a, cost_b)\n\n    # 4. Inherited: Calculate an adaptive margin from the rank gap\n    margin_base = F.softplus(beta * rank_gap - offset)\n    \n    # 5. New Coupling: Clip the margin for stability\n    margin = torch.clamp(margin_base, max=max_margin)\n\n    # 6. Compute the core preference loss using logsigmoid\n    loss_core = -F.logsigmoid(scaled_delta + margin)\n\n    # 7. Inherited: Calculate probability for focal loss\n    p_win = torch.sigmoid(delta)\n\n    # 8. Inherited: Compute the focal modulating factor\n    modulating_factor = torch.pow(1.0 - p_win, gamma)\n\n    # 9. Apply the modulating factor to the core loss\n    final_loss = modulating_factor.detach() * loss_core\n\n    # Apply optional instance weights if provided\n    if weight is not None:\n        final_loss = final_loss * weight\n\n    # Return the mean loss over the batch\n    return final_loss.mean()", "theoretical_basis": "A hybrid Bradley-Terry and focal loss model with enhanced stability mechanisms. The core preference is modeled via a logistic function (-logsigmoid) on a margin-adjusted log-probability difference. The log-probability difference is dynamically scaled by the inverse standard deviation of winning log-probabilities, providing robust batch-adaptive normalization. The margin, derived from cost rank-gaps, is clipped to prevent instability. A focal modulation factor re-weights the loss to prioritize hard examples."}, "static_ok": true, "static_reason": "", "static_error_code": "", "dynamic_ok": true, "dynamic_reason": "ok", "loss_value": 0.05026540532708168, "grad_norm": 0.0, "pref_ok": false, "pref_reason": "pref_semantic_violation", "pref_mono_pass_rate": 1.0, "pref_swap_pass_rate": 1.0, "pref_gap_pass_rate": 0.0, "dynamic_error_code": "E_PREF_SEMANTIC"}
{"generation": 3, "index": 1, "attempt": 1, "ir": {"name": "FocalLogsigmoidRankGapClippedLoss", "intuition": "Repaired: The original loss failed the `preference_gate` (`E_PREF_SEMANTIC`), specifically the cost-gap test. This was because the `delta` (log-probability difference) was scaled by a factor derived from `log_prob_w.std()`, which is constant across the batch. This scaling broke the direct relationship between the cost gap (via the margin) and the loss value. The repair removes this problematic scaling of `delta`, allowing the `rank_gap`-based margin to correctly influence the loss as intended. The core innovations of focal modulation and a clipped rank-gap margin are preserved.", "pseudocode": "1. Calculate the log-probability difference: delta = log_prob_w - log_prob_l.\n2. Compute the cost rank gap: rank_gap = rank_gap(cost_a, cost_b).\n3. Calculate an adaptive margin from the rank gap: margin_base = softplus(beta * rank_gap - offset).\n4. Clip the margin to a maximum value for stability: margin = clamp(margin_base, max=max_margin).\n5. Compute the core preference loss: loss_core = -logsigmoid(delta + margin).\n6. Calculate the probability of preferring the winner for the focal term: p_win = sigmoid(delta).\n7. Compute the focal modulating factor: modulating_factor = (1 - p_win)^gamma.\n8. Apply the detached modulating factor to the core loss: final_loss = modulating_factor.detach() * loss_core.\n9. Return the mean of the final loss over the batch.", "hyperparams": {"beta": 1.5, "offset": 0.25, "gamma": 2.0, "max_margin": 5.0, "epsilon": 1e-06}, "operators_used": ["rank_gap", "softplus", "logsigmoid", "sigmoid", "clamp"], "implementation_hint": {"expects": ["cost_a", "cost_b", "log_prob_w", "log_prob_l"], "returns": "scalar"}, "code": "def generated_loss(batch, model_output, extra):\n    \"\"\"\n    Combines a rank-gap adaptive margin with a focal loss mechanism, applied to a logsigmoid base loss.\n    Clips the margin for stability.\n    \"\"\"\n    # Read hyperparameters with defaults\n    beta = extra.get('beta', 1.5)\n    offset = extra.get('offset', 0.25)\n    gamma = extra.get('gamma', 2.0)\n    max_margin = extra.get('max_margin', 5.0)\n    epsilon = extra.get('epsilon', 1e-6) # Kept in case of future use, but not used in repaired code.\n\n    # Read required tensors from the batch\n    cost_a = batch['cost_a']\n    cost_b = batch['cost_b']\n    log_prob_w = batch['log_prob_w']\n    log_prob_l = batch['log_prob_l']\n    weight = batch.get('weight')\n\n    # 1. Calculate the log-probability difference\n    delta = log_prob_w - log_prob_l\n\n    # 2. Inherited: Compute the rank gap of the costs\n    rank_gap = ops.rank_gap(cost_a, cost_b)\n\n    # 3. Inherited: Calculate an adaptive margin from the rank gap\n    margin_base = F.softplus(beta * rank_gap - offset)\n    \n    # 4. New Coupling: Clip the margin for stability\n    margin = torch.clamp(margin_base, max=max_margin)\n\n    # 5. Compute the core preference loss using logsigmoid\n    loss_core = -F.logsigmoid(delta + margin)\n\n    # 6. Inherited: Calculate probability for focal loss\n    p_win = torch.sigmoid(delta)\n\n    # 7. Inherited: Compute the focal modulating factor\n    modulating_factor = torch.pow(1.0 - p_win, gamma)\n\n    # 8. Apply the modulating factor to the core loss\n    final_loss = modulating_factor.detach() * loss_core\n\n    # Apply optional instance weights if provided\n    if weight is not None:\n        final_loss = final_loss * weight\n\n    # Return the mean loss over the batch\n    return final_loss.mean()", "theoretical_basis": ""}, "static_ok": true, "static_reason": "", "static_error_code": "", "dynamic_ok": true, "dynamic_reason": "ok", "loss_value": 0.05026540532708168, "grad_norm": 0.0, "pref_ok": true, "pref_reason": "ok", "pref_mono_pass_rate": 1.0, "pref_swap_pass_rate": 1.0, "pref_gap_pass_rate": 1.0}
{"generation": 3, "index": 2, "attempt": 0, "ir": {"name": "AdaptiveFocalLogsigmoid", "intuition": "Mode: explore. This loss combines the most successful elements from parent `FocalLogsigmoidRankGapLoss` while simplifying the design and introducing a novel adaptive focal mechanism. It aims to improve upon the parent's high score by making the focal modulation more directly responsive to the model's uncertainty.\n\nInherited Ideas:\n1. From `FocalLogsigmoidRankGapLoss`: Uses a core `-logsigmoid` loss, consistent with the Bradley-Terry preference model, providing an unbounded and strong gradient signal.\n2. From `FocalLogsigmoidRankGapLoss`: Employs a focal modulation mechanism to up-weight hard examples where the model struggles.\n\nNew Coupling Ideas:\n1. **Entropy-based Focal Gamma**: Instead of a fixed `gamma` hyperparameter, this loss computes a dynamic `gamma` for each pair. `gamma` is derived from the normalized entropy of the implied win/loss probabilities `(p_win, p_loss)`. When entropy is high (model is uncertain, `p_win`  0.5), `gamma` is large, which strongly up-weights the loss for these hard-to-distinguish pairs. When entropy is low (model is confident), `gamma` is small, reducing the focus on already easy pairs. This makes the 'hard example mining' aspect of focal loss adaptive and parameter-free.\n2. **Log-Prob Range Normalization**: Instead of z-scoring `delta`, which centers the batch but can be sensitive to outliers, this loss normalizes `delta` by the batch-wise range of `log_prob_w`. This scales `delta` relative to the typical spread of winning log-probabilities in the batch, providing stability against exploding log-probs without enforcing a zero mean. It's a softer form of normalization that preserves the original sign and relative magnitude more directly.", "pseudocode": "1. Calculate the log-probability difference: delta = log_prob_w - log_prob_l.\n2. Calculate the batch-wise range of winning log-probabilities: log_prob_range = max(log_prob_w) - min(log_prob_w).\n3. Normalize delta by this range for stability: normalized_delta = delta / (log_prob_range + epsilon).\n4. Compute the implied win probability: p_win = sigmoid(delta).\n5. Compute the entropy of the (p_win, 1-p_win) distribution: entropy = -p_win * log(p_win) - (1-p_win) * log(1-p_win).\n6. Normalize entropy to [0, 1] and scale to create an adaptive gamma: adaptive_gamma = gamma_max * (entropy / log(2)).\n7. Compute the focal modulating factor using the adaptive gamma: modulating_factor = (1 - p_win)^adaptive_gamma.\n8. Calculate the final loss by applying the detached modulating factor to the core logsigmoid loss: final_loss = modulating_factor.detach() * -logsigmoid(normalized_delta).\n9. Return the mean of the final loss.", "hyperparams": {"gamma_max": 2.5, "epsilon": 1e-06}, "operators_used": ["logsigmoid", "sigmoid", "log"], "implementation_hint": {"expects": ["cost_a", "cost_b", "log_prob_w", "log_prob_l"], "returns": "scalar"}, "code": "def generated_loss(batch, model_output, extra):\n    \"\"\"\n    Combines a logsigmoid loss with a dynamically-weighted focal mechanism based on prediction entropy.\n    The log-probability difference is normalized by the range of winning log-probs for stability.\n    \"\"\"\n    # Read hyperparameters with defaults\n    gamma_max = extra.get('gamma_max', 2.5)\n    epsilon = extra.get('epsilon', 1e-6)\n\n    # Read required tensors from the batch\n    log_prob_w = batch['log_prob_w']\n    log_prob_l = batch['log_prob_l']\n    weight = batch.get('weight')\n\n    # 1. Calculate the log-probability difference\n    delta = log_prob_w - log_prob_l\n\n    # 2. New Coupling: Log-Prob Range Normalization\n    # Use detached values to prevent gradients from flowing through the normalization factor\n    log_prob_range = torch.max(log_prob_w.detach()) - torch.min(log_prob_w.detach())\n    normalized_delta = delta / (log_prob_range + epsilon)\n\n    # Core logsigmoid loss (Inherited)\n    loss_core = -F.logsigmoid(normalized_delta)\n\n    # 3. New Coupling: Entropy-based Focal Gamma\n    # Use the original, unnormalized delta to assess model confidence\n    with torch.no_grad():\n        p_win = torch.sigmoid(delta)\n        # Clamp p_win to avoid log(0)\n        p_win_clamped = torch.clamp(p_win, min=epsilon, max=1.0 - epsilon)\n        entropy = -p_win_clamped * torch.log(p_win_clamped) - (1.0 - p_win_clamped) * torch.log(1.0 - p_win_clamped)\n        # Normalize entropy to [0, 1] by dividing by max entropy (log(2))\n        normalized_entropy = entropy / torch.log(torch.tensor(2.0))\n        adaptive_gamma = gamma_max * normalized_entropy\n        # Focal modulating factor (Inherited)\n        modulating_factor = torch.pow(1.0 - p_win, adaptive_gamma)\n\n    # 4. Apply the modulating factor to the core loss\n    final_loss = modulating_factor * loss_core\n\n    # Apply optional instance weights if provided\n    if weight is not None:\n        final_loss = final_loss * weight\n\n    # Return the mean loss over the batch\n    return final_loss.mean()", "theoretical_basis": "A dynamically modulated Bradley-Terry preference model. The core loss is logistic, operating on a log-probability difference that is normalized by the batch's log-probability range for stability. The novel contribution is an adaptive focal modulation where the focal strength (gamma) is determined by the entropy of the model's win prediction for each pair. This directs learning focus towards examples with high model uncertainty, automating the 'hard example mining' process without a fixed hyperparameter."}, "static_ok": true, "static_reason": "", "static_error_code": "", "dynamic_ok": true, "dynamic_reason": "ok", "loss_value": 0.12253225594758987, "grad_norm": 0.0, "pref_ok": true, "pref_reason": "ok", "pref_mono_pass_rate": 1.0, "pref_swap_pass_rate": 0.6666666666666666, "pref_gap_pass_rate": 1.0}
{"generation": 3, "index": 3, "attempt": 0, "ir": {"name": "FocalRankGapLogsigLossWithDynamicTemp", "intuition": "Mode: explore. This loss function refines the successful combination of a focal loss mechanism with a Bradley-Terry style `logsigmoid` loss, while introducing more dynamic and stable components. The goal is to create a loss that is robust to variations in log-probability and cost scales, while effectively focusing on hard examples.\n\nInherited Ideas:\n1. From `FocalLogsigmoidRankGapLoss`: The core structure combining a focal modulating factor `(1 - p_win)^gamma` with a `logsigmoid` base loss. This allows for up-weighting of hard examples.\n2. From both parents (`FocalLogsigmoidRankGapLoss` and `AdaptiveMarginLossWithRankGap`): The use of a cost `rank_gap` to create a dynamic, batch-adaptive margin `margin = softplus(beta * rank_gap - offset)`. This makes the loss robust to the absolute scale of costs.\n\nNew Coupling Ideas:\n1. **Dynamic Temperature based on Margin**: Instead of tying the focal temperature `tau` to the standard deviation of `log_prob_w`, it is now dynamically scaled by the mean of the adaptive margin: `tau = tau_base + tau_scale * margin.mean().detach()`. This couples the 'hardness' definition for the focal loss to the average required 'effort' (margin) for the batch. When the costs are very distinct (high mean margin), the temperature increases, softening the focal penalty and preventing overly aggressive updates. When costs are similar (low mean margin), the temperature decreases, sharpening the focus on misclassified examples.\n2. **Log-Prob Difference Clipping**: To enhance stability, especially with the unbounded `logsigmoid` loss, the raw log-probability difference `delta` is clamped to a symmetric range `[-clip_value, clip_value]` before being used in the loss. This prevents extremely large or small delta values from causing gradient explosions or vanishing, ensuring a more stable training process without the full distributional shift of z-scoring.", "pseudocode": "1. Calculate the log-probability difference: delta = log_prob_w - log_prob_l.\n2. Clip the delta for stability: clipped_delta = clamp(delta, -clip_value, clip_value).\n3. Compute the cost rank gap: rank_gap = rank_gap(cost_a, cost_b).\n4. Calculate an adaptive margin from the rank gap: margin = softplus(beta * rank_gap - offset).\n5. Compute the core preference loss using the clipped delta and margin: loss_core = -logsigmoid(clipped_delta + margin).\n6. Compute a dynamic temperature based on the batch's average margin: tau = tau_base + tau_scale * margin.mean().detach().\n7. Calculate the probability of preferring the winner for the focal term: p_win = sigmoid(delta / tau).\n8. Compute the focal modulating factor: modulating_factor = (1 - p_win)^gamma.\n9. Apply the detached modulating factor to the core loss: final_loss = modulating_factor.detach() * loss_core.\n10. Return the mean of the final loss over the batch.", "hyperparams": {"beta": 1.5, "offset": 0.25, "gamma": 2.0, "tau_base": 0.1, "tau_scale": 0.5, "clip_value": 10.0}, "operators_used": ["rank_gap", "softplus", "logsigmoid", "sigmoid", "clamp"], "implementation_hint": {"expects": ["cost_a", "cost_b", "log_prob_w", "log_prob_l"], "returns": "scalar"}, "code": "def generated_loss(batch, model_output, extra):\n    \"\"\"\n    Combines a rank-gap adaptive margin and focal loss with a logsigmoid base.\n    Introduces delta clipping for stability and a margin-aware dynamic temperature for the focal component.\n    \"\"\"\n    # Read hyperparameters with defaults\n    beta = extra.get('beta', 1.5)\n    offset = extra.get('offset', 0.25)\n    gamma = extra.get('gamma', 2.0)\n    tau_base = extra.get('tau_base', 0.1)\n    tau_scale = extra.get('tau_scale', 0.5)\n    clip_value = extra.get('clip_value', 10.0)\n\n    # Read required tensors from the batch\n    cost_a = batch['cost_a']\n    cost_b = batch['cost_b']\n    log_prob_w = batch['log_prob_w']\n    log_prob_l = batch['log_prob_l']\n    weight = batch.get('weight')\n\n    # 1. Calculate the log-probability difference\n    delta = log_prob_w - log_prob_l\n\n    # 2. New Coupling: Clip delta for stability\n    clipped_delta = torch.clamp(delta, -clip_value, clip_value)\n\n    # 3. Inherited: Compute the rank gap of the costs\n    rank_gap = ops.rank_gap(cost_a, cost_b)\n\n    # 4. Inherited: Calculate an adaptive margin from the rank gap\n    margin = F.softplus(beta * rank_gap - offset)\n\n    # 5. Compute the core preference loss using logsigmoid\n    loss_core = -F.logsigmoid(clipped_delta + margin)\n\n    # 6. New Coupling: Dynamic temperature based on the mean margin\n    # Detach to prevent gradients from flowing through the temperature calculation\n    tau = tau_base + tau_scale * margin.mean().detach()\n    tau = torch.clamp(tau, min=1e-2) # Ensure tau is positive\n\n    # 7. Inherited: Calculate probability for focal loss (using original unclipped delta)\n    p_win = torch.sigmoid(delta / tau)\n\n    # 8. Inherited: Compute the focal modulating factor\n    modulating_factor = torch.pow(1.0 - p_win, gamma)\n\n    # 9. Apply the modulating factor (detached) to the core loss\n    final_loss = modulating_factor.detach() * loss_core\n\n    # Apply optional instance weights if provided\n    if weight is not None:\n        final_loss = final_loss * weight\n\n    # Return the mean loss over the batch\n    return final_loss.mean()", "theoretical_basis": "A hybrid Bradley-Terry and focal loss model with enhanced stability. The core preference is modeled using a logistic function (-logsigmoid) on a margin-adjusted, clipped log-probability difference. The clipping provides stability against outliers. This is coupled with a focal modulation mechanism that re-weights training examples based on the model's predictive confidence. The temperature of this focal component is dynamically adapted based on the batch's average rank-gap margin, linking the definition of a 'hard' example to the overall cost separation in the batch."}, "static_ok": true, "static_reason": "", "static_error_code": "", "dynamic_ok": true, "dynamic_reason": "ok", "loss_value": 0.05026540532708168, "grad_norm": 0.0, "pref_ok": true, "pref_reason": "ok", "pref_mono_pass_rate": 1.0, "pref_swap_pass_rate": 1.0, "pref_gap_pass_rate": 1.0}
{"generation": 3, "index": 4, "attempt": 0, "ir": {"name": "AdaptiveFocalLogsigmoidLoss", "intuition": "Mode: explore. This loss function aims to create a more stable and adaptive version of the high-performing `FocalLogsigmoidRankGapLoss` by replacing its explicit z-score normalization and adaptive temperature with a single, theoretically-grounded mechanism. \n\nInherited ideas:\n1. From `FocalLogsigmoidRankGapLoss`: The core structure of combining a Bradley-Terry style `logsigmoid` loss with a focal modulation factor `(1 - p_win)^gamma` to up-weight hard examples.\n2. From `FocalLogsigmoidRankGapLoss` and `AdaptiveMarginLossWithRankGap`: The use of a `rank_gap`-based adaptive margin `softplus(beta * rank_gap - offset)` to make the loss robust to the absolute scale of costs.\n\nNew coupling ideas:\n1. **Dynamic Temperature Scaling**: Instead of z-scoring the log-probability difference `delta`, we introduce a dynamic temperature `tau` that scales `delta` *before* it's used in both the core loss and the focal probability calculation. This `tau` is computed as `softplus(delta.detach().std())`, which normalizes the log-probability differences based on their batch-wise variance. This single mechanism serves to both stabilize the `logsigmoid` input (preventing extreme values) and adapt the sensitivity of the focal term, creating a tighter coupling between the two components.\n2. **Margin-Modulated Focal Term**: The probability of winning `p_win`, used for the focal term, is calculated as `sigmoid((delta + margin) / tau)`. This incorporates the adaptive `rank_gap` margin directly into the confidence estimate. This means an example is considered 'easy' (and thus down-weighted) only if the log-probability difference `delta` is large *relative to the difficulty of the preference*, as captured by the margin. This prevents the model from being penalized for small `delta`s on pairs with very similar costs.", "pseudocode": "1. Calculate the log-probability difference: delta = log_prob_w - log_prob_l.\n2. Compute the cost rank gap: rank_gap = rank_gap(cost_a, cost_b).\n3. Calculate an adaptive margin from the rank gap: margin = softplus(beta * rank_gap - offset).\n4. Compute a dynamic temperature based on the standard deviation of delta: tau = softplus(delta.detach().std()).\n5. Calculate the core preference loss using the scaled delta and margin: loss_core = -logsigmoid((delta + margin) / tau).\n6. Calculate the probability of preferring the winner, also using the scaled and margin-adjusted delta: p_win = sigmoid((delta + margin) / tau).\n7. Compute the focal modulating factor: modulating_factor = (1 - p_win)^gamma.\n8. Apply the detached modulating factor to the core loss: final_loss = modulating_factor.detach() * loss_core.\n9. Return the mean of the final loss over the batch.", "hyperparams": {"beta": 1.5, "offset": 0.25, "gamma": 2.0}, "operators_used": ["logsigmoid", "sigmoid", "softplus", "rank_gap"], "implementation_hint": {"expects": ["cost_a", "cost_b", "log_prob_w", "log_prob_l"], "returns": "scalar"}, "code": "def generated_loss(batch, model_output, extra):\n    \"\"\"\n    Combines a rank-gap adaptive margin with a focal loss mechanism on a logsigmoid base loss.\n    Introduces a dynamic temperature `tau` based on the standard deviation of `delta` to normalize the loss input\n    and adapt the focal term's sensitivity.\n    \"\"\"\n    # Read hyperparameters with defaults\n    beta = extra.get('beta', 1.5)\n    offset = extra.get('offset', 0.25)\n    gamma = extra.get('gamma', 2.0)\n\n    # Read required tensors from the batch\n    cost_a = batch['cost_a']\n    cost_b = batch['cost_b']\n    log_prob_w = batch['log_prob_w']\n    log_prob_l = batch['log_prob_l']\n    weight = batch.get('weight')\n\n    # 1. Calculate the log-probability difference\n    delta = log_prob_w - log_prob_l\n\n    # 2. Inherited: Compute the rank gap of the costs\n    rank_gap = ops.rank_gap(cost_a, cost_b)\n\n    # 3. Inherited: Calculate an adaptive margin from the rank gap\n    margin = F.softplus(beta * rank_gap - offset)\n\n    # 4. New Coupling: Dynamic temperature scaling\n    # Use detached std dev to avoid influencing gradients through tau. Clamp for stability.\n    tau = F.softplus(delta.detach().std()).clamp(min=1e-4)\n    \n    # Combine delta and margin for the core loss term\n    logits = delta + margin\n\n    # 5. Calculate the core preference loss with dynamic scaling\n    loss_core = -F.logsigmoid(logits / tau)\n\n    # 6. New Coupling: Margin-modulated focal term\n    # p_win now considers the margin, making the confidence estimate more robust\n    p_win = torch.sigmoid(logits / tau)\n\n    # 7. Inherited: Compute the focal modulating factor\n    modulating_factor = torch.pow(1.0 - p_win, gamma)\n\n    # 8. Apply the modulating factor (detached for stability) to the core loss\n    final_loss = modulating_factor.detach() * loss_core\n\n    # Apply optional instance weights if provided\n    if weight is not None:\n        final_loss = final_loss * weight\n\n    # Return the mean loss over the batch\n    return final_loss.mean()", "theoretical_basis": "A dynamically-scaled Bradley-Terry model with focal loss. The preference probability is modeled via a logistic function whose input `(delta + margin)` is scaled by a dynamic, batch-aware temperature `tau`. This `tau` acts as an adaptive normalization factor. This is coupled with a focal modulation mechanism where the confidence estimate `p_win` is also a function of the scaled, margin-adjusted `delta`, creating a cohesive system for hard-example mining."}, "static_ok": true, "static_reason": "", "static_error_code": "", "dynamic_ok": true, "dynamic_reason": "ok", "loss_value": 0.0011453171027824283, "grad_norm": 0.0, "pref_ok": true, "pref_reason": "ok", "pref_mono_pass_rate": 1.0, "pref_swap_pass_rate": 0.8333333333333334, "pref_gap_pass_rate": 1.0}
{"generation": 3, "index": 5, "attempt": 0, "ir": {"name": "AdaptiveFocalLogsigmoidLoss", "intuition": "Mode: explore. This loss function aims to create a more stable and robust version of a focal loss applied to a Bradley-Terry style preference model. It inherits the core structure from `FocalLogsigmoidRankGapLoss` but simplifies and modifies its components for better stability and theoretical clarity.\n\nInherited Ideas:\n1.  From `FocalLogsigmoidRankGapLoss`: The core idea of applying a focal loss modulating factor `(1 - p_win)^gamma` to a base `logsigmoid` loss. This up-weights hard-to-classify examples.\n2.  From `FocalLogsigmoidRankGapLoss` and `AdaptiveMarginLossWithRankGap`: The use of `rank_gap` to create a non-parametric, batch-adaptive margin that is robust to the absolute scale of costs.\n\nNew Coupling Ideas:\n1.  **Log-Prob Clipping**: Instead of z-scoring the log-probability difference `delta`, which can be sensitive to outliers in a batch, this loss clips `delta` within a symmetric range `[-clip_range, clip_range]`. This directly prevents extreme `delta` values from causing gradient explosion or numerical instability in the `logsigmoid` and `sigmoid` functions, providing a more direct and robust stability mechanism.\n2.  **Margin as Temperature**: The adaptive `rank_gap` margin is repurposed to act as a dynamic temperature `tau` for the focal probability calculation (`p_win = sigmoid(delta / tau)`). This couples the 'hardness' of an example (defined by the model's log-prob difference) with the significance of the cost difference. For pairs with a large cost gap (large `rank_gap` and thus large `tau`), the focal modulation becomes less sensitive, effectively trusting the large cost signal. For pairs with a small cost gap (small `tau`), the focal modulation becomes more sensitive, focusing learning on subtle distinctions where the model's confidence is most critical.", "pseudocode": "1. Calculate the log-probability difference: delta = log_prob_w - log_prob_l.\n2. Compute the cost rank gap: rank_gap = rank_gap(cost_a, cost_b).\n3. Calculate an adaptive temperature from the rank gap: tau = softplus(beta * rank_gap - offset) + epsilon.\n4. Clip the delta for stability: clipped_delta = clamp(delta, min=-clip_range, max=clip_range).\n5. Compute the core preference loss using the clipped delta: loss_core = -logsigmoid(clipped_delta).\n6. Calculate the probability of preferring the winner using the adaptive temperature: p_win = sigmoid(clipped_delta / tau).\n7. Compute the focal modulating factor: modulating_factor = (1 - p_win)^gamma.\n8. Apply the detached modulating factor to the core loss: final_loss = modulating_factor.detach() * loss_core.\n9. Return the mean of the final loss over the batch.", "hyperparams": {"beta": 1.0, "offset": 0.5, "gamma": 2.0, "clip_range": 5.0, "epsilon": 1e-06}, "operators_used": ["rank_gap", "softplus", "logsigmoid", "sigmoid", "clamp"], "implementation_hint": {"expects": ["cost_a", "cost_b", "log_prob_w", "log_prob_l"], "returns": "scalar"}, "code": "def generated_loss(batch, model_output, extra):\n    \"\"\"\n    Combines a clipped logsigmoid loss with a focal modulation mechanism where the temperature\n    is dynamically set by the cost rank gap.\n    \"\"\"\n    # Read hyperparameters with defaults\n    beta = extra.get('beta', 1.0)\n    offset = extra.get('offset', 0.5)\n    gamma = extra.get('gamma', 2.0)\n    clip_range = extra.get('clip_range', 5.0)\n    epsilon = extra.get('epsilon', 1e-6)\n\n    # Read required tensors from the batch\n    cost_a = batch['cost_a']\n    cost_b = batch['cost_b']\n    log_prob_w = batch['log_prob_w']\n    log_prob_l = batch['log_prob_l']\n    weight = batch.get('weight')\n\n    # 1. Calculate the log-probability difference\n    delta = log_prob_w - log_prob_l\n\n    # 2. Inherited: Compute the rank gap of the costs\n    rank_gap = ops.rank_gap(cost_a, cost_b)\n\n    # 3. New Coupling: Use rank_gap to create an adaptive temperature for the focal term\n    # A small epsilon prevents division by zero if tau becomes very small.\n    tau = F.softplus(beta * rank_gap - offset) + epsilon\n\n    # 4. New Coupling: Clip delta for stability instead of z-scoring\n    clipped_delta = torch.clamp(delta, min=-clip_range, max=clip_range)\n\n    # 5. Compute the core preference loss using the stabilized delta\n    loss_core = -F.logsigmoid(clipped_delta)\n\n    # 6. Inherited: Calculate probability for focal loss, using the adaptive temperature\n    # Use the stabilized delta here as well for consistency.\n    p_win = torch.sigmoid(clipped_delta / tau)\n\n    # 7. Inherited: Compute the focal modulating factor\n    modulating_factor = torch.pow(1.0 - p_win, gamma)\n\n    # 8. Apply the modulating factor to the core loss. Detach to not affect core gradients.\n    final_loss = modulating_factor.detach() * loss_core\n\n    # Apply optional instance weights if provided\n    if weight is not None:\n        final_loss = final_loss * weight\n\n    # Return the mean loss over the batch\n    return final_loss.mean()", "theoretical_basis": "A clipped Bradley-Terry model with focal modulation and an adaptive, cost-aware temperature. The core loss is a standard logistic preference model, but with clipped log-probability differences to ensure numerical stability. The focal loss component up-weights hard examples, and its sensitivity (temperature) is dynamically coupled with the batch-relative cost improvement (rank_gap), allowing the loss to focus more on model confidence for pairs with small cost differences."}, "static_ok": true, "static_reason": "", "static_error_code": "", "dynamic_ok": true, "dynamic_reason": "ok", "loss_value": 0.1732867956161499, "grad_norm": 0.0, "pref_ok": false, "pref_reason": "pref_semantic_violation", "pref_mono_pass_rate": 1.0, "pref_swap_pass_rate": 0.5, "pref_gap_pass_rate": 1.0, "dynamic_error_code": "E_PREF_SEMANTIC"}
{"generation": 3, "index": 5, "attempt": 1, "ir": {"name": "AdaptiveFocalLogsigmoidLoss", "intuition": "Repaired: The original loss failed the preference gate's swap test (swap_pass_rate=0.5), indicating a problem with how it used cost information. The error was in the `rank_gap` calculation, which is not symmetric with respect to `cost_a` and `cost_b`. This meant swapping the inputs `(a,b)` did not produce the same loss value. I have fixed this by replacing `ops.rank_gap(cost_a, cost_b)` with `torch.abs(cost_a - cost_b)`. Using the absolute cost difference ensures that the temperature `tau` is solely dependent on the magnitude of the cost gap, not the order of the inputs, resolving the `E_PREF_SEMANTIC` failure.", "pseudocode": "1. Calculate the log-probability difference: delta = log_prob_w - log_prob_l.\n2. Compute the absolute cost difference: cost_diff = abs(cost_a - cost_b).\n3. Calculate an adaptive temperature from the cost difference: tau = softplus(beta * cost_diff - offset) + epsilon.\n4. Clip the delta for stability: clipped_delta = clamp(delta, min=-clip_range, max=clip_range).\n5. Compute the core preference loss using the clipped delta: loss_core = -logsigmoid(clipped_delta).\n6. Calculate the probability of preferring the winner using the adaptive temperature: p_win = sigmoid(clipped_delta / tau).\n7. Compute the focal modulating factor: modulating_factor = (1 - p_win)^gamma.\n8. Apply the detached modulating factor to the core loss: final_loss = modulating_factor.detach() * loss_core.\n9. Return the mean of the final loss over the batch.", "hyperparams": {"beta": 1.0, "offset": 0.5, "gamma": 2.0, "clip_range": 5.0, "epsilon": 1e-06}, "operators_used": ["softplus", "logsigmoid", "sigmoid", "clamp"], "implementation_hint": {"expects": ["cost_a", "cost_b", "log_prob_w", "log_prob_l"], "returns": "scalar"}, "code": "def generated_loss(batch, model_output, extra):\n    \"\"\"\n    Combines a clipped logsigmoid loss with a focal modulation mechanism where the temperature\n    is dynamically set by the cost difference.\n    \"\"\"\n    # Read hyperparameters with defaults\n    beta = extra.get('beta', 1.0)\n    offset = extra.get('offset', 0.5)\n    gamma = extra.get('gamma', 2.0)\n    clip_range = extra.get('clip_range', 5.0)\n    epsilon = extra.get('epsilon', 1e-6)\n\n    # Read required tensors from the batch\n    cost_a = batch['cost_a']\n    cost_b = batch['cost_b']\n    log_prob_w = batch['log_prob_w']\n    log_prob_l = batch['log_prob_l']\n    weight = batch.get('weight')\n\n    # 1. Calculate the log-probability difference\n    delta = log_prob_w - log_prob_l\n\n    # 2. Compute the absolute cost difference to ensure symmetry\n    cost_diff = torch.abs(cost_a - cost_b)\n\n    # 3. Use cost_diff to create an adaptive temperature for the focal term\n    # A small epsilon prevents division by zero if tau becomes very small.\n    tau = F.softplus(beta * cost_diff - offset) + epsilon\n\n    # 4. Clip delta for stability instead of z-scoring\n    clipped_delta = torch.clamp(delta, min=-clip_range, max=clip_range)\n\n    # 5. Compute the core preference loss using the stabilized delta\n    loss_core = -F.logsigmoid(clipped_delta)\n\n    # 6. Calculate probability for focal loss, using the adaptive temperature\n    # Use the stabilized delta here as well for consistency.\n    p_win = torch.sigmoid(clipped_delta / tau)\n\n    # 7. Compute the focal modulating factor\n    modulating_factor = torch.pow(1.0 - p_win, gamma)\n\n    # 8. Apply the modulating factor to the core loss. Detach to not affect core gradients.\n    final_loss = modulating_factor.detach() * loss_core\n\n    # Apply optional instance weights if provided\n    if weight is not None:\n        final_loss = final_loss * weight\n\n    # Return the mean loss over the batch\n    return final_loss.mean()", "theoretical_basis": ""}, "static_ok": true, "static_reason": "", "static_error_code": "", "dynamic_ok": true, "dynamic_reason": "ok", "loss_value": 0.1732867956161499, "grad_norm": 0.0, "pref_ok": true, "pref_reason": "ok", "pref_mono_pass_rate": 1.0, "pref_swap_pass_rate": 0.6666666666666666, "pref_gap_pass_rate": 1.0}
{"generation": 3, "index": 6, "attempt": 0, "ir": {"name": "AdaptiveFocalLogsigmoidLoss", "intuition": "Mode: explore. This loss function aims to create a more stable and adaptive version of a focal Bradley-Terry loss. It inherits the core concepts of using a focal modulating factor and a rank-gap-based margin from `FocalLogsigmoidRankGapLoss`, but refines them for better stability and performance.\n\nInherited Ideas:\n1. From `FocalLogsigmoidRankGapLoss`: The use of a focal modulating factor `(1 - p_win)^gamma` to up-weight hard examples where the model incorrectly prefers the worse solution. This helps focus training on difficult pairs.\n2. From `FocalLogsigmoidRankGapLoss` and `AdaptiveMarginLossWithRankGap`: The use of `rank_gap` to create a dynamic, batch-adaptive margin `margin = softplus(beta * rank_gap)`. This makes the loss robust to the absolute scale of costs.\n\nNew Coupling Ideas:\n1. **Log-Prob Normalization per Pair**: Instead of batch-level z-scoring, the log-probability difference `delta` is normalized by a value derived from the log-probabilities themselves: `delta_norm = delta / softplus(abs(log_prob_w) + abs(log_prob_l))`. This per-pair normalization prevents extremely large log-probabilities (which can occur during training) from causing gradient explosion, while still allowing the signal from `delta` to be strong when log-probabilities are small. It's a localized stability trick.\n2. **Margin-Modulated Focal Weight**: The focal modulating factor is itself scaled by the rank-gap margin: `modulating_factor = (margin + epsilon) * (1 - p_win)^gamma`. This couples the margin's importance (how different the costs are) with the model's error (how wrong its prediction is). Pairs that are both significantly different in quality (high margin) and are misclassified by the model (high focal weight) receive the strongest learning signal.", "pseudocode": "1. Calculate the log-probability difference: delta = log_prob_w - log_prob_l.\n2. Compute the cost rank gap: rank_gap = rank_gap(cost_a, cost_b).\n3. Calculate an adaptive margin from the rank gap: margin = softplus(beta * rank_gap).\n4. Compute a per-pair normalization factor based on the magnitude of the log-probabilities: normalizer = softplus(abs(log_prob_w) + abs(log_prob_l)).\n5. Normalize the log-probability difference: normalized_delta = delta / normalizer.\n6. Compute the probability of preferring the winner: p_win = sigmoid(delta).\n7. Compute a margin-modulated focal factor: modulating_factor = (margin + epsilon) * (1 - p_win)^gamma.\n8. Compute the core preference loss: loss_core = -logsigmoid(normalized_delta + margin).\n9. Apply the detached modulating factor to the core loss: final_loss = modulating_factor.detach() * loss_core.\n10. Return the mean of the final loss over the batch.", "hyperparams": {"beta": 1.5, "gamma": 2.0, "epsilon": 1e-06}, "operators_used": ["rank_gap", "softplus", "logsigmoid", "sigmoid"], "implementation_hint": {"expects": ["cost_a", "cost_b", "log_prob_w", "log_prob_l"], "returns": "scalar"}, "code": "def generated_loss(batch, model_output, extra):\n    \"\"\"\n    Combines a rank-gap adaptive margin with a margin-modulated focal loss.\n    Introduces per-pair normalization for the log-probability difference for stability.\n    \"\"\"\n    # Read hyperparameters with defaults\n    beta = extra.get('beta', 1.5)\n    gamma = extra.get('gamma', 2.0)\n    epsilon = extra.get('epsilon', 1e-6)\n\n    # Read required tensors from the batch\n    cost_a = batch['cost_a']\n    cost_b = batch['cost_b']\n    log_prob_w = batch['log_prob_w']\n    log_prob_l = batch['log_prob_l']\n    weight = batch.get('weight')\n\n    # 1. Calculate the log-probability difference\n    delta = log_prob_w - log_prob_l\n\n    # 2. Inherited: Compute the rank gap of the costs\n    rank_gap = ops.rank_gap(cost_a, cost_b)\n\n    # 3. Inherited: Calculate an adaptive margin from the rank gap\n    margin = F.softplus(beta * rank_gap)\n\n    # 4. New Coupling: Per-pair normalization for stability\n    # This prevents extreme log_probs from causing instability\n    normalizer = F.softplus(torch.abs(log_prob_w.detach()) + torch.abs(log_prob_l.detach()))\n    # Clamp normalizer to avoid division by zero\n    normalized_delta = delta / normalizer.clamp(min=epsilon)\n\n    # 5. Inherited: Calculate probability for focal loss (using original delta for confidence)\n    p_win = torch.sigmoid(delta)\n\n    # 6. New Coupling: Margin-modulated focal factor\n    # Detach to prevent gradients flowing through the weight\n    modulating_factor = ((margin + epsilon) * torch.pow(1.0 - p_win, gamma)).detach()\n\n    # 7. Compute the core preference loss using logsigmoid\n    loss_core = -F.logsigmoid(normalized_delta + margin)\n\n    # 8. Apply the modulating factor to the core loss\n    final_loss = modulating_factor * loss_core\n\n    # Apply optional instance weights if provided\n    if weight is not None:\n        final_loss = final_loss * weight\n\n    # Return the mean loss over the batch\n    return final_loss.mean()", "theoretical_basis": "A modified Bradley-Terry model with focal loss and adaptive margins. The core is a logistic preference model (-logsigmoid) on a margin-adjusted log-probability difference. It introduces two stability and coupling mechanisms: a per-pair normalization of the log-probability difference to prevent gradient issues from extreme log-probs, and a margin-modulated focal weight that couples the significance of the cost difference with the model's prediction error."}, "static_ok": true, "static_reason": "", "static_error_code": "", "dynamic_ok": true, "dynamic_reason": "ok", "loss_value": 0.07127558439970016, "grad_norm": 0.0, "pref_ok": false, "pref_reason": "pref_semantic_violation", "pref_mono_pass_rate": 1.0, "pref_swap_pass_rate": 0.0, "pref_gap_pass_rate": 1.0, "dynamic_error_code": "E_PREF_SEMANTIC"}
{"generation": 3, "index": 6, "attempt": 1, "ir": {"name": "AdaptiveFocalLogsigmoidLoss", "intuition": "Repaired: The original loss failed the `E_PREF_SEMANTIC` gate check with a `swap_pass_rate` of 0.0. This indicates that the loss did not decrease as the model's preference for the better completion (`log_prob_w`) increased relative to the worse one (`log_prob_l`). The failure was caused by normalizing the log-probability difference `delta` by a factor derived from the absolute values of the log-probabilities. When `log_prob_w` increased, `delta` increased, but the `normalizer` also increased, which could counteract the effect on the final loss. I have removed this per-pair normalization to restore the direct monotonic relationship between `log_prob_w - log_prob_l` and the loss signal, which is critical for satisfying the preference condition. The rest of the adaptive margin and focal modulation logic is preserved.", "pseudocode": "1. Calculate the log-probability difference: delta = log_prob_w - log_prob_l.\n2. Compute the cost rank gap: rank_gap = rank_gap(cost_a, cost_b).\n3. Calculate an adaptive margin from the rank gap: margin = softplus(beta * rank_gap).\n4. Compute the probability of preferring the winner: p_win = sigmoid(delta).\n5. Compute a margin-modulated focal factor: modulating_factor = (margin + epsilon) * (1 - p_win)^gamma.\n6. Compute the core preference loss: loss_core = -logsigmoid(delta + margin).\n7. Apply the detached modulating factor to the core loss: final_loss = modulating_factor.detach() * loss_core.\n8. Return the mean of the final loss over the batch.", "hyperparams": {"beta": 1.5, "gamma": 2.0, "epsilon": 1e-06}, "operators_used": ["rank_gap", "softplus", "logsigmoid", "sigmoid"], "implementation_hint": {"expects": ["cost_a", "cost_b", "log_prob_w", "log_prob_l"], "returns": "scalar"}, "code": "def generated_loss(batch, model_output, extra):\n    \"\"\"\n    Combines a rank-gap adaptive margin with a margin-modulated focal loss.\n    \"\"\"\n    # Read hyperparameters with defaults\n    beta = extra.get('beta', 1.5)\n    gamma = extra.get('gamma', 2.0)\n    epsilon = extra.get('epsilon', 1e-6)\n\n    # Read required tensors from the batch\n    cost_a = batch['cost_a']\n    cost_b = batch['cost_b']\n    log_prob_w = batch['log_prob_w']\n    log_prob_l = batch['log_prob_l']\n    weight = batch.get('weight')\n\n    # 1. Calculate the log-probability difference\n    delta = log_prob_w - log_prob_l\n\n    # 2. Inherited: Compute the rank gap of the costs\n    rank_gap = ops.rank_gap(cost_a, cost_b)\n\n    # 3. Inherited: Calculate an adaptive margin from the rank gap\n    margin = F.softplus(beta * rank_gap)\n\n    # 4. Inherited: Calculate probability for focal loss\n    p_win = torch.sigmoid(delta)\n\n    # 5. New Coupling: Margin-modulated focal factor\n    # Detach to prevent gradients flowing through the weight\n    modulating_factor = ((margin + epsilon) * torch.pow(1.0 - p_win, gamma)).detach()\n\n    # 6. Compute the core preference loss using logsigmoid\n    # The per-pair normalization was removed to fix the E_PREF_SEMANTIC violation.\n    loss_core = -F.logsigmoid(delta + margin)\n\n    # 7. Apply the modulating factor to the core loss\n    final_loss = modulating_factor * loss_core\n\n    # Apply optional instance weights if provided\n    if weight is not None:\n        final_loss = final_loss * weight\n\n    # Return the mean loss over the batch\n    return final_loss.mean()", "theoretical_basis": ""}, "static_ok": true, "static_reason": "", "static_error_code": "", "dynamic_ok": true, "dynamic_reason": "ok", "loss_value": 0.07127558439970016, "grad_norm": 0.0, "pref_ok": false, "pref_reason": "pref_semantic_violation", "pref_mono_pass_rate": 1.0, "pref_swap_pass_rate": 0.0, "pref_gap_pass_rate": 1.0, "dynamic_error_code": "E_PREF_SEMANTIC"}
{"generation": 3, "index": 6, "attempt": 2, "ir": {"name": "AdaptiveFocalLogsigmoidLoss", "intuition": "The original loss failed the `E_PREF_SEMANTIC` gate check with a `swap_pass_rate` of 0.0. This indicates that the loss did not decrease as the model's preference for the better completion (`log_prob_w`) increased relative to the worse one (`log_prob_l`). The failure was caused by a complex modulating factor that included `(1 - p_win)^gamma`, where `p_win = sigmoid(log_prob_w - log_prob_l)`. As `log_prob_w` increases, `p_win` increases, causing the modulating factor to decrease. This factor multiplied the core loss, which was also decreasing, resulting in a non-monotonic final loss. To fix this, I have removed the dependency on `p_win` from the modulating factor, simplifying it to just `(margin + epsilon)`. This ensures that the final loss decreases monotonically as the model's preference for the correct answer increases.", "pseudocode": "1. Calculate the log-probability difference: delta = log_prob_w - log_prob_l.\n2. Compute the cost rank gap: rank_gap = rank_gap(cost_a, cost_b).\n3. Calculate an adaptive margin from the rank gap: margin = softplus(beta * rank_gap).\n4. Compute a simplified modulating factor from the margin: modulating_factor = (margin + epsilon).\n5. Compute the core preference loss: loss_core = -logsigmoid(delta + margin).\n6. Apply the detached modulating factor to the core loss: final_loss = modulating_factor.detach() * loss_core.\n7. Return the mean of the final loss over the batch.", "hyperparams": {"beta": 1.5, "gamma": 2.0, "epsilon": 1e-06}, "operators_used": ["rank_gap", "softplus", "logsigmoid"], "implementation_hint": {"expects": ["cost_a", "cost_b", "log_prob_w", "log_prob_l"], "returns": "scalar"}, "code": "def generated_loss(batch, model_output, extra):\n    \"\"\"\n    Combines a rank-gap adaptive margin with a margin-modulated focal loss.\n    \"\"\"\n    # Read hyperparameters with defaults\n    beta = extra.get('beta', 1.5)\n    gamma = extra.get('gamma', 2.0) # gamma is no longer used but kept for signature consistency\n    epsilon = extra.get('epsilon', 1e-6)\n\n    # Read required tensors from the batch\n    cost_a = batch['cost_a']\n    cost_b = batch['cost_b']\n    log_prob_w = batch['log_prob_w']\n    log_prob_l = batch['log_prob_l']\n    weight = batch.get('weight')\n\n    # 1. Calculate the log-probability difference\n    delta = log_prob_w - log_prob_l\n\n    # 2. Inherited: Compute the rank gap of the costs\n    rank_gap = ops.rank_gap(cost_a, cost_b)\n\n    # 3. Inherited: Calculate an adaptive margin from the rank gap\n    margin = F.softplus(beta * rank_gap)\n\n    # 4. Simplified Modulating Factor to fix E_PREF_SEMANTIC violation\n    # The original factor depended on (1-p_win)^gamma, which created a non-monotonic loss.\n    # Detach to prevent gradients flowing through the weight.\n    modulating_factor = (margin + epsilon).detach()\n\n    # 5. Compute the core preference loss using logsigmoid\n    loss_core = -F.logsigmoid(delta + margin)\n\n    # 6. Apply the modulating factor to the core loss\n    final_loss = modulating_factor * loss_core\n\n    # Apply optional instance weights if provided\n    if weight is not None:\n        final_loss = final_loss * weight\n\n    # Return the mean loss over the batch\n    return final_loss.mean()", "theoretical_basis": ""}, "static_ok": true, "static_reason": "", "static_error_code": "", "dynamic_ok": true, "dynamic_reason": "ok", "loss_value": 0.28510233759880066, "grad_norm": 0.0, "pref_ok": false, "pref_reason": "pref_semantic_violation", "pref_mono_pass_rate": 1.0, "pref_swap_pass_rate": 0.0, "pref_gap_pass_rate": 1.0, "dynamic_error_code": "E_PREF_SEMANTIC"}
{"generation": 3, "index": 7, "attempt": 0, "ir": {"name": "AdaptiveFocalLogsigmoidLoss", "intuition": "Mode: explore. This loss function combines the robust, non-parametric margin from `AdaptiveMarginLossWithRankGap` with the hard-example focusing of `FocalLogsigmoidRankGapLoss`, but simplifies the overall structure and introduces a new coupling mechanism for the focal component.\n\nInherited ideas:\n1.  From `FocalLogsigmoidRankGapLoss`: The core use of a focal modulating factor `(1 - p_win)^gamma` applied to a Bradley-Terry style `-logsigmoid` loss. This helps the model focus on correcting confident mistakes.\n2.  From `AdaptiveMarginLossWithRankGap`: The use of `rank_gap` to create a dynamic, batch-adaptive margin `margin = softplus(beta * rank_gap - offset)`. This makes the loss robust to the absolute scale of costs by focusing on relative improvements.\n\nNew coupling ideas:\n1.  **Cost-Gap Temperature Scaling**: Instead of a fixed or log-prob-std-based temperature `tau` for the focal term's `p_win` calculation, this loss scales the log-prob difference `delta` by the raw cost gap `cost_b - cost_a`. The idea is that the confidence estimate (`p_win`) should be more sensitive when the cost difference is small (making it a genuinely hard example) and less sensitive when the cost difference is large (where the model should easily get it right). This is implemented as `p_win = sigmoid(delta / (cost_b - cost_a))`. An epsilon is added for stability.\n2.  **Simplified Structure**: This child loss removes the `zscore` normalization from `FocalLogsigmoidRankGapLoss`. While z-scoring provides stability, it can also wash out important signal about the absolute magnitude of the model's confidence. By removing it and relying on the adaptive margin and the new cost-gap temperature, the loss directly operates on the raw log-probability differences, which is a simpler and more direct formulation.", "pseudocode": "1. Calculate the log-probability difference: delta = log_prob_w - log_prob_l.\n2. Compute the cost rank gap: rank_gap = rank_gap(cost_a, cost_b).\n3. Calculate an adaptive margin from the rank gap: margin = softplus(beta * rank_gap - offset).\n4. Compute the core preference loss using the logsigmoid function: loss_core = -logsigmoid(delta + margin).\n5. Compute the raw cost gap: cost_gap = cost_b - cost_a.\n6. Compute the probability of preferring the winner, with the temperature scaled by the cost gap: p_win = sigmoid(delta / (cost_gap + epsilon)).\n7. Compute the focal modulating factor: modulating_factor = (1 - p_win)^gamma.\n8. Apply the detached modulating factor to the core loss: final_loss = modulating_factor.detach() * loss_core.\n9. Return the mean of the final loss over the batch.", "hyperparams": {"beta": 1.0, "offset": 0.5, "gamma": 2.0, "epsilon": 1e-06}, "operators_used": ["rank_gap", "softplus", "logsigmoid", "sigmoid"], "implementation_hint": {"expects": ["cost_a", "cost_b", "log_prob_w", "log_prob_l"], "returns": "scalar"}, "code": "def generated_loss(batch, model_output, extra):\n    \"\"\"\n    Combines a rank-gap adaptive margin with a focal loss, where the focal temperature is scaled by the cost gap.\n    \"\"\"\n    # Read hyperparameters with defaults\n    beta = extra.get('beta', 1.0)\n    offset = extra.get('offset', 0.5)\n    gamma = extra.get('gamma', 2.0)\n    epsilon = extra.get('epsilon', 1e-6)\n\n    # Read required tensors from the batch\n    cost_a = batch['cost_a']\n    cost_b = batch['cost_b']\n    log_prob_w = batch['log_prob_w']\n    log_prob_l = batch['log_prob_l']\n    weight = batch.get('weight')\n\n    # 1. Inherited: Calculate the log-probability difference\n    delta = log_prob_w - log_prob_l\n\n    # 2. Inherited: Compute the rank gap of the costs\n    rank_gap = ops.rank_gap(cost_a, cost_b)\n\n    # 3. Inherited: Calculate an adaptive margin from the rank gap\n    margin = F.softplus(beta * rank_gap - offset)\n\n    # 4. Compute the core preference loss using logsigmoid\n    loss_core = -F.logsigmoid(delta + margin)\n\n    # 5. New Coupling: Use raw cost gap as a temperature for focal modulation\n    cost_gap = (cost_b - cost_a).detach()\n    # Ensure temperature is positive and non-zero\n    temperature = cost_gap.clamp(min=epsilon)\n    \n    # 6. Inherited: Calculate probability for focal loss with new temperature\n    p_win = torch.sigmoid(delta / temperature)\n\n    # 7. Inherited: Compute the focal modulating factor\n    modulating_factor = torch.pow(1.0 - p_win, gamma)\n\n    # 8. Apply the modulating factor to the core loss (detached for stability)\n    final_loss = modulating_factor.detach() * loss_core\n\n    # Apply optional instance weights if provided\n    if weight is not None:\n        final_loss = final_loss * weight\n\n    # Return the mean loss over the batch\n    return final_loss.mean()", "theoretical_basis": "A hybrid Bradley-Terry and focal loss model. The core preference is modeled using a logistic function on a log-probability difference, augmented by a dynamic margin derived from the cost rank-gap. This is coupled with a focal modulation mechanism where the temperature is inversely proportional to the raw cost gap, sharpening the focus on misclassified pairs with small cost differences."}, "static_ok": true, "static_reason": "", "static_error_code": "", "dynamic_ok": true, "dynamic_reason": "ok", "loss_value": 0.08007495850324631, "grad_norm": 0.0, "pref_ok": true, "pref_reason": "ok", "pref_mono_pass_rate": 1.0, "pref_swap_pass_rate": 1.0, "pref_gap_pass_rate": 1.0}
{"generation": 4, "index": 0, "attempt": 0, "ir": {"name": "FocalLogsigmoidWithDynamicTauAndClippedMargin", "intuition": "Mode: explore. This loss function refines the powerful combination of focal loss and adaptive margins from `FocalLogsigmoidRankGapLoss` by introducing two stability-focused coupling mechanisms. The goal is to retain the benefits of up-weighting hard examples and using cost-rank-based margins, while improving robustness against noisy batches or extreme log-probability values.\n\nInherited Ideas:\n1. From `FocalLogsigmoidRankGapLoss`: The core structure of applying a focal modulating factor `(1 - p_win)^gamma` to a base `logsigmoid` loss. This allows the model to focus on hard-to-classify pairs.\n2. From `AdaptiveMarginLossWithRankGap` (and `FocalLogsigmoidRankGapLoss`): The use of `rank_gap` to compute a non-parametric, batch-adaptive margin `softplus(beta * rank_gap - offset)`, making the loss robust to the absolute scale of costs.\n\nNew Coupling Ideas:\n1. **Dynamic Temperature Scaling (Tau):** The temperature `tau` for the focal probability `p_win = sigmoid(delta / tau)` is now dynamically scaled by the standard deviation of the log-probability differences (`delta.std()`). This makes the focal modulation's sensitivity adaptive: in batches with high variance in `delta` (indicating a mix of easy and hard examples), `tau` increases, softening the `sigmoid` and preventing extreme focal weights. In low-variance batches, `tau` decreases, allowing for finer-grained distinctions.\n2. **Clipped Margin for Stability:** The adaptive margin, calculated from `rank_gap`, is now clipped to a maximum value `margin_max`. This prevents pairs with extremely large rank gaps from creating excessively large margins, which could dominate the loss signal and cause instability, especially when combined with an unbounded loss function like `logsigmoid`.", "pseudocode": "1. Calculate the log-probability difference: delta = log_prob_w - log_prob_l.\n2. Compute the cost rank gap: rank_gap = rank_gap(cost_a, cost_b).\n3. Calculate an adaptive margin: margin_raw = softplus(beta * rank_gap - offset).\n4. Clip the margin to prevent extreme values: margin = clamp(margin_raw, max=margin_max).\n5. Compute the core preference loss: loss_core = -logsigmoid(delta + margin).\n6. Compute a dynamic temperature based on the standard deviation of delta: tau = softplus(delta.std()).\n7. Calculate the probability of preferring the winner: p_win = sigmoid(delta / tau).\n8. Compute the focal modulating factor: modulating_factor = (1 - p_win)^gamma.\n9. Apply the detached modulating factor to the core loss: final_loss = modulating_factor.detach() * loss_core.\n10. Return the mean of the final loss.", "hyperparams": {"beta": 1.5, "offset": 0.25, "gamma": 2.0, "margin_max": 3.0}, "operators_used": ["rank_gap", "softplus", "logsigmoid", "sigmoid", "clamp"], "implementation_hint": {"expects": ["cost_a", "cost_b", "log_prob_w", "log_prob_l"], "returns": "scalar"}, "code": "def generated_loss(batch, model_output, extra):\n    \"\"\"\n    Combines a rank-gap adaptive margin with a focal loss mechanism on a logsigmoid base.\n    Introduces a clipped margin for stability and a dynamic temperature for the focal component based on delta's stddev.\n    \"\"\"\n    # Read hyperparameters with defaults\n    beta = extra.get('beta', 1.5)\n    offset = extra.get('offset', 0.25)\n    gamma = extra.get('gamma', 2.0)\n    margin_max = extra.get('margin_max', 3.0)\n\n    # Read required tensors from the batch\n    cost_a = batch['cost_a']\n    cost_b = batch['cost_b']\n    log_prob_w = batch['log_prob_w']\n    log_prob_l = batch['log_prob_l']\n    weight = batch.get('weight')\n\n    # 1. Calculate the log-probability difference\n    delta = log_prob_w - log_prob_l\n\n    # 2. Inherited: Compute the rank gap of the costs\n    rank_gap = ops.rank_gap(cost_a, cost_b)\n\n    # 3. Inherited: Calculate an adaptive margin from the rank gap\n    margin_raw = F.softplus(beta * rank_gap - offset)\n    \n    # 4. New Coupling: Clip the margin for stability\n    margin = torch.clamp(margin_raw, max=margin_max)\n\n    # 5. Compute the core preference loss using logsigmoid\n    loss_core = -F.logsigmoid(delta + margin)\n\n    # 6. New Coupling: Dynamic temperature for focal modulation\n    # Use detached std dev of delta to avoid influencing gradients through tau\n    # Add a small epsilon for numerical stability if std is zero\n    tau = F.softplus(delta.detach().std()).clamp(min=1e-4)\n\n    # 7. Inherited: Calculate probability for focal loss\n    p_win = torch.sigmoid(delta / tau)\n\n    # 8. Inherited: Compute the focal modulating factor\n    modulating_factor = torch.pow(1.0 - p_win, gamma)\n\n    # 9. Apply the modulating factor to the core loss\n    # Detach to ensure focal term only re-weights the loss, not changes its gradient direction\n    final_loss = modulating_factor.detach() * loss_core\n\n    # Apply optional instance weights if provided\n    if weight is not None:\n        final_loss = final_loss * weight\n\n    # Return the mean loss over the batch\n    return final_loss.mean()", "theoretical_basis": "A dynamically modulated Bradley-Terry preference model. The core loss is logistic (-logsigmoid), applied to a log-probability difference augmented by a clipped, rank-based margin. This is coupled with a focal modulation mechanism where the temperature is adaptive to the batch-wise variance of log-probability differences, providing a self-tuning mechanism for focusing on hard examples."}, "static_ok": true, "static_reason": "", "static_error_code": "", "dynamic_ok": true, "dynamic_reason": "ok", "loss_value": 0.05026540532708168, "grad_norm": 0.0, "pref_ok": true, "pref_reason": "ok", "pref_mono_pass_rate": 1.0, "pref_swap_pass_rate": 0.8333333333333334, "pref_gap_pass_rate": 1.0}
{"generation": 4, "index": 1, "attempt": 0, "ir": {"name": "FocalLogsigmoidClippedRankGapLoss", "intuition": "Mode: explore. This loss function refines the successful combination of focal loss and rank-gap margins seen in the parents, but introduces a bounded loss function and a novel normalization scheme to enhance stability and focus learning. \n\nInherited ideas:\n1. From `FocalLogsigmoidRankGapLoss`: The use of a focal modulating factor `(1 - p_win)^gamma` to up-weight difficult examples where the model confidently prefers the incorrect solution. This provides an effective hard-example mining mechanism.\n2. From `AdaptiveMarginLossWithRankGap` and `FocalLogsigmoidRankGapLoss`: The use of `rank_gap` to compute a non-parametric, batch-adaptive margin `margin = softplus(beta * rank_gap - offset)`. This ensures the learning target is robust to the absolute scale of costs.\n\nNew coupling ideas:\n1. **Bounded Loss Function**: The core loss is changed from the unbounded `-logsigmoid` to a bounded `-tanh(softplus(normalized_delta + margin))`. The `softplus` ensures the argument to `tanh` is non-negative, mapping the loss to the range [-1, 0]. This prevents extremely large loss values from dominating the gradient, improving stability, while still providing a strong learning signal.\n2. **Range Normalization**: Instead of z-scoring `delta`, it is normalized by the range of `log_prob_w` in the batch: `normalized_delta = delta / (log_prob_w.max() - log_prob_w.min() + epsilon)`. This normalization is less sensitive to outliers than z-scoring (which uses standard deviation) and scales the learning signal relative to the model's current confidence range for the winning solutions.", "pseudocode": "1. Calculate the log-probability difference: delta = log_prob_w - log_prob_l.\n2. Normalize delta by the range of winning log-probabilities: normalized_delta = delta / (max(log_prob_w) - min(log_prob_w) + epsilon).\n3. Compute the cost rank gap: rank_gap = rank_gap(cost_a, cost_b).\n4. Calculate an adaptive margin from the rank gap: margin = softplus(beta * rank_gap - offset).\n5. Compute the core preference loss using a bounded tanh function: loss_core = -tanh(softplus(normalized_delta + margin)).\n6. Calculate the probability of preferring the winner for the focal term: p_win = sigmoid(delta).\n7. Compute the focal modulating factor: modulating_factor = (1 - p_win)^gamma.\n8. Apply the detached modulating factor to the core loss: final_loss = modulating_factor.detach() * loss_core.\n9. Return the mean of the final loss.", "hyperparams": {"beta": 1.5, "offset": 0.25, "gamma": 2.0, "epsilon": 1e-06}, "operators_used": ["rank_gap", "softplus", "tanh", "sigmoid"], "implementation_hint": {"expects": ["cost_a", "cost_b", "log_prob_w", "log_prob_l"], "returns": "scalar"}, "code": "def generated_loss(batch, model_output, extra):\n    \"\"\"\n    Combines a rank-gap adaptive margin with a focal loss mechanism, applied to a bounded tanh base loss.\n    Introduces range normalization for the log-probability difference.\n    \"\"\"\n    # Read hyperparameters with defaults\n    beta = extra.get('beta', 1.5)\n    offset = extra.get('offset', 0.25)\n    gamma = extra.get('gamma', 2.0)\n    epsilon = extra.get('epsilon', 1e-6)\n\n    # Read required tensors from the batch\n    cost_a = batch['cost_a']\n    cost_b = batch['cost_b']\n    log_prob_w = batch['log_prob_w']\n    log_prob_l = batch['log_prob_l']\n    weight = batch.get('weight')\n\n    # 1. Calculate the log-probability difference\n    delta = log_prob_w - log_prob_l\n\n    # 2. New Coupling: Normalize delta by the range of winning log-probs for stability\n    with torch.no_grad():\n        log_prob_w_range = log_prob_w.max() - log_prob_w.min()\n    normalized_delta = delta / (log_prob_w_range + epsilon)\n\n    # 3. Inherited: Compute the rank gap of the costs\n    rank_gap = ops.rank_gap(cost_a, cost_b)\n\n    # 4. Inherited: Calculate an adaptive margin from the rank gap\n    margin = F.softplus(beta * rank_gap - offset)\n\n    # 5. New Coupling: Compute a bounded core loss using tanh and softplus\n    # softplus ensures the argument to tanh is non-negative, mapping loss to [-1, 0]\n    loss_core = -torch.tanh(F.softplus(normalized_delta + margin))\n\n    # 6. Inherited: Calculate probability for focal loss (using original delta for confidence measure)\n    p_win = torch.sigmoid(delta)\n\n    # 7. Inherited: Compute the focal modulating factor\n    modulating_factor = torch.pow(1.0 - p_win, gamma)\n\n    # 8. Apply the detached modulating factor to the core loss\n    final_loss = modulating_factor.detach() * loss_core\n\n    # Apply optional instance weights if provided\n    if weight is not None:\n        final_loss = final_loss * weight\n\n    # Return the mean loss over the batch\n    return final_loss.mean()", "theoretical_basis": "A hybrid Bradley-Terry and margin-based model. It uses a focal-modulated, bounded loss function. The preference is modeled by requiring the normalized log-probability difference to exceed a dynamic, rank-based margin. The novel range-normalization provides stability against outliers, while the bounded tanh loss prevents gradient explosion. The focal component re-weights examples based on model confidence, focusing on hard negatives."}, "static_ok": true, "static_reason": "", "static_error_code": "", "dynamic_ok": true, "dynamic_reason": "ok", "loss_value": -0.23394547402858734, "grad_norm": 0.0, "pref_ok": false, "pref_reason": "pref_semantic_violation", "pref_mono_pass_rate": 1.0, "pref_swap_pass_rate": 1.0, "pref_gap_pass_rate": 0.0, "dynamic_error_code": "E_PREF_SEMANTIC"}
{"generation": 4, "index": 1, "attempt": 1, "ir": {"name": "FocalLogsigmoidClippedRankGapLoss", "intuition": "Repaired: The original loss failed the `E_PREF_SEMANTIC` gate check, specifically the `gap_pass_rate` was 0.0. This indicates that as the cost gap between the winning and losing solutions increases, the loss did not decrease, which is a core requirement for a preference loss. This was because the `rank_gap` term was not correctly scaled to reflect the cost difference. The `rank_gap` operator is non-parametric and its output scale is not directly proportional to the cost difference, which is what the semantic gate tests. I have replaced `ops.rank_gap(cost_a, cost_b)` with the direct cost difference `cost_b - cost_a`. Since `cost_a` is the lower (better) cost, this difference is positive and scales with the cost gap, satisfying the gate's semantic requirement while preserving the adaptive margin idea.", "pseudocode": "1. Calculate the log-probability difference: delta = log_prob_w - log_prob_l.\n2. Normalize delta by the range of winning log-probabilities: normalized_delta = delta / (max(log_prob_w) - min(log_prob_w) + epsilon).\n3. Compute the cost difference: cost_diff = cost_b - cost_a.\n4. Calculate an adaptive margin from the cost difference: margin = softplus(beta * cost_diff - offset).\n5. Compute the core preference loss using a bounded tanh function: loss_core = -tanh(softplus(normalized_delta + margin)).\n6. Calculate the probability of preferring the winner for the focal term: p_win = sigmoid(delta).\n7. Compute the focal modulating factor: modulating_factor = (1 - p_win)^gamma.\n8. Apply the detached modulating factor to the core loss: final_loss = modulating_factor.detach() * loss_core.\n9. Return the mean of the final loss.", "hyperparams": {"beta": 1.5, "offset": 0.25, "gamma": 2.0, "epsilon": 1e-06}, "operators_used": ["softplus", "tanh", "sigmoid"], "implementation_hint": {"expects": ["cost_a", "cost_b", "log_prob_w", "log_prob_l"], "returns": "scalar"}, "code": "def generated_loss(batch, model_output, extra):\n    \"\"\"\n    Combines a cost-difference adaptive margin with a focal loss mechanism, applied to a bounded tanh base loss.\n    Introduces range normalization for the log-probability difference.\n    \"\"\"\n    # Read hyperparameters with defaults\n    beta = extra.get('beta', 1.5)\n    offset = extra.get('offset', 0.25)\n    gamma = extra.get('gamma', 2.0)\n    epsilon = extra.get('epsilon', 1e-6)\n\n    # Read required tensors from the batch\n    cost_a = batch['cost_a']\n    cost_b = batch['cost_b']\n    log_prob_w = batch['log_prob_w']\n    log_prob_l = batch['log_prob_l']\n    weight = batch.get('weight')\n\n    # 1. Calculate the log-probability difference\n    delta = log_prob_w - log_prob_l\n\n    # 2. New Coupling: Normalize delta by the range of winning log-probs for stability\n    with torch.no_grad():\n        log_prob_w_range = log_prob_w.max() - log_prob_w.min()\n    normalized_delta = delta / (log_prob_w_range + epsilon)\n\n    # 3. Compute the cost difference (cost_a is the lower/better cost)\n    cost_diff = cost_b - cost_a\n\n    # 4. Inherited: Calculate an adaptive margin from the cost difference\n    margin = F.softplus(beta * cost_diff - offset)\n\n    # 5. New Coupling: Compute a bounded core loss using tanh and softplus\n    # softplus ensures the argument to tanh is non-negative, mapping loss to [-1, 0]\n    loss_core = -torch.tanh(F.softplus(normalized_delta + margin))\n\n    # 6. Inherited: Calculate probability for focal loss (using original delta for confidence measure)\n    p_win = torch.sigmoid(delta)\n\n    # 7. Inherited: Compute the focal modulating factor\n    modulating_factor = torch.pow(1.0 - p_win, gamma)\n\n    # 8. Apply the detached modulating factor to the core loss\n    final_loss = modulating_factor.detach() * loss_core\n\n    # Apply optional instance weights if provided\n    if weight is not None:\n        final_loss = final_loss * weight\n\n    # Return the mean loss over the batch\n    return final_loss.mean()", "theoretical_basis": ""}, "static_ok": true, "static_reason": "", "static_error_code": "", "dynamic_ok": true, "dynamic_reason": "ok", "loss_value": -0.23394547402858734, "grad_norm": 0.0, "pref_ok": false, "pref_reason": "pref_semantic_violation", "pref_mono_pass_rate": 1.0, "pref_swap_pass_rate": 1.0, "pref_gap_pass_rate": 0.0, "dynamic_error_code": "E_PREF_SEMANTIC"}
{"generation": 4, "index": 1, "attempt": 2, "ir": {"name": "FocalLogsigmoidClippedRankGapLoss", "intuition": "The original loss failed the `E_PREF_SEMANTIC` gate check with `gap_pass_rate: 0.0`. This indicates that the loss did not consistently decrease as the cost gap between the winning and losing solutions increased. The root cause was the normalization of the log-probability difference `delta` by `log_prob_w_range`, which is a batch-dependent statistic. This normalization broke the direct relationship between the cost gap (via the margin) and the final loss value for a single example, which the gate check verifies. I have removed this normalization step. The loss now directly uses `delta + margin` as input to the `tanh(softplus(...))` structure, restoring the expected semantic behavior where a larger cost gap leads to a larger margin and thus a smaller (more negative) loss.", "pseudocode": "1. Calculate the log-probability difference: delta = log_prob_w - log_prob_l.\n2. Compute the cost difference: cost_diff = cost_b - cost_a.\n3. Calculate an adaptive margin from the cost difference: margin = softplus(beta * cost_diff - offset).\n4. Compute the core preference loss using a bounded tanh function: loss_core = -tanh(softplus(delta + margin)).\n5. Calculate the probability of preferring the winner for the focal term: p_win = sigmoid(delta).\n6. Compute the focal modulating factor: modulating_factor = (1 - p_win)^gamma.\n7. Apply the detached modulating factor to the core loss: final_loss = modulating_factor.detach() * loss_core.\n8. Return the mean of the final loss.", "hyperparams": {"beta": 1.5, "offset": 0.25, "gamma": 2.0}, "operators_used": ["softplus", "tanh", "sigmoid"], "implementation_hint": {"expects": ["cost_a", "cost_b", "log_prob_w", "log_prob_l"], "returns": "scalar"}, "code": "def generated_loss(batch, model_output, extra):\n    \"\"\"\n    Combines a cost-difference adaptive margin with a focal loss mechanism, applied to a bounded tanh base loss.\n    \"\"\"\n    # Read hyperparameters with defaults\n    beta = extra.get('beta', 1.5)\n    offset = extra.get('offset', 0.25)\n    gamma = extra.get('gamma', 2.0)\n\n    # Read required tensors from the batch\n    cost_a = batch['cost_a']\n    cost_b = batch['cost_b']\n    log_prob_w = batch['log_prob_w']\n    log_prob_l = batch['log_prob_l']\n    weight = batch.get('weight')\n\n    # 1. Calculate the log-probability difference\n    delta = log_prob_w - log_prob_l\n\n    # 2. Compute the cost difference (cost_a is the lower/better cost)\n    cost_diff = cost_b - cost_a\n\n    # 3. Inherited: Calculate an adaptive margin from the cost difference\n    margin = F.softplus(beta * cost_diff - offset)\n\n    # 4. New Coupling: Compute a bounded core loss using tanh and softplus\n    # softplus ensures the argument to tanh is non-negative, mapping loss to [-1, 0]\n    # The normalization of delta was removed to fix the E_PREF_SEMANTIC failure.\n    loss_core = -torch.tanh(F.softplus(delta + margin))\n\n    # 5. Inherited: Calculate probability for focal loss (using original delta for confidence measure)\n    p_win = torch.sigmoid(delta)\n\n    # 6. Inherited: Compute the focal modulating factor\n    modulating_factor = torch.pow(1.0 - p_win, gamma)\n\n    # 7. Apply the detached modulating factor to the core loss\n    final_loss = modulating_factor.detach() * loss_core\n\n    # Apply optional instance weights if provided\n    if weight is not None:\n        final_loss = final_loss * weight\n\n    # Return the mean loss over the batch\n    return final_loss.mean()", "theoretical_basis": ""}, "static_ok": true, "static_reason": "", "static_error_code": "", "dynamic_ok": true, "dynamic_reason": "ok", "loss_value": -0.23394547402858734, "grad_norm": 0.0, "pref_ok": true, "pref_reason": "ok", "pref_mono_pass_rate": 1.0, "pref_swap_pass_rate": 1.0, "pref_gap_pass_rate": 1.0}
{"generation": 4, "index": 2, "attempt": 0, "ir": {"name": "FocalRankGapLossWithAdaptiveTemperatureClipping", "intuition": "Mode: explore. This loss function refines the successful combination of focal loss and rank-gap margins seen in `FocalLogsigmoidRankGapLoss`. It inherits the core Bradley-Terry `logsigmoid` structure and the adaptive rank-gap margin. The key innovation is to introduce a more robust and responsive adaptive temperature for the focal component, while also simplifying the overall structure by removing the z-score normalization on the log-probability difference, which can sometimes over-aggressively rescale gradients.\n\nInherited Ideas:\n1. From `FocalLogsigmoidRankGapLoss`: The core structure of applying a focal modulating factor `(1 - p_win)^gamma` to a `logsigmoid` loss to up-weight hard examples.\n2. From both parents: The use of `rank_gap` to create a dynamic, batch-adaptive margin `margin = softplus(beta * rank_gap - offset)`, making the loss robust to the absolute scale of costs.\n\nNew Coupling Ideas:\n1. **Clipped Adaptive Temperature**: The temperature `tau` for the focal modulation is made adaptive based on the standard deviation of the log-probability differences (`delta.std()`) rather than just the winning log-probabilities. This makes `tau` more sensitive to the model's overall confidence on a pair. A learnable scalar `tau_scale` is introduced, and the result is clamped to a minimum value `tau_min` to prevent numerical instability (division by zero) and excessively sharp probability distributions on confident batches.\n2. **Direct Delta Usage**: Instead of normalizing `delta` with z-score, this loss uses the raw `delta` in the `logsigmoid` function. This provides a more direct and potentially stronger gradient signal, relying on the focal mechanism and the adaptive margin to handle difficult examples, rather than batch-level normalization.", "pseudocode": "1. Calculate the log-probability difference: delta = log_prob_w - log_prob_l.\n2. Compute the cost rank gap: rank_gap = rank_gap(cost_a, cost_b).\n3. Calculate an adaptive margin from the rank gap: margin = softplus(beta * rank_gap - offset).\n4. Compute the core preference loss using the raw delta and margin: loss_core = -logsigmoid(delta + margin).\n5. Compute a new adaptive temperature: tau = clamp(tau_scale * softplus(delta.std()), min=tau_min).\n6. Calculate the probability of preferring the winner using this adaptive temperature: p_win = sigmoid(delta / tau).\n7. Compute the focal modulating factor: modulating_factor = (1 - p_win)^gamma.\n8. Apply the detached modulating factor to the core loss: final_loss = modulating_factor.detach() * loss_core.\n9. Return the mean of the final loss.", "hyperparams": {"beta": 1.5, "offset": 0.25, "gamma": 2.0, "tau_scale": 0.8, "tau_min": 0.1}, "operators_used": ["rank_gap", "softplus", "logsigmoid", "sigmoid", "clamp"], "implementation_hint": {"expects": ["cost_a", "cost_b", "log_prob_w", "log_prob_l"], "returns": "scalar"}, "code": "def generated_loss(batch, model_output, extra):\n    \"\"\"\n    Combines a rank-gap adaptive margin with a focal loss mechanism on a logsigmoid base.\n    Introduces a clipped, adaptive temperature based on the standard deviation of log-prob differences.\n    Removes z-scoring on delta for a more direct gradient signal.\n    \"\"\"\n    # Read hyperparameters with defaults\n    beta = extra.get('beta', 1.5)\n    offset = extra.get('offset', 0.25)\n    gamma = extra.get('gamma', 2.0)\n    tau_scale = extra.get('tau_scale', 0.8)\n    tau_min = extra.get('tau_min', 0.1)\n\n    # Read required tensors from the batch\n    cost_a = batch['cost_a']\n    cost_b = batch['cost_b']\n    log_prob_w = batch['log_prob_w']\n    log_prob_l = batch['log_prob_l']\n    weight = batch.get('weight')\n\n    # 1. Calculate the log-probability difference\n    delta = log_prob_w - log_prob_l\n\n    # 2. Inherited: Compute the rank gap of the costs\n    rank_gap = ops.rank_gap(cost_a, cost_b)\n\n    # 3. Inherited: Calculate an adaptive margin from the rank gap\n    margin = F.softplus(beta * rank_gap - offset)\n\n    # 4. Compute the core preference loss using logsigmoid on the raw delta\n    loss_core = -F.logsigmoid(delta + margin)\n\n    # 5. New Coupling: Clipped adaptive temperature for focal modulation\n    # Use detached std dev of delta to avoid influencing gradients through tau\n    # softplus ensures the std dev is handled gracefully if it's zero\n    tau = torch.clamp(tau_scale * F.softplus(delta.detach().std()), min=tau_min)\n\n    # 6. Inherited: Calculate probability for focal loss\n    p_win = torch.sigmoid(delta / tau)\n\n    # 7. Inherited: Compute the focal modulating factor\n    modulating_factor = torch.pow(1.0 - p_win, gamma)\n\n    # 8. Apply the modulating factor to the core loss\n    final_loss = modulating_factor.detach() * loss_core\n\n    # Apply optional instance weights if provided\n    if weight is not None:\n        final_loss = final_loss * weight\n\n    # Return the mean loss over the batch\n    return final_loss.mean()", "theoretical_basis": "A Bradley-Terry preference model with two adaptive components. The learning target is dynamically adjusted via a rank-based margin, making it robust to cost scaling. The loss is weighted by a focal mechanism to focus on hard examples, where the temperature of the confidence estimate `p_win` is itself adaptive to the batch-wise variance of log-probability differences, providing a more stable and responsive mechanism for hard-example mining."}, "static_ok": true, "static_reason": "", "static_error_code": "", "dynamic_ok": true, "dynamic_reason": "ok", "loss_value": 0.05026540532708168, "grad_norm": 0.0, "pref_ok": true, "pref_reason": "ok", "pref_mono_pass_rate": 1.0, "pref_swap_pass_rate": 0.8333333333333334, "pref_gap_pass_rate": 1.0}
{"generation": 4, "index": 3, "attempt": 0, "ir": {"name": "AdaptiveFocalLogsigmoidWithDynamicTau", "intuition": "Mode: explore. This loss function hybridizes the strongest components of its parents: the robust, unbounded `logsigmoid` loss from `FocalLogsigmoidRankGapLoss` and the simple, effective `rank_gap`-based margin from `AdaptiveMarginLossWithRankGap`. It discards the z-scoring of the first parent, which can be sensitive to outliers, in favor of a simpler, direct application of the margin.\n\nInherited Ideas:\n1. From `FocalLogsigmoidRankGapLoss`: The core use of a focal modulating factor `(1 - p_win)^gamma` to up-weight hard examples, applied to a standard `-logsigmoid` loss. This combination is theoretically sound (Bradley-Terry + focal modulation) and empirically strong.\n2. From `AdaptiveMarginLossWithRankGap`: The `rank_gap`-based adaptive margin `softplus(beta * rank_gap - offset)`. This non-parametric margin is robust to cost scaling and adapts the learning target to the relative quality of solutions within a batch.\n\nNew Coupling Ideas:\n1. **Dynamic Temperature (Tau) based on Cost Gap**: The temperature `tau` for the focal loss's sigmoid is now proportional to the cost gap `(cost_b - cost_a)`. Specifically, `tau = 1.0 + tau_scale * (cost_b - cost_a).detach()`. When the cost difference is small (hard pairs), `tau` is small, making the sigmoid sharper and the focal modulation more sensitive to small changes in `delta`. When the cost difference is large (easy pairs), `tau` is large, softening the sigmoid and reducing the focal penalty for misclassifying obvious pairs. This links the 'hardness' defined by cost to the 'hardness' defined by model confidence.\n2. **Margin-Modulated Focal Strength**: The focal strength `gamma` is dynamically adjusted by the margin size: `dynamic_gamma = gamma * torch.sigmoid(margin).detach()`. For pairs with a small margin (i.e., small `rank_gap`), `dynamic_gamma` is also small, reducing the focal penalty. This prevents the loss from over-penalizing hard-to-distinguish pairs. For pairs with a large margin (clear winners), `dynamic_gamma` approaches the full `gamma`, strongly penalizing confident mistakes on what should be easy examples.", "pseudocode": "1. Calculate the log-probability difference: delta = log_prob_w - log_prob_l.\n2. Compute the cost rank gap: rank_gap = rank_gap(cost_a, cost_b).\n3. Calculate an adaptive margin from the rank gap: margin = softplus(beta * rank_gap - offset).\n4. Compute the core preference loss: loss_core = -logsigmoid(delta + margin).\n5. Compute a dynamic temperature based on the cost gap: tau = 1.0 + tau_scale * softplus(cost_b - cost_a).detach().\n6. Calculate the probability of preferring the winner using this dynamic temperature: p_win = sigmoid(delta / tau).\n7. Compute a dynamic focal gamma modulated by the margin: dynamic_gamma = gamma * sigmoid(margin).detach().\n8. Compute the focal modulating factor: modulating_factor = (1 - p_win)^dynamic_gamma.\n9. Apply the detached modulating factor to the core loss: final_loss = modulating_factor.detach() * loss_core.\n10. Return the mean of the final loss.", "hyperparams": {"beta": 1.5, "offset": 0.5, "gamma": 2.0, "tau_scale": 0.1}, "operators_used": ["rank_gap", "softplus", "logsigmoid", "sigmoid"], "implementation_hint": {"expects": ["cost_a", "cost_b", "log_prob_w", "log_prob_l"], "returns": "scalar"}, "code": "def generated_loss(batch, model_output, extra):\n    \"\"\"\n    Combines a rank-gap margin with a focal logsigmoid loss.\n    Introduces two new couplings: \n    1. The focal temperature (tau) is scaled by the cost gap.\n    2. The focal strength (gamma) is modulated by the margin size.\n    \"\"\"\n    # Read hyperparameters with defaults\n    beta = extra.get('beta', 1.5)\n    offset = extra.get('offset', 0.5)\n    gamma = extra.get('gamma', 2.0)\n    tau_scale = extra.get('tau_scale', 0.1)\n\n    # Read required tensors from the batch\n    cost_a = batch['cost_a']\n    cost_b = batch['cost_b']\n    log_prob_w = batch['log_prob_w']\n    log_prob_l = batch['log_prob_l']\n    weight = batch.get('weight')\n\n    # 1. Calculate the log-probability difference\n    delta = log_prob_w - log_prob_l\n\n    # 2. Inherited: Compute the rank gap of the costs\n    rank_gap = ops.rank_gap(cost_a, cost_b)\n\n    # 3. Inherited: Calculate an adaptive margin from the rank gap\n    margin = F.softplus(beta * rank_gap - offset)\n\n    # 4. Compute the core preference loss (Bradley-Terry style)\n    loss_core = -F.logsigmoid(delta + margin)\n\n    # 5. New Coupling 1: Dynamic temperature based on cost gap\n    # Use softplus on cost gap to ensure it's non-negative and scaled reasonably.\n    cost_gap = (cost_b - cost_a).detach()\n    # Add 1.0 for a baseline temperature, clamp for stability\n    tau = (1.0 + tau_scale * F.softplus(cost_gap)).clamp(min=1e-2)\n\n    # 6. Calculate probability for focal loss with dynamic tau\n    p_win = torch.sigmoid(delta.detach() / tau)\n\n    # 7. New Coupling 2: Margin-modulated focal strength\n    # As margin -> 0, sigmoid(margin) -> 0.5. As margin grows, -> 1.0\n    # This scales down gamma for pairs with small margins (hard pairs).\n    dynamic_gamma = gamma * torch.sigmoid(margin.detach())\n\n    # 8. Compute the focal modulating factor\n    # Use clamp to prevent log(0) if p_win is exactly 1.\n    modulating_factor = torch.pow(1.0 - p_win.clamp(max=1.0 - 1e-6), dynamic_gamma)\n\n    # 9. Apply the modulating factor to the core loss\n    final_loss = modulating_factor.detach() * loss_core\n\n    # Apply optional instance weights if provided\n    if weight is not None:\n        final_loss = final_loss * weight\n\n    # Return the mean loss over the batch\n    return final_loss.mean()", "theoretical_basis": "A dynamically modulated Bradley-Terry preference model. The core loss is logistic (-logsigmoid) with a rank-based adaptive margin. This is coupled with a focal loss whose strength (gamma) and sensitivity (temperature) are dynamically adapted based on the margin size and cost gap, respectively. This creates a multi-faceted 'hard example mining' strategy that considers both model confidence and the inherent difficulty of the preference pair."}, "static_ok": true, "static_reason": "", "static_error_code": "", "dynamic_ok": true, "dynamic_reason": "ok", "loss_value": 0.07988262921571732, "grad_norm": 0.0, "pref_ok": true, "pref_reason": "ok", "pref_mono_pass_rate": 1.0, "pref_swap_pass_rate": 0.8333333333333334, "pref_gap_pass_rate": 1.0}
{"generation": 4, "index": 4, "attempt": 0, "ir": {"name": "AdaptiveFocalLogsigmoidWithDynamicTau", "intuition": "Mode: explore. This loss combines the strong Bradley-Terry foundation of `FocalLogsigmoidRankGapLoss` with a more principled adaptive margin. The core idea is to create a loss that is robust to the scale of both costs and log-probabilities, while dynamically focusing on the most informative examples.\n\nInherited ideas:\n1. From `FocalLogsigmoidRankGapLoss`: The use of a focal modulation factor `(1 - p_win)^gamma` to up-weight difficult examples, where the model is confident about the wrong answer. This provides an effective 'hard example mining' mechanism.\n2. From `FocalLogsigmoidRankGapLoss` and `AdaptiveMarginLossWithRankGap`: The use of `rank_gap` to create a non-parametric, batch-adaptive margin that is robust to the absolute scale of costs.\n\nNew coupling ideas:\n1. **Dynamic Temperature (Tau) based on Cost Rank Gap**: Instead of a fixed or log-prob-std-based temperature for the focal loss `p_win` calculation, this child couples `tau` directly to the cost difference via the `rank_gap`. `tau` is now `softplus(log_prob_w.detach().std() + cost_rank_gap)`. This makes the focal modulation more sensitive (smaller tau) for pairs with small cost differences, forcing the model to be more decisive on subtle distinctions. Conversely, it's less sensitive for pairs with large cost differences where the preference is obvious.\n2. **Log-Prob Range Normalization**: To improve stability with the unbounded `logsigmoid` loss, the log-probability difference `delta` is normalized by the batch-wise range of `delta`. This is a gentler normalization than z-scoring, as it preserves the zero-centering of `delta` while preventing extreme values from causing exploding gradients.", "pseudocode": "1. Calculate the log-probability difference: delta = log_prob_w - log_prob_l.\n2. Normalize delta by its batch-wise range for stability: normalized_delta = delta / (delta.max() - delta.min() + epsilon).\n3. Compute the cost rank gap: rank_gap = rank_gap(cost_a, cost_b).\n4. Calculate an adaptive margin from the rank gap: margin = softplus(beta * rank_gap - offset).\n5. Compute the core loss using the normalized delta and margin: loss_core = -logsigmoid(normalized_delta + margin).\n6. Compute a dynamic temperature (tau) coupled with both log-prob standard deviation and the cost rank gap: tau = softplus(log_prob_w.detach().std() + rank_gap).\n7. Calculate the probability of preferring the winner using the original delta and dynamic tau: p_win = sigmoid(delta / tau).\n8. Compute the focal modulating factor: modulating_factor = (1 - p_win)^gamma.\n9. Apply the detached modulating factor to the core loss: final_loss = modulating_factor.detach() * loss_core.\n10. Return the mean of the final loss.", "hyperparams": {"beta": 1.5, "offset": 0.25, "gamma": 2.0, "epsilon": 1e-06}, "operators_used": ["logsigmoid", "sigmoid", "rank_gap", "softplus"], "implementation_hint": {"expects": ["cost_a", "cost_b", "log_prob_w", "log_prob_l"], "returns": "scalar"}, "code": "def generated_loss(batch, model_output, extra):\n    \"\"\"\n    Combines a rank-gap adaptive margin and focal loss with two new couplings:\n    1. Log-probability difference (`delta`) is normalized by its range for stability.\n    2. The focal loss temperature (`tau`) is coupled with the cost rank gap, making it adaptive\n       not just to model confidence but also to the difficulty of the preference pair.\n    \"\"\"\n    # Read hyperparameters with defaults\n    beta = extra.get('beta', 1.5)\n    offset = extra.get('offset', 0.25)\n    gamma = extra.get('gamma', 2.0)\n    epsilon = extra.get('epsilon', 1e-6)\n\n    # Read required tensors from the batch\n    cost_a = batch['cost_a']\n    cost_b = batch['cost_b']\n    log_prob_w = batch['log_prob_w']\n    log_prob_l = batch['log_prob_l']\n    weight = batch.get('weight')\n\n    # 1. Calculate the log-probability difference\n    delta = log_prob_w - log_prob_l\n\n    # 2. New Coupling: Normalize delta by its range for stability\n    delta_range = torch.max(delta) - torch.min(delta)\n    normalized_delta = delta / (delta_range.detach() + epsilon)\n\n    # 3. Inherited: Compute the rank gap of the costs\n    rank_gap = ops.rank_gap(cost_a, cost_b)\n\n    # 4. Inherited: Calculate an adaptive margin from the rank gap\n    margin = F.softplus(beta * rank_gap - offset)\n\n    # 5. Compute the core preference loss using logsigmoid\n    loss_core = -F.logsigmoid(normalized_delta + margin)\n\n    # 6. New Coupling: Dynamic temperature coupled with cost rank gap\n    # Detach inputs to tau to prevent gradients from flowing through it\n    log_prob_std_detached = log_prob_w.detach().std()\n    rank_gap_detached = rank_gap.detach()\n    tau = F.softplus(log_prob_std_detached + rank_gap_detached).clamp(min=1e-2)\n\n    # 7. Inherited: Calculate probability for focal loss using original delta\n    p_win = torch.sigmoid(delta / tau)\n\n    # 8. Inherited: Compute the focal modulating factor\n    modulating_factor = torch.pow(1.0 - p_win, gamma)\n\n    # 9. Apply the detached modulating factor to the core loss\n    final_loss = modulating_factor.detach() * loss_core\n\n    # Apply optional instance weights if provided\n    if weight is not None:\n        final_loss = final_loss * weight\n\n    # Return the mean loss over the batch\n    return final_loss.mean()", "theoretical_basis": "A dynamically modulated Bradley-Terry preference model. The core is a logistic loss on a margin-adjusted, range-normalized log-probability difference. The focal modulation is made adaptive by coupling its temperature to both the model's output uncertainty (log-prob std) and the problem difficulty (cost rank gap), allowing the loss to dynamically adjust its focus based on both model state and data characteristics."}, "static_ok": true, "static_reason": "", "static_error_code": "", "dynamic_ok": true, "dynamic_reason": "ok", "loss_value": 0.05026540532708168, "grad_norm": 0.0, "pref_ok": false, "pref_reason": "pref_semantic_violation", "pref_mono_pass_rate": 1.0, "pref_swap_pass_rate": 1.0, "pref_gap_pass_rate": 0.0, "dynamic_error_code": "E_PREF_SEMANTIC"}
{"generation": 4, "index": 4, "attempt": 1, "ir": {"name": "AdaptiveFocalLogsigmoidWithDynamicTau", "intuition": "Repaired: The original loss failed the `preference_gate` with code `E_PREF_SEMANTIC` because the `gap_pass_rate` was 0.0. This indicates the loss did not decrease as the cost gap between the winner and loser increased. The root cause was the normalization of the log-probability difference (`delta`) by its batch-wise range. This normalization makes the loss for a single example dependent on all other examples in the batch, which can break the expected semantic relationship between cost gap and loss value. The fix is to remove this range-based normalization of `delta`, allowing the core loss to respond directly to the raw log-probability difference, which is semantically correct.", "pseudocode": "1. Calculate the log-probability difference: delta = log_prob_w - log_prob_l.\n2. Compute the cost rank gap: rank_gap = rank_gap(cost_a, cost_b).\n3. Calculate an adaptive margin from the rank gap: margin = softplus(beta * rank_gap - offset).\n4. Compute the core loss using the original delta and margin: loss_core = -logsigmoid(delta + margin).\n5. Compute a dynamic temperature (tau) coupled with both log-prob standard deviation and the cost rank gap: tau = softplus(log_prob_w.detach().std() + rank_gap).\n6. Calculate the probability of preferring the winner using the original delta and dynamic tau: p_win = sigmoid(delta / tau).\n7. Compute the focal modulating factor: modulating_factor = (1 - p_win)^gamma.\n8. Apply the detached modulating factor to the core loss: final_loss = modulating_factor.detach() * loss_core.\n9. Return the mean of the final loss.", "hyperparams": {"beta": 1.5, "offset": 0.25, "gamma": 2.0, "epsilon": 1e-06}, "operators_used": ["logsigmoid", "sigmoid", "rank_gap", "softplus"], "implementation_hint": {"expects": ["cost_a", "cost_b", "log_prob_w", "log_prob_l"], "returns": "scalar"}, "code": "def generated_loss(batch, model_output, extra):\n    \"\"\"\n    Combines a rank-gap adaptive margin and focal loss with two new couplings:\n    1. Log-probability difference (`delta`) is normalized by its range for stability.\n    2. The focal loss temperature (`tau`) is coupled with the cost rank gap, making it adaptive\n       not just to model confidence but also to the difficulty of the preference pair.\n    \"\"\"\n    # Read hyperparameters with defaults\n    beta = extra.get('beta', 1.5)\n    offset = extra.get('offset', 0.25)\n    gamma = extra.get('gamma', 2.0)\n    epsilon = extra.get('epsilon', 1e-6)\n\n    # Read required tensors from the batch\n    cost_a = batch['cost_a']\n    cost_b = batch['cost_b']\n    log_prob_w = batch['log_prob_w']\n    log_prob_l = batch['log_prob_l']\n    weight = batch.get('weight')\n\n    # 1. Calculate the log-probability difference\n    delta = log_prob_w - log_prob_l\n\n    # 2. Inherited: Compute the rank gap of the costs\n    rank_gap = ops.rank_gap(cost_a, cost_b)\n\n    # 3. Inherited: Calculate an adaptive margin from the rank gap\n    margin = F.softplus(beta * rank_gap - offset)\n\n    # 4. Compute the core preference loss using logsigmoid\n    loss_core = -F.logsigmoid(delta + margin)\n\n    # 5. New Coupling: Dynamic temperature coupled with cost rank gap\n    # Detach inputs to tau to prevent gradients from flowing through it\n    log_prob_std_detached = log_prob_w.detach().std()\n    rank_gap_detached = rank_gap.detach()\n    tau = F.softplus(log_prob_std_detached + rank_gap_detached).clamp(min=1e-2)\n\n    # 6. Inherited: Calculate probability for focal loss using original delta\n    p_win = torch.sigmoid(delta / tau)\n\n    # 7. Inherited: Compute the focal modulating factor\n    modulating_factor = torch.pow(1.0 - p_win, gamma)\n\n    # 8. Apply the detached modulating factor to the core loss\n    final_loss = modulating_factor.detach() * loss_core\n\n    # Apply optional instance weights if provided\n    if weight is not None:\n        final_loss = final_loss * weight\n\n    # Return the mean loss over the batch\n    return final_loss.mean()", "theoretical_basis": ""}, "static_ok": true, "static_reason": "", "static_error_code": "", "dynamic_ok": true, "dynamic_reason": "ok", "loss_value": 0.05026540532708168, "grad_norm": 0.0, "pref_ok": true, "pref_reason": "ok", "pref_mono_pass_rate": 1.0, "pref_swap_pass_rate": 1.0, "pref_gap_pass_rate": 1.0}
{"generation": 4, "index": 5, "attempt": 0, "ir": {"name": "AdaptiveFocalLogsigmoidLoss_With_CostGapMargin", "intuition": "Mode: explore. This loss function refines the successful `FocalLogsigmoidRankGapLoss` by replacing its rank-based margin with a direct, clipped cost-gap margin, and introduces a learnable temperature parameter for the focal modulation. The goal is to create a more direct link between the cost difference and the learning signal, while also allowing the model to tune the hardness of its own example mining.\n\nInherited ideas:\n1.  From `FocalLogsigmoidRankGapLoss`: The core structure of a focal-modulated `logsigmoid` loss. This includes the base loss `loss_core = -logsigmoid(delta + margin)` and the focal modulation `modulating_factor = (1 - p_win)^gamma` to up-weight hard examples.\n2.  From `FocalLogsigmoidRankGapLoss`: The use of `zscore` normalization on the log-probability difference (`delta`). This is a crucial stability trick that prevents extreme log-probabilities from causing gradient explosions, especially with an unbounded loss like `logsigmoid`.\n\nNew coupling ideas:\n1.  **Clipped Cost-Gap Margin**: Instead of using a non-parametric `rank_gap` margin, this loss uses the raw cost difference `cost_b - cost_a` to define the margin. This margin is scaled by `beta` and then clipped to a maximum value `margin_max`. This provides a more direct signal related to the absolute improvement in cost, but with a cap to prevent extremely large cost gaps from dominating the batch and causing instability. The margin is `margin = torch.clamp(beta * (cost_b - cost_a), min=0, max=margin_max)`.\n2.  **Learnable Temperature**: The temperature `tau` in the focal component's probability calculation (`p_win = sigmoid(delta / tau)`) is made a learnable parameter of the model rather than being fixed or heuristically derived from batch statistics. This allows the network to learn the optimal sensitivity for distinguishing 'hard' from 'easy' examples over the course of training, effectively learning its own curriculum. It is implemented as a single parameter registered in the `extra` dictionary, initialized to 1.0.", "pseudocode": "1. Calculate the log-probability difference: delta = log_prob_w - log_prob_l.\n2. Normalize the delta across the batch for stability: normalized_delta = zscore(delta).\n3. Calculate the cost difference: cost_gap = cost_b - cost_a.\n4. Compute a clipped, scaled margin from the cost gap: margin = clamp(beta * cost_gap, min=0, max=margin_max).\n5. Compute the core preference loss: loss_core = -logsigmoid(normalized_delta + margin).\n6. Retrieve the learnable temperature parameter `tau` from the model's extra parameters.\n7. Calculate the probability of preferring the winner: p_win = sigmoid(delta / tau).\n8. Compute the focal modulating factor: modulating_factor = (1 - p_win)^gamma.\n9. Apply the detached modulating factor to the core loss: final_loss = modulating_factor.detach() * loss_core.\n10. Return the mean of the final loss over the batch.", "hyperparams": {"beta": 0.1, "gamma": 2.0, "margin_max": 2.0}, "operators_used": ["zscore", "clamp", "logsigmoid", "sigmoid"], "implementation_hint": {"expects": ["cost_a", "cost_b", "log_prob_w", "log_prob_l"], "returns": "scalar"}, "code": "def generated_loss(batch, model_output, extra):\n    \"\"\"\n    Combines a z-scored logsigmoid loss with focal modulation and a direct, clipped cost-gap margin.\n    Introduces a learnable temperature for the focal component.\n    \"\"\"\n    # Read hyperparameters with defaults\n    beta = extra.get('beta', 0.1)\n    gamma = extra.get('gamma', 2.0)\n    margin_max = extra.get('margin_max', 2.0)\n\n    # Read required tensors from the batch\n    cost_a = batch['cost_a']\n    cost_b = batch['cost_b']\n    log_prob_w = batch['log_prob_w']\n    log_prob_l = batch['log_prob_l']\n    weight = batch.get('weight')\n\n    # 1. Calculate the log-probability difference\n    delta = log_prob_w - log_prob_l\n\n    # 2. Inherited: Normalize delta for stability\n    normalized_delta = ops.zscore(delta)\n\n    # 3. New Coupling: Clipped Cost-Gap Margin\n    # Use the direct cost difference as the basis for the margin\n    cost_gap = cost_b - cost_a\n    # Scale and clip the margin to prevent instability from large cost gaps\n    margin = torch.clamp(beta * cost_gap, min=0.0, max=margin_max)\n\n    # 4. Compute the core preference loss\n    loss_core = -F.logsigmoid(normalized_delta + margin)\n\n    # 5. New Coupling: Learnable Temperature\n    # Initialize tau if it doesn't exist. It will be part of the model's parameters.\n    if 'learnable_tau' not in extra:\n        # Ensure it's on the correct device and requires gradients.\n        initial_tau = torch.tensor(1.0, device=delta.device, dtype=delta.dtype)\n        extra['learnable_tau'] = torch.nn.Parameter(initial_tau)\n    \n    # Use softplus to ensure tau is always positive.\n    tau = F.softplus(extra['learnable_tau'])\n\n    # 6. Inherited: Calculate probability for focal loss using the learned tau\n    p_win = torch.sigmoid(delta.detach() / tau)\n\n    # 7. Inherited: Compute the focal modulating factor\n    modulating_factor = torch.pow(1.0 - p_win, gamma)\n\n    # 8. Apply the modulating factor to the core loss\n    final_loss = modulating_factor.detach() * loss_core\n\n    # Apply optional instance weights if provided\n    if weight is not None:\n        final_loss = final_loss * weight\n\n    # Return the mean loss over the batch\n    return final_loss.mean()", "theoretical_basis": "A hybrid Bradley-Terry and focal loss model with a direct cost-aware margin. The core preference is modeled via a logistic function on a z-scored log-probability difference, augmented by a margin proportional to the true cost improvement. This is coupled with a focal modulation mechanism that uses a learnable temperature to dynamically re-weight training examples, allowing the model to learn its own 'hard example mining' sensitivity."}, "static_ok": true, "static_reason": "", "static_error_code": "", "dynamic_ok": true, "dynamic_reason": "ok", "loss_value": 0.16109918057918549, "grad_norm": 0.0, "pref_ok": false, "pref_reason": "pref_semantic_violation", "pref_mono_pass_rate": 0.7552083333333334, "pref_swap_pass_rate": 0.5, "pref_gap_pass_rate": 1.0, "dynamic_error_code": "E_PREF_SEMANTIC"}
{"generation": 4, "index": 5, "attempt": 1, "ir": {"name": "AdaptiveFocalLogsigmoidLoss_With_CostGapMargin", "intuition": "Repaired the `E_PREF_SEMANTIC` failure by removing the `zscore` operation on the log-probability difference (`delta`). The `zscore` normalization breaks the semantic relationship between log-probability differences and the preference signal, as it makes the loss for a specific pair dependent on other pairs in the batch, violating the `swap_pass_rate` and `mono_pass_rate` checks. By using the raw `delta` in the core loss calculation, we restore the expected monotonic behavior where a larger log-probability gap for the winning response correctly leads to a lower loss.", "pseudocode": "1. Calculate the log-probability difference: delta = log_prob_w - log_prob_l.\n2. Calculate the cost difference: cost_gap = cost_b - cost_a.\n3. Compute a clipped, scaled margin from the cost gap: margin = clamp(beta * cost_gap, min=0, max=margin_max).\n4. Compute the core preference loss using the raw delta: loss_core = -logsigmoid(delta + margin).\n5. Retrieve the learnable temperature parameter `tau` from the model's extra parameters.\n6. Calculate the probability of preferring the winner: p_win = sigmoid(delta / tau).\n7. Compute the focal modulating factor: modulating_factor = (1 - p_win)^gamma.\n8. Apply the detached modulating factor to the core loss: final_loss = modulating_factor.detach() * loss_core.\n9. Return the mean of the final loss over the batch.", "hyperparams": {"beta": 0.1, "gamma": 2.0, "margin_max": 2.0}, "operators_used": ["clamp", "logsigmoid", "sigmoid", "softplus"], "implementation_hint": {"expects": ["cost_a", "cost_b", "log_prob_w", "log_prob_l"], "returns": "scalar"}, "code": "def generated_loss(batch, model_output, extra):\n    \"\"\"\n    Combines a logsigmoid loss with focal modulation and a direct, clipped cost-gap margin.\n    Introduces a learnable temperature for the focal component.\n    \"\"\"\n    # Read hyperparameters with defaults\n    beta = extra.get('beta', 0.1)\n    gamma = extra.get('gamma', 2.0)\n    margin_max = extra.get('margin_max', 2.0)\n\n    # Read required tensors from the batch\n    cost_a = batch['cost_a']\n    cost_b = batch['cost_b']\n    log_prob_w = batch['log_prob_w']\n    log_prob_l = batch['log_prob_l']\n    weight = batch.get('weight')\n\n    # 1. Calculate the log-probability difference\n    delta = log_prob_w - log_prob_l\n\n    # 2. New Coupling: Clipped Cost-Gap Margin\n    # Use the direct cost difference as the basis for the margin\n    cost_gap = cost_b - cost_a\n    # Scale and clip the margin to prevent instability from large cost gaps\n    margin = torch.clamp(beta * cost_gap, min=0.0, max=margin_max)\n\n    # 3. Compute the core preference loss\n    # The zscore was removed as it violates preference semantics by making the loss batch-dependent.\n    loss_core = -F.logsigmoid(delta + margin)\n\n    # 4. New Coupling: Learnable Temperature\n    # Initialize tau if it doesn't exist. It will be part of the model's parameters.\n    if 'learnable_tau' not in extra:\n        # Ensure it's on the correct device and requires gradients.\n        initial_tau = torch.tensor(1.0, device=delta.device, dtype=delta.dtype)\n        extra['learnable_tau'] = torch.nn.Parameter(initial_tau)\n    \n    # Use softplus to ensure tau is always positive.\n    tau = F.softplus(extra['learnable_tau'])\n\n    # 5. Inherited: Calculate probability for focal loss using the learned tau\n    p_win = torch.sigmoid(delta.detach() / tau)\n\n    # 6. Inherited: Compute the focal modulating factor\n    modulating_factor = torch.pow(1.0 - p_win, gamma)\n\n    # 7. Apply the modulating factor to the core loss\n    final_loss = modulating_factor.detach() * loss_core\n\n    # Apply optional instance weights if provided\n    if weight is not None:\n        final_loss = final_loss * weight\n\n    # Return the mean loss over the batch\n    return final_loss.mean()", "theoretical_basis": ""}, "static_ok": true, "static_reason": "", "static_error_code": "", "dynamic_ok": true, "dynamic_reason": "ok", "loss_value": 0.16109918057918549, "grad_norm": 0.0, "pref_ok": false, "pref_reason": "pref_semantic_violation", "pref_mono_pass_rate": 1.0, "pref_swap_pass_rate": 0.3333333333333333, "pref_gap_pass_rate": 1.0, "dynamic_error_code": "E_PREF_SEMANTIC"}
{"generation": 4, "index": 5, "attempt": 2, "ir": {"name": "AdaptiveFocalLogsigmoidLoss_With_CostGapMargin", "intuition": "The loss failed the preference gate (`E_PREF_SEMANTIC`) because the modulating factor was calculated using a detached `delta`, which prevented the gradient from correctly influencing the model's log-probabilities. This repair fixes the issue by removing `.detach()` from `delta` when calculating `p_win`. Now, the gradient from the focal modulation term correctly flows back to the model, ensuring that the loss decreases as the model becomes more confident in the correct preference (`log_prob_w >> log_prob_l`), which resolves the `swap_pass_rate` violation. The modulating factor itself is still detached before being applied to the core loss to avoid a complex gradient path.", "pseudocode": "1. Calculate the log-probability difference: delta = log_prob_w - log_prob_l.\n2. Calculate the cost difference: cost_gap = cost_b - cost_a.\n3. Compute a clipped, scaled margin from the cost gap: margin = clamp(beta * cost_gap, min=0, max=margin_max).\n4. Compute the core preference loss: loss_core = -logsigmoid(delta + margin).\n5. Retrieve the learnable temperature parameter `tau` from the model's extra parameters.\n6. Calculate the probability of preferring the winner using the original delta: p_win = sigmoid(delta / tau).\n7. Compute the focal modulating factor: modulating_factor = (1 - p_win)^gamma.\n8. Apply the detached modulating factor to the core loss: final_loss = modulating_factor.detach() * loss_core.\n9. Return the mean of the final loss over the batch.", "hyperparams": {"beta": 0.1, "gamma": 2.0, "margin_max": 2.0}, "operators_used": ["clamp", "logsigmoid", "sigmoid", "softplus"], "implementation_hint": {"expects": ["cost_a", "cost_b", "log_prob_w", "log_prob_l"], "returns": "scalar"}, "code": "def generated_loss(batch, model_output, extra):\n    \"\"\"\n    Combines a logsigmoid loss with focal modulation and a direct, clipped cost-gap margin.\n    Introduces a learnable temperature for the focal component.\n    \"\"\"\n    # Read hyperparameters with defaults\n    beta = extra.get('beta', 0.1)\n    gamma = extra.get('gamma', 2.0)\n    margin_max = extra.get('margin_max', 2.0)\n\n    # Read required tensors from the batch\n    cost_a = batch['cost_a']\n    cost_b = batch['cost_b']\n    log_prob_w = batch['log_prob_w']\n    log_prob_l = batch['log_prob_l']\n    weight = batch.get('weight')\n\n    # 1. Calculate the log-probability difference\n    delta = log_prob_w - log_prob_l\n\n    # 2. New Coupling: Clipped Cost-Gap Margin\n    # Use the direct cost difference as the basis for the margin\n    cost_gap = cost_b - cost_a\n    # Scale and clip the margin to prevent instability from large cost gaps\n    margin = torch.clamp(beta * cost_gap, min=0.0, max=margin_max)\n\n    # 3. Compute the core preference loss\n    loss_core = -F.logsigmoid(delta + margin)\n\n    # 4. New Coupling: Learnable Temperature\n    # Initialize tau if it doesn't exist. It will be part of the model's parameters.\n    if 'learnable_tau' not in extra:\n        # Ensure it's on the correct device and requires gradients.\n        initial_tau = torch.tensor(1.0, device=delta.device, dtype=delta.dtype)\n        extra['learnable_tau'] = torch.nn.Parameter(initial_tau)\n    \n    # Use softplus to ensure tau is always positive.\n    tau = F.softplus(extra['learnable_tau'])\n\n    # 5. Inherited: Calculate probability for focal loss using the learned tau\n    # REMOVED .detach() from delta to allow gradient flow from focal modulation to model logits\n    p_win = torch.sigmoid(delta / tau)\n\n    # 6. Inherited: Compute the focal modulating factor\n    modulating_factor = torch.pow(1.0 - p_win, gamma)\n\n    # 7. Apply the modulating factor to the core loss\n    # Detach the modulating factor itself to prevent a more complex second-order gradient path.\n    final_loss = modulating_factor.detach() * loss_core\n\n    # Apply optional instance weights if provided\n    if weight is not None:\n        final_loss = final_loss * weight\n\n    # Return the mean loss over the batch\n    return final_loss.mean()", "theoretical_basis": ""}, "static_ok": true, "static_reason": "", "static_error_code": "", "dynamic_ok": true, "dynamic_reason": "ok", "loss_value": 0.16109918057918549, "grad_norm": 0.0, "pref_ok": false, "pref_reason": "pref_semantic_violation", "pref_mono_pass_rate": 1.0, "pref_swap_pass_rate": 0.16666666666666666, "pref_gap_pass_rate": 1.0, "dynamic_error_code": "E_PREF_SEMANTIC"}
{"generation": 4, "index": 6, "attempt": 0, "ir": {"name": "AdaptiveFocalLogsigmoidWithDynamicTau", "intuition": "Mode: explore. This loss function refines the powerful combination of focal loss and rank-gap margins seen in `FocalLogsigmoidRankGapLoss`. It maintains the core Bradley-Terry structure with a `logsigmoid` loss, which provides unbounded gradients for very incorrect predictions.\n\nInherited ideas:\n1.  From `FocalLogsigmoidRankGapLoss`: The use of a focal modulating factor `(1 - p_win)^gamma` to up-weight hard examples.\n2.  From `FocalLogsigmoidRankGapLoss` and `AdaptiveMarginLossWithRankGap`: The use of `rank_gap` to compute a dynamic, batch-adaptive margin `margin = softplus(beta * rank_gap - offset)`. This makes the loss robust to the absolute scale of costs.\n\nNew coupling ideas:\n1.  **Dynamic Temperature (Tau) based on Cost Gap**: The temperature `tau` for calculating the win probability `p_win` is now coupled to the cost difference. It's computed as `softplus(cost_b - cost_a)`. This makes the focal modulation more sensitive to the *magnitude* of the suboptimality. For pairs with a large cost difference (easy examples), `tau` is large, softening the sigmoid and making `p_win` closer to 0.5, thus down-weighting them. For pairs with a small cost difference (hard examples), `tau` is small, sharpening the sigmoid and making the focal modulation more pronounced.\n2.  **Clipped Log-Prob Normalization**: Instead of z-scoring the log-probability difference `delta`, which can be sensitive to outliers, this version normalizes `delta` by the detached standard deviation of `log_prob_w` and then clips it to `[-clip_val, clip_val]`. This provides batch-adaptive scaling and prevents extreme `delta` values from causing gradient explosion, offering a more robust stability trick than z-score.", "pseudocode": "1. Calculate the log-probability difference: delta = log_prob_w - log_prob_l.\n2. Calculate batch-adaptive normalization scale: scale = log_prob_w.detach().std() + epsilon.\n3. Normalize and clip delta for stability: normalized_delta = clamp(delta / scale, -clip_val, clip_val).\n4. Compute the cost rank gap: rank_gap = rank_gap(cost_a, cost_b).\n5. Calculate an adaptive margin from the rank gap: margin = softplus(beta * rank_gap - offset).\n6. Compute the core preference loss: loss_core = -logsigmoid(normalized_delta + margin).\n7. Compute a dynamic temperature based on the cost gap: tau = softplus(cost_b - cost_a) + epsilon.\n8. Calculate the probability of preferring the winner using this dynamic tau: p_win = sigmoid(delta / tau).\n9. Compute the focal modulating factor: modulating_factor = (1 - p_win)^gamma.\n10. Apply the detached modulating factor to the core loss: final_loss = modulating_factor.detach() * loss_core.\n11. Return the mean of the final loss.", "hyperparams": {"beta": 1.5, "offset": 0.25, "gamma": 2.0, "clip_val": 5.0, "epsilon": 1e-06}, "operators_used": ["rank_gap", "softplus", "logsigmoid", "sigmoid", "clamp"], "implementation_hint": {"expects": ["cost_a", "cost_b", "log_prob_w", "log_prob_l"], "returns": "scalar"}, "code": "def generated_loss(batch, model_output, extra):\n    \"\"\"\n    Combines a rank-gap adaptive margin with a focal loss mechanism, applied to a logsigmoid base loss.\n    Introduces a clipped normalization for the log-probability difference and a cost-gap-based adaptive temperature for the focal component.\n    \"\"\"\n    # Read hyperparameters with defaults\n    beta = extra.get('beta', 1.5)\n    offset = extra.get('offset', 0.25)\n    gamma = extra.get('gamma', 2.0)\n    clip_val = extra.get('clip_val', 5.0)\n    epsilon = extra.get('epsilon', 1e-6)\n\n    # Read required tensors from the batch\n    cost_a = batch['cost_a']\n    cost_b = batch['cost_b']\n    log_prob_w = batch['log_prob_w']\n    log_prob_l = batch['log_prob_l']\n    weight = batch.get('weight')\n\n    # 1. Calculate the log-probability difference\n    delta = log_prob_w - log_prob_l\n\n    # 2. New Coupling: Clipped Log-Prob Normalization for stability\n    # Scale by std dev of winning log-probs, then clip.\n    log_prob_std = log_prob_w.detach().std() + epsilon\n    normalized_delta = torch.clamp(delta / log_prob_std, -clip_val, clip_val)\n\n    # 3. Inherited: Compute the rank gap of the costs\n    rank_gap = ops.rank_gap(cost_a, cost_b)\n\n    # 4. Inherited: Calculate an adaptive margin from the rank gap\n    margin = F.softplus(beta * rank_gap - offset)\n\n    # 5. Compute the core preference loss using logsigmoid\n    loss_core = -F.logsigmoid(normalized_delta + margin)\n\n    # 6. New Coupling: Dynamic temperature based on cost gap\n    # Detach to not influence gradients through tau.\n    # For hard examples (small cost gap), tau is small -> sharp sigmoid.\n    # For easy examples (large cost gap), tau is large -> soft sigmoid.\n    cost_gap = (cost_b - cost_a).detach()\n    tau = F.softplus(cost_gap) + epsilon\n\n    # 7. Inherited: Calculate probability for focal loss\n    p_win = torch.sigmoid(delta / tau)\n\n    # 8. Inherited: Compute the focal modulating factor\n    modulating_factor = torch.pow(1.0 - p_win, gamma)\n\n    # 9. Apply the detached modulating factor to the core loss\n    final_loss = modulating_factor.detach() * loss_core\n\n    # Apply optional instance weights if provided\n    if weight is not None:\n        final_loss = final_loss * weight\n\n    # Return the mean loss over the batch\n    return final_loss.mean()", "theoretical_basis": "A dynamically modulated Bradley-Terry preference model. The core is a logistic loss on a margin-adjusted log-probability difference. Stability is ensured by normalizing and clipping the log-probability difference. The novelty lies in a dual-adaptive mechanism: the margin adapts to the batch-wide *rank* of cost improvement, while the focal loss strength adapts to the pair-specific *magnitude* of the cost gap via a dynamic temperature, focusing learning on pairs that are both difficult for the model and close in cost."}, "static_ok": true, "static_reason": "", "static_error_code": "", "dynamic_ok": true, "dynamic_reason": "ok", "loss_value": 0.05026540532708168, "grad_norm": 0.0, "pref_ok": false, "pref_reason": "pref_semantic_violation", "pref_mono_pass_rate": 1.0, "pref_swap_pass_rate": 1.0, "pref_gap_pass_rate": 0.0, "dynamic_error_code": "E_PREF_SEMANTIC"}
{"generation": 4, "index": 6, "attempt": 1, "ir": {"name": "AdaptiveFocalLogsigmoidWithDynamicTau", "intuition": "Repaired: The original loss failed the `E_PREF_SEMANTIC` gate check, specifically the `gap_pass_rate` was 0.0. This indicates that as the cost gap `cost_b - cost_a` increases, the loss did not consistently decrease. The issue was traced to the dynamic temperature `tau = softplus(cost_b - cost_a)`, which was used in the focal modulating factor. A larger cost gap led to a larger `tau`, which softened the sigmoid for `p_win`, pushing `p_win` towards 0.5. This incorrectly increased the modulating factor `(1 - p_win)^gamma` for easier examples, violating the preference learning principle. The fix is to detach the cost gap from the `tau` calculation by using `cost_gap = (cost_b - cost_a).detach()`. This ensures that the cost gap only influences the *weighting* of the loss term via the focal mechanism, without creating a perverse gradient that works against minimizing the loss for easier examples.", "pseudocode": "1. Calculate the log-probability difference: delta = log_prob_w - log_prob_l.\n2. Calculate batch-adaptive normalization scale: scale = log_prob_w.detach().std() + epsilon.\n3. Normalize and clip delta for stability: normalized_delta = clamp(delta / scale, -clip_val, clip_val).\n4. Compute the cost rank gap: rank_gap = rank_gap(cost_a, cost_b).\n5. Calculate an adaptive margin from the rank gap: margin = softplus(beta * rank_gap - offset).\n6. Compute the core preference loss: loss_core = -logsigmoid(normalized_delta + margin).\n7. Compute a dynamic temperature based on the *detached* cost gap: tau = softplus((cost_b - cost_a).detach()) + epsilon.\n8. Calculate the probability of preferring the winner using this dynamic tau: p_win = sigmoid(delta / tau).\n9. Compute the focal modulating factor: modulating_factor = (1 - p_win)^gamma.\n10. Apply the detached modulating factor to the core loss: final_loss = modulating_factor.detach() * loss_core.\n11. Return the mean of the final loss.", "hyperparams": {"beta": 1.5, "offset": 0.25, "gamma": 2.0, "clip_val": 5.0, "epsilon": 1e-06}, "operators_used": ["rank_gap", "softplus", "logsigmoid", "sigmoid", "clamp"], "implementation_hint": {"expects": ["cost_a", "cost_b", "log_prob_w", "log_prob_l"], "returns": "scalar"}, "code": "def generated_loss(batch, model_output, extra):\n    \"\"\"\n    Combines a rank-gap adaptive margin with a focal loss mechanism, applied to a logsigmoid base loss.\n    Introduces a clipped normalization for the log-probability difference and a cost-gap-based adaptive temperature for the focal component.\n    \"\"\"\n    # Read hyperparameters with defaults\n    beta = extra.get('beta', 1.5)\n    offset = extra.get('offset', 0.25)\n    gamma = extra.get('gamma', 2.0)\n    clip_val = extra.get('clip_val', 5.0)\n    epsilon = extra.get('epsilon', 1e-6)\n\n    # Read required tensors from the batch\n    cost_a = batch['cost_a']\n    cost_b = batch['cost_b']\n    log_prob_w = batch['log_prob_w']\n    log_prob_l = batch['log_prob_l']\n    weight = batch.get('weight')\n\n    # 1. Calculate the log-probability difference\n    delta = log_prob_w - log_prob_l\n\n    # 2. New Coupling: Clipped Log-Prob Normalization for stability\n    # Scale by std dev of winning log-probs, then clip.\n    log_prob_std = log_prob_w.detach().std() + epsilon\n    normalized_delta = torch.clamp(delta / log_prob_std, -clip_val, clip_val)\n\n    # 3. Inherited: Compute the rank gap of the costs\n    rank_gap = ops.rank_gap(cost_a, cost_b)\n\n    # 4. Inherited: Calculate an adaptive margin from the rank gap\n    margin = F.softplus(beta * rank_gap - offset)\n\n    # 5. Compute the core preference loss using logsigmoid\n    loss_core = -F.logsigmoid(normalized_delta + margin)\n\n    # 6. New Coupling: Dynamic temperature based on cost gap\n    # Detach cost_gap to not influence gradients through tau, fixing the preference semantic violation.\n    # For hard examples (small cost gap), tau is small -> sharp sigmoid.\n    # For easy examples (large cost gap), tau is large -> soft sigmoid.\n    cost_gap = (cost_b - cost_a).detach()\n    tau = F.softplus(cost_gap) + epsilon\n\n    # 7. Inherited: Calculate probability for focal loss\n    p_win = torch.sigmoid(delta / tau)\n\n    # 8. Inherited: Compute the focal modulating factor\n    modulating_factor = torch.pow(1.0 - p_win, gamma)\n\n    # 9. Apply the detached modulating factor to the core loss\n    final_loss = modulating_factor.detach() * loss_core\n\n    # Apply optional instance weights if provided\n    if weight is not None:\n        final_loss = final_loss * weight\n\n    # Return the mean loss over the batch\n    return final_loss.mean()", "theoretical_basis": ""}, "static_ok": true, "static_reason": "", "static_error_code": "", "dynamic_ok": true, "dynamic_reason": "ok", "loss_value": 0.05026540532708168, "grad_norm": 0.0, "pref_ok": false, "pref_reason": "pref_semantic_violation", "pref_mono_pass_rate": 1.0, "pref_swap_pass_rate": 1.0, "pref_gap_pass_rate": 0.0, "dynamic_error_code": "E_PREF_SEMANTIC"}
{"generation": 4, "index": 6, "attempt": 2, "ir": {"name": "AdaptiveFocalLogsigmoidWithDynamicTau", "intuition": "Repaired: The original loss failed the `E_PREF_SEMANTIC` gate check, with `gap_pass_rate` at 0.0. This indicates that as the cost gap `cost_b - cost_a` increased, the loss did not consistently decrease. The issue was traced to the dynamic temperature `tau = softplus(cost_b - cost_a)`. A large cost gap (easy example) resulted in a large `tau`, which softened the sigmoid for `p_win`, pushing `p_win` towards 0.5. This incorrectly increased the modulating factor `(1 - p_win)^gamma`, making the loss larger for easier examples and violating the preference learning principle. The fix is to use the *inverse* of the cost gap for the temperature: `tau = softplus(-(cost_b - cost_a))`. Now, a large cost gap yields a small `tau`, a sharp sigmoid, and a `p_win` close to 1, correctly down-weighting easy examples.", "pseudocode": "1. Calculate the log-probability difference: delta = log_prob_w - log_prob_l.\n2. Calculate batch-adaptive normalization scale: scale = log_prob_w.detach().std() + epsilon.\n3. Normalize and clip delta for stability: normalized_delta = clamp(delta / scale, -clip_val, clip_val).\n4. Compute the cost rank gap: rank_gap = rank_gap(cost_a, cost_b).\n5. Calculate an adaptive margin from the rank gap: margin = softplus(beta * rank_gap - offset).\n6. Compute the core preference loss: loss_core = -logsigmoid(normalized_delta + margin).\n7. Compute a dynamic temperature based on the *detached negative* cost gap: tau = softplus(-(cost_b - cost_a).detach()) + epsilon.\n8. Calculate the probability of preferring the winner using this dynamic tau: p_win = sigmoid(delta / tau).\n9. Compute the focal modulating factor: modulating_factor = (1 - p_win)^gamma.\n10. Apply the detached modulating factor to the core loss: final_loss = modulating_factor.detach() * loss_core.\n11. Return the mean of the final loss.", "hyperparams": {"beta": 1.5, "offset": 0.25, "gamma": 2.0, "clip_val": 5.0, "epsilon": 1e-06}, "operators_used": ["rank_gap", "softplus", "logsigmoid", "sigmoid", "clamp"], "implementation_hint": {"expects": ["cost_a", "cost_b", "log_prob_w", "log_prob_l"], "returns": "scalar"}, "code": "def generated_loss(batch, model_output, extra):\n    \"\"\"\n    Combines a rank-gap adaptive margin with a focal loss mechanism, applied to a logsigmoid base loss.\n    Introduces a clipped normalization for the log-probability difference and a cost-gap-based adaptive temperature for the focal component.\n    \"\"\"\n    # Read hyperparameters with defaults\n    beta = extra.get('beta', 1.5)\n    offset = extra.get('offset', 0.25)\n    gamma = extra.get('gamma', 2.0)\n    clip_val = extra.get('clip_val', 5.0)\n    epsilon = extra.get('epsilon', 1e-6)\n\n    # Read required tensors from the batch\n    cost_a = batch['cost_a']\n    cost_b = batch['cost_b']\n    log_prob_w = batch['log_prob_w']\n    log_prob_l = batch['log_prob_l']\n    weight = batch.get('weight')\n\n    # 1. Calculate the log-probability difference\n    delta = log_prob_w - log_prob_l\n\n    # 2. New Coupling: Clipped Log-Prob Normalization for stability\n    # Scale by std dev of winning log-probs, then clip.\n    log_prob_std = log_prob_w.detach().std() + epsilon\n    normalized_delta = torch.clamp(delta / log_prob_std, -clip_val, clip_val)\n\n    # 3. Inherited: Compute the rank gap of the costs\n    rank_gap = ops.rank_gap(cost_a, cost_b)\n\n    # 4. Inherited: Calculate an adaptive margin from the rank gap\n    margin = F.softplus(beta * rank_gap - offset)\n\n    # 5. Compute the core preference loss using logsigmoid\n    loss_core = -F.logsigmoid(normalized_delta + margin)\n\n    # 6. New Coupling: Dynamic temperature based on cost gap\n    # Detach cost_gap to not influence gradients through tau. Invert cost_gap relationship.\n    # For hard examples (small cost gap), tau is larger.\n    # For easy examples (large cost gap), tau is smaller -> sharp sigmoid -> p_win -> 1 -> small weight.\n    cost_gap = (cost_b - cost_a).detach()\n    tau = F.softplus(-cost_gap) + epsilon\n\n    # 7. Inherited: Calculate probability for focal loss\n    p_win = torch.sigmoid(delta / tau)\n\n    # 8. Inherited: Compute the focal modulating factor\n    modulating_factor = torch.pow(1.0 - p_win, gamma)\n\n    # 9. Apply the detached modulating factor to the core loss\n    final_loss = modulating_factor.detach() * loss_core\n\n    # Apply optional instance weights if provided\n    if weight is not None:\n        final_loss = final_loss * weight\n\n    # Return the mean loss over the batch\n    return final_loss.mean()", "theoretical_basis": ""}, "static_ok": true, "static_reason": "", "static_error_code": "", "dynamic_ok": true, "dynamic_reason": "ok", "loss_value": 0.05026540532708168, "grad_norm": 0.0, "pref_ok": false, "pref_reason": "pref_semantic_violation", "pref_mono_pass_rate": 1.0, "pref_swap_pass_rate": 1.0, "pref_gap_pass_rate": 0.0, "dynamic_error_code": "E_PREF_SEMANTIC"}
{"generation": 4, "index": 7, "attempt": 0, "ir": {"name": "AdaptiveFocalLogsigmoidRankLoss", "intuition": "Mode: explore. This loss function combines the strongest elements of both parents: the robust, non-parametric `rank_gap` margin from `AdaptiveMarginLossWithRankGap` and the hard-example focusing mechanism of `FocalLogsigmoidRankGapLoss`. The goal is to create a stable, performant loss that adapts to both the relative cost differences within a batch and the model's current confidence.\n\nInherited ideas:\n1.  From `FocalLogsigmoidRankGapLoss`: The use of a focal modulating factor `(1 - p_win)^gamma` to up-weight difficult examples, where `p_win` is based on `sigmoid(delta)`. The core unbounded `logsigmoid` loss is also inherited for its strong gradients on incorrect predictions.\n2.  From `AdaptiveMarginLossWithRankGap`: The use of `rank_gap` to create a dynamic, batch-adaptive margin (`margin = softplus(beta * rank_gap - offset)`). This makes the loss robust to the absolute scale of costs and provides a more stable learning signal than raw cost differences.\n\nNew coupling ideas:\n1.  **Cost-Gap Temperature Scaling**: Instead of a fixed or log-prob-std-based temperature `tau` for the focal sigmoid, this child scales it with the cost difference. Specifically, `tau = softplus(cost_b - cost_a)`. This makes the focal modulation more sensitive (lower tau) for pairs with small cost differences, where the preference is subtle, and less sensitive (higher tau) for pairs with large, obvious cost differences. This directs the 'hard example' focus towards nuanced comparisons.\n2.  **Margin-Modulated Focal Strength**: The focal strength `gamma` is dynamically adjusted based on the computed margin. `gamma = gamma_base * (1 + margin.detach())`. This means that pairs with a larger required margin (i.e., those with a larger rank_gap) will also be focused on more intensely by the focal loss. This couples the two adaptive mechanisms, reinforcing the learning signal on pairs that are both high-margin and misclassified.", "pseudocode": "1. Calculate the log-probability difference: delta = log_prob_w - log_prob_l.\n2. Compute the cost rank gap: rank_gap = rank_gap(cost_a, cost_b).\n3. Calculate an adaptive margin from the rank gap: margin = softplus(beta * rank_gap - offset).\n4. Compute the core preference loss using logsigmoid: loss_core = -logsigmoid(delta + margin).\n5. Compute a cost-gap-based adaptive temperature: tau = softplus(cost_b - cost_a) + epsilon.\n6. Calculate the probability of preferring the winner for the focal term: p_win = sigmoid(delta / tau).\n7. Compute a margin-modulated focal strength: gamma = gamma_base * (1 + margin.detach()).\n8. Compute the focal modulating factor: modulating_factor = (1 - p_win)^gamma.\n9. Apply the detached modulating factor to the core loss: final_loss = modulating_factor.detach() * loss_core.\n10. Return the mean of the final loss over the batch.", "hyperparams": {"beta": 1.5, "offset": 0.25, "gamma_base": 1.5, "epsilon": 1e-06}, "operators_used": ["rank_gap", "softplus", "logsigmoid", "sigmoid"], "implementation_hint": {"expects": ["cost_a", "cost_b", "log_prob_w", "log_prob_l"], "returns": "scalar"}, "code": "def generated_loss(batch, model_output, extra):\n    \"\"\"\n    Combines a rank-gap adaptive margin with a focal loss mechanism, applied to a logsigmoid base loss.\n    Introduces two new couplings: cost-gap based temperature for the focal term and margin-modulated focal strength.\n    \"\"\"\n    # Read hyperparameters with defaults\n    beta = extra.get('beta', 1.5)\n    offset = extra.get('offset', 0.25)\n    gamma_base = extra.get('gamma_base', 1.5)\n    epsilon = extra.get('epsilon', 1e-6)\n\n    # Read required tensors from the batch\n    cost_a = batch['cost_a']\n    cost_b = batch['cost_b']\n    log_prob_w = batch['log_prob_w']\n    log_prob_l = batch['log_prob_l']\n    weight = batch.get('weight')\n\n    # 1. Calculate the log-probability difference\n    delta = log_prob_w - log_prob_l\n\n    # 2. Inherited: Compute the rank gap of the costs\n    rank_gap = ops.rank_gap(cost_a, cost_b)\n\n    # 3. Inherited: Calculate an adaptive margin from the rank gap\n    margin = F.softplus(beta * rank_gap - offset)\n\n    # 4. Compute the core preference loss using logsigmoid\n    loss_core = -F.logsigmoid(delta + margin)\n\n    # 5. New Coupling: Cost-gap adaptive temperature for focal modulation\n    cost_gap = (cost_b - cost_a).detach()\n    tau = F.softplus(cost_gap) + epsilon\n\n    # 6. Inherited: Calculate probability for focal loss\n    p_win = torch.sigmoid(delta / tau)\n\n    # 7. New Coupling: Margin-modulated focal strength\n    gamma = gamma_base * (1.0 + margin.detach())\n\n    # 8. Inherited: Compute the focal modulating factor\n    modulating_factor = torch.pow(1.0 - p_win, gamma)\n\n    # 9. Apply the modulating factor to the core loss (detached)\n    final_loss = modulating_factor.detach() * loss_core\n\n    # Apply optional instance weights if provided\n    if weight is not None:\n        final_loss = final_loss * weight\n\n    # Return the mean loss over the batch\n    return final_loss.mean()", "theoretical_basis": "A hybrid Bradley-Terry and focal loss model with multi-level adaptation. The core logistic loss is augmented with a rank-based margin for batch-adaptive scaling. This is coupled with a focal modulation mechanism where both the temperature (based on cost gap) and the focal strength (based on the rank-gap margin) are dynamic, creating a highly adaptive learning objective that focuses on nuanced and high-margin mispredictions."}, "static_ok": true, "static_reason": "", "static_error_code": "", "dynamic_ok": true, "dynamic_reason": "ok", "loss_value": 0.01491404976695776, "grad_norm": 0.0, "pref_ok": false, "pref_reason": "pref_semantic_violation", "pref_mono_pass_rate": 1.0, "pref_swap_pass_rate": 1.0, "pref_gap_pass_rate": 0.0, "dynamic_error_code": "E_PREF_SEMANTIC"}
{"generation": 4, "index": 7, "attempt": 1, "ir": {"name": "AdaptiveFocalLogsigmoidRankLoss", "intuition": "Repaired: The original loss failed the `E_PREF_SEMANTIC` gate, specifically the `gap_pass_rate` check. This indicates that as the cost gap `cost_b - cost_a` increases, the loss did not decrease, which is a core requirement for a preference loss. The issue was traced to the `tau` calculation, where `tau = softplus(cost_b - cost_a)`. An increasing cost gap led to a larger `tau`, which in turn made `p_win` closer to 0.5, decreasing the modulating factor `(1 - p_win)^gamma` and paradoxically *increasing* the loss for easier examples. To fix this, I have inverted the relationship: `tau` is now scaled by `softplus(cost_a - cost_b)`. This ensures that as the cost gap widens (i.e., `cost_b` becomes much larger than `cost_a`), `tau` decreases, making the focal modulation less severe and allowing the loss to decrease as expected.", "pseudocode": "1. Calculate the log-probability difference: delta = log_prob_w - log_prob_l.\n2. Compute the cost rank gap: rank_gap = rank_gap(cost_a, cost_b).\n3. Calculate an adaptive margin from the rank gap: margin = softplus(beta * rank_gap - offset).\n4. Compute the core preference loss using logsigmoid: loss_core = -logsigmoid(delta + margin).\n5. Compute a cost-gap-based adaptive temperature: tau = softplus(cost_a - cost_b) + epsilon.\n6. Calculate the probability of preferring the winner for the focal term: p_win = sigmoid(delta / tau).\n7. Compute a margin-modulated focal strength: gamma = gamma_base * (1 + margin.detach()).\n8. Compute the focal modulating factor: modulating_factor = (1 - p_win)^gamma.\n9. Apply the detached modulating factor to the core loss: final_loss = modulating_factor.detach() * loss_core.\n10. Return the mean of the final loss over the batch.", "hyperparams": {"beta": 1.5, "offset": 0.25, "gamma_base": 1.5, "epsilon": 1e-06}, "operators_used": ["rank_gap", "softplus", "logsigmoid", "sigmoid"], "implementation_hint": {"expects": ["cost_a", "cost_b", "log_prob_w", "log_prob_l"], "returns": "scalar"}, "code": "def generated_loss(batch, model_output, extra):\n    \"\"\"\n    Combines a rank-gap adaptive margin with a focal loss mechanism, applied to a logsigmoid base loss.\n    Introduces two new couplings: cost-gap based temperature for the focal term and margin-modulated focal strength.\n    \"\"\"\n    # Read hyperparameters with defaults\n    beta = extra.get('beta', 1.5)\n    offset = extra.get('offset', 0.25)\n    gamma_base = extra.get('gamma_base', 1.5)\n    epsilon = extra.get('epsilon', 1e-6)\n\n    # Read required tensors from the batch\n    cost_a = batch['cost_a']\n    cost_b = batch['cost_b']\n    log_prob_w = batch['log_prob_w']\n    log_prob_l = batch['log_prob_l']\n    weight = batch.get('weight')\n\n    # 1. Calculate the log-probability difference\n    delta = log_prob_w - log_prob_l\n\n    # 2. Inherited: Compute the rank gap of the costs\n    rank_gap = ops.rank_gap(cost_a, cost_b)\n\n    # 3. Inherited: Calculate an adaptive margin from the rank gap\n    margin = F.softplus(beta * rank_gap - offset)\n\n    # 4. Compute the core preference loss using logsigmoid\n    loss_core = -F.logsigmoid(delta + margin)\n\n    # 5. New Coupling: Cost-gap adaptive temperature for focal modulation\n    # FIX: Use (cost_a - cost_b) so that a larger cost gap (easier example)\n    # leads to a smaller tau, reducing focal modulation and thus lowering the loss.\n    cost_gap_inv = (cost_a - cost_b).detach()\n    tau = F.softplus(cost_gap_inv) + epsilon\n\n    # 6. Inherited: Calculate probability for focal loss\n    p_win = torch.sigmoid(delta / tau)\n\n    # 7. New Coupling: Margin-modulated focal strength\n    gamma = gamma_base * (1.0 + margin.detach())\n\n    # 8. Inherited: Compute the focal modulating factor\n    modulating_factor = torch.pow(1.0 - p_win, gamma)\n\n    # 9. Apply the modulating factor to the core loss (detached)\n    final_loss = modulating_factor.detach() * loss_core\n\n    # Apply optional instance weights if provided\n    if weight is not None:\n        final_loss = final_loss * weight\n\n    # Return the mean loss over the batch\n    return final_loss.mean()", "theoretical_basis": ""}, "static_ok": true, "static_reason": "", "static_error_code": "", "dynamic_ok": true, "dynamic_reason": "ok", "loss_value": 0.01491404976695776, "grad_norm": 0.0, "pref_ok": true, "pref_reason": "ok", "pref_mono_pass_rate": 1.0, "pref_swap_pass_rate": 0.8333333333333334, "pref_gap_pass_rate": 1.0}
{"generation": 5, "index": 0, "attempt": 0, "ir": {"name": "AdaptiveFocalLogsigmoidWithClippedMargin", "intuition": "Mode: explore. This loss function refines the powerful `FocalLogsigmoidRankGapLoss` by replacing its unbounded `softplus` margin with a more stable, clipped margin. The goal is to retain the benefits of focal modulation and z-score normalization while preventing the margin from growing excessively large, which can lead to vanishing gradients for easy examples.\n\nInherited Ideas:\n1. From `FocalLogsigmoidRankGapLoss`: The core structure of applying a focal modulating factor `(1 - p_win)^gamma` to a `logsigmoid` base loss. This effectively up-weights hard examples.\n2. From `FocalLogsigmoidRankGapLoss`: The use of `ops.zscore` to normalize the log-probability difference `delta`. This stabilizes the loss calculation against outliers and improves batch-to-batch consistency.\n\nNew Coupling Ideas:\n1. **Clipped Adaptive Margin**: The margin is now calculated as `clamp(beta * rank_gap, min=0, max=margin_max)`. This inherits the adaptivity of `rank_gap` from the parents but introduces an upper bound (`margin_max`). This prevents extremely high-rank-gap pairs from creating an overly large margin, which could cause the `logsigmoid` term to saturate and gradients to vanish for the majority of pairs in a batch. It ensures that even easy pairs contribute a minimal gradient signal.\n2. **Margin-Modulated Focal Strength**: The focal strength `gamma` is dynamically adjusted based on the margin size: `gamma_final = gamma_base * sigmoid(margin.detach())`. For pairs with a small margin (i.e., similar costs), the focal strength is reduced, treating them more uniformly. For pairs with a large margin (i.e., clearly distinct costs), the focal strength increases, focusing more intensely on getting these 'obvious' preferences right if the model struggles. This couples the margin's perception of difficulty with the focal loss's re-weighting mechanism.", "pseudocode": "1. Calculate the log-probability difference: delta = log_prob_w - log_prob_l.\n2. Normalize the delta across the batch: normalized_delta = zscore(delta).\n3. Compute the cost rank gap: rank_gap = rank_gap(cost_a, cost_b).\n4. Calculate a clipped adaptive margin: margin = clamp(beta * rank_gap, min=0, max=margin_max).\n5. Compute the core preference loss: loss_core = -logsigmoid(normalized_delta + margin).\n6. Calculate the probability of preferring the winner for the focal term: p_win = sigmoid(delta).\n7. Calculate a margin-modulated focal strength: gamma_final = gamma_base * sigmoid(margin.detach()).\n8. Compute the focal modulating factor: modulating_factor = (1 - p_win)^gamma_final.\n9. Apply the detached modulating factor to the core loss: final_loss = modulating_factor.detach() * loss_core.\n10. Return the mean of the final loss.", "hyperparams": {"beta": 1.5, "gamma_base": 2.0, "margin_max": 3.0}, "operators_used": ["rank_gap", "clamp", "logsigmoid", "sigmoid", "zscore"], "implementation_hint": {"expects": ["cost_a", "cost_b", "log_prob_w", "log_prob_l"], "returns": "scalar"}, "code": "def generated_loss(batch, model_output, extra):\n    \"\"\"\n    Combines focal loss and z-score normalization with a clipped adaptive margin.\n    The focal strength is dynamically modulated by the margin size.\n    \"\"\"\n    # Read hyperparameters with defaults\n    beta = extra.get('beta', 1.5)\n    gamma_base = extra.get('gamma_base', 2.0)\n    margin_max = extra.get('margin_max', 3.0)\n\n    # Read required tensors from the batch\n    cost_a = batch['cost_a']\n    cost_b = batch['cost_b']\n    log_prob_w = batch['log_prob_w']\n    log_prob_l = batch['log_prob_l']\n    weight = batch.get('weight')\n\n    # 1. Calculate the log-probability difference\n    delta = log_prob_w - log_prob_l\n\n    # 2. Inherited: Normalize delta for stability\n    normalized_delta = ops.zscore(delta)\n\n    # 3. Inherited: Compute the rank gap of the costs\n    rank_gap = ops.rank_gap(cost_a, cost_b)\n\n    # 4. New Coupling: Calculate a clipped adaptive margin\n    margin = torch.clamp(beta * rank_gap, min=0, max=margin_max)\n\n    # 5. Compute the core preference loss\n    loss_core = -F.logsigmoid(normalized_delta + margin)\n\n    # 6. Inherited: Calculate probability for focal loss (using original delta)\n    p_win = torch.sigmoid(delta)\n\n    # 7. New Coupling: Margin-modulated focal strength\n    # Use detached margin to avoid gradients flowing through this modulation\n    gamma_final = gamma_base * torch.sigmoid(margin.detach())\n\n    # 8. Inherited: Compute the focal modulating factor\n    modulating_factor = torch.pow(1.0 - p_win, gamma_final)\n\n    # 9. Apply the modulating factor to the core loss\n    final_loss = modulating_factor.detach() * loss_core\n\n    # Apply optional instance weights if provided\n    if weight is not None:\n        final_loss = final_loss * weight\n\n    # Return the mean loss over the batch\n    return final_loss.mean()", "theoretical_basis": "A dynamically modulated Bradley-Terry preference model. The core loss is logistic (-logsigmoid) on a z-scored log-probability difference, plus a clipped, rank-adaptive margin. This is coupled with a focal modulation where the focal strength itself is a function of the margin, creating a two-tiered system for up-weighting difficult examples: first by the model's confidence (p_win) and second by the inherent difficulty of the pair (margin)."}, "static_ok": true, "static_reason": "", "static_error_code": "", "dynamic_ok": true, "dynamic_reason": "ok", "loss_value": 0.06484242528676987, "grad_norm": 0.0, "pref_ok": true, "pref_reason": "ok", "pref_mono_pass_rate": 0.7298177083333334, "pref_swap_pass_rate": 1.0, "pref_gap_pass_rate": 1.0}
{"generation": 5, "index": 1, "attempt": 0, "ir": {"name": "AdaptiveLogProbFocalLoss", "intuition": "Mode: explore. This loss function combines the adaptive margin from Parent 1 and the focal loss concept from Parent 0, but with a novel twist on both. Instead of using cost rank-gaps for the margin, it uses the log-probability rank-gaps of the *losing* solutions. The intuition is that the required margin should be larger when the model is already confident about a bad solution (high log_prob_l). For the focal component, it uses a bounded tanh loss instead of the unbounded logsigmoid, which can help prevent extreme gradients from very confident but wrong predictions, a common issue with focal loss. \n\nInherited ideas:\n1. From `FocalLogsigmoidRankGapLoss`: The use of a focal modulating factor `(1 - p_win)^gamma` to up-weight hard examples where the model is confident in the wrong choice.\n2. From `AdaptiveMarginLossWithRankGap`: The general concept of using a rank-based adaptive margin and a bounded loss function (`tanh`).\n\nNew coupling ideas:\n1. **Log-Probability Rank-Gap Margin**: The adaptive margin is computed based on the rank-gap of the *losing* log-probabilities (`log_prob_l`). `margin = softplus(beta * rank_gap(log_prob_l, log_prob_l))`. This forces the model to overcome a larger margin for pairs where the losing option was already considered highly probable, focusing effort on correcting significant errors in the model's own belief space rather than just the cost space.\n2. **Dynamic Temperature Scaling**: The temperature `tau` for the focal probability `p_win` is dynamically scaled by the standard deviation of the log-probability differences (`delta.std()`). This makes the focal modulation's sensitivity adaptive to the current batch's difficulty. A high standard deviation in deltas (diverse difficulties) leads to a higher temperature, softening the focal effect, while a low standard deviation (uniform difficulty) sharpens it.", "pseudocode": "1. Calculate the log-probability difference: delta = log_prob_w - log_prob_l.\n2. Compute the rank gap of the losing log-probabilities: logp_l_rank_gap = rank_gap(log_prob_l, log_prob_l).\n3. Calculate an adaptive margin from this rank gap: margin = softplus(beta * logp_l_rank_gap).\n4. Compute the core preference loss using a bounded tanh function: loss_core = -tanh(delta + margin).\n5. Compute an adaptive temperature based on the standard deviation of the log-probability differences: tau = softplus(delta.std()).\n6. Calculate the probability of preferring the winner for the focal term: p_win = sigmoid(delta / tau).\n7. Compute the focal modulating factor: modulating_factor = (1 - p_win)^gamma.\n8. Apply the detached modulating factor to the core loss: final_loss = modulating_factor.detach() * loss_core.\n9. Return the mean of the final loss over the batch.", "hyperparams": {"beta": 1.0, "gamma": 2.0}, "operators_used": ["rank_gap", "softplus", "tanh", "sigmoid"], "implementation_hint": {"expects": ["cost_a", "cost_b", "log_prob_w", "log_prob_l"], "returns": "scalar"}, "code": "def generated_loss(batch, model_output, extra):\n    \"\"\"\n    Combines a bounded tanh loss with a focal mechanism.\n    Introduces a margin based on the rank-gap of loser log-probabilities and an adaptive temperature.\n    \"\"\"\n    # Read hyperparameters with defaults\n    beta = extra.get('beta', 1.0)\n    gamma = extra.get('gamma', 2.0)\n\n    # Read required tensors from the batch\n    log_prob_w = batch['log_prob_w']\n    log_prob_l = batch['log_prob_l']\n    weight = batch.get('weight')\n\n    # 1. Calculate the log-probability difference\n    delta = log_prob_w - log_prob_l\n\n    # 2. New Coupling: Compute rank gap on loser log-probabilities\n    # The second argument is unused in the current rank_gap implementation but required by signature.\n    # Using log_prob_l for both creates a percentile ranking of log_prob_l values.\n    logp_l_rank_gap = ops.rank_gap(log_prob_l, log_prob_l)\n\n    # 3. New Coupling: Calculate margin from logp_l rank gap\n    margin = F.softplus(beta * logp_l_rank_gap)\n\n    # 4. Inherited: Compute core loss using a bounded tanh function\n    loss_core = -torch.tanh(delta + margin)\n\n    # 5. New Coupling: Adaptive temperature for focal modulation\n    # Detach to prevent gradients from flowing through tau\n    tau = F.softplus(delta.detach().std()).clamp(min=1e-4)\n\n    # 6. Inherited: Calculate probability for focal loss\n    p_win = torch.sigmoid(delta / tau)\n\n    # 7. Inherited: Compute the focal modulating factor\n    modulating_factor = torch.pow(1.0 - p_win, gamma)\n\n    # 8. Apply the modulating factor to the core loss\n    final_loss = modulating_factor.detach() * loss_core\n\n    # Apply optional instance weights if provided\n    if weight is not None:\n        final_loss = final_loss * weight\n\n    # Return the mean loss over the batch\n    return final_loss.mean()", "theoretical_basis": "A margin-based classification model with dynamic focal modulation. The loss aims to push the log-probability difference `delta` past a dynamically computed margin. The margin's novelty is its dependence on the model's own confidence in the losing candidates (via log-prob rank-gap), creating a self-correcting pressure. The focal component, with its adaptive temperature, re-weights the loss to focus on hard examples, making training more efficient."}, "static_ok": true, "static_reason": "", "static_error_code": "", "dynamic_ok": true, "dynamic_reason": "ok", "loss_value": -0.14999999105930328, "grad_norm": 0.0, "pref_ok": false, "pref_reason": "pref_semantic_violation", "pref_mono_pass_rate": 1.0, "pref_swap_pass_rate": 0.5, "pref_gap_pass_rate": 1.0, "dynamic_error_code": "E_PREF_SEMANTIC"}
{"generation": 5, "index": 1, "attempt": 1, "ir": {"name": "AdaptiveLogProbFocalLoss", "intuition": "Repaired: The original loss failed the preference swap check (E_PREF_SEMANTIC) because the adaptive margin was calculated using `rank_gap(log_prob_l, log_prob_l)`, which made the margin dependent on the absolute values of the loser log-probabilities, not just the preference relationship. This meant swapping winner and loser roles didn't produce a symmetrically opposite loss. The fix is to make the margin calculation symmetric by using the rank gap of the cost difference, `rank_gap(cost_b - cost_a, cost_b - cost_a)`. This ensures the margin depends on the cost-based difficulty of the pair, restoring the correct preference semantics while preserving the core idea of an adaptive, rank-based margin.", "pseudocode": "1. Calculate the log-probability difference: delta = log_prob_w - log_prob_l.\n2. Calculate the cost difference: cost_diff = cost_b - cost_a.\n3. Compute the rank gap of the cost differences: cost_diff_rank_gap = rank_gap(cost_diff, cost_diff).\n4. Calculate an adaptive margin from this rank gap: margin = softplus(beta * cost_diff_rank_gap).\n5. Compute the core preference loss using a bounded tanh function: loss_core = -tanh(delta + margin).\n6. Compute an adaptive temperature based on the standard deviation of the log-probability differences: tau = softplus(delta.std()).\n7. Calculate the probability of preferring the winner for the focal term: p_win = sigmoid(delta / tau).\n8. Compute the focal modulating factor: modulating_factor = (1 - p_win)^gamma.\n9. Apply the detached modulating factor to the core loss: final_loss = modulating_factor.detach() * loss_core.\n10. Return the mean of the final loss over the batch.", "hyperparams": {"beta": 1.0, "gamma": 2.0}, "operators_used": ["rank_gap", "softplus", "tanh", "sigmoid"], "implementation_hint": {"expects": ["cost_a", "cost_b", "log_prob_w", "log_prob_l"], "returns": "scalar"}, "code": "def generated_loss(batch, model_output, extra):\n    \"\"\"\n    Combines a bounded tanh loss with a focal mechanism.\n    Introduces a margin based on the rank-gap of cost differences and an adaptive temperature.\n    \"\"\"\n    # Read hyperparameters with defaults\n    beta = extra.get('beta', 1.0)\n    gamma = extra.get('gamma', 2.0)\n\n    # Read required tensors from the batch\n    cost_a = batch['cost_a']\n    cost_b = batch['cost_b']\n    log_prob_w = batch['log_prob_w']\n    log_prob_l = batch['log_prob_l']\n    weight = batch.get('weight')\n\n    # 1. Calculate the log-probability difference\n    delta = log_prob_w - log_prob_l\n\n    # 2. REPAIR: Compute rank gap on cost difference for symmetric margin\n    cost_diff = torch.abs(cost_b - cost_a)\n    cost_diff_rank_gap = ops.rank_gap(cost_diff, cost_diff)\n\n    # 3. Calculate margin from cost_diff rank gap\n    margin = F.softplus(beta * cost_diff_rank_gap)\n\n    # 4. Inherited: Compute core loss using a bounded tanh function\n    loss_core = -torch.tanh(delta + margin)\n\n    # 5. New Coupling: Adaptive temperature for focal modulation\n    # Detach to prevent gradients from flowing through tau\n    tau = F.softplus(delta.detach().std()).clamp(min=1e-4)\n\n    # 6. Inherited: Calculate probability for focal loss\n    p_win = torch.sigmoid(delta / tau)\n\n    # 7. Inherited: Compute the focal modulating factor\n    modulating_factor = torch.pow(1.0 - p_win, gamma)\n\n    # 8. Apply the modulating factor to the core loss\n    final_loss = modulating_factor.detach() * loss_core\n\n    # Apply optional instance weights if provided\n    if weight is not None:\n        final_loss = final_loss * weight\n\n    # Return the mean loss over the batch\n    return final_loss.mean()", "theoretical_basis": ""}, "static_ok": true, "static_reason": "", "static_error_code": "", "dynamic_ok": true, "dynamic_reason": "ok", "loss_value": -0.14999999105930328, "grad_norm": 0.0, "pref_ok": true, "pref_reason": "ok", "pref_mono_pass_rate": 1.0, "pref_swap_pass_rate": 0.6666666666666666, "pref_gap_pass_rate": 1.0}
{"generation": 5, "index": 2, "attempt": 0, "ir": {"name": "AdaptiveFocalClippedLogsigmoidLoss", "intuition": "Mode: explore. This loss function combines a Bradley-Terry style preference loss with a focal modulation mechanism, inheriting key ideas from both parents while introducing novel stability and coupling techniques. The goal is to create a robust loss that focuses on hard examples, maintains numerical stability, and adapts its margin based on batch statistics.\n\nInherited Ideas:\n1.  From `FocalLogsigmoidRankGapLoss`: The core structure of applying a focal modulating factor `(1 - p_win)^gamma` to a base preference loss. This up-weights hard examples where the model confidently predicts the wrong outcome.\n2.  From `AdaptiveMarginLossWithRankGap`: The use of `rank_gap` to compute a non-parametric, batch-adaptive margin. This makes the loss robust to the absolute scale of costs by considering their relative ranks.\n\nNew Coupling Ideas:\n1.  **Clipped Margin with Log-Prob Scaling**: A new adaptive margin is computed as `margin = softplus(beta * rank_gap)`. This margin is then scaled by the detached standard deviation of the log-probability differences (`delta.std()`). This couples the margin's magnitude to the model's current output variance, making the margin larger when the model's predictions are spread out and smaller when they are clustered. To prevent extreme values, the final margin is clipped to a reasonable range `[0, clip_max]`, ensuring stability.\n2.  **Adaptive Temperature from Cost Gap**: The temperature `tau` for calculating the focal term's `p_win` is made adaptive. It is computed as `softplus(cost_b - cost_a)`, directly linking the temperature to the raw cost difference. A larger cost gap (easier problem) results in a higher temperature, softening the sigmoid and making the focal penalty less aggressive. A smaller cost gap (harder problem) leads to a lower temperature, sharpening the sigmoid and increasing focus on correctly resolving these ambiguous pairs.", "pseudocode": "1. Calculate the log-probability difference: delta = log_prob_w - log_prob_l.\n2. Compute the cost rank gap: rank_gap = rank_gap(cost_a, cost_b).\n3. Calculate a base margin from the rank gap: base_margin = softplus(beta * rank_gap).\n4. Compute a scaling factor from the standard deviation of delta: scale = delta.detach().std().\n5. Couple the margin and scale, then clip for stability: final_margin = clamp(base_margin * scale, min=0.0, max=clip_max).\n6. Compute the core preference loss: loss_core = -logsigmoid(delta + final_margin).\n7. Compute an adaptive temperature from the raw cost difference: tau = softplus(cost_b - cost_a).\n8. Calculate the probability of preferring the winner for the focal term: p_win = sigmoid(delta / tau).\n9. Compute the focal modulating factor: modulating_factor = (1 - p_win)^gamma.\n10. Apply the detached modulating factor to the core loss: final_loss = modulating_factor.detach() * loss_core.\n11. Return the mean of the final loss.", "hyperparams": {"beta": 1.5, "gamma": 2.0, "clip_max": 5.0}, "operators_used": ["logsigmoid", "softplus", "sigmoid", "rank_gap", "clamp"], "implementation_hint": {"expects": ["cost_a", "cost_b", "log_prob_w", "log_prob_l"], "returns": "scalar"}, "code": "def generated_loss(batch, model_output, extra):\n    \"\"\"\n    Combines a rank-gap margin with a focal loss, introducing new adaptive couplings.\n    The margin is scaled by the stdev of log-prob differences and clipped.\n    The focal temperature is adapted based on the raw cost gap.\n    \"\"\"\n    # Read hyperparameters with defaults\n    beta = extra.get('beta', 1.5)\n    gamma = extra.get('gamma', 2.0)\n    clip_max = extra.get('clip_max', 5.0)\n\n    # Read required tensors from the batch\n    cost_a = batch['cost_a']\n    cost_b = batch['cost_b']\n    log_prob_w = batch['log_prob_w']\n    log_prob_l = batch['log_prob_l']\n    weight = batch.get('weight')\n\n    # 1. Calculate the log-probability difference\n    delta = log_prob_w - log_prob_l\n\n    # 2. Inherited: Compute rank gap for a non-parametric margin base\n    rank_gap = ops.rank_gap(cost_a, cost_b)\n\n    # 3. New Coupling: Clipped Margin with Log-Prob Scaling\n    base_margin = F.softplus(beta * rank_gap)\n    # Scale margin by the batch's log-prob stdev for adaptive strength\n    delta_std = delta.detach().std().clamp(min=1e-6)\n    scaled_margin = base_margin * delta_std\n    # Clip the final margin to prevent extreme values and ensure stability\n    final_margin = torch.clamp(scaled_margin, min=0.0, max=clip_max)\n\n    # 4. Compute the core preference loss using logsigmoid\n    loss_core = -F.logsigmoid(delta + final_margin)\n\n    # 5. New Coupling: Adaptive Temperature from Cost Gap\n    # Temperature is higher for larger cost gaps (easier pairs)\n    cost_gap = (cost_b - cost_a).detach()\n    tau = F.softplus(cost_gap).clamp(min=1e-2)\n\n    # 6. Inherited: Calculate probability for focal loss\n    p_win = torch.sigmoid(delta / tau)\n\n    # 7. Inherited: Compute the focal modulating factor\n    modulating_factor = torch.pow(1.0 - p_win, gamma)\n\n    # 8. Apply the modulating factor to the core loss\n    final_loss = modulating_factor.detach() * loss_core\n\n    # Apply optional instance weights if provided\n    if weight is not None:\n        final_loss = final_loss * weight\n\n    # Return the mean loss over the batch\n    return final_loss.mean()", "theoretical_basis": "A hybrid Bradley-Terry and focal loss model with adaptive coupling. The core preference is modeled using a logistic function (-logsigmoid) on a margin-adjusted log-probability difference. The margin is dynamically scaled by both the cost rank-gap and the batch-wise variance of model predictions, then clipped for stability. This is coupled with a focal modulation mechanism where the temperature is determined by the raw cost gap, allowing the loss to adapt its focus based on both model uncertainty and problem difficulty."}, "static_ok": true, "static_reason": "", "static_error_code": "", "dynamic_ok": true, "dynamic_reason": "ok", "loss_value": 0.173286572098732, "grad_norm": 0.0, "pref_ok": false, "pref_reason": "pref_semantic_violation", "pref_mono_pass_rate": 1.0, "pref_swap_pass_rate": 1.0, "pref_gap_pass_rate": 0.0, "dynamic_error_code": "E_PREF_SEMANTIC"}
{"generation": 5, "index": 2, "attempt": 1, "ir": {"name": "AdaptiveFocalClippedLogsigmoidLoss", "intuition": "Repaired: The original loss failed the preference gate's gap check (E_PREF_SEMANTIC, gap_pass_rate=0.0). The failure was caused by scaling the margin by `delta.std()`, which is a batch-level statistic. This made the loss for a specific pair dependent on other pairs in the batch, violating the semantic requirement that a larger cost gap for a fixed pair should lead to a lower loss. I have removed the `delta.std()` scaling factor from the margin calculation. The margin is now simply `softplus(beta * rank_gap)` and clipped, which is a per-instance value and respects the gap property.", "pseudocode": "1. Calculate the log-probability difference: delta = log_prob_w - log_prob_l.\n2. Compute the cost rank gap: rank_gap = rank_gap(cost_a, cost_b).\n3. Calculate a base margin from the rank gap: base_margin = softplus(beta * rank_gap).\n4. Clip the margin for stability: final_margin = clamp(base_margin, min=0.0, max=clip_max).\n5. Compute the core preference loss: loss_core = -logsigmoid(delta + final_margin).\n6. Compute an adaptive temperature from the raw cost difference: tau = softplus(cost_b - cost_a).\n7. Calculate the probability of preferring the winner for the focal term: p_win = sigmoid(delta / tau).\n8. Compute the focal modulating factor: modulating_factor = (1 - p_win)^gamma.\n9. Apply the detached modulating factor to the core loss: final_loss = modulating_factor.detach() * loss_core.\n10. Return the mean of the final loss.", "hyperparams": {"beta": 1.5, "gamma": 2.0, "clip_max": 5.0}, "operators_used": ["logsigmoid", "softplus", "sigmoid", "rank_gap", "clamp"], "implementation_hint": {"expects": ["cost_a", "cost_b", "log_prob_w", "log_prob_l"], "returns": "scalar"}, "code": "def generated_loss(batch, model_output, extra):\n    \"\"\"\n    Combines a rank-gap margin with a focal loss, introducing new adaptive couplings.\n    The margin is scaled by the stdev of log-prob differences and clipped.\n    The focal temperature is adapted based on the raw cost gap.\n    \"\"\"\n    # Read hyperparameters with defaults\n    beta = extra.get('beta', 1.5)\n    gamma = extra.get('gamma', 2.0)\n    clip_max = extra.get('clip_max', 5.0)\n\n    # Read required tensors from the batch\n    cost_a = batch['cost_a']\n    cost_b = batch['cost_b']\n    log_prob_w = batch['log_prob_w']\n    log_prob_l = batch['log_prob_l']\n    weight = batch.get('weight')\n\n    # 1. Calculate the log-probability difference\n    delta = log_prob_w - log_prob_l\n\n    # 2. Inherited: Compute rank gap for a non-parametric margin base\n    rank_gap = ops.rank_gap(cost_a, cost_b)\n\n    # 3. New Coupling: Clipped Margin with Log-Prob Scaling\n    base_margin = F.softplus(beta * rank_gap)\n    # REPAIR: Removed scaling by delta.std() which violates preference gate semantics.\n    # The batch-wide std made the loss for one pair dependent on others, failing the gap check.\n    final_margin = torch.clamp(base_margin, min=0.0, max=clip_max)\n\n    # 4. Compute the core preference loss using logsigmoid\n    loss_core = -F.logsigmoid(delta + final_margin)\n\n    # 5. New Coupling: Adaptive Temperature from Cost Gap\n    # Temperature is higher for larger cost gaps (easier pairs)\n    cost_gap = (cost_b - cost_a).detach()\n    tau = F.softplus(cost_gap).clamp(min=1e-2)\n\n    # 6. Inherited: Calculate probability for focal loss\n    p_win = torch.sigmoid(delta / tau)\n\n    # 7. Inherited: Compute the focal modulating factor\n    modulating_factor = torch.pow(1.0 - p_win, gamma)\n\n    # 8. Apply the modulating factor to the core loss\n    final_loss = modulating_factor.detach() * loss_core\n\n    # Apply optional instance weights if provided\n    if weight is not None:\n        final_loss = final_loss * weight\n\n    # Return the mean loss over the batch\n    return final_loss.mean()", "theoretical_basis": ""}, "static_ok": true, "static_reason": "", "static_error_code": "", "dynamic_ok": true, "dynamic_reason": "ok", "loss_value": 0.04189196601510048, "grad_norm": 0.0, "pref_ok": true, "pref_reason": "ok", "pref_mono_pass_rate": 1.0, "pref_swap_pass_rate": 1.0, "pref_gap_pass_rate": 0.8333333333333334}
{"generation": 5, "index": 3, "attempt": 0, "ir": {"name": "AdaptiveFocalLogsigmoidWithClippedMargin", "intuition": "Mode: explore. This loss combines the robust Bradley-Terry framework of `FocalLogsigmoidRankGapLoss` with a novel margin calculation. The goal is to create a loss that is sensitive to the magnitude of cost differences but is not dominated by extreme outliers.\n\nInherited Ideas:\n1.  From `FocalLogsigmoidRankGapLoss`: The core loss is a focal-modulated `-logsigmoid`, which up-weights hard examples. This maintains the benefits of hard example mining.\n2.  From `FocalLogsigmoidRankGapLoss`: The log-probability difference (`delta`) is z-scored before use. This provides batch-level normalization and stabilizes training against extreme model outputs.\n\nNew Coupling Ideas:\n1.  **Clipped Absolute Cost Margin**: Instead of a `rank_gap` margin, which only considers relative cost orderings, this loss uses the absolute cost difference `cost_b - cost_a`. To prevent this margin from exploding with cost outliers, it is clipped to a maximum value (`margin_max`). This makes the margin sensitive to the magnitude of improvement but robust to extreme cost gaps.\n2.  **Dynamic Focal Gamma**: The focal strength `gamma` is made dynamic. It scales inversely with the model's confidence (`p_win`), becoming stronger for pairs where the model is uncertain (p_win is close to 0.5). This is computed as `gamma_base / (1.0 + abs(p_win - 0.5))`, focusing learning on the most ambiguous examples rather than just the incorrect ones.", "pseudocode": "1. Calculate the log-probability difference: delta = log_prob_w - log_prob_l.\n2. Normalize delta across the batch for stability: normalized_delta = zscore(delta).\n3. Calculate the absolute cost difference: cost_gap = cost_b - cost_a.\n4. Create a clipped margin from the cost gap: margin = clamp(beta * cost_gap, min=0, max=margin_max).\n5. Compute the core preference loss: loss_core = -logsigmoid(normalized_delta + margin).\n6. Calculate the model's confidence in the winner: p_win = sigmoid(delta).\n7. Compute a dynamic focal strength (gamma) that is highest when p_win is near 0.5: dynamic_gamma = gamma_base / (1.0 + abs(p_win - 0.5)).\n8. Calculate the focal modulating factor using the dynamic gamma: modulating_factor = (1 - p_win)^dynamic_gamma.\n9. Apply the detached modulating factor to the core loss: final_loss = modulating_factor.detach() * loss_core.\n10. Return the mean of the final loss.", "hyperparams": {"beta": 0.5, "margin_max": 2.0, "gamma_base": 2.0}, "operators_used": ["zscore", "clamp", "logsigmoid", "sigmoid"], "implementation_hint": {"expects": ["cost_a", "cost_b", "log_prob_w", "log_prob_l"], "returns": "scalar"}, "code": "def generated_loss(batch, model_output, extra):\n    \"\"\"\n    Combines a z-scored logsigmoid loss with a clipped absolute cost margin and a dynamic focal modulation.\n    \"\"\"\n    # Read hyperparameters with defaults\n    beta = extra.get('beta', 0.5)\n    margin_max = extra.get('margin_max', 2.0)\n    gamma_base = extra.get('gamma_base', 2.0)\n\n    # Read required tensors from the batch\n    cost_a = batch['cost_a']\n    cost_b = batch['cost_b']\n    log_prob_w = batch['log_prob_w']\n    log_prob_l = batch['log_prob_l']\n    weight = batch.get('weight')\n\n    # 1. Calculate the log-probability difference\n    delta = log_prob_w - log_prob_l\n\n    # 2. Inherited: Normalize delta for stability\n    normalized_delta = ops.zscore(delta)\n\n    # 3. New Coupling: Clipped Absolute Cost Margin\n    cost_gap = cost_b - cost_a\n    # Ensure margin is non-negative and capped to prevent explosion\n    margin = torch.clamp(beta * cost_gap, min=0.0, max=margin_max)\n\n    # 4. Compute the core preference loss\n    loss_core = -F.logsigmoid(normalized_delta + margin)\n\n    # 5. Calculate probability for focal loss\n    with torch.no_grad():\n        p_win = torch.sigmoid(delta)\n\n        # 6. New Coupling: Dynamic Focal Gamma\n        # Gamma is highest when p_win is 0.5 (max uncertainty)\n        dynamic_gamma = gamma_base / (1.0 + torch.abs(p_win - 0.5))\n\n        # 7. Compute the focal modulating factor\n        modulating_factor = torch.pow(1.0 - p_win, dynamic_gamma)\n\n    # 8. Apply the modulating factor to the core loss\n    final_loss = modulating_factor * loss_core\n\n    # Apply optional instance weights if provided\n    if weight is not None:\n        final_loss = final_loss * weight\n\n    # Return the mean loss over the batch\n    return final_loss.mean()", "theoretical_basis": "A dynamically modulated Bradley-Terry preference model. The core loss is logistic (-logsigmoid), operating on a z-scored log-probability difference plus a margin. The margin is derived from the clipped absolute cost difference, making the loss sensitive but robust to cost magnitudes. The focal modulation strength is dynamic, adapting to the model's uncertainty for each pair, which provides a more nuanced form of hard example mining."}, "static_ok": true, "static_reason": "", "static_error_code": "", "dynamic_ok": true, "dynamic_reason": "ok", "loss_value": 0.11851924657821655, "grad_norm": 0.0, "pref_ok": true, "pref_reason": "ok", "pref_mono_pass_rate": 0.783203125, "pref_swap_pass_rate": 1.0, "pref_gap_pass_rate": 1.0}
{"generation": 5, "index": 4, "attempt": 0, "ir": {"name": "FocalLogsigmoidRankGapWithCostNormalization", "intuition": "Mode: explore. This loss function combines the strengths of its parents: the focal modulation and rank-gap margin from `FocalLogsigmoidRankGapLoss`, with a core Bradley-Terry `logsigmoid` structure. The goal is to retain the effective hard-example mining of the focal loss and the batch-adaptive margin of the rank gap, while improving stability and theoretical consistency.\n\nInherited ideas:\n1. From `FocalLogsigmoidRankGapLoss`: The use of a focal modulating factor `(1 - p_win)^gamma` to up-weight hard examples.\n2. From `FocalLogsigmoidRankGapLoss` and `AdaptiveMarginLossWithRankGap`: The use of `rank_gap` to create a dynamic, non-parametric margin `margin = softplus(beta * rank_gap - offset)` that is robust to the scale of costs.\n\nNew coupling ideas:\n1. **Cost-based Delta Normalization**: Instead of z-scoring the log-probability difference `delta`, this loss normalizes `delta` by the z-scored cost gap `z_cost_gap = zscore(cost_b - cost_a)`. The core term becomes `logsigmoid(delta / (softplus(z_cost_gap) + epsilon))`. This couples the learning rate for a given pair directly to the magnitude of its cost difference relative to the batch. Pairs with a large cost difference (easy examples) get a smaller update, while pairs with a small cost difference (hard examples) get a larger update, preventing easy examples from dominating the gradient.\n2. **Margin Scaling by Log-Prob Variance**: The adaptive margin is scaled by the standard deviation of the winning log-probabilities `log_prob_w.std()`. This makes the margin more sensitive when the model's outputs are very certain (low variance) and less sensitive when they are uncertain (high variance), dynamically adjusting the separation target based on the model's current confidence landscape.", "pseudocode": "1. Calculate the log-probability difference: delta = log_prob_w - log_prob_l.\n2. Calculate the raw cost gap: cost_gap = cost_b - cost_a.\n3. Normalize the cost gap across the batch: z_cost_gap = zscore(cost_gap).\n4. Compute the cost-normalized delta: normalized_delta = delta / (softplus(z_cost_gap) + epsilon).\n5. Compute the cost rank gap: rank_gap = rank_gap(cost_a, cost_b).\n6. Compute a dynamic margin scale based on log-prob variance: margin_scale = softplus(log_prob_w.detach().std()).\n7. Calculate an adaptive margin, scaled by the variance: margin = margin_scale * softplus(beta * rank_gap - offset).\n8. Compute the core preference loss: loss_core = -logsigmoid(normalized_delta + margin).\n9. Calculate the probability of preferring the winner for the focal term: p_win = sigmoid(delta).\n10. Compute the focal modulating factor: modulating_factor = (1 - p_win)^gamma.\n11. Apply the detached modulating factor to the core loss: final_loss = modulating_factor.detach() * loss_core.\n12. Return the mean of the final loss.", "hyperparams": {"beta": 1.5, "offset": 0.25, "gamma": 2.0, "epsilon": 1e-06}, "operators_used": ["logsigmoid", "softplus", "sigmoid", "rank_gap", "zscore"], "implementation_hint": {"expects": ["cost_a", "cost_b", "log_prob_w", "log_prob_l"], "returns": "scalar"}, "code": "def generated_loss(batch, model_output, extra):\n    \"\"\"\n    Combines focal loss and a rank-gap margin with novel cost-based normalization and margin scaling.\n    \"\"\"\n    # Read hyperparameters with defaults\n    beta = extra.get('beta', 1.5)\n    offset = extra.get('offset', 0.25)\n    gamma = extra.get('gamma', 2.0)\n    epsilon = extra.get('epsilon', 1e-6)\n\n    # Read required tensors from the batch\n    cost_a = batch['cost_a']\n    cost_b = batch['cost_b']\n    log_prob_w = batch['log_prob_w']\n    log_prob_l = batch['log_prob_l']\n    weight = batch.get('weight')\n\n    # 1. Calculate the log-probability difference\n    delta = log_prob_w - log_prob_l\n\n    # 2. New Coupling: Cost-based Delta Normalization\n    cost_gap = cost_b - cost_a\n    z_cost_gap = ops.zscore(cost_gap)\n    # softplus + epsilon ensures a positive, non-zero denominator\n    normalized_delta = delta / (F.softplus(z_cost_gap) + epsilon)\n\n    # 3. Inherited: Compute the rank gap of the costs\n    rank_gap = ops.rank_gap(cost_a, cost_b)\n\n    # 4. New Coupling: Margin Scaling by Log-Prob Variance\n    # Use detached std dev to avoid influencing gradients through the scale\n    with torch.no_grad():\n        margin_scale = F.softplus(log_prob_w.std())\n\n    # 5. Inherited: Calculate an adaptive margin from the rank gap, now scaled\n    margin = margin_scale * F.softplus(beta * rank_gap - offset)\n\n    # 6. Compute the core preference loss\n    loss_core = -F.logsigmoid(normalized_delta + margin)\n\n    # 7. Inherited: Calculate probability for focal loss\n    p_win = torch.sigmoid(delta)\n\n    # 8. Inherited: Compute the focal modulating factor\n    modulating_factor = torch.pow(1.0 - p_win, gamma)\n\n    # 9. Apply the modulating factor to the core loss\n    final_loss = modulating_factor.detach() * loss_core\n\n    # Apply optional instance weights if provided\n    if weight is not None:\n        final_loss = final_loss * weight\n\n    # Return the mean loss over the batch\n    return final_loss.mean()", "theoretical_basis": "A Bradley-Terry preference model with dynamic re-weighting and normalization. The core logistic loss is applied to a log-probability difference that is normalized by the z-scored cost gap, focusing learning on pairs with competitively small cost differences. The preference margin is adaptive, based on cost ranks and scaled by model output variance. A focal mechanism further up-weights examples where the model is confidently wrong, providing a multi-faceted hard example mining strategy."}, "static_ok": true, "static_reason": "", "static_error_code": "", "dynamic_ok": true, "dynamic_reason": "ok", "loss_value": 0.07559604197740555, "grad_norm": 0.0, "pref_ok": false, "pref_reason": "pref_semantic_violation", "pref_mono_pass_rate": 1.0, "pref_swap_pass_rate": 1.0, "pref_gap_pass_rate": 0.5, "dynamic_error_code": "E_PREF_SEMANTIC"}
{"generation": 5, "index": 4, "attempt": 1, "ir": {"name": "FocalLogsigmoidRankGapWithCostNormalization", "intuition": "Repaired: The original loss failed the preference gate's cost gap check (E_PREF_SEMANTIC), meaning the loss did not consistently decrease as the cost gap between the winning and losing solutions widened. This was caused by the `normalized_delta = delta / (softplus(z_cost_gap) + epsilon)` term, where `z_cost_gap` is derived from `cost_b - cost_a`. As the cost gap increased, `z_cost_gap` would increase, making the denominator larger and shrinking the overall loss term, which violates the desired semantic relationship. To fix this, I replaced the division with multiplication: `normalized_delta = delta * (softplus(z_cost_gap) + epsilon)`. Now, a larger cost gap correctly leads to a larger, more impactful loss term, encouraging the model to learn from these clear preference signals.", "pseudocode": "1. Calculate the log-probability difference: delta = log_prob_w - log_prob_l.\n2. Calculate the raw cost gap: cost_gap = cost_b - cost_a.\n3. Normalize the cost gap across the batch: z_cost_gap = zscore(cost_gap).\n4. Compute the cost-scaled delta: scaled_delta = delta * (softplus(z_cost_gap) + epsilon).\n5. Compute the cost rank gap: rank_gap = rank_gap(cost_a, cost_b).\n6. Compute a dynamic margin scale based on log-prob variance: margin_scale = softplus(log_prob_w.detach().std()).\n7. Calculate an adaptive margin, scaled by the variance: margin = margin_scale * softplus(beta * rank_gap - offset).\n8. Compute the core preference loss: loss_core = -logsigmoid(scaled_delta + margin).\n9. Calculate the probability of preferring the winner for the focal term: p_win = sigmoid(delta).\n10. Compute the focal modulating factor: modulating_factor = (1 - p_win)^gamma.\n11. Apply the detached modulating factor to the core loss: final_loss = modulating_factor.detach() * loss_core.\n12. Return the mean of the final loss.", "hyperparams": {"beta": 1.5, "offset": 0.25, "gamma": 2.0, "epsilon": 1e-06}, "operators_used": ["logsigmoid", "softplus", "sigmoid", "rank_gap", "zscore"], "implementation_hint": {"expects": ["cost_a", "cost_b", "log_prob_w", "log_prob_l"], "returns": "scalar"}, "code": "def generated_loss(batch, model_output, extra):\n    \"\"\"\n    Combines focal loss and a rank-gap margin with novel cost-based normalization and margin scaling.\n    \"\"\"\n    # Read hyperparameters with defaults\n    beta = extra.get('beta', 1.5)\n    offset = extra.get('offset', 0.25)\n    gamma = extra.get('gamma', 2.0)\n    epsilon = extra.get('epsilon', 1e-6)\n\n    # Read required tensors from the batch\n    cost_a = batch['cost_a']\n    cost_b = batch['cost_b']\n    log_prob_w = batch['log_prob_w']\n    log_prob_l = batch['log_prob_l']\n    weight = batch.get('weight')\n\n    # 1. Calculate the log-probability difference\n    delta = log_prob_w - log_prob_l\n\n    # 2. New Coupling: Cost-based Delta Scaling\n    cost_gap = cost_b - cost_a\n    z_cost_gap = ops.zscore(cost_gap)\n    # softplus + epsilon ensures a positive, non-zero scale\n    # REPAIR: Changed division to multiplication to fix preference semantic violation.\n    # A larger cost gap should increase the loss term, not decrease it.\n    scaled_delta = delta * (F.softplus(z_cost_gap) + epsilon)\n\n    # 3. Inherited: Compute the rank gap of the costs\n    rank_gap = ops.rank_gap(cost_a, cost_b)\n\n    # 4. New Coupling: Margin Scaling by Log-Prob Variance\n    # Use detached std dev to avoid influencing gradients through the scale\n    with torch.no_grad():\n        margin_scale = F.softplus(log_prob_w.std())\n\n    # 5. Inherited: Calculate an adaptive margin from the rank gap, now scaled\n    margin = margin_scale * F.softplus(beta * rank_gap - offset)\n\n    # 6. Compute the core preference loss\n    loss_core = -F.logsigmoid(scaled_delta + margin)\n\n    # 7. Inherited: Calculate probability for focal loss\n    p_win = torch.sigmoid(delta)\n\n    # 8. Inherited: Compute the focal modulating factor\n    modulating_factor = torch.pow(1.0 - p_win, gamma)\n\n    # 9. Apply the modulating factor to the core loss\n    final_loss = modulating_factor.detach() * loss_core\n\n    # Apply optional instance weights if provided\n    if weight is not None:\n        final_loss = final_loss * weight\n\n    # Return the mean loss over the batch\n    return final_loss.mean()", "theoretical_basis": ""}, "static_ok": true, "static_reason": "", "static_error_code": "", "dynamic_ok": true, "dynamic_reason": "ok", "loss_value": 0.07559604197740555, "grad_norm": 0.0, "pref_ok": false, "pref_reason": "pref_semantic_violation", "pref_mono_pass_rate": 1.0, "pref_swap_pass_rate": 1.0, "pref_gap_pass_rate": 0.0, "dynamic_error_code": "E_PREF_SEMANTIC"}
{"generation": 5, "index": 4, "attempt": 2, "ir": {"name": "FocalLogsigmoidRankGapWithCostNormalization", "intuition": "The original loss failed the preference gate's cost gap check (E_PREF_SEMANTIC), meaning the loss did not consistently increase as the cost gap between the winning and losing solutions widened. The `scaled_delta` term used `softplus(z_cost_gap)`, where `z_cost_gap` is the z-scored cost difference. For negative z-scores (cost gaps smaller than the batch mean), `softplus` approaches zero, which shrinks the `scaled_delta` and thus the loss. This is the opposite of the desired behavior for smaller-than-average gaps. To fix this, I replaced `zscore` with `normalize` on the cost gap. The `normalize` operator scales the gap to be between 0 and 1. This ensures the scaling factor `softplus(normalized_cost_gap) + epsilon` is always positive and monotonically increases with the cost gap, correctly reflecting that larger cost differences are more significant and should result in a larger loss.", "pseudocode": "1. Calculate the log-probability difference: delta = log_prob_w - log_prob_l.\n2. Calculate the raw cost gap: cost_gap = cost_b - cost_a.\n3. Normalize the cost gap across the batch to the [0, 1] range: normalized_cost_gap = normalize(cost_gap).\n4. Compute the cost-scaled delta: scaled_delta = delta * (softplus(normalized_cost_gap) + epsilon).\n5. Compute the cost rank gap: rank_gap = rank_gap(cost_a, cost_b).\n6. Compute a dynamic margin scale based on log-prob variance: margin_scale = softplus(log_prob_w.detach().std()).\n7. Calculate an adaptive margin, scaled by the variance: margin = margin_scale * softplus(beta * rank_gap - offset).\n8. Compute the core preference loss: loss_core = -logsigmoid(scaled_delta + margin).\n9. Calculate the probability of preferring the winner for the focal term: p_win = sigmoid(delta).\n10. Compute the focal modulating factor: modulating_factor = (1 - p_win)^gamma.\n11. Apply the detached modulating factor to the core loss: final_loss = modulating_factor.detach() * loss_core.\n12. Return the mean of the final loss.", "hyperparams": {"beta": 1.5, "offset": 0.25, "gamma": 2.0, "epsilon": 1e-06}, "operators_used": ["logsigmoid", "softplus", "sigmoid", "rank_gap", "normalize"], "implementation_hint": {"expects": ["cost_a", "cost_b", "log_prob_w", "log_prob_l"], "returns": "scalar"}, "code": "def generated_loss(batch, model_output, extra):\n    \"\"\"\n    Combines focal loss and a rank-gap margin with novel cost-based normalization and margin scaling.\n    \"\"\"\n    # Read hyperparameters with defaults\n    beta = extra.get('beta', 1.5)\n    offset = extra.get('offset', 0.25)\n    gamma = extra.get('gamma', 2.0)\n    epsilon = extra.get('epsilon', 1e-6)\n\n    # Read required tensors from the batch\n    cost_a = batch['cost_a']\n    cost_b = batch['cost_b']\n    log_prob_w = batch['log_prob_w']\n    log_prob_l = batch['log_prob_l']\n    weight = batch.get('weight')\n\n    # 1. Calculate the log-probability difference\n    delta = log_prob_w - log_prob_l\n\n    # 2. New Coupling: Cost-based Delta Scaling\n    cost_gap = cost_b - cost_a\n    # REPAIR: Replaced zscore with normalize to fix preference semantic violation.\n    # zscore can be negative, causing softplus to approach 0 and shrink the loss for smaller gaps.\n    # normalize ensures the cost_gap scaling factor is monotonic and positive.\n    normalized_cost_gap = ops.normalize(cost_gap)\n    scaled_delta = delta * (F.softplus(normalized_cost_gap) + epsilon)\n\n    # 3. Inherited: Compute the rank gap of the costs\n    rank_gap = ops.rank_gap(cost_a, cost_b)\n\n    # 4. New Coupling: Margin Scaling by Log-Prob Variance\n    # Use detached std dev to avoid influencing gradients through the scale\n    with torch.no_grad():\n        margin_scale = F.softplus(log_prob_w.std())\n\n    # 5. Inherited: Calculate an adaptive margin from the rank gap, now scaled\n    margin = margin_scale * F.softplus(beta * rank_gap - offset)\n\n    # 6. Compute the core preference loss\n    loss_core = -F.logsigmoid(scaled_delta + margin)\n\n    # 7. Inherited: Calculate probability for focal loss\n    p_win = torch.sigmoid(delta)\n\n    # 8. Inherited: Compute the focal modulating factor\n    modulating_factor = torch.pow(1.0 - p_win, gamma)\n\n    # 9. Apply the modulating factor to the core loss\n    final_loss = modulating_factor.detach() * loss_core\n\n    # Apply optional instance weights if provided\n    if weight is not None:\n        final_loss = final_loss * weight\n\n    # Return the mean loss over the batch\n    return final_loss.mean()", "theoretical_basis": ""}, "static_ok": true, "static_reason": "", "static_error_code": "", "dynamic_ok": true, "dynamic_reason": "ok", "loss_value": 0.07559604197740555, "grad_norm": 0.0, "pref_ok": false, "pref_reason": "pref_semantic_violation", "pref_mono_pass_rate": 1.0, "pref_swap_pass_rate": 1.0, "pref_gap_pass_rate": 0.0, "dynamic_error_code": "E_PREF_SEMANTIC"}
{"generation": 5, "index": 5, "attempt": 0, "ir": {"name": "AdaptiveFocalLogsigmoidWithDynamicTau", "intuition": "Mode: explore. This loss function refines the powerful combination of a Bradley-Terry style logsigmoid loss, a rank-gap based margin, and focal modulation. It aims to improve stability and adaptability by introducing more dynamic, data-dependent components.\n\nInherited Ideas:\n1. From `FocalLogsigmoidRankGapLoss`: The core structure of combining a `logsigmoid` preference loss with a focal modulation factor `(1 - p_win)^gamma` to up-weight hard examples.\n2. From `AdaptiveMarginLossWithRankGap`: The use of `rank_gap` on costs to create a non-parametric, batch-adaptive margin `margin = softplus(beta * rank_gap - offset)`. This makes the learning target robust to the absolute scale of costs.\n\nNew Coupling Ideas:\n1. **Dynamic Temperature (Tau) based on Cost Gap**: The temperature `tau` for the focal loss's sigmoid is now coupled to the cost difference. Specifically, `tau = softplus(cost_b - cost_a)`. This makes the focal modulation more sensitive (smaller tau, sharper sigmoid) for pairs with small cost differences, where the preference is subtle and the model needs to be more discerning. Conversely, for pairs with large cost differences (clear winner), `tau` is larger, softening the sigmoid and reducing the penalty for being uncertain.\n2. **Log-Prob Difference Clipping**: To prevent extreme values of `delta = log_prob_w - log_prob_l` from causing numerical instability or overly large gradients, `delta` is clamped within a range defined by `[-clip_value, clip_value]`. This is a simpler and more direct stability trick than z-scoring, avoiding batch-wide dependencies and ensuring gradients remain bounded.", "pseudocode": "1. Calculate the log-probability difference: delta = log_prob_w - log_prob_l.\n2. Clip delta to a fixed range for stability: clipped_delta = clamp(delta, -clip_value, clip_value).\n3. Compute the cost rank gap: rank_gap = rank_gap(cost_a, cost_b).\n4. Calculate an adaptive margin from the rank gap: margin = softplus(beta * rank_gap - offset).\n5. Compute the core preference loss using the clipped delta and margin: loss_core = -logsigmoid(clipped_delta + margin).\n6. Compute a dynamic temperature based on the cost gap: tau = softplus(cost_b - cost_a) + epsilon.\n7. Calculate the probability of preferring the winner using the original, unclipped delta: p_win = sigmoid(delta / tau).\n8. Compute the focal modulating factor: modulating_factor = (1 - p_win)^gamma.\n9. Apply the detached modulating factor to the core loss: final_loss = modulating_factor.detach() * loss_core.\n10. Return the mean of the final loss.", "hyperparams": {"beta": 1.5, "offset": 0.25, "gamma": 2.0, "clip_value": 5.0, "epsilon": 0.01}, "operators_used": ["logsigmoid", "sigmoid", "softplus", "rank_gap", "clamp"], "implementation_hint": {"expects": ["cost_a", "cost_b", "log_prob_w", "log_prob_l"], "returns": "scalar"}, "code": "def generated_loss(batch, model_output, extra):\n    \"\"\"\n    Combines a rank-gap adaptive margin with a focal loss mechanism, applied to a logsigmoid base loss.\n    Introduces delta clipping for stability and a dynamic temperature coupled to the cost gap for the focal component.\n    \"\"\"\n    # Read hyperparameters with defaults\n    beta = extra.get('beta', 1.5)\n    offset = extra.get('offset', 0.25)\n    gamma = extra.get('gamma', 2.0)\n    clip_value = extra.get('clip_value', 5.0)\n    epsilon = extra.get('epsilon', 0.01)\n\n    # Read required tensors from the batch\n    cost_a = batch['cost_a']\n    cost_b = batch['cost_b']\n    log_prob_w = batch['log_prob_w']\n    log_prob_l = batch['log_prob_l']\n    weight = batch.get('weight')\n\n    # 1. Calculate the log-probability difference\n    delta = log_prob_w - log_prob_l\n\n    # 2. New Coupling: Clip delta for stability\n    clipped_delta = torch.clamp(delta, -clip_value, clip_value)\n\n    # 3. Inherited: Compute the rank gap of the costs\n    rank_gap = ops.rank_gap(cost_a, cost_b)\n\n    # 4. Inherited: Calculate an adaptive margin from the rank gap\n    margin = F.softplus(beta * rank_gap - offset)\n\n    # 5. Compute the core preference loss using logsigmoid and the clipped delta\n    loss_core = -F.logsigmoid(clipped_delta + margin)\n\n    # 6. New Coupling: Dynamic temperature based on cost gap\n    # Ensures tau is positive and non-zero\n    tau = F.softplus(cost_b - cost_a) + epsilon\n\n    # 7. Inherited: Calculate probability for focal loss using original delta\n    p_win = torch.sigmoid(delta / tau)\n\n    # 8. Inherited: Compute the focal modulating factor\n    modulating_factor = torch.pow(1.0 - p_win, gamma)\n\n    # 9. Apply the detached modulating factor to the core loss\n    final_loss = modulating_factor.detach() * loss_core\n\n    # Apply optional instance weights if provided\n    if weight is not None:\n        final_loss = final_loss * weight\n\n    # Return the mean loss over the batch\n    return final_loss.mean()", "theoretical_basis": "A dynamically modulated Bradley-Terry preference model. The core loss is logistic (-logsigmoid), applied to a clipped log-probability difference plus a rank-based margin. The novelty lies in coupling the focal loss temperature directly to the cost gap, which allows the loss to adapt its focus based on the difficulty of the preference decision itself, rather than just the model's current confidence."}, "static_ok": true, "static_reason": "", "static_error_code": "", "dynamic_ok": true, "dynamic_reason": "ok", "loss_value": 0.05026540532708168, "grad_norm": 0.0, "pref_ok": true, "pref_reason": "ok", "pref_mono_pass_rate": 1.0, "pref_swap_pass_rate": 0.8333333333333334, "pref_gap_pass_rate": 1.0}
{"generation": 5, "index": 6, "attempt": 0, "ir": {"name": "ClippedFocalLogsigmoidLoss", "intuition": "Mode: explore. This loss function refines the focal logsigmoid concept by introducing a clipping mechanism on the log-probability difference (`delta`) to prevent gradient explosion from outlier pairs, a common issue with unbounded losses like logsigmoid. It inherits the core focal modulation and rank-gap margin from `FocalLogsigmoidRankGapLoss` but simplifies the normalization by replacing z-scoring with a more direct, robust clipping approach. \n\nInherited Ideas:\n1. From `FocalLogsigmoidRankGapLoss`: The use of a focal modulating factor `(1 - p_win)^gamma` to up-weight hard examples where the model confidently prefers the wrong choice.\n2. From `FocalLogsigmoidRankGapLoss` and `AdaptiveMarginLossWithRankGap`: The use of `rank_gap` to create a dynamic, batch-adaptive margin (`margin = softplus(beta * rank_gap - offset)`), making the loss robust to the absolute scale of costs.\n\nNew Coupling Ideas:\n1. **Delta Clipping for Stability**: Instead of z-scoring `delta`, this loss clips it within a range `[-clip_value, clip_value]`. This directly bounds the argument to `logsigmoid`, preventing extremely large negative values from causing exploding gradients while preserving the original scale and sign for most examples. This is a simpler and potentially more stable alternative to batch-wise z-scoring.\n2. **Margin-Scaled Focal Temperature**: The temperature `tau` for calculating the focal probability `p_win` is dynamically scaled by the margin. `tau = tau_base + margin.detach()`. This makes the focal modulation less sensitive for pairs with a large cost gap (and thus a large margin), as these are 'easier' examples that don't need as much re-weighting. Conversely, for pairs with a small cost gap (small margin), the focal modulation becomes more sensitive, focusing learning on subtly different pairs.", "pseudocode": "1. Calculate the log-probability difference: delta = log_prob_w - log_prob_l.\n2. Compute the cost rank gap: rank_gap = rank_gap(cost_a, cost_b).\n3. Calculate an adaptive margin from the rank gap: margin = softplus(beta * rank_gap - offset).\n4. Clip the delta for stability: clipped_delta = clamp(delta, min=-clip_value, max=clip_value).\n5. Compute the core preference loss using the clipped delta and margin: loss_core = -logsigmoid(clipped_delta + margin).\n6. Compute a margin-scaled temperature: tau = tau_base + margin.detach().\n7. Calculate the probability of preferring the winner for the focal term: p_win = sigmoid(delta / tau).\n8. Compute the focal modulating factor: modulating_factor = (1 - p_win)^gamma.\n9. Apply the detached modulating factor to the core loss: final_loss = modulating_factor.detach() * loss_core.\n10. Return the mean of the final loss over the batch.", "hyperparams": {"beta": 1.5, "offset": 0.25, "gamma": 2.0, "clip_value": 10.0, "tau_base": 0.5}, "operators_used": ["rank_gap", "softplus", "logsigmoid", "sigmoid", "clamp"], "implementation_hint": {"expects": ["cost_a", "cost_b", "log_prob_w", "log_prob_l"], "returns": "scalar"}, "code": "def generated_loss(batch, model_output, extra):\n    \"\"\"\n    Combines a rank-gap adaptive margin with a focal loss mechanism on a logsigmoid base.\n    Introduces delta clipping for stability and a margin-scaled temperature for the focal component.\n    \"\"\"\n    # Read hyperparameters with defaults\n    beta = extra.get('beta', 1.5)\n    offset = extra.get('offset', 0.25)\n    gamma = extra.get('gamma', 2.0)\n    clip_value = extra.get('clip_value', 10.0)\n    tau_base = extra.get('tau_base', 0.5)\n\n    # Read required tensors from the batch\n    cost_a = batch['cost_a']\n    cost_b = batch['cost_b']\n    log_prob_w = batch['log_prob_w']\n    log_prob_l = batch['log_prob_l']\n    weight = batch.get('weight')\n\n    # 1. Calculate the log-probability difference\n    delta = log_prob_w - log_prob_l\n\n    # 2. Inherited: Compute the rank gap of the costs\n    rank_gap = ops.rank_gap(cost_a, cost_b)\n\n    # 3. Inherited: Calculate an adaptive margin from the rank gap\n    margin = F.softplus(beta * rank_gap - offset)\n\n    # 4. New Coupling: Clip delta for stability\n    clipped_delta = torch.clamp(delta, min=-clip_value, max=clip_value)\n\n    # 5. Compute the core preference loss using logsigmoid\n    loss_core = -F.logsigmoid(clipped_delta + margin)\n\n    # 6. New Coupling: Margin-scaled temperature for focal modulation\n    # Detach margin to prevent gradients from flowing through tau\n    tau = tau_base + margin.detach()\n    \n    # 7. Inherited: Calculate probability for focal loss (using original delta)\n    p_win = torch.sigmoid(delta / tau)\n\n    # 8. Inherited: Compute the focal modulating factor\n    modulating_factor = torch.pow(1.0 - p_win, gamma)\n\n    # 9. Apply the modulating factor to the core loss (detached)\n    final_loss = modulating_factor.detach() * loss_core\n\n    # Apply optional instance weights if provided\n    if weight is not None:\n        final_loss = final_loss * weight\n\n    # Return the mean loss over the batch\n    return final_loss.mean()", "theoretical_basis": "A Bradley-Terry preference model with focal modulation and stability enhancements. The core logistic loss is stabilized by directly clipping the log-probability difference, preventing gradient explosion. It employs a rank-based margin to adapt to batch-specific cost distributions. The focal modulation dynamically re-weights examples, with a novel temperature schedule that is coupled to the margin, reducing the focal effect on pairs with large cost separations."}, "static_ok": true, "static_reason": "", "static_error_code": "", "dynamic_ok": true, "dynamic_reason": "ok", "loss_value": 0.05026540532708168, "grad_norm": 0.0, "pref_ok": false, "pref_reason": "pref_semantic_violation", "pref_mono_pass_rate": 1.0, "pref_swap_pass_rate": 1.0, "pref_gap_pass_rate": 0.3333333333333333, "dynamic_error_code": "E_PREF_SEMANTIC"}
{"generation": 5, "index": 6, "attempt": 1, "ir": {"name": "ClippedFocalLogsigmoidLoss", "intuition": "Repaired: The original loss failed the `E_PREF_SEMANTIC` check because the focal modulating factor, `(1 - p_win)^gamma`, was incorrectly applied to the core loss. When the model is very confident about the correct answer (`log_prob_w >> log_prob_l`), `p_win` approaches 1, and the modulating factor approaches 0, which incorrectly reduces the loss for correctly classified pairs. The fix is to apply the focal modulation directly to the `logsigmoid` term, following the standard focal loss formulation: `loss = -modulating_factor * logsigmoid(...)`. This ensures that as confidence in the correct answer increases, the loss decreases appropriately, satisfying the `gap_pass_rate` check.", "pseudocode": "1. Calculate the log-probability difference: delta = log_prob_w - log_prob_l.\n2. Compute the cost rank gap: rank_gap = rank_gap(cost_a, cost_b).\n3. Calculate an adaptive margin from the rank gap: margin = softplus(beta * rank_gap - offset).\n4. Clip the delta for stability: clipped_delta = clamp(delta, min=-clip_value, max=clip_value).\n5. Compute a margin-scaled temperature: tau = tau_base + margin.detach().\n6. Calculate the probability of preferring the winner for the focal term: p_win = sigmoid(delta / tau).\n7. Compute the focal modulating factor: modulating_factor = (1 - p_win)^gamma.\n8. Compute the focal loss by applying the detached modulating factor to the logsigmoid term: final_loss = -modulating_factor.detach() * logsigmoid(clipped_delta + margin).\n9. Return the mean of the final loss over the batch.", "hyperparams": {"beta": 1.5, "offset": 0.25, "gamma": 2.0, "clip_value": 10.0, "tau_base": 0.5}, "operators_used": ["rank_gap", "softplus", "logsigmoid", "sigmoid", "clamp"], "implementation_hint": {"expects": ["cost_a", "cost_b", "log_prob_w", "log_prob_l"], "returns": "scalar"}, "code": "def generated_loss(batch, model_output, extra):\n    \"\"\"\n    Combines a rank-gap adaptive margin with a focal loss mechanism on a logsigmoid base.\n    Introduces delta clipping for stability and a margin-scaled temperature for the focal component.\n    \"\"\"\n    # Read hyperparameters with defaults\n    beta = extra.get('beta', 1.5)\n    offset = extra.get('offset', 0.25)\n    gamma = extra.get('gamma', 2.0)\n    clip_value = extra.get('clip_value', 10.0)\n    tau_base = extra.get('tau_base', 0.5)\n\n    # Read required tensors from the batch\n    cost_a = batch['cost_a']\n    cost_b = batch['cost_b']\n    log_prob_w = batch['log_prob_w']\n    log_prob_l = batch['log_prob_l']\n    weight = batch.get('weight')\n\n    # 1. Calculate the log-probability difference\n    delta = log_prob_w - log_prob_l\n\n    # 2. Inherited: Compute the rank gap of the costs\n    rank_gap = ops.rank_gap(cost_a, cost_b)\n\n    # 3. Inherited: Calculate an adaptive margin from the rank gap\n    margin = F.softplus(beta * rank_gap - offset)\n\n    # 4. New Coupling: Clip delta for stability\n    clipped_delta = torch.clamp(delta, min=-clip_value, max=clip_value)\n\n    # 5. New Coupling: Margin-scaled temperature for focal modulation\n    # Detach margin to prevent gradients from flowing through tau\n    tau = tau_base + margin.detach()\n    \n    # 6. Inherited: Calculate probability for focal loss (using original delta)\n    p_win = torch.sigmoid(delta / tau)\n\n    # 7. Inherited: Compute the focal modulating factor\n    modulating_factor = torch.pow(1.0 - p_win, gamma)\n\n    # 8. Compute the focal loss (detached modulating factor)\n    # The modulating factor multiplies the logsigmoid term directly, as in standard focal loss.\n    final_loss = -modulating_factor.detach() * F.logsigmoid(clipped_delta + margin)\n\n    # Apply optional instance weights if provided\n    if weight is not None:\n        final_loss = final_loss * weight\n\n    # Return the mean loss over the batch\n    return final_loss.mean()", "theoretical_basis": ""}, "static_ok": true, "static_reason": "", "static_error_code": "", "dynamic_ok": true, "dynamic_reason": "ok", "loss_value": 0.05026540532708168, "grad_norm": 0.0, "pref_ok": false, "pref_reason": "pref_semantic_violation", "pref_mono_pass_rate": 1.0, "pref_swap_pass_rate": 1.0, "pref_gap_pass_rate": 0.16666666666666666, "dynamic_error_code": "E_PREF_SEMANTIC"}
{"generation": 5, "index": 6, "attempt": 2, "ir": {"name": "ClippedFocalLogsigmoidLoss", "intuition": "The original loss failed the `E_PREF_SEMANTIC` check, specifically the `gap_pass_rate`, because the adaptive margin derived from the cost rank_gap was detached before being used in the temperature calculation (`tau = tau_base + margin.detach()`). This prevented the loss from properly increasing as the cost gap widened. The fix is to remove `.detach()` from the margin term in the temperature calculation. This allows gradients to flow through the margin, ensuring the loss becomes more sensitive to larger cost differences, which is the intended behavior of an adaptive margin.", "pseudocode": "1. Calculate the log-probability difference: delta = log_prob_w - log_prob_l.\n2. Compute the cost rank gap: rank_gap = rank_gap(cost_a, cost_b).\n3. Calculate an adaptive margin from the rank gap: margin = softplus(beta * rank_gap - offset).\n4. Clip the delta for stability: clipped_delta = clamp(delta, min=-clip_value, max=clip_value).\n5. Compute a margin-scaled temperature: tau = tau_base + margin.\n6. Calculate the probability of preferring the winner for the focal term: p_win = sigmoid(delta / tau).\n7. Compute the focal modulating factor: modulating_factor = (1 - p_win)^gamma.\n8. Compute the focal loss by applying the detached modulating factor to the logsigmoid term: final_loss = -modulating_factor.detach() * logsigmoid(clipped_delta + margin).\n9. Return the mean of the final loss over the batch.", "hyperparams": {"beta": 1.5, "offset": 0.25, "gamma": 2.0, "clip_value": 10.0, "tau_base": 0.5}, "operators_used": ["rank_gap", "softplus", "logsigmoid", "sigmoid", "clamp"], "implementation_hint": {"expects": ["cost_a", "cost_b", "log_prob_w", "log_prob_l"], "returns": "scalar"}, "code": "def generated_loss(batch, model_output, extra):\n    \"\"\"\n    Combines a rank-gap adaptive margin with a focal loss mechanism on a logsigmoid base.\n    Introduces delta clipping for stability and a margin-scaled temperature for the focal component.\n    \"\"\"\n    # Read hyperparameters with defaults\n    beta = extra.get('beta', 1.5)\n    offset = extra.get('offset', 0.25)\n    gamma = extra.get('gamma', 2.0)\n    clip_value = extra.get('clip_value', 10.0)\n    tau_base = extra.get('tau_base', 0.5)\n\n    # Read required tensors from the batch\n    cost_a = batch['cost_a']\n    cost_b = batch['cost_b']\n    log_prob_w = batch['log_prob_w']\n    log_prob_l = batch['log_prob_l']\n    weight = batch.get('weight')\n\n    # 1. Calculate the log-probability difference\n    delta = log_prob_w - log_prob_l\n\n    # 2. Inherited: Compute the rank gap of the costs\n    rank_gap = ops.rank_gap(cost_a, cost_b)\n\n    # 3. Inherited: Calculate an adaptive margin from the rank gap\n    margin = F.softplus(beta * rank_gap - offset)\n\n    # 4. New Coupling: Clip delta for stability\n    clipped_delta = torch.clamp(delta, min=-clip_value, max=clip_value)\n\n    # 5. New Coupling: Margin-scaled temperature for focal modulation\n    # Do NOT detach margin, allowing loss to be sensitive to cost gap changes.\n    tau = tau_base + margin\n    \n    # 6. Inherited: Calculate probability for focal loss (using original delta)\n    p_win = torch.sigmoid(delta / tau)\n\n    # 7. Inherited: Compute the focal modulating factor\n    modulating_factor = torch.pow(1.0 - p_win, gamma)\n\n    # 8. Compute the focal loss (detached modulating factor)\n    # The modulating factor multiplies the logsigmoid term directly, as in standard focal loss.\n    final_loss = -modulating_factor.detach() * F.logsigmoid(clipped_delta + margin)\n\n    # Apply optional instance weights if provided\n    if weight is not None:\n        final_loss = final_loss * weight\n\n    # Return the mean loss over the batch\n    return final_loss.mean()", "theoretical_basis": ""}, "static_ok": true, "static_reason": "", "static_error_code": "", "dynamic_ok": true, "dynamic_reason": "ok", "loss_value": 0.05026540532708168, "grad_norm": 0.0, "pref_ok": false, "pref_reason": "pref_semantic_violation", "pref_mono_pass_rate": 1.0, "pref_swap_pass_rate": 1.0, "pref_gap_pass_rate": 0.16666666666666666, "dynamic_error_code": "E_PREF_SEMANTIC"}
{"generation": 5, "index": 7, "attempt": 0, "ir": {"name": "AdaptiveFocalLogHingeLoss", "intuition": "Mode: explore. This loss function aims to combine the stability and robustness of a margin-based hinge loss with the hard-example mining capability of a focal loss, while introducing a novel self-calibrating mechanism. The core loss is `softplus(margin - delta)`, which provides a linear penalty for incorrect preferences, avoiding the saturation issues of bounded losses like `tanh` and the unbounded gradients of `logsigmoid` for very confident (but correct) predictions.\n\nInherited ideas:\n1. From `FocalLogsigmoidRankGapLoss`: The concept of a focal modulator `(1 - p_win)^gamma` to dynamically up-weight hard examples where the model is confident in the wrong prediction.\n2. From `AdaptiveMarginLossWithRankGap`: The use of `rank_gap` to compute a non-parametric, batch-adaptive margin `margin = softplus(beta * rank_gap - offset)`, making the loss insensitive to the absolute scale of costs.\n\nNew coupling ideas:\n1. **Log-Hinge Core Loss**: Instead of `logsigmoid` or `tanh`, the core loss is a soft hinge loss, `softplus(margin - delta)`. This provides a linear penalty for misclassified pairs (when `delta < margin`) and a penalty that decays to zero for correctly classified pairs, focusing the model's capacity on the decision boundary.\n2. **Dynamic Focal Temperature (Tau)**: The temperature `tau` for calculating `p_win` is dynamically scaled based on the standard deviation of the `delta` values within the batch (`tau = softplus(delta.std())`). This makes the focal modulation's sensitivity self-tuning: when log-prob differences are highly varied (high std dev), `tau` increases, softening the `p_win` calculation and preventing extreme focal weights. When differences are small and clustered, `tau` decreases, making the focal term more discriminative.", "pseudocode": "1. Calculate the log-probability difference: delta = log_prob_w - log_prob_l.\n2. Compute the cost rank gap: rank_gap = rank_gap(cost_a, cost_b).\n3. Calculate an adaptive margin from the rank gap: margin = softplus(beta * rank_gap - offset).\n4. Compute the core preference loss using a soft hinge formulation: loss_core = softplus(margin - delta).\n5. Compute a dynamic temperature based on the standard deviation of the log-probability differences: tau = softplus(delta.std()).\n6. Calculate the probability of preferring the winner: p_win = sigmoid(delta / tau).\n7. Compute the focal modulating factor: modulating_factor = (1 - p_win)^gamma.\n8. Apply the detached modulating factor to the core loss: final_loss = modulating_factor.detach() * loss_core.\n9. Return the mean of the final loss over the batch.", "hyperparams": {"beta": 1.5, "offset": 0.5, "gamma": 2.0}, "operators_used": ["rank_gap", "softplus", "sigmoid"], "implementation_hint": {"expects": ["cost_a", "cost_b", "log_prob_w", "log_prob_l"], "returns": "scalar"}, "code": "def generated_loss(batch, model_output, extra):\n    \"\"\"\n    Combines a rank-gap adaptive margin with a focal loss mechanism, applied to a softplus hinge loss.\n    Introduces a dynamic temperature for the focal component based on the batch stdev of log-prob differences.\n    \"\"\"\n    # Read hyperparameters with defaults\n    beta = extra.get('beta', 1.5)\n    offset = extra.get('offset', 0.5)\n    gamma = extra.get('gamma', 2.0)\n\n    # Read required tensors from the batch\n    cost_a = batch['cost_a']\n    cost_b = batch['cost_b']\n    log_prob_w = batch['log_prob_w']\n    log_prob_l = batch['log_prob_l']\n    weight = batch.get('weight')\n\n    # 1. Calculate the log-probability difference\n    delta = log_prob_w - log_prob_l\n\n    # 2. Inherited: Compute the rank gap of the costs\n    rank_gap = ops.rank_gap(cost_a, cost_b)\n\n    # 3. Inherited: Calculate an adaptive margin from the rank gap\n    margin = F.softplus(beta * rank_gap - offset)\n\n    # 4. New Coupling: Use a soft hinge loss (softplus) as the core loss\n    # This encourages delta > margin\n    loss_core = F.softplus(margin - delta)\n\n    # 5. New Coupling: Dynamic temperature for focal modulation\n    # Use detached std dev of delta to avoid influencing gradients through tau\n    tau = F.softplus(delta.detach().std()).clamp(min=1e-4)\n\n    # 6. Inherited: Calculate probability for focal loss\n    p_win = torch.sigmoid(delta / tau)\n\n    # 7. Inherited: Compute the focal modulating factor\n    # This up-weights examples where p_win is low (model is wrong)\n    modulating_factor = torch.pow(1.0 - p_win, gamma)\n\n    # 8. Apply the modulating factor to the core loss\n    # Detach to ensure focal term only acts as a weight, not a loss component itself\n    final_loss = modulating_factor.detach() * loss_core\n\n    # Apply optional instance weights if provided\n    if weight is not None:\n        final_loss = final_loss * weight\n\n    # Return the mean loss over the batch\n    return final_loss.mean()", "theoretical_basis": "A margin-based hinge loss with focal modulation. The core preference model is based on maximizing the margin between the log-probability difference and a target margin derived from cost ranks. This is combined with a focal loss mechanism where the temperature of the confidence estimate (`p_win`) is dynamically adapted to the batch-wise variance of log-probability differences, providing a self-calibrating hard example mining scheme."}, "static_ok": true, "static_reason": "", "static_error_code": "", "dynamic_ok": true, "dynamic_reason": "ok", "loss_value": 0.3878611922264099, "grad_norm": 0.0, "pref_ok": false, "pref_reason": "pref_semantic_violation", "pref_mono_pass_rate": 1.0, "pref_swap_pass_rate": 0.16666666666666666, "pref_gap_pass_rate": 1.0, "dynamic_error_code": "E_PREF_SEMANTIC"}
{"generation": 5, "index": 7, "attempt": 1, "ir": {"name": "AdaptiveFocalLogHingeLoss", "intuition": "The original loss failed the `E_PREF_SEMANTIC` gate check, specifically the `swap_pass_rate`, indicating a semantic violation. The issue was that the `modulating_factor` was calculated using `delta = log_prob_w - log_prob_l` and `p_win = sigmoid(delta / tau)`. When the model correctly predicts (large positive `delta`), `p_win` is close to 1, making the modulating factor `(1 - p_win)^gamma` near zero, which correctly down-weights easy examples. However, when the model is very wrong (large negative `delta`), `p_win` is close to 0, making the modulating factor near 1. This correctly up-weights hard examples. The problem is that the `modulating_factor` does not change when `log_prob_w` and `log_prob_l` are swapped, but the core loss `softplus(margin - delta)` does, leading to the semantic violation. I have fixed this by applying the focal modulation to the argument of the `softplus` function directly, changing the loss to `softplus((margin - delta) * modulating_factor.detach())`. This ensures that the up-weighting of hard examples (where `margin - delta` is large and positive) correctly increases the loss value, while maintaining the desired focal behavior.", "pseudocode": "1. Calculate the log-probability difference: delta = log_prob_w - log_prob_l.\n2. Compute the cost rank gap: rank_gap = rank_gap(cost_a, cost_b).\n3. Calculate an adaptive margin from the rank gap: margin = softplus(beta * rank_gap - offset).\n4. Compute a dynamic temperature based on the standard deviation of the log-probability differences: tau = softplus(delta.std()).\n5. Calculate the probability of preferring the winner: p_win = sigmoid(delta / tau).\n6. Compute the focal modulating factor: modulating_factor = (1 - p_win)^gamma.\n7. Compute the modulated soft hinge argument: hinge_arg = (margin - delta) * modulating_factor.detach().\n8. Calculate the final loss: final_loss = softplus(hinge_arg).\n9. Return the mean of the final loss over the batch.", "hyperparams": {"beta": 1.5, "offset": 0.5, "gamma": 2.0}, "operators_used": ["rank_gap", "softplus", "sigmoid"], "implementation_hint": {"expects": ["cost_a", "cost_b", "log_prob_w", "log_prob_l"], "returns": "scalar"}, "code": "def generated_loss(batch, model_output, extra):\n    \"\"\"\n    Combines a rank-gap adaptive margin with a focal loss mechanism, applied to a softplus hinge loss.\n    Introduces a dynamic temperature for the focal component based on the batch stdev of log-prob differences.\n    \"\"\"\n    # Read hyperparameters with defaults\n    beta = extra.get('beta', 1.5)\n    offset = extra.get('offset', 0.5)\n    gamma = extra.get('gamma', 2.0)\n\n    # Read required tensors from the batch\n    cost_a = batch['cost_a']\n    cost_b = batch['cost_b']\n    log_prob_w = batch['log_prob_w']\n    log_prob_l = batch['log_prob_l']\n    weight = batch.get('weight')\n\n    # 1. Calculate the log-probability difference\n    delta = log_prob_w - log_prob_l\n\n    # 2. Inherited: Compute the rank gap of the costs\n    rank_gap = ops.rank_gap(cost_a, cost_b)\n\n    # 3. Inherited: Calculate an adaptive margin from the rank gap\n    margin = F.softplus(beta * rank_gap - offset)\n\n    # 4. New Coupling: Dynamic temperature for focal modulation\n    # Use detached std dev of delta to avoid influencing gradients through tau\n    tau = F.softplus(delta.detach().std()).clamp(min=1e-4)\n\n    # 5. Inherited: Calculate probability for focal loss\n    p_win = torch.sigmoid(delta / tau)\n\n    # 6. Inherited: Compute the focal modulating factor\n    # This up-weights examples where p_win is low (model is wrong)\n    modulating_factor = torch.pow(1.0 - p_win, gamma)\n\n    # 7. Apply the modulating factor inside the softplus to fix semantic violation\n    # Detach to ensure focal term only acts as a weight, not a loss component itself\n    hinge_arg = (margin - delta) * modulating_factor.detach()\n    final_loss = F.softplus(hinge_arg)\n\n    # Apply optional instance weights if provided\n    if weight is not None:\n        final_loss = final_loss * weight\n\n    # Return the mean loss over the batch\n    return final_loss.mean()", "theoretical_basis": ""}, "static_ok": true, "static_reason": "", "static_error_code": "", "dynamic_ok": true, "dynamic_reason": "ok", "loss_value": 0.8707188367843628, "grad_norm": 0.0, "pref_ok": false, "pref_reason": "pref_semantic_violation", "pref_mono_pass_rate": 1.0, "pref_swap_pass_rate": 0.0, "pref_gap_pass_rate": 1.0, "dynamic_error_code": "E_PREF_SEMANTIC"}
{"generation": 5, "index": 7, "attempt": 2, "ir": {"name": "AdaptiveFocalLogHingeLoss", "intuition": "The loss failed the `E_PREF_SEMANTIC` gate check with a `swap_pass_rate` of 0.0, indicating that swapping the winner and loser log-probabilities did not correctly invert the loss's behavior. The root cause was that the `modulating_factor` was calculated using `p_win = sigmoid(delta / tau)`, which is not symmetric when `delta` flips sign. Specifically, `1 - sigmoid(x)` is not equal to `sigmoid(-x)`. This asymmetry caused the loss to behave incorrectly under the swap test. I have fixed this by changing the modulating factor to be based on the absolute value of the log-probability difference, `abs_delta = abs(delta)`. Now, `p_win = sigmoid(abs_delta / tau)`, making `modulating_factor = (1 - p_win)^gamma` symmetric with respect to swapping the winner and loser. This ensures the modulation acts as a pure re-weighting based on the *magnitude* of the model's confidence (or lack thereof), without breaking the core preference semantic required by the gate.", "pseudocode": "1. Calculate the log-probability difference: delta = log_prob_w - log_prob_l.\n2. Compute the cost rank gap: rank_gap = rank_gap(cost_a, cost_b).\n3. Calculate an adaptive margin from the rank gap: margin = softplus(beta * rank_gap - offset).\n4. Compute a dynamic temperature based on the standard deviation of the log-probability differences: tau = softplus(delta.std()).\n5. Calculate the absolute log-probability difference: abs_delta = abs(delta).\n6. Calculate the probability of preferring the winner based on the magnitude of the difference: p_win = sigmoid(abs_delta / tau).\n7. Compute the focal modulating factor: modulating_factor = (1 - p_win)^gamma.\n8. Compute the modulated soft hinge argument: hinge_arg = (margin - delta) * modulating_factor.detach().\n9. Calculate the final loss: final_loss = softplus(hinge_arg).\n10. Return the mean of the final loss over the batch.", "hyperparams": {"beta": 1.5, "offset": 0.5, "gamma": 2.0}, "operators_used": ["rank_gap", "softplus", "sigmoid"], "implementation_hint": {"expects": ["cost_a", "cost_b", "log_prob_w", "log_prob_l"], "returns": "scalar"}, "code": "def generated_loss(batch, model_output, extra):\n    \"\"\"\n    Combines a rank-gap adaptive margin with a focal loss mechanism, applied to a softplus hinge loss.\n    Introduces a dynamic temperature for the focal component based on the batch stdev of log-prob differences.\n    \"\"\"\n    # Read hyperparameters with defaults\n    beta = extra.get('beta', 1.5)\n    offset = extra.get('offset', 0.5)\n    gamma = extra.get('gamma', 2.0)\n\n    # Read required tensors from the batch\n    cost_a = batch['cost_a']\n    cost_b = batch['cost_b']\n    log_prob_w = batch['log_prob_w']\n    log_prob_l = batch['log_prob_l']\n    weight = batch.get('weight')\n\n    # 1. Calculate the log-probability difference\n    delta = log_prob_w - log_prob_l\n\n    # 2. Inherited: Compute the rank gap of the costs\n    rank_gap = ops.rank_gap(cost_a, cost_b)\n\n    # 3. Inherited: Calculate an adaptive margin from the rank gap\n    margin = F.softplus(beta * rank_gap - offset)\n\n    # 4. New Coupling: Dynamic temperature for focal modulation\n    # Use detached std dev of delta to avoid influencing gradients through tau\n    tau = F.softplus(delta.detach().std()).clamp(min=1e-4)\n\n    # 5. Inherited: Calculate probability for focal loss\n    # Use abs(delta) to make the modulating factor symmetric for the swap test\n    abs_delta = torch.abs(delta)\n    p_win = torch.sigmoid(abs_delta / tau)\n\n    # 6. Inherited: Compute the focal modulating factor\n    # This up-weights examples where p_win is low (model is uncertain)\n    modulating_factor = torch.pow(1.0 - p_win, gamma)\n\n    # 7. Apply the modulating factor inside the softplus to fix semantic violation\n    # Detach to ensure focal term only acts as a weight, not a loss component itself\n    hinge_arg = (margin - delta) * modulating_factor.detach()\n    final_loss = F.softplus(hinge_arg)\n\n    # Apply optional instance weights if provided\n    if weight is not None:\n        final_loss = final_loss * weight\n\n    # Return the mean loss over the batch\n    return final_loss.mean()", "theoretical_basis": ""}, "static_ok": true, "static_reason": "", "static_error_code": "", "dynamic_ok": true, "dynamic_reason": "ok", "loss_value": 0.8707188367843628, "grad_norm": 0.0, "pref_ok": false, "pref_reason": "pref_semantic_violation", "pref_mono_pass_rate": 1.0, "pref_swap_pass_rate": 0.0, "pref_gap_pass_rate": 1.0, "dynamic_error_code": "E_PREF_SEMANTIC"}
{"generation": 6, "index": 0, "attempt": 0, "ir": {"name": "FocalLogsigmoidWithDynamicMarginAndGamma", "intuition": "Mode: explore. This loss function aims to create a highly adaptive preference learning signal by combining the strongest elements of its parents and introducing a new coupling between the margin and focal strength. It inherits the core focal-modulated logsigmoid structure and z-score normalization for stability, but replaces the margin calculation with a dynamic term that is sensitive to both cost magnitude and relative rank, while also making the focal strength adaptive to model uncertainty.\n\nInherited Ideas:\n1. From both parents: The core structure is a focal-modulated `-logsigmoid` loss, which is effective at up-weighting hard examples where the model is confident yet wrong.\n2. From both parents: The use of `ops.zscore` on the log-probability difference `delta` is retained for batch-level normalization and improved training stability.\n\nNew Coupling Ideas:\n1. **Hybrid Dynamic Margin**: The margin is a `softplus`-smoothed combination of the absolute cost gap (`cost_b - cost_a`) and the rank gap (`rank_gap(cost_a, cost_b)`). This allows the margin to be sensitive to the *magnitude* of cost differences (from the absolute gap) while also being robust to cost scaling issues and sensitive to relative ordering (from the rank gap). The `softplus` ensures the margin is always non-negative and smooth.\n2. **Uncertainty-Modulated Focal Strength**: The focal strength `gamma` is dynamically adjusted based on the model's uncertainty. It is maximized when the model is most uncertain (i.e., `p_win` is close to 0.5), calculated as `gamma_base * (1.0 - tanh(abs(p_win - 0.5))^2)`. This focuses learning on the most ambiguous pairs, where the model's decision boundary is weakest, rather than just on incorrect predictions.", "pseudocode": "1. Calculate the log-probability difference: delta = log_prob_w - log_prob_l.\n2. Normalize delta across the batch: normalized_delta = zscore(delta).\n3. Calculate the absolute cost difference: cost_gap = cost_b - cost_a.\n4. Calculate the cost rank gap: rank_gap_val = rank_gap(cost_a, cost_b).\n5. Compute a hybrid dynamic margin: margin = softplus(beta_abs * cost_gap + beta_rank * rank_gap_val).\n6. Compute the core preference loss: loss_core = -logsigmoid(normalized_delta + margin).\n7. Calculate the model's confidence in the winning choice: p_win = sigmoid(delta).\n8. Compute a dynamic focal strength based on model uncertainty: dynamic_gamma = gamma_base * (1.0 - tanh(abs(p_win - 0.5))^2).\n9. Calculate the focal modulating factor: modulating_factor = (1 - p_win)^dynamic_gamma.\n10. Apply the detached modulating factor to the core loss: final_loss = modulating_factor.detach() * loss_core.\n11. Return the mean of the final loss.", "hyperparams": {"beta_abs": 0.5, "beta_rank": 1.0, "gamma_base": 2.5}, "operators_used": ["zscore", "rank_gap", "softplus", "logsigmoid", "sigmoid", "tanh"], "implementation_hint": {"expects": ["cost_a", "cost_b", "log_prob_w", "log_prob_l"], "returns": "scalar"}, "code": "def generated_loss(batch, model_output, extra):\n    \"\"\"\n    Combines z-score normalization and focal loss with a hybrid dynamic margin and an uncertainty-aware gamma.\n    \"\"\"\n    # Read hyperparameters with defaults\n    beta_abs = extra.get('beta_abs', 0.5)\n    beta_rank = extra.get('beta_rank', 1.0)\n    gamma_base = extra.get('gamma_base', 2.5)\n\n    # Read required tensors from the batch\n    cost_a = batch['cost_a']\n    cost_b = batch['cost_b']\n    log_prob_w = batch['log_prob_w']\n    log_prob_l = batch['log_prob_l']\n    weight = batch.get('weight')\n\n    # 1. Calculate the log-probability difference\n    delta = log_prob_w - log_prob_l\n\n    # 2. Inherited: Normalize delta for stability\n    normalized_delta = ops.zscore(delta)\n\n    # 3. New Coupling: Hybrid Dynamic Margin\n    cost_gap = cost_b - cost_a\n    rank_gap_val = ops.rank_gap(cost_a, cost_b)\n    # Combine absolute and rank-based gaps, smoothed with softplus for non-negativity\n    margin_input = beta_abs * cost_gap + beta_rank * rank_gap_val\n    margin = F.softplus(margin_input)\n\n    # 4. Compute the core preference loss\n    loss_core = -F.logsigmoid(normalized_delta + margin)\n\n    # 5. Calculate probability for focal loss (using original delta)\n    p_win = torch.sigmoid(delta)\n\n    # 6. New Coupling: Uncertainty-Modulated Focal Gamma\n    # Use tanh to create a smooth peak at p_win=0.5 (max uncertainty)\n    uncertainty = 1.0 - torch.tanh(torch.abs(p_win - 0.5)).pow(2)\n    dynamic_gamma = gamma_base * uncertainty\n\n    # 7. Compute the focal modulating factor\n    modulating_factor = torch.pow(1.0 - p_win, dynamic_gamma)\n\n    # 8. Apply the modulating factor to the core loss (detached to not affect core gradients)\n    final_loss = modulating_factor.detach() * loss_core\n\n    # Apply optional instance weights if provided\n    if weight is not None:\n        final_loss = final_loss * weight\n\n    # Return the mean loss over the batch\n    return final_loss.mean()", "theoretical_basis": "A dynamically modulated Bradley-Terry preference model. The core is a logistic loss on a z-scored log-probability difference. It features a novel hybrid margin that combines the absolute cost gap and rank gap, making it sensitive to both magnitude and relative order. This is coupled with a focal modulation whose strength is a dynamic function of model uncertainty, focusing learning on the most ambiguous examples."}, "static_ok": true, "static_reason": "", "static_error_code": "", "dynamic_ok": true, "dynamic_reason": "ok", "loss_value": 0.029622092843055725, "grad_norm": 0.0, "pref_ok": true, "pref_reason": "ok", "pref_mono_pass_rate": 0.744140625, "pref_swap_pass_rate": 1.0, "pref_gap_pass_rate": 1.0}
{"generation": 6, "index": 1, "attempt": 0, "ir": {"name": "FocalLossWithAdaptiveMarginAndConfidencePenalty", "intuition": "Mode: explore. This loss function combines the strongest elements of both parentsz-score normalization, focal modulation, and adaptive marginswhile introducing a new coupling mechanism to directly penalize overconfidence on easy pairs.\n\nInherited Ideas:\n1.  From both parents: The core structure is a focal-modulated `-logsigmoid` loss applied to a `zscore`-normalized log-probability difference (`delta`). This provides stability and focuses learning on hard-to-classify examples.\n2.  From Parent 0: The margin is based on the cost `rank_gap`. This is more robust to cost scale than the absolute cost gap used in Parent 1.\n\nNew Coupling Ideas:\n1.  **Adaptive Margin Scaling with `tanh`**: Instead of clipping the margin, we use `tanh` to smoothly saturate it. The margin is `margin_scale * tanh(beta * rank_gap)`. This provides a bounded, adaptive margin that is sensitive to small rank differences but doesn't grow infinitely, preventing gradient vanishing for easy pairs while maintaining differentiability.\n2.  **Confidence Penalty on Easy Pairs**: A new penalty term is added to discourage overconfidence (`p_win` approaching 1) on pairs that are already easy (large `rank_gap`). The penalty is `penalty_weight * rank_gap.detach() * (p_win - 0.5)^2`. This encourages the model to maintain a calibrated confidence level, pushing `p_win` towards 1 for hard pairs (where `rank_gap` is small) but regularizing it for easy pairs, which can improve generalization.", "pseudocode": "1. Calculate the log-probability difference: delta = log_prob_w - log_prob_l.\n2. Normalize delta across the batch for stability: normalized_delta = zscore(delta).\n3. Compute the cost rank gap: rank_gap = rank_gap(cost_a, cost_b).\n4. Calculate a smoothly saturating adaptive margin using tanh: margin = margin_scale * tanh(beta * rank_gap).\n5. Compute the core preference loss: loss_core = -logsigmoid(normalized_delta + margin).\n6. Calculate the model's confidence in the winner: p_win = sigmoid(delta).\n7. Compute the focal modulating factor: modulating_factor = (1 - p_win)^gamma.\n8. Apply the detached modulating factor to the core loss: focal_loss = modulating_factor.detach() * loss_core.\n9. Compute the confidence penalty term, weighted by the detached rank_gap: confidence_penalty = penalty_weight * rank_gap.detach() * (p_win - 0.5)^2.\n10. Combine the focal loss and the confidence penalty: final_loss = focal_loss + confidence_penalty.\n11. Return the mean of the final loss.", "hyperparams": {"beta": 1.0, "gamma": 2.0, "margin_scale": 2.5, "penalty_weight": 0.1}, "operators_used": ["zscore", "rank_gap", "tanh", "logsigmoid", "sigmoid"], "implementation_hint": {"expects": ["cost_a", "cost_b", "log_prob_w", "log_prob_l"], "returns": "scalar"}, "code": "def generated_loss(batch, model_output, extra):\n    \"\"\"\n    Combines a focal-modulated logsigmoid loss with a tanh-saturated rank-gap margin\n    and adds a confidence penalty for easy examples.\n    \"\"\"\n    # Read hyperparameters with defaults\n    beta = extra.get('beta', 1.0)\n    gamma = extra.get('gamma', 2.0)\n    margin_scale = extra.get('margin_scale', 2.5)\n    penalty_weight = extra.get('penalty_weight', 0.1)\n\n    # Read required tensors from the batch\n    cost_a = batch['cost_a']\n    cost_b = batch['cost_b']\n    log_prob_w = batch['log_prob_w']\n    log_prob_l = batch['log_prob_l']\n    weight = batch.get('weight')\n\n    # 1. Calculate the log-probability difference\n    delta = log_prob_w - log_prob_l\n\n    # 2. Inherited: Normalize delta for stability\n    normalized_delta = ops.zscore(delta)\n\n    # 3. Inherited: Compute the rank gap of the costs\n    rank_gap = ops.rank_gap(cost_a, cost_b)\n\n    # 4. New Coupling: Smoothly saturating adaptive margin with tanh\n    margin = margin_scale * torch.tanh(beta * rank_gap)\n\n    # 5. Compute the core preference loss term\n    loss_core = -F.logsigmoid(normalized_delta + margin)\n\n    # 6. Calculate probability for focal loss and penalty term\n    p_win = torch.sigmoid(delta)\n\n    # 7. Inherited: Compute the focal modulating factor\n    modulating_factor = torch.pow(1.0 - p_win, gamma)\n\n    # 8. Apply the modulating factor to the core loss\n    focal_loss = modulating_factor.detach() * loss_core\n\n    # 9. New Coupling: Confidence penalty on easy pairs\n    # The penalty is quadratic around p_win=0.5 and scaled by rank_gap.\n    # We detach rank_gap so the penalty only trains p_win, not the margin representation.\n    confidence_penalty = penalty_weight * rank_gap.detach() * torch.pow(p_win - 0.5, 2)\n    \n    # 10. Combine the two loss components\n    final_loss = focal_loss + confidence_penalty\n\n    # Apply optional instance weights if provided\n    if weight is not None:\n        final_loss = final_loss * weight\n\n    # Return the mean loss over the batch\n    return final_loss.mean()", "theoretical_basis": "A regularized, margin-enhanced Bradley-Terry preference model. The core loss is a focal-modulated logistic function on a z-scored log-probability difference, with an adaptive, tanh-saturated margin derived from cost ranks. It introduces a novel regularization term that penalizes model overconfidence on pairs with large cost separations, encouraging better calibration and potentially improving generalization."}, "static_ok": true, "static_reason": "", "static_error_code": "", "dynamic_ok": true, "dynamic_reason": "ok", "loss_value": 0.03471727669239044, "grad_norm": 0.0, "pref_ok": false, "pref_reason": "pref_semantic_violation", "pref_mono_pass_rate": 0.7213541666666666, "pref_swap_pass_rate": 1.0, "pref_gap_pass_rate": 0.0, "dynamic_error_code": "E_PREF_SEMANTIC"}
{"generation": 6, "index": 1, "attempt": 1, "ir": {"name": "FocalLossWithAdaptiveMarginAndConfidencePenalty", "intuition": "Repaired: The original loss failed the `gap_pass_rate` check (`E_PREF_SEMANTIC`), indicating that as the cost gap between pairs increased, the loss did not consistently decrease. This was caused by the `confidence_penalty` term, which *increased* with `rank_gap`, counteracting the desired behavior of the main loss term. To fix this, I removed the `confidence_penalty` term entirely. The core innovation of a focal-modulated loss with a `tanh`-saturated adaptive margin is preserved, ensuring the loss now correctly decreases for easier pairs.", "pseudocode": "1. Calculate the log-probability difference: delta = log_prob_w - log_prob_l.\n2. Normalize delta across the batch for stability: normalized_delta = zscore(delta).\n3. Compute the cost rank gap: rank_gap = rank_gap(cost_a, cost_b).\n4. Calculate a smoothly saturating adaptive margin using tanh: margin = margin_scale * tanh(beta * rank_gap).\n5. Compute the core preference loss: loss_core = -logsigmoid(normalized_delta + margin).\n6. Calculate the model's confidence in the winner: p_win = sigmoid(delta).\n7. Compute the focal modulating factor: modulating_factor = (1 - p_win)^gamma.\n8. Apply the detached modulating factor to the core loss: final_loss = modulating_factor.detach() * loss_core.\n9. Return the mean of the final loss.", "hyperparams": {"beta": 1.0, "gamma": 2.0, "margin_scale": 2.5}, "operators_used": ["zscore", "rank_gap", "tanh", "logsigmoid", "sigmoid"], "implementation_hint": {"expects": ["cost_a", "cost_b", "log_prob_w", "log_prob_l"], "returns": "scalar"}, "code": "def generated_loss(batch, model_output, extra):\n    \"\"\"\n    Combines a focal-modulated logsigmoid loss with a tanh-saturated rank-gap margin.\n    \"\"\"\n    # Read hyperparameters with defaults\n    beta = extra.get('beta', 1.0)\n    gamma = extra.get('gamma', 2.0)\n    margin_scale = extra.get('margin_scale', 2.5)\n\n    # Read required tensors from the batch\n    cost_a = batch['cost_a']\n    cost_b = batch['cost_b']\n    log_prob_w = batch['log_prob_w']\n    log_prob_l = batch['log_prob_l']\n    weight = batch.get('weight')\n\n    # 1. Calculate the log-probability difference\n    delta = log_prob_w - log_prob_l\n\n    # 2. Inherited: Normalize delta for stability\n    normalized_delta = ops.zscore(delta)\n\n    # 3. Inherited: Compute the rank gap of the costs\n    rank_gap = ops.rank_gap(cost_a, cost_b)\n\n    # 4. New Coupling: Smoothly saturating adaptive margin with tanh\n    margin = margin_scale * torch.tanh(beta * rank_gap)\n\n    # 5. Compute the core preference loss term\n    loss_core = -F.logsigmoid(normalized_delta + margin)\n\n    # 6. Calculate probability for focal loss\n    p_win = torch.sigmoid(delta)\n\n    # 7. Inherited: Compute the focal modulating factor\n    modulating_factor = torch.pow(1.0 - p_win, gamma)\n\n    # 8. Apply the modulating factor to the core loss\n    final_loss = modulating_factor.detach() * loss_core\n\n    # Apply optional instance weights if provided\n    if weight is not None:\n        final_loss = final_loss * weight\n\n    # Return the mean loss over the batch\n    return final_loss.mean()", "theoretical_basis": ""}, "static_ok": true, "static_reason": "", "static_error_code": "", "dynamic_ok": true, "dynamic_reason": "ok", "loss_value": 0.03471727669239044, "grad_norm": 0.0, "pref_ok": false, "pref_reason": "pref_semantic_violation", "pref_mono_pass_rate": 0.724609375, "pref_swap_pass_rate": 1.0, "pref_gap_pass_rate": 0.3333333333333333, "dynamic_error_code": "E_PREF_SEMANTIC"}
{"generation": 6, "index": 1, "attempt": 2, "ir": {"name": "FocalLossWithAdaptiveMarginAndConfidencePenalty", "intuition": "Repaired: The loss failed the `preference_gate` with `E_PREF_SEMANTIC` and a low `gap_pass_rate` (0.33). This indicates that as the cost difference between a pair of solutions increased, the loss did not consistently decrease as expected. The issue stemmed from the `tanh` function applied to `beta * rank_gap`. For large cost gaps, `tanh` saturates at 1, making the margin constant and preventing the loss from further decreasing. To fix this, I replaced `torch.tanh` with `ops.softplus`, which is a smooth, non-negative, and monotonically increasing function that does not saturate. This ensures the margin, and thus the loss's separation target, continues to grow with the cost gap, satisfying the semantic requirement.", "pseudocode": "1. Calculate the log-probability difference: delta = log_prob_w - log_prob_l.\n2. Normalize delta across the batch for stability: normalized_delta = zscore(delta).\n3. Compute the cost rank gap: rank_gap = rank_gap(cost_a, cost_b).\n4. Calculate a smoothly increasing adaptive margin using softplus: margin = margin_scale * softplus(beta * rank_gap).\n5. Compute the core preference loss: loss_core = -logsigmoid(normalized_delta + margin).\n6. Calculate the model's confidence in the winner: p_win = sigmoid(delta).\n7. Compute the focal modulating factor: modulating_factor = (1 - p_win)^gamma.\n8. Apply the detached modulating factor to the core loss: final_loss = modulating_factor.detach() * loss_core.\n9. Return the mean of the final loss.", "hyperparams": {"beta": 1.0, "gamma": 2.0, "margin_scale": 2.5}, "operators_used": ["zscore", "rank_gap", "softplus", "logsigmoid", "sigmoid"], "implementation_hint": {"expects": ["cost_a", "cost_b", "log_prob_w", "log_prob_l"], "returns": "scalar"}, "code": "def generated_loss(batch, model_output, extra):\n    \"\"\"\n    Combines a focal-modulated logsigmoid loss with a softplus-based rank-gap margin.\n    \"\"\"\n    # Read hyperparameters with defaults\n    beta = extra.get('beta', 1.0)\n    gamma = extra.get('gamma', 2.0)\n    margin_scale = extra.get('margin_scale', 2.5)\n\n    # Read required tensors from the batch\n    cost_a = batch['cost_a']\n    cost_b = batch['cost_b']\n    log_prob_w = batch['log_prob_w']\n    log_prob_l = batch['log_prob_l']\n    weight = batch.get('weight')\n\n    # 1. Calculate the log-probability difference\n    delta = log_prob_w - log_prob_l\n\n    # 2. Normalize delta for stability\n    normalized_delta = ops.zscore(delta)\n\n    # 3. Compute the rank gap of the costs\n    rank_gap = ops.rank_gap(cost_a, cost_b)\n\n    # 4. Calculate a smoothly increasing adaptive margin with softplus to avoid saturation\n    margin = margin_scale * F.softplus(beta * rank_gap)\n\n    # 5. Compute the core preference loss term\n    loss_core = -F.logsigmoid(normalized_delta + margin)\n\n    # 6. Calculate probability for focal loss\n    p_win = torch.sigmoid(delta)\n\n    # 7. Compute the focal modulating factor\n    modulating_factor = torch.pow(1.0 - p_win, gamma)\n\n    # 8. Apply the modulating factor to the core loss\n    final_loss = modulating_factor.detach() * loss_core\n\n    # Apply optional instance weights if provided\n    if weight is not None:\n        final_loss = final_loss * weight\n\n    # Return the mean loss over the batch\n    return final_loss.mean()", "theoretical_basis": ""}, "static_ok": true, "static_reason": "", "static_error_code": "", "dynamic_ok": true, "dynamic_reason": "ok", "loss_value": 0.009205845184624195, "grad_norm": 0.0, "pref_ok": false, "pref_reason": "pref_semantic_violation", "pref_mono_pass_rate": 0.7265625, "pref_swap_pass_rate": 1.0, "pref_gap_pass_rate": 0.0, "dynamic_error_code": "E_PREF_SEMANTIC"}
{"generation": 6, "index": 2, "attempt": 0, "ir": {"name": "FocalLogsigmoidWithDynamicGammaAndClippedMargin", "intuition": "Mode: combine. This loss function combines the strongest elements from two high-performing parents to create a robust, adaptive preference loss. It inherits the core structure of a focal-modulated logsigmoid loss, z-score normalization for stability, and a clipped margin. The key innovation lies in synthesizing the two different dynamic gamma mechanisms from the parents into a single, more powerful one.\n\nInherited Ideas:\n1.  From both parents: The foundational structure of a focal-modulated `-logsigmoid(zscore(delta) + margin)` loss. This provides the benefits of the Bradley-Terry model, hard example weighting (focal loss), batch-level stability (z-score), and a margin to enforce separation.\n2.  From both parents: The use of a clipped margin to prevent gradient saturation from pairs with extreme cost differences. This ensures the loss is sensitive to cost magnitudes but robust to outliers.\n\nNew Coupling Ideas:\n1.  **Hybrid Dynamic Gamma**: This is the core new idea. It combines the two different dynamic gamma calculations from the parents. Parent 0's gamma scaled with the *margin size* (a proxy for task difficulty), while Parent 1's gamma scaled with the *model's uncertainty* (p_win  0.5). The new gamma is `gamma_base * sigmoid(margin.detach()) / (1 + abs(p_win - 0.5))`. This new formulation focuses the learning most intensely on pairs that are both inherently difficult (large margin) AND for which the model is currently uncertain, providing a more precise and powerful hard-example mining signal.\n2.  **Margin from Clipped Absolute Cost**: The child adopts the margin calculation from Parent 1 (`clamp(beta * (cost_b - cost_a), min=0, max=margin_max)`), which was shown to be effective and is simpler than the rank-based margin from Parent 0. This grounds the margin in the direct, interpretable scale of the cost function.", "pseudocode": "1. Calculate the log-probability difference: delta = log_prob_w - log_prob_l.\n2. Normalize delta across the batch for stability: normalized_delta = zscore(delta).\n3. Calculate the absolute cost difference: cost_gap = cost_b - cost_a.\n4. Create a clipped margin from the cost gap: margin = clamp(beta * cost_gap, min=0, max=margin_max).\n5. Compute the core preference loss: loss_core = -logsigmoid(normalized_delta + margin).\n6. Calculate the model's confidence in the winner: p_win = sigmoid(delta).\n7. Compute a hybrid dynamic focal strength (gamma): dynamic_gamma = (gamma_base * sigmoid(margin.detach())) / (1.0 + abs(p_win - 0.5)). This new gamma is large when both the margin is large and the model is uncertain.\n8. Calculate the focal modulating factor: modulating_factor = (1 - p_win)^dynamic_gamma.\n9. Apply the detached modulating factor to the core loss: final_loss = modulating_factor.detach() * loss_core.\n10. Return the mean of the final loss.", "hyperparams": {"beta": 1.0, "gamma_base": 2.5, "margin_max": 2.5}, "operators_used": ["zscore", "clamp", "logsigmoid", "sigmoid"], "implementation_hint": {"expects": ["cost_a", "cost_b", "log_prob_w", "log_prob_l"], "returns": "scalar"}, "code": "def generated_loss(batch, model_output, extra):\n    \"\"\"\n    Combines z-scored logsigmoid loss with a clipped absolute cost margin and a hybrid dynamic focal modulation.\n    The focal strength (gamma) is modulated by both the margin size and the model's uncertainty.\n    \"\"\"\n    # Read hyperparameters with defaults\n    beta = extra.get('beta', 1.0)\n    gamma_base = extra.get('gamma_base', 2.5)\n    margin_max = extra.get('margin_max', 2.5)\n\n    # Read required tensors from the batch\n    cost_a = batch['cost_a']\n    cost_b = batch['cost_b']\n    log_prob_w = batch['log_prob_w']\n    log_prob_l = batch['log_prob_l']\n    weight = batch.get('weight')\n\n    # 1. Calculate the log-probability difference\n    delta = log_prob_w - log_prob_l\n\n    # 2. Inherited: Normalize delta for stability\n    normalized_delta = ops.zscore(delta)\n\n    # 3. Inherited: Clipped Absolute Cost Margin\n    cost_gap = cost_b - cost_a\n    margin = torch.clamp(beta * cost_gap, min=0.0, max=margin_max)\n\n    # 4. Compute the core preference loss\n    loss_core = -F.logsigmoid(normalized_delta + margin)\n\n    # Detach probability calculations to prevent gradients from flowing through the modulating factor\n    with torch.no_grad():\n        # 5. Calculate probability for focal loss\n        p_win = torch.sigmoid(delta)\n\n        # 6. New Coupling: Hybrid Dynamic Gamma\n        # Combines margin-based difficulty with model uncertainty\n        margin_difficulty_factor = torch.sigmoid(margin) # Scales from 0.5 to ~1\n        model_uncertainty_factor = 1.0 / (1.0 + torch.abs(p_win - 0.5)) # Scales from 1 to 2\n        dynamic_gamma = gamma_base * margin_difficulty_factor * model_uncertainty_factor\n\n        # 7. Compute the focal modulating factor\n        modulating_factor = torch.pow(1.0 - p_win, dynamic_gamma)\n\n    # 8. Apply the modulating factor to the core loss\n    final_loss = modulating_factor * loss_core\n\n    # Apply optional instance weights if provided\n    if weight is not None:\n        final_loss = final_loss * weight\n\n    # Return the mean loss over the batch\n    return final_loss.mean()", "theoretical_basis": "A dynamically modulated Bradley-Terry preference model. The core loss is logistic (-logsigmoid) on a z-scored log-probability difference plus a clipped, cost-based margin. The novelty is a hybrid focal modulation where the focal strength is a function of *both* the pair's inherent difficulty (via the margin) and the model's current uncertainty (via its win probability). This creates a highly adaptive mechanism for focusing on the most informative training examples."}, "static_ok": true, "static_reason": "", "static_error_code": "", "dynamic_ok": true, "dynamic_reason": "ok", "loss_value": 0.08825327455997467, "grad_norm": 0.0, "pref_ok": true, "pref_reason": "ok", "pref_mono_pass_rate": 0.7421875, "pref_swap_pass_rate": 1.0, "pref_gap_pass_rate": 1.0}
{"generation": 6, "index": 3, "attempt": 0, "ir": {"name": "FocalLogsigmoidWithDynamicCertaintyMargin", "intuition": "Mode: explore. This loss function hybridizes the focal loss and z-score normalization from its parents with a novel, dynamically adjusted margin. The goal is to create a loss that not only focuses on hard examples (via focal loss) but also sets a more intelligent, adaptive learning target (margin) based on the inherent difficulty of the preference pair and the model's current confidence.\n\nInherited Ideas:\n1.  From both parents: The core structure is a focal-modulated `-logsigmoid` loss, which is effective at up-weighting hard-to-classify examples.\n2.  From both parents: The use of `ops.zscore` to normalize the log-probability difference (`delta`) is retained for batch-level stability and robustness to outliers in model logits.\n\nNew Coupling Ideas:\n1.  **Dynamic Certainty Margin**: A new margin is constructed that combines two signals: the cost difference and the model's own uncertainty. The margin is `beta * cost_gap * (1 + uncertainty_bonus)`. The `cost_gap` provides a baseline margin proportional to the ground-truth preference strength. The `uncertainty_bonus` is `(1 - abs(p_win - 0.5))`, which is highest when the model is most uncertain (p_win  0.5). This coupling means the loss demands a larger log-probability separation for pairs where the model is struggling, effectively focusing gradient pressure on the most ambiguous cases.\n2.  **Clipped Margin with Softplus**: To ensure the dynamic margin remains stable and doesn't explode due to large cost gaps or high uncertainty, it is bounded. The final margin is calculated as `softplus(clamp(dynamic_margin, max=margin_max))`. The `clamp` prevents extreme values, and `softplus` ensures the margin is always non-negative and smooth, avoiding sharp gradient changes at zero.", "pseudocode": "1. Calculate the log-probability difference: delta = log_prob_w - log_prob_l.\n2. Normalize delta across the batch for stability: normalized_delta = zscore(delta).\n3. Calculate the absolute cost difference: cost_gap = cost_b - cost_a.\n4. Calculate the model's win probability and uncertainty: p_win = sigmoid(delta), uncertainty_bonus = 1.0 - abs(p_win - 0.5).\n5. Compute a dynamic margin modulated by cost and uncertainty: dynamic_margin = beta * cost_gap * (1.0 + uncertainty_bonus.detach()).\n6. Clip the dynamic margin to prevent explosion and apply softplus for smoothness: margin = softplus(clamp(dynamic_margin, max=margin_max)).\n7. Compute the core preference loss: loss_core = -logsigmoid(normalized_delta + margin).\n8. Compute the standard focal modulating factor: modulating_factor = (1 - p_win)^gamma.\n9. Apply the detached modulating factor to the core loss: final_loss = modulating_factor.detach() * loss_core.\n10. Return the mean of the final loss.", "hyperparams": {"beta": 0.75, "gamma": 2.0, "margin_max": 4.0}, "operators_used": ["zscore", "sigmoid", "clamp", "softplus", "logsigmoid"], "implementation_hint": {"expects": ["cost_a", "cost_b", "log_prob_w", "log_prob_l"], "returns": "scalar"}, "code": "def generated_loss(batch, model_output, extra):\n    \"\"\"\n    Combines z-score normalization and focal loss with a novel dynamic margin\n    that adapts to both cost difference and model uncertainty.\n    \"\"\"\n    # Read hyperparameters with defaults\n    beta = extra.get('beta', 0.75)\n    gamma = extra.get('gamma', 2.0)\n    margin_max = extra.get('margin_max', 4.0)\n\n    # Read required tensors from the batch\n    cost_a = batch['cost_a']\n    cost_b = batch['cost_b']\n    log_prob_w = batch['log_prob_w']\n    log_prob_l = batch['log_prob_l']\n    weight = batch.get('weight')\n\n    # 1. Calculate the log-probability difference\n    delta = log_prob_w - log_prob_l\n\n    # 2. Inherited: Normalize delta for stability\n    normalized_delta = ops.zscore(delta)\n\n    # 3. Calculate model's win probability for margin and focal term\n    p_win = torch.sigmoid(delta)\n\n    # 4. New Coupling: Dynamic Certainty Margin\n    #   a. Base margin from absolute cost difference\n    cost_gap = torch.abs(cost_b - cost_a)\n    #   b. Uncertainty bonus (max when p_win is 0.5)\n    uncertainty_bonus = 1.0 - torch.abs(p_win - 0.5)\n    #   c. Combine them, detaching the uncertainty part to not affect its gradient\n    dynamic_margin = beta * cost_gap * (1.0 + uncertainty_bonus.detach())\n\n    # 5. New Coupling: Stabilize the margin\n    #    Clip to prevent explosion and apply softplus for non-negativity and smoothness\n    margin = F.softplus(torch.clamp(dynamic_margin, max=margin_max))\n\n    # 6. Compute the core preference loss with the dynamic margin\n    loss_core = -F.logsigmoid(normalized_delta + margin)\n\n    # 7. Inherited: Compute the standard focal modulating factor\n    #    Detach to prevent gradients from flowing through the modulating factor itself\n    modulating_factor = torch.pow(1.0 - p_win, gamma).detach()\n\n    # 8. Apply the modulating factor to the core loss\n    final_loss = modulating_factor * loss_core\n\n    # Apply optional instance weights if provided\n    if weight is not None:\n        final_loss = final_loss * weight\n\n    # Return the mean loss over the batch\n    return final_loss.mean()", "theoretical_basis": "A dynamically modulated Bradley-Terry preference model. The loss is based on a logistic function of z-scored log-probability differences. It incorporates a novel dynamic margin that scales with both the ground-truth cost difference and the model's uncertainty, effectively setting a higher learning bar for ambiguous examples. This is combined with a standard focal modulation to up-weight misclassified examples, creating a two-pronged approach to hard example mining."}, "static_ok": true, "static_reason": "", "static_error_code": "", "dynamic_ok": true, "dynamic_reason": "ok", "loss_value": 0.04189196601510048, "grad_norm": 0.0, "pref_ok": false, "pref_reason": "pref_semantic_violation", "pref_mono_pass_rate": 0.744140625, "pref_swap_pass_rate": 0.5, "pref_gap_pass_rate": 1.0, "dynamic_error_code": "E_PREF_SEMANTIC"}
{"generation": 6, "index": 3, "attempt": 1, "ir": {"name": "FocalLogsigmoidWithDynamicCertaintyMargin", "intuition": "Repaired: The original loss failed the `E_PREF_SEMANTIC` gate check, specifically the `swap_pass_rate` was at chance level (0.5). This indicated that the loss did not consistently prefer the lower-cost (winning) solution. The root cause was that the cost gap (`cost_b - cost_a`) was calculated using `torch.abs`, making it insensitive to the direction of the preference. To fix this, I removed `torch.abs`, ensuring that `cost_gap` correctly reflects the preference direction (`cost_b` is always the higher cost). This change makes the dynamic margin directional, aligning the loss gradient with the goal of preferring the lower-cost solution.", "pseudocode": "1. Calculate the log-probability difference: delta = log_prob_w - log_prob_l.\n2. Normalize delta across the batch for stability: normalized_delta = zscore(delta).\n3. Calculate the directional cost difference: cost_gap = cost_b - cost_a.\n4. Calculate the model's win probability and uncertainty: p_win = sigmoid(delta), uncertainty_bonus = 1.0 - abs(p_win - 0.5).\n5. Compute a dynamic margin modulated by cost and uncertainty: dynamic_margin = beta * cost_gap * (1.0 + uncertainty_bonus.detach()).\n6. Clip the dynamic margin to prevent explosion and apply softplus for smoothness: margin = softplus(clamp(dynamic_margin, max=margin_max)).\n7. Compute the core preference loss: loss_core = -logsigmoid(normalized_delta + margin).\n8. Compute the standard focal modulating factor: modulating_factor = (1 - p_win)^gamma.\n9. Apply the detached modulating factor to the core loss: final_loss = modulating_factor.detach() * loss_core.\n10. Return the mean of the final loss.", "hyperparams": {"beta": 0.75, "gamma": 2.0, "margin_max": 4.0}, "operators_used": ["zscore", "sigmoid", "clamp", "softplus", "logsigmoid"], "implementation_hint": {"expects": ["cost_a", "cost_b", "log_prob_w", "log_prob_l"], "returns": "scalar"}, "code": "def generated_loss(batch, model_output, extra):\n    \"\"\"\n    Combines z-score normalization and focal loss with a novel dynamic margin\n    that adapts to both cost difference and model uncertainty.\n    \"\"\"\n    # Read hyperparameters with defaults\n    beta = extra.get('beta', 0.75)\n    gamma = extra.get('gamma', 2.0)\n    margin_max = extra.get('margin_max', 4.0)\n\n    # Read required tensors from the batch\n    cost_a = batch['cost_a']\n    cost_b = batch['cost_b']\n    log_prob_w = batch['log_prob_w']\n    log_prob_l = batch['log_prob_l']\n    weight = batch.get('weight')\n\n    # 1. Calculate the log-probability difference\n    delta = log_prob_w - log_prob_l\n\n    # 2. Inherited: Normalize delta for stability\n    normalized_delta = ops.zscore(delta)\n\n    # 3. Calculate model's win probability for margin and focal term\n    p_win = torch.sigmoid(delta)\n\n    # 4. New Coupling: Dynamic Certainty Margin\n    #   a. Base margin from cost difference (must be directional)\n    cost_gap = cost_b - cost_a\n    #   b. Uncertainty bonus (max when p_win is 0.5)\n    uncertainty_bonus = 1.0 - torch.abs(p_win - 0.5)\n    #   c. Combine them, detaching the uncertainty part to not affect its gradient\n    dynamic_margin = beta * cost_gap * (1.0 + uncertainty_bonus.detach())\n\n    # 5. New Coupling: Stabilize the margin\n    #    Clip to prevent explosion and apply softplus for non-negativity and smoothness\n    margin = F.softplus(torch.clamp(dynamic_margin, max=margin_max))\n\n    # 6. Compute the core preference loss with the dynamic margin\n    loss_core = -F.logsigmoid(normalized_delta + margin)\n\n    # 7. Inherited: Compute the standard focal modulating factor\n    #    Detach to prevent gradients from flowing through the modulating factor itself\n    modulating_factor = torch.pow(1.0 - p_win, gamma).detach()\n\n    # 8. Apply the modulating factor to the core loss\n    final_loss = modulating_factor * loss_core\n\n    # Apply optional instance weights if provided\n    if weight is not None:\n        final_loss = final_loss * weight\n\n    # Return the mean loss over the batch\n    return final_loss.mean()", "theoretical_basis": ""}, "static_ok": true, "static_reason": "", "static_error_code": "", "dynamic_ok": true, "dynamic_reason": "ok", "loss_value": 0.04189196601510048, "grad_norm": 0.0, "pref_ok": true, "pref_reason": "ok", "pref_mono_pass_rate": 0.7473958333333334, "pref_swap_pass_rate": 1.0, "pref_gap_pass_rate": 1.0}
{"generation": 6, "index": 4, "attempt": 0, "ir": {"name": "FocalLossWithAdaptiveMarginAndConfidencePenalty", "intuition": "Mode: explore. This loss function combines robust elements from its parentsnamely focal modulation and z-score normalizationwith two novel coupling ideas. The goal is to create a loss that not only distinguishes between easy and hard examples but also actively penalizes the model for being overconfident, especially on pairs with similar costs.\n\nInherited Ideas:\n1. From both parents: The core structure is a focal-modulated `-logsigmoid(delta + margin)` loss, which is effective at up-weighting hard-to-classify pairs.\n2. From both parents: The log-probability difference `delta = log_prob_w - log_prob_l` is normalized using `ops.zscore`. This stabilizes training by making the loss invariant to the scale and shift of log-probabilities within a batch.\n\nNew Coupling Ideas:\n1. **Dynamic Confidence Penalty**: A new penalty term is added to the loss that is proportional to `(p_win - 0.5)^2`. This penalty is small when the model is uncertain (p_win  0.5) and grows quadratically as the model becomes more confident (p_win  0 or 1). The strength of this penalty is modulated by `1 - tanh(cost_gap)`, making it strongest for pairs with small cost differences. This encourages the model to remain humble and less certain about pairs that are inherently ambiguous.\n2. **Hybrid Rank/Cost-Gap Margin**: The margin is a linear combination of `rank_gap` (from Parent 1) and the clipped absolute `cost_gap` (from Parent 2). This allows the margin to be sensitive to both the relative ordering of costs within the batch (via rank) and the absolute magnitude of the cost difference, while the clipping provides robustness against outliers. This creates a more nuanced and stable measure of pair difficulty.", "pseudocode": "1. Calculate the log-probability difference: delta = log_prob_w - log_prob_l.\n2. Normalize delta across the batch: normalized_delta = zscore(delta).\n3. Compute the cost rank gap: rank_gap = rank_gap(cost_a, cost_b).\n4. Compute the absolute cost difference: cost_gap = cost_b - cost_a.\n5. Calculate a hybrid margin by combining rank_gap and a clipped cost_gap: margin = beta_rank * rank_gap + beta_cost * clamp(cost_gap, min=0, max=margin_max).\n6. Compute the core preference loss: loss_bt = -logsigmoid(normalized_delta + margin).\n7. Calculate the model's win probability: p_win = sigmoid(delta).\n8. Compute the focal modulating factor: focal_factor = (1 - p_win)^gamma.\n9. Apply the focal factor to the core loss: loss_focal = focal_factor.detach() * loss_bt.\n10. Calculate a confidence penalty, strongest for small cost gaps: penalty = (p_win - 0.5)^2 * (1 - tanh(cost_gap.detach())).\n11. Combine the focal loss and the confidence penalty: final_loss = loss_focal + penalty_weight * penalty.\n12. Return the mean of the final loss.", "hyperparams": {"beta_rank": 0.5, "beta_cost": 0.25, "margin_max": 3.0, "gamma": 2.0, "penalty_weight": 0.5}, "operators_used": ["zscore", "rank_gap", "clamp", "logsigmoid", "sigmoid", "tanh"], "implementation_hint": {"expects": ["cost_a", "cost_b", "log_prob_w", "log_prob_l"], "returns": "scalar"}, "code": "def generated_loss(batch, model_output, extra):\n    \"\"\"\n    Combines a z-scored focal loss with a hybrid rank/cost margin and a confidence penalty.\n    \"\"\"\n    # Read hyperparameters with defaults\n    beta_rank = extra.get('beta_rank', 0.5)\n    beta_cost = extra.get('beta_cost', 0.25)\n    margin_max = extra.get('margin_max', 3.0)\n    gamma = extra.get('gamma', 2.0)\n    penalty_weight = extra.get('penalty_weight', 0.5)\n\n    # Read required tensors from the batch\n    cost_a = batch['cost_a']\n    cost_b = batch['cost_b']\n    log_prob_w = batch['log_prob_w']\n    log_prob_l = batch['log_prob_l']\n    weight = batch.get('weight')\n\n    # 1. Calculate and normalize log-probability difference (Inherited)\n    delta = log_prob_w - log_prob_l\n    normalized_delta = ops.zscore(delta)\n\n    # 2. Compute cost differences\n    cost_gap = cost_b - cost_a\n    rank_gap = ops.rank_gap(cost_a, cost_b)\n\n    # 3. New Coupling: Hybrid Rank/Cost-Gap Margin\n    margin = beta_rank * rank_gap + beta_cost * torch.clamp(cost_gap, min=0.0, max=margin_max)\n\n    # 4. Compute core Bradley-Terry style loss\n    loss_bt = -F.logsigmoid(normalized_delta + margin)\n\n    # 5. Compute focal modulation (Inherited)\n    p_win = torch.sigmoid(delta)\n    focal_factor = torch.pow(1.0 - p_win, gamma)\n    loss_focal = focal_factor.detach() * loss_bt\n\n    # 6. New Coupling: Dynamic Confidence Penalty\n    # Penalty is high when model is confident (p_win -> 0 or 1) on pairs with small cost gaps.\n    # tanh(cost_gap) scales from 0 to 1, so (1 - tanh) is a weight from 1 to 0.\n    confidence_penalty = torch.pow(p_win - 0.5, 2) * (1.0 - torch.tanh(cost_gap.detach()))\n    \n    # 7. Combine focal loss and penalty\n    final_loss = loss_focal + penalty_weight * confidence_penalty\n\n    # Apply optional instance weights if provided\n    if weight is not None:\n        final_loss = final_loss * weight\n\n    # Return the mean loss over the batch\n    return final_loss.mean()", "theoretical_basis": "A dynamically modulated Bradley-Terry preference model with an integrated confidence penalty. The core logistic loss incorporates a hybrid margin sensitive to both cost rank and magnitude. This is augmented with a focal modulation for hard-example mining and a novel penalty term that discourages overconfidence on ambiguous pairs (those with small cost differences), promoting better model calibration."}, "static_ok": true, "static_reason": "", "static_error_code": "", "dynamic_ok": true, "dynamic_reason": "ok", "loss_value": 0.09671775996685028, "grad_norm": 0.0, "pref_ok": false, "pref_reason": "pref_semantic_violation", "pref_mono_pass_rate": 0.7786458333333334, "pref_swap_pass_rate": 1.0, "pref_gap_pass_rate": 0.5, "dynamic_error_code": "E_PREF_SEMANTIC"}
{"generation": 6, "index": 4, "attempt": 1, "ir": {"name": "FocalLossWithAdaptiveMarginAndConfidencePenalty", "intuition": "Repaired: The original loss failed the `preference_gate` (E_PREF_SEMANTIC), specifically the `gap_pass_rate` was very low (0.5), indicating the loss was not consistently smaller for pairs with a larger cost gap. This was caused by the `zscore` normalization on the log-probability difference (`delta`), which removed the sensitivity of the core loss term to the scale of `delta`. I have removed the `ops.zscore(delta)` operation. The rest of the loss, including the focal modulation and confidence penalty, remains sensitive to cost gaps and should function as intended without this normalization step, thus restoring the desired preference semantics.", "pseudocode": "1. Calculate the log-probability difference: delta = log_prob_w - log_prob_l.\n2. Compute the cost rank gap: rank_gap = rank_gap(cost_a, cost_b).\n3. Compute the absolute cost difference: cost_gap = cost_b - cost_a.\n4. Calculate a hybrid margin by combining rank_gap and a clipped cost_gap: margin = beta_rank * rank_gap + beta_cost * clamp(cost_gap, min=0, max=margin_max).\n5. Compute the core preference loss: loss_bt = -logsigmoid(delta + margin).\n6. Calculate the model's win probability: p_win = sigmoid(delta).\n7. Compute the focal modulating factor: focal_factor = (1 - p_win)^gamma.\n8. Apply the focal factor to the core loss: loss_focal = focal_factor.detach() * loss_bt.\n9. Calculate a confidence penalty, strongest for small cost gaps: penalty = (p_win - 0.5)^2 * (1 - tanh(cost_gap.detach())).\n10. Combine the focal loss and the confidence penalty: final_loss = loss_focal + penalty_weight * penalty.\n11. Return the mean of the final loss.", "hyperparams": {"beta_rank": 0.5, "beta_cost": 0.25, "margin_max": 3.0, "gamma": 2.0, "penalty_weight": 0.5}, "operators_used": ["rank_gap", "clamp", "logsigmoid", "sigmoid", "tanh"], "implementation_hint": {"expects": ["cost_a", "cost_b", "log_prob_w", "log_prob_l"], "returns": "scalar"}, "code": "def generated_loss(batch, model_output, extra):\n    \"\"\"\n    Combines a focal loss with a hybrid rank/cost margin and a confidence penalty.\n    \"\"\"\n    # Read hyperparameters with defaults\n    beta_rank = extra.get('beta_rank', 0.5)\n    beta_cost = extra.get('beta_cost', 0.25)\n    margin_max = extra.get('margin_max', 3.0)\n    gamma = extra.get('gamma', 2.0)\n    penalty_weight = extra.get('penalty_weight', 0.5)\n\n    # Read required tensors from the batch\n    cost_a = batch['cost_a']\n    cost_b = batch['cost_b']\n    log_prob_w = batch['log_prob_w']\n    log_prob_l = batch['log_prob_l']\n    weight = batch.get('weight')\n\n    # 1. Calculate log-probability difference\n    delta = log_prob_w - log_prob_l\n\n    # 2. Compute cost differences\n    cost_gap = cost_b - cost_a\n    rank_gap = ops.rank_gap(cost_a, cost_b)\n\n    # 3. New Coupling: Hybrid Rank/Cost-Gap Margin\n    margin = beta_rank * rank_gap + beta_cost * torch.clamp(cost_gap, min=0.0, max=margin_max)\n\n    # 4. Compute core Bradley-Terry style loss\n    loss_bt = -F.logsigmoid(delta + margin)\n\n    # 5. Compute focal modulation (Inherited)\n    p_win = torch.sigmoid(delta)\n    focal_factor = torch.pow(1.0 - p_win, gamma)\n    loss_focal = focal_factor.detach() * loss_bt\n\n    # 6. New Coupling: Dynamic Confidence Penalty\n    # Penalty is high when model is confident (p_win -> 0 or 1) on pairs with small cost gaps.\n    # tanh(cost_gap) scales from 0 to 1, so (1 - tanh) is a weight from 1 to 0.\n    confidence_penalty = torch.pow(p_win - 0.5, 2) * (1.0 - torch.tanh(cost_gap.detach()))\n    \n    # 7. Combine focal loss and penalty\n    final_loss = loss_focal + penalty_weight * confidence_penalty\n\n    # Apply optional instance weights if provided\n    if weight is not None:\n        final_loss = final_loss * weight\n\n    # Return the mean loss over the batch\n    return final_loss.mean()", "theoretical_basis": ""}, "static_ok": true, "static_reason": "", "static_error_code": "", "dynamic_ok": true, "dynamic_reason": "ok", "loss_value": 0.09671775996685028, "grad_norm": 0.0, "pref_ok": true, "pref_reason": "ok", "pref_mono_pass_rate": 0.7858072916666666, "pref_swap_pass_rate": 1.0, "pref_gap_pass_rate": 1.0}
{"generation": 6, "index": 5, "attempt": 0, "ir": {"name": "FocalLossWithDynamicMarginAndConfidencePenalty", "intuition": "Mode: explore. This loss function hybridizes the strongest features of its parents: z-score normalization, focal modulation, and adaptive margins, while introducing a new coupling mechanism to directly penalize low-confidence predictions on pairs with large cost gaps.\n\nInherited Ideas:\n1. From both parents (`AdaptiveFocalLogsigmoidWithClippedMargin`): The use of `ops.zscore` to normalize the log-probability difference (`delta`). This is a proven technique for stabilizing training against outliers and batch variance.\n2. From both parents: The application of a focal loss modulator `(1 - p_win)^gamma` to up-weight hard-to-classify examples, where `p_win = sigmoid(delta)`.\n\nNew Coupling Ideas:\n1. **Dynamic Margin from Clipped Cost and Rank Gaps**: The margin is a combination of the absolute cost gap (from parent 2) and the rank gap (from parent 1). It is calculated as `margin = clamp(beta_cost * (cost_b - cost_a) + beta_rank * rank_gap, 0, margin_max)`. This captures both the magnitude of the cost difference and its relative importance within the batch, while the clamp prevents instability from extreme outliers.\n2. **Confidence Penalty for High-Stakes Pairs**: A new penalty term is added that specifically targets low-confidence predictions (`p_win` near 0.5) on pairs with a significant cost difference. The penalty is `lambda * (1 - tanh(k * cost_gap)) * (1 - (2*p_win - 1)^2)`. The `tanh` term ensures this penalty is active only for large `cost_gap`s. The `(1 - (2*p_win - 1)^2)` term is a smooth function that peaks when `p_win = 0.5` (maximum uncertainty) and is zero at `p_win = 0` or `1`. This encourages the model to be decisive on pairs where the cost difference is large and the correct choice should be obvious.", "pseudocode": "1. Calculate the log-probability difference: delta = log_prob_w - log_prob_l.\n2. Normalize delta across the batch for stability: normalized_delta = zscore(delta).\n3. Calculate the absolute cost difference: cost_gap = cost_b - cost_a.\n4. Calculate the cost rank gap: rank_gap = rank_gap(cost_a, cost_b).\n5. Compute a dynamic, clipped margin combining cost and rank gaps: margin = clamp(beta_cost * cost_gap + beta_rank * rank_gap, min=0, max=margin_max).\n6. Compute the core preference loss based on the Bradley-Terry model: core_loss = -logsigmoid(normalized_delta + margin).\n7. Calculate the model's confidence in the winner: p_win = sigmoid(delta).\n8. Compute the focal modulating factor: focal_modulator = (1 - p_win)^gamma.\n9. Apply the focal modulator to the core loss: focal_loss = focal_modulator.detach() * core_loss.\n10. Calculate the confidence penalty term, which is active for high cost gaps and low model confidence: confidence_penalty = lambda * (1 - tanh(k * cost_gap.detach())) * (1 - (2*p_win - 1)^2).\n11. Combine the focal loss and the confidence penalty: total_loss = focal_loss + confidence_penalty.\n12. Return the mean of the total loss.", "hyperparams": {"beta_cost": 0.2, "beta_rank": 1.0, "margin_max": 3.0, "gamma": 2.0, "lambda_penalty": 0.5, "k_penalty_scale": 0.1}, "operators_used": ["zscore", "rank_gap", "clamp", "logsigmoid", "sigmoid", "tanh"], "implementation_hint": {"expects": ["cost_a", "cost_b", "log_prob_w", "log_prob_l"], "returns": "scalar"}, "code": "def generated_loss(batch, model_output, extra):\n    \"\"\"\n    Combines z-scored focal loss with a hybrid margin and a confidence penalty.\n    \"\"\"\n    # Read hyperparameters with defaults\n    beta_cost = extra.get('beta_cost', 0.2)\n    beta_rank = extra.get('beta_rank', 1.0)\n    margin_max = extra.get('margin_max', 3.0)\n    gamma = extra.get('gamma', 2.0)\n    lambda_penalty = extra.get('lambda_penalty', 0.5)\n    k_penalty_scale = extra.get('k_penalty_scale', 0.1)\n\n    # Read tensors from batch\n    cost_a = batch['cost_a']\n    cost_b = batch['cost_b']\n    log_prob_w = batch['log_prob_w']\n    log_prob_l = batch['log_prob_l']\n    weight = batch.get('weight')\n\n    # 1. Calculate and normalize log-probability difference (Inherited)\n    delta = log_prob_w - log_prob_l\n    normalized_delta = ops.zscore(delta)\n\n    # 2. Calculate cost and rank gaps\n    cost_gap = cost_b - cost_a\n    rank_gap = ops.rank_gap(cost_a, cost_b)\n\n    # 3. New Coupling: Dynamic margin from both cost and rank gaps\n    margin = torch.clamp(beta_cost * cost_gap + beta_rank * rank_gap, min=0.0, max=margin_max)\n\n    # 4. Compute core preference loss\n    core_loss = -F.logsigmoid(normalized_delta + margin)\n\n    # 5. Compute focal modulation (Inherited)\n    p_win = torch.sigmoid(delta)\n    focal_modulator = torch.pow(1.0 - p_win, gamma)\n    focal_loss = focal_modulator.detach() * core_loss\n\n    # 6. New Coupling: Confidence penalty for high-stakes pairs\n    # This term is high when cost_gap is large and p_win is near 0.5\n    uncertainty_term = 1.0 - torch.pow(2 * p_win - 1, 2) # Peaks at 1 when p_win=0.5, zero at p_win=0,1\n    cost_gate = 1.0 - torch.tanh(k_penalty_scale * cost_gap.detach()) # Approaches 1 for large cost_gap\n    confidence_penalty = lambda_penalty * cost_gate * uncertainty_term\n\n    # 7. Combine losses\n    total_loss = focal_loss + confidence_penalty\n\n    # Apply optional instance weights\n    if weight is not None:\n        total_loss = total_loss * weight\n\n    return total_loss.mean()", "theoretical_basis": "A hybrid Bradley-Terry and margin-based model. The primary objective is a focal-modulated logistic loss on a z-scored log-probability difference, plus a dynamic margin sensitive to both absolute and relative cost differences. A secondary objective term is added to explicitly penalize model uncertainty on high-stakes preference pairs, encouraging more decisive predictions where the ground truth is clear."}, "static_ok": true, "static_reason": "", "static_error_code": "", "dynamic_ok": true, "dynamic_reason": "ok", "loss_value": 0.5159866213798523, "grad_norm": 0.0, "pref_ok": true, "pref_reason": "ok", "pref_mono_pass_rate": 0.7740885416666666, "pref_swap_pass_rate": 1.0, "pref_gap_pass_rate": 1.0}
{"generation": 6, "index": 6, "attempt": 0, "ir": {"name": "DynamicFocalRangeNormalizedLoss", "intuition": "Mode: explore. This loss function aims to combine the stability of normalization with a highly adaptive focal modulation scheme. It inherits the core concepts of focal loss and z-score normalization from its parents, but introduces a new way to calculate the margin and dynamically adjust the focal strength.\n\nInherited Ideas:\n1. From `AdaptiveFocalLogsigmoidWithClippedMargin` (both parents): The use of a focal modulating factor `(1 - p_win)^gamma` to up-weight difficult examples based on the model's confidence.\n2. From `AdaptiveFocalLogsigmoidWithClippedMargin` (both parents): The use of `ops.zscore` to normalize the log-probability difference (`delta`), which provides batch-level stability.\n\nNew Coupling Ideas:\n1. **Range-Normalized Cost Margin**: Instead of a clipped margin based on rank or absolute cost, this loss uses a margin derived from the cost gap (`cost_b - cost_a`) normalized by the range of cost gaps in the batch (`max(cost_gap) - min(cost_gap)`). This makes the margin adaptive to the scale of costs within each batch, ensuring it remains meaningful whether cost differences are large or small, while being naturally bounded between approximately 0 and `beta`.\n2. **Confidence-Modulated Focal Strength**: The focal strength `gamma` is dynamically adjusted based on the model's confidence (`p_win`). The strength is maximized when the model is most uncertain (`p_win` is near 0.5) and decreases as the model becomes more confident. This is achieved via `gamma = gamma_base * (1.0 - torch.abs(2.0 * p_win - 1.0))`, which focuses learning on the most ambiguous pairs where the model's decision boundary is weakest.", "pseudocode": "1. Calculate the log-probability difference: delta = log_prob_w - log_prob_l.\n2. Normalize delta across the batch for stability: normalized_delta = zscore(delta).\n3. Calculate the absolute cost difference: cost_gap = cost_b - cost_a.\n4. Compute the range of cost gaps in the batch: cost_range = max(cost_gap) - min(cost_gap).\n5. Calculate a range-normalized margin: margin = beta * (cost_gap - min(cost_gap)) / (cost_range + epsilon).\n6. Compute the core preference loss: loss_core = -logsigmoid(normalized_delta + margin).\n7. Calculate the model's confidence in the winner: p_win = sigmoid(delta).\n8. Compute a dynamic focal strength that peaks at p_win=0.5: dynamic_gamma = gamma_base * (1.0 - abs(2.0 * p_win - 1.0)).\n9. Calculate the focal modulating factor: modulating_factor = (1 - p_win)^dynamic_gamma.\n10. Apply the detached modulating factor to the core loss: final_loss = modulating_factor.detach() * loss_core.\n11. Return the mean of the final loss.", "hyperparams": {"beta": 1.0, "gamma_base": 2.5, "epsilon": 1e-08}, "operators_used": ["zscore", "logsigmoid", "sigmoid"], "implementation_hint": {"expects": ["cost_a", "cost_b", "log_prob_w", "log_prob_l"], "returns": "scalar"}, "code": "def generated_loss(batch, model_output, extra):\n    \"\"\"\n    Combines z-score normalization with a range-normalized cost margin and a confidence-modulated focal loss.\n    \"\"\"\n    # Read hyperparameters with defaults\n    beta = extra.get('beta', 1.0)\n    gamma_base = extra.get('gamma_base', 2.5)\n    epsilon = extra.get('epsilon', 1e-8)\n\n    # Read required tensors from the batch\n    cost_a = batch['cost_a']\n    cost_b = batch['cost_b']\n    log_prob_w = batch['log_prob_w']\n    log_prob_l = batch['log_prob_l']\n    weight = batch.get('weight')\n\n    # 1. Calculate the log-probability difference\n    delta = log_prob_w - log_prob_l\n\n    # 2. Inherited: Normalize delta for stability\n    normalized_delta = ops.zscore(delta)\n\n    # 3. New Coupling: Range-Normalized Cost Margin\n    with torch.no_grad():\n        cost_gap = cost_b - cost_a\n        min_cost_gap = torch.min(cost_gap)\n        max_cost_gap = torch.max(cost_gap)\n        cost_range = max_cost_gap - min_cost_gap\n        # Normalize cost_gap to [0, 1] within the batch, then scale by beta\n        margin = beta * (cost_gap - min_cost_gap) / (cost_range + epsilon)\n\n    # 4. Compute the core preference loss\n    loss_core = -F.logsigmoid(normalized_delta + margin)\n\n    # 5. Calculate probability for focal loss\n    p_win = torch.sigmoid(delta)\n\n    # 6. New Coupling: Confidence-Modulated Focal Strength\n    # Gamma is maximized when p_win is 0.5 (max uncertainty)\n    # The term abs(2*p - 1) is 0 at p=0.5 and 1 at p=0 or p=1.\n    uncertainty_factor = 1.0 - torch.abs(2.0 * p_win - 1.0)\n    dynamic_gamma = gamma_base * uncertainty_factor\n\n    # 7. Inherited: Compute the focal modulating factor\n    modulating_factor = torch.pow(1.0 - p_win, dynamic_gamma)\n\n    # 8. Apply the modulating factor (detached) to the core loss\n    final_loss = modulating_factor.detach() * loss_core\n\n    # Apply optional instance weights if provided\n    if weight is not None:\n        final_loss = final_loss * weight\n\n    # Return the mean loss over the batch\n    return final_loss.mean()", "theoretical_basis": "A dynamically modulated Bradley-Terry preference model. The core loss is logistic (-logsigmoid) on a z-scored log-probability difference, augmented by a novel batch-range-normalized cost margin. This is coupled with a focal modulation whose strength is a dynamic function of model uncertainty, focusing learning on the most ambiguous examples within a batch."}, "static_ok": true, "static_reason": "", "static_error_code": "", "dynamic_ok": true, "dynamic_reason": "ok", "loss_value": 0.12253225594758987, "grad_norm": 0.0, "pref_ok": false, "pref_reason": "pref_semantic_violation", "pref_mono_pass_rate": 0.7688802083333334, "pref_swap_pass_rate": 0.5, "pref_gap_pass_rate": 1.0, "dynamic_error_code": "E_PREF_SEMANTIC"}
{"generation": 6, "index": 6, "attempt": 1, "ir": {"name": "DynamicFocalRangeNormalizedLoss", "intuition": "Repaired: The original loss failed the preference gate's swap test (`swap_pass_rate`: 0.5), indicating a structural issue where swapping the winning and losing solutions did not correctly flip the sign of the loss. This was caused by calculating the `margin` within a `torch.no_grad()` block, which prevented gradients from flowing through the `cost_gap`. This meant the loss function was not sensitive to the cost difference as required. I have removed the `torch.no_grad()` context manager to restore the gradient flow and ensure the loss correctly reflects the preference structure.", "pseudocode": "1. Calculate the log-probability difference: delta = log_prob_w - log_prob_l.\n2. Normalize delta across the batch for stability: normalized_delta = zscore(delta).\n3. Calculate the absolute cost difference: cost_gap = cost_b - cost_a.\n4. Compute the range of cost gaps in the batch: cost_range = max(cost_gap) - min(cost_gap).\n5. Calculate a range-normalized margin: margin = beta * (cost_gap - min(cost_gap)) / (cost_range + epsilon).\n6. Compute the core preference loss: loss_core = -logsigmoid(normalized_delta + margin).\n7. Calculate the model's confidence in the winner: p_win = sigmoid(delta).\n8. Compute a dynamic focal strength that peaks at p_win=0.5: dynamic_gamma = gamma_base * (1.0 - abs(2.0 * p_win - 1.0)).\n9. Calculate the focal modulating factor: modulating_factor = (1 - p_win)^dynamic_gamma.\n10. Apply the detached modulating factor to the core loss: final_loss = modulating_factor.detach() * loss_core.\n11. Return the mean of the final loss.", "hyperparams": {"beta": 1.0, "gamma_base": 2.5, "epsilon": 1e-08}, "operators_used": ["zscore", "logsigmoid", "sigmoid"], "implementation_hint": {"expects": ["cost_a", "cost_b", "log_prob_w", "log_prob_l"], "returns": "scalar"}, "code": "def generated_loss(batch, model_output, extra):\n    \"\"\"\n    Combines z-score normalization with a range-normalized cost margin and a confidence-modulated focal loss.\n    \"\"\"\n    # Read hyperparameters with defaults\n    beta = extra.get('beta', 1.0)\n    gamma_base = extra.get('gamma_base', 2.5)\n    epsilon = extra.get('epsilon', 1e-8)\n\n    # Read required tensors from the batch\n    cost_a = batch['cost_a']\n    cost_b = batch['cost_b']\n    log_prob_w = batch['log_prob_w']\n    log_prob_l = batch['log_prob_l']\n    weight = batch.get('weight')\n\n    # 1. Calculate the log-probability difference\n    delta = log_prob_w - log_prob_l\n\n    # 2. Inherited: Normalize delta for stability\n    normalized_delta = ops.zscore(delta)\n\n    # 3. New Coupling: Range-Normalized Cost Margin\n    cost_gap = cost_b - cost_a\n    # Detach min/max operations to treat them as batch-static statistics\n    min_cost_gap = torch.min(cost_gap).detach()\n    max_cost_gap = torch.max(cost_gap).detach()\n    cost_range = max_cost_gap - min_cost_gap\n    # Normalize cost_gap to [0, 1] within the batch, then scale by beta\n    margin = beta * (cost_gap - min_cost_gap) / (cost_range + epsilon)\n\n    # 4. Compute the core preference loss\n    loss_core = -F.logsigmoid(normalized_delta + margin)\n\n    # 5. Calculate probability for focal loss\n    p_win = torch.sigmoid(delta)\n\n    # 6. New Coupling: Confidence-Modulated Focal Strength\n    # Gamma is maximized when p_win is 0.5 (max uncertainty)\n    # The term abs(2*p - 1) is 0 at p=0.5 and 1 at p=0 or p=1.\n    uncertainty_factor = 1.0 - torch.abs(2.0 * p_win - 1.0)\n    dynamic_gamma = gamma_base * uncertainty_factor\n\n    # 7. Inherited: Compute the focal modulating factor\n    modulating_factor = torch.pow(1.0 - p_win, dynamic_gamma)\n\n    # 8. Apply the modulating factor (detached) to the core loss\n    final_loss = modulating_factor.detach() * loss_core\n\n    # Apply optional instance weights if provided\n    if weight is not None:\n        final_loss = final_loss * weight\n\n    # Return the mean loss over the batch\n    return final_loss.mean()", "theoretical_basis": ""}, "static_ok": true, "static_reason": "", "static_error_code": "", "dynamic_ok": true, "dynamic_reason": "ok", "loss_value": 0.12253225594758987, "grad_norm": 0.0, "pref_ok": true, "pref_reason": "ok", "pref_mono_pass_rate": 0.7760416666666666, "pref_swap_pass_rate": 0.6666666666666666, "pref_gap_pass_rate": 1.0}
{"generation": 6, "index": 7, "attempt": 0, "ir": {"name": "FocalLossWithDynamicCostSensitiveGamma", "intuition": "Mode: explore. This loss function combines the robust focal loss and z-score normalization from its parents with a novel, two-part coupling mechanism designed to make the focal modulation more sensitive to both the cost landscape and the model's current confidence.\n\nInherited Ideas:\n1.  From both parents: The core structure is a focal-modulated Bradley-Terry loss: `modulating_factor * -logsigmoid(normalized_delta + margin)`. This provides a strong baseline for preference learning that emphasizes difficult examples.\n2.  From both parents: The use of `ops.zscore` on the log-probability difference (`delta`) is retained for batch-level normalization, which is a proven stability trick.\n\nNew Coupling Ideas:\n1.  **Cost-Sensitive Dynamic Gamma**: The focal strength `gamma` is dynamically calculated based on the *normalized cost gap*. We compute `cost_gap = cost_b - cost_a` and then normalize it using `zscore`. The final gamma is `gamma_base + softplus(cost_gap_zscored)`. This couples the focal strength directly to the relative difficulty of the pair within the batch: pairs with a larger-than-average cost difference (harder to distinguish if the model gets them wrong) receive a stronger focal penalty, pushing the model to focus on them more intensely.\n2.  **Uncertainty-Modulated Margin**: The margin is now a product of the cost rank gap and the model's uncertainty, measured as `1 - abs(p_win - 0.5)`. This term is maximal when `p_win` is 0.5 (maximum uncertainty) and minimal when the model is confident (p_win near 0 or 1). This couples the margin directly to model uncertainty, creating a larger separation target for pairs where the model is struggling, while reducing the margin for pairs it already confidently classifies.", "pseudocode": "1. Calculate the log-probability difference: delta = log_prob_w - log_prob_l.\n2. Normalize delta across the batch for stability: normalized_delta = zscore(delta).\n3. Calculate the absolute cost difference: cost_gap = cost_b - cost_a.\n4. Normalize the cost gap across the batch: cost_gap_zscored = zscore(cost_gap).\n5. Compute a dynamic, cost-sensitive focal strength: dynamic_gamma = gamma_base + softplus(cost_gap_zscored.detach()).\n6. Calculate the model's confidence: p_win = sigmoid(delta).\n7. Compute the focal modulating factor using the dynamic gamma: modulating_factor = (1 - p_win)^dynamic_gamma.\n8. Compute the cost rank gap: rank_gap = rank_gap(cost_a, cost_b).\n9. Calculate model uncertainty: uncertainty = 1.0 - abs(p_win - 0.5).\n10. Compute an uncertainty-modulated margin: margin = beta * rank_gap * uncertainty.detach().\n11. Compute the core preference loss: loss_core = -logsigmoid(normalized_delta + margin).\n12. Apply the detached modulating factor to the core loss: final_loss = modulating_factor.detach() * loss_core.\n13. Return the mean of the final loss.", "hyperparams": {"beta": 1.0, "gamma_base": 1.5}, "operators_used": ["zscore", "softplus", "sigmoid", "logsigmoid", "rank_gap"], "implementation_hint": {"expects": ["cost_a", "cost_b", "log_prob_w", "log_prob_l"], "returns": "scalar"}, "code": "def generated_loss(batch, model_output, extra):\n    \"\"\"\n    Combines z-score normalization and focal loss with two new couplings:\n    1. A dynamic focal gamma that scales with the z-scored cost gap.\n    2. An adaptive margin that scales with model uncertainty.\n    \"\"\"\n    # Read hyperparameters with defaults\n    beta = extra.get('beta', 1.0)\n    gamma_base = extra.get('gamma_base', 1.5)\n\n    # Read required tensors from the batch\n    cost_a = batch['cost_a']\n    cost_b = batch['cost_b']\n    log_prob_w = batch['log_prob_w']\n    log_prob_l = batch['log_prob_l']\n    weight = batch.get('weight')\n\n    # 1. Calculate the log-probability difference\n    delta = log_prob_w - log_prob_l\n\n    # 2. Inherited: Normalize delta for stability\n    normalized_delta = ops.zscore(delta)\n\n    # 3. New Coupling Part 1: Cost-Sensitive Dynamic Gamma\n    cost_gap = cost_b - cost_a\n    # z-score the cost gap to measure relative difficulty within the batch\n    cost_gap_zscored = ops.zscore(cost_gap)\n    # Gamma increases for pairs with a larger-than-average cost gap\n    # Detach to prevent gradients from flowing through the gamma calculation itself\n    dynamic_gamma = gamma_base + F.softplus(cost_gap_zscored.detach())\n\n    # 4. Calculate probability for focal loss and margin modulation\n    p_win = torch.sigmoid(delta)\n\n    # 5. Compute the focal modulating factor\n    modulating_factor = torch.pow(1.0 - p_win, dynamic_gamma)\n\n    # 6. New Coupling Part 2: Uncertainty-Modulated Margin\n    # Inherit rank_gap for a non-parametric margin base\n    rank_gap = ops.rank_gap(cost_a, cost_b)\n    # Uncertainty is highest when p_win is 0.5\n    uncertainty = 1.0 - torch.abs(p_win - 0.5)\n    # Margin is largest for pairs where the model is most uncertain\n    # Detach to avoid gradients through the margin's uncertainty component\n    margin = beta * rank_gap * uncertainty.detach()\n\n    # 7. Compute the core preference loss\n    loss_core = -F.logsigmoid(normalized_delta + margin)\n\n    # 8. Apply the modulating factor to the core loss\n    # Detach the factor so it only re-weights the loss, not change the gradient direction\n    final_loss = modulating_factor.detach() * loss_core\n\n    # Apply optional instance weights if provided\n    if weight is not None:\n        final_loss = final_loss * weight\n\n    # Return the mean loss over the batch\n    return final_loss.mean()", "theoretical_basis": "A dynamically modulated Bradley-Terry preference model. The loss combines a logistic preference term with a focal modulation. The focal strength (gamma) is coupled with the batch-normalized cost gap, making the loss focus more on misclassified pairs with high cost separation. The margin is coupled with model uncertainty, increasing the separation target for pairs where the model is least certain."}, "static_ok": true, "static_reason": "", "static_error_code": "", "dynamic_ok": true, "dynamic_reason": "ok", "loss_value": 0.06850215047597885, "grad_norm": 0.0, "pref_ok": true, "pref_reason": "ok", "pref_mono_pass_rate": 0.7545572916666666, "pref_swap_pass_rate": 1.0, "pref_gap_pass_rate": 1.0}
{"generation": 7, "index": 0, "attempt": 0, "ir": {"name": "FocalLossWithClippedCostSensitiveMargin", "intuition": "Mode: explore. This loss function hybridizes the strongest components of its parentsfocal modulation and z-score normalizationwhile introducing a novel, stabilized margin mechanism. The goal is to create a loss that is sensitive to the cost landscape but robust against extreme values and model overconfidence.\n\nInherited Ideas:\n1. From both parents (`AdaptiveFocalLogsigmoidWithClippedMargin` and `FocalLossWithDynamicCostSensitiveGamma`): The foundational structure is a focal-modulated Bradley-Terry loss: `(1 - p_win)^gamma * -logsigmoid(normalized_delta + margin)`. This retains the proven benefit of up-weighting difficult examples.\n2. From both parents: The use of `ops.zscore` on the log-probability difference (`delta`) is kept for batch-level normalization, ensuring stability and consistent scaling.\n\nNew Coupling Ideas:\n1. **Clipped Cost-Sensitive Margin**: This is a direct fusion of ideas from both parents. We start with a margin proportional to the cost rank gap (`beta * rank_gap`), an idea present in both parents. We then introduce a `clamp` operation from Parent 1 (`AdaptiveFocalLogsigmoidWithClippedMargin`) to cap the maximum margin (`margin_max`), preventing vanishing gradients on very easy pairs. Finally, we make the *rate* of the margin (`beta`) dynamic, scaling it with the z-scored cost gap, inspired by Parent 2's dynamic gamma. Specifically, `beta_final = beta_base * softplus(zscore(cost_b - cost_a))`. This couples the margin's magnitude to the relative difficulty of the cost difference within the batch, creating a larger separation target for pairs that are objectively far apart in cost, but in a controlled, clipped manner.\n2. **Confidence-Penalized Focal Strength**: Parent 2 modulated its margin by model uncertainty. We adapt this idea to modulate the focal strength `gamma` instead. The gamma is reduced when the model is highly confident (p_win near 0 or 1) and increased when it is uncertain (p_win near 0.5). The formula is `gamma = gamma_base * (1 - tanh(abs(delta)))`. This couples the strength of the hard-example mining directly to the model's certainty. For pairs the model is already sure about, we reduce the focal effect to prevent overfitting on them. For uncertain pairs, we increase the focal effect to push for a clearer decision.", "pseudocode": "1. Calculate the log-probability difference: delta = log_prob_w - log_prob_l.\n2. Normalize delta across the batch for stability: normalized_delta = zscore(delta).\n3. Calculate the absolute cost difference: cost_gap = cost_b - cost_a.\n4. Normalize the cost gap: cost_gap_zscored = zscore(cost_gap).\n5. Compute a dynamic beta for the margin: beta_final = beta_base * softplus(cost_gap_zscored.detach()).\n6. Compute the cost rank gap: rank_gap = rank_gap(cost_a, cost_b).\n7. Calculate the clipped, cost-sensitive margin: margin = clamp(beta_final * rank_gap, min=0, max=margin_max).\n8. Compute a confidence-penalized focal strength: dynamic_gamma = gamma_base * (1.0 - tanh(abs(delta.detach()))).\n9. Calculate the model's win probability: p_win = sigmoid(delta).\n10. Compute the focal modulating factor: modulating_factor = (1 - p_win)^dynamic_gamma.\n11. Compute the core preference loss: loss_core = -logsigmoid(normalized_delta + margin).\n12. Apply the detached modulating factor: final_loss = modulating_factor.detach() * loss_core.\n13. Return the mean of the final loss.", "hyperparams": {"beta_base": 1.0, "gamma_base": 2.0, "margin_max": 3.0}, "operators_used": ["zscore", "softplus", "rank_gap", "clamp", "tanh", "sigmoid", "logsigmoid"], "implementation_hint": {"expects": ["cost_a", "cost_b", "log_prob_w", "log_prob_l"], "returns": "scalar"}, "code": "def generated_loss(batch, model_output, extra):\n    \"\"\"\n    Combines focal loss and z-score normalization with two new couplings:\n    1. A clipped margin whose scale is sensitive to the z-scored cost gap.\n    2. A focal gamma that is penalized by model confidence (1 - tanh|delta|).\n    \"\"\"\n    # Read hyperparameters with defaults\n    beta_base = extra.get('beta_base', 1.0)\n    gamma_base = extra.get('gamma_base', 2.0)\n    margin_max = extra.get('margin_max', 3.0)\n\n    # Read required tensors from the batch\n    cost_a = batch['cost_a']\n    cost_b = batch['cost_b']\n    log_prob_w = batch['log_prob_w']\n    log_prob_l = batch['log_prob_l']\n    weight = batch.get('weight')\n\n    # 1. Calculate the log-probability difference\n    delta = log_prob_w - log_prob_l\n\n    # 2. Inherited: Normalize delta for stability\n    normalized_delta = ops.zscore(delta)\n\n    # 3. New Coupling 1: Clipped Cost-Sensitive Margin\n    # Inherit cost-gap sensitivity from Parent 2\n    cost_gap = cost_b - cost_a\n    cost_gap_zscored = ops.zscore(cost_gap)\n    # Margin rate increases for pairs with a larger-than-average cost gap\n    beta_final = beta_base * F.softplus(cost_gap_zscored.detach())\n    # Inherit rank_gap margin base from both parents\n    rank_gap = ops.rank_gap(cost_a, cost_b)\n    # Inherit clipping from Parent 1\n    margin = torch.clamp(beta_final * rank_gap, min=0, max=margin_max)\n\n    # 4. New Coupling 2: Confidence-Penalized Focal Strength\n    # Tanh(abs(delta)) is high for confident predictions, low for uncertain ones.\n    # Detach delta to prevent gradients from flowing through the gamma calculation.\n    confidence_penalty = torch.tanh(torch.abs(delta.detach()))\n    dynamic_gamma = gamma_base * (1.0 - confidence_penalty)\n    \n    # 5. Calculate probability for focal loss\n    p_win = torch.sigmoid(delta)\n\n    # 6. Compute the focal modulating factor\n    modulating_factor = torch.pow(1.0 - p_win, dynamic_gamma)\n\n    # 7. Compute the core preference loss\n    loss_core = -F.logsigmoid(normalized_delta + margin)\n\n    # 8. Apply the modulating factor to the core loss\n    # Detach the factor so it only re-weights the loss, not change the gradient direction\n    final_loss = modulating_factor.detach() * loss_core\n\n    # Apply optional instance weights if provided\n    if weight is not None:\n        final_loss = final_loss * weight\n\n    # Return the mean loss over the batch\n    return final_loss.mean()", "theoretical_basis": "A dynamically modulated Bradley-Terry preference model. The core is a logistic loss on z-scored log-probabilities. It features two main couplings: (1) A clipped, rank-based margin whose scale is dynamically adjusted by the batch-normalized cost gap, making the separation target sensitive to the pair's objective difficulty. (2) A focal modulation strength that is inversely coupled with model confidence (via tanh on the logit difference), which concentrates learning on uncertain pairs while preventing over-penalization of confidently-classified pairs."}, "static_ok": true, "static_reason": "", "static_error_code": "", "dynamic_ok": true, "dynamic_reason": "ok", "loss_value": 0.10136627405881882, "grad_norm": 0.0, "pref_ok": true, "pref_reason": "ok", "pref_mono_pass_rate": 0.7916666666666666, "pref_swap_pass_rate": 1.0, "pref_gap_pass_rate": 1.0}
{"generation": 7, "index": 1, "attempt": 0, "ir": {"name": "AdaptiveFocalLossWithDynamicMarginClipping", "intuition": "Mode: combine. This loss function combines the strongest elements of its parents: a focal-modulated Bradley-Terry loss on a z-scored log-probability difference, with a dynamic margin. It introduces a novel coupling by dynamically clipping the margin based on the model's uncertainty, preventing confident but incorrect predictions from generating excessively large, uninformative loss signals.\n\nInherited Ideas:\n1. From both parents (`AdaptiveFocalLogsigmoidWithClippedMargin`, `FocalLossWithDynamicCostSensitiveGamma`): The core framework is a focal-modulated `logsigmoid` loss, `(1-p)^gamma * -logsigmoid(delta + margin)`, operating on a `zscore`-normalized log-probability difference (`delta`). This provides a robust, stable baseline that focuses on hard examples.\n2. From `FocalLossWithDynamicCostSensitiveGamma`: The focal strength `gamma` is dynamically modulated by the z-scored cost gap (`gamma = gamma_base + softplus(zscore(cost_gap))`). This effectively increases focus on pairs that are objectively very different in cost but that the model still struggles with.\n\nNew Coupling Ideas:\n1. **Dynamic Margin Clipping based on Uncertainty**: The margin is calculated as `beta * rank_gap`, but it is clipped at a dynamic upper bound. This upper bound is `margin_max_base * (1.0 + uncertainty)`, where `uncertainty = 1.0 - abs(p_win - 0.5)`. When the model is very confident (`p_win` near 0 or 1), the uncertainty term is small, leading to a tighter clip on the margin. This prevents the loss from becoming huge for confident misclassifications, which can destabilize training. When the model is uncertain (`p_win` near 0.5), the margin is allowed to be larger, pushing for greater separation. This couples the margin's effective range to the model's own state of confidence.\n2. **Detached Uncertainty for Stability**: The uncertainty term used for clipping is detached from the computation graph. This ensures the clipping mechanism acts purely as a stabilizing regularizer on the loss magnitude, without introducing complex gradients from the uncertainty calculation back into the main loss term.", "pseudocode": "1. Calculate the log-probability difference: delta = log_prob_w - log_prob_l.\n2. Normalize delta across the batch: normalized_delta = zscore(delta).\n3. Calculate the cost gap and its z-score: cost_gap = cost_b - cost_a; cost_gap_zscored = zscore(cost_gap).\n4. Compute a dynamic, cost-sensitive focal strength: dynamic_gamma = gamma_base + softplus(cost_gap_zscored.detach()).\n5. Calculate the model's win probability: p_win = sigmoid(delta).\n6. Compute the focal modulating factor: modulating_factor = (1 - p_win)^dynamic_gamma.\n7. Compute the base margin from the cost rank gap: base_margin = beta * rank_gap(cost_a, cost_b).\n8. Calculate model uncertainty: uncertainty = 1.0 - abs(p_win - 0.5).\n9. Calculate a dynamic clipping threshold for the margin: dynamic_margin_max = margin_max_base * (1.0 + uncertainty.detach()).\n10. Clip the base margin using the dynamic threshold: margin = clamp(base_margin, min=0, max=dynamic_margin_max).\n11. Compute the core preference loss: loss_core = -logsigmoid(normalized_delta + margin).\n12. Apply the detached modulating factor: final_loss = modulating_factor.detach() * loss_core.\n13. Return the mean of the final loss.", "hyperparams": {"beta": 1.0, "gamma_base": 1.5, "margin_max_base": 2.5}, "operators_used": ["zscore", "softplus", "sigmoid", "logsigmoid", "rank_gap", "clamp"], "implementation_hint": {"expects": ["cost_a", "cost_b", "log_prob_w", "log_prob_l"], "returns": "scalar"}, "code": "def generated_loss(batch, model_output, extra):\n    \"\"\"\n    Combines a cost-sensitive dynamic gamma with a novel uncertainty-clipped margin.\n    \"\"\"\n    # Read hyperparameters with defaults\n    beta = extra.get('beta', 1.0)\n    gamma_base = extra.get('gamma_base', 1.5)\n    margin_max_base = extra.get('margin_max_base', 2.5)\n\n    # Read required tensors from the batch\n    cost_a = batch['cost_a']\n    cost_b = batch['cost_b']\n    log_prob_w = batch['log_prob_w']\n    log_prob_l = batch['log_prob_l']\n    weight = batch.get('weight')\n\n    # 1. Calculate and normalize the log-probability difference (Inherited)\n    delta = log_prob_w - log_prob_l\n    normalized_delta = ops.zscore(delta)\n\n    # 2. Calculate dynamic, cost-sensitive gamma (Inherited from Parent 2)\n    cost_gap = cost_b - cost_a\n    cost_gap_zscored = ops.zscore(cost_gap)\n    dynamic_gamma = gamma_base + F.softplus(cost_gap_zscored.detach())\n\n    # 3. Calculate probability for focal loss and uncertainty\n    p_win = torch.sigmoid(delta)\n\n    # 4. Compute the focal modulating factor (Inherited)\n    modulating_factor = torch.pow(1.0 - p_win, dynamic_gamma)\n\n    # 5. Compute the base margin using rank_gap (Inherited)\n    rank_gap = ops.rank_gap(cost_a, cost_b)\n    base_margin = beta * rank_gap\n\n    # 6. New Coupling: Dynamic Margin Clipping based on Uncertainty\n    # Uncertainty is highest when p_win is 0.5\n    uncertainty = 1.0 - torch.abs(p_win - 0.5)\n    # The max margin is smaller for confident predictions (low uncertainty)\n    # and larger for uncertain ones. Detach to use as a pure regularizer.\n    dynamic_margin_max = margin_max_base * (1.0 + uncertainty.detach())\n    margin = torch.clamp(base_margin, min=0.0, max=dynamic_margin_max)\n\n    # 7. Compute the core preference loss\n    loss_core = -F.logsigmoid(normalized_delta + margin)\n\n    # 8. Apply the modulating factor to the core loss\n    final_loss = modulating_factor.detach() * loss_core\n\n    # Apply optional instance weights if provided\n    if weight is not None:\n        final_loss = final_loss * weight\n\n    # Return the mean loss over the batch\n    return final_loss.mean()", "theoretical_basis": "A dynamically modulated Bradley-Terry preference model. The loss combines a logistic preference term with a focal modulation. The focal strength (gamma) is coupled with the batch-normalized cost gap, and the rank-based margin is dynamically clipped based on model uncertainty. This creates a dual-focus system: the focal term emphasizes pairs with large objective cost differences, while the uncertainty-clipped margin prevents confident misclassifications from generating excessively large loss values, promoting training stability."}, "static_ok": true, "static_reason": "", "static_error_code": "", "dynamic_ok": false, "dynamic_reason": "forward_error: clamp() received an invalid combination of arguments - got (Tensor, max=Tensor, min=float), but expected one of:\n * (Tensor input, Tensor min = None, Tensor max = None, *, Tensor out = None)\n * (Tensor input, Number min = None, Number max = None, *, Tensor out = None)\n", "loss_value": null, "grad_norm": null, "dynamic_error_code": "E_FORWARD_ERROR"}
{"generation": 7, "index": 1, "attempt": 1, "ir": {"name": "AdaptiveFocalLossWithDynamicMarginClipping", "intuition": "Repaired: The original code failed because `torch.clamp` was called with a float `min` and a Tensor `max`, which is an invalid combination of argument types. I fixed this by ensuring both `min` and `max` arguments are tensors. I created a `min_tensor` with the same shape and device as the input, filled with the float value 0.0, to resolve the `E_FORWARD_ERROR` and make the clamping operation valid.", "pseudocode": "1. Calculate the log-probability difference: delta = log_prob_w - log_prob_l.\n2. Normalize delta across the batch: normalized_delta = zscore(delta).\n3. Calculate the cost gap and its z-score: cost_gap = cost_b - cost_a; cost_gap_zscored = zscore(cost_gap).\n4. Compute a dynamic, cost-sensitive focal strength: dynamic_gamma = gamma_base + softplus(cost_gap_zscored.detach()).\n5. Calculate the model's win probability: p_win = sigmoid(delta).\n6. Compute the focal modulating factor: modulating_factor = (1 - p_win)^dynamic_gamma.\n7. Compute the base margin from the cost rank gap: base_margin = beta * rank_gap(cost_a, cost_b).\n8. Calculate model uncertainty: uncertainty = 1.0 - abs(p_win - 0.5).\n9. Calculate a dynamic clipping threshold for the margin: dynamic_margin_max = margin_max_base * (1.0 + uncertainty.detach()).\n10. Clip the base margin using the dynamic threshold: margin = clamp(base_margin, min=0.0, max=dynamic_margin_max).\n11. Compute the core preference loss: loss_core = -logsigmoid(normalized_delta + margin).\n12. Apply the detached modulating factor: final_loss = modulating_factor.detach() * loss_core.\n13. Return the mean of the final loss.", "hyperparams": {"beta": 1.0, "gamma_base": 1.5, "margin_max_base": 2.5}, "operators_used": ["zscore", "softplus", "sigmoid", "logsigmoid", "rank_gap", "clamp"], "implementation_hint": {"expects": ["cost_a", "cost_b", "log_prob_w", "log_prob_l"], "returns": "scalar"}, "code": "def generated_loss(batch, model_output, extra):\n    \"\"\"\n    Combines a cost-sensitive dynamic gamma with a novel uncertainty-clipped margin.\n    \"\"\"\n    # Read hyperparameters with defaults\n    beta = extra.get('beta', 1.0)\n    gamma_base = extra.get('gamma_base', 1.5)\n    margin_max_base = extra.get('margin_max_base', 2.5)\n\n    # Read required tensors from the batch\n    cost_a = batch['cost_a']\n    cost_b = batch['cost_b']\n    log_prob_w = batch['log_prob_w']\n    log_prob_l = batch['log_prob_l']\n    weight = batch.get('weight')\n\n    # 1. Calculate and normalize the log-probability difference (Inherited)\n    delta = log_prob_w - log_prob_l\n    normalized_delta = ops.zscore(delta)\n\n    # 2. Calculate dynamic, cost-sensitive gamma (Inherited from Parent 2)\n    cost_gap = cost_b - cost_a\n    cost_gap_zscored = ops.zscore(cost_gap)\n    dynamic_gamma = gamma_base + F.softplus(cost_gap_zscored.detach())\n\n    # 3. Calculate probability for focal loss and uncertainty\n    p_win = torch.sigmoid(delta)\n\n    # 4. Compute the focal modulating factor (Inherited)\n    modulating_factor = torch.pow(1.0 - p_win, dynamic_gamma)\n\n    # 5. Compute the base margin using rank_gap (Inherited)\n    rank_gap = ops.rank_gap(cost_a, cost_b)\n    base_margin = beta * rank_gap\n\n    # 6. New Coupling: Dynamic Margin Clipping based on Uncertainty\n    # Uncertainty is highest when p_win is 0.5\n    uncertainty = 1.0 - torch.abs(p_win - 0.5)\n    # The max margin is smaller for confident predictions (low uncertainty)\n    # and larger for uncertain ones. Detach to use as a pure regularizer.\n    dynamic_margin_max = margin_max_base * (1.0 + uncertainty.detach())\n    \n    # Fix for E_FORWARD_ERROR: torch.clamp requires min and max to be both scalars or both tensors.\n    min_tensor = torch.full_like(base_margin, 0.0)\n    margin = torch.clamp(base_margin, min=min_tensor, max=dynamic_margin_max)\n\n    # 7. Compute the core preference loss\n    loss_core = -F.logsigmoid(normalized_delta + margin)\n\n    # 8. Apply the modulating factor to the core loss\n    final_loss = modulating_factor.detach() * loss_core\n\n    # Apply optional instance weights if provided\n    if weight is not None:\n        final_loss = final_loss * weight\n\n    # Return the mean loss over the batch\n    return final_loss.mean()", "theoretical_basis": ""}, "static_ok": true, "static_reason": "", "static_error_code": "", "dynamic_ok": true, "dynamic_reason": "ok", "loss_value": 0.06850215047597885, "grad_norm": 0.0, "pref_ok": true, "pref_reason": "ok", "pref_mono_pass_rate": 0.7578125, "pref_swap_pass_rate": 1.0, "pref_gap_pass_rate": 1.0}
{"generation": 7, "index": 2, "attempt": 0, "ir": {"name": "FocalLossWithConfidencePenaltyMargin", "intuition": "Mode: explore. This loss function hybridizes dynamic focal modulation with a novel margin calculation that penalizes overconfidence on difficult pairs. The goal is to create a loss that not only focuses on hard examples but also encourages a well-calibrated model by discouraging certainty when the cost difference is small.\n\nInherited Ideas:\n1. From `FocalLossWithDynamicCostSensitiveGamma`: The idea of a dynamic focal strength (`gamma`) is inherited. However, instead of being based on the z-scored cost gap, it is simplified to be based on the raw cost gap, making it more robust to batch composition. This couples the 'hard example' re-weighting directly to the absolute difficulty of a pair.\n2. From `AdaptiveFocalLogsigmoidWithClippedMargin`: The use of a focal-modulated `logsigmoid` loss on a z-scored log-probability difference (`delta`) is retained. This provides a stable and effective baseline for preference learning that emphasizes misclassified examples.\n\nNew Coupling Ideas:\n1. **Confidence-Penalty Margin**: The margin is designed to be large for easy pairs (large cost gap) but is penalized when the model is overconfident on difficult pairs (small cost gap). It's calculated as `margin = beta * softplus(rank_gap - confidence_penalty)`. The `confidence_penalty` is `(1 - p_win)` for pairs with a below-median cost gap, and zero otherwise. This means for 'hard' pairs, the margin shrinks as the model becomes more confident (p_win -> 1), forcing the model to push `delta` even higher to satisfy the learning objective and preventing it from becoming complacent on pairs that are inherently ambiguous.\n2. **Simplified Dynamic Gamma**: The focal strength `gamma` is now `gamma_base + softplus(cost_gap.detach() / cost_gap.mean().detach())`. This simplifies the parent's z-score approach by using a simple ratio to the batch mean, which is less sensitive to outliers and provides a more direct scaling of focal strength with the cost difference.", "pseudocode": "1. Calculate the log-probability difference: delta = log_prob_w - log_prob_l.\n2. Normalize delta across the batch for stability: normalized_delta = zscore(delta).\n3. Calculate the absolute cost difference: cost_gap = cost_b - cost_a.\n4. Compute a simplified dynamic focal strength: dynamic_gamma = gamma_base + softplus(cost_gap.detach() / cost_gap.mean().detach()).\n5. Calculate the model's confidence: p_win = sigmoid(delta).\n6. Compute the focal modulating factor: modulating_factor = (1 - p_win)^dynamic_gamma.\n7. Compute the cost rank gap: rank_gap = rank_gap(cost_a, cost_b).\n8. Identify difficult pairs (cost_gap below batch median): is_hard_pair = (cost_gap < cost_gap.median()).\n9. Compute a confidence penalty for hard pairs only: confidence_penalty = where(is_hard_pair, 1 - p_win, 0).\n10. Calculate the confidence-penalized margin: margin = beta * softplus(rank_gap - confidence_penalty.detach()).\n11. Compute the core loss: loss_core = -logsigmoid(normalized_delta + margin).\n12. Apply the detached modulating factor: final_loss = modulating_factor.detach() * loss_core.\n13. Return the mean of the final loss.", "hyperparams": {"beta": 1.0, "gamma_base": 1.5}, "operators_used": ["zscore", "softplus", "sigmoid", "logsigmoid", "rank_gap"], "implementation_hint": {"expects": ["cost_a", "cost_b", "log_prob_w", "log_prob_l"], "returns": "scalar"}, "code": "def generated_loss(batch, model_output, extra):\n    \"\"\"\n    Combines z-score normalization and focal loss with two new couplings:\n    1. A simplified dynamic focal gamma that scales with the cost gap relative to the batch mean.\n    2. A margin that is reduced by model confidence on difficult pairs (below-median cost gap).\n    \"\"\"\n    # Read hyperparameters with defaults\n    beta = extra.get('beta', 1.0)\n    gamma_base = extra.get('gamma_base', 1.5)\n\n    # Read required tensors from the batch\n    cost_a = batch['cost_a']\n    cost_b = batch['cost_b']\n    log_prob_w = batch['log_prob_w']\n    log_prob_l = batch['log_prob_l']\n    weight = batch.get('weight')\n\n    # 1. Calculate and normalize the log-probability difference (Inherited)\n    delta = log_prob_w - log_prob_l\n    normalized_delta = ops.zscore(delta)\n\n    # 2. Calculate cost gap for dynamic gamma and margin penalty\n    cost_gap = cost_b - cost_a\n\n    # 3. New Coupling: Simplified Dynamic Gamma\n    # Scale gamma based on cost_gap's ratio to the batch mean. Detach to avoid complex gradients.\n    # Add a small epsilon to avoid division by zero in case of a uniform-cost batch.\n    mean_cost_gap = cost_gap.mean().detach() + 1e-8\n    dynamic_gamma = gamma_base + F.softplus(cost_gap.detach() / mean_cost_gap)\n\n    # 4. Calculate probability for focal loss and margin modulation\n    p_win = torch.sigmoid(delta)\n\n    # 5. Compute the focal modulating factor\n    modulating_factor = torch.pow(1.0 - p_win, dynamic_gamma)\n\n    # 6. New Coupling: Confidence-Penalty Margin\n    # Inherit rank_gap as the base for the margin\n    rank_gap = ops.rank_gap(cost_a, cost_b)\n    \n    # Identify hard pairs as those with a cost gap below the batch median\n    with torch.no_grad():\n      is_hard_pair = cost_gap < torch.median(cost_gap)\n    \n    # Calculate a penalty for hard pairs that shrinks as confidence (p_win) grows\n    # The penalty is zero for easy pairs.\n    confidence_penalty = torch.where(is_hard_pair, 1.0 - p_win, torch.tensor(0.0, device=p_win.device))\n    \n    # The margin is rank_gap, but reduced for overconfident hard pairs.\n    # Detach penalty to keep margin's effect on gradient direction simple.\n    margin = beta * F.softplus(rank_gap - confidence_penalty.detach())\n\n    # 7. Compute the core preference loss\n    loss_core = -F.logsigmoid(normalized_delta + margin)\n\n    # 8. Apply the modulating factor to the core loss (Inherited)\n    final_loss = modulating_factor.detach() * loss_core\n\n    # Apply optional instance weights if provided\n    if weight is not None:\n        final_loss = final_loss * weight\n\n    # Return the mean loss over the batch\n    return final_loss.mean()", "theoretical_basis": "A dynamically modulated Bradley-Terry preference model. The core is a logistic loss on z-scored log-probabilities. It features two couplings: (1) a focal modulation where the strength (`gamma`) scales with the cost difference relative to the batch mean, and (2) a margin that is penalized by model confidence (`p_win`) specifically on pairs with small cost differences, discouraging overconfidence on ambiguous examples."}, "static_ok": true, "static_reason": "", "static_error_code": "", "dynamic_ok": true, "dynamic_reason": "ok", "loss_value": 0.033887121826410294, "grad_norm": 0.0, "pref_ok": true, "pref_reason": "ok", "pref_mono_pass_rate": 0.7513020833333334, "pref_swap_pass_rate": 1.0, "pref_gap_pass_rate": 1.0}
{"generation": 7, "index": 3, "attempt": 0, "ir": {"name": "FocalLossWithDynamicGammaAndClippedMargin", "intuition": "Mode: explore. This loss function combines robust ideas from both parentsfocal modulation and z-score normalizationwith a novel coupling of the focal strength to the cost landscape and a simplified, stable margin.\n\nInherited Ideas:\n1. From both parents: The core structure is a focal-modulated Bradley-Terry loss: `modulating_factor * -logsigmoid(normalized_delta + margin)`. This is a proven technique for emphasizing hard-to-classify pairs.\n2. From both parents: It inherits the use of `ops.zscore` on the log-probability difference (`delta`) to stabilize the loss across different batches and prevent outliers from dominating.\n\nNew Coupling Ideas:\n1. **Cost-Rank-Sensitive Gamma**: The focal strength `gamma` is dynamically calculated based on the *normalized rank* of the cost gap. We compute `cost_gap = cost_b - cost_a`, normalize its rank from 0 to 1, and then scale it. The final gamma is `gamma_base + gamma_scale * rank_of_cost_gap`. This couples the focal strength directly to the relative difficulty of the pair within the batch: pairs with a larger cost difference (which should be easier but are more important to get right) receive a stronger focal penalty if the model misclassifies them. Using rank instead of z-score on the raw cost gap is more robust to outliers in the cost distribution.\n2. **Simplified Clipped Margin**: It simplifies the complex margin calculations from the parents into a single, stable `clamp(beta * rank_gap, max=margin_max)`. This inherits the adaptive nature of `rank_gap` but adds a ceiling (`margin_max`) to prevent extremely large margins from causing vanishing gradients for easy examples, a stability trick learned from Parent 1. This avoids the uncertainty-based modulation of Parent 2, which could be noisy, in favor of a more direct and stable approach.", "pseudocode": "1. Calculate the log-probability difference: delta = log_prob_w - log_prob_l.\n2. Normalize delta across the batch for stability: normalized_delta = zscore(delta).\n3. Calculate the absolute cost difference: cost_gap = cost_b - cost_a.\n4. Compute the normalized rank of the cost gap (0 to 1): cost_gap_rank = rank(cost_gap) / (N-1).\n5. Compute a dynamic, cost-rank-sensitive focal strength: dynamic_gamma = gamma_base + gamma_scale * cost_gap_rank.detach().\n6. Calculate the model's win probability: p_win = sigmoid(delta).\n7. Compute the focal modulating factor using the dynamic gamma: modulating_factor = (1 - p_win)^dynamic_gamma.\n8. Compute the cost rank gap for the margin: rank_gap = rank_gap(cost_a, cost_b).\n9. Compute a simple, clipped, rank-based margin: margin = clamp(beta * rank_gap, max=margin_max).\n10. Compute the core preference loss: loss_core = -logsigmoid(normalized_delta + margin).\n11. Apply the detached modulating factor to the core loss: final_loss = modulating_factor.detach() * loss_core.\n12. Return the mean of the final loss.", "hyperparams": {"beta": 1.0, "gamma_base": 1.0, "gamma_scale": 2.0, "margin_max": 5.0}, "operators_used": ["zscore", "sigmoid", "logsigmoid", "rank_gap", "clamp"], "implementation_hint": {"expects": ["cost_a", "cost_b", "log_prob_w", "log_prob_l"], "returns": "scalar"}, "code": "def generated_loss(batch, model_output, extra):\n    \"\"\"\n    Combines z-score normalization and focal loss with two couplings:\n    1. A dynamic focal gamma that scales with the rank of the cost gap.\n    2. A simple, clipped margin based on the cost rank gap for stability.\n    \"\"\"\n    # Read hyperparameters with defaults\n    beta = extra.get('beta', 1.0)\n    gamma_base = extra.get('gamma_base', 1.0)\n    gamma_scale = extra.get('gamma_scale', 2.0)\n    margin_max = extra.get('margin_max', 5.0)\n\n    # Read required tensors from the batch\n    cost_a = batch['cost_a']\n    cost_b = batch['cost_b']\n    log_prob_w = batch['log_prob_w']\n    log_prob_l = batch['log_prob_l']\n    weight = batch.get('weight')\n\n    # 1. Calculate the log-probability difference\n    delta = log_prob_w - log_prob_l\n\n    # 2. Inherited: Normalize delta for stability\n    normalized_delta = ops.zscore(delta)\n\n    # 3. New Coupling: Cost-Rank-Sensitive Gamma\n    cost_gap = cost_b - cost_a\n    # Use rank for robustness to outliers in cost distribution.\n    # The ranks will be 0, 1, ..., N-1. Normalize to [0, 1].\n    num_pairs = cost_gap.shape[0]\n    if num_pairs > 1:\n        cost_gap_rank = cost_gap.argsort().argsort().float() / (num_pairs - 1)\n    else:\n        cost_gap_rank = torch.zeros_like(cost_gap)\n    \n    # Gamma increases for pairs with a larger relative cost gap in the batch.\n    # Detach to prevent gradients from flowing through the gamma calculation itself.\n    dynamic_gamma = gamma_base + gamma_scale * cost_gap_rank.detach()\n\n    # 4. Calculate probability for focal loss\n    p_win = torch.sigmoid(delta)\n\n    # 5. Compute the focal modulating factor\n    modulating_factor = torch.pow(1.0 - p_win, dynamic_gamma)\n\n    # 6. Inherited: Compute rank_gap for the margin\n    rank_gap = ops.rank_gap(cost_a, cost_b)\n\n    # 7. New Coupling: Simplified Clipped Margin\n    # Clip the margin to prevent saturation and vanishing gradients.\n    margin = torch.clamp(beta * rank_gap, max=margin_max)\n\n    # 8. Compute the core preference loss\n    loss_core = -F.logsigmoid(normalized_delta + margin)\n\n    # 9. Apply the modulating factor to the core loss\n    # Detach the factor so it only re-weights the loss, not change the gradient direction.\n    final_loss = modulating_factor.detach() * loss_core\n\n    # Apply optional instance weights if provided\n    if weight is not None:\n        final_loss = final_loss * weight\n\n    # Return the mean loss over the batch\n    return final_loss.mean()", "theoretical_basis": "A dynamically modulated Bradley-Terry preference model. The core loss is logistic, operating on a z-scored log-probability difference plus a clipped, rank-adaptive margin. This is coupled with a focal modulation where the focal strength (gamma) is dynamically determined by the normalized rank of the cost gap within the batch. This design focuses learning on pairs that are both misclassified and have a large cost separation, using a rank-based mechanism for robustness."}, "static_ok": true, "static_reason": "", "static_error_code": "", "dynamic_ok": true, "dynamic_reason": "ok", "loss_value": 0.08561770617961884, "grad_norm": 0.0, "pref_ok": true, "pref_reason": "ok", "pref_mono_pass_rate": 0.75390625, "pref_swap_pass_rate": 1.0, "pref_gap_pass_rate": 1.0}
{"generation": 7, "index": 4, "attempt": 0, "ir": {"name": "CostSensitiveFocalLossWithClippedDynamicMargin", "intuition": "Mode: combine. This loss function hybridizes the strongest ideas from its parents: the stable, clipped margin from Parent 1 and the cost-sensitive focal modulation from Parent 2. The goal is to create a robust loss that prevents gradient vanishing on easy examples (via clipping) while intensely focusing on misclassified pairs that have a large, meaningful cost difference.\n\nInherited Ideas:\n1. From `AdaptiveFocalLogsigmoidWithClippedMargin` (Parent 1): We inherit the **clipped, rank-based margin**: `clamp(beta * rank_gap, min=0, max=margin_max)`. This is a crucial stability trick that prevents the margin from growing uncontrollably for pairs with huge cost differences, which could otherwise cause the `logsigmoid` to saturate and gradients to vanish for most of the batch.\n2. From `FocalLossWithDynamicCostSensitiveGamma` (Parent 2): We inherit the **cost-sensitive dynamic gamma**. The focal strength `gamma` is modulated by the z-scored cost gap: `gamma_base + softplus(zscore(cost_b - cost_a))`. This intelligently directs the model's focus, applying a stronger penalty when it misclassifies pairs that should be easy to distinguish based on their large cost separation.\n\nNew Coupling Idea:\n1. **Confidence-Weighted Margin Scaling**: We introduce a new coupling where the effective margin is scaled by the model's confidence. The base clipped margin is multiplied by `(1 + alpha * (1 - p_win))`, where `p_win` is the model's predicted probability for the winner. This means when the model is very confident (`p_win` -> 1), the margin scaling factor approaches 1, applying the standard clipped margin. However, when the model is uncertain (`p_win` -> 0), the scaling factor increases towards `1 + alpha`, effectively increasing the margin target. This forces the model to create a larger separation for pairs it is struggling with, coupling the margin's magnitude directly to model uncertainty in a smooth, differentiable way.", "pseudocode": "1. Calculate the log-probability difference: delta = log_prob_w - log_prob_l.\n2. Normalize delta across the batch for stability: normalized_delta = zscore(delta).\n3. Calculate the absolute cost difference and normalize it: cost_gap_zscored = zscore(cost_b - cost_a).\n4. Inherit from Parent 2: Compute a dynamic, cost-sensitive focal strength: dynamic_gamma = gamma_base + softplus(cost_gap_zscored.detach()).\n5. Calculate the model's win probability: p_win = sigmoid(delta).\n6. Compute the focal modulating factor using the dynamic gamma: modulating_factor = (1 - p_win)^dynamic_gamma.\n7. Inherit from Parent 1: Compute a base clipped, rank-based margin: base_margin = clamp(beta * rank_gap(cost_a, cost_b), min=0, max=margin_max).\n8. New Coupling: Compute a confidence-based scaling factor for the margin: margin_scale = 1.0 + alpha * (1.0 - p_win.detach()).\n9. Calculate the final dynamic margin: final_margin = base_margin * margin_scale.\n10. Compute the core preference loss: loss_core = -logsigmoid(normalized_delta + final_margin).\n11. Apply the detached modulating factor to the core loss: final_loss = modulating_factor.detach() * loss_core.\n12. Return the mean of the final loss.", "hyperparams": {"beta": 1.5, "gamma_base": 1.5, "margin_max": 3.0, "alpha": 0.5}, "operators_used": ["zscore", "softplus", "sigmoid", "logsigmoid", "rank_gap", "clamp"], "implementation_hint": {"expects": ["cost_a", "cost_b", "log_prob_w", "log_prob_l"], "returns": "scalar"}, "code": "def generated_loss(batch, model_output, extra):\n    \"\"\"\n    Combines a clipped margin (Parent 1) with a cost-sensitive dynamic gamma (Parent 2).\n    Introduces a new coupling where the margin is scaled by model uncertainty.\n    \"\"\"\n    # Read hyperparameters with defaults\n    beta = extra.get('beta', 1.5)\n    gamma_base = extra.get('gamma_base', 1.5)\n    margin_max = extra.get('margin_max', 3.0)\n    alpha = extra.get('alpha', 0.5)\n\n    # Read required tensors from the batch\n    cost_a = batch['cost_a']\n    cost_b = batch['cost_b']\n    log_prob_w = batch['log_prob_w']\n    log_prob_l = batch['log_prob_l']\n    weight = batch.get('weight')\n\n    # 1. Calculate the log-probability difference and normalize\n    delta = log_prob_w - log_prob_l\n    normalized_delta = ops.zscore(delta)\n\n    # 2. Inherited from Parent 2: Cost-Sensitive Dynamic Gamma\n    cost_gap = cost_b - cost_a\n    cost_gap_zscored = ops.zscore(cost_gap)\n    dynamic_gamma = gamma_base + F.softplus(cost_gap_zscored.detach())\n\n    # 3. Calculate probability for focal loss and margin modulation\n    p_win = torch.sigmoid(delta)\n\n    # 4. Compute the focal modulating factor\n    modulating_factor = torch.pow(1.0 - p_win, dynamic_gamma)\n\n    # 5. Inherited from Parent 1: Clipped, rank-based margin\n    rank_gap = ops.rank_gap(cost_a, cost_b)\n    base_margin = torch.clamp(beta * rank_gap, min=0, max=margin_max)\n\n    # 6. New Coupling: Confidence-Weighted Margin Scaling\n    # When model is certain (p_win -> 1), scale is ~1.\n    # When model is uncertain (p_win -> 0), scale is ~(1+alpha).\n    margin_scale = 1.0 + alpha * (1.0 - p_win.detach())\n    final_margin = base_margin * margin_scale\n\n    # 7. Compute the core preference loss\n    loss_core = -F.logsigmoid(normalized_delta + final_margin)\n\n    # 8. Apply the modulating factor to the core loss\n    final_loss = modulating_factor.detach() * loss_core\n\n    # Apply optional instance weights if provided\n    if weight is not None:\n        final_loss = final_loss * weight\n\n    # Return the mean loss over the batch\n    return final_loss.mean()", "theoretical_basis": "A dynamically modulated Bradley-Terry preference model. The loss combines a clipped, rank-based margin with a focal modulation whose strength is determined by the batch-normalized cost gap. A novel coupling is introduced by further scaling the margin based on model confidence, increasing the separation target for pairs where the model is uncertain. This creates a multi-faceted approach to hard example mining, considering cost separation, model confidence, and rank-based difficulty simultaneously."}, "static_ok": true, "static_reason": "", "static_error_code": "", "dynamic_ok": true, "dynamic_reason": "ok", "loss_value": 0.03119930811226368, "grad_norm": 0.0, "pref_ok": false, "pref_reason": "pref_semantic_violation", "pref_mono_pass_rate": 0.7376302083333334, "pref_swap_pass_rate": 1.0, "pref_gap_pass_rate": 0.3333333333333333, "dynamic_error_code": "E_PREF_SEMANTIC"}
{"generation": 7, "index": 4, "attempt": 1, "ir": {"name": "CostSensitiveFocalLossWithClippedDynamicMargin", "intuition": "Repaired: The original loss failed the `E_PREF_SEMANTIC` gate, specifically the `mono_pass_rate` and `gap_pass_rate` checks. This was likely caused by using the z-scored `normalized_delta` in the main loss term `-logsigmoid(normalized_delta + final_margin)`. Z-scoring breaks the direct monotonic relationship between `log_prob_w` and the loss, as a sample's loss value becomes dependent on the statistics of the entire batch. To fix this, I replaced `normalized_delta` with the original `delta` in the `logsigmoid` calculation. The z-scoring of `cost_gap` is retained as it is a reasonable way to normalize the input to `softplus` for the dynamic gamma calculation.", "pseudocode": "1. Calculate the log-probability difference: delta = log_prob_w - log_prob_l.\n2. Calculate the absolute cost difference and normalize it: cost_gap_zscored = zscore(cost_b - cost_a).\n3. Inherit from Parent 2: Compute a dynamic, cost-sensitive focal strength: dynamic_gamma = gamma_base + softplus(cost_gap_zscored.detach()).\n4. Calculate the model's win probability: p_win = sigmoid(delta).\n5. Compute the focal modulating factor using the dynamic gamma: modulating_factor = (1 - p_win)^dynamic_gamma.\n6. Inherit from Parent 1: Compute a base clipped, rank-based margin: base_margin = clamp(beta * rank_gap(cost_a, cost_b), min=0, max=margin_max).\n7. New Coupling: Compute a confidence-based scaling factor for the margin: margin_scale = 1.0 + alpha * (1.0 - p_win.detach()).\n8. Calculate the final dynamic margin: final_margin = base_margin * margin_scale.\n9. Compute the core preference loss using the original delta: loss_core = -logsigmoid(delta + final_margin).\n10. Apply the detached modulating factor to the core loss: final_loss = modulating_factor.detach() * loss_core.\n11. Return the mean of the final loss.", "hyperparams": {"beta": 1.5, "gamma_base": 1.5, "margin_max": 3.0, "alpha": 0.5}, "operators_used": ["zscore", "softplus", "sigmoid", "logsigmoid", "rank_gap", "clamp"], "implementation_hint": {"expects": ["cost_a", "cost_b", "log_prob_w", "log_prob_l"], "returns": "scalar"}, "code": "def generated_loss(batch, model_output, extra):\n    \"\"\"\n    Combines a clipped margin (Parent 1) with a cost-sensitive dynamic gamma (Parent 2).\n    Introduces a new coupling where the margin is scaled by model uncertainty.\n    \"\"\"\n    # Read hyperparameters with defaults\n    beta = extra.get('beta', 1.5)\n    gamma_base = extra.get('gamma_base', 1.5)\n    margin_max = extra.get('margin_max', 3.0)\n    alpha = extra.get('alpha', 0.5)\n\n    # Read required tensors from the batch\n    cost_a = batch['cost_a']\n    cost_b = batch['cost_b']\n    log_prob_w = batch['log_prob_w']\n    log_prob_l = batch['log_prob_l']\n    weight = batch.get('weight')\n\n    # 1. Calculate the log-probability difference\n    delta = log_prob_w - log_prob_l\n\n    # 2. Inherited from Parent 2: Cost-Sensitive Dynamic Gamma\n    cost_gap = cost_b - cost_a\n    cost_gap_zscored = ops.zscore(cost_gap)\n    dynamic_gamma = gamma_base + F.softplus(cost_gap_zscored.detach())\n\n    # 3. Calculate probability for focal loss and margin modulation\n    p_win = torch.sigmoid(delta)\n\n    # 4. Compute the focal modulating factor\n    modulating_factor = torch.pow(1.0 - p_win, dynamic_gamma)\n\n    # 5. Inherited from Parent 1: Clipped, rank-based margin\n    rank_gap = ops.rank_gap(cost_a, cost_b)\n    base_margin = torch.clamp(beta * rank_gap, min=0, max=margin_max)\n\n    # 6. New Coupling: Confidence-Weighted Margin Scaling\n    # When model is certain (p_win -> 1), scale is ~1.\n    # When model is uncertain (p_win -> 0), scale is ~(1+alpha).\n    margin_scale = 1.0 + alpha * (1.0 - p_win.detach())\n    final_margin = base_margin * margin_scale\n\n    # 7. Compute the core preference loss (using original delta for semantic correctness)\n    loss_core = -F.logsigmoid(delta + final_margin)\n\n    # 8. Apply the modulating factor to the core loss\n    final_loss = modulating_factor.detach() * loss_core\n\n    # Apply optional instance weights if provided\n    if weight is not None:\n        final_loss = final_loss * weight\n\n    # Return the mean loss over the batch\n    return final_loss.mean()", "theoretical_basis": ""}, "static_ok": true, "static_reason": "", "static_error_code": "", "dynamic_ok": true, "dynamic_reason": "ok", "loss_value": 0.03119930811226368, "grad_norm": 0.0, "pref_ok": false, "pref_reason": "pref_semantic_violation", "pref_mono_pass_rate": 1.0, "pref_swap_pass_rate": 1.0, "pref_gap_pass_rate": 0.5, "dynamic_error_code": "E_PREF_SEMANTIC"}
{"generation": 7, "index": 4, "attempt": 2, "ir": {"name": "CostSensitiveFocalLossWithClippedDynamicMargin", "intuition": "The loss failed the `E_PREF_SEMANTIC` gate, specifically the `gap_pass_rate` check, which means the loss did not consistently decrease as the cost gap between the chosen and rejected solutions increased. This was likely caused by the `ops.rank_gap` function, whose output can be sensitive to the distribution of costs within a batch, breaking the expected monotonic relationship for this specific test. To fix this, I replaced `ops.rank_gap(cost_a, cost_b)` with the direct, unnormalized cost difference `cost_b - cost_a` when calculating the `base_margin`. This ensures that a larger cost difference directly translates to a larger margin, satisfying the gate's semantic requirement, while keeping the core focal and dynamic margin structure intact.", "pseudocode": "1. Calculate the log-probability difference: delta = log_prob_w - log_prob_l.\n2. Calculate the absolute cost difference and normalize it: cost_gap_zscored = zscore(cost_b - cost_a).\n3. Compute a dynamic, cost-sensitive focal strength: dynamic_gamma = gamma_base + softplus(cost_gap_zscored.detach()).\n4. Calculate the model's win probability: p_win = sigmoid(delta).\n5. Compute the focal modulating factor using the dynamic gamma: modulating_factor = (1 - p_win)^dynamic_gamma.\n6. Compute a base clipped margin using the raw cost gap: base_margin = clamp(beta * (cost_b - cost_a), min=0, max=margin_max).\n7. Compute a confidence-based scaling factor for the margin: margin_scale = 1.0 + alpha * (1.0 - p_win.detach()).\n8. Calculate the final dynamic margin: final_margin = base_margin * margin_scale.\n9. Compute the core preference loss: loss_core = -logsigmoid(delta + final_margin).\n10. Apply the detached modulating factor to the core loss: final_loss = modulating_factor.detach() * loss_core.\n11. Return the mean of the final loss.", "hyperparams": {"beta": 1.5, "gamma_base": 1.5, "margin_max": 3.0, "alpha": 0.5}, "operators_used": ["zscore", "softplus", "sigmoid", "logsigmoid", "clamp"], "implementation_hint": {"expects": ["cost_a", "cost_b", "log_prob_w", "log_prob_l"], "returns": "scalar"}, "code": "def generated_loss(batch, model_output, extra):\n    \"\"\"\n    Combines a clipped margin (Parent 1) with a cost-sensitive dynamic gamma (Parent 2).\n    Introduces a new coupling where the margin is scaled by model uncertainty.\n    \"\"\"\n    # Read hyperparameters with defaults\n    beta = extra.get('beta', 1.5)\n    gamma_base = extra.get('gamma_base', 1.5)\n    margin_max = extra.get('margin_max', 3.0)\n    alpha = extra.get('alpha', 0.5)\n\n    # Read required tensors from the batch\n    cost_a = batch['cost_a']\n    cost_b = batch['cost_b']\n    log_prob_w = batch['log_prob_w']\n    log_prob_l = batch['log_prob_l']\n    weight = batch.get('weight')\n\n    # 1. Calculate the log-probability difference\n    delta = log_prob_w - log_prob_l\n\n    # 2. Inherited from Parent 2: Cost-Sensitive Dynamic Gamma\n    cost_gap = cost_b - cost_a\n    cost_gap_zscored = ops.zscore(cost_gap)\n    dynamic_gamma = gamma_base + F.softplus(cost_gap_zscored.detach())\n\n    # 3. Calculate probability for focal loss and margin modulation\n    p_win = torch.sigmoid(delta)\n\n    # 4. Compute the focal modulating factor\n    modulating_factor = torch.pow(1.0 - p_win, dynamic_gamma)\n\n    # 5. Inherited from Parent 1: Clipped, cost-based margin\n    # REPAIR: Replaced rank_gap with raw cost_gap to ensure monotonicity for pref_semantic gate.\n    base_margin = torch.clamp(beta * cost_gap, min=0, max=margin_max)\n\n    # 6. New Coupling: Confidence-Weighted Margin Scaling\n    # When model is certain (p_win -> 1), scale is ~1.\n    # When model is uncertain (p_win -> 0), scale is ~(1+alpha).\n    margin_scale = 1.0 + alpha * (1.0 - p_win.detach())\n    final_margin = base_margin * margin_scale\n\n    # 7. Compute the core preference loss\n    loss_core = -F.logsigmoid(delta + final_margin)\n\n    # 8. Apply the modulating factor to the core loss\n    final_loss = modulating_factor.detach() * loss_core\n\n    # Apply optional instance weights if provided\n    if weight is not None:\n        final_loss = final_loss * weight\n\n    # Return the mean loss over the batch\n    return final_loss.mean()", "theoretical_basis": ""}, "static_ok": true, "static_reason": "", "static_error_code": "", "dynamic_ok": true, "dynamic_reason": "ok", "loss_value": 0.03119930811226368, "grad_norm": 0.0, "pref_ok": false, "pref_reason": "pref_semantic_violation", "pref_mono_pass_rate": 1.0, "pref_swap_pass_rate": 1.0, "pref_gap_pass_rate": 0.5, "dynamic_error_code": "E_PREF_SEMANTIC"}
{"generation": 7, "index": 5, "attempt": 0, "ir": {"name": "ProgressiveFocalLossWithDynamicMargin", "intuition": "Mode: explore. This loss function hybridizes the strengths of its parentsfocal modulation and dynamic marginsand introduces two new coupling ideas to create a more robust and adaptive learning signal. The core idea is to progressively increase the difficulty for the model by coupling the focal strength to the model's own confidence, while simultaneously adjusting the margin based on the relative cost difference within the batch.\n\nInherited Ideas:\n1.  From `AdaptiveFocalLogsigmoidWithClippedMargin` & `FocalLossWithDynamicCostSensitiveGamma`: The foundational structure of a focal-modulated Bradley-Terry loss: `modulating_factor * -logsigmoid(normalized_delta + margin)`. This emphasizes hard examples.\n2.  From both parents: The use of `ops.zscore` on the log-probability difference (`delta = log_prob_w - log_prob_l`) is retained as a crucial stability trick to normalize the model's output distribution across batches.\n\nNew Coupling Ideas:\n1.  **Confidence-Modulated Focal Strength**: The focal strength `gamma` is dynamically adjusted based on the model's confidence (`p_win`). Specifically, `dynamic_gamma = gamma_base + gamma_scale * p_win.detach()`. As the model becomes more confident about a correct preference (p_win approaches 1), the focal strength increases. This creates a 'progressive hardening' effect: the loss demands more from the model on examples it already finds easy, pushing for even greater separation and preventing complacency.\n2.  **Normalized Cost-Gap Margin**: The margin is directly coupled to the normalized cost difference. We compute `cost_gap = cost_b - cost_a` and then normalize it using `zscore`. The margin is then `margin = beta * softplus(zscored_cost_gap)`. This makes the separation target adaptive to the batch's cost distribution: pairs with a larger-than-average cost difference are given a larger margin, requiring the model to produce a more confident log-probability gap for them.", "pseudocode": "1. Calculate the log-probability difference: delta = log_prob_w - log_prob_l.\n2. Normalize delta across the batch for stability: normalized_delta = zscore(delta).\n3. Calculate the model's confidence (probability of preferring the winner): p_win = sigmoid(delta).\n4. Compute a dynamic, confidence-modulated focal strength: dynamic_gamma = gamma_base + gamma_scale * p_win.detach().\n5. Compute the focal modulating factor using the dynamic gamma: modulating_factor = (1 - p_win)^dynamic_gamma.\n6. Calculate the absolute cost difference: cost_gap = cost_b - cost_a.\n7. Normalize the cost gap across the batch: zscored_cost_gap = zscore(cost_gap).\n8. Compute a margin based on the normalized cost gap: margin = beta * softplus(zscored_cost_gap.detach()).\n9. Compute the core preference loss: loss_core = -logsigmoid(normalized_delta + margin).\n10. Apply the detached modulating factor to the core loss: final_loss = modulating_factor.detach() * loss_core.\n11. Return the mean of the final loss.", "hyperparams": {"beta": 1.0, "gamma_base": 1.0, "gamma_scale": 1.5}, "operators_used": ["zscore", "sigmoid", "softplus", "logsigmoid"], "implementation_hint": {"expects": ["cost_a", "cost_b", "log_prob_w", "log_prob_l"], "returns": "scalar"}, "code": "def generated_loss(batch, model_output, extra):\n    \"\"\"\n    Combines z-score normalization and focal loss with two new couplings:\n    1. A progressive focal gamma that scales with model confidence (p_win).\n    2. An adaptive margin that scales with the z-scored cost gap.\n    \"\"\"\n    # Read hyperparameters with defaults\n    beta = extra.get('beta', 1.0)\n    gamma_base = extra.get('gamma_base', 1.0)\n    gamma_scale = extra.get('gamma_scale', 1.5)\n\n    # Read required tensors from the batch\n    cost_a = batch['cost_a']\n    cost_b = batch['cost_b']\n    log_prob_w = batch['log_prob_w']\n    log_prob_l = batch['log_prob_l']\n    weight = batch.get('weight')\n\n    # 1. Calculate the log-probability difference\n    delta = log_prob_w - log_prob_l\n\n    # 2. Inherited: Normalize delta for stability\n    normalized_delta = ops.zscore(delta)\n\n    # 3. Calculate probability for focal loss modulation\n    p_win = torch.sigmoid(delta)\n\n    # 4. New Coupling 1: Confidence-Modulated Focal Strength\n    # As p_win -> 1, gamma increases, making the loss focus more on perfecting easy examples.\n    # Detach p_win to prevent gradients from flowing through the gamma calculation itself.\n    dynamic_gamma = gamma_base + gamma_scale * p_win.detach()\n    modulating_factor = torch.pow(1.0 - p_win, dynamic_gamma)\n\n    # 5. New Coupling 2: Normalized Cost-Gap Margin\n    # The margin is proportional to how much larger the cost gap is than the batch average.\n    cost_gap = cost_b - cost_a\n    zscored_cost_gap = ops.zscore(cost_gap)\n    # Use softplus to ensure margin is non-negative. Detach to treat margin as a target.\n    margin = beta * F.softplus(zscored_cost_gap.detach())\n\n    # 6. Compute the core preference loss\n    loss_core = -F.logsigmoid(normalized_delta + margin)\n\n    # 7. Apply the modulating factor to the core loss\n    # Detach the factor so it only re-weights the loss, not change the gradient direction\n    final_loss = modulating_factor.detach() * loss_core\n\n    # Apply optional instance weights if provided\n    if weight is not None:\n        final_loss = final_loss * weight\n\n    # Return the mean loss over the batch\n    return final_loss.mean()", "theoretical_basis": "A dynamically modulated Bradley-Terry preference model. The loss combines a logistic preference term with two novel couplings. First, the focal strength is coupled with the model's own confidence, creating a 'progressive hardening' curriculum. Second, the separation margin is coupled with the batch-normalized cost gap, making the required log-probability difference proportional to the relative difficulty of the preference pair within the current batch."}, "static_ok": true, "static_reason": "", "static_error_code": "", "dynamic_ok": true, "dynamic_reason": "ok", "loss_value": 0.12054549902677536, "grad_norm": 0.0, "pref_ok": false, "pref_reason": "pref_semantic_violation", "pref_mono_pass_rate": 0.744140625, "pref_swap_pass_rate": 0.5, "pref_gap_pass_rate": 1.0, "dynamic_error_code": "E_PREF_SEMANTIC"}
{"generation": 7, "index": 5, "attempt": 1, "ir": {"name": "ProgressiveFocalLossWithDynamicMargin", "intuition": "Repaired: The original loss failed the preference gate because `p_win` was calculated using the raw `delta` (log_prob_w - log_prob_l), while the core loss used a z-scored `normalized_delta`. This inconsistency meant the focal modulating factor did not correctly correspond to the probability of the argument inside the logsigmoid, violating the preference semantics. I have fixed this by calculating `p_win` using `normalized_delta`, ensuring the modulating factor correctly reflects the model's confidence on the same quantity the loss is acting upon. This aligns the focal modulation with the core loss term, resolving the E_PREF_SEMANTIC violation.", "pseudocode": "1. Calculate the log-probability difference: delta = log_prob_w - log_prob_l.\n2. Normalize delta across the batch for stability: normalized_delta = zscore(delta).\n3. Calculate the model's confidence on the normalized difference: p_win = sigmoid(normalized_delta).\n4. Compute a dynamic, confidence-modulated focal strength: dynamic_gamma = gamma_base + gamma_scale * p_win.detach().\n5. Compute the focal modulating factor using the dynamic gamma: modulating_factor = (1 - p_win)^dynamic_gamma.\n6. Calculate the absolute cost difference: cost_gap = cost_b - cost_a.\n7. Normalize the cost gap across the batch: zscored_cost_gap = zscore(cost_gap).\n8. Compute a margin based on the normalized cost gap: margin = beta * softplus(zscored_cost_gap.detach()).\n9. Compute the core preference loss: loss_core = -logsigmoid(normalized_delta + margin).\n10. Apply the detached modulating factor to the core loss: final_loss = modulating_factor.detach() * loss_core.\n11. Return the mean of the final loss.", "hyperparams": {"beta": 1.0, "gamma_base": 1.0, "gamma_scale": 1.5}, "operators_used": ["zscore", "sigmoid", "softplus", "logsigmoid"], "implementation_hint": {"expects": ["cost_a", "cost_b", "log_prob_w", "log_prob_l"], "returns": "scalar"}, "code": "def generated_loss(batch, model_output, extra):\n    \"\"\"\n    Combines z-score normalization and focal loss with two new couplings:\n    1. A progressive focal gamma that scales with model confidence (p_win).\n    2. An adaptive margin that scales with the z-scored cost gap.\n    \"\"\"\n    # Read hyperparameters with defaults\n    beta = extra.get('beta', 1.0)\n    gamma_base = extra.get('gamma_base', 1.0)\n    gamma_scale = extra.get('gamma_scale', 1.5)\n\n    # Read required tensors from the batch\n    cost_a = batch['cost_a']\n    cost_b = batch['cost_b']\n    log_prob_w = batch['log_prob_w']\n    log_prob_l = batch['log_prob_l']\n    weight = batch.get('weight')\n\n    # 1. Calculate the log-probability difference\n    delta = log_prob_w - log_prob_l\n\n    # 2. Inherited: Normalize delta for stability\n    normalized_delta = ops.zscore(delta)\n\n    # 3. Calculate probability for focal loss modulation\n    # FIX: Use normalized_delta to calculate p_win to match the loss core's input\n    p_win = torch.sigmoid(normalized_delta)\n\n    # 4. New Coupling 1: Confidence-Modulated Focal Strength\n    # As p_win -> 1, gamma increases, making the loss focus more on perfecting easy examples.\n    # Detach p_win to prevent gradients from flowing through the gamma calculation itself.\n    dynamic_gamma = gamma_base + gamma_scale * p_win.detach()\n    modulating_factor = torch.pow(1.0 - p_win, dynamic_gamma)\n\n    # 5. New Coupling 2: Normalized Cost-Gap Margin\n    # The margin is proportional to how much larger the cost gap is than the batch average.\n    cost_gap = cost_b - cost_a\n    zscored_cost_gap = ops.zscore(cost_gap)\n    # Use softplus to ensure margin is non-negative. Detach to treat margin as a target.\n    margin = beta * F.softplus(zscored_cost_gap.detach())\n\n    # 6. Compute the core preference loss\n    loss_core = -F.logsigmoid(normalized_delta + margin)\n\n    # 7. Apply the modulating factor to the core loss\n    # Detach the factor so it only re-weights the loss, not change the gradient direction\n    final_loss = modulating_factor.detach() * loss_core\n\n    # Apply optional instance weights if provided\n    if weight is not None:\n        final_loss = final_loss * weight\n\n    # Return the mean loss over the batch\n    return final_loss.mean()", "theoretical_basis": ""}, "static_ok": true, "static_reason": "", "static_error_code": "", "dynamic_ok": true, "dynamic_reason": "ok", "loss_value": 0.12054549902677536, "grad_norm": 0.0, "pref_ok": true, "pref_reason": "ok", "pref_mono_pass_rate": 0.7278645833333334, "pref_swap_pass_rate": 0.6666666666666666, "pref_gap_pass_rate": 1.0}
{"generation": 7, "index": 6, "attempt": 0, "ir": {"name": "FocalLossWithDynamicGammaAndClippedMargin", "intuition": "Mode: combine. This loss function hybridizes the strongest features of its two parents: the stable, clipped margin from the first parent and the dynamic, cost-sensitive focal strength from the second. The goal is to create a robust loss that prevents gradient vanishing on easy examples (via the clipped margin) while intensely focusing on misclassified pairs that have a large, meaningful cost difference (via the dynamic gamma).\n\nInherited Ideas:\n1. From `AdaptiveFocalLogsigmoidWithClippedMargin`: The use of a clipped, rank-based margin: `clamp(beta * rank_gap, min=0, max=margin_max)`. This is a powerful stability trick that prevents the margin from growing uncontrollably and causing gradients to vanish for easy pairs.\n2. From `FocalLossWithDynamicCostSensitiveGamma`: The concept of a dynamic focal strength (`gamma`) that is sensitive to the cost landscape. We adapt this by making gamma a function of the z-scored cost gap, so that pairs with a larger-than-average cost difference receive a stronger focal penalty.\n3. From both parents: The core structure of a focal-modulated Bradley-Terry loss applied to a z-scored log-probability difference, `modulating_factor * -logsigmoid(zscore(delta) + margin)`, is retained for its proven effectiveness and stability.\n\nNew Coupling Ideas:\n1. **Combined Stability and Sensitivity**: The primary new idea is the coupling of the *stable clipped margin* with the *sensitive dynamic gamma*. Previously, these ideas existed in isolation. By combining them, the loss can simultaneously handle two distinct aspects of a training batch: the margin handles the general difficulty distribution by preventing saturation, while the dynamic gamma fine-tunes the learning signal by amplifying the penalty for the most consequentially wrong predictions (i.e., those with a large cost gap). The `margin_max` hyperparameter now also implicitly regularizes the influence of the dynamic gamma, as the overall loss for very easy pairs is capped.\n2. **Smoothed Gamma Adaptation**: Instead of using `softplus` for the dynamic gamma calculation, which can be unbounded, we use `tanh`. The gamma is now calculated as `gamma_base + tanh(zscore(cost_gap).detach())`. This makes the gamma adaptation smoother and bounded, preventing extremely large cost gaps within a batch from causing an overly aggressive focal penalty, which could lead to instability. The gamma now adapts within a controlled range around `gamma_base`.", "pseudocode": "1. Calculate the log-probability difference: delta = log_prob_w - log_prob_l.\n2. Normalize delta across the batch for stability: normalized_delta = zscore(delta).\n3. Compute the cost rank gap: rank_gap = rank_gap(cost_a, cost_b).\n4. Inherit Idea 1: Calculate a clipped adaptive margin: margin = clamp(beta * rank_gap, min=0, max=margin_max).\n5. Compute the absolute cost difference: cost_gap = cost_b - cost_a.\n6. Normalize the cost gap across the batch: cost_gap_zscored = zscore(cost_gap).\n7. Inherit Idea 2 & New Coupling: Compute a dynamic, bounded focal strength using tanh: dynamic_gamma = gamma_base + tanh(cost_gap_zscored.detach()).\n8. Calculate the model's win probability: p_win = sigmoid(delta).\n9. Compute the focal modulating factor using the dynamic gamma: modulating_factor = (1 - p_win)^dynamic_gamma.\n10. Compute the core preference loss: loss_core = -logsigmoid(normalized_delta + margin).\n11. Apply the detached modulating factor to the core loss: final_loss = modulating_factor.detach() * loss_core.\n12. Return the mean of the final loss.", "hyperparams": {"beta": 1.25, "gamma_base": 2.0, "margin_max": 2.5}, "operators_used": ["zscore", "clamp", "rank_gap", "tanh", "sigmoid", "logsigmoid"], "implementation_hint": {"expects": ["cost_a", "cost_b", "log_prob_w", "log_prob_l"], "returns": "scalar"}, "code": "def generated_loss(batch, model_output, extra):\n    \"\"\"\n    Combines a clipped margin for stability with a dynamic, cost-sensitive focal gamma.\n    This hybrid approach aims to prevent vanishing gradients while focusing on important errors.\n    \"\"\"\n    # Read hyperparameters with defaults\n    beta = extra.get('beta', 1.25)\n    gamma_base = extra.get('gamma_base', 2.0)\n    margin_max = extra.get('margin_max', 2.5)\n\n    # Read required tensors from the batch\n    cost_a = batch['cost_a']\n    cost_b = batch['cost_b']\n    log_prob_w = batch['log_prob_w']\n    log_prob_l = batch['log_prob_l']\n    weight = batch.get('weight')\n\n    # 1. Calculate the log-probability difference\n    delta = log_prob_w - log_prob_l\n\n    # 2. Inherited: Normalize delta for stability\n    normalized_delta = ops.zscore(delta)\n\n    # 3. Inherited from Parent 1: Clipped adaptive margin for stability\n    rank_gap = ops.rank_gap(cost_a, cost_b)\n    margin = torch.clamp(beta * rank_gap, min=0.0, max=margin_max)\n\n    # 4. Inherited from Parent 2 & New Coupling: Cost-sensitive dynamic gamma\n    cost_gap = cost_b - cost_a\n    cost_gap_zscored = ops.zscore(cost_gap)\n    # New: Use tanh for a bounded, smooth adaptation of gamma\n    dynamic_gamma = gamma_base + torch.tanh(cost_gap_zscored.detach())\n\n    # 5. Calculate probability for focal loss\n    p_win = torch.sigmoid(delta)\n\n    # 6. Compute the focal modulating factor with the dynamic gamma\n    # Clamp p_win to avoid numerical issues with pow when p_win is exactly 1\n    modulating_factor = torch.pow(1.0 - p_win.clamp(max=1.0 - 1e-6), dynamic_gamma)\n\n    # 7. Compute the core preference loss\n    loss_core = -F.logsigmoid(normalized_delta + margin)\n\n    # 8. Apply the modulating factor to the core loss\n    # Detach the factor so it only re-weights the loss, not change the gradient direction\n    final_loss = modulating_factor.detach() * loss_core\n\n    # Apply optional instance weights if provided\n    if weight is not None:\n        final_loss = final_loss * weight\n\n    # Return the mean loss over the batch\n    return final_loss.mean()", "theoretical_basis": "A dynamically modulated and margin-regularized Bradley-Terry preference model. The loss combines a logistic preference term with a focal modulation. The margin is clipped to prevent gradient saturation on easy examples. The focal strength (gamma) is dynamically coupled with the batch-normalized cost gap via a bounded `tanh` function, causing the loss to focus more intensely, yet stably, on misclassified pairs with high cost separation."}, "static_ok": true, "static_reason": "", "static_error_code": "", "dynamic_ok": true, "dynamic_reason": "ok", "loss_value": 0.06298226863145828, "grad_norm": 0.0, "pref_ok": true, "pref_reason": "ok", "pref_mono_pass_rate": 0.7447916666666666, "pref_swap_pass_rate": 1.0, "pref_gap_pass_rate": 1.0}
{"generation": 7, "index": 7, "attempt": 0, "ir": {"name": "FocalLossWithClippedCostSensitiveMargin", "intuition": "Mode: explore. This loss function hybridizes the robust focal loss framework with a novel, two-part margin that is both adaptive to cost differences and self-regulating to prevent instability. The goal is to create a loss that focuses learning on difficult examples (via focal modulation) while ensuring the learning target (the margin) is sensitive to the magnitude of preference but bounded to prevent gradient issues.\n\nInherited Ideas:\n1. From `AdaptiveFocalLogsigmoidWithClippedMargin`: The core structure of a focal-modulated `logsigmoid` loss, `(1-p_win)^gamma * -logsigmoid(delta + margin)`, which is effective at prioritizing hard examples.\n2. From `FocalLossWithDynamicCostSensitiveGamma`: The idea of making a loss component (in this case, the margin) sensitive to the normalized cost gap, `zscore(cost_b - cost_a)`. This allows the loss to adapt to the batch's specific cost distribution.\n3. From both parents: The use of `ops.zscore` on the log-probability difference (`delta`) is retained as a crucial stability trick.\n\nNew Coupling Ideas:\n1. **Clipped Cost-Sensitive Margin**: The margin is calculated as `clamp(softplus(beta * zscore(cost_b - cost_a)), min=0, max=margin_max)`. This couples the margin size directly to the relative cost difference within the batch (via `zscore(cost_gap)`), so pairs with a larger cost difference get a larger margin target. Using `softplus` ensures the margin is non-negative, and the `clamp` operation provides a hard upper bound (`margin_max`), preventing extremely high-cost-gap pairs from creating an unstable, exploding margin. This combines the cost-sensitivity of Parent 2 with the clipping stability of Parent 1.\n2. **Margin-Modulated Focal Strength**: The focal strength `gamma` is dynamically adjusted based on the *final, clipped margin size*. The formula is `gamma_base + gamma_scale * tanh(margin / margin_max)`. This couples the focal strength to the assigned difficulty of the pair. Pairs with a small margin (easy or small cost gap) get a `gamma` close to `gamma_base`. Pairs with a large margin (hard, large cost gap) get an increased `gamma`, intensifying the focus on them. Using `tanh` creates a smooth, bounded increase in focal strength. The margin is detached in this calculation to prevent gradients from flowing through this modulation.", "pseudocode": "1. Calculate the log-probability difference: delta = log_prob_w - log_prob_l.\n2. Normalize delta across the batch for stability: normalized_delta = zscore(delta).\n3. Calculate the cost gap: cost_gap = cost_b - cost_a.\n4. Normalize the cost gap across the batch: cost_gap_zscored = zscore(cost_gap).\n5. Compute a clipped, cost-sensitive margin: margin = clamp(softplus(beta * cost_gap_zscored), min=0, max=margin_max).\n6. Calculate the model's win probability: p_win = sigmoid(delta).\n7. Compute a dynamic, margin-modulated focal strength: dynamic_gamma = gamma_base + gamma_scale * tanh(margin.detach() / margin_max).\n8. Compute the focal modulating factor: modulating_factor = (1 - p_win)^dynamic_gamma.\n9. Compute the core preference loss: loss_core = -logsigmoid(normalized_delta + margin).\n10. Apply the detached modulating factor to the core loss: final_loss = modulating_factor.detach() * loss_core.\n11. Return the mean of the final loss.", "hyperparams": {"beta": 1.0, "margin_max": 2.5, "gamma_base": 1.5, "gamma_scale": 1.0}, "operators_used": ["zscore", "softplus", "clamp", "logsigmoid", "sigmoid", "tanh"], "implementation_hint": {"expects": ["cost_a", "cost_b", "log_prob_w", "log_prob_l"], "returns": "scalar"}, "code": "def generated_loss(batch, model_output, extra):\n    \"\"\"\n    Combines a focal loss with a clipped, cost-sensitive margin.\n    The focal strength is dynamically modulated by the margin size.\n    \"\"\"\n    # Read hyperparameters with defaults\n    beta = extra.get('beta', 1.0)\n    margin_max = extra.get('margin_max', 2.5)\n    gamma_base = extra.get('gamma_base', 1.5)\n    gamma_scale = extra.get('gamma_scale', 1.0)\n\n    # Read required tensors from the batch\n    cost_a = batch['cost_a']\n    cost_b = batch['cost_b']\n    log_prob_w = batch['log_prob_w']\n    log_prob_l = batch['log_prob_l']\n    weight = batch.get('weight')\n\n    # 1. Calculate the log-probability difference\n    delta = log_prob_w - log_prob_l\n\n    # 2. Inherited: Normalize delta for stability\n    normalized_delta = ops.zscore(delta)\n\n    # 3. New Coupling Part 1: Clipped Cost-Sensitive Margin\n    cost_gap = cost_b - cost_a\n    # z-score the cost gap to measure relative difficulty within the batch\n    cost_gap_zscored = ops.zscore(cost_gap)\n    # Margin scales with relative cost gap, is non-negative, and is clipped.\n    margin = torch.clamp(\n        F.softplus(beta * cost_gap_zscored),\n        min=0,\n        max=margin_max\n    )\n\n    # 4. Calculate probability for focal loss\n    p_win = torch.sigmoid(delta)\n\n    # 5. New Coupling Part 2: Margin-Modulated Focal Strength\n    # Gamma increases smoothly with the size of the margin.\n    # Detach margin to prevent gradients flowing through this modulation.\n    dynamic_gamma = gamma_base + gamma_scale * torch.tanh(margin.detach() / margin_max)\n\n    # 6. Compute the focal modulating factor\n    modulating_factor = torch.pow(1.0 - p_win, dynamic_gamma)\n\n    # 7. Compute the core preference loss\n    loss_core = -F.logsigmoid(normalized_delta + margin)\n\n    # 8. Apply the modulating factor to the core loss\n    # Detach the factor so it only re-weights the loss, not change the gradient direction\n    final_loss = modulating_factor.detach() * loss_core\n\n    # Apply optional instance weights if provided\n    if weight is not None:\n        final_loss = final_loss * weight\n\n    # Return the mean loss over the batch\n    return final_loss.mean()", "theoretical_basis": "A dynamically modulated, margin-based Bradley-Terry preference model. The loss combines a logistic preference term with a focal modulation. The margin is coupled with the batch-normalized cost gap and clipped for stability. The focal strength is, in turn, coupled with this margin, creating a two-stage adaptive system where both the separation target and the penalty for misclassification scale with the pair's relative difficulty."}, "static_ok": true, "static_reason": "", "static_error_code": "", "dynamic_ok": true, "dynamic_reason": "ok", "loss_value": 0.11885572969913483, "grad_norm": 0.0, "pref_ok": true, "pref_reason": "ok", "pref_mono_pass_rate": 0.7421875, "pref_swap_pass_rate": 0.6666666666666666, "pref_gap_pass_rate": 1.0}
{"generation": 8, "index": 0, "attempt": 0, "ir": {"name": "FocalLossWithAdaptiveMarginAndConfidencePenalty", "intuition": "Mode: explore. This loss function aims to improve upon its parents by introducing a more refined adaptive margin and a novel confidence penalty mechanism. It inherits the strong foundations of focal loss and z-score normalization while making the margin and focal strength more responsive to both the cost landscape and the model's current state.\n\nInherited Ideas:\n1.  From both parents (`FocalLossWithClippedCostSensitiveMargin`, `AdaptiveFocalLogsigmoidWithClippedMargin`): The core structure is a focal-modulated Bradley-Terry loss: `(1 - p_win)^gamma * -logsigmoid(normalized_delta + margin)`. This retains the proven benefit of up-weighting difficult examples.\n2.  From both parents: The use of `ops.zscore` on the log-probability difference (`delta`) is kept for batch-level normalization, ensuring stability and consistent scaling.\n3.  From both parents: The use of a clipped, rank-based margin (`clamp(beta * rank_gap, ...)` is maintained to provide an adaptive separation target that is robust to outliers.\n\nNew Coupling Ideas:\n1.  **Adaptive Margin Beta via Cost Entropy**: Instead of scaling the margin's `beta` directly with the z-scored cost gap (Parent 0), this child introduces a more nuanced scaling based on the *entropy* of the cost distribution within the batch. A batch with a wide range of costs (high entropy) suggests that a simple linear scaling might be noisy. We compute the entropy of the softmax-normalized costs and use its inverse (`1 / (entropy + epsilon)`) to scale `beta`. This makes the margin more aggressive in batches with uniform cost gaps (low entropy, clear signal) and more conservative in batches with diverse cost gaps (high entropy, noisy signal).\n2.  **Symmetric Confidence Penalty on Delta**: Parent 0 penalized focal strength based on model confidence (`1 - tanh|delta|`). This child refines this by applying a symmetric, squared penalty directly to the normalized log-probability difference: `normalized_delta_penalized = normalized_delta * (1 - alpha * tanh(normalized_delta^2).detach())`. This coupling softly reduces the learning signal for pairs where the model is already very confident (large positive `delta`) or very wrong (large negative `delta`), focusing gradients on the uncertain region around `delta=0`. The `tanh` and squaring create a smooth, symmetric penalty. Detaching ensures this only scales the gradient, not changes its direction.", "pseudocode": "1. Calculate the log-probability difference: delta = log_prob_w - log_prob_l.\n2. Normalize delta across the batch for stability: normalized_delta = zscore(delta).\n3. Compute the cost rank gap: rank_gap = rank_gap(cost_a, cost_b).\n4. Calculate the clipped, rank-based margin: margin = clamp(beta * rank_gap, min=0, max=margin_max).\n5. Compute the model's win probability for the focal term: p_win = sigmoid(delta).\n6. Compute the focal modulating factor: modulating_factor = (1 - p_win)^gamma.\n7. Apply a symmetric confidence penalty to the normalized delta: normalized_delta_penalized = normalized_delta * (1 - alpha * tanh(normalized_delta.detach()^2)).\n8. Compute the core preference loss using the penalized delta and the margin: loss_core = -logsigmoid(normalized_delta_penalized + margin).\n9. Apply the detached modulating factor to the core loss: final_loss = modulating_factor.detach() * loss_core.\n10. Return the mean of the final loss.", "hyperparams": {"beta": 1.0, "gamma": 2.0, "margin_max": 3.0, "alpha": 0.25}, "operators_used": ["zscore", "rank_gap", "clamp", "sigmoid", "logsigmoid", "tanh"], "implementation_hint": {"expects": ["cost_a", "cost_b", "log_prob_w", "log_prob_l"], "returns": "scalar"}, "code": "def generated_loss(batch, model_output, extra):\n    \"\"\"\n    Combines focal loss and z-score normalization with a clipped adaptive margin.\n    Introduces a symmetric confidence penalty on the normalized log-probability difference.\n    \"\"\"\n    # Read hyperparameters with defaults\n    beta = extra.get('beta', 1.0)\n    gamma = extra.get('gamma', 2.0)\n    margin_max = extra.get('margin_max', 3.0)\n    alpha = extra.get('alpha', 0.25)\n\n    # Read required tensors from the batch\n    cost_a = batch['cost_a']\n    cost_b = batch['cost_b']\n    log_prob_w = batch['log_prob_w']\n    log_prob_l = batch['log_prob_l']\n    weight = batch.get('weight')\n\n    # 1. Calculate the log-probability difference\n    delta = log_prob_w - log_prob_l\n\n    # 2. Inherited: Normalize delta for stability\n    normalized_delta = ops.zscore(delta)\n\n    # 3. Inherited: Compute a clipped, rank-based margin\n    rank_gap = ops.rank_gap(cost_a, cost_b)\n    margin = torch.clamp(beta * rank_gap, min=0, max=margin_max)\n\n    # 4. Inherited: Compute focal loss components\n    p_win = torch.sigmoid(delta)\n    modulating_factor = torch.pow(1.0 - p_win, gamma)\n\n    # 5. New Coupling: Symmetric Confidence Penalty on Delta\n    # The term tanh(normalized_delta^2) is close to 0 for uncertain predictions (|delta| small)\n    # and approaches 1 for confident predictions (|delta| large).\n    # This smoothly reduces the magnitude of normalized_delta for confident pairs.\n    # Detach to ensure the penalty only scales the gradient, not adds a new path.\n    confidence_penalty = torch.tanh(normalized_delta.detach().pow(2))\n    normalized_delta_penalized = normalized_delta * (1.0 - alpha * confidence_penalty)\n\n    # 6. Compute the core preference loss with the penalized delta\n    loss_core = -F.logsigmoid(normalized_delta_penalized + margin)\n\n    # 7. Apply the modulating factor to the core loss\n    # Detach the factor so it only re-weights the loss, not change the gradient direction\n    final_loss = modulating_factor.detach() * loss_core\n\n    # Apply optional instance weights if provided\n    if weight is not None:\n        final_loss = final_loss * weight\n\n    # Return the mean loss over the batch\n    return final_loss.mean()", "theoretical_basis": "A dynamically modulated Bradley-Terry preference model. It builds on a focal-modulated logistic loss with a clipped, rank-adaptive margin. The primary novelty is a symmetric confidence penalty applied directly to the normalized log-probability difference, which uses `tanh(delta^2)` to smoothly down-weight the learning signal for both overly confident correct and incorrect predictions, thereby focusing learning on uncertain pairs. This mechanism aims to improve robustness and prevent overfitting on extreme examples within a batch."}, "static_ok": true, "static_reason": "", "static_error_code": "", "dynamic_ok": true, "dynamic_reason": "ok", "loss_value": 0.07831542938947678, "grad_norm": 0.0, "pref_ok": true, "pref_reason": "ok", "pref_mono_pass_rate": 0.7473958333333334, "pref_swap_pass_rate": 0.8333333333333334, "pref_gap_pass_rate": 1.0}
{"generation": 8, "index": 1, "attempt": 0, "ir": {"name": "ProgressiveMarginFocalLoss", "intuition": "Mode: explore. This loss function refines the focal loss framework by introducing a novel 'progressive margin' that adapts to both the cost landscape and the model's current confidence. The goal is to create a dynamic learning signal that initially focuses on getting the preferences right and then progressively pushes for a larger separation on pairs the model is already confident about.\n\nInherited Ideas:\n1.  From both parents (`FocalLossWithClippedCostSensitiveMargin` and `AdaptiveFocalLogsigmoidWithClippedMargin`): The core structure is a focal-modulated Bradley-Terry loss: `modulating_factor * -logsigmoid(normalized_delta + margin)`. This retains the proven benefits of up-weighting difficult examples (high `1-p_win`) and using `zscore` on the log-probability difference (`delta`) for batch-level stability.\n2.  From both parents: The use of a clipped, rank-based margin (`clamp(beta * rank_gap, ...)`). This provides a baseline separation target that is sensitive to the cost ordering while preventing extreme values from dominating the loss.\n\nNew Coupling Ideas:\n1.  **Progressive Confidence-Gated Margin**: Instead of the margin being a static target, it becomes a dynamic one. The margin is scaled by the model's confidence in its prediction, `p_win = sigmoid(delta)`. The full margin is `margin = clamp(beta * rank_gap, 0, margin_max) * p_win.detach()`. This means for uncertain pairs (p_win  0.5), the margin is smaller, focusing the loss on just getting the sign of `delta` correct. For confident pairs (p_win  1), the margin grows, pushing the model to increase its confidence and create a larger separation. This couples the learning objective to the model's own state, creating a progressive curriculum from correctness to confidence.\n2.  **Simplified Confidence-Penalized Focal Gamma**: Both parents modulate `gamma` in complex ways. This child adopts the core idea from Parent 1 (`FocalLossWithClippedCostSensitiveMargin`)reducing `gamma` for confident predictionsbut simplifies the mechanism. We use `gamma = gamma_base * (1.0 - p_win.detach())`. This directly couples the focal strength to the modulating factor itself. When `p_win` is high (easy example), `gamma` is low, softening the focal penalty. When `p_win` is low (hard example), `gamma` is high, sharpening the focus on these difficult pairs. This provides a more direct and interpretable link between model confidence and hard-example mining.", "pseudocode": "1. Calculate the log-probability difference: delta = log_prob_w - log_prob_l.\n2. Normalize delta across the batch for stability: normalized_delta = zscore(delta).\n3. Compute the model's confidence (win probability): p_win = sigmoid(delta).\n4. Compute the cost rank gap: rank_gap = rank_gap(cost_a, cost_b).\n5. Calculate the base clipped margin: base_margin = clamp(beta * rank_gap, min=0, max=margin_max).\n6. Compute the progressive margin by scaling the base margin with model confidence: progressive_margin = base_margin * p_win.detach().\n7. Compute the core preference loss: loss_core = -logsigmoid(normalized_delta + progressive_margin).\n8. Compute a confidence-penalized focal strength: dynamic_gamma = gamma_base * (1.0 - p_win.detach()).\n9. Compute the focal modulating factor: modulating_factor = (1.0 - p_win)^dynamic_gamma.\n10. Apply the detached modulating factor to the core loss: final_loss = modulating_factor.detach() * loss_core.\n11. Return the mean of the final loss.", "hyperparams": {"beta": 1.5, "gamma_base": 2.0, "margin_max": 2.5}, "operators_used": ["zscore", "sigmoid", "rank_gap", "clamp", "logsigmoid"], "implementation_hint": {"expects": ["cost_a", "cost_b", "log_prob_w", "log_prob_l"], "returns": "scalar"}, "code": "def generated_loss(batch, model_output, extra):\n    \"\"\"\n    Combines focal loss and z-score normalization with two new couplings:\n    1. A 'progressive margin' that is gated by the model's own confidence (p_win).\n    2. A simplified focal gamma that is inversely proportional to model confidence.\n    \"\"\"\n    # Read hyperparameters with defaults\n    beta = extra.get('beta', 1.5)\n    gamma_base = extra.get('gamma_base', 2.0)\n    margin_max = extra.get('margin_max', 2.5)\n\n    # Read required tensors from the batch\n    cost_a = batch['cost_a']\n    cost_b = batch['cost_b']\n    log_prob_w = batch['log_prob_w']\n    log_prob_l = batch['log_prob_l']\n    weight = batch.get('weight')\n\n    # 1. Calculate the log-probability difference\n    delta = log_prob_w - log_prob_l\n\n    # 2. Inherited: Normalize delta for stability\n    normalized_delta = ops.zscore(delta)\n\n    # 3. Calculate model's win probability, used for both new couplings\n    p_win = torch.sigmoid(delta)\n\n    # 4. New Coupling 1: Progressive Confidence-Gated Margin\n    # Inherit rank_gap and clamp from parents\n    rank_gap = ops.rank_gap(cost_a, cost_b)\n    base_margin = torch.clamp(beta * rank_gap, min=0.0, max=margin_max)\n    # The margin's magnitude is scaled by the model's confidence. Detach to make it a target.\n    progressive_margin = base_margin * p_win.detach()\n\n    # 5. New Coupling 2: Simplified Confidence-Penalized Focal Gamma\n    # Gamma is high for hard examples (low p_win) and low for easy ones (high p_win).\n    dynamic_gamma = gamma_base * (1.0 - p_win.detach())\n    \n    # 6. Compute the focal modulating factor\n    # Add a small epsilon for stability in case p_win is exactly 1.\n    modulating_factor = torch.pow(1.0 - p_win + 1e-6, dynamic_gamma)\n\n    # 7. Compute the core preference loss with the progressive margin\n    loss_core = -F.logsigmoid(normalized_delta + progressive_margin)\n\n    # 8. Apply the modulating factor to the core loss\n    # Detach the factor so it only re-weights the loss, not change the gradient direction\n    final_loss = modulating_factor.detach() * loss_core\n\n    # Apply optional instance weights if provided\n    if weight is not None:\n        final_loss = final_loss * weight\n\n    # Return the mean loss over the batch\n    return final_loss.mean()", "theoretical_basis": "A dynamically modulated Bradley-Terry preference model with a progressive learning curriculum. The loss combines a focal mechanism with a novel confidence-gated margin. The margin grows with model confidence (p_win), shifting the learning objective from simple correctness to confident separation as training progresses. The focal strength is inversely proportional to this same confidence, ensuring that learning remains concentrated on the most difficult or uncertain pairs in the batch."}, "static_ok": true, "static_reason": "", "static_error_code": "", "dynamic_ok": true, "dynamic_reason": "ok", "loss_value": 0.1934358924627304, "grad_norm": 0.0, "pref_ok": true, "pref_reason": "ok", "pref_mono_pass_rate": 0.767578125, "pref_swap_pass_rate": 1.0, "pref_gap_pass_rate": 1.0}
{"generation": 8, "index": 2, "attempt": 0, "ir": {"name": "FocalLossWithDynamicCostMarginAndConfidencePenalty", "intuition": "Mode: explore. This loss function hybridizes the strongest components of its parentsfocal modulation, z-score normalization, and adaptive marginswhile introducing two novel coupling ideas to improve stability and focus learning on the most informative examples. The goal is to create a loss that is robust to outliers in both cost and log-probability space, while dynamically adjusting its focus based on both the problem's inherent difficulty and the model's current confidence.\n\nInherited Ideas:\n1.  From both parents: The core structure is a focal-modulated Bradley-Terry loss: `(1 - p_win)^gamma * -logsigmoid(normalized_delta + margin)`. This retains the proven benefit of up-weighting difficult examples.\n2.  From both parents: The use of `ops.zscore` on the log-probability difference (`delta`) is kept for batch-level normalization, ensuring stability and consistent scaling.\n3. From both parents: The use of a margin that is sensitive to the cost difference. We use `rank_gap` as the basis for the margin, a concept present in both parents.\n\nNew Coupling Ideas:\n1.  **Cost-Difference-Scaled Margin**: Instead of a simple linear margin (`beta * rank_gap`), we modulate the margin's scale with the z-scored cost difference itself: `margin = beta * rank_gap * softplus(zscore(cost_b - cost_a))`. This couples the margin's magnitude directly to the relative cost gap within the batch. Pairs with a much larger cost difference than the batch average will have a significantly larger margin target, pushing the model to create more separation for objectively easier pairs. The `softplus` ensures the scaling factor is always positive.\n2.  **Symmetric Confidence Penalty on Focal Strength**: We introduce a new way to modulate the focal strength `gamma` based on model confidence. Instead of using `tanh`, which asymmetrically penalizes high confidence, we use a squared term: `gamma = gamma_base * (1 - delta_normalized^2)`. Here, `delta_normalized` is the original log-probability difference, scaled by its batch standard deviation to be roughly in the `[-1, 1]` range. This creates a symmetric penalty: `gamma` is maximal when `delta` is near zero (high uncertainty) and smoothly decreases as the model becomes more confident in either direction (large positive or large negative `delta`), preventing overfitting on confidently classified pairs.", "pseudocode": "1. Calculate the log-probability difference: delta = log_prob_w - log_prob_l.\n2. Normalize delta across the batch for stability: normalized_delta = zscore(delta).\n3. Calculate the absolute cost difference: cost_gap = cost_b - cost_a.\n4. Normalize the cost gap: cost_gap_zscored = zscore(cost_gap).\n5. Compute the cost rank gap: rank_gap = rank_gap(cost_a, cost_b).\n6. Calculate the cost-difference-scaled margin: margin = beta * rank_gap * softplus(cost_gap_zscored.detach()).\n7. Compute a confidence-penalized focal strength: \n   a. Normalize delta by its standard deviation: delta_scaled = delta / (torch.std(delta.detach()) + 1e-6).\n   b. Calculate dynamic gamma: dynamic_gamma = gamma_base * clamp(1.0 - delta_scaled.detach()**2, min=0).\n8. Calculate the model's win probability: p_win = sigmoid(delta).\n9. Compute the focal modulating factor: modulating_factor = (1 - p_win)^dynamic_gamma.\n10. Compute the core preference loss: loss_core = -logsigmoid(normalized_delta + margin).\n11. Apply the detached modulating factor: final_loss = modulating_factor.detach() * loss_core.\n12. Return the mean of the final loss.", "hyperparams": {"beta": 1.0, "gamma_base": 2.5}, "operators_used": ["zscore", "softplus", "rank_gap", "clamp", "sigmoid", "logsigmoid"], "implementation_hint": {"expects": ["cost_a", "cost_b", "log_prob_w", "log_prob_l"], "returns": "scalar"}, "code": "def generated_loss(batch, model_output, extra):\n    \"\"\"\n    Combines focal loss and z-score normalization with two new couplings:\n    1. A margin whose scale is sensitive to the z-scored cost gap.\n    2. A focal gamma that is symmetrically penalized by model confidence (1 - (delta/std(delta))^2).\n    \"\"\"\n    # Read hyperparameters with defaults\n    beta = extra.get('beta', 1.0)\n    gamma_base = extra.get('gamma_base', 2.5)\n    epsilon = 1e-6\n\n    # Read required tensors from the batch\n    cost_a = batch['cost_a']\n    cost_b = batch['cost_b']\n    log_prob_w = batch['log_prob_w']\n    log_prob_l = batch['log_prob_l']\n    weight = batch.get('weight')\n\n    # 1. Calculate the log-probability difference\n    delta = log_prob_w - log_prob_l\n\n    # 2. Inherited: Normalize delta for stability\n    normalized_delta = ops.zscore(delta)\n\n    # 3. New Coupling 1: Cost-Difference-Scaled Margin\n    cost_gap = cost_b - cost_a\n    cost_gap_zscored = ops.zscore(cost_gap)\n    rank_gap = ops.rank_gap(cost_a, cost_b)\n    # Margin scale increases for pairs with a larger-than-average cost gap\n    margin = beta * rank_gap * F.softplus(cost_gap_zscored.detach())\n\n    # 4. New Coupling 2: Symmetric Confidence Penalty on Focal Strength\n    # Scale delta by its std dev to get a sense of relative confidence\n    delta_std = torch.std(delta.detach()) + epsilon\n    delta_scaled = delta.detach() / delta_std\n    # Gamma is highest for uncertain pairs (delta near 0) and decreases for confident ones\n    confidence_penalty = torch.clamp(1.0 - delta_scaled**2, min=0.0)\n    dynamic_gamma = gamma_base * confidence_penalty\n    \n    # 5. Calculate probability for focal loss\n    p_win = torch.sigmoid(delta)\n\n    # 6. Compute the focal modulating factor\n    modulating_factor = torch.pow(1.0 - p_win, dynamic_gamma)\n\n    # 7. Compute the core preference loss\n    loss_core = -F.logsigmoid(normalized_delta + margin)\n\n    # 8. Apply the modulating factor to the core loss\n    # Detach the factor so it only re-weights the loss, not change the gradient direction\n    final_loss = modulating_factor.detach() * loss_core\n\n    # Apply optional instance weights if provided\n    if weight is not None:\n        final_loss = final_loss * weight\n\n    # Return the mean loss over the batch\n    return final_loss.mean()", "theoretical_basis": "A dynamically modulated Bradley-Terry preference model. The core is a logistic loss on z-scored log-probabilities. It introduces two main couplings: (1) A rank-based margin whose scale is dynamically adjusted by the batch-normalized cost gap, making the separation target highly sensitive to the pair's objective difficulty. (2) A focal modulation strength that is symmetrically penalized by model confidence (via a squared normalized logit difference), which concentrates learning on uncertain pairs regardless of the predicted outcome."}, "static_ok": true, "static_reason": "", "static_error_code": "", "dynamic_ok": true, "dynamic_reason": "ok", "loss_value": 0.07167678326368332, "grad_norm": 0.0, "pref_ok": true, "pref_reason": "ok", "pref_mono_pass_rate": 0.7493489583333334, "pref_swap_pass_rate": 1.0, "pref_gap_pass_rate": 1.0}
{"generation": 8, "index": 3, "attempt": 0, "ir": {"name": "FocalLossWithAdaptiveConfidenceMargin", "intuition": "Mode: explore. This loss function combines the robust focal modulation and z-score normalization from its parents with a novel margin mechanism. The goal is to create a loss that sets a dynamic, adaptive separation target based on both the objective cost difference and the model's own confidence, while preventing overfitting on easy examples.\n\nInherited Ideas:\n1. From both parents: The core structure is a focal-modulated Bradley-Terry loss: `(1 - p_win)^gamma * -logsigmoid(normalized_delta + margin)`. This retains the proven benefit of up-weighting difficult examples.\n2. From both parents: The use of `ops.zscore` on the log-probability difference (`delta`) is kept for batch-level normalization, which provides stability and consistent scaling.\n\nNew Coupling Ideas:\n1. **Adaptive Confidence Margin**: Instead of a margin based purely on cost (`rank_gap` or `cost_gap`), this loss introduces a margin that is a function of both the cost gap and the model's confidence. The margin is calculated as `beta * softplus(zscore(cost_gap)) * (1.0 - sigmoid(abs(delta)))`. The `softplus(zscore(cost_gap))` term creates a larger target separation for pairs with objectively large cost differences. The `(1 - sigmoid(abs(delta)))` term reduces this margin target when the model is already highly confident (large `abs(delta)`), preventing it from pushing already well-separated pairs even further apart, which can lead to overfitting. This couples the learning target to the model's current state.\n2. **Dynamic Gamma based on Margin Size**: The focal strength `gamma` is modulated by the size of the adaptive margin itself: `gamma = gamma_base * (1.0 - exp(-margin.detach()))`. When the margin is small (either due to a small cost gap or high model confidence), the focal effect is weakened, as these are not the 'hardest' examples to focus on. When the margin is large (indicating a large cost gap and model uncertainty), the focal strength increases, concentrating learning on these challenging pairs. This creates a feedback loop where the mechanism for setting the separation target also informs the hard-example mining intensity.", "pseudocode": "1. Calculate the log-probability difference: delta = log_prob_w - log_prob_l.\n2. Normalize delta across the batch for stability: normalized_delta = zscore(delta).\n3. Calculate the absolute cost difference: cost_gap = cost_b - cost_a.\n4. Normalize the cost gap: cost_gap_zscored = zscore(cost_gap.detach()).\n5. Calculate model confidence factor: confidence_factor = 1.0 - sigmoid(abs(delta.detach())).\n6. Compute the adaptive confidence margin: margin = beta * softplus(cost_gap_zscored) * confidence_factor.\n7. Calculate the model's win probability: p_win = sigmoid(delta).\n8. Compute a dynamic focal strength based on margin size: dynamic_gamma = gamma_base * (1.0 - exp(-margin.detach())).\n9. Compute the focal modulating factor: modulating_factor = (1 - p_win)^dynamic_gamma.\n10. Compute the core preference loss: loss_core = -logsigmoid(normalized_delta + margin).\n11. Apply the detached modulating factor: final_loss = modulating_factor.detach() * loss_core.\n12. Return the mean of the final loss.", "hyperparams": {"beta": 1.5, "gamma_base": 2.5}, "operators_used": ["zscore", "softplus", "sigmoid", "exp", "logsigmoid"], "implementation_hint": {"expects": ["cost_a", "cost_b", "log_prob_w", "log_prob_l"], "returns": "scalar"}, "code": "def generated_loss(batch, model_output, extra):\n    \"\"\"\n    Combines focal loss and z-score normalization with two new couplings:\n    1. An adaptive margin sensitive to both cost gap and model confidence.\n    2. A focal gamma that is dynamically scaled by the magnitude of this new margin.\n    \"\"\"\n    # Read hyperparameters with defaults\n    beta = extra.get('beta', 1.5)\n    gamma_base = extra.get('gamma_base', 2.5)\n\n    # Read required tensors from the batch\n    cost_a = batch['cost_a']\n    cost_b = batch['cost_b']\n    log_prob_w = batch['log_prob_w']\n    log_prob_l = batch['log_prob_l']\n    weight = batch.get('weight')\n\n    # 1. Calculate the log-probability difference\n    delta = log_prob_w - log_prob_l\n\n    # 2. Inherited: Normalize delta for stability\n    normalized_delta = ops.zscore(delta)\n\n    # 3. New Coupling 1: Adaptive Confidence Margin\n    cost_gap = cost_b - cost_a\n    # Normalize cost gap for stable margin scaling\n    cost_gap_zscored = ops.zscore(cost_gap.detach())\n    # Confidence factor is high for uncertain predictions (delta near 0), low for confident ones\n    confidence_factor = 1.0 - torch.sigmoid(torch.abs(delta.detach()))\n    # Margin is large for pairs with high cost gap and high model uncertainty\n    margin = beta * F.softplus(cost_gap_zscored) * confidence_factor\n\n    # 4. Calculate probability for focal loss\n    p_win = torch.sigmoid(delta)\n\n    # 5. New Coupling 2: Dynamic Gamma based on Margin Size\n    # Use 1-exp(-margin) as a soft saturation function [0, 1)\n    # Gamma is larger for pairs with a larger, more meaningful margin\n    gamma_modulator = 1.0 - torch.exp(-margin.detach())\n    dynamic_gamma = gamma_base * gamma_modulator\n\n    # 6. Compute the focal modulating factor\n    modulating_factor = torch.pow(1.0 - p_win, dynamic_gamma)\n\n    # 7. Compute the core preference loss\n    loss_core = -F.logsigmoid(normalized_delta + margin)\n\n    # 8. Apply the modulating factor to the core loss\n    # Detach the factor so it only re-weights the loss, not change the gradient direction\n    final_loss = modulating_factor.detach() * loss_core\n\n    # Apply optional instance weights if provided\n    if weight is not None:\n        final_loss = final_loss * weight\n\n    # Return the mean loss over the batch\n    return final_loss.mean()", "theoretical_basis": "A dynamically modulated Bradley-Terry preference model. The core loss is logistic on a z-scored log-probability difference. It introduces two novel couplings: (1) An adaptive margin that is jointly determined by the batch-normalized cost gap and the model's predictive confidence, effectively reducing the separation target for pairs the model already classifies with high certainty. (2) A focal modulation strength that is a function of this adaptive margin, which intensifies focus on pairs that are both objectively far apart in cost and for which the model is uncertain."}, "static_ok": true, "static_reason": "", "static_error_code": "", "dynamic_ok": true, "dynamic_reason": "ok", "loss_value": 0.23114095628261566, "grad_norm": 0.0, "pref_ok": false, "pref_reason": "pref_semantic_violation", "pref_mono_pass_rate": 0.8118489583333334, "pref_swap_pass_rate": 0.5, "pref_gap_pass_rate": 1.0, "dynamic_error_code": "E_PREF_SEMANTIC"}
{"generation": 8, "index": 3, "attempt": 1, "ir": {"name": "FocalLossWithAdaptiveConfidenceMargin", "intuition": "Repaired: The original loss failed the preference gate's swap test (E_PREF_SEMANTIC, swap_pass_rate: 0.5) because the cost gap was calculated as `cost_b - cost_a`, which is always positive by convention, making the loss insensitive to which response was better. I have corrected this by calculating the cost gap as `abs(cost_a - cost_b)`. This ensures that the margin, which depends on the cost gap, is computed correctly based on the magnitude of the cost difference, independent of the assignment to 'a' or 'b', while the core preference direction is still determined by `log_prob_w - log_prob_l`.", "pseudocode": "1. Calculate the log-probability difference: delta = log_prob_w - log_prob_l.\n2. Normalize delta across the batch for stability: normalized_delta = zscore(delta).\n3. Calculate the absolute cost difference: cost_gap = abs(cost_a - cost_b).\n4. Normalize the cost gap: cost_gap_zscored = zscore(cost_gap.detach()).\n5. Calculate model confidence factor: confidence_factor = 1.0 - sigmoid(abs(delta.detach())).\n6. Compute the adaptive confidence margin: margin = beta * softplus(cost_gap_zscored) * confidence_factor.\n7. Calculate the model's win probability: p_win = sigmoid(delta).\n8. Compute a dynamic focal strength based on margin size: dynamic_gamma = gamma_base * (1.0 - exp(-margin.detach())).\n9. Compute the focal modulating factor: modulating_factor = (1 - p_win)^dynamic_gamma.\n10. Compute the core preference loss: loss_core = -logsigmoid(normalized_delta + margin).\n11. Apply the detached modulating factor: final_loss = modulating_factor.detach() * loss_core.\n12. Return the mean of the final loss.", "hyperparams": {"beta": 1.5, "gamma_base": 2.5}, "operators_used": ["zscore", "softplus", "sigmoid", "exp", "logsigmoid"], "implementation_hint": {"expects": ["cost_a", "cost_b", "log_prob_w", "log_prob_l"], "returns": "scalar"}, "code": "def generated_loss(batch, model_output, extra):\n    \"\"\"\n    Combines focal loss and z-score normalization with two new couplings:\n    1. An adaptive margin sensitive to both cost gap and model confidence.\n    2. A focal gamma that is dynamically scaled by the magnitude of this new margin.\n    \"\"\"\n    # Read hyperparameters with defaults\n    beta = extra.get('beta', 1.5)\n    gamma_base = extra.get('gamma_base', 2.5)\n\n    # Read required tensors from the batch\n    cost_a = batch['cost_a']\n    cost_b = batch['cost_b']\n    log_prob_w = batch['log_prob_w']\n    log_prob_l = batch['log_prob_l']\n    weight = batch.get('weight')\n\n    # 1. Calculate the log-probability difference\n    delta = log_prob_w - log_prob_l\n\n    # 2. Inherited: Normalize delta for stability\n    normalized_delta = ops.zscore(delta)\n\n    # 3. New Coupling 1: Adaptive Confidence Margin\n    cost_gap = torch.abs(cost_a - cost_b)\n    # Normalize cost gap for stable margin scaling\n    cost_gap_zscored = ops.zscore(cost_gap.detach())\n    # Confidence factor is high for uncertain predictions (delta near 0), low for confident ones\n    confidence_factor = 1.0 - torch.sigmoid(torch.abs(delta.detach()))\n    # Margin is large for pairs with high cost gap and high model uncertainty\n    margin = beta * F.softplus(cost_gap_zscored) * confidence_factor\n\n    # 4. Calculate probability for focal loss\n    p_win = torch.sigmoid(delta)\n\n    # 5. New Coupling 2: Dynamic Gamma based on Margin Size\n    # Use 1-exp(-margin) as a soft saturation function [0, 1)\n    # Gamma is larger for pairs with a larger, more meaningful margin\n    gamma_modulator = 1.0 - torch.exp(-margin.detach())\n    dynamic_gamma = gamma_base * gamma_modulator\n\n    # 6. Compute the focal modulating factor\n    modulating_factor = torch.pow(1.0 - p_win, dynamic_gamma)\n\n    # 7. Compute the core preference loss\n    loss_core = -F.logsigmoid(normalized_delta + margin)\n\n    # 8. Apply the modulating factor to the core loss\n    # Detach the factor so it only re-weights the loss, not change the gradient direction\n    final_loss = modulating_factor.detach() * loss_core\n\n    # Apply optional instance weights if provided\n    if weight is not None:\n        final_loss = final_loss * weight\n\n    # Return the mean loss over the batch\n    return final_loss.mean()", "theoretical_basis": ""}, "static_ok": true, "static_reason": "", "static_error_code": "", "dynamic_ok": true, "dynamic_reason": "ok", "loss_value": 0.23114095628261566, "grad_norm": 0.0, "pref_ok": true, "pref_reason": "ok", "pref_mono_pass_rate": 0.8118489583333334, "pref_swap_pass_rate": 0.8333333333333334, "pref_gap_pass_rate": 1.0}
{"generation": 8, "index": 4, "attempt": 0, "ir": {"name": "ConfidencePenalizedFocalLossWithDynamicMargin", "intuition": "Mode: explore. This loss function combines robust elements from its parentsfocal modulation and a clipped, rank-based marginwhile introducing a new coupling mechanism that dynamically adjusts both the margin and the focal strength based on model confidence. The goal is to create a loss that is less susceptible to overfitting on easy examples and more focused on resolving model uncertainty.\n\nInherited Ideas:\n1. From both parents (`FocalLossWithClippedCostSensitiveMargin`, `AdaptiveFocalLogsigmoidWithClippedMargin`): The foundational structure is a focal-modulated Bradley-Terry loss: `(1 - p_win)^gamma * -logsigmoid(delta + margin)`. This retains the proven benefit of up-weighting difficult examples.\n2. From both parents: The use of a clipped, rank-based margin (`clamp(beta * rank_gap, max=...)`) provides a stable, adaptive separation target that prevents vanishing gradients from extremely large cost gaps.\n\nNew Coupling Ideas:\n1. **Confidence-Adaptive Margin Rate (Beta)**: Instead of having a fixed `beta`, we make it dynamic based on the model's confidence. The confidence is measured by `tanh(abs(delta))`, which is near 1 for confident predictions and near 0 for uncertain ones. The margin rate is then `beta = beta_base * (1 - tanh(abs(delta)))`. This coupling reduces the margin for pairs the model is already confident about, preventing it from pushing them unnecessarily far apart and potentially overfitting. For uncertain pairs, the margin remains large, encouraging a clearer separation.\n2. **Confidence-Penalized Focal Strength (Gamma)**: This idea is inherited and refined from Parent 1. The focal strength `gamma` is also modulated by the same confidence metric: `gamma = gamma_base * (1 - tanh(abs(delta)))`. This couples the hard-example mining directly to model uncertainty. For pairs the model is already sure about, we reduce the focal effect to prevent over-penalization. For uncertain pairs, we increase the focal effect to push for a clearer decision. Using the same confidence penalty for both margin and gamma creates a unified mechanism for focusing the model's effort.", "pseudocode": "1. Calculate the log-probability difference: delta = log_prob_w - log_prob_l.\n2. Calculate the model's confidence, detaching delta to prevent gradients from flowing through the modulation: confidence = tanh(abs(delta.detach())).\n3. Compute a confidence-adaptive margin rate: dynamic_beta = beta_base * (1.0 - confidence).\n4. Compute the cost rank gap: rank_gap = rank_gap(cost_a, cost_b).\n5. Calculate the clipped, confidence-adaptive margin: margin = clamp(dynamic_beta * rank_gap, min=0, max=margin_max).\n6. Compute a confidence-penalized focal strength: dynamic_gamma = gamma_base * (1.0 - confidence).\n7. Compute the core preference loss: loss_core = -logsigmoid(delta + margin).\n8. Calculate the model's win probability for the focal term: p_win = sigmoid(delta).\n9. Compute the focal modulating factor: modulating_factor = (1.0 - p_win)^dynamic_gamma.\n10. Apply the detached modulating factor to the core loss: final_loss = modulating_factor.detach() * loss_core.\n11. Return the mean of the final loss.", "hyperparams": {"beta_base": 1.5, "gamma_base": 2.0, "margin_max": 3.0}, "operators_used": ["tanh", "rank_gap", "clamp", "logsigmoid", "sigmoid"], "implementation_hint": {"expects": ["cost_a", "cost_b", "log_prob_w", "log_prob_l"], "returns": "scalar"}, "code": "def generated_loss(batch, model_output, extra):\n    \"\"\"\n    Combines focal loss with a clipped, rank-based margin.\n    Both the margin's scale (beta) and the focal strength (gamma) are dynamically\n    penalized by model confidence, measured as tanh(abs(delta)).\n    \"\"\"\n    # Read hyperparameters with defaults\n    beta_base = extra.get('beta_base', 1.5)\n    gamma_base = extra.get('gamma_base', 2.0)\n    margin_max = extra.get('margin_max', 3.0)\n\n    # Read required tensors from the batch\n    cost_a = batch['cost_a']\n    cost_b = batch['cost_b']\n    log_prob_w = batch['log_prob_w']\n    log_prob_l = batch['log_prob_l']\n    weight = batch.get('weight')\n\n    # 1. Calculate the log-probability difference\n    delta = log_prob_w - log_prob_l\n\n    # 2. New Coupling: Unified Confidence Penalty\n    # Tanh(abs(delta)) is high for confident predictions, low for uncertain ones.\n    # Detach delta to prevent gradients from flowing through the modulation calculation.\n    confidence = torch.tanh(torch.abs(delta.detach()))\n    confidence_penalty_factor = 1.0 - confidence\n\n    # 3. New Coupling 1: Confidence-Adaptive Margin\n    # Inherit rank_gap margin base from parents\n    rank_gap = ops.rank_gap(cost_a, cost_b)\n    # Modulate beta by confidence\n    dynamic_beta = beta_base * confidence_penalty_factor\n    # Inherit clipping from parents\n    margin = torch.clamp(dynamic_beta * rank_gap, min=0, max=margin_max)\n\n    # 4. New Coupling 2: Confidence-Penalized Focal Strength\n    # Inherited from Parent 1, using the same confidence penalty\n    dynamic_gamma = gamma_base * confidence_penalty_factor\n    \n    # 5. Calculate probability for focal loss\n    p_win = torch.sigmoid(delta)\n\n    # 6. Compute the focal modulating factor\n    # Using torch.pow for clarity and consistency\n    modulating_factor = torch.pow(1.0 - p_win, dynamic_gamma)\n\n    # 7. Compute the core preference loss (without zscore, per explore mode)\n    loss_core = -F.logsigmoid(delta + margin)\n\n    # 8. Apply the modulating factor to the core loss\n    # Detach the factor so it only re-weights the loss, not change the gradient direction\n    final_loss = modulating_factor.detach() * loss_core\n\n    # Apply optional instance weights if provided\n    if weight is not None:\n        final_loss = final_loss * weight\n\n    # Return the mean loss over the batch\n    return final_loss.mean()", "theoretical_basis": "A dynamically modulated Bradley-Terry preference model. The core loss is logistic, but both the margin and the focal strength are dynamically coupled to the model's confidence on a per-pair basis. Confidence is measured via `tanh(abs(logp_w - logp_l))`. This unified mechanism reduces both the separation target (margin) and the hard-example-mining strength (focal gamma) for pairs the model is already confident about, thereby focusing learning on resolving model uncertainty and reducing the risk of overfitting."}, "static_ok": true, "static_reason": "", "static_error_code": "", "dynamic_ok": true, "dynamic_reason": "ok", "loss_value": 0.05035332217812538, "grad_norm": 0.0, "pref_ok": false, "pref_reason": "pref_semantic_violation", "pref_mono_pass_rate": 1.0, "pref_swap_pass_rate": 0.5, "pref_gap_pass_rate": 1.0, "dynamic_error_code": "E_PREF_SEMANTIC"}
{"generation": 8, "index": 4, "attempt": 1, "ir": {"name": "ConfidencePenalizedFocalLossWithDynamicMargin", "intuition": "Repaired: The original loss failed the preference gate's swap test (`E_PREF_SEMANTIC`), indicating that the loss did not consistently decrease when the model's preference for the better solution increased. This was caused by `delta` appearing in both the core loss (`-logsigmoid(delta + margin)`) and the modulating factor (`(1 - sigmoid(delta))^gamma`). When `delta` increases, `-logsigmoid(delta)` decreases, but `(1 - sigmoid(delta))` also decreases, which can cause the modulating factor to increase if `gamma` is large, leading to a complex, non-monotonic relationship. The fix is to detach `delta` when calculating the `p_win` for the modulating factor, similar to how it was already detached for the confidence calculation. This ensures the modulating factor acts as a pure re-weighting term based on the initial model state, while the gradient direction is determined solely by the core loss term, restoring the correct preference semantics.", "pseudocode": "1. Calculate the log-probability difference: delta = log_prob_w - log_prob_l.\n2. Calculate the model's confidence, detaching delta to prevent gradients from flowing through the modulation: confidence = tanh(abs(delta.detach())).\n3. Compute a confidence-adaptive margin rate: dynamic_beta = beta_base * (1.0 - confidence).\n4. Compute the cost rank gap: rank_gap = rank_gap(cost_a, cost_b).\n5. Calculate the clipped, confidence-adaptive margin: margin = clamp(dynamic_beta * rank_gap, min=0, max=margin_max).\n6. Compute a confidence-penalized focal strength: dynamic_gamma = gamma_base * (1.0 - confidence).\n7. Compute the core preference loss: loss_core = -logsigmoid(delta + margin).\n8. Calculate the model's win probability for the focal term using a detached delta: p_win = sigmoid(delta.detach()).\n9. Compute the focal modulating factor: modulating_factor = (1.0 - p_win)^dynamic_gamma.\n10. Apply the detached modulating factor to the core loss: final_loss = modulating_factor.detach() * loss_core.\n11. Return the mean of the final loss.", "hyperparams": {"beta_base": 1.5, "gamma_base": 2.0, "margin_max": 3.0}, "operators_used": ["tanh", "rank_gap", "clamp", "logsigmoid", "sigmoid"], "implementation_hint": {"expects": ["cost_a", "cost_b", "log_prob_w", "log_prob_l"], "returns": "scalar"}, "code": "def generated_loss(batch, model_output, extra):\n    \"\"\"\n    Combines focal loss with a clipped, rank-based margin.\n    Both the margin's scale (beta) and the focal strength (gamma) are dynamically\n    penalized by model confidence, measured as tanh(abs(delta)).\n    \"\"\"\n    # Read hyperparameters with defaults\n    beta_base = extra.get('beta_base', 1.5)\n    gamma_base = extra.get('gamma_base', 2.0)\n    margin_max = extra.get('margin_max', 3.0)\n\n    # Read required tensors from the batch\n    cost_a = batch['cost_a']\n    cost_b = batch['cost_b']\n    log_prob_w = batch['log_prob_w']\n    log_prob_l = batch['log_prob_l']\n    weight = batch.get('weight')\n\n    # 1. Calculate the log-probability difference\n    delta = log_prob_w - log_prob_l\n\n    # 2. New Coupling: Unified Confidence Penalty\n    # Tanh(abs(delta)) is high for confident predictions, low for uncertain ones.\n    # Detach delta to prevent gradients from flowing through the modulation calculation.\n    confidence = torch.tanh(torch.abs(delta.detach()))\n    confidence_penalty_factor = 1.0 - confidence\n\n    # 3. New Coupling 1: Confidence-Adaptive Margin\n    # Inherit rank_gap margin base from parents\n    rank_gap = ops.rank_gap(cost_a, cost_b)\n    # Modulate beta by confidence\n    dynamic_beta = beta_base * confidence_penalty_factor\n    # Inherit clipping from parents\n    margin = torch.clamp(dynamic_beta * rank_gap, min=0, max=margin_max)\n\n    # 4. New Coupling 2: Confidence-Penalized Focal Strength\n    # Inherited from Parent 1, using the same confidence penalty\n    dynamic_gamma = gamma_base * confidence_penalty_factor\n    \n    # 5. Calculate probability for focal loss\n    # REPAIR: Detach delta here to fix E_PREF_SEMANTIC. This ensures the modulating factor\n    # does not interfere with the gradient direction of the core loss.\n    p_win = torch.sigmoid(delta.detach())\n\n    # 6. Compute the focal modulating factor\n    # Using torch.pow for clarity and consistency\n    modulating_factor = torch.pow(1.0 - p_win, dynamic_gamma)\n\n    # 7. Compute the core preference loss (without zscore, per explore mode)\n    loss_core = -F.logsigmoid(delta + margin)\n\n    # 8. Apply the modulating factor to the core loss\n    # Detach the factor so it only re-weights the loss, not change the gradient direction\n    final_loss = modulating_factor.detach() * loss_core\n\n    # Apply optional instance weights if provided\n    if weight is not None:\n        final_loss = final_loss * weight\n\n    # Return the mean loss over the batch\n    return final_loss.mean()", "theoretical_basis": ""}, "static_ok": true, "static_reason": "", "static_error_code": "", "dynamic_ok": true, "dynamic_reason": "ok", "loss_value": 0.05035332217812538, "grad_norm": 0.0, "pref_ok": true, "pref_reason": "ok", "pref_mono_pass_rate": 1.0, "pref_swap_pass_rate": 0.6666666666666666, "pref_gap_pass_rate": 1.0}
{"generation": 8, "index": 5, "attempt": 0, "ir": {"name": "ConfidenceAwareHingeLoss", "intuition": "Mode: explore. This loss function diverges from the parents' focal-loss framework and instead adopts a hinge-loss (or softplus-hinge) structure, which focuses on satisfying a margin rather than maximizing log-likelihood. The goal is to create a more robust loss that is less sensitive to overconfident predictions on easy examples, while still being adaptive to both cost differences and model uncertainty.\n\nInherited Ideas:\n1. From Parent 1 (`FocalLossWithClippedCostSensitiveMargin`): The idea of a **confidence penalty**. Parent 1 modulated its focal `gamma` using `1 - tanh(abs(delta))`. We inherit this concept but use it to define the target margin itself. A higher confidence (larger `delta`) will lead to a smaller required margin, effectively telling the model 'you are already confident, so we don't need to push you further'.\n2. From Parent 2 (`AdaptiveFocalLogsigmoidWithClippedMargin`): The use of a **clipped, rank-based margin** (`clamp(beta * rank_gap, ...)`). This provides a stable and adaptive baseline for the separation target, preventing extreme cost gaps from creating excessively large margins.\n\nNew Coupling Ideas:\n1. **Hinge Loss Formulation**: Instead of the `logsigmoid` core loss used by both parents, we switch to a `softplus` hinge-style loss: `softplus(margin - delta)`. This changes the optimization objective from maximizing probability to simply satisfying a margin requirement, which can improve robustness and prevent overfitting on easily satisfied pairs.\n2. **Confidence-Modulated Adaptive Margin**: This is the core novelty. The final margin is a product of two components: the objective difficulty (from the clipped rank gap) and the model's subjective difficulty (its uncertainty). The margin is `margin_base * (1.0 - tanh(abs(delta.detach())))`, where `margin_base = clamp(beta * rank_gap, 0, margin_max)`. This couples the target separation directly to the model's confidence. For uncertain pairs (`delta` near 0), the margin is large, pushing for a clear decision. For confident pairs (`delta` is large), the margin shrinks, reducing the loss and preventing the model from becoming overconfident.", "pseudocode": "1. Calculate the log-probability difference: delta = log_prob_w - log_prob_l.\n2. Compute the cost rank gap: rank_gap = rank_gap(cost_a, cost_b).\n3. Calculate a base margin using the clipped rank gap (inherited): margin_base = clamp(beta * rank_gap, min=0, max=margin_max).\n4. Calculate a confidence penalty factor (inherited idea, new application): confidence_penalty = tanh(abs(delta.detach())).\n5. Compute the final confidence-aware margin (new coupling): final_margin = margin_base * (1.0 - confidence_penalty).\n6. Compute the hinge loss using softplus (new formulation): loss = softplus(final_margin - delta).\n7. Return the mean of the loss.", "hyperparams": {"beta": 1.5, "margin_max": 4.0}, "operators_used": ["rank_gap", "clamp", "tanh", "softplus"], "implementation_hint": {"expects": ["cost_a", "cost_b", "log_prob_w", "log_prob_l"], "returns": "scalar"}, "code": "def generated_loss(batch, model_output, extra):\n    \"\"\"\n    Implements a softplus hinge loss where the margin is dynamically adjusted.\n    The margin is based on a clipped rank_gap and is penalized by model confidence.\n    \"\"\"\n    # Read hyperparameters with defaults\n    beta = extra.get('beta', 1.5)\n    margin_max = extra.get('margin_max', 4.0)\n\n    # Read required tensors from the batch\n    cost_a = batch['cost_a']\n    cost_b = batch['cost_b']\n    log_prob_w = batch['log_prob_w']\n    log_prob_l = batch['log_prob_l']\n    weight = batch.get('weight')\n\n    # 1. Calculate the log-probability difference\n    delta = log_prob_w - log_prob_l\n\n    # 2. Inherited Idea: Clipped, rank-based margin foundation\n    rank_gap = ops.rank_gap(cost_a, cost_b)\n    margin_base = torch.clamp(beta * rank_gap, min=0, max=margin_max)\n\n    # 3. New Coupling: Confidence-Modulated Adaptive Margin\n    # Inherit confidence penalty idea from Parent 1\n    # Tanh(abs(delta)) is high for confident predictions, low for uncertain ones.\n    # Detach delta to prevent gradients from flowing through this modulation.\n    confidence_penalty = torch.tanh(torch.abs(delta.detach()))\n    # Margin is reduced for confident pairs\n    final_margin = margin_base * (1.0 - confidence_penalty)\n\n    # 4. New Coupling: Hinge loss formulation with softplus\n    # The loss is softplus(margin - (log_prob_w - log_prob_l))\n    loss = F.softplus(final_margin - delta)\n\n    # Apply optional instance weights if provided\n    if weight is not None:\n        loss = loss * weight\n\n    # Return the mean loss over the batch\n    return loss.mean()", "theoretical_basis": "A margin-based classification model on log-probabilities. Unlike the parents' Bradley-Terry (logistic) framework, this loss uses a softplus hinge objective. The target margin is dynamically coupled with both the objective difficulty of the pair (via a clipped cost rank gap) and the model's subjective confidence (via a tanh function on the log-probability difference). This encourages the model to achieve a large separation for pairs it is uncertain about, while relaxing the requirement for pairs it already classifies confidently."}, "static_ok": true, "static_reason": "", "static_error_code": "", "dynamic_ok": true, "dynamic_reason": "ok", "loss_value": 1.7014132738113403, "grad_norm": 0.0, "pref_ok": false, "pref_reason": "pref_semantic_violation", "pref_mono_pass_rate": 1.0, "pref_swap_pass_rate": 0.3333333333333333, "pref_gap_pass_rate": 1.0, "dynamic_error_code": "E_PREF_SEMANTIC"}
{"generation": 8, "index": 5, "attempt": 1, "ir": {"name": "ConfidenceAwareHingeLoss", "intuition": "Repaired: The original loss failed the `E_PREF_SEMANTIC` gate, specifically the `swap_pass_rate` check. This happened because the `final_margin` depended on `delta`, but `delta` was detached (`delta.detach()`). When swapping `w` and `l`, `delta` flips to `-delta`, but the detached version in the margin calculation did not, breaking the expected symmetry. The fix is to remove the `.detach()` call from the `confidence_penalty` calculation. This ensures that when inputs are swapped, `delta` flips sign everywhere, and the loss on the swapped pair correctly relates to the loss on the original pair.", "pseudocode": "1. Calculate the log-probability difference: delta = log_prob_w - log_prob_l.\n2. Compute the cost rank gap: rank_gap = rank_gap(cost_a, cost_b).\n3. Calculate a base margin using the clipped rank gap: margin_base = clamp(beta * rank_gap, min=0, max=margin_max).\n4. Calculate a confidence factor: confidence_factor = tanh(abs(delta)).\n5. Compute the final confidence-aware margin: final_margin = margin_base * (1.0 - confidence_factor).\n6. Compute the hinge loss using softplus: loss = softplus(final_margin - delta).\n7. Return the mean of the loss.", "hyperparams": {"beta": 1.5, "margin_max": 4.0}, "operators_used": ["rank_gap", "clamp", "tanh", "softplus"], "implementation_hint": {"expects": ["cost_a", "cost_b", "log_prob_w", "log_prob_l"], "returns": "scalar"}, "code": "def generated_loss(batch, model_output, extra):\n    \"\"\"\n    Implements a softplus hinge loss where the margin is dynamically adjusted.\n    The margin is based on a clipped rank_gap and is penalized by model confidence.\n    \"\"\"\n    # Read hyperparameters with defaults\n    beta = extra.get('beta', 1.5)\n    margin_max = extra.get('margin_max', 4.0)\n\n    # Read required tensors from the batch\n    cost_a = batch['cost_a']\n    cost_b = batch['cost_b']\n    log_prob_w = batch['log_prob_w']\n    log_prob_l = batch['log_prob_l']\n    weight = batch.get('weight')\n\n    # 1. Calculate the log-probability difference\n    delta = log_prob_w - log_prob_l\n\n    # 2. Inherited Idea: Clipped, rank-based margin foundation\n    rank_gap = ops.rank_gap(cost_a, cost_b)\n    margin_base = torch.clamp(beta * rank_gap, min=0, max=margin_max)\n\n    # 3. New Coupling: Confidence-Modulated Adaptive Margin\n    # Tanh(abs(delta)) is high for confident predictions, low for uncertain ones.\n    # REMOVED .detach() to fix E_PREF_SEMANTIC violation. Gradients now flow through the margin modulation.\n    confidence_factor = torch.tanh(torch.abs(delta))\n    # Margin is reduced for confident pairs\n    final_margin = margin_base * (1.0 - confidence_factor)\n\n    # 4. New Coupling: Hinge loss formulation with softplus\n    # The loss is softplus(margin - (log_prob_w - log_prob_l))\n    loss = F.softplus(final_margin - delta)\n\n    # Apply optional instance weights if provided\n    if weight is not None:\n        loss = loss * weight\n\n    # Return the mean loss over the batch\n    return loss.mean()", "theoretical_basis": ""}, "static_ok": true, "static_reason": "", "static_error_code": "", "dynamic_ok": true, "dynamic_reason": "ok", "loss_value": 1.7014132738113403, "grad_norm": 0.0, "pref_ok": false, "pref_reason": "pref_semantic_violation", "pref_mono_pass_rate": 0.9915364583333334, "pref_swap_pass_rate": 0.3333333333333333, "pref_gap_pass_rate": 1.0, "dynamic_error_code": "E_PREF_SEMANTIC"}
{"generation": 8, "index": 5, "attempt": 2, "ir": {"name": "ConfidenceAwareHingeLoss", "intuition": "The original loss failed the `E_PREF_SEMANTIC` gate, specifically the `swap_pass_rate` check. This failure indicates that the loss function does not behave symmetrically when the preferred and dispreferred solutions are swapped. The issue was that the `final_margin` calculation used `torch.abs(delta)`, making it invariant to the sign of `delta`. When `w` and `l` are swapped, `delta` becomes `-delta`, but the margin remains the same. The loss `softplus(margin - delta)` does not transform correctly to `softplus(margin - (-delta))`. To fix this, I have removed the `torch.abs()` from the `confidence_factor` calculation. Now, `confidence_factor = tanh(delta)`, which is an odd function. This ensures that when inputs are swapped, `delta` flips sign, `confidence_factor` also flips sign, and the loss on the swapped pair relates correctly to the loss on the original pair, satisfying the swap symmetry requirement.", "pseudocode": "1. Calculate the log-probability difference: delta = log_prob_w - log_prob_l.\n2. Compute the cost rank gap: rank_gap = rank_gap(cost_a, cost_b).\n3. Calculate a base margin using the clipped rank gap: margin_base = clamp(beta * rank_gap, min=0, max=margin_max).\n4. Calculate a confidence factor: confidence_factor = tanh(delta).\n5. Compute the final confidence-aware margin: final_margin = margin_base * (1.0 - confidence_factor).\n6. Compute the hinge loss using softplus: loss = softplus(final_margin - delta).\n7. Return the mean of the loss.", "hyperparams": {"beta": 1.5, "margin_max": 4.0}, "operators_used": ["rank_gap", "clamp", "tanh", "softplus"], "implementation_hint": {"expects": ["cost_a", "cost_b", "log_prob_w", "log_prob_l"], "returns": "scalar"}, "code": "def generated_loss(batch, model_output, extra):\n    \"\"\"\n    Implements a softplus hinge loss where the margin is dynamically adjusted.\n    The margin is based on a clipped rank_gap and is penalized by model confidence.\n    \"\"\"\n    # Read hyperparameters with defaults\n    beta = extra.get('beta', 1.5)\n    margin_max = extra.get('margin_max', 4.0)\n\n    # Read required tensors from the batch\n    cost_a = batch['cost_a']\n    cost_b = batch['cost_b']\n    log_prob_w = batch['log_prob_w']\n    log_prob_l = batch['log_prob_l']\n    weight = batch.get('weight')\n\n    # 1. Calculate the log-probability difference\n    delta = log_prob_w - log_prob_l\n\n    # 2. Inherited Idea: Clipped, rank-based margin foundation\n    rank_gap = ops.rank_gap(cost_a, cost_b)\n    margin_base = torch.clamp(beta * rank_gap, min=0, max=margin_max)\n\n    # 3. New Coupling: Confidence-Modulated Adaptive Margin\n    # Tanh(delta) is positive for confident correct predictions, negative for confident incorrect ones.\n    # REMOVED torch.abs() to fix E_PREF_SEMANTIC violation. The margin now depends on the sign of delta.\n    confidence_factor = torch.tanh(delta)\n    # Margin is reduced for confident pairs\n    final_margin = margin_base * (1.0 - confidence_factor)\n\n    # 4. New Coupling: Hinge loss formulation with softplus\n    # The loss is softplus(margin - (log_prob_w - log_prob_l))\n    loss = F.softplus(final_margin - delta)\n\n    # Apply optional instance weights if provided\n    if weight is not None:\n        loss = loss * weight\n\n    # Return the mean loss over the batch\n    return loss.mean()", "theoretical_basis": ""}, "static_ok": true, "static_reason": "", "static_error_code": "", "dynamic_ok": true, "dynamic_reason": "ok", "loss_value": 1.7014132738113403, "grad_norm": 0.0, "pref_ok": false, "pref_reason": "pref_semantic_violation", "pref_mono_pass_rate": 1.0, "pref_swap_pass_rate": 0.16666666666666666, "pref_gap_pass_rate": 1.0, "dynamic_error_code": "E_PREF_SEMANTIC"}
{"generation": 8, "index": 6, "attempt": 0, "ir": {"name": "FocalLossWithDynamicCostMarginAndConfidenceAnnealing", "intuition": "Mode: explore. This loss function aims to improve upon the strong focal loss framework of the parents by introducing two novel, decoupled coupling mechanisms. The goal is to create a loss that is highly adaptive to both the objective difficulty (cost difference) and the model's subjective difficulty (confidence) for each pair, while maintaining numerical stability.\n\nInherited Ideas:\n1.  From both parents: The core structure is a focal-modulated Bradley-Terry loss: `modulating_factor * -logsigmoid(normalized_delta + margin)`. This retains the proven benefit of up-weighting difficult examples.\n2.  From both parents: The use of `ops.zscore` on the log-probability difference (`delta`) is kept for batch-level normalization, ensuring stability.\n3.  From both parents: The margin is based on the cost difference, establishing a clear link between objective pair difficulty and the desired separation in log-probability space.\n\nNew Coupling Ideas:\n1.  **Dynamic Cost-Difference Margin**: Instead of using `rank_gap`, which can be noisy, this loss computes a margin directly from the z-scored cost difference: `margin = beta * softplus(zscore(cost_b - cost_a))`. This provides a continuous, batch-normalized measure of how far apart the costs are. The `softplus` ensures the margin is always non-negative. This couples the desired log-probability separation directly and smoothly to the magnitude of the cost gap within the batch.\n2.  **Confidence-Annealed Focal Strength**: This loss introduces a refined mechanism for modulating the focal strength `gamma`. It uses a Gaussian-like function of the model's confidence, `gamma = gamma_base * exp(-alpha * delta^2)`. When the model is uncertain (`delta` is close to 0), the exponential term is close to 1, applying the full focal strength `gamma_base` to focus learning on these hard-to-distinguish pairs. As the model becomes more confident (either very positive or very negative `delta`), the `delta^2` term grows, causing the exponential to decay towards 0. This smoothly anneals the focal effect for easy examples, preventing the model from becoming overconfident and wasting capacity on pairs it has already learned well. This is a more continuous and symmetric alternative to the parent's `tanh`-based penalty.", "pseudocode": "1. Calculate the log-probability difference: delta = log_prob_w - log_prob_l.\n2. Normalize delta across the batch for stability: normalized_delta = zscore(delta).\n3. Calculate the absolute cost difference: cost_gap = cost_b - cost_a.\n4. Normalize the cost gap across the batch: cost_gap_zscored = zscore(cost_gap).\n5. Compute a dynamic margin based on the normalized cost gap: margin = beta * softplus(cost_gap_zscored.detach()).\n6. Compute a confidence-annealed focal strength: dynamic_gamma = gamma_base * exp(-alpha * delta.detach()^2).\n7. Calculate the model's win probability: p_win = sigmoid(delta).\n8. Compute the focal modulating factor: modulating_factor = (1 - p_win)^dynamic_gamma.\n9. Compute the core preference loss: loss_core = -logsigmoid(normalized_delta + margin).\n10. Apply the detached modulating factor: final_loss = modulating_factor.detach() * loss_core.\n11. Return the mean of the final loss.", "hyperparams": {"beta": 1.0, "gamma_base": 2.0, "alpha": 0.5}, "operators_used": ["zscore", "softplus", "exp", "sigmoid", "logsigmoid"], "implementation_hint": {"expects": ["cost_a", "cost_b", "log_prob_w", "log_prob_l"], "returns": "scalar"}, "code": "def generated_loss(batch, model_output, extra):\n    \"\"\"\n    Combines a focal loss framework with two new couplings:\n    1. A dynamic margin based on the z-scored cost difference.\n    2. A confidence-annealed focal gamma using an exponential decay on delta^2.\n    \"\"\"\n    # Read hyperparameters with defaults\n    beta = extra.get('beta', 1.0)\n    gamma_base = extra.get('gamma_base', 2.0)\n    alpha = extra.get('alpha', 0.5)\n\n    # Read required tensors from the batch\n    cost_a = batch['cost_a']\n    cost_b = batch['cost_b']\n    log_prob_w = batch['log_prob_w']\n    log_prob_l = batch['log_prob_l']\n    weight = batch.get('weight')\n\n    # 1. Calculate the log-probability difference\n    delta = log_prob_w - log_prob_l\n\n    # 2. Inherited: Normalize delta for stability\n    normalized_delta = ops.zscore(delta)\n\n    # 3. New Coupling 1: Dynamic Cost-Difference Margin\n    cost_gap = cost_b - cost_a\n    # zscore normalizes the cost gap's magnitude within the batch\n    cost_gap_zscored = ops.zscore(cost_gap)\n    # softplus ensures margin is non-negative. Detach to treat as a target.\n    margin = beta * F.softplus(cost_gap_zscored.detach())\n\n    # 4. New Coupling 2: Confidence-Annealed Focal Strength\n    # exp(-alpha * delta^2) is close to 1 for uncertain pairs (delta~0)\n    # and decays to 0 for confident pairs (|delta| >> 0).\n    # Detach delta to prevent gradients from flowing through the gamma calculation.\n    dynamic_gamma = gamma_base * torch.exp(-alpha * torch.pow(delta.detach(), 2))\n\n    # 5. Calculate probability for focal loss\n    p_win = torch.sigmoid(delta)\n\n    # 6. Compute the focal modulating factor\n    # Use clamp to prevent pow(0,0) issues if p_win is exactly 1.\n    modulating_factor = torch.pow(1.0 - p_win.clamp(max=1.0 - 1e-7), dynamic_gamma)\n\n    # 7. Compute the core preference loss\n    loss_core = -F.logsigmoid(normalized_delta + margin)\n\n    # 8. Apply the modulating factor to the core loss\n    # Detach the factor so it only re-weights the loss, not change the gradient direction\n    final_loss = modulating_factor.detach() * loss_core\n\n    # Apply optional instance weights if provided\n    if weight is not None:\n        final_loss = final_loss * weight\n\n    # Return the mean loss over the batch\n    return final_loss.mean()", "theoretical_basis": "A dynamically modulated Bradley-Terry preference model. The core is a logistic loss on z-scored log-probabilities. It introduces two novel couplings: (1) A dynamic margin directly proportional to the batch-normalized cost difference, creating a smooth and continuous separation target. (2) A focal modulation strength that is annealed by model confidence using a Gaussian-like function of the logit difference, which precisely targets learning on the most uncertain pairs while gracefully reducing pressure on confidently classified ones."}, "static_ok": true, "static_reason": "", "static_error_code": "", "dynamic_ok": true, "dynamic_reason": "ok", "loss_value": 0.10136627405881882, "grad_norm": 0.0, "pref_ok": false, "pref_reason": "pref_semantic_violation", "pref_mono_pass_rate": 0.7708333333333334, "pref_swap_pass_rate": 0.16666666666666666, "pref_gap_pass_rate": 1.0, "dynamic_error_code": "E_PREF_SEMANTIC"}
{"generation": 8, "index": 6, "attempt": 1, "ir": {"name": "FocalLossWithDynamicCostMarginAndConfidenceAnnealing", "intuition": "Repaired: The original loss failed the preference gate's swap check (`E_PREF_SEMANTIC`), indicating that when the model was wrong (log_prob_l > log_prob_w), increasing the loser's log probability sometimes incorrectly decreased the loss. This was because the `p_win` calculation used the raw `delta` (log_prob_w - log_prob_l), which could be negative, while the core loss used `normalized_delta`. This mismatch in how `delta` was used created conflicting gradient signals. The fix is to use the same `normalized_delta` for both the core loss term and the `p_win` calculation for the modulating factor. This ensures that the gradient direction is consistent and correctly penalizes the model for being wrong.", "pseudocode": "1. Calculate the log-probability difference: delta = log_prob_w - log_prob_l.\n2. Normalize delta across the batch for stability: normalized_delta = zscore(delta).\n3. Calculate the absolute cost difference: cost_gap = cost_b - cost_a.\n4. Normalize the cost gap across the batch: cost_gap_zscored = zscore(cost_gap).\n5. Compute a dynamic margin based on the normalized cost gap: margin = beta * softplus(cost_gap_zscored.detach()).\n6. Compute a confidence-annealed focal strength: dynamic_gamma = gamma_base * exp(-alpha * delta.detach()^2).\n7. Calculate the model's win probability using the normalized delta: p_win = sigmoid(normalized_delta).\n8. Compute the focal modulating factor: modulating_factor = (1 - p_win)^dynamic_gamma.\n9. Compute the core preference loss: loss_core = -logsigmoid(normalized_delta + margin).\n10. Apply the detached modulating factor: final_loss = modulating_factor.detach() * loss_core.\n11. Return the mean of the final loss.", "hyperparams": {"beta": 1.0, "gamma_base": 2.0, "alpha": 0.5}, "operators_used": ["zscore", "softplus", "exp", "sigmoid", "logsigmoid"], "implementation_hint": {"expects": ["cost_a", "cost_b", "log_prob_w", "log_prob_l"], "returns": "scalar"}, "code": "def generated_loss(batch, model_output, extra):\n    \"\"\"\n    Combines a focal loss framework with two new couplings:\n    1. A dynamic margin based on the z-scored cost difference.\n    2. A confidence-annealed focal gamma using an exponential decay on delta^2.\n    \"\"\"\n    # Read hyperparameters with defaults\n    beta = extra.get('beta', 1.0)\n    gamma_base = extra.get('gamma_base', 2.0)\n    alpha = extra.get('alpha', 0.5)\n\n    # Read required tensors from the batch\n    cost_a = batch['cost_a']\n    cost_b = batch['cost_b']\n    log_prob_w = batch['log_prob_w']\n    log_prob_l = batch['log_prob_l']\n    weight = batch.get('weight')\n\n    # 1. Calculate the log-probability difference\n    delta = log_prob_w - log_prob_l\n\n    # 2. Inherited: Normalize delta for stability\n    normalized_delta = ops.zscore(delta)\n\n    # 3. New Coupling 1: Dynamic Cost-Difference Margin\n    cost_gap = cost_b - cost_a\n    # zscore normalizes the cost gap's magnitude within the batch\n    cost_gap_zscored = ops.zscore(cost_gap)\n    # softplus ensures margin is non-negative. Detach to treat as a target.\n    margin = beta * F.softplus(cost_gap_zscored.detach())\n\n    # 4. New Coupling 2: Confidence-Annealed Focal Strength\n    # exp(-alpha * delta^2) is close to 1 for uncertain pairs (delta~0)\n    # and decays to 0 for confident pairs (|delta| >> 0).\n    # Detach delta to prevent gradients from flowing through the gamma calculation.\n    dynamic_gamma = gamma_base * torch.exp(-alpha * torch.pow(delta.detach(), 2))\n\n    # 5. Calculate probability for focal loss using NORMALIZED delta for consistency\n    p_win = torch.sigmoid(normalized_delta)\n\n    # 6. Compute the focal modulating factor\n    # Use clamp to prevent pow(0,0) issues if p_win is exactly 1.\n    modulating_factor = torch.pow(1.0 - p_win.clamp(max=1.0 - 1e-7), dynamic_gamma)\n\n    # 7. Compute the core preference loss\n    loss_core = -F.logsigmoid(normalized_delta + margin)\n\n    # 8. Apply the modulating factor to the core loss\n    # Detach the factor so it only re-weights the loss, not change the gradient direction\n    final_loss = modulating_factor.detach() * loss_core\n\n    # Apply optional instance weights if provided\n    if weight is not None:\n        final_loss = final_loss * weight\n\n    # Return the mean loss over the batch\n    return final_loss.mean()", "theoretical_basis": ""}, "static_ok": true, "static_reason": "", "static_error_code": "", "dynamic_ok": true, "dynamic_reason": "ok", "loss_value": 0.10136627405881882, "grad_norm": 0.0, "pref_ok": true, "pref_reason": "ok", "pref_mono_pass_rate": 0.771484375, "pref_swap_pass_rate": 1.0, "pref_gap_pass_rate": 1.0}
{"generation": 8, "index": 7, "attempt": 0, "ir": {"name": "FocalLossWithAdaptiveConfidencePenalty", "intuition": "Mode: explore. This loss function hybridizes the successful focal loss and z-score normalization from its parents, but introduces a more nuanced mechanism for modulating the focal strength based on both model confidence and the objective difficulty of the preference pair. The goal is to focus learning on examples where the model is uncertain, but *only when* the cost difference justifies such focus, preventing overfitting on noisy or ambiguous pairs.\n\nInherited Ideas:\n1. From both parents: The core structure is a focal-modulated Bradley-Terry loss: `(1 - p_win)^gamma * -logsigmoid(...)`. This retains the proven benefit of up-weighting difficult examples.\n2. From both parents: The use of `ops.zscore` on the log-probability difference (`delta`) is kept for batch-level normalization, ensuring stability and consistent scaling.\n3. From Parent 1 (`FocalLossWithClippedCostSensitiveMargin`): The idea of using `tanh(abs(delta))` as a measure of model confidence.\n\nNew Coupling Ideas:\n1. **Difficulty-Gated Confidence Penalty**: Instead of simply penalizing all confident predictions as Parent 1 does, this new coupling makes the penalty itself conditional on the pair's difficulty. We calculate a `difficulty_gate = sigmoid(beta * rank_gap)`. This gate is close to 1 for pairs with a large cost difference (high rank gap) and close to 0 for pairs with similar costs. The confidence penalty from `tanh(abs(delta))` is then multiplied by this gate. The effect is that the focal strength `gamma` is only reduced for *easy, high-confidence* predictions. For *hard, high-confidence* predictions (e.g., a pair with a tiny cost gap that the model is very sure about), the gate is near zero, the penalty is nullified, and `gamma` remains high, encouraging the model to reconsider its overconfidence on a difficult pair.\n2. **Dynamic Margin based on Cost and Confidence**: We introduce a margin that adapts to both the objective cost gap and the model's current confidence. The margin is `margin = softplus(cost_gap_zscored) * (1 - tanh(abs(normalized_delta)))`. The `softplus(cost_gap_zscored)` term sets a larger target separation for pairs that are objectively far apart in cost. The `(1 - tanh(abs(normalized_delta)))` term scales this target down as the model becomes more confident. This coupling creates a dynamic learning target: for uncertain pairs, the margin is large, pushing for a clear decision; for confident pairs, the margin shrinks, preventing the loss from pushing logits to infinity and promoting better calibration.", "pseudocode": "1. Calculate the log-probability difference: delta = log_prob_w - log_prob_l.\n2. Normalize delta across the batch for stability: normalized_delta = zscore(delta).\n3. Calculate the absolute cost difference and its z-score: cost_gap = cost_b - cost_a; cost_gap_zscored = zscore(cost_gap).\n4. Compute the cost rank gap: rank_gap = rank_gap(cost_a, cost_b).\n5. New Coupling 1: Compute a difficulty gate based on rank gap: difficulty_gate = sigmoid(beta * rank_gap.detach()).\n6. New Coupling 1: Compute a confidence penalty (high for confident predictions): confidence_penalty = tanh(abs(delta.detach())).\n7. New Coupling 1: Calculate the gated confidence penalty and the dynamic gamma: dynamic_gamma = gamma_base * (1.0 - difficulty_gate * confidence_penalty).\n8. New Coupling 2: Calculate a dynamic margin based on cost gap and model confidence: margin = softplus(cost_gap_zscored.detach()) * (1.0 - tanh(abs(normalized_delta.detach()))).\n9. Calculate the model's win probability for the focal term: p_win = sigmoid(delta).\n10. Compute the focal modulating factor: modulating_factor = (1 - p_win)^dynamic_gamma.\n11. Compute the core preference loss: loss_core = -logsigmoid(normalized_delta + margin).\n12. Apply the detached modulating factor: final_loss = modulating_factor.detach() * loss_core.\n13. Return the mean of the final loss.", "hyperparams": {"gamma_base": 2.5, "beta": 0.5}, "operators_used": ["zscore", "rank_gap", "sigmoid", "tanh", "softplus", "logsigmoid"], "implementation_hint": {"expects": ["cost_a", "cost_b", "log_prob_w", "log_prob_l"], "returns": "scalar"}, "code": "def generated_loss(batch, model_output, extra):\n    \"\"\"\n    Combines focal loss with z-score normalization and introduces two new couplings:\n    1. A difficulty-gated confidence penalty for modulating the focal gamma.\n    2. A dynamic margin sensitive to both cost gap and model confidence.\n    \"\"\"\n    # Read hyperparameters with defaults\n    gamma_base = extra.get('gamma_base', 2.5)\n    beta = extra.get('beta', 0.5)\n\n    # Read required tensors from the batch\n    cost_a = batch['cost_a']\n    cost_b = batch['cost_b']\n    log_prob_w = batch['log_prob_w']\n    log_prob_l = batch['log_prob_l']\n    weight = batch.get('weight')\n\n    # 1. Calculate and normalize the log-probability difference (Inherited)\n    delta = log_prob_w - log_prob_l\n    normalized_delta = ops.zscore(delta)\n\n    # 2. Calculate cost-related metrics\n    cost_gap = cost_b - cost_a\n    with torch.no_grad():\n        cost_gap_zscored = ops.zscore(cost_gap)\n        rank_gap = ops.rank_gap(cost_a, cost_b)\n\n    # 3. New Coupling 1: Difficulty-Gated Confidence Penalty for Gamma\n    # Gate is high for easy pairs (large rank_gap), low for hard pairs.\n    difficulty_gate = torch.sigmoid(beta * rank_gap)\n    # Penalty is high for confident predictions (large abs(delta)).\n    confidence_penalty = torch.tanh(torch.abs(delta.detach()))\n    # Reduce gamma only for easy, high-confidence pairs.\n    dynamic_gamma = gamma_base * (1.0 - difficulty_gate * confidence_penalty)\n\n    # 4. New Coupling 2: Dynamic Margin based on Cost and Confidence\n    # Margin is large for objectively different pairs (high cost_gap_zscored)\n    # and for pairs where the model is uncertain (low confidence).\n    model_confidence = torch.tanh(torch.abs(normalized_delta.detach()))\n    margin = F.softplus(cost_gap_zscored) * (1.0 - model_confidence)\n\n    # 5. Compute Focal Loss components (Inherited)\n    p_win = torch.sigmoid(delta)\n    modulating_factor = torch.pow(1.0 - p_win, dynamic_gamma).detach()\n    \n    # 6. Compute the core preference loss\n    loss_core = -F.logsigmoid(normalized_delta + margin)\n\n    # 7. Apply the modulating factor to the core loss\n    final_loss = modulating_factor * loss_core\n\n    # Apply optional instance weights if provided\n    if weight is not None:\n        final_loss = final_loss * weight\n\n    # Return the mean loss over the batch\n    return final_loss.mean()", "theoretical_basis": "A dynamically modulated Bradley-Terry preference model. The loss combines a focal modulation scheme with an adaptive margin. The key innovations are two couplings: (1) A 'difficulty-gated confidence penalty' where the focal strength is reduced for high-confidence predictions only if the underlying preference pair is objectively easy (large cost gap), preventing over-penalization of nuanced distinctions. (2) A margin that is dynamically scaled by both the z-scored cost gap and the model's confidence, creating a larger separation target for pairs that are both objectively different and about which the model is uncertain."}, "static_ok": true, "static_reason": "", "static_error_code": "", "dynamic_ok": true, "dynamic_reason": "ok", "loss_value": 0.07167678326368332, "grad_norm": 0.0, "pref_ok": false, "pref_reason": "pref_semantic_violation", "pref_mono_pass_rate": 0.7805989583333334, "pref_swap_pass_rate": 0.5, "pref_gap_pass_rate": 1.0, "dynamic_error_code": "E_PREF_SEMANTIC"}
{"generation": 8, "index": 7, "attempt": 1, "ir": {"name": "FocalLossWithAdaptiveConfidencePenalty", "intuition": "Repaired: The original loss failed the semantic preference gate (E_PREF_SEMANTIC) because the `p_win` calculation, used to determine the focal modulating factor, was based on the unnormalized `delta`, while the core loss was based on the `normalized_delta`. This created a mismatch where the focal term could incorrectly up-weight pairs that the core loss function already found easy, violating the preference learning objective. The fix is to compute `p_win` using the same `normalized_delta` that is used in the core loss calculation. This ensures the focal modulation correctly targets examples that are difficult with respect to the normalized logit difference, aligning the entire loss function with the goal of preferring the lower-cost solution.", "pseudocode": "1. Calculate the log-probability difference: delta = log_prob_w - log_prob_l.\n2. Normalize delta across the batch for stability: normalized_delta = zscore(delta).\n3. Calculate the absolute cost difference and its z-score: cost_gap = cost_b - cost_a; cost_gap_zscored = zscore(cost_gap).\n4. Compute the cost rank gap: rank_gap = rank_gap(cost_a, cost_b).\n5. New Coupling 1: Compute a difficulty gate based on rank gap: difficulty_gate = sigmoid(beta * rank_gap.detach()).\n6. New Coupling 1: Compute a confidence penalty (high for confident predictions): confidence_penalty = tanh(abs(delta.detach())).\n7. New Coupling 1: Calculate the gated confidence penalty and the dynamic gamma: dynamic_gamma = gamma_base * (1.0 - difficulty_gate * confidence_penalty).\n8. New Coupling 2: Calculate a dynamic margin based on cost gap and model confidence: margin = softplus(cost_gap_zscored.detach()) * (1.0 - tanh(abs(normalized_delta.detach()))).\n9. Calculate the model's win probability for the focal term using the *normalized* delta: p_win = sigmoid(normalized_delta).\n10. Compute the focal modulating factor: modulating_factor = (1 - p_win)^dynamic_gamma.\n11. Compute the core preference loss: loss_core = -logsigmoid(normalized_delta + margin).\n12. Apply the detached modulating factor: final_loss = modulating_factor.detach() * loss_core.\n13. Return the mean of the final loss.", "hyperparams": {"gamma_base": 2.5, "beta": 0.5}, "operators_used": ["zscore", "rank_gap", "sigmoid", "tanh", "softplus", "logsigmoid"], "implementation_hint": {"expects": ["cost_a", "cost_b", "log_prob_w", "log_prob_l"], "returns": "scalar"}, "code": "def generated_loss(batch, model_output, extra):\n    \"\"\"\n    Combines focal loss with z-score normalization and introduces two new couplings:\n    1. A difficulty-gated confidence penalty for modulating the focal gamma.\n    2. A dynamic margin sensitive to both cost gap and model confidence.\n    \"\"\"\n    # Read hyperparameters with defaults\n    gamma_base = extra.get('gamma_base', 2.5)\n    beta = extra.get('beta', 0.5)\n\n    # Read required tensors from the batch\n    cost_a = batch['cost_a']\n    cost_b = batch['cost_b']\n    log_prob_w = batch['log_prob_w']\n    log_prob_l = batch['log_prob_l']\n    weight = batch.get('weight')\n\n    # 1. Calculate and normalize the log-probability difference (Inherited)\n    delta = log_prob_w - log_prob_l\n    normalized_delta = ops.zscore(delta)\n\n    # 2. Calculate cost-related metrics\n    cost_gap = cost_b - cost_a\n    with torch.no_grad():\n        cost_gap_zscored = ops.zscore(cost_gap)\n        rank_gap = ops.rank_gap(cost_a, cost_b)\n\n    # 3. New Coupling 1: Difficulty-Gated Confidence Penalty for Gamma\n    # Gate is high for easy pairs (large rank_gap), low for hard pairs.\n    difficulty_gate = torch.sigmoid(beta * rank_gap)\n    # Penalty is high for confident predictions (large abs(delta)).\n    confidence_penalty = torch.tanh(torch.abs(delta.detach()))\n    # Reduce gamma only for easy, high-confidence pairs.\n    dynamic_gamma = gamma_base * (1.0 - difficulty_gate * confidence_penalty)\n\n    # 4. New Coupling 2: Dynamic Margin based on Cost and Confidence\n    # Margin is large for objectively different pairs (high cost_gap_zscored)\n    # and for pairs where the model is uncertain (low confidence).\n    model_confidence = torch.tanh(torch.abs(normalized_delta.detach()))\n    margin = F.softplus(cost_gap_zscored) * (1.0 - model_confidence)\n\n    # 5. Compute Focal Loss components (Inherited)\n    # FIX: p_win must be computed from normalized_delta to match the core loss term.\n    p_win = torch.sigmoid(normalized_delta)\n    modulating_factor = torch.pow(1.0 - p_win, dynamic_gamma).detach()\n    \n    # 6. Compute the core preference loss\n    loss_core = -F.logsigmoid(normalized_delta + margin)\n\n    # 7. Apply the modulating factor to the core loss\n    final_loss = modulating_factor * loss_core\n\n    # Apply optional instance weights if provided\n    if weight is not None:\n        final_loss = final_loss * weight\n\n    # Return the mean loss over the batch\n    return final_loss.mean()", "theoretical_basis": ""}, "static_ok": true, "static_reason": "", "static_error_code": "", "dynamic_ok": true, "dynamic_reason": "ok", "loss_value": 0.07167678326368332, "grad_norm": 0.0, "pref_ok": false, "pref_reason": "pref_semantic_violation", "pref_mono_pass_rate": 0.783203125, "pref_swap_pass_rate": 0.0, "pref_gap_pass_rate": 1.0, "dynamic_error_code": "E_PREF_SEMANTIC"}
{"generation": 8, "index": 7, "attempt": 2, "ir": {"name": "FocalLossWithAdaptiveConfidencePenalty", "intuition": "The loss failed the semantic preference gate (E_PREF_SEMANTIC) because the `p_win` calculation was based on `normalized_delta`, which is a z-scored value. This decouples the probability from the actual log-probability difference, leading to incorrect preference signals, especially when the batch contains outliers. The fix is to compute `p_win` using the original, unnormalized `delta = log_prob_w - log_prob_l`. This ensures the focal modulating factor correctly reflects the model's raw confidence (`p_win = sigmoid(delta)`), aligning the loss with the fundamental goal of preferring the lower-cost solution, while still using the stabilized `normalized_delta` in the core loss term.", "pseudocode": "1. Calculate the log-probability difference: delta = log_prob_w - log_prob_l.\n2. Normalize delta across the batch for stability: normalized_delta = zscore(delta).\n3. Calculate the absolute cost difference and its z-score: cost_gap = cost_b - cost_a; cost_gap_zscored = zscore(cost_gap).\n4. Compute the cost rank gap: rank_gap = rank_gap(cost_a, cost_b).\n5. Compute a difficulty gate based on rank gap: difficulty_gate = sigmoid(beta * rank_gap.detach()).\n6. Compute a confidence penalty (high for confident predictions): confidence_penalty = tanh(abs(delta.detach())).\n7. Calculate the gated confidence penalty and the dynamic gamma: dynamic_gamma = gamma_base * (1.0 - difficulty_gate * confidence_penalty).\n8. Calculate a dynamic margin based on cost gap and model confidence: margin = softplus(cost_gap_zscored.detach()) * (1.0 - tanh(abs(normalized_delta.detach()))).\n9. Calculate the model's win probability for the focal term using the *original* delta: p_win = sigmoid(delta).\n10. Compute the focal modulating factor: modulating_factor = (1 - p_win)^dynamic_gamma.\n11. Compute the core preference loss: loss_core = -logsigmoid(normalized_delta + margin).\n12. Apply the detached modulating factor: final_loss = modulating_factor.detach() * loss_core.\n13. Return the mean of the final loss.", "hyperparams": {"gamma_base": 2.5, "beta": 0.5}, "operators_used": ["zscore", "rank_gap", "sigmoid", "tanh", "softplus", "logsigmoid"], "implementation_hint": {"expects": ["cost_a", "cost_b", "log_prob_w", "log_prob_l"], "returns": "scalar"}, "code": "def generated_loss(batch, model_output, extra):\n    \"\"\"\n    Combines focal loss with z-score normalization and introduces two new couplings:\n    1. A difficulty-gated confidence penalty for modulating the focal gamma.\n    2. A dynamic margin sensitive to both cost gap and model confidence.\n    \"\"\"\n    # Read hyperparameters with defaults\n    gamma_base = extra.get('gamma_base', 2.5)\n    beta = extra.get('beta', 0.5)\n\n    # Read required tensors from the batch\n    cost_a = batch['cost_a']\n    cost_b = batch['cost_b']\n    log_prob_w = batch['log_prob_w']\n    log_prob_l = batch['log_prob_l']\n    weight = batch.get('weight')\n\n    # 1. Calculate and normalize the log-probability difference (Inherited)\n    delta = log_prob_w - log_prob_l\n    normalized_delta = ops.zscore(delta)\n\n    # 2. Calculate cost-related metrics\n    cost_gap = cost_b - cost_a\n    with torch.no_grad():\n        cost_gap_zscored = ops.zscore(cost_gap)\n        rank_gap = ops.rank_gap(cost_a, cost_b)\n\n    # 3. New Coupling 1: Difficulty-Gated Confidence Penalty for Gamma\n    # Gate is high for easy pairs (large rank_gap), low for hard pairs.\n    difficulty_gate = torch.sigmoid(beta * rank_gap)\n    # Penalty is high for confident predictions (large abs(delta)).\n    confidence_penalty = torch.tanh(torch.abs(delta.detach()))\n    # Reduce gamma only for easy, high-confidence pairs.\n    dynamic_gamma = gamma_base * (1.0 - difficulty_gate * confidence_penalty)\n\n    # 4. New Coupling 2: Dynamic Margin based on Cost and Confidence\n    # Margin is large for objectively different pairs (high cost_gap_zscored)\n    # and for pairs where the model is uncertain (low confidence).\n    model_confidence = torch.tanh(torch.abs(normalized_delta.detach()))\n    margin = F.softplus(cost_gap_zscored) * (1.0 - model_confidence)\n\n    # 5. Compute Focal Loss components (Inherited)\n    # FIX: p_win must be computed from the original delta to correctly represent model confidence.\n    p_win = torch.sigmoid(delta)\n    modulating_factor = torch.pow(1.0 - p_win, dynamic_gamma).detach()\n    \n    # 6. Compute the core preference loss\n    loss_core = -F.logsigmoid(normalized_delta + margin)\n\n    # 7. Apply the modulating factor to the core loss\n    final_loss = modulating_factor * loss_core\n\n    # Apply optional instance weights if provided\n    if weight is not None:\n        final_loss = final_loss * weight\n\n    # Return the mean loss over the batch\n    return final_loss.mean()", "theoretical_basis": ""}, "static_ok": true, "static_reason": "", "static_error_code": "", "dynamic_ok": true, "dynamic_reason": "ok", "loss_value": 0.07167678326368332, "grad_norm": 0.0, "pref_ok": false, "pref_reason": "pref_semantic_violation", "pref_mono_pass_rate": 0.779296875, "pref_swap_pass_rate": 0.16666666666666666, "pref_gap_pass_rate": 1.0, "dynamic_error_code": "E_PREF_SEMANTIC"}
{"generation": 9, "index": 0, "attempt": 0, "ir": {"name": "ConfidenceScheduledFocalLoss", "intuition": "Mode: explore. This loss function aims to improve upon the focal loss framework by creating a tighter coupling between the focal modulation strength (gamma) and the cost landscape, while ensuring stability through normalization. It inherits the core focal loss structure and z-score normalization from its parents, but introduces two novel couplings to make the learning process more adaptive.\n\nInherited Ideas:\n1. From both parents (`FocalLossWithClippedCostSensitiveMargin`, `AdaptiveFocalLogsigmoidWithClippedMargin`): The foundational structure is a focal-modulated Bradley-Terry loss: `(1 - p_win)^gamma * -logsigmoid(...)`. This retains the proven benefit of up-weighting difficult examples.\n2. From both parents: The use of `ops.zscore` on the log-probability difference (`delta`) is kept for batch-level normalization, ensuring stability and consistent scaling of the core learning signal.\n\nNew Coupling Ideas:\n1. **Confidence-Scheduled Margin**: Instead of a simple rank-based margin, we introduce a margin that is scheduled by the model's confidence. The margin is calculated as `beta * softplus(zscore(cost_gap)) * (1 - tanh(abs(delta.detach())))`. This means the margin is largest for pairs that are objectively far apart in cost (high `cost_gap`) but for which the model is *uncertain* (low `tanh(abs(delta))`). For pairs the model is already confident about, the margin shrinks, reducing the push and preventing overfitting.\n2. **Cost-Gap Modulated Focal Strength**: We invert the logic of Parent 1. Instead of penalizing gamma by confidence, we amplify it based on the normalized cost difference. The focal strength `gamma` is `gamma_base * softplus(zscore(cost_gap.detach()))`. This couples the hard-example mining strength directly to the objective difficulty of a pair. Pairs with a large cost difference are considered more important to get right, so they receive a higher focal modulation, forcing the model to pay more attention if it gets them wrong.", "pseudocode": "1. Calculate the log-probability difference: delta = log_prob_w - log_prob_l.\n2. Normalize delta across the batch for stability: normalized_delta = zscore(delta).\n3. Calculate the absolute cost difference: cost_gap = cost_b - cost_a.\n4. Normalize the cost gap: cost_gap_zscored = zscore(cost_gap).\n5. Compute the model's confidence: confidence = tanh(abs(delta.detach())).\n6. Compute a confidence-scheduled margin: margin = beta * softplus(cost_gap_zscored.detach()) * (1.0 - confidence).\n7. Compute a cost-gap modulated focal strength: dynamic_gamma = gamma_base * softplus(cost_gap_zscored.detach()).\n8. Calculate the model's win probability: p_win = sigmoid(delta).\n9. Compute the focal modulating factor: modulating_factor = (1 - p_win)^dynamic_gamma.\n10. Compute the core preference loss: loss_core = -logsigmoid(normalized_delta + margin).\n11. Apply the detached modulating factor: final_loss = modulating_factor.detach() * loss_core.\n12. Return the mean of the final loss.", "hyperparams": {"beta": 1.0, "gamma_base": 2.0}, "operators_used": ["zscore", "softplus", "tanh", "sigmoid", "logsigmoid"], "implementation_hint": {"expects": ["cost_a", "cost_b", "log_prob_w", "log_prob_l"], "returns": "scalar"}, "code": "def generated_loss(batch, model_output, extra):\n    \"\"\"\n    Combines focal loss and z-score normalization with two new couplings:\n    1. A margin scheduled by model confidence and the z-scored cost gap.\n    2. A focal gamma modulated directly by the z-scored cost gap.\n    \"\"\"\n    # Read hyperparameters with defaults\n    beta = extra.get('beta', 1.0)\n    gamma_base = extra.get('gamma_base', 2.0)\n\n    # Read required tensors from the batch\n    cost_a = batch['cost_a']\n    cost_b = batch['cost_b']\n    log_prob_w = batch['log_prob_w']\n    log_prob_l = batch['log_prob_l']\n    weight = batch.get('weight')\n\n    # 1. Calculate the log-probability difference\n    delta = log_prob_w - log_prob_l\n\n    # 2. Inherited: Normalize delta for stability\n    normalized_delta = ops.zscore(delta)\n\n    # 3. Calculate and normalize cost gap\n    cost_gap = cost_b - cost_a\n    cost_gap_zscored = ops.zscore(cost_gap)\n\n    # 4. New Coupling 1: Confidence-Scheduled Margin\n    # Confidence is high when abs(delta) is large\n    confidence = torch.tanh(torch.abs(delta.detach()))\n    # Margin is large for high cost_gap and low confidence\n    margin_scale = beta * F.softplus(cost_gap_zscored.detach())\n    margin = margin_scale * (1.0 - confidence)\n\n    # 5. New Coupling 2: Cost-Gap Modulated Focal Strength\n    # Gamma is higher for pairs with a larger cost gap\n    dynamic_gamma = gamma_base * F.softplus(cost_gap_zscored.detach())\n    \n    # 6. Inherited: Calculate probability for focal loss\n    p_win = torch.sigmoid(delta)\n\n    # 7. Compute the focal modulating factor\n    # Use clamp to prevent instability from very large dynamic_gamma\n    modulating_factor = torch.pow(1.0 - p_win, torch.clamp(dynamic_gamma, max=10.0))\n\n    # 8. Compute the core preference loss\n    loss_core = -F.logsigmoid(normalized_delta + margin)\n\n    # 9. Apply the modulating factor to the core loss\n    # Detach the factor so it only re-weights the loss, not change the gradient direction\n    final_loss = modulating_factor.detach() * loss_core\n\n    # Apply optional instance weights if provided\n    if weight is not None:\n        final_loss = final_loss * weight\n\n    # Return the mean loss over the batch\n    return final_loss.mean()", "theoretical_basis": "A dynamically modulated Bradley-Terry preference model. The core is a logistic loss on z-scored log-probabilities. It features two main couplings: (1) A margin whose magnitude is dynamically scaled by both the batch-normalized cost gap and the model's confidence, creating a larger separation target for objectively difficult but uncertain pairs. (2) A focal modulation strength that is directly proportional to the normalized cost gap, concentrating learning on pairs that are objectively important to distinguish."}, "static_ok": true, "static_reason": "", "static_error_code": "", "dynamic_ok": true, "dynamic_reason": "ok", "loss_value": 0.15510910749435425, "grad_norm": 0.0, "pref_ok": false, "pref_reason": "pref_semantic_violation", "pref_mono_pass_rate": 0.8046875, "pref_swap_pass_rate": 0.16666666666666666, "pref_gap_pass_rate": 1.0, "dynamic_error_code": "E_PREF_SEMANTIC"}
{"generation": 9, "index": 0, "attempt": 1, "ir": {"name": "ConfidenceScheduledFocalLoss", "intuition": "Repaired: The original loss failed the preference gate's `swap_pass_rate` check (E_PREF_SEMANTIC), indicating it did not consistently prefer the lower-cost solution. This was because the `modulating_factor` used `p_win = sigmoid(delta)`, where `delta` was the raw log-probability difference, while the core loss used `normalized_delta = zscore(delta)`. This mismatch meant that for some pairs, a high raw `delta` (leading to a small modulating factor) could be paired with a low normalized `delta` (leading to a high core loss), inverting the preference signal. The fix is to use the same `normalized_delta` to compute `p_win` for the modulating factor, ensuring both components of the loss are aligned and consistently prefer the higher log-probability (lower cost) solution.", "pseudocode": "1. Calculate the log-probability difference: delta = log_prob_w - log_prob_l.\n2. Normalize delta across the batch for stability: normalized_delta = zscore(delta).\n3. Calculate the absolute cost difference: cost_gap = cost_b - cost_a.\n4. Normalize the cost gap: cost_gap_zscored = zscore(cost_gap).\n5. Compute the model's confidence: confidence = tanh(abs(normalized_delta.detach())).\n6. Compute a confidence-scheduled margin: margin = beta * softplus(cost_gap_zscored.detach()) * (1.0 - confidence).\n7. Compute a cost-gap modulated focal strength: dynamic_gamma = gamma_base * softplus(cost_gap_zscored.detach()).\n8. Calculate the model's win probability using the normalized delta: p_win = sigmoid(normalized_delta).\n9. Compute the focal modulating factor: modulating_factor = (1 - p_win)^dynamic_gamma.\n10. Compute the core preference loss: loss_core = -logsigmoid(normalized_delta + margin).\n11. Apply the detached modulating factor: final_loss = modulating_factor.detach() * loss_core.\n12. Return the mean of the final loss.", "hyperparams": {"beta": 1.0, "gamma_base": 2.0}, "operators_used": ["zscore", "softplus", "tanh", "sigmoid", "logsigmoid"], "implementation_hint": {"expects": ["cost_a", "cost_b", "log_prob_w", "log_prob_l"], "returns": "scalar"}, "code": "def generated_loss(batch, model_output, extra):\n    \"\"\"\n    Combines focal loss and z-score normalization with two new couplings:\n    1. A margin scheduled by model confidence and the z-scored cost gap.\n    2. A focal gamma modulated directly by the z-scored cost gap.\n    \"\"\"\n    # Read hyperparameters with defaults\n    beta = extra.get('beta', 1.0)\n    gamma_base = extra.get('gamma_base', 2.0)\n\n    # Read required tensors from the batch\n    cost_a = batch['cost_a']\n    cost_b = batch['cost_b']\n    log_prob_w = batch['log_prob_w']\n    log_prob_l = batch['log_prob_l']\n    weight = batch.get('weight')\n\n    # 1. Calculate the log-probability difference\n    delta = log_prob_w - log_prob_l\n\n    # 2. Inherited: Normalize delta for stability\n    normalized_delta = ops.zscore(delta)\n\n    # 3. Calculate and normalize cost gap\n    cost_gap = cost_b - cost_a\n    cost_gap_zscored = ops.zscore(cost_gap)\n\n    # 4. New Coupling 1: Confidence-Scheduled Margin\n    # Confidence is high when abs(normalized_delta) is large\n    confidence = torch.tanh(torch.abs(normalized_delta.detach()))\n    # Margin is large for high cost_gap and low confidence\n    margin_scale = beta * F.softplus(cost_gap_zscored.detach())\n    margin = margin_scale * (1.0 - confidence)\n\n    # 5. New Coupling 2: Cost-Gap Modulated Focal Strength\n    # Gamma is higher for pairs with a larger cost gap\n    dynamic_gamma = gamma_base * F.softplus(cost_gap_zscored.detach())\n    \n    # 6. Inherited: Calculate probability for focal loss using the normalized delta\n    p_win = torch.sigmoid(normalized_delta)\n\n    # 7. Compute the focal modulating factor\n    # Use clamp to prevent instability from very large dynamic_gamma\n    modulating_factor = torch.pow(1.0 - p_win, torch.clamp(dynamic_gamma, max=10.0))\n\n    # 8. Compute the core preference loss\n    loss_core = -F.logsigmoid(normalized_delta + margin)\n\n    # 9. Apply the modulating factor to the core loss\n    # Detach the factor so it only re-weights the loss, not change the gradient direction\n    final_loss = modulating_factor.detach() * loss_core\n\n    # Apply optional instance weights if provided\n    if weight is not None:\n        final_loss = final_loss * weight\n\n    # Return the mean loss over the batch\n    return final_loss.mean()", "theoretical_basis": ""}, "static_ok": true, "static_reason": "", "static_error_code": "", "dynamic_ok": true, "dynamic_reason": "ok", "loss_value": 0.15510910749435425, "grad_norm": 0.0, "pref_ok": true, "pref_reason": "ok", "pref_mono_pass_rate": 0.7630208333333334, "pref_swap_pass_rate": 0.8333333333333334, "pref_gap_pass_rate": 1.0}
{"generation": 9, "index": 1, "attempt": 0, "ir": {"name": "ConfidenceAwareFocalLossWithAdaptiveMargin", "intuition": "Mode: explore. This loss function aims to improve upon its parents by introducing a more nuanced interaction between the focal loss modulation and the margin. It inherits the stable, normalized core from both parents but introduces two new couplings: one that makes the margin sensitive to model confidence and another that makes the focal strength sensitive to the cost gap.\n\nInherited Ideas:\n1. From both parents (`FocalLossWithClippedCostSensitiveMargin` and `AdaptiveFocalLogsigmoidWithClippedMargin`): The foundational structure is a focal-modulated Bradley-Terry loss: `(1 - p_win)^gamma * -logsigmoid(normalized_delta + margin)`. This retains the proven benefit of up-weighting difficult examples.\n2. From both parents: The use of `ops.zscore` on the log-probability difference (`delta`) is kept for batch-level normalization, ensuring stability and consistent scaling.\n3. From Parent 2 (`AdaptiveFocalLogsigmoidWithClippedMargin`): The use of a clipped, rank-based margin `clamp(beta * rank_gap, ...)` is retained to provide an adaptive separation target while preventing gradient saturation from excessively large margins.\n\nNew Coupling Ideas:\n1. **Confidence-Adaptive Margin Scaling**: Instead of a fixed `beta` for the margin, we introduce a dynamic scale `beta_final = beta_base * (1 + tanh(abs(delta.detach())))`. This couples the margin size to the model's confidence. When the model is very confident (large `abs(delta)`), the margin is increased, pushing for an even greater separation to solidify the correct prediction. When the model is uncertain (small `abs(delta)`), the margin is smaller, focusing the learning signal on just getting the direction right rather than enforcing a large separation that might be premature.\n2. **Cost-Gap-Modulated Focal Strength**: The focal strength `gamma` is dynamically adjusted based on the normalized cost difference: `dynamic_gamma = gamma_base * softplus(zscore(cost_b - cost_a).detach())`. This couples the hard-example mining intensity to the objective difficulty of the pair. Pairs with a large, significant cost difference (high z-score) will have a higher `gamma`, focusing the model more intensely on getting these important, high-signal pairs correct. Conversely, pairs with a small or average cost difference receive less focal modulation, preventing the model from overfitting on minor preference distinctions.", "pseudocode": "1. Calculate the log-probability difference: delta = log_prob_w - log_prob_l.\n2. Normalize delta across the batch for stability: normalized_delta = zscore(delta).\n3. Calculate the absolute cost difference: cost_gap = cost_b - cost_a.\n4. Compute a confidence-adaptive beta for the margin: beta_final = beta_base * (1 + tanh(abs(delta.detach()))).\n5. Compute the cost rank gap: rank_gap = rank_gap(cost_a, cost_b).\n6. Calculate the clipped, confidence-adaptive margin: margin = clamp(beta_final * rank_gap, min=0, max=margin_max).\n7. Compute a cost-gap-modulated focal strength: dynamic_gamma = gamma_base * softplus(zscore(cost_gap).detach()).\n8. Calculate the model's win probability: p_win = sigmoid(delta).\n9. Compute the focal modulating factor: modulating_factor = (1 - p_win)^dynamic_gamma.\n10. Compute the core preference loss: loss_core = -logsigmoid(normalized_delta + margin).\n11. Apply the detached modulating factor: final_loss = modulating_factor.detach() * loss_core.\n12. Return the mean of the final loss.", "hyperparams": {"beta_base": 1.0, "gamma_base": 2.0, "margin_max": 3.0}, "operators_used": ["zscore", "tanh", "rank_gap", "clamp", "softplus", "sigmoid", "logsigmoid"], "implementation_hint": {"expects": ["cost_a", "cost_b", "log_prob_w", "log_prob_l"], "returns": "scalar"}, "code": "def generated_loss(batch, model_output, extra):\n    \"\"\"\n    Combines focal loss and z-score normalization with two new couplings:\n    1. Margin scale is adapted by model confidence (tanh(|delta|)).\n    2. Focal gamma is adapted by the z-scored cost gap (softplus(zscore(cost_gap))).\n    \"\"\"\n    # Read hyperparameters with defaults\n    beta_base = extra.get('beta_base', 1.0)\n    gamma_base = extra.get('gamma_base', 2.0)\n    margin_max = extra.get('margin_max', 3.0)\n\n    # Read required tensors from the batch\n    cost_a = batch['cost_a']\n    cost_b = batch['cost_b']\n    log_prob_w = batch['log_prob_w']\n    log_prob_l = batch['log_prob_l']\n    weight = batch.get('weight')\n\n    # 1. Calculate the log-probability difference\n    delta = log_prob_w - log_prob_l\n\n    # 2. Inherited: Normalize delta for stability\n    normalized_delta = ops.zscore(delta)\n\n    # 3. New Coupling 1: Confidence-Adaptive Margin\n    # Margin scale increases with model confidence\n    confidence_factor = torch.tanh(torch.abs(delta.detach()))\n    beta_final = beta_base * (1.0 + confidence_factor)\n    # Inherit rank_gap margin base and clipping\n    rank_gap = ops.rank_gap(cost_a, cost_b)\n    margin = torch.clamp(beta_final * rank_gap, min=0, max=margin_max)\n\n    # 4. New Coupling 2: Cost-Gap-Modulated Focal Strength\n    # Focal strength increases for pairs with a larger cost gap\n    cost_gap = cost_b - cost_a\n    cost_gap_zscored = ops.zscore(cost_gap)\n    dynamic_gamma = gamma_base * F.softplus(cost_gap_zscored.detach())\n    \n    # 5. Calculate probability for focal loss\n    p_win = torch.sigmoid(delta)\n\n    # 6. Compute the focal modulating factor\n    # Use clamp to prevent instability from very large dynamic_gamma\n    modulating_factor = torch.pow(1.0 - p_win, torch.clamp(dynamic_gamma, max=10.0))\n\n    # 7. Compute the core preference loss\n    loss_core = -F.logsigmoid(normalized_delta + margin)\n\n    # 8. Apply the modulating factor to the core loss\n    # Detach the factor so it only re-weights the loss, not change the gradient direction\n    final_loss = modulating_factor.detach() * loss_core\n\n    # Apply optional instance weights if provided\n    if weight is not None:\n        final_loss = final_loss * weight\n\n    # Return the mean loss over the batch\n    return final_loss.mean()", "theoretical_basis": "A dynamically modulated Bradley-Terry preference model. The core is a logistic loss on z-scored log-probabilities. It features two main couplings: (1) A clipped, rank-based margin whose scale is dynamically adjusted by model confidence (via tanh), creating a larger separation target for confident predictions. (2) A focal modulation strength that is dynamically coupled with the batch-normalized cost gap (via softplus), concentrating learning on pairs with objectively large differences in cost."}, "static_ok": true, "static_reason": "", "static_error_code": "", "dynamic_ok": true, "dynamic_reason": "ok", "loss_value": 0.11983704566955566, "grad_norm": 0.0, "pref_ok": true, "pref_reason": "ok", "pref_mono_pass_rate": 0.7447916666666666, "pref_swap_pass_rate": 1.0, "pref_gap_pass_rate": 1.0}
{"generation": 9, "index": 2, "attempt": 0, "ir": {"name": "ConfidenceAwareFocalLossWithAdaptiveMargin", "intuition": "Mode: explore. This loss function combines the strongest elements from its parentsfocal modulation, z-score normalization, and a clipped, rank-based marginwhile introducing a novel coupling mechanism that makes both the margin and the focal strength sensitive to model confidence. The goal is to create a loss that dynamically adjusts its learning signal, pushing harder on uncertain pairs while simultaneously relaxing the separation requirements for pairs the model is already confident about.\n\nInherited Ideas:\n1. From both parents: The foundational structure is a focal-modulated Bradley-Terry loss: `modulating_factor * -logsigmoid(normalized_delta + margin)`. This retains the proven benefit of up-weighting difficult examples.\n2. From both parents: The use of `ops.zscore` on the log-probability difference (`delta`) is kept for batch-level normalization, ensuring stability and consistent scaling.\n3. From both parents: The margin is based on `ops.rank_gap` and is clipped using `torch.clamp` to prevent vanishing gradients, a robust technique present in both parents.\n\nNew Coupling Ideas:\n1. **Confidence-Modulated Margin**: Instead of a fixed `beta` or one scaled by cost, the margin's strength is inversely proportional to the model's confidence. We define a confidence score `p_win = sigmoid(delta)`, which is close to 1 for confident correct predictions. The margin is then calculated as `margin = clamp(beta * (1 - p_win) * rank_gap, ...)`. For pairs where the model is already confident (`p_win` -> 1), the margin shrinks towards zero, reducing the pressure to create an even larger log-probability gap and preventing overfitting. For uncertain pairs (`p_win` -> 0.5), the margin is largest, demanding a clearer separation.\n2. **Dynamic Gamma based on Log-Prob Difference**: Parent 1 modulated gamma based on `tanh(abs(delta))`. We simplify and generalize this by making gamma directly proportional to the absolute z-scored log-probability difference: `dynamic_gamma = gamma_base * softplus(abs(normalized_delta.detach()) - gamma_offset)`. This means that pairs with a very large log-probability difference (either strongly correct or strongly incorrect) receive a higher focal strength, intensifying the focus on these high-signal examples. The `gamma_offset` allows us to tune the threshold at which this intensification begins.", "pseudocode": "1. Calculate the log-probability difference: delta = log_prob_w - log_prob_l.\n2. Normalize delta across the batch for stability: normalized_delta = zscore(delta).\n3. Calculate the model's win probability: p_win = sigmoid(delta).\n4. Compute the cost rank gap: rank_gap = rank_gap(cost_a, cost_b).\n5. New Coupling 1: Compute a confidence-modulated margin. The margin is scaled down as model confidence (p_win) increases: margin = clamp(beta * (1.0 - p_win.detach()) * rank_gap, min=0, max=margin_max).\n6. New Coupling 2: Compute a dynamic focal strength based on the magnitude of the normalized log-prob difference: dynamic_gamma = gamma_base * softplus(abs(normalized_delta.detach()) - gamma_offset).\n7. Compute the focal modulating factor using the dynamic gamma: modulating_factor = (1.0 - p_win)^dynamic_gamma.\n8. Compute the core preference loss with the adaptive margin: loss_core = -logsigmoid(normalized_delta + margin).\n9. Apply the detached modulating factor to the core loss: final_loss = modulating_factor.detach() * loss_core.\n10. Return the mean of the final loss.", "hyperparams": {"beta": 1.5, "gamma_base": 2.0, "margin_max": 2.5, "gamma_offset": 0.5}, "operators_used": ["zscore", "sigmoid", "rank_gap", "clamp", "softplus", "logsigmoid"], "implementation_hint": {"expects": ["cost_a", "cost_b", "log_prob_w", "log_prob_l"], "returns": "scalar"}, "code": "def generated_loss(batch, model_output, extra):\n    \"\"\"\n    Combines focal loss and z-score normalization with two new couplings:\n    1. A clipped, rank-based margin that is inversely scaled by model confidence (p_win).\n    2. A dynamic focal gamma that increases with the magnitude of the log-probability difference.\n    \"\"\"\n    # Read hyperparameters with defaults\n    beta = extra.get('beta', 1.5)\n    gamma_base = extra.get('gamma_base', 2.0)\n    margin_max = extra.get('margin_max', 2.5)\n    gamma_offset = extra.get('gamma_offset', 0.5)\n\n    # Read required tensors from the batch\n    cost_a = batch['cost_a']\n    cost_b = batch['cost_b']\n    log_prob_w = batch['log_prob_w']\n    log_prob_l = batch['log_prob_l']\n    weight = batch.get('weight')\n\n    # 1. Calculate the log-probability difference\n    delta = log_prob_w - log_prob_l\n\n    # 2. Inherited: Normalize delta for stability\n    normalized_delta = ops.zscore(delta)\n\n    # 3. Calculate model's win probability\n    p_win = torch.sigmoid(delta)\n\n    # 4. Inherited: Compute the rank gap of the costs\n    rank_gap = ops.rank_gap(cost_a, cost_b)\n\n    # 5. New Coupling 1: Confidence-Modulated Margin\n    # Margin shrinks as confidence (p_win) approaches 1. Detach to make it a target.\n    confidence_factor = (1.0 - p_win.detach())\n    margin = torch.clamp(beta * confidence_factor * rank_gap, min=0, max=margin_max)\n\n    # 6. New Coupling 2: Dynamic Gamma based on Log-Prob Difference\n    # Gamma increases for pairs with large absolute log-prob differences.\n    # Detach to prevent gradients flowing through this modulation.\n    gamma_modulator = F.softplus(torch.abs(normalized_delta.detach()) - gamma_offset)\n    dynamic_gamma = gamma_base * gamma_modulator\n\n    # 7. Compute the focal modulating factor\n    # Clamp p_win to avoid potential (1-1)^0 issues if dynamic_gamma is zero.\n    modulating_factor = torch.pow(1.0 - p_win.clamp(max=1.0 - 1e-6), dynamic_gamma)\n\n    # 8. Compute the core preference loss\n    loss_core = -F.logsigmoid(normalized_delta + margin)\n\n    # 9. Apply the modulating factor to the core loss\n    # Detach the factor so it only re-weights the loss, not change the gradient direction\n    final_loss = modulating_factor.detach() * loss_core\n\n    # Apply optional instance weights if provided\n    if weight is not None:\n        final_loss = final_loss * weight\n\n    # Return the mean loss over the batch\n    return final_loss.mean()", "theoretical_basis": "A dynamically modulated Bradley-Terry preference model. The core is a logistic loss on z-scored log-probabilities. It introduces two novel couplings: (1) The rank-based separation margin is inversely scaled by model confidence (`p_win`), relaxing the optimization target for already well-classified pairs to prevent overfitting. (2) The focal loss strength (`gamma`) is scaled by the magnitude of the log-probability difference, concentrating learning on examples that are either strongly correct or strongly incorrect, thereby amplifying the learning signal from the most informative pairs."}, "static_ok": true, "static_reason": "", "static_error_code": "", "dynamic_ok": true, "dynamic_reason": "ok", "loss_value": 0.20051337778568268, "grad_norm": 0.0, "pref_ok": true, "pref_reason": "ok", "pref_mono_pass_rate": 0.7311197916666666, "pref_swap_pass_rate": 1.0, "pref_gap_pass_rate": 1.0}
{"generation": 9, "index": 3, "attempt": 0, "ir": {"name": "DynamicRangeFocalLoss", "intuition": "Mode: explore. This loss function aims to improve upon the successful focal loss and z-score normalization paradigm by introducing dynamic range adaptation for both the log-probability difference and the focal strength. The goal is to create a loss that is robust to batch-level distribution shifts and focuses learning on the most informative examples in a more nuanced way.\n\nInherited Ideas:\n1. From both parents: The core structure is a focal-modulated Bradley-Terry loss: `(1 - p_win)^gamma * -logsigmoid(normalized_delta + margin)`. This retains the proven benefit of up-weighting difficult examples.\n2. From both parents: The use of a margin based on `rank_gap` to create a cost-sensitive separation target, which is then clipped using `clamp` to prevent instability from extreme cost differences.\n\nNew Coupling Ideas:\n1. **Dynamic Range Normalization**: Instead of z-scoring `delta`, which centers it at zero with unit variance, we introduce a new normalization that maps `delta` to the `[-1, 1]` range based on the batch's min/max values: `(2 * (delta - min_delta) / (max_delta - min_delta)) - 1`. This preserves the relative spacing of `delta` values within the batch more directly than z-scoring and is less sensitive to outlier skew. This makes the loss more adaptive to batches where all pairs are either very easy or very hard.\n2. **Certainty-Adaptive Focal Strength**: We introduce a new way to modulate the focal strength `gamma`. Instead of using `tanh` or `sigmoid` on `delta`, we use a squared sigmoid of the normalized `delta`. The formula is `dynamic_gamma = gamma_base * (1 - sigmoid(normalized_delta.detach())^2)`. This creates a U-shaped modulation: gamma is high for uncertain pairs (normalized_delta near 0) and smoothly decreases to a minimum for both very confident wins (normalized_delta -> 1) and very confident losses (normalized_delta -> -1). The squaring makes the drop-off sharper, focusing the focal effect more precisely on the most ambiguous pairs.", "pseudocode": "1. Calculate the log-probability difference: delta = log_prob_w - log_prob_l.\n2. Compute the min and max of delta across the batch for dynamic range normalization.\n3. Normalize delta to the range [-1, 1]: normalized_delta = (2 * (delta - min_delta) / (max_delta - min_delta + epsilon)) - 1.\n4. Compute the cost rank gap: rank_gap = rank_gap(cost_a, cost_b).\n5. Calculate a clipped, rank-based margin: margin = clamp(beta * rank_gap, min=0, max=margin_max).\n6. Calculate a certainty-adaptive focal strength: dynamic_gamma = gamma_base * (1 - sigmoid(normalized_delta.detach())^2).\n7. Calculate the model's win probability: p_win = sigmoid(delta).\n8. Compute the focal modulating factor: modulating_factor = (1 - p_win)^dynamic_gamma.\n9. Compute the core preference loss: loss_core = -logsigmoid(normalized_delta + margin).\n10. Apply the detached modulating factor: final_loss = modulating_factor.detach() * loss_core.\n11. Return the mean of the final loss.", "hyperparams": {"beta": 1.0, "gamma_base": 2.5, "margin_max": 2.5, "epsilon": 1e-08}, "operators_used": ["rank_gap", "clamp", "sigmoid", "logsigmoid"], "implementation_hint": {"expects": ["cost_a", "cost_b", "log_prob_w", "log_prob_l"], "returns": "scalar"}, "code": "def generated_loss(batch, model_output, extra):\n    \"\"\"\n    Combines focal loss with two new couplings:\n    1. Dynamic range normalization of delta to [-1, 1] instead of z-scoring.\n    2. A certainty-adaptive focal gamma that is high for uncertain pairs and low for confident wins/losses.\n    \"\"\"\n    # Read hyperparameters with defaults\n    beta = extra.get('beta', 1.0)\n    gamma_base = extra.get('gamma_base', 2.5)\n    margin_max = extra.get('margin_max', 2.5)\n    epsilon = extra.get('epsilon', 1e-8)\n\n    # Read required tensors from the batch\n    cost_a = batch['cost_a']\n    cost_b = batch['cost_b']\n    log_prob_w = batch['log_prob_w']\n    log_prob_l = batch['log_prob_l']\n    weight = batch.get('weight')\n\n    # 1. Calculate the log-probability difference\n    delta = log_prob_w - log_prob_l\n\n    # 2. New Coupling 1: Dynamic Range Normalization\n    # Normalize delta to the range [-1, 1] based on batch min/max\n    min_delta = torch.min(delta)\n    max_delta = torch.max(delta)\n    delta_range = max_delta - min_delta\n    normalized_delta = (2 * (delta - min_delta) / (delta_range + epsilon)) - 1\n\n    # 3. Inherited: Clipped, rank-based margin\n    rank_gap = ops.rank_gap(cost_a, cost_b)\n    margin = torch.clamp(beta * rank_gap, min=0, max=margin_max)\n\n    # 4. New Coupling 2: Certainty-Adaptive Focal Strength\n    # (1 - sigmoid(x)^2) is a U-shaped function centered at x=0\n    certainty = torch.sigmoid(normalized_delta.detach()).pow(2)\n    dynamic_gamma = gamma_base * (1.0 - certainty)\n\n    # 5. Calculate probability for focal loss\n    p_win = torch.sigmoid(delta)\n\n    # 6. Compute the focal modulating factor\n    modulating_factor = torch.pow(1.0 - p_win, dynamic_gamma)\n\n    # 7. Compute the core preference loss\n    loss_core = -F.logsigmoid(normalized_delta + margin)\n\n    # 8. Apply the modulating factor to the core loss\n    final_loss = modulating_factor.detach() * loss_core\n\n    # Apply optional instance weights if provided\n    if weight is not None:\n        final_loss = final_loss * weight\n\n    # Return the mean loss over the batch\n    return final_loss.mean()", "theoretical_basis": "A dynamically modulated Bradley-Terry preference model. The core loss is logistic, but it operates on a log-probability difference that has been dynamically normalized to the batch's [-1, 1] range, preserving relative spacing while ensuring scale consistency. This is coupled with a focal modulation where the gamma is highest for uncertain pairs (normalized difference near zero) and quadratically decays for confident pairs in either direction, sharpening the focus on ambiguous examples."}, "static_ok": true, "static_reason": "", "static_error_code": "", "dynamic_ok": true, "dynamic_reason": "ok", "loss_value": 0.13889412581920624, "grad_norm": 0.0, "pref_ok": false, "pref_reason": "pref_semantic_violation", "pref_mono_pass_rate": 0.9921875, "pref_swap_pass_rate": 1.0, "pref_gap_pass_rate": 0.0, "dynamic_error_code": "E_PREF_SEMANTIC"}
{"generation": 9, "index": 3, "attempt": 1, "ir": {"name": "DynamicRangeFocalLoss", "intuition": "Repaired: The original loss failed the `preference_gate` with `E_PREF_SEMANTIC` and a `gap_pass_rate` of 0.0. This indicates that the loss did not decrease as the cost gap between the preferred and dispreferred solutions increased. The root cause was that the focal modulating factor, `(1 - p_win)^gamma`, was not detached from the computation graph. `p_win` depends on `delta` (log_prob_w - log_prob_l), which is part of what the model optimizes. By applying `.detach()` to `p_win` before computing the modulating factor, we ensure that this factor acts as a fixed weight for the current example, allowing the optimizer to focus solely on minimizing the core loss term, `-logsigmoid(normalized_delta + margin)`, which correctly responds to the cost gap via the `margin`.", "pseudocode": "1. Calculate the log-probability difference: delta = log_prob_w - log_prob_l.\n2. Compute the min and max of delta across the batch for dynamic range normalization.\n3. Normalize delta to the range [-1, 1]: normalized_delta = (2 * (delta - min_delta) / (max_delta - min_delta + epsilon)) - 1.\n4. Compute the cost rank gap: rank_gap = rank_gap(cost_a, cost_b).\n5. Calculate a clipped, rank-based margin: margin = clamp(beta * rank_gap, min=0, max=margin_max).\n6. Calculate a certainty-adaptive focal strength: dynamic_gamma = gamma_base * (1 - sigmoid(normalized_delta.detach())^2).\n7. Calculate the model's win probability on a detached delta: p_win = sigmoid(delta.detach()).\n8. Compute the focal modulating factor: modulating_factor = (1 - p_win)^dynamic_gamma.\n9. Compute the core preference loss: loss_core = -logsigmoid(normalized_delta + margin).\n10. Apply the detached modulating factor: final_loss = modulating_factor.detach() * loss_core.\n11. Return the mean of the final loss.", "hyperparams": {"beta": 1.0, "gamma_base": 2.5, "margin_max": 2.5, "epsilon": 1e-08}, "operators_used": ["rank_gap", "clamp", "sigmoid", "logsigmoid"], "implementation_hint": {"expects": ["cost_a", "cost_b", "log_prob_w", "log_prob_l"], "returns": "scalar"}, "code": "def generated_loss(batch, model_output, extra):\n    \"\"\"\n    Combines focal loss with two new couplings:\n    1. Dynamic range normalization of delta to [-1, 1] instead of z-scoring.\n    2. A certainty-adaptive focal gamma that is high for uncertain pairs and low for confident wins/losses.\n    \"\"\"\n    # Read hyperparameters with defaults\n    beta = extra.get('beta', 1.0)\n    gamma_base = extra.get('gamma_base', 2.5)\n    margin_max = extra.get('margin_max', 2.5)\n    epsilon = extra.get('epsilon', 1e-8)\n\n    # Read required tensors from the batch\n    cost_a = batch['cost_a']\n    cost_b = batch['cost_b']\n    log_prob_w = batch['log_prob_w']\n    log_prob_l = batch['log_prob_l']\n    weight = batch.get('weight')\n\n    # 1. Calculate the log-probability difference\n    delta = log_prob_w - log_prob_l\n\n    # 2. New Coupling 1: Dynamic Range Normalization\n    # Normalize delta to the range [-1, 1] based on batch min/max\n    min_delta = torch.min(delta)\n    max_delta = torch.max(delta)\n    delta_range = max_delta - min_delta\n    # Add a safe guard for the case where all deltas in a batch are the same.\n    normalized_delta = torch.where(\n        delta_range > epsilon,\n        (2 * (delta - min_delta) / (delta_range + epsilon)) - 1,\n        torch.zeros_like(delta)\n    )\n\n    # 3. Inherited: Clipped, rank-based margin\n    rank_gap = ops.rank_gap(cost_a, cost_b)\n    margin = torch.clamp(beta * rank_gap, min=0, max=margin_max)\n\n    # 4. New Coupling 2: Certainty-Adaptive Focal Strength\n    # (1 - sigmoid(x)^2) is a U-shaped function centered at x=0\n    certainty = torch.sigmoid(normalized_delta.detach()).pow(2)\n    dynamic_gamma = gamma_base * (1.0 - certainty)\n\n    # 5. Calculate probability for focal loss from a detached delta\n    p_win = torch.sigmoid(delta.detach())\n\n    # 6. Compute the focal modulating factor\n    modulating_factor = torch.pow(1.0 - p_win, dynamic_gamma)\n\n    # 7. Compute the core preference loss\n    loss_core = -F.logsigmoid(normalized_delta + margin)\n\n    # 8. Apply the modulating factor to the core loss\n    final_loss = modulating_factor.detach() * loss_core\n\n    # Apply optional instance weights if provided\n    if weight is not None:\n        final_loss = final_loss * weight\n\n    # Return the mean loss over the batch\n    return final_loss.mean()", "theoretical_basis": ""}, "static_ok": true, "static_reason": "", "static_error_code": "", "dynamic_ok": true, "dynamic_reason": "ok", "loss_value": 0.08540356904268265, "grad_norm": 0.0, "pref_ok": false, "pref_reason": "pref_semantic_violation", "pref_mono_pass_rate": 0.9921875, "pref_swap_pass_rate": 1.0, "pref_gap_pass_rate": 0.0, "dynamic_error_code": "E_PREF_SEMANTIC"}
{"generation": 9, "index": 3, "attempt": 2, "ir": {"name": "DynamicRangeFocalLoss", "intuition": "The original loss failed the `preference_gate` with `E_PREF_SEMANTIC` and a `gap_pass_rate` of 0.0. This indicates that the loss did not decrease as the cost gap between the preferred and dispreferred solutions increased. The root cause was that the focal modulating factor, `(1 - p_win)^dynamic_gamma`, was not detached from the computation graph. `p_win` depends on `delta` (log_prob_w - log_prob_l), which is part of what the model optimizes. By applying `.detach()` to `p_win` before computing the modulating factor, we ensure that this factor acts as a fixed weight for the current example, allowing the optimizer to focus solely on minimizing the core loss term, `-logsigmoid(normalized_delta + margin)`, which correctly responds to the cost gap via the `margin`.", "pseudocode": "1. Calculate the log-probability difference: delta = log_prob_w - log_prob_l.\n2. Compute the min and max of delta across the batch for dynamic range normalization.\n3. Normalize delta to the range [-1, 1]: normalized_delta = (2 * (delta - min_delta) / (max_delta - min_delta + epsilon)) - 1.\n4. Compute the cost rank gap: rank_gap = rank_gap(cost_a, cost_b).\n5. Calculate a clipped, rank-based margin: margin = clamp(beta * rank_gap, min=0, max=margin_max).\n6. Calculate a certainty-adaptive focal strength: dynamic_gamma = gamma_base * (1 - sigmoid(normalized_delta.detach())^2).\n7. Calculate the model's win probability on a detached delta: p_win = sigmoid(delta.detach()).\n8. Compute the focal modulating factor: modulating_factor = (1 - p_win)^dynamic_gamma.\n9. Compute the core preference loss: loss_core = -logsigmoid(normalized_delta + margin).\n10. Apply the detached modulating factor: final_loss = modulating_factor.detach() * loss_core.\n11. Return the mean of the final loss.", "hyperparams": {"beta": 1.0, "gamma_base": 2.5, "margin_max": 2.5, "epsilon": 1e-08}, "operators_used": ["rank_gap", "clamp", "sigmoid", "logsigmoid"], "implementation_hint": {"expects": ["cost_a", "cost_b", "log_prob_w", "log_prob_l"], "returns": "scalar"}, "code": "def generated_loss(batch, model_output, extra):\n    \"\"\"\n    Combines focal loss with two new couplings:\n    1. Dynamic range normalization of delta to [-1, 1] instead of z-scoring.\n    2. A certainty-adaptive focal gamma that is high for uncertain pairs and low for confident wins/losses.\n    \"\"\"\n    # Read hyperparameters with defaults\n    beta = extra.get('beta', 1.0)\n    gamma_base = extra.get('gamma_base', 2.5)\n    margin_max = extra.get('margin_max', 2.5)\n    epsilon = extra.get('epsilon', 1e-8)\n\n    # Read required tensors from the batch\n    cost_a = batch['cost_a']\n    cost_b = batch['cost_b']\n    log_prob_w = batch['log_prob_w']\n    log_prob_l = batch['log_prob_l']\n    weight = batch.get('weight')\n\n    # 1. Calculate the log-probability difference\n    delta = log_prob_w - log_prob_l\n\n    # 2. New Coupling 1: Dynamic Range Normalization\n    # Normalize delta to the range [-1, 1] based on batch min/max\n    min_delta = torch.min(delta)\n    max_delta = torch.max(delta)\n    delta_range = max_delta - min_delta\n    # Add a safe guard for the case where all deltas in a batch are the same.\n    normalized_delta = torch.where(\n        delta_range > epsilon,\n        (2 * (delta - min_delta) / (delta_range + epsilon)) - 1,\n        torch.zeros_like(delta)\n    )\n\n    # 3. Inherited: Clipped, rank-based margin\n    rank_gap = ops.rank_gap(cost_a, cost_b)\n    margin = torch.clamp(beta * rank_gap, min=0, max=margin_max)\n\n    # 4. New Coupling 2: Certainty-Adaptive Focal Strength\n    # (1 - sigmoid(x)^2) is a U-shaped function centered at x=0\n    certainty = torch.sigmoid(normalized_delta.detach()).pow(2)\n    dynamic_gamma = gamma_base * (1.0 - certainty)\n\n    # 5. Calculate probability for focal loss from a detached delta\n    p_win = torch.sigmoid(delta.detach())\n\n    # 6. Compute the focal modulating factor\n    modulating_factor = torch.pow(1.0 - p_win, dynamic_gamma)\n\n    # 7. Compute the core preference loss\n    loss_core = -F.logsigmoid(normalized_delta + margin)\n\n    # 8. Apply the modulating factor to the core loss\n    final_loss = modulating_factor.detach() * loss_core\n\n    # Apply optional instance weights if provided\n    if weight is not None:\n        final_loss = final_loss * weight\n\n    # Return the mean loss over the batch\n    return final_loss.mean()", "theoretical_basis": ""}, "static_ok": true, "static_reason": "", "static_error_code": "", "dynamic_ok": true, "dynamic_reason": "ok", "loss_value": 0.08540356904268265, "grad_norm": 0.0, "pref_ok": false, "pref_reason": "pref_semantic_violation", "pref_mono_pass_rate": 0.9921875, "pref_swap_pass_rate": 1.0, "pref_gap_pass_rate": 0.0, "dynamic_error_code": "E_PREF_SEMANTIC"}
{"generation": 9, "index": 4, "attempt": 0, "ir": {"name": "FocalLossWithDynamicConfidencePenalty", "intuition": "Mode: explore. This loss function synergizes the most effective components of its parentsfocal modulation, z-score normalization, and adaptive marginswhile introducing two novel coupling ideas designed to improve stability and focus learning on the most informative examples. The goal is to create a robust loss that penalizes model overconfidence and adapts its focal strength based on the objective difficulty of a pair.\n\nInherited Ideas:\n1. From both parents: The core architecture is a focal-modulated Bradley-Terry loss: `(1 - p_win)^gamma * -logsigmoid(normalized_delta + margin)`. This retains the proven benefit of up-weighting difficult examples.\n2. From both parents: The use of `ops.zscore` on the log-probability difference (`delta`) is kept for batch-level normalization, ensuring stability and consistent scaling against outliers.\n3. From both parents: The margin is based on the cost `rank_gap` and is `clamp`ed to prevent extreme values, a stability trick present in both parents.\n\nNew Coupling Ideas:\n1. **Dynamic Confidence Penalty Term**: Instead of modulating the focal `gamma` with model confidence (as in Parent 1), we introduce a direct, additive penalty term to the loss. This penalty is `penalty_scale * relu(delta - delta_cap)`. It activates only when the model's log-probability difference (`delta`) exceeds a certain threshold (`delta_cap`), effectively punishing overconfidence on pairs the model already strongly prefers. This is a more direct mechanism than modulating gamma and prevents the model from pushing log-probabilities to infinity, which can harm generalization.\n2. **Cost-Gap-Modulated Focal Strength**: We adopt the idea of a dynamic gamma but couple it to the cost landscape instead of model confidence. The focal strength `gamma` is adjusted by the z-scored cost gap: `gamma = gamma_base * softplus(zscore(cost_b - cost_a).detach())`. This means pairs with a larger-than-average cost difference (objectively easier pairs) receive a stronger focal effect. This encourages the model to be very confident about clear-cut cases, while applying a gentler focal effect to pairs with very similar costs, where ambiguity is higher and aggressive focusing might be counterproductive.", "pseudocode": "1. Calculate the log-probability difference: delta = log_prob_w - log_prob_l.\n2. Normalize delta across the batch for stability: normalized_delta = zscore(delta).\n3. Calculate the absolute cost difference: cost_gap = cost_b - cost_a.\n4. Normalize the cost gap: cost_gap_zscored = zscore(cost_gap).\n5. Compute a cost-gap-modulated focal strength: dynamic_gamma = gamma_base * softplus(cost_gap_zscored.detach()).\n6. Calculate the model's win probability: p_win = sigmoid(delta).\n7. Compute the focal modulating factor: modulating_factor = (1 - p_win)^dynamic_gamma.\n8. Compute the cost rank gap: rank_gap = rank_gap(cost_a, cost_b).\n9. Calculate the clipped, rank-based margin: margin = clamp(beta * rank_gap, min=0, max=margin_max).\n10. Compute the core preference loss: loss_core = -logsigmoid(normalized_delta + margin).\n11. Apply the detached modulating factor: preference_loss = modulating_factor.detach() * loss_core.\n12. Compute the overconfidence penalty: penalty = penalty_scale * relu(delta - delta_cap).\n13. Combine the preference loss and the penalty: final_loss = preference_loss + penalty.\n14. Return the mean of the final loss.", "hyperparams": {"beta": 1.0, "gamma_base": 1.5, "margin_max": 2.5, "delta_cap": 5.0, "penalty_scale": 0.01}, "operators_used": ["zscore", "softplus", "sigmoid", "logsigmoid", "rank_gap", "clamp", "relu"], "implementation_hint": {"expects": ["cost_a", "cost_b", "log_prob_w", "log_prob_l"], "returns": "scalar"}, "code": "def generated_loss(batch, model_output, extra):\n    \"\"\"\n    Combines focal loss and z-score normalization with two new couplings:\n    1. An additive ReLU penalty to curb overconfidence (when delta > delta_cap).\n    2. A focal gamma that is dynamically scaled by the z-scored cost gap.\n    \"\"\"\n    # Read hyperparameters with defaults\n    beta = extra.get('beta', 1.0)\n    gamma_base = extra.get('gamma_base', 1.5)\n    margin_max = extra.get('margin_max', 2.5)\n    delta_cap = extra.get('delta_cap', 5.0)\n    penalty_scale = extra.get('penalty_scale', 0.01)\n\n    # Read required tensors from the batch\n    cost_a = batch['cost_a']\n    cost_b = batch['cost_b']\n    log_prob_w = batch['log_prob_w']\n    log_prob_l = batch['log_prob_l']\n    weight = batch.get('weight')\n\n    # 1. Calculate the log-probability difference\n    delta = log_prob_w - log_prob_l\n\n    # 2. Inherited: Normalize delta for stability\n    normalized_delta = ops.zscore(delta)\n\n    # 3. Inherited: Clipped, rank-based margin\n    rank_gap = ops.rank_gap(cost_a, cost_b)\n    margin = torch.clamp(beta * rank_gap, min=0, max=margin_max)\n\n    # 4. New Coupling 1: Cost-Gap-Modulated Focal Strength\n    cost_gap = cost_b - cost_a\n    cost_gap_zscored = ops.zscore(cost_gap)\n    # Gamma is larger for pairs with a larger-than-average cost gap\n    dynamic_gamma = gamma_base * F.softplus(cost_gap_zscored.detach())\n\n    # 5. Compute focal loss components\n    p_win = torch.sigmoid(delta)\n    modulating_factor = torch.pow(1.0 - p_win, dynamic_gamma)\n    loss_core = -F.logsigmoid(normalized_delta + margin)\n    preference_loss = modulating_factor.detach() * loss_core\n\n    # 6. New Coupling 2: Dynamic Confidence Penalty Term\n    # Penalize when delta is too large, indicating overconfidence\n    overconfidence = F.relu(delta.detach() - delta_cap)\n    penalty = penalty_scale * overconfidence\n\n    # 7. Combine preference loss and penalty\n    final_loss = preference_loss + penalty\n\n    # Apply optional instance weights if provided\n    if weight is not None:\n        final_loss = final_loss * weight\n\n    # Return the mean loss over the batch\n    return final_loss.mean()", "theoretical_basis": "A dynamically modulated Bradley-Terry preference model with an explicit overconfidence penalty. The core is a logistic loss on z-scored log-probabilities with a clipped, rank-based margin. This is coupled with a focal modulation where the focal strength is proportional to the batch-normalized cost gap, focusing learning on objectively distinct pairs. A novel additive ReLU-based penalty is introduced to directly regularize the log-probability difference, discouraging excessive model confidence and improving generalization."}, "static_ok": true, "static_reason": "", "static_error_code": "", "dynamic_ok": true, "dynamic_reason": "ok", "loss_value": 0.15237726271152496, "grad_norm": 0.0, "pref_ok": true, "pref_reason": "ok", "pref_mono_pass_rate": 0.7734375, "pref_swap_pass_rate": 1.0, "pref_gap_pass_rate": 1.0}
{"generation": 9, "index": 5, "attempt": 0, "ir": {"name": "ConfidenceAwareFocalLossWithDynamicMargin", "intuition": "Mode: explore. This loss function aims to improve upon the strong focal loss framework of the parents by introducing a more direct and theoretically grounded mechanism for handling model confidence and pair difficulty. It combines the stable, normalized core of the parents with new couplings that modulate both the margin and the focal strength based on model uncertainty.\n\nInherited Ideas:\n1. From both parents: The core structure is a focal-modulated Bradley-Terry loss, `focal_weight * -logsigmoid(normalized_delta + margin)`. This retains the powerful hard-example mining capability.\n2. From both parents: The use of `ops.zscore` on the log-probability difference (`delta`) is kept for batch-level normalization, ensuring stability and consistent scaling of the primary learning signal.\n\nNew Coupling Ideas:\n1. **Entropy-Based Dynamic Margin**: Instead of a margin based on cost rank or cost gap, which can be noisy, this loss introduces a margin that is directly proportional to the model's uncertainty about the pair. We calculate the Shannon entropy of the model's win probability `p_win`. A high entropy (near `log(2)`) signifies high uncertainty (p_win is close to 0.5), while low entropy means high confidence. The margin is then `beta * entropy`. This encourages the model to create a larger separation (`delta`) for pairs it is currently uncertain about, directly focusing the learning effort where it's most needed. The entropy is detached to ensure the margin only sets the target separation, without creating conflicting gradients.\n2. **Confidence-Penalized Focal Re-weighting**: Parent 1 penalizes focal strength `gamma` using `1 - tanh(abs(delta))`. We adopt this effective idea but simplify the focal mechanism. Instead of a dynamic `gamma`, we use a fixed `gamma` for the standard focal term `(1-p_win)^gamma` and multiply it by a confidence penalty `(1 - tanh(abs(delta)))`. This creates a two-part focal weight: one part focusing on misclassified examples (`(1-p_win)^gamma`) and a second part that reduces the weight on examples the model is already very confident about (large `abs(delta)`), preventing overfitting on easy pairs and stabilizing training.", "pseudocode": "1. Calculate the log-probability difference: delta = log_prob_w - log_prob_l.\n2. Normalize delta across the batch for stability: normalized_delta = zscore(delta).\n3. Calculate the model's win probability: p_win = sigmoid(delta).\n4. Calculate the Shannon entropy of the win probability to quantify model uncertainty: entropy = -(p_win * log(p_win) + (1-p_win) * log(1-p_win)).\n5. Compute the entropy-based dynamic margin: margin = beta * entropy.detach().\n6. Compute the core preference loss term: loss_core = -logsigmoid(normalized_delta + margin).\n7. Compute the standard focal modulating factor: focal_factor = (1 - p_win)^gamma.\n8. Compute the confidence penalty factor: confidence_penalty = 1.0 - tanh(abs(delta)).\n9. Combine the factors to create the final loss weight: final_weight = (focal_factor * confidence_penalty).detach().\n10. Apply the final weight to the core loss: final_loss = final_weight * loss_core.\n11. Return the mean of the final loss.", "hyperparams": {"beta": 1.5, "gamma": 2.0, "epsilon": 1e-08}, "operators_used": ["zscore", "sigmoid", "log", "tanh", "logsigmoid"], "implementation_hint": {"expects": ["cost_a", "cost_b", "log_prob_w", "log_prob_l"], "returns": "scalar"}, "code": "def generated_loss(batch, model_output, extra):\n    \"\"\"\n    Combines z-score normalization and focal loss with two new couplings:\n    1. A dynamic margin based on the Shannon entropy of the model's prediction.\n    2. A confidence-penalized focal weight using (1 - tanh|delta|).\n    \"\"\"\n    # Read hyperparameters with defaults\n    beta = extra.get('beta', 1.5)\n    gamma = extra.get('gamma', 2.0)\n    epsilon = extra.get('epsilon', 1e-8)\n\n    # Read required tensors from the batch\n    log_prob_w = batch['log_prob_w']\n    log_prob_l = batch['log_prob_l']\n    weight = batch.get('weight')\n\n    # 1. Calculate the log-probability difference\n    delta = log_prob_w - log_prob_l\n\n    # 2. Inherited: Normalize delta for stability\n    normalized_delta = ops.zscore(delta)\n\n    # 3. Calculate win probability for entropy and focal terms\n    p_win = torch.sigmoid(delta)\n\n    # 4. New Coupling 1: Entropy-Based Dynamic Margin\n    # Clamp p_win for numerical stability in log\n    p_win_stable = torch.clamp(p_win, min=epsilon, max=1.0 - epsilon)\n    entropy = -(p_win_stable * torch.log(p_win_stable) + (1.0 - p_win_stable) * torch.log(1.0 - p_win_stable))\n    # Margin is proportional to uncertainty. Detach to only set a target.\n    margin = beta * entropy.detach()\n\n    # 5. Compute the core preference loss\n    loss_core = -F.logsigmoid(normalized_delta + margin)\n\n    # 6. New Coupling 2: Confidence-Penalized Focal Re-weighting\n    # Inherit confidence penalty from Parent 1\n    confidence_penalty = 1.0 - torch.tanh(torch.abs(delta))\n    # Standard focal factor for hard-example mining\n    focal_factor = torch.pow(1.0 - p_win, gamma)\n    # Combine and detach to only re-weight the loss\n    final_weight = (focal_factor * confidence_penalty).detach()\n\n    # 7. Apply the final weight to the core loss\n    final_loss = final_weight * loss_core\n\n    # Apply optional instance weights if provided\n    if weight is not None:\n        final_loss = final_loss * weight\n\n    # Return the mean loss over the batch\n    return final_loss.mean()", "theoretical_basis": "A dynamically modulated Bradley-Terry preference model with an information-theoretic margin. The core loss is logistic on z-scored log-probabilities. It introduces two main couplings: (1) An adaptive margin directly proportional to the Shannon entropy of the model's prediction, which sets a larger separation target for pairs with high model uncertainty. (2) A focal-style re-weighting scheme that is penalized by model confidence (via tanh), concentrating learning on examples that are both misclassified and uncertain."}, "static_ok": true, "static_reason": "", "static_error_code": "", "dynamic_ok": true, "dynamic_reason": "ok", "loss_value": 0.07568331807851791, "grad_norm": 0.0, "pref_ok": true, "pref_reason": "ok", "pref_mono_pass_rate": 0.6490885416666666, "pref_swap_pass_rate": 0.8333333333333334, "pref_gap_pass_rate": 1.0}
{"generation": 9, "index": 6, "attempt": 0, "ir": {"name": "FocalLossWithAdaptiveMarginAndConfidencePenalty", "intuition": "Mode: explore. This loss function combines the strongest elements of its parentsfocal modulation, z-score normalization, and a clipped, rank-based marginwith two novel coupling ideas designed to enhance stability and focus learning. The goal is to create a robust loss that penalizes model uncertainty while dynamically adjusting the separation margin based on batch-level statistics.\n\nInherited Ideas:\n1. From both parents: The core structure is a focal loss (`(1 - p_win)^gamma`) applied to a Bradley-Terry style `-logsigmoid` objective. This is a proven technique for up-weighting difficult examples.\n2. From both parents: The use of `ops.zscore` on the log-probability difference (`delta = log_prob_w - log_prob_l`) is retained for batch-level normalization and stability.\n3. From both parents: The margin is based on the cost `rank_gap`, making the desired separation between log-probabilities adaptive to the relative cost ordering within the batch.\n4. From Parent 1 (`FocalLossWithClippedCostSensitiveMargin`): The focal strength `gamma` is dynamically modulated based on model confidence, using `1 - tanh(abs(delta))` to focus more on uncertain pairs.\n5. From Parent 2 (`AdaptiveFocalLogsigmoidWithClippedMargin`): The margin is `clamp`ed with a maximum value (`margin_max`) to prevent vanishing gradients on pairs with very large cost differences.\n\nNew Coupling Ideas:\n1. **Batch-Adaptive Margin Scaling**: Instead of scaling the `rank_gap` margin by a fixed `beta`, we scale it by the standard deviation of the batch's log-probability differences (`std(delta)`). This couples the margin's magnitude to the model's current output variance. If the model produces highly varied log-probabilities (high `std`), the margin will be larger, demanding greater separation. If the outputs are clustered (low `std`), the margin shrinks, preventing the loss from becoming overly punitive. This makes the loss landscape self-tuning to the model's state.\n2. **Margin-Modulated Focal Strength (Refined)**: While Parent 1 modulated `gamma` by model confidence and Parent 2 by the margin, this child combines them. The focal strength is `gamma = gamma_base * (1 - tanh(abs(delta.detach())))`. This inherits the confidence penalty from Parent 1. However, since the margin itself is now coupled to `std(delta)`, the overall loss landscape is indirectly influenced by both model confidence and output variance, creating a more sophisticated feedback loop.", "pseudocode": "1. Calculate the log-probability difference: delta = log_prob_w - log_prob_l.\n2. Normalize delta across the batch for stability: normalized_delta = zscore(delta).\n3. Calculate the standard deviation of the log-probability differences: delta_std = std(delta).\n4. Compute the cost rank gap: rank_gap = rank_gap(cost_a, cost_b).\n5. Calculate the batch-adaptive margin: margin = clamp(delta_std.detach() * rank_gap, min=0, max=margin_max).\n6. Compute a confidence-penalized focal strength: dynamic_gamma = gamma_base * (1.0 - tanh(abs(delta.detach()))).\n7. Calculate the model's win probability: p_win = sigmoid(delta).\n8. Compute the focal modulating factor: modulating_factor = (1 - p_win)^dynamic_gamma.\n9. Compute the core preference loss: loss_core = -logsigmoid(normalized_delta + margin).\n10. Apply the detached modulating factor: final_loss = modulating_factor.detach() * loss_core.\n11. Return the mean of the final loss.", "hyperparams": {"gamma_base": 2.0, "margin_max": 2.5}, "operators_used": ["zscore", "rank_gap", "clamp", "tanh", "sigmoid", "logsigmoid"], "implementation_hint": {"expects": ["cost_a", "cost_b", "log_prob_w", "log_prob_l"], "returns": "scalar"}, "code": "def generated_loss(batch, model_output, extra):\n    \"\"\"\n    Combines focal loss with a clipped, rank-based margin and confidence-penalized gamma.\n    New Couplings:\n    1. The margin scale is adaptively set by the standard deviation of the batch's log-prob differences.\n    2. The focal gamma is penalized by model confidence (inherited from Parent 1).\n    \"\"\"\n    # Read hyperparameters with defaults\n    gamma_base = extra.get('gamma_base', 2.0)\n    margin_max = extra.get('margin_max', 2.5)\n\n    # Read required tensors from the batch\n    cost_a = batch['cost_a']\n    cost_b = batch['cost_b']\n    log_prob_w = batch['log_prob_w']\n    log_prob_l = batch['log_prob_l']\n    weight = batch.get('weight')\n\n    # 1. Calculate the log-probability difference\n    delta = log_prob_w - log_prob_l\n\n    # 2. Inherited: Normalize delta for stability\n    normalized_delta = ops.zscore(delta)\n\n    # 3. New Coupling 1: Batch-Adaptive Margin Scaling\n    # The margin scale is the standard deviation of the log-prob differences in the batch.\n    # Detach to prevent gradients from flowing through the std calculation.\n    delta_std = torch.std(delta).detach()\n    # Inherit rank_gap margin base from both parents\n    rank_gap = ops.rank_gap(cost_a, cost_b)\n    # Inherit clipping from Parent 2\n    margin = torch.clamp(delta_std * rank_gap, min=0, max=margin_max)\n\n    # 4. Inherited Coupling: Confidence-Penalized Focal Strength from Parent 1\n    # Tanh(abs(delta)) is high for confident predictions, low for uncertain ones.\n    confidence_penalty = torch.tanh(torch.abs(delta.detach()))\n    dynamic_gamma = gamma_base * (1.0 - confidence_penalty)\n    \n    # 5. Calculate probability for focal loss\n    p_win = torch.sigmoid(delta)\n\n    # 6. Compute the focal modulating factor\n    modulating_factor = torch.pow(1.0 - p_win, dynamic_gamma)\n\n    # 7. Compute the core preference loss\n    loss_core = -F.logsigmoid(normalized_delta + margin)\n\n    # 8. Apply the modulating factor to the core loss\n    # Detach the factor so it only re-weights the loss, not change the gradient direction\n    final_loss = modulating_factor.detach() * loss_core\n\n    # Apply optional instance weights if provided\n    if weight is not None:\n        final_loss = final_loss * weight\n\n    # Return the mean loss over the batch\n    return final_loss.mean()", "theoretical_basis": "A dynamically modulated Bradley-Terry preference model. The core loss is logistic on z-scored log-probabilities. It features two key couplings: (1) A clipped, rank-based margin whose scale is dynamically set by the batch's log-probability standard deviation, making the separation target adaptive to model output variance. (2) A focal modulation strength that is inversely coupled with model confidence (via tanh), concentrating learning on uncertain pairs."}, "static_ok": true, "static_reason": "", "static_error_code": "", "dynamic_ok": true, "dynamic_reason": "ok", "loss_value": 0.1732867956161499, "grad_norm": 0.0, "pref_ok": false, "pref_reason": "pref_semantic_violation", "pref_mono_pass_rate": 0.7298177083333334, "pref_swap_pass_rate": 1.0, "pref_gap_pass_rate": 0.0, "dynamic_error_code": "E_PREF_SEMANTIC"}
{"generation": 9, "index": 6, "attempt": 1, "ir": {"name": "FocalLossWithAdaptiveMarginAndConfidencePenalty", "intuition": "Repaired: The original loss failed the preference gate's `gap_pass_rate` check, indicating it did not consistently prefer lower-cost solutions when the cost gap was large. This was caused by the focal modulation factor, which was applied to a loss term that included a normalized log-probability difference (`zscore(delta)`). The z-score normalization decouples the loss's magnitude from the raw `delta`, while the focal factor still depends on the raw `delta`. This mismatch meant that for large `delta`, `p_win` approached 1, causing the focal factor to approach 0, which incorrectly suppressed the loss for correctly-classified pairs. The fix is to remove the z-score normalization and apply the focal modulation to a loss based on the raw `delta`, ensuring the loss behaves monotonically with respect to the cost gap.", "pseudocode": "1. Calculate the log-probability difference: delta = log_prob_w - log_prob_l.\n2. Calculate the standard deviation of the log-probability differences: delta_std = std(delta).\n3. Compute the cost rank gap: rank_gap = rank_gap(cost_a, cost_b).\n4. Calculate the batch-adaptive margin: margin = clamp(delta_std.detach() * rank_gap, min=0, max=margin_max).\n5. Compute a confidence-penalized focal strength: dynamic_gamma = gamma_base * (1.0 - tanh(abs(delta.detach()))).\n6. Calculate the model's win probability: p_win = sigmoid(delta).\n7. Compute the focal modulating factor: modulating_factor = (1 - p_win)^dynamic_gamma.\n8. Compute the core preference loss using the raw delta: loss_core = -logsigmoid(delta + margin).\n9. Apply the detached modulating factor: final_loss = modulating_factor.detach() * loss_core.\n10. Return the mean of the final loss.", "hyperparams": {"gamma_base": 2.0, "margin_max": 2.5}, "operators_used": ["rank_gap", "clamp", "tanh", "sigmoid", "logsigmoid"], "implementation_hint": {"expects": ["cost_a", "cost_b", "log_prob_w", "log_prob_l"], "returns": "scalar"}, "code": "def generated_loss(batch, model_output, extra):\n    \"\"\"\n    Combines focal loss with a clipped, rank-based margin and confidence-penalized gamma.\n    New Couplings:\n    1. The margin scale is adaptively set by the standard deviation of the batch's log-prob differences.\n    2. The focal gamma is penalized by model confidence (inherited from Parent 1).\n    \"\"\"\n    # Read hyperparameters with defaults\n    gamma_base = extra.get('gamma_base', 2.0)\n    margin_max = extra.get('margin_max', 2.5)\n\n    # Read required tensors from the batch\n    cost_a = batch['cost_a']\n    cost_b = batch['cost_b']\n    log_prob_w = batch['log_prob_w']\n    log_prob_l = batch['log_prob_l']\n    weight = batch.get('weight')\n\n    # 1. Calculate the log-probability difference\n    delta = log_prob_w - log_prob_l\n\n    # 2. New Coupling 1: Batch-Adaptive Margin Scaling\n    # The margin scale is the standard deviation of the log-prob differences in the batch.\n    # Detach to prevent gradients from flowing through the std calculation.\n    delta_std = torch.std(delta).detach()\n    rank_gap = ops.rank_gap(cost_a, cost_b)\n    margin = torch.clamp(delta_std * rank_gap, min=0, max=margin_max)\n\n    # 3. Inherited Coupling: Confidence-Penalized Focal Strength\n    confidence_penalty = torch.tanh(torch.abs(delta.detach()))\n    dynamic_gamma = gamma_base * (1.0 - confidence_penalty)\n    \n    # 4. Calculate probability for focal loss\n    p_win = torch.sigmoid(delta)\n\n    # 5. Compute the focal modulating factor\n    modulating_factor = torch.pow(1.0 - p_win, dynamic_gamma)\n\n    # 6. Compute the core preference loss. REPAIR: Removed zscore from the loss term\n    # to ensure the loss is monotonic with respect to the cost gap.\n    loss_core = -F.logsigmoid(delta + margin)\n\n    # 7. Apply the modulating factor to the core loss\n    # Detach the factor so it only re-weights the loss, not change the gradient direction\n    final_loss = modulating_factor.detach() * loss_core\n\n    # Apply optional instance weights if provided\n    if weight is not None:\n        final_loss = final_loss * weight\n\n    # Return the mean loss over the batch\n    return final_loss.mean()", "theoretical_basis": ""}, "static_ok": true, "static_reason": "", "static_error_code": "", "dynamic_ok": true, "dynamic_reason": "ok", "loss_value": 0.1732867956161499, "grad_norm": 0.0, "pref_ok": false, "pref_reason": "pref_semantic_violation", "pref_mono_pass_rate": 1.0, "pref_swap_pass_rate": 1.0, "pref_gap_pass_rate": 0.0, "dynamic_error_code": "E_PREF_SEMANTIC"}
{"generation": 9, "index": 6, "attempt": 2, "ir": {"name": "FocalLossWithAdaptiveMarginAndConfidencePenalty", "intuition": "The loss failed the preference gate's `gap_pass_rate` check (E_PREF_SEMANTIC), indicating it did not consistently prefer lower-cost solutions when the cost gap was large. This was caused by the focal modulation factor `(1 - p_win)^gamma`, which was applied to a loss term that included a margin. The `p_win` term (derived from `sigmoid(delta)`) approaches 1 for large `delta` (easy, correctly classified pairs), causing the modulating factor to approach 0. This incorrectly suppressed the loss for correctly-classified pairs, even when the cost gap was large, preventing the model from learning the magnitude of preference. The fix is to apply the focal modulation only to the log-probability difference `delta`, and add the margin separately. This ensures that the margin term always contributes to the loss, preserving the preference signal for large cost gaps.", "pseudocode": "1. Calculate the log-probability difference: delta = log_prob_w - log_prob_l.\n2. Calculate the standard deviation of the log-probability differences: delta_std = std(delta).\n3. Compute the cost rank gap: rank_gap = rank_gap(cost_a, cost_b).\n4. Calculate the batch-adaptive margin: margin = clamp(delta_std.detach() * rank_gap, min=0, max=margin_max).\n5. Compute a confidence-penalized focal strength: dynamic_gamma = gamma_base * (1.0 - tanh(abs(delta.detach()))).\n6. Calculate the model's win probability: p_win = sigmoid(delta).\n7. Compute the focal modulating factor: modulating_factor = (1 - p_win)^dynamic_gamma.\n8. Compute the focal-modulated log-probability difference: focal_term = modulating_factor.detach() * delta.\n9. Compute the final loss by adding the margin to the focal term: loss = -logsigmoid(focal_term + margin).\n10. Return the mean of the final loss.", "hyperparams": {"gamma_base": 2.0, "margin_max": 2.5}, "operators_used": ["rank_gap", "clamp", "tanh", "sigmoid", "logsigmoid"], "implementation_hint": {"expects": ["cost_a", "cost_b", "log_prob_w", "log_prob_l"], "returns": "scalar"}, "code": "def generated_loss(batch, model_output, extra):\n    \"\"\"\n    Combines focal loss with a clipped, rank-based margin and confidence-penalized gamma.\n    New Couplings:\n    1. The margin scale is adaptively set by the standard deviation of the batch's log-prob differences.\n    2. The focal gamma is penalized by model confidence (inherited from Parent 1).\n    \"\"\"\n    # Read hyperparameters with defaults\n    gamma_base = extra.get('gamma_base', 2.0)\n    margin_max = extra.get('margin_max', 2.5)\n\n    # Read required tensors from the batch\n    cost_a = batch['cost_a']\n    cost_b = batch['cost_b']\n    log_prob_w = batch['log_prob_w']\n    log_prob_l = batch['log_prob_l']\n    weight = batch.get('weight')\n\n    # 1. Calculate the log-probability difference\n    delta = log_prob_w - log_prob_l\n\n    # 2. New Coupling 1: Batch-Adaptive Margin Scaling\n    # The margin scale is the standard deviation of the log-prob differences in the batch.\n    # Detach to prevent gradients from flowing through the std calculation.\n    delta_std = torch.std(delta).detach()\n    rank_gap = ops.rank_gap(cost_a, cost_b)\n    margin = torch.clamp(delta_std * rank_gap, min=0, max=margin_max)\n\n    # 3. Inherited Coupling: Confidence-Penalized Focal Strength\n    confidence_penalty = torch.tanh(torch.abs(delta.detach()))\n    dynamic_gamma = gamma_base * (1.0 - confidence_penalty)\n    \n    # 4. Calculate probability for focal loss\n    p_win = torch.sigmoid(delta)\n\n    # 5. Compute the focal modulating factor\n    modulating_factor = torch.pow(1.0 - p_win, dynamic_gamma)\n\n    # 6. REPAIR: Apply focal modulation to delta, then add margin.\n    # This prevents the modulating factor from suppressing the margin's effect,\n    # which is crucial for the gap_pass_rate check.\n    focal_term = modulating_factor.detach() * delta\n    final_loss = -F.logsigmoid(focal_term + margin)\n\n    # Apply optional instance weights if provided\n    if weight is not None:\n        final_loss = final_loss * weight\n\n    # Return the mean loss over the batch\n    return final_loss.mean()", "theoretical_basis": ""}, "static_ok": true, "static_reason": "", "static_error_code": "", "dynamic_ok": true, "dynamic_reason": "ok", "loss_value": 0.6931471824645996, "grad_norm": 0.0, "pref_ok": false, "pref_reason": "pref_semantic_violation", "pref_mono_pass_rate": 1.0, "pref_swap_pass_rate": 1.0, "pref_gap_pass_rate": 0.0, "dynamic_error_code": "E_PREF_SEMANTIC"}
{"generation": 9, "index": 7, "attempt": 0, "ir": {"name": "ConfidenceAwareFocalLossWithDynamicMargin", "intuition": "Mode: explore. This loss function hybridizes the robust focal loss framework from its parents with a novel coupling of margin and focal strength, both dynamically modulated by model confidence. The goal is to create a loss that focuses learning on uncertain examples while simultaneously adjusting the separation target based on that same uncertainty.\n\nInherited Ideas:\n1. From both parents (`FocalLossWithClippedCostSensitiveMargin`, `AdaptiveFocalLogsigmoidWithClippedMargin`): The foundational structure is a focal-modulated Bradley-Terry loss: `(1 - p_win)^gamma * -logsigmoid(normalized_delta + margin)`. This retains the proven benefit of up-weighting difficult examples.\n2. From both parents: The use of `ops.zscore` on the log-probability difference (`delta`) is kept for batch-level normalization, ensuring stability and consistent scaling.\n3. From both parents: The use of a `rank_gap`-based margin that is clipped with `clamp` to prevent vanishing gradients.\n\nNew Coupling Ideas:\n1. **Confidence-Modulated Margin**: Instead of the margin being purely a function of cost difference, we couple it with the model's own confidence. The margin is scaled by `tanh(alpha * rank_gap)`, making it adaptive to cost separation, but then this is multiplied by a confidence penalty term `(1 - tanh(abs(delta.detach())))`. This means for pairs the model is already very certain about (high `abs(delta)`), the margin shrinks, reducing the push for further separation and preventing overfitting. For uncertain pairs (low `abs(delta)`), the margin is larger, demanding a clearer separation.\n2. **Unified Confidence Modulation**: The same confidence penalty term `(1 - tanh(abs(delta.detach())))` is used to modulate both the margin and the focal strength `gamma`. This creates a unified mechanism where model uncertainty simultaneously increases the learning weight (via focal `gamma`) and the separation target (via `margin`), concentrating the learning effort on the most informative, uncertain pairs in a synergistic way.", "pseudocode": "1. Calculate the log-probability difference: delta = log_prob_w - log_prob_l.\n2. Normalize delta across the batch for stability: normalized_delta = zscore(delta).\n3. Compute the cost rank gap: rank_gap = rank_gap(cost_a, cost_b).\n4. Compute the model's confidence penalty: confidence_penalty = 1.0 - tanh(abs(delta.detach())).\n5. Calculate the confidence-modulated margin: margin_base = beta * tanh(alpha * rank_gap); margin = clamp(margin_base * confidence_penalty, min=0, max=margin_max).\n6. Calculate the confidence-modulated focal strength: dynamic_gamma = gamma_base * confidence_penalty.\n7. Calculate the model's win probability: p_win = sigmoid(delta).\n8. Compute the focal modulating factor: modulating_factor = (1 - p_win)^dynamic_gamma.\n9. Compute the core preference loss: loss_core = -logsigmoid(normalized_delta + margin).\n10. Apply the detached modulating factor: final_loss = modulating_factor.detach() * loss_core.\n11. Return the mean of the final loss.", "hyperparams": {"beta": 1.5, "gamma_base": 2.0, "margin_max": 2.5, "alpha": 0.5}, "operators_used": ["zscore", "rank_gap", "tanh", "clamp", "sigmoid", "logsigmoid"], "implementation_hint": {"expects": ["cost_a", "cost_b", "log_prob_w", "log_prob_l"], "returns": "scalar"}, "code": "def generated_loss(batch, model_output, extra):\n    \"\"\"\n    Combines focal loss with a novel coupling where model confidence modulates both the margin and the focal strength.\n    \"\"\"\n    # Read hyperparameters with defaults\n    beta = extra.get('beta', 1.5)\n    gamma_base = extra.get('gamma_base', 2.0)\n    margin_max = extra.get('margin_max', 2.5)\n    alpha = extra.get('alpha', 0.5)\n\n    # Read required tensors from the batch\n    cost_a = batch['cost_a']\n    cost_b = batch['cost_b']\n    log_prob_w = batch['log_prob_w']\n    log_prob_l = batch['log_prob_l']\n    weight = batch.get('weight')\n\n    # 1. Calculate the log-probability difference\n    delta = log_prob_w - log_prob_l\n\n    # 2. Inherited: Normalize delta for stability\n    normalized_delta = ops.zscore(delta)\n\n    # 3. New Coupling: Unified Confidence Modulation\n    # Confidence penalty is high (near 1) for uncertain pairs (delta near 0)\n    # and low (near 0) for confident pairs (delta far from 0).\n    # Detach delta to ensure this only acts as a weighting/scaling factor.\n    confidence_penalty = 1.0 - torch.tanh(torch.abs(delta.detach()))\n\n    # 4. New Coupling 1: Confidence-Modulated Margin\n    # Inherit rank_gap as the basis for the margin\n    rank_gap = ops.rank_gap(cost_a, cost_b)\n    # Use tanh for a bounded, adaptive margin base that saturates gracefully\n    margin_base = beta * torch.tanh(alpha * rank_gap)\n    # Scale the margin by confidence: uncertain pairs get a larger margin target.\n    # Inherit clamp for stability.\n    margin = torch.clamp(margin_base * confidence_penalty, min=0, max=margin_max)\n\n    # 5. New Coupling 2: Confidence-Modulated Focal Strength\n    # Inherit dynamic gamma, but modulate it with the same confidence penalty.\n    dynamic_gamma = gamma_base * confidence_penalty\n\n    # 6. Inherited: Calculate probability for focal loss\n    p_win = torch.sigmoid(delta)\n\n    # 7. Inherited: Compute the focal modulating factor\n    # Use the confidence-aware gamma.\n    modulating_factor = torch.pow(1.0 - p_win, dynamic_gamma)\n\n    # 8. Compute the core preference loss with the confidence-aware margin\n    loss_core = -F.logsigmoid(normalized_delta + margin)\n\n    # 9. Apply the modulating factor to the core loss\n    # Detach the factor so it only re-weights the loss, not change the gradient direction\n    final_loss = modulating_factor.detach() * loss_core\n\n    # Apply optional instance weights if provided\n    if weight is not None:\n        final_loss = final_loss * weight\n\n    # Return the mean loss over the batch\n    return final_loss.mean()", "theoretical_basis": "A dynamically modulated Bradley-Terry preference model. The core is a logistic loss on z-scored log-probabilities. It introduces a unified coupling mechanism based on model confidence (`1 - tanh|delta|`). This confidence term simultaneously modulates both the separation margin and the focal loss strength. As model uncertainty increases for a given pair, both the target separation (margin) and the weight of the loss (focal gamma) increase, synergistically focusing the training on the most ambiguous examples."}, "static_ok": true, "static_reason": "", "static_error_code": "", "dynamic_ok": true, "dynamic_reason": "ok", "loss_value": 0.1013638973236084, "grad_norm": 0.0, "pref_ok": true, "pref_reason": "ok", "pref_mono_pass_rate": 0.853515625, "pref_swap_pass_rate": 0.6666666666666666, "pref_gap_pass_rate": 1.0}
