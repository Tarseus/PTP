{"generation": 0, "index": 3, "ir": {"name": "Sigmoid_Cost_Modulated_Margin_Loss", "intuition": "I fixed a `E_COMPILE_ERROR` caused by disallowed `import` statements. The original code imported `torch` and `torch.nn.functional`, which is not permitted. I removed these imports and replaced the call to `F.softplus` with `torch.nn.functional.softplus`, as the `torch` module is already available in the execution environment. The core logic of using a dynamic, cost-modulated margin remains unchanged.", "pseudocode": "1. Calculate the cost difference: delta_cost = cost(b) - cost(a).\n2. Normalize the cost difference using Z-score normalization: z_cost = (delta_cost - mean(delta_cost)) / (std(delta_cost) + epsilon).\n3. Compute a dynamic margin by passing the normalized cost difference through a sigmoid function and scaling it by a maximum margin hyperparameter: margin = margin_max * sigmoid(z_cost * temperature).\n4. Calculate the log-probability difference: delta_logp = logp(a) - logp(b).\n5. Compute the core loss term, which is the softplus of the margin minus the log-probability difference. This penalizes cases where logp(a) - logp(b) < margin.\n6. Return the mean of this loss across the batch.", "hyperparams": {"margin_max": 1.0, "temperature": 1.0, "epsilon": 1e-08}, "operators_used": ["zscore", "sigmoid", "softplus"], "implementation_hint": {"expects": ["The `batch` dictionary is expected to contain `cost_a`, `cost_b`, `log_prob_w`, `log_prob_l`, and optionally `weight`. The `extra` dictionary should contain a `hyperparams` object with `margin_max`, `temperature`, and `epsilon`."], "returns": "Returns a single scalar tensor representing the mean loss for the batch."}, "code": "def generated_loss(batch, model_output, extra):\n    \"\"\"\n    A preference loss with a margin modulated by the sigmoid of the normalized cost difference.\n    \"\"\"\n    cost_a = batch['cost_a']\n    cost_b = batch['cost_b']\n    logp_a = batch['log_prob_w'] # log_prob of the winner (preferred)\n    logp_b = batch['log_prob_l'] # log_prob of the loser (not preferred)\n    weights = batch.get('weight')\n\n    # Hyperparameters from the provided config\n    hyperparams = extra['hyperparams']\n    margin_max = hyperparams.get('margin_max', 1.0)\n    temperature = hyperparams.get('temperature', 1.0)\n    epsilon = hyperparams.get('epsilon', 1e-8)\n\n    # Ensure cost_a is better than cost_b\n    # In the provided setting, w is winner, l is loser, so cost_w < cost_l\n    # Let a=w, b=l. We want to encourage logp(a) > logp(b).\n\n    # 1. Calculate cost difference\n    # cost_b is higher (worse), cost_a is lower (better), so delta_cost is positive.\n    delta_cost = cost_b - cost_a\n\n    # 2. Normalize cost difference (Z-score)\n    # This makes the margin robust to the scale of costs in the batch.\n    if delta_cost.numel() > 1:\n        mean_delta = delta_cost.mean()\n        std_delta = delta_cost.std()\n        z_cost = (delta_cost - mean_delta) / (std_delta + epsilon)\n    else:\n        # Handle batch size of 1 to avoid NaN from std=0\n        z_cost = torch.zeros_like(delta_cost)\n\n    # 3. Compute dynamic margin\n    # Sigmoid maps the normalized cost diff to (0, 1).\n    # Large positive z_cost (big cost gap) -> sigmoid -> ~1 -> margin -> ~margin_max\n    # Small/negative z_cost (small/no gap) -> sigmoid -> ~0.5/0 -> margin -> ~margin_max/2 or 0\n    margin = margin_max * torch.sigmoid(z_cost * temperature)\n\n    # 4. Calculate log-probability difference\n    delta_logp = logp_a - logp_b\n\n    # 5. Compute the core loss term using softplus for a smooth hinge-like loss\n    # We want delta_logp > margin. So we penalize margin - delta_logp > 0.\n    # softplus(x) = log(1 + exp(x)). It's a smooth version of ReLU.\n    loss_per_pair = torch.nn.functional.softplus(margin - delta_logp)\n\n    # 6. Apply optional weights and compute the mean loss\n    if weights is not None:\n        loss = (loss_per_pair * weights).sum() / weights.sum().clamp(min=epsilon)\n    else:\n        loss = loss_per_pair.mean()\n\n    return loss", "theoretical_basis": ""}, "fitness": {"hf_like_score": 20.192237506103517, "validation_objective": 20.192237506103517, "generalization_penalty": 0.0, "generalization_objectives": {"100": 20.185000161743165}, "train_score_mean": 24.82542202949524, "train_loss_mean": 1.6890569913387299, "pair_count": 31679973, "early_eval": {"enabled": true, "steps": 100, "baseline_validation_objective": 8.412976063537597, "candidate_validation_objective": 20.192237506103517, "early_stopped": true}, "phases": {"f1": {"steps": 15630, "train_score_mean": 24.82542202949524, "train_loss_mean": 1.6890569913387299, "pair_count": 31679973}, "f2": {"steps": 0, "train_score_mean": null, "train_loss_mean": null, "pair_count": 0}}, "config": {"hf": {"problem": "tsp", "hf_steps": 0, "hf_epochs": 10, "hf_instances_per_epoch": 100000, "train_problem_size": 100, "valid_problem_sizes": [100], "train_batch_size": 64, "pomo_size": 100, "learning_rate": 0.0003, "weight_decay": 1e-06, "alpha": 0.05, "device": "cuda:0", "seed": 1234, "num_validation_episodes": 10000, "validation_batch_size": 64, "generalization_penalty_weight": 1.0, "pool_version": "v0"}, "free_loss": {"f1_steps": 0, "total_train_steps": 15630, "f2_steps": 0, "f3_enabled": false}}, "loss_ir": {"name": "Sigmoid_Cost_Modulated_Margin_Loss", "intuition": "I fixed a `E_COMPILE_ERROR` caused by disallowed `import` statements. The original code imported `torch` and `torch.nn.functional`, which is not permitted. I removed these imports and replaced the call to `F.softplus` with `torch.nn.functional.softplus`, as the `torch` module is already available in the execution environment. The core logic of using a dynamic, cost-modulated margin remains unchanged.", "hyperparams": {"margin_max": 1.0, "temperature": 1.0, "epsilon": 1e-08}, "operators_used": ["zscore", "sigmoid", "softplus"]}}, "better_than_baseline": false}
{"generation": 0, "index": 6, "ir": {"name": "Sigmoid_Cost_Modulated_Margin_Loss", "intuition": "Based on the compile error E_COMPILE_ERROR, I removed the `import torch` and `import torch.nn.functional as F` statements from the code. The loss function's execution environment provides these modules implicitly, so explicit imports are forbidden. The core logic of the loss, which uses a sigmoid-scaled cost difference to modulate a margin for the log-probability difference, remains unchanged.", "pseudocode": "1. Calculate the cost difference: delta_cost = cost(b) - cost(a).\n2. Normalize this difference into a [0, 1] range using a sigmoid function: cost_weight = sigmoid(delta_cost / temperature). This represents the 'importance' of the preference.\n3. Calculate the log-probability difference: delta_logp = logp(a) - logp(b).\n4. Create a cost-modulated margin: margin = margin_scale * cost_weight.\n5. The core term is -(delta_logp - margin). A large positive value means the model's preference aligns with the cost difference and exceeds the required margin, so the loss should be low. A negative value means the model's preference is wrong or insufficient, so the loss should be high.\n6. Apply a softplus function to this core term to get the final loss: softplus(-(delta_logp - margin)). This creates a smooth, non-negative loss that penalizes incorrect or weak preferences.", "hyperparams": {"temperature": 1.0, "margin_scale": 0.5}, "operators_used": ["sigmoid", "softplus"], "implementation_hint": {"expects": ["A batch dictionary with 'log_prob_w', 'log_prob_l', 'cost_a', 'cost_b', and optional 'weight'. Assumes cost_a (winner) < cost_b (loser)."], "returns": "A scalar tensor representing the mean loss over the batch."}, "code": "def generated_loss(batch, model_output, extra):\n    \"\"\"\n    A preference loss with a margin modulated by a sigmoid-scaled cost difference.\n    \"\"\"\n    # Read hyperparameters\n    # Use extra.get() to allow for external hyperparameter tuning during experiments\n    temperature = extra.get('temperature', 1.0)\n    margin_scale = extra.get('margin_scale', 0.5)\n\n    # The dataset provides log_prob_w and log_prob_l, where 'w' is the winner (lower cost)\n    # and 'l' is the loser (higher cost).\n    # So, cost_a corresponds to log_prob_w and cost_b to log_prob_l.\n    logp_a = batch['log_prob_w']\n    logp_b = batch['log_prob_l']\n    cost_a = batch['cost_a']\n    cost_b = batch['cost_b']\n\n    # 1. Calculate cost and log-probability differences\n    # delta_cost is positive, representing how much better solution 'a' is.\n    delta_cost = cost_b - cost_a\n    # delta_logp is the model's preference score for 'a' over 'b'.\n    # We want this to be positive.\n    delta_logp = logp_a - logp_b\n\n    # 2. Normalize the cost difference into a weight/importance factor using a sigmoid\n    # The temperature controls the steepness of the sigmoid.\n    cost_weight = torch.sigmoid(delta_cost / temperature)\n\n    # 3. Define the margin, which is scaled by the cost weight\n    # The margin is between 0 and margin_scale, depending on the cost gap.\n    margin = margin_scale * cost_weight\n\n    # 4. Calculate the core loss term\n    # We want delta_logp > margin. So, (delta_logp - margin) should be positive.\n    # We penalize when this is not the case by taking the negative.\n    loss_arg = -(delta_logp - margin)\n\n    # 5. Apply softplus to get a smooth, non-negative loss\n    # softplus(x) = log(1 + exp(x)). It's a smooth version of ReLU.\n    pair_loss = F.softplus(loss_arg)\n\n    # 6. Apply optional instance weights and compute the mean\n    weights = batch.get('weight')\n    if weights is not None:\n        return (pair_loss * weights).mean()\n    else:\n        return pair_loss.mean()", "theoretical_basis": ""}, "fitness": {"hf_like_score": 20.491081283569336, "validation_objective": 20.478725329589842, "generalization_penalty": 0.012355953979493961, "generalization_objectives": {"100": 20.491081283569336}, "train_score_mean": 24.996521215438843, "train_loss_mean": 1.6508665311336517, "pair_count": 31679987, "early_eval": {"enabled": true, "steps": 100, "baseline_validation_objective": 8.412976063537597, "candidate_validation_objective": 20.478725329589842, "early_stopped": true}, "phases": {"f1": {"steps": 15630, "train_score_mean": 24.996521215438843, "train_loss_mean": 1.6508665311336517, "pair_count": 31679987}, "f2": {"steps": 0, "train_score_mean": null, "train_loss_mean": null, "pair_count": 0}}, "config": {"hf": {"problem": "tsp", "hf_steps": 0, "hf_epochs": 10, "hf_instances_per_epoch": 100000, "train_problem_size": 100, "valid_problem_sizes": [100], "train_batch_size": 64, "pomo_size": 100, "learning_rate": 0.0003, "weight_decay": 1e-06, "alpha": 0.05, "device": "cuda:1", "seed": 1234, "num_validation_episodes": 10000, "validation_batch_size": 64, "generalization_penalty_weight": 1.0, "pool_version": "v0"}, "free_loss": {"f1_steps": 0, "total_train_steps": 15630, "f2_steps": 0, "f3_enabled": false}}, "loss_ir": {"name": "Sigmoid_Cost_Modulated_Margin_Loss", "intuition": "Based on the compile error E_COMPILE_ERROR, I removed the `import torch` and `import torch.nn.functional as F` statements from the code. The loss function's execution environment provides these modules implicitly, so explicit imports are forbidden. The core logic of the loss, which uses a sigmoid-scaled cost difference to modulate a margin for the log-probability difference, remains unchanged.", "hyperparams": {"temperature": 1.0, "margin_scale": 0.5}, "operators_used": ["sigmoid", "softplus"]}}, "better_than_baseline": false}
{"generation": 1, "index": 0, "ir": {"name": "Adaptive_Cost_Margin_Loss_with_Logp_Normalization", "intuition": "Based on the compile error E_COMPILE_ERROR, I removed the `import torch` and `import torch.nn.functional as F` statements from the code. These are not allowed as the execution environment provides these libraries. The core logic of the loss, which uses a rank-based cost margin and a log-probability magnitude normalizer for stability, remains unchanged.", "pseudocode": "1. Calculate the log-probability difference: delta_logp = logp(a) - logp(b).\n2. Calculate the cost difference: delta_cost = cost(b) - cost(a).\n3. Normalize the cost difference using rank_gap, which maps the cost differences in the batch to a [0, 1] range based on their rank. This produces `cost_weight`.\n4. Compute a dynamic margin by scaling the `cost_weight` with a hyperparameter `margin_scale`.\n5. Compute the core preference loss term as `softplus(margin - delta_logp)`. This penalizes cases where the log-probability difference does not exceed the cost-dependent margin.\n6. Compute a normalization factor based on the absolute magnitude of the log-probabilities: `logp_norm = 1.0 + softplus(abs(logp_a) + abs(logp_b))`. This factor is larger when log probabilities are large, and close to 1 when they are near zero.\n7. Calculate the final loss for each pair by dividing the core preference loss by the `logp_norm` factor.\n8. Return the mean of the final loss across the batch.", "hyperparams": {"margin_scale": 1.0, "epsilon": 1e-08}, "operators_used": ["softplus"], "implementation_hint": {"expects": ["A batch containing 'cost_a', 'cost_b', 'log_prob_w', 'log_prob_l'. 'log_prob_w' and 'log_prob_l' are the log-probabilities of the preferred and non-preferred completions, respectively."], "returns": "A scalar tensor representing the mean loss over the batch."}, "code": "def generated_loss(batch, model_output, extra):\n    \"\"\"\n    A preference loss with a margin modulated by the rank-normalized cost difference,\n    and a stability term that normalizes the loss by the magnitude of log-probabilities.\n    \"\"\"\n    # The execution environment provides 'torch' and 'torch.nn.functional as F'\n\n    # Read tensors from the batch\n    cost_a = batch['cost_a']\n    cost_b = batch['cost_b']\n    logp_a = batch['log_prob_w']\n    logp_b = batch['log_prob_l']\n    weights = batch.get('weight')\n\n    # Read hyperparameters\n    hyperparams = extra['hyperparams']\n    margin_scale = hyperparams.get('margin_scale', 1.0)\n    epsilon = hyperparams.get('epsilon', 1e-8)\n\n    # 1. Calculate log-probability and cost differences\n    delta_logp = logp_a - logp_b\n    delta_cost = cost_b - cost_a\n\n    # 3. Normalize cost difference using rank_gap\n    # rank_gap maps values to [0, 1] based on their sorted order\n    if delta_cost.numel() > 1:\n        cost_weight = (delta_cost.argsort().argsort().float() / (delta_cost.numel() - 1))\n    else:\n        cost_weight = torch.zeros_like(delta_cost)\n\n    # 4. Compute the dynamic margin\n    margin = margin_scale * cost_weight\n\n    # 5. Compute the core preference loss term\n    # We want delta_logp > margin, so we penalize margin - delta_logp > 0\n    core_loss = F.softplus(margin - delta_logp)\n\n    # 6. Compute the logp magnitude normalization factor for stability\n    # This term grows as log-probs become very large (positive or negative),\n    # dampening the loss and gradients to prevent instability.\n    logp_abs_sum = torch.abs(logp_a) + torch.abs(logp_b)\n    logp_norm = 1.0 + F.softplus(logp_abs_sum - 4.0) # Shift to activate mainly for large magnitudes\n\n    # 7. Calculate the final loss, normalized by logp magnitude\n    loss_per_pair = core_loss / logp_norm\n\n    # 8. Apply optional weights and return the mean loss\n    if weights is not None:\n        loss = (loss_per_pair * weights).sum() / weights.sum().clamp(min=epsilon)\n    else:\n        loss = loss_per_pair.mean()\n\n    return loss", "theoretical_basis": ""}, "fitness": {"hf_like_score": 10.463138148498535, "validation_objective": 10.463138148498535, "generalization_penalty": 0.0, "generalization_objectives": {"100": 10.462981301879882}, "train_score_mean": 23.203106031417846, "train_loss_mean": 0.004575619278475642, "pair_count": 31679990, "early_eval": {"enabled": true, "steps": 100, "baseline_validation_objective": 8.412976063537597, "candidate_validation_objective": 10.463138148498535, "early_stopped": true}, "phases": {"f1": {"steps": 15630, "train_score_mean": 23.203106031417846, "train_loss_mean": 0.004575619278475642, "pair_count": 31679990}, "f2": {"steps": 0, "train_score_mean": null, "train_loss_mean": null, "pair_count": 0}}, "config": {"hf": {"problem": "tsp", "hf_steps": 0, "hf_epochs": 10, "hf_instances_per_epoch": 100000, "train_problem_size": 100, "valid_problem_sizes": [100], "train_batch_size": 64, "pomo_size": 100, "learning_rate": 0.0003, "weight_decay": 1e-06, "alpha": 0.05, "device": "cuda:0", "seed": 1234, "num_validation_episodes": 10000, "validation_batch_size": 64, "generalization_penalty_weight": 1.0, "pool_version": "v0"}, "free_loss": {"f1_steps": 0, "total_train_steps": 15630, "f2_steps": 0, "f3_enabled": false}}, "loss_ir": {"name": "Adaptive_Cost_Margin_Loss_with_Logp_Normalization", "intuition": "Based on the compile error E_COMPILE_ERROR, I removed the `import torch` and `import torch.nn.functional as F` statements from the code. These are not allowed as the execution environment provides these libraries. The core logic of the loss, which uses a rank-based cost margin and a log-probability magnitude normalizer for stability, remains unchanged.", "hyperparams": {"margin_scale": 1.0, "epsilon": 1e-08}, "operators_used": ["softplus"]}}, "better_than_baseline": false}
{"generation": 1, "index": 2, "ir": {"name": "Adaptive_Log_Margin_Loss", "intuition": "Mode: explore. This loss combines ideas from both parents while introducing new couplings. From Parent 0, it inherits the use of Z-score normalization on the cost difference (`delta_cost`) for robustness to cost scaling. From Parent 1, it adopts the idea of a simple, direct sigmoid modulation without a separate `margin_max` hyperparameter. The key new coupling is to apply the cost-based modulation *logarithmically* to the margin, inspired by DPO-style objectives. Instead of a linear margin `m`, the loss aims for `logp_a - logp_b > m`. We model `m` as `log(sigmoid(z_cost * temperature) + 1)`. This creates a margin that grows from `log(1.5) approx 0.4` for average cost gaps to `log(2) approx 0.69` for large gaps, providing a soft, non-saturating target for the log-probability difference. A second coupling is a `beta` hyperparameter that directly scales the log-probability difference, `beta * (logp_a - logp_b)`, allowing explicit control over the gradient magnitude, similar to the inverse temperature in a Bradley-Terry model. This design remains within a margin-based framework but adapts the margin's scale and influence in a novel, logarithmic way.", "pseudocode": "1. Calculate the difference in log-probabilities: delta_logp = logp(a) - logp(b).\n2. Calculate the difference in costs: delta_cost = cost(b) - cost(a).\n3. Normalize the cost difference using Z-score normalization across the batch: z_cost = (delta_cost - mean(delta_cost)) / (std(delta_cost) + epsilon).\n4. Compute a dynamic, logarithmic margin. First, pass the normalized cost through a sigmoid: `s = sigmoid(z_cost * temperature)`. Then compute the margin as `margin = log(s + 1)`. This ensures the margin is always positive and grows smoothly with the cost gap.\n5. Scale the log-probability difference by a hyperparameter `beta`.\n6. The core loss is `logsigmoid(margin - beta * delta_logp)`. This is a logistic loss that penalizes cases where `beta * delta_logp` is not greater than the dynamic margin.\n7. Return the negative mean of this value across the batch to formulate it as a minimization problem.", "hyperparams": {"beta": 0.5, "temperature": 1.0, "epsilon": 1e-08}, "operators_used": ["zscore", "sigmoid", "log", "logsigmoid"], "implementation_hint": {"expects": ["cost_a", "cost_b", "logp_a", "logp_b"], "returns": "scalar"}, "code": "def generated_loss(batch, model_output, extra):\n    \"\"\"A preference loss with a dynamic, logarithmic margin based on normalized cost differences.\"\"\"\n    # Read tensors from the batch\n    cost_a = batch['cost_a']\n    cost_b = batch['cost_b']\n    logp_a = batch['log_prob_w']\n    logp_b = batch['log_prob_l']\n    weights = batch.get('weight')\n\n    # Read hyperparameters\n    hyperparams = extra['hyperparams']\n    beta = hyperparams.get('beta', 0.5)\n    temperature = hyperparams.get('temperature', 1.0)\n    epsilon = hyperparams.get('epsilon', 1e-8)\n\n    # 1. Calculate log-probability difference\n    delta_logp = logp_a - logp_b\n\n    # 2. Calculate cost difference\n    delta_cost = cost_b - cost_a\n\n    # 3. Z-score normalize the cost difference for scale invariance (from Parent 0)\n    if delta_cost.numel() > 1:\n        mean_delta_cost = delta_cost.mean()\n        std_delta_cost = delta_cost.std()\n        z_cost = (delta_cost - mean_delta_cost) / (std_delta_cost + epsilon)\n    else:\n        z_cost = torch.zeros_like(delta_cost)\n\n    # 4. Compute the dynamic logarithmic margin (New Coupling 1)\n    # Sigmoid modulation is inspired by both parents.\n    # The log(sigmoid(...) + 1) structure creates a soft, non-saturating margin.\n    cost_modulation = torch.sigmoid(z_cost * temperature)\n    margin = torch.log(cost_modulation + 1.0) # Margin is always > log(1)=0\n\n    # 5. Scale the log-probability difference (New Coupling 2)\n    # 'beta' acts like an inverse temperature on the policy preference.\n    scaled_delta_logp = beta * delta_logp\n\n    # 6. Compute the logistic loss\n    # We want scaled_delta_logp > margin. The loss is -logsigmoid(scaled_delta_logp - margin).\n    # Using logsigmoid(margin - scaled_delta_logp) is equivalent but can be less stable if the argument is very large.\n    # We use the formulation -logsigmoid(X) where X = scaled_delta_logp - margin.\n    loss_arg = scaled_delta_logp - margin\n    pair_loss = -torch.nn.functional.logsigmoid(loss_arg)\n\n    # 7. Apply optional weights and compute the mean loss\n    if weights is not None:\n        # Ensure weights sum is not zero to avoid division by zero\n        loss = (pair_loss * weights).sum() / weights.sum().clamp(min=epsilon)\n    else:\n        loss = pair_loss.mean()\n\n    return loss", "theoretical_basis": "A margin-based classification loss on log-probabilities, where the margin is a logarithmic function of the normalized cost difference. It can be interpreted as a variant of a Bradley-Terry model where the target log-odds of preference are not zero but a dynamic, cost-dependent margin."}, "fitness": {"hf_like_score": 11.605984782409667, "validation_objective": 11.605984782409667, "generalization_penalty": 0.0, "generalization_objectives": {"100": 11.60033742980957}, "train_score_mean": 23.356021642684937, "train_loss_mean": 1.0539184480905532, "pair_count": 31679980, "early_eval": {"enabled": true, "steps": 100, "baseline_validation_objective": 8.412976063537597, "candidate_validation_objective": 11.605984782409667, "early_stopped": true}, "phases": {"f1": {"steps": 15630, "train_score_mean": 23.356021642684937, "train_loss_mean": 1.0539184480905532, "pair_count": 31679980}, "f2": {"steps": 0, "train_score_mean": null, "train_loss_mean": null, "pair_count": 0}}, "config": {"hf": {"problem": "tsp", "hf_steps": 0, "hf_epochs": 10, "hf_instances_per_epoch": 100000, "train_problem_size": 100, "valid_problem_sizes": [100], "train_batch_size": 64, "pomo_size": 100, "learning_rate": 0.0003, "weight_decay": 1e-06, "alpha": 0.05, "device": "cuda:1", "seed": 1234, "num_validation_episodes": 10000, "validation_batch_size": 64, "generalization_penalty_weight": 1.0, "pool_version": "v0"}, "free_loss": {"f1_steps": 0, "total_train_steps": 15630, "f2_steps": 0, "f3_enabled": false}}, "loss_ir": {"name": "Adaptive_Log_Margin_Loss", "intuition": "Mode: explore. This loss combines ideas from both parents while introducing new couplings. From Parent 0, it inherits the use of Z-score normalization on the cost difference (`delta_cost`) for robustness to cost scaling. From Parent 1, it adopts the idea of a simple, direct sigmoid modulation without a separate `margin_max` hyperparameter. The key new coupling is to apply the cost-based modulation *logarithmically* to the margin, inspired by DPO-style objectives. Instead of a linear margin `m`, the loss aims for `logp_a - logp_b > m`. We model `m` as `log(sigmoid(z_cost * temperature) + 1)`. This creates a margin that grows from `log(1.5) approx 0.4` for average cost gaps to `log(2) approx 0.69` for large gaps, providing a soft, non-saturating target for the log-probability difference. A second coupling is a `beta` hyperparameter that directly scales the log-probability difference, `beta * (logp_a - logp_b)`, allowing explicit control over the gradient magnitude, similar to the inverse temperature in a Bradley-Terry model. This design remains within a margin-based framework but adapts the margin's scale and influence in a novel, logarithmic way.", "hyperparams": {"beta": 0.5, "temperature": 1.0, "epsilon": 1e-08}, "operators_used": ["zscore", "sigmoid", "log", "logsigmoid"]}}, "better_than_baseline": false}
{"generation": 1, "index": 3, "ir": {"name": "Adaptive_Log_Sigmoid_Loss_with_Cost_Margin", "intuition": "Mode: explore. This loss combines the Bradley-Terry model's logistic structure with a dynamic, cost-aware margin. It inherits the use of a margin from both parents, which ensures the log-probability gap reflects the magnitude of the cost difference. From Parent 0, it inherits the idea of using batch-level statistics (z-score) to normalize the cost difference, making the loss robust to cost scaling. The core loss structure is changed from a hinge-like `softplus` to a logistic `logsigmoid`, which is common in preference learning and provides a smooth, bounded loss. As a new coupling idea, this loss introduces an adaptive beta (`beta_adaptive`). This beta is inversely proportional to the standard deviation of the log-probability differences in the batch, which acts as a stability trick. When model preferences are diverse (high std dev), beta is lowered to soften the gradients and prevent instability. When preferences are uniform (low std dev), beta is increased to sharpen the learning signal. A second coupling is the direct addition of a scaled, normalized cost difference as a margin inside the logsigmoid, simplifying the margin calculation compared to the parents.", "pseudocode": "1. Calculate the difference in log-probabilities: delta_logp = logp(a) - logp(b).\n2. Calculate the difference in costs: delta_cost = cost(b) - cost(a).\n3. Normalize the cost difference using Z-score normalization across the batch: z_cost = (delta_cost - mean(delta_cost)) / (std(delta_cost) + epsilon).\n4. Calculate an adaptive temperature (beta) for the logistic loss. It is the base beta hyperparameter divided by the standard deviation of the log-probability differences in the batch. This stabilizes training by reducing the gradient magnitude when model preferences are very diverse.\n5. Compute the final loss argument by adding a scaled margin to the log-probability difference: loss_arg = delta_logp + margin_scale * z_cost.\n6. Apply the negative log-sigmoid function to the argument, scaled by the adaptive beta: loss = -logsigmoid(beta_adaptive * loss_arg).\n7. Return the mean of the loss across the batch.", "hyperparams": {"beta": 1.0, "margin_scale": 0.5, "epsilon": 1e-08}, "operators_used": ["zscore", "logsigmoid"], "implementation_hint": {"expects": ["cost_a", "cost_b", "logp_a", "logp_b"], "returns": "scalar"}, "code": "def generated_loss(batch, model_output, extra):\n    \"\"\"\n    A preference loss based on the Bradley-Terry model, with a dynamic margin from\n    normalized cost differences and an adaptive beta for stability.\n    \"\"\"\n    # Read tensors from the batch\n    cost_a = batch['cost_a']\n    cost_b = batch['cost_b']\n    logp_a = batch['log_prob_w']\n    logp_b = batch['log_prob_l']\n    weights = batch.get('weight')\n\n    # Read hyperparameters\n    hyperparams = extra['hyperparams']\n    beta = hyperparams.get('beta', 1.0)\n    margin_scale = hyperparams.get('margin_scale', 0.5)\n    epsilon = hyperparams.get('epsilon', 1e-8)\n\n    # Step 1: Calculate log-probability difference\n    delta_logp = logp_a - logp_b\n\n    # Step 2: Calculate cost difference\n    delta_cost = cost_b - cost_a\n\n    # Step 3: Normalize cost difference (Z-score)\n    if delta_cost.numel() > 1:\n        mean_delta_cost = delta_cost.mean()\n        std_delta_cost = delta_cost.std()\n        z_cost = (delta_cost - mean_delta_cost) / (std_delta_cost + epsilon)\n    else:\n        z_cost = torch.zeros_like(delta_cost)\n\n    # Step 4: Calculate adaptive beta for stability\n    # This is a new coupling idea.\n    # When logp differences have high variance, we reduce beta to prevent instability.\n    if delta_logp.numel() > 1:\n        std_delta_logp = delta_logp.std().detach() # Detach to not backprop through std dev\n        beta_adaptive = beta / (std_delta_logp + epsilon)\n    else:\n        beta_adaptive = beta\n\n    # Step 5: Compute the loss argument with the cost-modulated margin\n    # The margin is directly added, scaled by the z-scored cost difference.\n    loss_arg = delta_logp + margin_scale * z_cost\n\n    # Step 6: Apply the negative log-sigmoid function\n    # This is equivalent to log(1 + exp(-beta * x)), which is a standard logistic loss.\n    loss_per_pair = -torch.nn.functional.logsigmoid(beta_adaptive * loss_arg)\n\n    # Step 7: Apply optional weights and compute the mean loss\n    if weights is not None:\n        loss = (loss_per_pair * weights).sum() / weights.sum().clamp(min=epsilon)\n    else:\n        loss = loss_per_pair.mean()\n\n    return loss", "theoretical_basis": "A modified Bradley-Terry logistic preference model where the preference probability depends not only on the log-probability difference but also on a margin term derived from the normalized cost difference. The logistic scaling factor (beta) is adapted based on batch statistics for stability."}, "fitness": {"hf_like_score": 7.908737715148926, "validation_objective": 7.906745343017578, "generalization_penalty": 0.0019923721313483966, "generalization_objectives": {"100": 7.908737715148926}, "train_score_mean": 8.161937943835024, "train_loss_mean": 0.5092373841116234, "pair_count": 4951555870, "early_eval": {"enabled": true, "steps": 100, "baseline_validation_objective": 8.412976063537597, "candidate_validation_objective": 8.325694194030762, "early_stopped": false}, "phases": {"f1": {"steps": 15630, "train_score_mean": 8.161937943835024, "train_loss_mean": 0.5092373841116234, "pair_count": 4951555870}, "f2": {"steps": 0, "train_score_mean": null, "train_loss_mean": null, "pair_count": 0}}, "config": {"hf": {"problem": "tsp", "hf_steps": 0, "hf_epochs": 10, "hf_instances_per_epoch": 100000, "train_problem_size": 100, "valid_problem_sizes": [100], "train_batch_size": 64, "pomo_size": 100, "learning_rate": 0.0003, "weight_decay": 1e-06, "alpha": 0.05, "device": "cuda:2", "seed": 1234, "num_validation_episodes": 10000, "validation_batch_size": 64, "generalization_penalty_weight": 1.0, "pool_version": "v0"}, "free_loss": {"f1_steps": 0, "total_train_steps": 15630, "f2_steps": 0, "f3_enabled": false}}, "loss_ir": {"name": "Adaptive_Log_Sigmoid_Loss_with_Cost_Margin", "intuition": "Mode: explore. This loss combines the Bradley-Terry model's logistic structure with a dynamic, cost-aware margin. It inherits the use of a margin from both parents, which ensures the log-probability gap reflects the magnitude of the cost difference. From Parent 0, it inherits the idea of using batch-level statistics (z-score) to normalize the cost difference, making the loss robust to cost scaling. The core loss structure is changed from a hinge-like `softplus` to a logistic `logsigmoid`, which is common in preference learning and provides a smooth, bounded loss. As a new coupling idea, this loss introduces an adaptive beta (`beta_adaptive`). This beta is inversely proportional to the standard deviation of the log-probability differences in the batch, which acts as a stability trick. When model preferences are diverse (high std dev), beta is lowered to soften the gradients and prevent instability. When preferences are uniform (low std dev), beta is increased to sharpen the learning signal. A second coupling is the direct addition of a scaled, normalized cost difference as a margin inside the logsigmoid, simplifying the margin calculation compared to the parents.", "hyperparams": {"beta": 1.0, "margin_scale": 0.5, "epsilon": 1e-08}, "operators_used": ["zscore", "logsigmoid"]}}, "better_than_baseline": false}
{"generation": 1, "index": 4, "ir": {"name": "ZScore_Cost_Modulated_LogSigmoid_Loss", "intuition": "Mode: explore. This loss function combines ideas from both parents while introducing a new theoretical basis and coupling. It inherits the Z-score normalization of the cost difference from Parent 0 to make the loss robust to cost scaling. It inherits the general idea of using the cost difference to modulate the learning signal from both parents. The core innovation is to move away from a margin-based hinge/softplus loss and instead use a structure inspired by the Bradley-Terry model, similar to standard preference optimization losses like DPO/IPO. The new coupling idea is to use the normalized cost difference to scale the argument of a `logsigmoid` function. A larger, more significant cost difference between a pair will lead to a steeper gradient and a stronger learning signal, effectively prioritizing learning from 'easy' or high-confidence pairs. A new hyperparameter, `cost_sensitivity`, controls how strongly the cost difference influences the loss, allowing for fine-tuning of this effect. This approach avoids explicit margins, which can be tricky to tune, and grounds the loss in a more standard probabilistic preference framework.", "pseudocode": "1. Calculate the log-probability difference: delta_logp = logp(a) - logp(b).\n2. Calculate the cost difference: delta_cost = cost(b) - cost(a).\n3. Normalize the cost difference using Z-score normalization across the batch: z_cost = (delta_cost - mean(delta_cost)) / (std(delta_cost) + epsilon).\n4. Create a cost-based scaling factor by applying a softplus function to the normalized cost, ensuring it's non-negative: cost_scale = softplus(z_cost * cost_sensitivity). This amplifies the learning signal for pairs with a large cost gap.\n5. Compute the final loss using a logistic (logsigmoid) formulation, where the log-probability difference is scaled by the cost factor: loss = -logsigmoid(delta_logp * cost_scale).\n6. Return the mean of this loss across the batch.", "hyperparams": {"cost_sensitivity": 0.5, "epsilon": 1e-08}, "operators_used": ["zscore", "softplus", "logsigmoid"], "implementation_hint": {"expects": ["cost_a", "cost_b", "logp_a", "logp_b"], "returns": "scalar"}, "code": "def generated_loss(batch, model_output, extra):\n    \"\"\"\n    A preference loss based on a logistic model, where the log-probability difference\n    is scaled by the Z-score normalized cost difference.\n    \"\"\"\n    # Read tensors from the batch\n    cost_a = batch['cost_a']\n    cost_b = batch['cost_b']\n    logp_a = batch['log_prob_w']  # log_prob of winner (preferred)\n    logp_b = batch['log_prob_l']  # log_prob of loser (not preferred)\n    weights = batch.get('weight')\n\n    # Read hyperparameters\n    hyperparams = extra['hyperparams']\n    cost_sensitivity = hyperparams.get('cost_sensitivity', 0.5)\n    epsilon = hyperparams.get('epsilon', 1e-8)\n\n    # 1. Calculate log-probability difference\n    # We want to encourage logp_a > logp_b, so delta_logp should be positive.\n    delta_logp = logp_a - logp_b\n\n    # 2. Calculate cost difference\n    # cost_b is higher (worse), cost_a is lower (better), so delta_cost is positive.\n    delta_cost = cost_b - cost_a\n\n    # 3. Normalize the cost difference (Z-score)\n    # This makes the cost scaling robust to the absolute magnitude of costs.\n    if delta_cost.numel() > 1:\n        mean_delta = delta_cost.mean()\n        std_delta = delta_cost.std()\n        z_cost = (delta_cost - mean_delta) / (std_delta + epsilon)\n    else:\n        # Handle batch size of 1 to avoid NaN from std=0\n        z_cost = torch.zeros_like(delta_cost)\n\n    # 4. Create a non-negative cost-based scaling factor\n    # softplus ensures the scaling factor is always positive, so the sign of the gradient\n    # is always determined by delta_logp. `cost_sensitivity` tunes the effect.\n    cost_scale = torch.nn.functional.softplus(z_cost * cost_sensitivity)\n\n    # 5. Compute the final loss using a scaled logsigmoid\n    # This is equivalent to -log(sigmoid(delta_logp * cost_scale)).\n    # A larger, positive (delta_logp * cost_scale) results in a lower loss.\n    loss_per_pair = -torch.nn.functional.logsigmoid(delta_logp * cost_scale)\n\n    # 6. Apply optional weights and compute the mean loss\n    if weights is not None:\n        loss = (loss_per_pair * weights).sum() / weights.sum().clamp(min=epsilon)\n    else:\n        loss = loss_per_pair.mean()\n\n    return loss", "theoretical_basis": "A Bradley-Terry style logistic preference model where the preference probability is a function of `(logp_a - logp_b) * f(cost_a, cost_b)`. The function `f` is a non-negative, batch-normalized measure of the cost gap's significance, making the model more sensitive to pairs with larger cost differences."}, "fitness": {"hf_like_score": 12.091722944641113, "validation_objective": 12.091722944641113, "generalization_penalty": 0.0, "generalization_objectives": {"100": 12.08037644958496}, "train_score_mean": 23.009811630249022, "train_loss_mean": 1.067847225666046, "pair_count": 31679977, "early_eval": {"enabled": true, "steps": 100, "baseline_validation_objective": 8.412976063537597, "candidate_validation_objective": 12.091722944641113, "early_stopped": true}, "phases": {"f1": {"steps": 15630, "train_score_mean": 23.009811630249022, "train_loss_mean": 1.067847225666046, "pair_count": 31679977}, "f2": {"steps": 0, "train_score_mean": null, "train_loss_mean": null, "pair_count": 0}}, "config": {"hf": {"problem": "tsp", "hf_steps": 0, "hf_epochs": 10, "hf_instances_per_epoch": 100000, "train_problem_size": 100, "valid_problem_sizes": [100], "train_batch_size": 64, "pomo_size": 100, "learning_rate": 0.0003, "weight_decay": 1e-06, "alpha": 0.05, "device": "cuda:3", "seed": 1234, "num_validation_episodes": 10000, "validation_batch_size": 64, "generalization_penalty_weight": 1.0, "pool_version": "v0"}, "free_loss": {"f1_steps": 0, "total_train_steps": 15630, "f2_steps": 0, "f3_enabled": false}}, "loss_ir": {"name": "ZScore_Cost_Modulated_LogSigmoid_Loss", "intuition": "Mode: explore. This loss function combines ideas from both parents while introducing a new theoretical basis and coupling. It inherits the Z-score normalization of the cost difference from Parent 0 to make the loss robust to cost scaling. It inherits the general idea of using the cost difference to modulate the learning signal from both parents. The core innovation is to move away from a margin-based hinge/softplus loss and instead use a structure inspired by the Bradley-Terry model, similar to standard preference optimization losses like DPO/IPO. The new coupling idea is to use the normalized cost difference to scale the argument of a `logsigmoid` function. A larger, more significant cost difference between a pair will lead to a steeper gradient and a stronger learning signal, effectively prioritizing learning from 'easy' or high-confidence pairs. A new hyperparameter, `cost_sensitivity`, controls how strongly the cost difference influences the loss, allowing for fine-tuning of this effect. This approach avoids explicit margins, which can be tricky to tune, and grounds the loss in a more standard probabilistic preference framework.", "hyperparams": {"cost_sensitivity": 0.5, "epsilon": 1e-08}, "operators_used": ["zscore", "softplus", "logsigmoid"]}}, "better_than_baseline": false}
{"generation": 1, "index": 6, "ir": {"name": "Adaptive_Sigmoid_Margin_Loss", "intuition": "Mode: explore. This loss combines the cost-modulated margin idea from both parents while introducing a new coupling mechanism for adaptive scaling. It inherits the sigmoid modulation of the cost difference from Parent 1 to create a margin, but instead of a fixed maximum margin (like Parent 0's `margin_max`), it uses a new adaptive scaling factor. This factor is the standard deviation of the log-probability differences within the batch, clipped for stability. This coupling makes the margin's scale automatically adapt to the current model's confidence range (i.e., the variance of `logp_a - logp_b`), preventing the margin from becoming trivially small or excessively large as training progresses. The loss term remains a softplus hinge loss, `softplus(margin - delta_logp)`, consistent with both parents.", "pseudocode": "1. Calculate the log-probability difference: delta_logp = logp(a) - logp(b).\n2. Calculate the cost difference: delta_cost = cost(b) - cost(a).\n3. Compute an adaptive margin scale based on the batch statistics of log-probability differences: scale = clip(std(delta_logp), min_scale, max_scale).\n4. Create a cost-dependent weight by applying a sigmoid to the cost difference: cost_weight = sigmoid(delta_cost / temperature).\n5. Compute the final dynamic margin by multiplying the adaptive scale and the cost weight: margin = scale * cost_weight.\n6. Calculate the loss for each pair using a softplus function on the margin minus the log-probability difference: loss = softplus(margin - delta_logp).\n7. Return the mean loss across the batch.", "hyperparams": {"temperature": 1.0, "min_scale": 0.1, "max_scale": 5.0, "epsilon": 1e-08}, "operators_used": ["sigmoid", "softplus", "clamp"], "implementation_hint": {"expects": ["cost_a", "cost_b", "logp_a", "logp_b"], "returns": "scalar"}, "code": "def generated_loss(batch, model_output, extra):\n    \"\"\"\n    A preference loss with a margin that is adaptively scaled by the standard deviation\n    of log-probability differences and modulated by the sigmoid of the cost difference.\n    \"\"\"\n    cost_a = batch['cost_a']\n    cost_b = batch['cost_b']\n    logp_a = batch['log_prob_w']\n    logp_b = batch['log_prob_l']\n    weights = batch.get('weight')\n\n    hyperparams = extra['hyperparams']\n    temperature = hyperparams.get('temperature', 1.0)\n    min_scale = hyperparams.get('min_scale', 0.1)\n    max_scale = hyperparams.get('max_scale', 5.0)\n    epsilon = hyperparams.get('epsilon', 1e-8)\n\n    # 1. Calculate log-probability and cost differences\n    delta_logp = logp_a - logp_b\n    delta_cost = cost_b - cost_a\n\n    # 2. Compute adaptive margin scale from logp stats\n    if delta_logp.numel() > 1:\n        # Detach to prevent gradients from flowing through the scale, making it a statistic\n        logp_std = delta_logp.detach().std()\n    else:\n        logp_std = torch.tensor(1.0, device=delta_logp.device)\n    \n    # Clip the scale for stability\n    adaptive_scale = torch.clamp(logp_std, min_scale, max_scale)\n\n    # 3. Create cost-dependent weight via sigmoid (from Parent 1)\n    cost_weight = torch.sigmoid(delta_cost / temperature)\n\n    # 4. Compute the final dynamic margin\n    margin = adaptive_scale * cost_weight\n\n    # 5. Calculate the loss per pair (from both parents)\n    # We want delta_logp > margin, so penalize margin - delta_logp > 0.\n    loss_per_pair = torch.nn.functional.softplus(margin - delta_logp)\n\n    # 6. Apply optional weights and compute the mean loss\n    if weights is not None:\n        loss = (loss_per_pair * weights).sum() / weights.sum().clamp(min=epsilon)\n    else:\n        loss = loss_per_pair.mean()\n\n    return loss", "theoretical_basis": "A margin-based classification loss on log-probabilities, where the margin is dynamically coupled with both the cost difference and the batch-wise variance of the model's own preference scores. This preserves the Bradley-Terry-style monotonicity while adapting the learning signal strength to the model's current state."}, "fitness": {"hf_like_score": 9.021516635131835, "validation_objective": 9.021516635131835, "generalization_penalty": 0.0, "generalization_objectives": {"100": 9.017612573242188}, "train_score_mean": 17.66859417915344, "train_loss_mean": 2.6590724658966063, "pair_count": 31679970, "early_eval": {"enabled": true, "steps": 100, "baseline_validation_objective": 8.412976063537597, "candidate_validation_objective": 9.021516635131835, "early_stopped": true}, "phases": {"f1": {"steps": 15630, "train_score_mean": 17.66859417915344, "train_loss_mean": 2.6590724658966063, "pair_count": 31679970}, "f2": {"steps": 0, "train_score_mean": null, "train_loss_mean": null, "pair_count": 0}}, "config": {"hf": {"problem": "tsp", "hf_steps": 0, "hf_epochs": 10, "hf_instances_per_epoch": 100000, "train_problem_size": 100, "valid_problem_sizes": [100], "train_batch_size": 64, "pomo_size": 100, "learning_rate": 0.0003, "weight_decay": 1e-06, "alpha": 0.05, "device": "cuda:0", "seed": 1234, "num_validation_episodes": 10000, "validation_batch_size": 64, "generalization_penalty_weight": 1.0, "pool_version": "v0"}, "free_loss": {"f1_steps": 0, "total_train_steps": 15630, "f2_steps": 0, "f3_enabled": false}}, "loss_ir": {"name": "Adaptive_Sigmoid_Margin_Loss", "intuition": "Mode: explore. This loss combines the cost-modulated margin idea from both parents while introducing a new coupling mechanism for adaptive scaling. It inherits the sigmoid modulation of the cost difference from Parent 1 to create a margin, but instead of a fixed maximum margin (like Parent 0's `margin_max`), it uses a new adaptive scaling factor. This factor is the standard deviation of the log-probability differences within the batch, clipped for stability. This coupling makes the margin's scale automatically adapt to the current model's confidence range (i.e., the variance of `logp_a - logp_b`), preventing the margin from becoming trivially small or excessively large as training progresses. The loss term remains a softplus hinge loss, `softplus(margin - delta_logp)`, consistent with both parents.", "hyperparams": {"temperature": 1.0, "min_scale": 0.1, "max_scale": 5.0, "epsilon": 1e-08}, "operators_used": ["sigmoid", "softplus", "clamp"]}}, "better_than_baseline": false}
{"generation": 1, "index": 7, "ir": {"name": "Adaptive_Sigmoid_Margin_Loss", "intuition": "Mode: combine\nThis loss function combines the core ideas of its parents to create a more robust, adaptive margin-based loss. It inherits the use of a dynamic margin from both parents, which is modulated by the difference in costs between the preferred and non-preferred candidates. \n\nFrom Parent 0 (`Sigmoid_Cost_Modulated_Margin_Loss` with z-score), it inherits the idea of normalizing the cost difference to make the margin's scale independent of the raw cost values. However, instead of a simple z-score, it uses a more robust normalization by dividing by the standard deviation of the log-probabilities, coupling the cost signal to the model's current confidence distribution.\n\nFrom Parent 1 (`Sigmoid_Cost_Modulated_Margin_Loss` with direct sigmoid), it inherits the direct use of a sigmoid function to map the cost difference into a smooth, bounded weight between 0 and 1, controlled by a temperature parameter.\n\nAs a new coupling idea, this child loss introduces two modifications: \n1. **Adaptive Normalization:** The cost difference is normalized by the standard deviation of the log-probability differences (`delta_logp`) across the batch. This adaptively scales the margin based on the model's current output variance. When the model is uncertain (high variance in `delta_logp`), the effective margin is smaller, allowing for more exploration. When the model is confident (low variance), the margin becomes more influential, encouraging finer-grained distinctions. This couples the cost-based margin to the model's own output distribution.\n2. **Log-Sigmoid Formulation:** Instead of the `softplus(margin - delta_logp)` formulation, this loss uses the Bradley-Terry-like `logsigmoid(delta_logp - margin)`. This is a more standard and often more stable way to frame a logistic preference loss, directly maximizing the log-probability of the preference `delta_logp` exceeding the dynamic `margin`.", "pseudocode": "1. Calculate the log-probability difference: delta_logp = logp(a) - logp(b).\n2. Calculate the cost difference: delta_cost = cost(b) - cost(a).\n3. Calculate the standard deviation of the log-probability differences across the batch: std_delta_logp.\n4. Create an adaptively normalized cost difference by dividing delta_cost by (std_delta_logp + epsilon). This scales the cost gap relative to the model's output variance.\n5. Compute a dynamic margin by applying a sigmoid function to the normalized cost difference, scaled by temperature, and then multiplying by a maximum margin hyperparameter: margin = margin_max * sigmoid(normalized_delta_cost * temperature).\n6. Calculate the loss for each pair as the negative log-sigmoid of the log-probability difference minus the dynamic margin: loss = -logsigmoid(delta_logp - margin).\n7. Return the mean of the loss across the batch.", "hyperparams": {"margin_max": 0.5, "temperature": 1.0, "epsilon": 1e-08}, "operators_used": ["sigmoid", "logsigmoid"], "implementation_hint": {"expects": ["cost_a", "cost_b", "logp_a", "logp_b"], "returns": "scalar"}, "code": "def generated_loss(batch, model_output, extra):\n    \"\"\"\n    An adaptive margin loss combining ideas from both parents.\n    The margin is derived from the cost difference, but normalized by the standard\n    deviation of the log-probability differences, coupling the margin to the model's\n    output distribution. The loss uses a stable logsigmoid formulation.\n    \"\"\"\n    cost_a = batch['cost_a']\n    cost_b = batch['cost_b']\n    logp_a = batch['log_prob_w']\n    logp_b = batch['log_prob_l']\n    weights = batch.get('weight')\n\n    # Hyperparameters\n    hyperparams = extra.get('hyperparams', {})\n    margin_max = hyperparams.get('margin_max', 0.5)\n    temperature = hyperparams.get('temperature', 1.0)\n    epsilon = hyperparams.get('epsilon', 1e-8)\n\n    # 1. Calculate log-probability and cost differences\n    delta_logp = logp_a - logp_b\n    delta_cost = cost_b - cost_a\n\n    # 2. Adaptive Normalization (New Coupling Idea)\n    # Normalize the cost difference by the standard deviation of logp differences.\n    # This makes the margin's scale sensitive to the model's current output variance.\n    if delta_logp.numel() > 1:\n        # Use detach() to prevent gradients from flowing through the normalization factor,\n        # which can be unstable. We only want the scale, not to optimize for a certain variance.\n        std_delta_logp = delta_logp.std().detach()\n        # Add a small clamp to prevent division by a very small number\n        norm_factor = (std_delta_logp + epsilon).clamp(min=0.1)\n        normalized_delta_cost = delta_cost / norm_factor\n    else:\n        # Handle batch size of 1\n        normalized_delta_cost = torch.zeros_like(delta_cost)\n\n    # 3. Compute dynamic margin (Inherited from Parents)\n    # Sigmoid maps the normalized cost to (0, 1), creating a weight.\n    # This is scaled by margin_max to set the final margin.\n    margin = margin_max * torch.sigmoid(normalized_delta_cost * temperature)\n\n    # 4. Calculate the loss using a Bradley-Terry like formulation (New Coupling Idea)\n    # We want delta_logp > margin. This is equivalent to delta_logp - margin > 0.\n    # Using -logsigmoid is a standard, stable way to formulate this preference loss.\n    loss_per_pair = -torch.nn.functional.logsigmoid(delta_logp - margin)\n\n    # 5. Apply optional weights and compute the mean loss\n    if weights is not None:\n        loss = (loss_per_pair * weights).sum() / weights.sum().clamp(min=epsilon)\n    else:\n        loss = loss_per_pair.mean()\n\n    return loss", "theoretical_basis": "A margin-based logistic preference model, where the margin is dynamically set based on the cost difference, adaptively normalized by the model's output variance. The loss maximizes the log-likelihood of the log-probability difference exceeding this adaptive margin."}, "fitness": {"hf_like_score": 21.253368423461914, "validation_objective": 21.253368423461914, "generalization_penalty": 0.0, "generalization_objectives": {"100": 21.235056036376953}, "train_score_mean": 25.03425350189209, "train_loss_mean": 1.6140861320495605, "pair_count": 31679988, "early_eval": {"enabled": true, "steps": 100, "baseline_validation_objective": 8.412976063537597, "candidate_validation_objective": 21.253368423461914, "early_stopped": true}, "phases": {"f1": {"steps": 15630, "train_score_mean": 25.03425350189209, "train_loss_mean": 1.6140861320495605, "pair_count": 31679988}, "f2": {"steps": 0, "train_score_mean": null, "train_loss_mean": null, "pair_count": 0}}, "config": {"hf": {"problem": "tsp", "hf_steps": 0, "hf_epochs": 10, "hf_instances_per_epoch": 100000, "train_problem_size": 100, "valid_problem_sizes": [100], "train_batch_size": 64, "pomo_size": 100, "learning_rate": 0.0003, "weight_decay": 1e-06, "alpha": 0.05, "device": "cuda:1", "seed": 1234, "num_validation_episodes": 10000, "validation_batch_size": 64, "generalization_penalty_weight": 1.0, "pool_version": "v0"}, "free_loss": {"f1_steps": 0, "total_train_steps": 15630, "f2_steps": 0, "f3_enabled": false}}, "loss_ir": {"name": "Adaptive_Sigmoid_Margin_Loss", "intuition": "Mode: combine\nThis loss function combines the core ideas of its parents to create a more robust, adaptive margin-based loss. It inherits the use of a dynamic margin from both parents, which is modulated by the difference in costs between the preferred and non-preferred candidates. \n\nFrom Parent 0 (`Sigmoid_Cost_Modulated_Margin_Loss` with z-score), it inherits the idea of normalizing the cost difference to make the margin's scale independent of the raw cost values. However, instead of a simple z-score, it uses a more robust normalization by dividing by the standard deviation of the log-probabilities, coupling the cost signal to the model's current confidence distribution.\n\nFrom Parent 1 (`Sigmoid_Cost_Modulated_Margin_Loss` with direct sigmoid), it inherits the direct use of a sigmoid function to map the cost difference into a smooth, bounded weight between 0 and 1, controlled by a temperature parameter.\n\nAs a new coupling idea, this child loss introduces two modifications: \n1. **Adaptive Normalization:** The cost difference is normalized by the standard deviation of the log-probability differences (`delta_logp`) across the batch. This adaptively scales the margin based on the model's current output variance. When the model is uncertain (high variance in `delta_logp`), the effective margin is smaller, allowing for more exploration. When the model is confident (low variance), the margin becomes more influential, encouraging finer-grained distinctions. This couples the cost-based margin to the model's own output distribution.\n2. **Log-Sigmoid Formulation:** Instead of the `softplus(margin - delta_logp)` formulation, this loss uses the Bradley-Terry-like `logsigmoid(delta_logp - margin)`. This is a more standard and often more stable way to frame a logistic preference loss, directly maximizing the log-probability of the preference `delta_logp` exceeding the dynamic `margin`.", "hyperparams": {"margin_max": 0.5, "temperature": 1.0, "epsilon": 1e-08}, "operators_used": ["sigmoid", "logsigmoid"]}}, "better_than_baseline": false}
{"generation": 2, "index": 0, "ir": {"name": "Sigmoid_Margin_Log_Loss_with_Adaptive_Scale_and_Temperature", "intuition": "Mode: explore. This loss combines the adaptive scaling of Parent 1 with the logistic loss structure of Parent 0. It inherits the `logsigmoid` loss from Parent 0, providing a smooth, probabilistic interpretation. From Parent 1, it inherits the idea of an adaptive margin scale derived from the standard deviation of log-probability differences (`delta_logp`), making the loss sensitive to the model's current confidence. The margin itself is a sigmoid function of the cost difference, also from Parent 1, ensuring it is bounded and reflects the cost gap. As a first new coupling idea, the temperature of this cost-sigmoid is made adaptive: it is set to the standard deviation of the cost differences in the batch. This allows the margin to be more or less sensitive to cost gaps depending on the diversity of costs in the current batch. A second new coupling is the introduction of a `beta` scaling factor inside the `logsigmoid`, which is tied to the inverse of the adaptive margin scale. This creates a push-pull dynamic: when the model is uncertain (high `logp_std`, large margin scale), `beta` is reduced, softening the loss and preventing overly aggressive updates. When the model is confident (low `logp_std`, small margin scale), `beta` is increased, sharpening the gradients.", "pseudocode": "1. Calculate the difference in log-probabilities: delta_logp = logp(a) - logp(b).\n2. Calculate the difference in costs: delta_cost = cost(b) - cost(a).\n3. Compute an adaptive margin scale based on the standard deviation of delta_logp, clipped for stability: adaptive_scale = clip(std(delta_logp), min_scale, max_scale).\n4. Compute an adaptive temperature based on the standard deviation of delta_cost: adaptive_temp = std(delta_cost) + epsilon.\n5. Calculate a cost-dependent margin by applying a sigmoid to the cost difference, using the adaptive temperature: cost_margin = sigmoid(delta_cost / adaptive_temp).\n6. Compute the final dynamic margin by multiplying the adaptive scale and the cost-margin: margin = adaptive_scale * cost_margin.\n7. Calculate an adaptive beta as the inverse of the adaptive scale, with a fixed base scaling factor: beta = beta_base / (adaptive_scale + epsilon).\n8. Compute the loss argument: loss_arg = delta_logp - margin.\n9. Calculate the final loss using a scaled negative log-sigmoid function: loss = -logsigmoid(beta * loss_arg).\n10. Return the mean of the loss across the batch.", "hyperparams": {"beta_base": 1.0, "min_scale": 0.1, "max_scale": 5.0, "epsilon": 1e-08}, "operators_used": ["sigmoid", "logsigmoid", "clamp"], "implementation_hint": {"expects": ["cost_a", "cost_b", "logp_a", "logp_b"], "returns": "scalar"}, "code": "def generated_loss(batch, model_output, extra):\n    \"\"\"\n    A logistic preference loss where the margin is adaptively scaled by logp variance\n    and cost variance, and the loss temperature is inversely scaled with the margin.\n    \"\"\"\n    cost_a = batch['cost_a']\n    cost_b = batch['cost_b']\n    logp_a = batch['log_prob_w']\n    logp_b = batch['log_prob_l']\n    weights = batch.get('weight')\n\n    hyperparams = extra['hyperparams']\n    beta_base = hyperparams.get('beta_base', 1.0)\n    min_scale = hyperparams.get('min_scale', 0.1)\n    max_scale = hyperparams.get('max_scale', 5.0)\n    epsilon = hyperparams.get('epsilon', 1e-8)\n\n    # 1. Calculate log-probability and cost differences\n    delta_logp = logp_a - logp_b\n    delta_cost = cost_b - cost_a\n\n    # 2. Compute adaptive margin scale from logp stats (from Parent 1)\n    if delta_logp.numel() > 1:\n        logp_std = delta_logp.detach().std()\n    else:\n        logp_std = torch.tensor(1.0, device=delta_logp.device)\n    adaptive_scale = torch.clamp(logp_std, min_scale, max_scale)\n\n    # 3. New Coupling 1: Adaptive temperature from cost stats\n    if delta_cost.numel() > 1:\n        adaptive_temp = delta_cost.detach().std() + epsilon\n    else:\n        adaptive_temp = torch.tensor(1.0, device=delta_cost.device)\n\n    # 4. Cost-dependent margin using sigmoid (from Parent 1) with new adaptive temp\n    cost_margin = torch.sigmoid(delta_cost / adaptive_temp)\n\n    # 5. Compute the final dynamic margin\n    margin = adaptive_scale * cost_margin\n\n    # 6. New Coupling 2: Beta inversely coupled to adaptive scale\n    beta = beta_base / (adaptive_scale.detach() + epsilon)\n\n    # 7. Compute loss argument for logistic loss (hinge-like `delta_logp - margin`)\n    loss_arg = delta_logp - margin\n\n    # 8. Apply negative log-sigmoid (from Parent 0)\n    # We want delta_logp > margin, so we penalize when delta_logp - margin is small or negative.\n    # logsigmoid(x) is large for large x, so we use logsigmoid(beta * loss_arg)\n    loss_per_pair = -torch.nn.functional.logsigmoid(beta * loss_arg)\n\n    # 9. Apply optional weights and compute the mean loss\n    if weights is not None:\n        loss = (loss_per_pair * weights).sum() / weights.sum().clamp(min=epsilon)\n    else:\n        loss = loss_per_pair.mean()\n\n    return loss", "theoretical_basis": "A Bradley-Terry logistic preference model where the target log-odds are a dynamic, non-zero margin. This margin is coupled to both the batch-wise variance of model preferences (log-probabilities) and the batch-wise variance of costs. The logistic loss's temperature (beta) is inversely coupled to the margin's scale, creating a self-stabilizing mechanism that adapts the gradient strength to the model's confidence."}, "fitness": {"hf_like_score": 7.873306460571289, "validation_objective": 7.871440267944336, "generalization_penalty": 0.0018661926269532003, "generalization_objectives": {"100": 7.873306460571289}, "train_score_mean": 8.557055223773705, "train_loss_mean": 0.5266474782810406, "pair_count": 4951579224, "early_eval": {"enabled": true, "steps": 100, "baseline_validation_objective": 8.412976063537597, "candidate_validation_objective": 8.334650708007812, "early_stopped": false}, "phases": {"f1": {"steps": 15630, "train_score_mean": 8.557055223773705, "train_loss_mean": 0.5266474782810406, "pair_count": 4951579224}, "f2": {"steps": 0, "train_score_mean": null, "train_loss_mean": null, "pair_count": 0}}, "config": {"hf": {"problem": "tsp", "hf_steps": 0, "hf_epochs": 10, "hf_instances_per_epoch": 100000, "train_problem_size": 100, "valid_problem_sizes": [100], "train_batch_size": 64, "pomo_size": 100, "learning_rate": 0.0003, "weight_decay": 1e-06, "alpha": 0.05, "device": "cuda:0", "seed": 1234, "num_validation_episodes": 10000, "validation_batch_size": 64, "generalization_penalty_weight": 1.0, "pool_version": "v0"}, "free_loss": {"f1_steps": 0, "total_train_steps": 15630, "f2_steps": 0, "f3_enabled": false}}, "loss_ir": {"name": "Sigmoid_Margin_Log_Loss_with_Adaptive_Scale_and_Temperature", "intuition": "Mode: explore. This loss combines the adaptive scaling of Parent 1 with the logistic loss structure of Parent 0. It inherits the `logsigmoid` loss from Parent 0, providing a smooth, probabilistic interpretation. From Parent 1, it inherits the idea of an adaptive margin scale derived from the standard deviation of log-probability differences (`delta_logp`), making the loss sensitive to the model's current confidence. The margin itself is a sigmoid function of the cost difference, also from Parent 1, ensuring it is bounded and reflects the cost gap. As a first new coupling idea, the temperature of this cost-sigmoid is made adaptive: it is set to the standard deviation of the cost differences in the batch. This allows the margin to be more or less sensitive to cost gaps depending on the diversity of costs in the current batch. A second new coupling is the introduction of a `beta` scaling factor inside the `logsigmoid`, which is tied to the inverse of the adaptive margin scale. This creates a push-pull dynamic: when the model is uncertain (high `logp_std`, large margin scale), `beta` is reduced, softening the loss and preventing overly aggressive updates. When the model is confident (low `logp_std`, small margin scale), `beta` is increased, sharpening the gradients.", "hyperparams": {"beta_base": 1.0, "min_scale": 0.1, "max_scale": 5.0, "epsilon": 1e-08}, "operators_used": ["sigmoid", "logsigmoid", "clamp"]}}, "better_than_baseline": true}
{"generation": 2, "index": 1, "ir": {"name": "LogSigmoid_Loss_with_Adaptive_Sigmoid_Margin", "intuition": "Mode: combine. This loss function combines the strongest elements from both parents to create a robust, adaptive preference learning objective. It inherits the core `logsigmoid` loss structure from Parent 0, which is a standard and effective choice for binary preference tasks, offering a smooth, probabilistic interpretation. From Parent 1, which has a very high `hf_like_score`, it inherits the sophisticated adaptive margin mechanism. This margin (`adaptive_scale * cost_weight`) is powerful because it couples the margin's magnitude to both the model's current confidence (via `std(delta_logp)`) and the actual cost difference (via `sigmoid(delta_cost)`). The child loss therefore aims to satisfy `delta_logp > margin`, but framed within a logistic loss rather than a hinge loss (`softplus`). As a new coupling idea, this child introduces an adaptive beta (temperature) for the `logsigmoid` function. This beta is set to the reciprocal of the `adaptive_scale` from Parent 1's margin calculation. This creates a self-balancing system: when the model is uncertain and `adaptive_scale` is large, beta becomes small, softening the logistic curve and preventing overly aggressive updates. Conversely, when the model is confident and the scale is small, beta increases, sharpening the loss and encouraging finer-grained preference distinctions. This dual use of `adaptive_scale` for both the margin and the loss temperature is the core innovation.", "pseudocode": "1. Calculate the difference in log-probabilities: delta_logp = logp(a) - logp(b).\n2. Calculate the difference in costs: delta_cost = cost(b) - cost(a).\n3. Compute an adaptive scale based on the standard deviation of log-probability differences in the batch, clipped for stability. This is inherited from Parent 1.\n4. Create a cost-dependent weight by applying a sigmoid function to the cost difference, scaled by a temperature hyperparameter. This is also inherited from Parent 1.\n5. Calculate the dynamic margin by multiplying the adaptive scale and the cost weight: margin = adaptive_scale * cost_weight.\n6. Introduce a new coupling: define an adaptive beta (temperature) for the logistic loss as the inverse of the adaptive scale, `beta_adaptive = 1.0 / adaptive_scale`. This stabilizes the loss by softening it when the required margin is large.\n7. Compute the argument for the loss function: `loss_arg = delta_logp - margin`. The goal is to make this positive.\n8. Apply the negative log-sigmoid function, scaled by the adaptive beta: `loss = -logsigmoid(beta_adaptive * loss_arg)`.\n9. Return the mean of the loss across the batch.", "hyperparams": {"temperature": 1.0, "min_scale": 0.1, "max_scale": 5.0, "epsilon": 1e-08}, "operators_used": ["logsigmoid", "sigmoid", "clamp"], "implementation_hint": {"expects": ["cost_a", "cost_b", "logp_a", "logp_b"], "returns": "scalar"}, "code": "def generated_loss(batch, model_output, extra):\n    \"\"\"\n    A logistic preference loss where the margin is adaptively scaled by logp variance\n    and cost, and the logistic temperature is inversely proportional to this scale.\n    \"\"\"\n    # Read tensors from the batch\n    cost_a = batch['cost_a']\n    cost_b = batch['cost_b']\n    logp_a = batch['log_prob_w']\n    logp_b = batch['log_prob_l']\n    weights = batch.get('weight')\n\n    # Read hyperparameters\n    hyperparams = extra['hyperparams']\n    temperature = hyperparams.get('temperature', 1.0)\n    min_scale = hyperparams.get('min_scale', 0.1)\n    max_scale = hyperparams.get('max_scale', 5.0)\n    epsilon = hyperparams.get('epsilon', 1e-8)\n\n    # Step 1: Calculate log-probability and cost differences\n    delta_logp = logp_a - logp_b\n    delta_cost = cost_b - cost_a\n\n    # Step 2: Compute adaptive scale (inherited from Parent 1)\n    if delta_logp.numel() > 1:\n        logp_std = delta_logp.detach().std()\n    else:\n        logp_std = torch.tensor(1.0, device=delta_logp.device)\n    adaptive_scale = torch.clamp(logp_std, min_scale, max_scale)\n\n    # Step 3: Create cost-dependent weight (inherited from Parent 1)\n    cost_weight = torch.sigmoid(delta_cost / temperature)\n\n    # Step 4: Compute the dynamic margin (inherited from Parent 1)\n    margin = adaptive_scale * cost_weight\n\n    # Step 5: New Coupling - Adaptive beta for logsigmoid\n    # Inversely couple beta to the adaptive scale for stability.\n    beta_adaptive = 1.0 / (adaptive_scale.detach() + epsilon)\n\n    # Step 6: Compute the loss argument\n    # The loss pushes delta_logp to be greater than the margin.\n    loss_arg = delta_logp - margin\n\n    # Step 7: Apply negative log-sigmoid loss (inherited from Parent 0)\n    loss_per_pair = -torch.nn.functional.logsigmoid(beta_adaptive * loss_arg)\n\n    # Step 8: Apply optional weights and compute the mean loss\n    if weights is not None:\n        loss = (loss_per_pair * weights).sum() / weights.sum().clamp(min=epsilon)\n    else:\n        loss = loss_per_pair.mean()\n\n    return loss", "theoretical_basis": "A Bradley-Terry logistic preference model where the target log-odds of preferring 'a' over 'b' is not zero, but a dynamic, cost-sensitive margin. The margin's magnitude is coupled with the model's output variance. The steepness (temperature) of the logistic loss is inversely coupled to this margin, creating a self-regulating system that adapts the learning signal's strength based on both model confidence and preference difficulty."}, "fitness": {"hf_like_score": 7.87194326171875, "validation_objective": 7.871516598510742, "generalization_penalty": 0.00042666320800766044, "generalization_objectives": {"100": 7.87194326171875}, "train_score_mean": 8.62938151893445, "train_loss_mean": 0.48043474490758475, "pair_count": 4951579322, "early_eval": {"enabled": true, "steps": 100, "baseline_validation_objective": 8.412976063537597, "candidate_validation_objective": 8.317042326354981, "early_stopped": false}, "phases": {"f1": {"steps": 15630, "train_score_mean": 8.62938151893445, "train_loss_mean": 0.48043474490758475, "pair_count": 4951579322}, "f2": {"steps": 0, "train_score_mean": null, "train_loss_mean": null, "pair_count": 0}}, "config": {"hf": {"problem": "tsp", "hf_steps": 0, "hf_epochs": 10, "hf_instances_per_epoch": 100000, "train_problem_size": 100, "valid_problem_sizes": [100], "train_batch_size": 64, "pomo_size": 100, "learning_rate": 0.0003, "weight_decay": 1e-06, "alpha": 0.05, "device": "cuda:1", "seed": 1234, "num_validation_episodes": 10000, "validation_batch_size": 64, "generalization_penalty_weight": 1.0, "pool_version": "v0"}, "free_loss": {"f1_steps": 0, "total_train_steps": 15630, "f2_steps": 0, "f3_enabled": false}}, "loss_ir": {"name": "LogSigmoid_Loss_with_Adaptive_Sigmoid_Margin", "intuition": "Mode: combine. This loss function combines the strongest elements from both parents to create a robust, adaptive preference learning objective. It inherits the core `logsigmoid` loss structure from Parent 0, which is a standard and effective choice for binary preference tasks, offering a smooth, probabilistic interpretation. From Parent 1, which has a very high `hf_like_score`, it inherits the sophisticated adaptive margin mechanism. This margin (`adaptive_scale * cost_weight`) is powerful because it couples the margin's magnitude to both the model's current confidence (via `std(delta_logp)`) and the actual cost difference (via `sigmoid(delta_cost)`). The child loss therefore aims to satisfy `delta_logp > margin`, but framed within a logistic loss rather than a hinge loss (`softplus`). As a new coupling idea, this child introduces an adaptive beta (temperature) for the `logsigmoid` function. This beta is set to the reciprocal of the `adaptive_scale` from Parent 1's margin calculation. This creates a self-balancing system: when the model is uncertain and `adaptive_scale` is large, beta becomes small, softening the logistic curve and preventing overly aggressive updates. Conversely, when the model is confident and the scale is small, beta increases, sharpening the loss and encouraging finer-grained preference distinctions. This dual use of `adaptive_scale` for both the margin and the loss temperature is the core innovation.", "hyperparams": {"temperature": 1.0, "min_scale": 0.1, "max_scale": 5.0, "epsilon": 1e-08}, "operators_used": ["logsigmoid", "sigmoid", "clamp"]}}, "better_than_baseline": true}
{"generation": 2, "index": 2, "ir": {"name": "Sigmoid_Log_Ratio_Loss_with_Adaptive_Margin", "intuition": "Mode: explore. This loss function combines a Bradley-Terry logistic framework with a novel adaptive margin, aiming to improve upon the parents' stability and performance. It inherits the `logsigmoid` loss structure from Parent 0, which provides a smooth, probabilistic interpretation. From Parent 1, it inherits the idea of an adaptive margin scale based on the standard deviation of `delta_logp`, which allows the loss to adjust to the model's confidence. The first new coupling idea is a change in the core loss argument: instead of `delta_logp + margin`, it uses `delta_logp - margin`. This reframes the loss to penalize `delta_logp` for being *smaller* than a target margin, which is a more standard formulation for margin losses. The second, more significant new coupling is the formulation of the margin itself. It's computed as `adaptive_scale * sigmoid(z_cost)`. This combines the adaptive scaling from Parent 1 with the robust `zscore` normalization of cost differences from Parent 0. Using `sigmoid` on the z-scored cost creates a bounded, normalized weight between 0 and 1, making the margin less sensitive to cost outliers than a linear scaling, while still being proportional to the relative cost difference within the batch.", "pseudocode": "1. Calculate the difference in log-probabilities: delta_logp = logp(a) - logp(b).\n2. Calculate the difference in costs: delta_cost = cost(b) - cost(a).\n3. Normalize the cost difference using Z-score normalization across the batch: z_cost = (delta_cost - mean(delta_cost)) / (std(delta_cost) + epsilon).\n4. Compute an adaptive margin scale based on the batch's log-probability difference standard deviation, clipped for stability: adaptive_scale = clip(std(delta_logp), min_scale, max_scale).\n5. Compute a cost-based margin weight by applying a sigmoid function to the normalized cost difference: cost_weight = sigmoid(z_cost).\n6. Calculate the final margin by multiplying the adaptive scale and the cost weight: margin = adaptive_scale * cost_weight.\n7. Compute the loss argument: loss_arg = beta * (delta_logp - margin). The goal is for delta_logp to be greater than the margin.\n8. Apply the negative log-sigmoid function to the argument: loss = -logsigmoid(loss_arg).\n9. Return the mean of the loss across the batch.", "hyperparams": {"beta": 1.0, "min_scale": 0.05, "max_scale": 2.0, "epsilon": 1e-08}, "operators_used": ["logsigmoid", "zscore", "sigmoid", "clamp"], "implementation_hint": {"expects": ["cost_a", "cost_b", "logp_a", "logp_b"], "returns": "scalar"}, "code": "def generated_loss(batch, model_output, extra):\n    \"\"\"\n    A preference loss combining a logsigmoid framework with a dynamic margin.\n    The margin is scaled by the logp standard deviation and modulated by the\n    sigmoid of the z-scored cost difference.\n    \"\"\"\n    # Read tensors from the batch\n    cost_a = batch['cost_a']\n    cost_b = batch['cost_b']\n    logp_a = batch['log_prob_w']\n    logp_b = batch['log_prob_l']\n    weights = batch.get('weight')\n\n    # Read hyperparameters\n    hyperparams = extra['hyperparams']\n    beta = hyperparams.get('beta', 1.0)\n    min_scale = hyperparams.get('min_scale', 0.05)\n    max_scale = hyperparams.get('max_scale', 2.0)\n    epsilon = hyperparams.get('epsilon', 1e-8)\n\n    # 1. Calculate log-probability difference\n    delta_logp = logp_a - logp_b\n\n    # 2. Calculate cost difference\n    delta_cost = cost_b - cost_a\n\n    # 3. Normalize cost difference (Z-score) - inherited from Parent 0\n    if delta_cost.numel() > 1:\n        mean_delta_cost = delta_cost.mean()\n        std_delta_cost = delta_cost.std()\n        z_cost = (delta_cost - mean_delta_cost) / (std_delta_cost + epsilon)\n    else:\n        z_cost = torch.zeros_like(delta_cost)\n\n    # 4. Compute adaptive margin scale - inherited from Parent 1\n    if delta_logp.numel() > 1:\n        logp_std = delta_logp.detach().std()\n    else:\n        logp_std = torch.tensor(1.0, device=delta_logp.device)\n    adaptive_scale = torch.clamp(logp_std, min_scale, max_scale)\n\n    # 5. New Coupling: Compute cost-based margin weight from z-scored cost\n    cost_weight = torch.sigmoid(z_cost)\n\n    # 6. Calculate final margin\n    margin = adaptive_scale * cost_weight\n\n    # 7. Compute the loss argument. We want delta_logp > margin.\n    loss_arg = beta * (delta_logp - margin)\n\n    # 8. Apply the negative log-sigmoid function - inherited from Parent 0\n    loss_per_pair = -torch.nn.functional.logsigmoid(loss_arg)\n\n    # 9. Apply optional weights and compute the mean loss\n    if weights is not None:\n        loss = (loss_per_pair * weights).sum() / weights.sum().clamp(min=epsilon)\n    else:\n        loss = loss_per_pair.mean()\n\n    return loss", "theoretical_basis": "A Bradley-Terry logistic preference model with a dynamic, cost-aware target margin. The model learns to make the log-probability difference exceed a margin that is adaptively scaled by the model's output variance and modulated by the sigmoid of the z-scored cost difference, providing a robust and self-tuning learning signal."}, "fitness": {"hf_like_score": 17.344595098876955, "validation_objective": 17.344595098876955, "generalization_penalty": 0.0, "generalization_objectives": {"100": 17.31822572631836}, "train_score_mean": 24.374080238342284, "train_loss_mean": 1.881919412612915, "pair_count": 31679983, "early_eval": {"enabled": true, "steps": 100, "baseline_validation_objective": 8.412976063537597, "candidate_validation_objective": 17.344595098876955, "early_stopped": true}, "phases": {"f1": {"steps": 15630, "train_score_mean": 24.374080238342284, "train_loss_mean": 1.881919412612915, "pair_count": 31679983}, "f2": {"steps": 0, "train_score_mean": null, "train_loss_mean": null, "pair_count": 0}}, "config": {"hf": {"problem": "tsp", "hf_steps": 0, "hf_epochs": 10, "hf_instances_per_epoch": 100000, "train_problem_size": 100, "valid_problem_sizes": [100], "train_batch_size": 64, "pomo_size": 100, "learning_rate": 0.0003, "weight_decay": 1e-06, "alpha": 0.05, "device": "cuda:2", "seed": 1234, "num_validation_episodes": 10000, "validation_batch_size": 64, "generalization_penalty_weight": 1.0, "pool_version": "v0"}, "free_loss": {"f1_steps": 0, "total_train_steps": 15630, "f2_steps": 0, "f3_enabled": false}}, "loss_ir": {"name": "Sigmoid_Log_Ratio_Loss_with_Adaptive_Margin", "intuition": "Mode: explore. This loss function combines a Bradley-Terry logistic framework with a novel adaptive margin, aiming to improve upon the parents' stability and performance. It inherits the `logsigmoid` loss structure from Parent 0, which provides a smooth, probabilistic interpretation. From Parent 1, it inherits the idea of an adaptive margin scale based on the standard deviation of `delta_logp`, which allows the loss to adjust to the model's confidence. The first new coupling idea is a change in the core loss argument: instead of `delta_logp + margin`, it uses `delta_logp - margin`. This reframes the loss to penalize `delta_logp` for being *smaller* than a target margin, which is a more standard formulation for margin losses. The second, more significant new coupling is the formulation of the margin itself. It's computed as `adaptive_scale * sigmoid(z_cost)`. This combines the adaptive scaling from Parent 1 with the robust `zscore` normalization of cost differences from Parent 0. Using `sigmoid` on the z-scored cost creates a bounded, normalized weight between 0 and 1, making the margin less sensitive to cost outliers than a linear scaling, while still being proportional to the relative cost difference within the batch.", "hyperparams": {"beta": 1.0, "min_scale": 0.05, "max_scale": 2.0, "epsilon": 1e-08}, "operators_used": ["logsigmoid", "zscore", "sigmoid", "clamp"]}}, "better_than_baseline": false}
{"generation": 2, "index": 4, "ir": {"name": "Hybrid_Sigmoid_LogSigmoid_Loss", "intuition": "Mode: explore. This loss function hybridizes the strongest components of its parents while introducing a new coupling for stability. It inherits the core `logsigmoid` structure from Parent 0, which is well-suited for probabilistic preference modeling. From Parent 1, which demonstrated superior performance, it inherits the use of a margin that is adaptively scaled by the standard deviation of the log-probability differences. This makes the margin's magnitude responsive to the model's current confidence. The new coupling idea is to combine these two concepts: the `logsigmoid` loss now operates on an argument of `delta_logp - margin`, where this adaptive margin from Parent 1 is used. A second new idea is to apply a `tanh` function to the log-probability differences before calculating their standard deviation. This bounds the input to the standard deviation calculation, preventing outlier log-probability pairs from causing an excessively large (and potentially unstable) adaptive scale, thus acting as a stability trick.", "pseudocode": "1. Calculate the difference in log-probabilities: delta_logp = logp(a) - logp(b).\n2. Calculate the difference in costs: delta_cost = cost(b) - cost(a).\n3. Compute a stabilized version of delta_logp by applying a tanh function. This prevents outliers from dominating the scale calculation.\n4. Calculate an adaptive margin scale based on the standard deviation of the stabilized log-probability differences: scale = clip(std(tanh(delta_logp)), min_scale, max_scale).\n5. Compute a cost-dependent weight using a sigmoid on the cost difference: cost_weight = sigmoid(delta_cost / temperature).\n6. Calculate the final margin by multiplying the adaptive scale and the cost weight: margin = scale * cost_weight.\n7. Compute the loss argument: loss_arg = delta_logp - margin.\n8. Apply the negative log-sigmoid function, scaled by a fixed beta: loss = -logsigmoid(beta * loss_arg).\n9. Return the mean of the loss across the batch.", "hyperparams": {"beta": 1.0, "temperature": 1.0, "min_scale": 0.1, "max_scale": 5.0, "epsilon": 1e-08}, "operators_used": ["logsigmoid", "sigmoid", "tanh", "clamp"], "implementation_hint": {"expects": ["cost_a", "cost_b", "logp_a", "logp_b"], "returns": "scalar"}, "code": "def generated_loss(batch, model_output, extra):\n    \"\"\"\n    A hybrid loss combining a logsigmoid structure with a margin that is adaptively\n    scaled by the stabilized standard deviation of log-probability differences and\n    modulated by the sigmoid of the cost difference.\n    \"\"\"\n    # Read tensors from the batch\n    cost_a = batch['cost_a']\n    cost_b = batch['cost_b']\n    logp_a = batch['log_prob_w']\n    logp_b = batch['log_prob_l']\n    weights = batch.get('weight')\n\n    # Read hyperparameters\n    hyperparams = extra['hyperparams']\n    beta = hyperparams.get('beta', 1.0)\n    temperature = hyperparams.get('temperature', 1.0)\n    min_scale = hyperparams.get('min_scale', 0.1)\n    max_scale = hyperparams.get('max_scale', 5.0)\n    epsilon = hyperparams.get('epsilon', 1e-8)\n\n    # 1. Calculate log-probability and cost differences\n    delta_logp = logp_a - logp_b\n    delta_cost = cost_b - cost_a\n\n    # 2. Compute adaptive margin scale from stabilized logp stats\n    # New Coupling: Use tanh to stabilize std dev calculation against outliers.\n    if delta_logp.numel() > 1:\n        # Detach to prevent gradients from flowing through the scale\n        stabilized_delta_logp = torch.tanh(delta_logp.detach())\n        logp_std = stabilized_delta_logp.std()\n    else:\n        logp_std = torch.tensor(1.0, device=delta_logp.device)\n    \n    # Inherited from Parent 1: Clip the scale for stability\n    adaptive_scale = torch.clamp(logp_std, min_scale, max_scale)\n\n    # 3. Inherited from Parent 1: Create cost-dependent weight via sigmoid\n    cost_weight = torch.sigmoid(delta_cost / temperature)\n\n    # 4. Inherited from Parent 1: Compute the final dynamic margin\n    margin = adaptive_scale * cost_weight\n\n    # 5. Compute the loss argument. We want delta_logp > margin.\n    loss_arg = delta_logp - margin\n\n    # 6. Inherited from Parent 0: Apply the negative log-sigmoid function\n    loss_per_pair = -torch.nn.functional.logsigmoid(beta * loss_arg)\n\n    # 7. Apply optional weights and compute the mean loss\n    if weights is not None:\n        loss = (loss_per_pair * weights).sum() / weights.sum().clamp(min=epsilon)\n    else:\n        loss = loss_per_pair.mean()\n\n    return loss", "theoretical_basis": "A hybrid Bradley-Terry logistic preference model where the target log-odds of preference is not zero but a dynamic, cost-dependent margin. The margin's scale is adaptively coupled with the variance of the model's preference scores, with a tanh stabilizer to prevent instability from outliers."}, "fitness": {"hf_like_score": 19.72122924194336, "validation_objective": 19.72122924194336, "generalization_penalty": 0.0, "generalization_objectives": {"100": 19.701753924560546}, "train_score_mean": 24.621794080734254, "train_loss_mean": 1.7677177488803864, "pair_count": 31679982, "early_eval": {"enabled": true, "steps": 100, "baseline_validation_objective": 8.412976063537597, "candidate_validation_objective": 19.72122924194336, "early_stopped": true}, "phases": {"f1": {"steps": 15630, "train_score_mean": 24.621794080734254, "train_loss_mean": 1.7677177488803864, "pair_count": 31679982}, "f2": {"steps": 0, "train_score_mean": null, "train_loss_mean": null, "pair_count": 0}}, "config": {"hf": {"problem": "tsp", "hf_steps": 0, "hf_epochs": 10, "hf_instances_per_epoch": 100000, "train_problem_size": 100, "valid_problem_sizes": [100], "train_batch_size": 64, "pomo_size": 100, "learning_rate": 0.0003, "weight_decay": 1e-06, "alpha": 0.05, "device": "cuda:3", "seed": 1234, "num_validation_episodes": 10000, "validation_batch_size": 64, "generalization_penalty_weight": 1.0, "pool_version": "v0"}, "free_loss": {"f1_steps": 0, "total_train_steps": 15630, "f2_steps": 0, "f3_enabled": false}}, "loss_ir": {"name": "Hybrid_Sigmoid_LogSigmoid_Loss", "intuition": "Mode: explore. This loss function hybridizes the strongest components of its parents while introducing a new coupling for stability. It inherits the core `logsigmoid` structure from Parent 0, which is well-suited for probabilistic preference modeling. From Parent 1, which demonstrated superior performance, it inherits the use of a margin that is adaptively scaled by the standard deviation of the log-probability differences. This makes the margin's magnitude responsive to the model's current confidence. The new coupling idea is to combine these two concepts: the `logsigmoid` loss now operates on an argument of `delta_logp - margin`, where this adaptive margin from Parent 1 is used. A second new idea is to apply a `tanh` function to the log-probability differences before calculating their standard deviation. This bounds the input to the standard deviation calculation, preventing outlier log-probability pairs from causing an excessively large (and potentially unstable) adaptive scale, thus acting as a stability trick.", "hyperparams": {"beta": 1.0, "temperature": 1.0, "min_scale": 0.1, "max_scale": 5.0, "epsilon": 1e-08}, "operators_used": ["logsigmoid", "sigmoid", "tanh", "clamp"]}}, "better_than_baseline": false}
{"generation": 2, "index": 5, "ir": {"name": "Sigmoid_Log_Ratio_Loss", "intuition": "Mode: explore. This loss function reframes the preference learning problem from a margin-based approach to a ratio-based one, while inheriting key adaptive mechanisms. From Parent 1 (Adaptive_Log_Sigmoid_Loss), it inherits the core logistic loss structure (`logsigmoid`) and the use of a temperature parameter (`beta`). From Parent 2 (Adaptive_Sigmoid_Margin_Loss), it inherits the idea of adaptively scaling the loss based on the standard deviation of the log-probability differences in the batch, which stabilizes training by modulating the learning signal strength. The first new coupling idea is to move away from an additive margin (`delta_logp + margin`) and instead use a multiplicative structure inside the sigmoid: `sigmoid(beta * (delta_logp / clipped_delta_cost))`. This frames the problem as learning to make the ratio of `delta_logp` to `delta_cost` large and positive. The second new coupling is to use `softplus` on the cost difference (`delta_cost`) to ensure it's always positive and non-zero, preventing division by zero and ensuring the learning signal's sign is correct. This `softplus`-transformed cost acts as a dynamic, per-sample normalization for the log-probability difference, encouraging `delta_logp` to be proportional to the cost gap. The final loss is `-log(sigmoid(...))`, which is equivalent to `softplus(-arg)`, providing a smooth, theoretically grounded loss.", "pseudocode": "1. Calculate the difference in log-probabilities: delta_logp = logp(a) - logp(b).\n2. Calculate the difference in costs: delta_cost = cost(b) - cost(a).\n3. Compute a safe, positive version of the cost difference by applying the softplus function: safe_delta_cost = softplus(delta_cost) + epsilon. This prevents division by zero.\n4. Calculate an adaptive temperature (beta) for the loss. It is the base beta hyperparameter multiplied by the standard deviation of the log-probability differences in the batch. This stabilizes training by adapting the gradient magnitude to the model's current confidence variance.\n5. Compute the core argument for the sigmoid function. This is the ratio of the log-probability difference to the safe cost difference, scaled by the adaptive beta: arg = beta_adaptive * (delta_logp / safe_delta_cost).\n6. Calculate the loss for each pair by taking the negative logarithm of the sigmoid of the argument: loss = -log(sigmoid(arg)). This is equivalent to softplus(-arg).\n7. Return the mean of the loss across the batch.", "hyperparams": {"beta": 1.0, "epsilon": 1e-08}, "operators_used": ["softplus", "sigmoid", "log"], "implementation_hint": {"expects": ["cost_a", "cost_b", "logp_a", "logp_b"], "returns": "scalar"}, "code": "def generated_loss(batch, model_output, extra):\n    \"\"\"\n    A preference loss that models the preference probability as a sigmoid of the ratio\n    between log-probability difference and cost difference, with adaptive scaling.\n    \"\"\"\n    # Read tensors from the batch\n    cost_a = batch['cost_a']\n    cost_b = batch['cost_b']\n    logp_a = batch['log_prob_w']\n    logp_b = batch['log_prob_l']\n    weights = batch.get('weight')\n\n    # Read hyperparameters\n    hyperparams = extra['hyperparams']\n    beta = hyperparams.get('beta', 1.0)\n    epsilon = hyperparams.get('epsilon', 1e-8)\n\n    # 1. Calculate log-probability difference\n    delta_logp = logp_a - logp_b\n\n    # 2. Calculate cost difference\n    delta_cost = cost_b - cost_a\n\n    # 3. Create a safe, positive cost difference to use as a denominator\n    # This is a new coupling idea to enable a ratio-based loss.\n    safe_delta_cost = torch.nn.functional.softplus(delta_cost) + epsilon\n\n    # 4. Calculate adaptive beta for stability (inspired by Parent 2's adaptive scale)\n    if delta_logp.numel() > 1:\n        # Detach to not backprop through std dev, treating it as a batch statistic\n        std_delta_logp = delta_logp.std().detach()\n        beta_adaptive = beta * std_delta_logp\n    else:\n        beta_adaptive = beta\n\n    # 5. Compute the ratio-based argument for the sigmoid\n    # This is the core new idea, moving from additive margin to multiplicative ratio.\n    loss_arg = beta_adaptive * (delta_logp / safe_delta_cost)\n\n    # 6. Calculate the loss per pair using a log-sigmoid formulation\n    # This is equivalent to softplus(-loss_arg) but can be more stable.\n    # Inherits the logistic loss structure from Parent 1.\n    # We use -log(sigmoid(x)) instead of -logsigmoid(x) because we want to maximize the arg.\n    # -log(sigmoid(x)) encourages x to be large and positive.\n    loss_per_pair = -torch.log(torch.sigmoid(loss_arg) + epsilon)\n\n    # 7. Apply optional weights and compute the mean loss\n    if weights is not None:\n        loss = (loss_per_pair * weights).sum() / weights.sum().clamp(min=epsilon)\n    else:\n        loss = loss_per_pair.mean()\n\n    return loss", "theoretical_basis": "A probabilistic preference model where the probability of preferring 'a' over 'b' is modeled as a sigmoid function of the ratio of log-probability difference to the cost difference. This can be seen as a variant of the Bradley-Terry model where the preference strength is normalized by the magnitude of the cost gap on a per-sample basis. The logistic scaling factor (beta) is adapted based on batch statistics for stability."}, "fitness": {"hf_like_score": 40.128509704589845, "validation_objective": 40.12432567749023, "generalization_penalty": 0.004184027099611853, "generalization_objectives": {"100": 40.128509704589845}, "train_score_mean": 40.86488009450989, "train_loss_mean": 0.70469392050756, "pair_count": 4951581262, "early_eval": {"enabled": true, "steps": 100, "baseline_validation_objective": 8.412976063537597, "candidate_validation_objective": 8.40212692565918, "early_stopped": false}, "phases": {"f1": {"steps": 15630, "train_score_mean": 40.86488009450989, "train_loss_mean": 0.70469392050756, "pair_count": 4951581262}, "f2": {"steps": 0, "train_score_mean": null, "train_loss_mean": null, "pair_count": 0}}, "config": {"hf": {"problem": "tsp", "hf_steps": 0, "hf_epochs": 10, "hf_instances_per_epoch": 100000, "train_problem_size": 100, "valid_problem_sizes": [100], "train_batch_size": 64, "pomo_size": 100, "learning_rate": 0.0003, "weight_decay": 1e-06, "alpha": 0.05, "device": "cuda:0", "seed": 1234, "num_validation_episodes": 10000, "validation_batch_size": 64, "generalization_penalty_weight": 1.0, "pool_version": "v0"}, "free_loss": {"f1_steps": 0, "total_train_steps": 15630, "f2_steps": 0, "f3_enabled": false}}, "loss_ir": {"name": "Sigmoid_Log_Ratio_Loss", "intuition": "Mode: explore. This loss function reframes the preference learning problem from a margin-based approach to a ratio-based one, while inheriting key adaptive mechanisms. From Parent 1 (Adaptive_Log_Sigmoid_Loss), it inherits the core logistic loss structure (`logsigmoid`) and the use of a temperature parameter (`beta`). From Parent 2 (Adaptive_Sigmoid_Margin_Loss), it inherits the idea of adaptively scaling the loss based on the standard deviation of the log-probability differences in the batch, which stabilizes training by modulating the learning signal strength. The first new coupling idea is to move away from an additive margin (`delta_logp + margin`) and instead use a multiplicative structure inside the sigmoid: `sigmoid(beta * (delta_logp / clipped_delta_cost))`. This frames the problem as learning to make the ratio of `delta_logp` to `delta_cost` large and positive. The second new coupling is to use `softplus` on the cost difference (`delta_cost`) to ensure it's always positive and non-zero, preventing division by zero and ensuring the learning signal's sign is correct. This `softplus`-transformed cost acts as a dynamic, per-sample normalization for the log-probability difference, encouraging `delta_logp` to be proportional to the cost gap. The final loss is `-log(sigmoid(...))`, which is equivalent to `softplus(-arg)`, providing a smooth, theoretically grounded loss.", "hyperparams": {"beta": 1.0, "epsilon": 1e-08}, "operators_used": ["softplus", "sigmoid", "log"]}}, "better_than_baseline": false}
{"generation": 2, "index": 6, "ir": {"name": "Sigmoid_Margin_LogSigmoid_Loss_with_Adaptive_Beta", "intuition": "Mode: explore. This loss function combines the strongest elements of both parents while introducing a new stability mechanism. From Parent 1 (Adaptive_Sigmoid_Margin_Loss), which has a strong hf_like_score, it inherits the idea of a dynamic margin that is a product of an adaptive scale (based on the standard deviation of logp differences) and a sigmoid-transformed cost difference. This allows the margin to adapt to both the model's confidence and the magnitude of the cost gap. From Parent 0 (Adaptive_Log_Sigmoid_Loss_with_Cost_Margin), it inherits the use of a `logsigmoid` loss function, which is a standard and often more stable alternative to the hinge-like `softplus` used in Parent 1. As a new coupling idea, this child loss introduces a novel formulation for the adaptive beta (temperature). Instead of being inversely proportional to the logp standard deviation (as in Parent 0), this beta is *directly* proportional to the standard deviation of the logp differences, but clamped to a reasonable range. The intuition is that when the model is uncertain (high logp variance), we should increase beta to sharpen the gradients and encourage a clearer preference signal, and when the model is confident (low variance), we can soften the gradients to avoid overfitting. This is a contrast to Parent 0's approach and represents an exploration of a different stability hypothesis. The final loss is `loss = -logsigmoid(beta_adaptive * (delta_logp - margin))`, directly fitting the Bradley-Terry framework where the target log-odds are shifted by a dynamic, cost-aware margin.", "pseudocode": "1. Calculate the difference in log-probabilities: delta_logp = logp(a) - logp(b).\n2. Calculate the difference in costs: delta_cost = cost(b) - cost(a).\n3. Calculate an adaptive margin scale based on the batch standard deviation of log-probability differences, clipped for stability: adaptive_scale = clip(std(delta_logp), min_scale, max_scale).\n4. Create a cost-dependent weight by applying a sigmoid to the cost difference: cost_weight = sigmoid(delta_cost / temperature).\n5. Compute the final dynamic margin by multiplying the adaptive scale and the cost weight: margin = adaptive_scale * cost_weight.\n6. Calculate a new adaptive beta that is directly proportional to the standard deviation of log-probability differences, also clipped for stability: beta_adaptive = clip(base_beta * std(delta_logp), min_beta, max_beta).\n7. Compute the loss argument: loss_arg = delta_logp - margin.\n8. Calculate the final loss using the negative log-sigmoid function, scaled by the adaptive beta: loss = -logsigmoid(beta_adaptive * loss_arg).\n9. Return the mean of the loss across the batch.", "hyperparams": {"temperature": 1.0, "min_scale": 0.1, "max_scale": 5.0, "base_beta": 1.0, "min_beta": 0.5, "max_beta": 2.0, "epsilon": 1e-08}, "operators_used": ["sigmoid", "logsigmoid", "clamp"], "implementation_hint": {"expects": ["cost_a", "cost_b", "logp_a", "logp_b"], "returns": "scalar"}, "code": "def generated_loss(batch, model_output, extra):\n    \"\"\"\n    A preference loss combining a dynamic sigmoid-based margin with a logsigmoid loss function\n    and a novel adaptive beta for stability and learning rate modulation.\n    \"\"\"\n    # Read tensors from the batch\n    cost_a = batch['cost_a']\n    cost_b = batch['cost_b']\n    logp_a = batch['log_prob_w']\n    logp_b = batch['log_prob_l']\n    weights = batch.get('weight')\n\n    # Read hyperparameters\n    hyperparams = extra['hyperparams']\n    temperature = hyperparams.get('temperature', 1.0)\n    min_scale = hyperparams.get('min_scale', 0.1)\n    max_scale = hyperparams.get('max_scale', 5.0)\n    base_beta = hyperparams.get('base_beta', 1.0)\n    min_beta = hyperparams.get('min_beta', 0.5)\n    max_beta = hyperparams.get('max_beta', 2.0)\n    epsilon = hyperparams.get('epsilon', 1e-8)\n\n    # 1. Calculate log-probability and cost differences\n    delta_logp = logp_a - logp_b\n    delta_cost = cost_b - cost_a\n\n    # Calculate logp_std once for use in both margin and beta\n    if delta_logp.numel() > 1:\n        logp_std = delta_logp.detach().std()\n    else:\n        logp_std = torch.tensor(1.0, device=delta_logp.device)\n\n    # 2. Inherit adaptive margin from Parent 1\n    adaptive_scale = torch.clamp(logp_std, min_scale, max_scale)\n    cost_weight = torch.sigmoid(delta_cost / temperature)\n    margin = adaptive_scale * cost_weight\n\n    # 3. New Coupling: Adaptive beta directly proportional to logp_std\n    beta_adaptive = torch.clamp(base_beta * logp_std, min_beta, max_beta)\n\n    # 4. Compute loss argument and apply logsigmoid (from Parent 0)\n    # We want delta_logp > margin, so we penalize when delta_logp - margin is small or negative.\n    loss_arg = delta_logp - margin\n    loss_per_pair = -torch.nn.functional.logsigmoid(beta_adaptive * loss_arg)\n\n    # 5. Apply optional weights and compute the mean loss\n    if weights is not None:\n        loss = (loss_per_pair * weights).sum() / weights.sum().clamp(min=epsilon)\n    else:\n        loss = loss_per_pair.mean()\n\n    return loss", "theoretical_basis": "A Bradley-Terry logistic preference model where the target log-odds of preference are dynamically shifted by a margin. This margin is coupled to both the cost difference (via a sigmoid) and the batch-wise variance of the model's preference scores. The logistic temperature (beta) is also adaptively scaled based on the model's output variance to modulate gradient strength."}, "fitness": {"hf_like_score": 9.197406683349609, "validation_objective": 9.197406683349609, "generalization_penalty": 0.0, "generalization_objectives": {"100": 9.192453652954102}, "train_score_mean": 18.431365938186644, "train_loss_mean": 5.2933757829666135, "pair_count": 31679983, "early_eval": {"enabled": true, "steps": 100, "baseline_validation_objective": 8.412976063537597, "candidate_validation_objective": 9.197406683349609, "early_stopped": true}, "phases": {"f1": {"steps": 15630, "train_score_mean": 18.431365938186644, "train_loss_mean": 5.2933757829666135, "pair_count": 31679983}, "f2": {"steps": 0, "train_score_mean": null, "train_loss_mean": null, "pair_count": 0}}, "config": {"hf": {"problem": "tsp", "hf_steps": 0, "hf_epochs": 10, "hf_instances_per_epoch": 100000, "train_problem_size": 100, "valid_problem_sizes": [100], "train_batch_size": 64, "pomo_size": 100, "learning_rate": 0.0003, "weight_decay": 1e-06, "alpha": 0.05, "device": "cuda:1", "seed": 1234, "num_validation_episodes": 10000, "validation_batch_size": 64, "generalization_penalty_weight": 1.0, "pool_version": "v0"}, "free_loss": {"f1_steps": 0, "total_train_steps": 15630, "f2_steps": 0, "f3_enabled": false}}, "loss_ir": {"name": "Sigmoid_Margin_LogSigmoid_Loss_with_Adaptive_Beta", "intuition": "Mode: explore. This loss function combines the strongest elements of both parents while introducing a new stability mechanism. From Parent 1 (Adaptive_Sigmoid_Margin_Loss), which has a strong hf_like_score, it inherits the idea of a dynamic margin that is a product of an adaptive scale (based on the standard deviation of logp differences) and a sigmoid-transformed cost difference. This allows the margin to adapt to both the model's confidence and the magnitude of the cost gap. From Parent 0 (Adaptive_Log_Sigmoid_Loss_with_Cost_Margin), it inherits the use of a `logsigmoid` loss function, which is a standard and often more stable alternative to the hinge-like `softplus` used in Parent 1. As a new coupling idea, this child loss introduces a novel formulation for the adaptive beta (temperature). Instead of being inversely proportional to the logp standard deviation (as in Parent 0), this beta is *directly* proportional to the standard deviation of the logp differences, but clamped to a reasonable range. The intuition is that when the model is uncertain (high logp variance), we should increase beta to sharpen the gradients and encourage a clearer preference signal, and when the model is confident (low variance), we can soften the gradients to avoid overfitting. This is a contrast to Parent 0's approach and represents an exploration of a different stability hypothesis. The final loss is `loss = -logsigmoid(beta_adaptive * (delta_logp - margin))`, directly fitting the Bradley-Terry framework where the target log-odds are shifted by a dynamic, cost-aware margin.", "hyperparams": {"temperature": 1.0, "min_scale": 0.1, "max_scale": 5.0, "base_beta": 1.0, "min_beta": 0.5, "max_beta": 2.0, "epsilon": 1e-08}, "operators_used": ["sigmoid", "logsigmoid", "clamp"]}}, "better_than_baseline": false}
{"generation": 2, "index": 7, "ir": {"name": "Sigmoid_Margin_LogSigmoid_Loss_with_Adaptive_Beta", "intuition": "Mode: explore. This loss function hybridizes a logistic Bradley-Terry model with a dynamic margin, drawing inspiration from both parents while introducing new coupling for stability and expressiveness. It inherits the core `-logsigmoid(beta * (delta_logp + margin))` structure from Parent 0, which is a standard and robust formulation for preference learning. From Parent 1, it inherits the idea of creating a margin by applying a `sigmoid` to the cost difference (`sigmoid(delta_cost / temp)`), which provides a smooth, bounded margin between 0 and 1. The child introduces two new coupling ideas. First, it combines these inherited ideas by using the sigmoid-based margin from Parent 1 *inside* the logsigmoid loss from Parent 0. Second, it adopts the adaptive beta mechanism from Parent 0 (`beta / std(delta_logp)`), but applies it to this new combined structure. This adaptive beta normalizes the learning signal based on the model's current output variance, preventing gradient explosion when the model is overconfident and sharpening the signal when it is uncertain. The result is a loss that encourages the log-probability gap to not only be positive but to also respect a smooth, cost-proportional margin, all while being dynamically scaled for training stability.", "pseudocode": "1. Calculate the difference in log-probabilities: delta_logp = logp(a) - logp(b).\n2. Calculate the difference in costs: delta_cost = cost(b) - cost(a).\n3. Compute a cost-dependent margin by applying a scaled sigmoid to the cost difference: margin = margin_scale * sigmoid(delta_cost / temperature).\n4. Calculate an adaptive temperature (beta) for the logistic loss. It is the base beta hyperparameter divided by the standard deviation of the log-probability differences in the batch (detached). This stabilizes training by reducing the gradient magnitude when model preferences are very diverse.\n5. Compute the final loss argument by adding the sigmoid margin to the log-probability difference: loss_arg = delta_logp + margin.\n6. Apply the negative log-sigmoid function to the argument, scaled by the adaptive beta: loss = -logsigmoid(beta_adaptive * loss_arg).\n7. Return the mean of the loss across the batch.", "hyperparams": {"beta": 1.0, "margin_scale": 0.5, "temperature": 1.0, "epsilon": 1e-08}, "operators_used": ["logsigmoid", "sigmoid"], "implementation_hint": {"expects": ["cost_a", "cost_b", "logp_a", "logp_b"], "returns": "scalar"}, "code": "def generated_loss(batch, model_output, extra):\n    \"\"\"\n    A preference loss combining a logsigmoid base with a sigmoid-based cost margin\n    and an adaptive beta for stability.\n    \"\"\"\n    # Read tensors from the batch\n    cost_a = batch['cost_a']\n    cost_b = batch['cost_b']\n    logp_a = batch['log_prob_w']\n    logp_b = batch['log_prob_l']\n    weights = batch.get('weight')\n\n    # Read hyperparameters\n    hyperparams = extra['hyperparams']\n    beta = hyperparams.get('beta', 1.0)\n    margin_scale = hyperparams.get('margin_scale', 0.5)\n    temperature = hyperparams.get('temperature', 1.0)\n    epsilon = hyperparams.get('epsilon', 1e-8)\n\n    # 1. Calculate log-probability difference\n    delta_logp = logp_a - logp_b\n\n    # 2. Calculate cost difference\n    delta_cost = cost_b - cost_a\n\n    # 3. Compute a cost-dependent margin via sigmoid (inspired by Parent 1)\n    margin = margin_scale * torch.sigmoid(delta_cost / temperature)\n\n    # 4. Calculate adaptive beta for stability (inspired by Parent 0)\n    # This is a new coupling idea: applying adaptive beta to a sigmoid margin loss.\n    if delta_logp.numel() > 1:\n        std_delta_logp = delta_logp.std().detach() # Detach to not backprop through std dev\n        beta_adaptive = beta / (std_delta_logp + epsilon)\n    else:\n        beta_adaptive = beta\n\n    # 5. Compute the loss argument with the margin\n    loss_arg = delta_logp + margin\n\n    # 6. Apply the negative log-sigmoid function (from Parent 0)\n    loss_per_pair = -torch.nn.functional.logsigmoid(beta_adaptive * loss_arg)\n\n    # 7. Apply optional weights and compute the mean loss\n    if weights is not None and weights.sum() > 0:\n        loss = (loss_per_pair * weights).sum() / weights.sum()\n    else:\n        loss = loss_per_pair.mean()\n\n    return loss", "theoretical_basis": "A modified Bradley-Terry logistic preference model where the target log-odds are shifted by a dynamic, cost-proportional margin. The margin is a sigmoid function of the cost difference, and the overall loss is scaled by an adaptive beta based on batch statistics of log-probability differences to ensure stable training."}, "fitness": {"hf_like_score": 7.907180215454102, "validation_objective": 7.904811708068848, "generalization_penalty": 0.0023685073852544036, "generalization_objectives": {"100": 7.907180215454102}, "train_score_mean": 8.136412848132737, "train_loss_mean": 0.5003814840919302, "pair_count": 4951541909, "early_eval": {"enabled": true, "steps": 100, "baseline_validation_objective": 8.412976063537597, "candidate_validation_objective": 8.349788247680664, "early_stopped": false}, "phases": {"f1": {"steps": 15630, "train_score_mean": 8.136412848132737, "train_loss_mean": 0.5003814840919302, "pair_count": 4951541909}, "f2": {"steps": 0, "train_score_mean": null, "train_loss_mean": null, "pair_count": 0}}, "config": {"hf": {"problem": "tsp", "hf_steps": 0, "hf_epochs": 10, "hf_instances_per_epoch": 100000, "train_problem_size": 100, "valid_problem_sizes": [100], "train_batch_size": 64, "pomo_size": 100, "learning_rate": 0.0003, "weight_decay": 1e-06, "alpha": 0.05, "device": "cuda:2", "seed": 1234, "num_validation_episodes": 10000, "validation_batch_size": 64, "generalization_penalty_weight": 1.0, "pool_version": "v0"}, "free_loss": {"f1_steps": 0, "total_train_steps": 15630, "f2_steps": 0, "f3_enabled": false}}, "loss_ir": {"name": "Sigmoid_Margin_LogSigmoid_Loss_with_Adaptive_Beta", "intuition": "Mode: explore. This loss function hybridizes a logistic Bradley-Terry model with a dynamic margin, drawing inspiration from both parents while introducing new coupling for stability and expressiveness. It inherits the core `-logsigmoid(beta * (delta_logp + margin))` structure from Parent 0, which is a standard and robust formulation for preference learning. From Parent 1, it inherits the idea of creating a margin by applying a `sigmoid` to the cost difference (`sigmoid(delta_cost / temp)`), which provides a smooth, bounded margin between 0 and 1. The child introduces two new coupling ideas. First, it combines these inherited ideas by using the sigmoid-based margin from Parent 1 *inside* the logsigmoid loss from Parent 0. Second, it adopts the adaptive beta mechanism from Parent 0 (`beta / std(delta_logp)`), but applies it to this new combined structure. This adaptive beta normalizes the learning signal based on the model's current output variance, preventing gradient explosion when the model is overconfident and sharpening the signal when it is uncertain. The result is a loss that encourages the log-probability gap to not only be positive but to also respect a smooth, cost-proportional margin, all while being dynamically scaled for training stability.", "hyperparams": {"beta": 1.0, "margin_scale": 0.5, "temperature": 1.0, "epsilon": 1e-08}, "operators_used": ["logsigmoid", "sigmoid"]}}, "better_than_baseline": false}
{"generation": 3, "index": 0, "ir": {"name": "Adaptive_Margin_LogSigmoid_with_Cost_Variance_Coupling", "intuition": "Mode: combine. This loss function hybridizes the adaptive mechanisms of both parents to create a more robust and responsive learning objective. It inherits the core `logsigmoid` structure and the adaptive margin scale based on `std(delta_logp)` from both parents (Parent 0 and Parent 1). This ensures the loss is grounded in a probabilistic framework and that the required preference margin adapts to the model's current output variance.\n\nFrom Parent 1, it inherits the idea of making the cost-to-margin transformation adaptive. However, instead of using a fixed temperature or `std(delta_cost)`, this child introduces a new coupling: the temperature of the cost sigmoid (`cost_temp`) is set to `1 + std(delta_cost)`. This makes the margin's sensitivity to cost differences dependent on the diversity of costs within the batch, while the `+ 1` ensures the temperature is always at least 1, preventing it from becoming too small and causing extreme sigmoid outputs when cost differences are uniform.\n\nAs a second new coupling, this child introduces a 'beta floor'. The adaptive beta (`beta_adaptive`), which is inversely proportional to the `adaptive_scale` (an idea present in both parents), is now calculated as `max(beta_floor, 1.0 / adaptive_scale)`. This prevents `beta_adaptive` from becoming excessively large when the model is very confident (low `adaptive_scale`), which could lead to vanishingly small gradients and training stagnation. This `beta_floor` acts as a regularizer, ensuring a minimum learning signal is always present.", "pseudocode": "1. Calculate the difference in log-probabilities: delta_logp = logp(a) - logp(b).\n2. Calculate the difference in costs: delta_cost = cost(b) - cost(a).\n3. Inherit from both parents: Compute an adaptive margin scale based on the standard deviation of delta_logp, clipped for stability: adaptive_scale = clip(std(delta_logp), min_scale, max_scale).\n4. New Coupling 1: Compute an adaptive temperature for the cost-to-margin transformation. cost_temp = 1.0 + std(delta_cost). This makes the sigmoid's steepness dependent on the batch's cost diversity, with a baseline to prevent instability.\n5. Inherit from both parents: Calculate a cost-dependent margin weight using a sigmoid function with the new adaptive temperature: cost_weight = sigmoid(delta_cost / cost_temp).\n6. Compute the final dynamic margin by multiplying the adaptive scale and the cost weight: margin = adaptive_scale * cost_weight.\n7. New Coupling 2: Calculate an adaptive beta for the logistic loss. It's the inverse of the adaptive scale, but with a minimum floor to prevent overly large beta values: beta_adaptive = max(beta_floor, 1.0 / adaptive_scale).\n8. Compute the loss argument: loss_arg = delta_logp - margin.\n9. Calculate the final loss using a scaled negative log-sigmoid function: loss = -logsigmoid(beta_adaptive * loss_arg).\n10. Return the mean of the loss across the batch.", "hyperparams": {"min_scale": 0.1, "max_scale": 5.0, "beta_floor": 0.1, "epsilon": 1e-08}, "operators_used": ["logsigmoid", "sigmoid", "clamp"], "implementation_hint": {"expects": ["cost_a", "cost_b", "logp_a", "logp_b"], "returns": "scalar"}, "code": "def generated_loss(batch, model_output, extra):\n    \"\"\"\n    A logistic preference loss with a margin adaptive to both logp and cost variance,\n    and a beta regularized by a floor to prevent gradient collapse.\n    \"\"\"\n    # Read tensors from the batch\n    cost_a = batch['cost_a']\n    cost_b = batch['cost_b']\n    logp_a = batch['log_prob_w']\n    logp_b = batch['log_prob_l']\n    weights = batch.get('weight')\n\n    # Read hyperparameters\n    hyperparams = extra['hyperparams']\n    min_scale = hyperparams.get('min_scale', 0.1)\n    max_scale = hyperparams.get('max_scale', 5.0)\n    beta_floor = hyperparams.get('beta_floor', 0.1)\n    epsilon = hyperparams.get('epsilon', 1e-8)\n\n    # 1. Calculate log-probability and cost differences\n    delta_logp = logp_a - logp_b\n    delta_cost = cost_b - cost_a\n\n    # 2. Inherited: Compute adaptive margin scale from logp stats\n    if delta_logp.numel() > 1:\n        logp_std = delta_logp.detach().std()\n    else:\n        logp_std = torch.tensor(1.0, device=delta_logp.device)\n    adaptive_scale = torch.clamp(logp_std, min_scale, max_scale)\n\n    # 3. New Coupling 1: Adaptive temperature from cost stats with a baseline\n    if delta_cost.numel() > 1:\n        cost_std = delta_cost.detach().std()\n    else:\n        cost_std = torch.tensor(0.0, device=delta_cost.device)\n    cost_temp = 1.0 + cost_std\n\n    # 4. Inherited: Cost-dependent margin weight using sigmoid with new adaptive temp\n    cost_weight = torch.sigmoid(delta_cost / cost_temp)\n\n    # 5. Compute the final dynamic margin\n    margin = adaptive_scale * cost_weight\n\n    # 6. New Coupling 2: Adaptive beta with a floor\n    # Inverse coupling from parents, but with max() to prevent beta from exploding\n    beta_adaptive = torch.max(torch.tensor(beta_floor, device=adaptive_scale.device), 1.0 / (adaptive_scale.detach() + epsilon))\n\n    # 7. Compute loss argument for logistic loss\n    loss_arg = delta_logp - margin\n\n    # 8. Inherited: Apply negative log-sigmoid loss\n    loss_per_pair = -torch.nn.functional.logsigmoid(beta_adaptive * loss_arg)\n\n    # 9. Apply optional weights and compute the mean loss\n    if weights is not None:\n        loss = (loss_per_pair * weights).sum() / weights.sum().clamp(min=epsilon)\n    else:\n        loss = loss_per_pair.mean()\n\n    return loss", "theoretical_basis": "A Bradley-Terry logistic preference model where the target log-odds are a dynamic margin. This margin is jointly determined by the model's output variance (`std(delta_logp)`) and the variance of the costs in the batch (`std(delta_cost)`). The logistic loss's temperature (beta) is inversely coupled to the margin's scale but is regularized with a floor to prevent excessively sharp gradients when the model is confident, ensuring stable learning."}, "fitness": {"hf_like_score": 7.874644753265381, "validation_objective": 7.8736080368042, "generalization_penalty": 0.0010367164611810864, "generalization_objectives": {"100": 7.874644753265381}, "train_score_mean": 8.663657280488115, "train_loss_mean": 0.4734332584450051, "pair_count": 4951579487, "early_eval": {"enabled": true, "steps": 100, "baseline_validation_objective": 8.412976063537597, "candidate_validation_objective": 8.360819259643554, "early_stopped": false}, "phases": {"f1": {"steps": 15630, "train_score_mean": 8.663657280488115, "train_loss_mean": 0.4734332584450051, "pair_count": 4951579487}, "f2": {"steps": 0, "train_score_mean": null, "train_loss_mean": null, "pair_count": 0}}, "config": {"hf": {"problem": "tsp", "hf_steps": 0, "hf_epochs": 10, "hf_instances_per_epoch": 100000, "train_problem_size": 100, "valid_problem_sizes": [100], "train_batch_size": 64, "pomo_size": 100, "learning_rate": 0.0003, "weight_decay": 1e-06, "alpha": 0.05, "device": "cuda:0", "seed": 1234, "num_validation_episodes": 10000, "validation_batch_size": 64, "generalization_penalty_weight": 1.0, "pool_version": "v0"}, "free_loss": {"f1_steps": 0, "total_train_steps": 15630, "f2_steps": 0, "f3_enabled": false}}, "loss_ir": {"name": "Adaptive_Margin_LogSigmoid_with_Cost_Variance_Coupling", "intuition": "Mode: combine. This loss function hybridizes the adaptive mechanisms of both parents to create a more robust and responsive learning objective. It inherits the core `logsigmoid` structure and the adaptive margin scale based on `std(delta_logp)` from both parents (Parent 0 and Parent 1). This ensures the loss is grounded in a probabilistic framework and that the required preference margin adapts to the model's current output variance.\n\nFrom Parent 1, it inherits the idea of making the cost-to-margin transformation adaptive. However, instead of using a fixed temperature or `std(delta_cost)`, this child introduces a new coupling: the temperature of the cost sigmoid (`cost_temp`) is set to `1 + std(delta_cost)`. This makes the margin's sensitivity to cost differences dependent on the diversity of costs within the batch, while the `+ 1` ensures the temperature is always at least 1, preventing it from becoming too small and causing extreme sigmoid outputs when cost differences are uniform.\n\nAs a second new coupling, this child introduces a 'beta floor'. The adaptive beta (`beta_adaptive`), which is inversely proportional to the `adaptive_scale` (an idea present in both parents), is now calculated as `max(beta_floor, 1.0 / adaptive_scale)`. This prevents `beta_adaptive` from becoming excessively large when the model is very confident (low `adaptive_scale`), which could lead to vanishingly small gradients and training stagnation. This `beta_floor` acts as a regularizer, ensuring a minimum learning signal is always present.", "hyperparams": {"min_scale": 0.1, "max_scale": 5.0, "beta_floor": 0.1, "epsilon": 1e-08}, "operators_used": ["logsigmoid", "sigmoid", "clamp"]}}, "better_than_baseline": true}
{"generation": 3, "index": 1, "ir": {"name": "SelfBalancing_Adaptive_LogSigmoid_Loss", "intuition": "Mode: combine. This loss function combines the most effective adaptive mechanisms from both parents and introduces a new self-balancing coupling to create a highly stable and performant objective. It inherits the core `logsigmoid` loss structure, which is a standard choice for probabilistic preference learning. From Parent 1, it inherits the use of an adaptive temperature for the `sigmoid(cost_difference)` term, making the margin's sensitivity to cost gaps dependent on the diversity of costs within the batch. From Parent 0, it inherits the idea of an adaptive `beta` (temperature) for the main `logsigmoid` loss, which is inversely coupled to an adaptive scale factor, preventing overly aggressive updates when the model is uncertain. The core innovation and new coupling idea is to make the `adaptive_scale` itself a function of *both* log-probability variance (from Parent 0/1) and cost variance (from Parent 1). Specifically, `adaptive_scale = logp_std + cost_std`. This directly couples the required margin size to the difficulty of the batch as measured by both model uncertainty (`logp_std`) and task ambiguity (`cost_std`). This creates a robust, self-balancing system where the margin and the loss steepness (`beta`) are simultaneously adjusted based on a comprehensive view of batch difficulty.", "pseudocode": "1. Calculate the difference in log-probabilities: delta_logp = logp(a) - logp(b).\n2. Calculate the difference in costs: delta_cost = cost(b) - cost(a).\n3. Compute the standard deviation of log-probability differences: logp_std.\n4. Compute the standard deviation of cost differences: cost_std.\n5. Introduce a new coupling: define a composite adaptive scale by summing the two standard deviations: `adaptive_scale = logp_std + cost_std`, clipped for stability. This captures both model uncertainty and task difficulty.\n6. Inherit from Parent 1: Compute an adaptive temperature for the cost-sigmoid based on `cost_std`: `cost_temp = cost_std + epsilon`.\n7. Compute a cost-dependent weight using the adaptive temperature: `cost_weight = sigmoid(delta_cost / cost_temp)`.\n8. Calculate the final dynamic margin: `margin = adaptive_scale * cost_weight`.\n9. Inherit from Parent 0: Define an adaptive beta for the logistic loss as the inverse of the composite adaptive scale: `beta = 1.0 / (adaptive_scale + epsilon)`. This softens the loss when the required margin is large.\n10. Compute the argument for the loss function: `loss_arg = delta_logp - margin`.\n11. Calculate the final loss using a scaled negative log-sigmoid function: `loss = -logsigmoid(beta * loss_arg)`.\n12. Return the mean of the loss across the batch.", "hyperparams": {"min_scale": 0.1, "max_scale": 10.0, "epsilon": 1e-08}, "operators_used": ["logsigmoid", "sigmoid", "clamp"], "implementation_hint": {"expects": ["cost_a", "cost_b", "logp_a", "logp_b"], "returns": "scalar"}, "code": "def generated_loss(batch, model_output, extra):\n    \"\"\"\n    A logistic preference loss with a composite adaptive margin and a self-balancing temperature.\n    The margin scale is derived from both logp and cost variance.\n    The loss temperature is inversely coupled to this composite scale.\n    \"\"\"\n    # Read tensors from the batch\n    cost_a = batch['cost_a']\n    cost_b = batch['cost_b']\n    logp_a = batch['log_prob_w']\n    logp_b = batch['log_prob_l']\n    weights = batch.get('weight')\n\n    # Read hyperparameters\n    hyperparams = extra['hyperparams']\n    min_scale = hyperparams.get('min_scale', 0.1)\n    max_scale = hyperparams.get('max_scale', 10.0)\n    epsilon = hyperparams.get('epsilon', 1e-8)\n\n    # 1. Calculate log-probability and cost differences\n    delta_logp = logp_a - logp_b\n    delta_cost = cost_b - cost_a\n\n    # 2. Compute batch statistics (standard deviations)\n    if delta_logp.numel() > 1:\n        logp_std = delta_logp.detach().std()\n        cost_std = delta_cost.detach().std()\n    else:\n        logp_std = torch.tensor(1.0, device=delta_logp.device)\n        cost_std = torch.tensor(1.0, device=delta_logp.device)\n\n    # 3. New Coupling: Composite adaptive scale from logp and cost stats\n    composite_scale = torch.clamp(logp_std + cost_std, min_scale, max_scale)\n\n    # 4. Adaptive cost temperature (Inherited from Parent 1)\n    cost_temp = cost_std + epsilon\n\n    # 5. Cost-dependent weight using adaptive temperature\n    cost_weight = torch.sigmoid(delta_cost / cost_temp)\n\n    # 6. Calculate the final dynamic margin\n    margin = composite_scale * cost_weight\n\n    # 7. Adaptive beta for logsigmoid (Inherited from Parent 0, using new composite scale)\n    beta = 1.0 / (composite_scale.detach() + epsilon)\n\n    # 8. Compute the argument for the logistic loss\n    loss_arg = delta_logp - margin\n\n    # 9. Apply scaled negative log-sigmoid loss\n    loss_per_pair = -torch.nn.functional.logsigmoid(beta * loss_arg)\n\n    # 10. Apply optional weights and compute the mean loss\n    if weights is not None:\n        loss = (loss_per_pair * weights).sum() / weights.sum().clamp(min=epsilon)\n    else:\n        loss = loss_per_pair.mean()\n\n    return loss", "theoretical_basis": "A Bradley-Terry logistic preference model where the target log-odds is a dynamic margin. The margin's scale is innovatively coupled to the sum of batch-wise log-probability variance and cost variance, holistically capturing batch difficulty. The steepness of the logistic loss is inversely coupled to this composite scale, creating a self-balancing mechanism that adapts both the learning target and gradient strength based on model confidence and task ambiguity."}, "fitness": {"hf_like_score": 7.8733028778076175, "validation_objective": 7.870814694976807, "generalization_penalty": 0.002488182830810537, "generalization_objectives": {"100": 7.8733028778076175}, "train_score_mean": 8.087408711341277, "train_loss_mean": 0.8420799051350076, "pair_count": 4951576936, "early_eval": {"enabled": true, "steps": 100, "baseline_validation_objective": 8.412976063537597, "candidate_validation_objective": 8.361937609863281, "early_stopped": false}, "phases": {"f1": {"steps": 15630, "train_score_mean": 8.087408711341277, "train_loss_mean": 0.8420799051350076, "pair_count": 4951576936}, "f2": {"steps": 0, "train_score_mean": null, "train_loss_mean": null, "pair_count": 0}}, "config": {"hf": {"problem": "tsp", "hf_steps": 0, "hf_epochs": 10, "hf_instances_per_epoch": 100000, "train_problem_size": 100, "valid_problem_sizes": [100], "train_batch_size": 64, "pomo_size": 100, "learning_rate": 0.0003, "weight_decay": 1e-06, "alpha": 0.05, "device": "cuda:1", "seed": 1234, "num_validation_episodes": 10000, "validation_batch_size": 64, "generalization_penalty_weight": 1.0, "pool_version": "v0"}, "free_loss": {"f1_steps": 0, "total_train_steps": 15630, "f2_steps": 0, "f3_enabled": false}}, "loss_ir": {"name": "SelfBalancing_Adaptive_LogSigmoid_Loss", "intuition": "Mode: combine. This loss function combines the most effective adaptive mechanisms from both parents and introduces a new self-balancing coupling to create a highly stable and performant objective. It inherits the core `logsigmoid` loss structure, which is a standard choice for probabilistic preference learning. From Parent 1, it inherits the use of an adaptive temperature for the `sigmoid(cost_difference)` term, making the margin's sensitivity to cost gaps dependent on the diversity of costs within the batch. From Parent 0, it inherits the idea of an adaptive `beta` (temperature) for the main `logsigmoid` loss, which is inversely coupled to an adaptive scale factor, preventing overly aggressive updates when the model is uncertain. The core innovation and new coupling idea is to make the `adaptive_scale` itself a function of *both* log-probability variance (from Parent 0/1) and cost variance (from Parent 1). Specifically, `adaptive_scale = logp_std + cost_std`. This directly couples the required margin size to the difficulty of the batch as measured by both model uncertainty (`logp_std`) and task ambiguity (`cost_std`). This creates a robust, self-balancing system where the margin and the loss steepness (`beta`) are simultaneously adjusted based on a comprehensive view of batch difficulty.", "hyperparams": {"min_scale": 0.1, "max_scale": 10.0, "epsilon": 1e-08}, "operators_used": ["logsigmoid", "sigmoid", "clamp"]}}, "better_than_baseline": true}
{"generation": 3, "index": 2, "ir": {"name": "LogSigmoid_Adaptive_Margin_with_Cost_Variance_Coupling", "intuition": "Mode: combine. This loss function hybridizes the strongest adaptive mechanisms from both parents while introducing a new coupling to further stabilize the margin. It inherits the core `logsigmoid` loss structure and the adaptive margin scaled by `std(delta_logp)` from both parents. This ensures the loss is probabilistic and that the margin's magnitude is sensitive to the model's confidence. From Parent 1, it specifically inherits the inverse coupling between the loss temperature (`beta`) and this adaptive scale (`adaptive_scale`), which is a powerful stability trick: when the model is uncertain and the required margin is large, the loss function softens to prevent aggressive updates. The innovation of this child is a new coupling idea: it modulates the cost-dependent part of the margin using the variance of costs within the batch. Specifically, it inherits the `sigmoid(delta_cost / temperature)` term from Parent 0, but makes the `temperature` adaptive, as inspired by Parent 1. This new `cost_temp` is set to `1 + std(delta_cost)`, which normalizes the cost signal. When costs in a batch are very similar (low std), the sigmoid becomes steeper, making the margin more sensitive to small cost differences. When costs are highly varied (high std), the sigmoid flattens, preventing the margin from being dominated by a few pairs with large cost gaps. This creates a more robust and self-normalizing margin calculation.", "pseudocode": "1. Calculate the difference in log-probabilities: delta_logp = logp(a) - logp(b).\n2. Calculate the difference in costs: delta_cost = cost(b) - cost(a).\n3. Inherit from both parents: Compute an adaptive scale for the margin based on the standard deviation of `delta_logp` in the batch, clipped for stability: `adaptive_scale = clip(std(delta_logp), min_scale, max_scale)`.\n4. New Coupling Idea: Compute an adaptive temperature for the cost sigmoid. `cost_temp = 1.0 + std(delta_cost)`. This normalizes the cost signal based on batch-wise cost variance.\n5. Inherit from Parent 0: Calculate a cost-dependent weight using a sigmoid function with the new adaptive temperature: `cost_weight = sigmoid(delta_cost / cost_temp)`.\n6. Combine inherited ideas: Calculate the final dynamic margin by multiplying the adaptive scale and the cost weight: `margin = adaptive_scale * cost_weight`.\n7. Inherit from Parent 1: Define an adaptive beta for the logistic loss as the inverse of the adaptive scale: `beta = 1.0 / (adaptive_scale + epsilon)`. This couples the loss steepness to model uncertainty.\n8. Compute the argument for the loss function: `loss_arg = delta_logp - margin`.\n9. Apply the negative log-sigmoid function, scaled by the adaptive beta: `loss = -logsigmoid(beta * loss_arg)`.\n10. Return the mean of the loss across the batch.", "hyperparams": {"min_scale": 0.1, "max_scale": 5.0, "epsilon": 1e-08}, "operators_used": ["logsigmoid", "sigmoid", "clamp"], "implementation_hint": {"expects": ["cost_a", "cost_b", "logp_a", "logp_b"], "returns": "scalar"}, "code": "def generated_loss(batch, model_output, extra):\n    \"\"\"\n    A logistic preference loss with a dynamic margin. The margin scale is adapted from logp variance,\n    and its cost sensitivity is adapted from cost variance. The loss temperature is inversely coupled to the margin scale.\n    \"\"\"\n    # Read tensors from the batch\n    cost_a = batch['cost_a']\n    cost_b = batch['cost_b']\n    logp_a = batch['log_prob_w']\n    logp_b = batch['log_prob_l']\n    weights = batch.get('weight')\n\n    # Read hyperparameters\n    hyperparams = extra['hyperparams']\n    min_scale = hyperparams.get('min_scale', 0.1)\n    max_scale = hyperparams.get('max_scale', 5.0)\n    epsilon = hyperparams.get('epsilon', 1e-8)\n\n    # 1. Calculate log-probability and cost differences\n    delta_logp = logp_a - logp_b\n    delta_cost = cost_b - cost_a\n\n    # 2. Inherit: Compute adaptive margin scale from logp stats\n    if delta_logp.numel() > 1:\n        logp_std = delta_logp.detach().std()\n    else:\n        logp_std = torch.tensor(1.0, device=delta_logp.device)\n    adaptive_scale = torch.clamp(logp_std, min_scale, max_scale)\n\n    # 3. New Coupling: Adaptive temperature for cost sigmoid based on cost variance\n    if delta_cost.numel() > 1:\n        cost_std = delta_cost.detach().std()\n    else:\n        cost_std = torch.tensor(0.0, device=delta_cost.device)\n    cost_temp = 1.0 + cost_std\n\n    # 4. Inherit: Cost-dependent weight using sigmoid with the new adaptive temperature\n    cost_weight = torch.sigmoid(delta_cost / cost_temp)\n\n    # 5. Combine: Compute the final dynamic margin\n    margin = adaptive_scale * cost_weight\n\n    # 6. Inherit: Beta inversely coupled to adaptive scale for stability\n    beta = 1.0 / (adaptive_scale.detach() + epsilon)\n\n    # 7. Compute loss argument for logistic loss\n    loss_arg = delta_logp - margin\n\n    # 8. Inherit: Apply negative log-sigmoid loss\n    loss_per_pair = -torch.nn.functional.logsigmoid(beta * loss_arg)\n\n    # 9. Apply optional weights and compute the mean loss\n    if weights is not None:\n        loss = (loss_per_pair * weights).sum() / weights.sum().clamp(min=epsilon)\n    else:\n        loss = loss_per_pair.mean()\n\n    return loss", "theoretical_basis": "A Bradley-Terry logistic preference model where the target log-odds is a dynamic margin. The margin's scale is coupled to model uncertainty (logp variance), and its sensitivity to cost differences is normalized by the batch-wise cost variance. The loss function's steepness (beta) is inversely coupled to the margin's scale, creating a self-regulating system that adapts both the learning target and the gradient strength based on batch statistics."}, "fitness": {"hf_like_score": 7.879697248840332, "validation_objective": 7.878110426330567, "generalization_penalty": 0.0015868225097648292, "generalization_objectives": {"100": 7.879697248840332}, "train_score_mean": 8.652934531469949, "train_loss_mean": 0.5076336446150861, "pair_count": 4951579114, "early_eval": {"enabled": true, "steps": 100, "baseline_validation_objective": 8.412976063537597, "candidate_validation_objective": 8.373296159362793, "early_stopped": false}, "phases": {"f1": {"steps": 15630, "train_score_mean": 8.652934531469949, "train_loss_mean": 0.5076336446150861, "pair_count": 4951579114}, "f2": {"steps": 0, "train_score_mean": null, "train_loss_mean": null, "pair_count": 0}}, "config": {"hf": {"problem": "tsp", "hf_steps": 0, "hf_epochs": 10, "hf_instances_per_epoch": 100000, "train_problem_size": 100, "valid_problem_sizes": [100], "train_batch_size": 64, "pomo_size": 100, "learning_rate": 0.0003, "weight_decay": 1e-06, "alpha": 0.05, "device": "cuda:2", "seed": 1234, "num_validation_episodes": 10000, "validation_batch_size": 64, "generalization_penalty_weight": 1.0, "pool_version": "v0"}, "free_loss": {"f1_steps": 0, "total_train_steps": 15630, "f2_steps": 0, "f3_enabled": false}}, "loss_ir": {"name": "LogSigmoid_Adaptive_Margin_with_Cost_Variance_Coupling", "intuition": "Mode: combine. This loss function hybridizes the strongest adaptive mechanisms from both parents while introducing a new coupling to further stabilize the margin. It inherits the core `logsigmoid` loss structure and the adaptive margin scaled by `std(delta_logp)` from both parents. This ensures the loss is probabilistic and that the margin's magnitude is sensitive to the model's confidence. From Parent 1, it specifically inherits the inverse coupling between the loss temperature (`beta`) and this adaptive scale (`adaptive_scale`), which is a powerful stability trick: when the model is uncertain and the required margin is large, the loss function softens to prevent aggressive updates. The innovation of this child is a new coupling idea: it modulates the cost-dependent part of the margin using the variance of costs within the batch. Specifically, it inherits the `sigmoid(delta_cost / temperature)` term from Parent 0, but makes the `temperature` adaptive, as inspired by Parent 1. This new `cost_temp` is set to `1 + std(delta_cost)`, which normalizes the cost signal. When costs in a batch are very similar (low std), the sigmoid becomes steeper, making the margin more sensitive to small cost differences. When costs are highly varied (high std), the sigmoid flattens, preventing the margin from being dominated by a few pairs with large cost gaps. This creates a more robust and self-normalizing margin calculation.", "hyperparams": {"min_scale": 0.1, "max_scale": 5.0, "epsilon": 1e-08}, "operators_used": ["logsigmoid", "sigmoid", "clamp"]}}, "better_than_baseline": true}
{"generation": 3, "index": 3, "ir": {"name": "Adaptive_LogSigmoid_with_Dual_Adaptive_Margin", "intuition": "Mode: combine. This loss function hybridizes the adaptive mechanisms of both parents to create a more nuanced and stable preference learning objective. It inherits the core `logsigmoid` loss structure, which is common to both parents and provides a solid probabilistic foundation. From Parent 1, it inherits the idea of an adaptive beta (temperature) for the `logsigmoid` function, which is inversely proportional to the standard deviation of `delta_logp`. This helps to soften the loss and prevent aggressive updates when the model's outputs are highly variable (i.e., it is uncertain). From Parent 2, it inherits the idea of making the cost temperature adaptive, based on the standard deviation of `delta_cost`. This makes the margin's sensitivity to cost differences dependent on the diversity of costs in the current batch. \n\nThe first new coupling idea is a **dual-component margin**. Instead of choosing one parent's margin, we combine them additively. The margin is now `margin = margin_A + margin_B`, where `margin_A` is the scaled cost-sigmoid from Parent 1 (`adaptive_scale * sigmoid(delta_cost / temp)`) and `margin_B` is a similar term using the adaptive temperature from Parent 2 (`adaptive_cost_temp`). This allows the margin to be influenced by both a fixed temperature and a batch-adaptive one, providing both stability and responsiveness. The second new coupling is a **dynamic re-weighting of the margin components**. I introduce a hyperparameter `alpha` (e.g., 0.5) to blend the two margin terms: `margin = alpha * margin_A + (1 - alpha) * margin_B`. This allows for explicit control over the influence of the fixed-temperature margin versus the adaptive-temperature margin, making the design more flexible and tunable. The overall structure remains a Bradley-Terry model with a sophisticated, dynamically constructed margin.", "pseudocode": "1. Calculate the difference in log-probabilities: delta_logp = logp(a) - logp(b).\n2. Calculate the difference in costs: delta_cost = cost(b) - cost(a).\n3. Compute an adaptive scale based on the standard deviation of delta_logp, clipped for stability. This is used for both the margin and the loss temperature.\n4. Inherit from Parent 1: Calculate the first margin component (margin_A) using a fixed temperature for the cost sigmoid: `margin_A = adaptive_scale * sigmoid(delta_cost / fixed_temp)`.\n5. Inherit from Parent 2: Compute an adaptive cost temperature based on the standard deviation of delta_cost: `adaptive_cost_temp = std(delta_cost) + epsilon`.\n6. New Coupling 1: Calculate the second margin component (margin_B) using the adaptive cost temperature: `margin_B = adaptive_scale * sigmoid(delta_cost / adaptive_cost_temp)`.\n7. New Coupling 2: Combine the two margin components using a blending factor `alpha`: `margin = alpha * margin_A + (1 - alpha) * margin_B`.\n8. Inherit from Parent 1/2: Define an adaptive beta for the logsigmoid loss as the inverse of the adaptive scale: `beta = 1.0 / (adaptive_scale + epsilon)`.\n9. Compute the final loss argument: `loss_arg = delta_logp - margin`.\n10. Calculate the final loss using the scaled negative log-sigmoid function: `loss = -logsigmoid(beta * loss_arg)`.\n11. Return the mean of the loss across the batch.", "hyperparams": {"fixed_temp": 1.0, "alpha": 0.5, "min_scale": 0.1, "max_scale": 5.0, "epsilon": 1e-08}, "operators_used": ["logsigmoid", "sigmoid", "clamp"], "implementation_hint": {"expects": ["cost_a", "cost_b", "logp_a", "logp_b"], "returns": "scalar"}, "code": "def generated_loss(batch, model_output, extra):\n    \"\"\"\n    A logistic preference loss with a blended margin from two adaptive schemes.\n    \"\"\"\n    # Read tensors from the batch\n    cost_a = batch['cost_a']\n    cost_b = batch['cost_b']\n    logp_a = batch['log_prob_w']\n    logp_b = batch['log_prob_l']\n    weights = batch.get('weight')\n\n    # Read hyperparameters\n    hyperparams = extra['hyperparams']\n    fixed_temp = hyperparams.get('fixed_temp', 1.0)\n    alpha = hyperparams.get('alpha', 0.5)\n    min_scale = hyperparams.get('min_scale', 0.1)\n    max_scale = hyperparams.get('max_scale', 5.0)\n    epsilon = hyperparams.get('epsilon', 1e-8)\n\n    # 1. Calculate log-probability and cost differences\n    delta_logp = logp_a - logp_b\n    delta_cost = cost_b - cost_a\n\n    # 2. Compute adaptive scale from logp stats (common element)\n    if delta_logp.numel() > 1:\n        logp_std = delta_logp.detach().std()\n    else:\n        logp_std = torch.tensor(1.0, device=delta_logp.device)\n    adaptive_scale = torch.clamp(logp_std, min_scale, max_scale)\n\n    # 3. Inherit from Parent 1: Calculate margin_A with fixed temperature\n    cost_weight_A = torch.sigmoid(delta_cost / fixed_temp)\n    margin_A = adaptive_scale * cost_weight_A\n\n    # 4. Inherit from Parent 2: Calculate adaptive cost temperature\n    if delta_cost.numel() > 1:\n        adaptive_cost_temp = delta_cost.detach().std() + epsilon\n    else:\n        adaptive_cost_temp = torch.tensor(1.0, device=delta_cost.device)\n\n    # 5. New Coupling 1: Calculate margin_B with adaptive temperature\n    cost_weight_B = torch.sigmoid(delta_cost / adaptive_cost_temp)\n    margin_B = adaptive_scale * cost_weight_B\n\n    # 6. New Coupling 2: Blend the two margins\n    margin = alpha * margin_A + (1.0 - alpha) * margin_B\n    \n    # 7. Inherit from both: Adaptive beta for logsigmoid\n    beta = 1.0 / (adaptive_scale.detach() + epsilon)\n\n    # 8. Compute the loss argument\n    loss_arg = delta_logp - margin\n\n    # 9. Apply negative log-sigmoid loss\n    loss_per_pair = -torch.nn.functional.logsigmoid(beta * loss_arg)\n\n    # 10. Apply optional weights and compute the mean loss\n    if weights is not None:\n        loss = (loss_per_pair * weights).sum() / weights.sum().clamp(min=epsilon)\n    else:\n        loss = loss_per_pair.mean()\n\n    return loss", "theoretical_basis": "A Bradley-Terry logistic preference model where the target log-odds is a dynamically constructed margin. This margin is a blended sum of two components: one sensitive to cost differences via a fixed temperature, and another sensitive via a batch-adaptive temperature. The loss function's steepness (beta) is inversely coupled to the model's output variance, ensuring stability by softening the loss during periods of high uncertainty."}, "fitness": {"hf_like_score": 7.870896125793457, "validation_objective": 7.8704054779052735, "generalization_penalty": 0.0004906478881832044, "generalization_objectives": {"100": 7.870896125793457}, "train_score_mean": 8.597075912331588, "train_loss_mean": 0.5132152538782346, "pair_count": 4951579292, "early_eval": {"enabled": true, "steps": 100, "baseline_validation_objective": 8.412976063537597, "candidate_validation_objective": 8.334532046508789, "early_stopped": false}, "phases": {"f1": {"steps": 15630, "train_score_mean": 8.597075912331588, "train_loss_mean": 0.5132152538782346, "pair_count": 4951579292}, "f2": {"steps": 0, "train_score_mean": null, "train_loss_mean": null, "pair_count": 0}}, "config": {"hf": {"problem": "tsp", "hf_steps": 0, "hf_epochs": 10, "hf_instances_per_epoch": 100000, "train_problem_size": 100, "valid_problem_sizes": [100], "train_batch_size": 64, "pomo_size": 100, "learning_rate": 0.0003, "weight_decay": 1e-06, "alpha": 0.05, "device": "cuda:3", "seed": 1234, "num_validation_episodes": 10000, "validation_batch_size": 64, "generalization_penalty_weight": 1.0, "pool_version": "v0"}, "free_loss": {"f1_steps": 0, "total_train_steps": 15630, "f2_steps": 0, "f3_enabled": false}}, "loss_ir": {"name": "Adaptive_LogSigmoid_with_Dual_Adaptive_Margin", "intuition": "Mode: combine. This loss function hybridizes the adaptive mechanisms of both parents to create a more nuanced and stable preference learning objective. It inherits the core `logsigmoid` loss structure, which is common to both parents and provides a solid probabilistic foundation. From Parent 1, it inherits the idea of an adaptive beta (temperature) for the `logsigmoid` function, which is inversely proportional to the standard deviation of `delta_logp`. This helps to soften the loss and prevent aggressive updates when the model's outputs are highly variable (i.e., it is uncertain). From Parent 2, it inherits the idea of making the cost temperature adaptive, based on the standard deviation of `delta_cost`. This makes the margin's sensitivity to cost differences dependent on the diversity of costs in the current batch. \n\nThe first new coupling idea is a **dual-component margin**. Instead of choosing one parent's margin, we combine them additively. The margin is now `margin = margin_A + margin_B`, where `margin_A` is the scaled cost-sigmoid from Parent 1 (`adaptive_scale * sigmoid(delta_cost / temp)`) and `margin_B` is a similar term using the adaptive temperature from Parent 2 (`adaptive_cost_temp`). This allows the margin to be influenced by both a fixed temperature and a batch-adaptive one, providing both stability and responsiveness. The second new coupling is a **dynamic re-weighting of the margin components**. I introduce a hyperparameter `alpha` (e.g., 0.5) to blend the two margin terms: `margin = alpha * margin_A + (1 - alpha) * margin_B`. This allows for explicit control over the influence of the fixed-temperature margin versus the adaptive-temperature margin, making the design more flexible and tunable. The overall structure remains a Bradley-Terry model with a sophisticated, dynamically constructed margin.", "hyperparams": {"fixed_temp": 1.0, "alpha": 0.5, "min_scale": 0.1, "max_scale": 5.0, "epsilon": 1e-08}, "operators_used": ["logsigmoid", "sigmoid", "clamp"]}}, "better_than_baseline": true}
{"generation": 3, "index": 4, "ir": {"name": "Adaptive_Temp_Sigmoid_Margin_LogSigmoid_Loss", "intuition": "Mode: combine. This loss function hybridizes the strongest adaptive mechanisms from both parents while introducing a new coupling for enhanced stability and performance. It inherits the core `logsigmoid` loss structure, which provides a smooth, probabilistic objective. From Parent 1, it adopts the innovative adaptive temperature for the cost-sigmoid (`adaptive_temp = std(delta_cost)`), making the margin's sensitivity to cost differences dependent on the diversity of costs within a batch. From Parent 0, it inherits the idea of an adaptive beta (temperature) for the main `logsigmoid` loss, which is inversely proportional to a scale factor, thereby softening the loss when the model is uncertain. The key new coupling idea is to redefine this scale factor. Instead of being based on the standard deviation of `delta_logp` (which can be noisy), the scale is now based on the *mean* of `delta_logp`, specifically `1.0 / mean(abs(delta_logp))`. This grounds the loss's temperature in the average magnitude of the model's preference scores for a batch, rather than their spread. This provides a more stable and direct measure of the model's current 'confidence scale'. The margin itself is a simple sigmoid function of the cost difference, scaled by the adaptive temperature, a proven component from the parents.", "pseudocode": "1. Calculate the difference in log-probabilities: delta_logp = logp(a) - logp(b).\n2. Calculate the difference in costs: delta_cost = cost(b) - cost(a).\n3. Inherit from Parent 1: Compute an adaptive temperature for the margin based on the standard deviation of cost differences in the batch: adaptive_temp = std(delta_cost) + epsilon.\n4. Calculate a cost-dependent margin by applying a sigmoid function to the cost difference, scaled by the adaptive temperature: margin = sigmoid(delta_cost / adaptive_temp).\n5. New Coupling: Compute an adaptive beta for the main loss. First, calculate a scale factor based on the inverse of the mean absolute `delta_logp`: scale = 1.0 / (mean(abs(delta_logp)) + epsilon). Then, clip this scale for stability. Finally, define beta = base_beta / scale.\n6. Compute the argument for the logistic loss: loss_arg = delta_logp - margin. The goal is to push delta_logp to be greater than the margin.\n7. Apply the negative log-sigmoid function, scaled by the adaptive beta: loss = -logsigmoid(beta * loss_arg).\n8. Return the mean of the loss across the batch.", "hyperparams": {"beta_base": 1.0, "min_scale": 0.1, "max_scale": 10.0, "epsilon": 1e-08}, "operators_used": ["logsigmoid", "sigmoid", "clamp"], "implementation_hint": {"expects": ["cost_a", "cost_b", "logp_a", "logp_b"], "returns": "scalar"}, "code": "def generated_loss(batch, model_output, extra):\n    \"\"\"\n    A logistic preference loss with a cost-variance-aware sigmoid margin and a loss temperature\n    inversely coupled to the mean magnitude of log-probability differences.\n    \"\"\"\n    # Read tensors from the batch\n    cost_a = batch['cost_a']\n    cost_b = batch['cost_b']\n    logp_a = batch['log_prob_w']\n    logp_b = batch['log_prob_l']\n    weights = batch.get('weight')\n\n    # Read hyperparameters\n    hyperparams = extra['hyperparams']\n    beta_base = hyperparams.get('beta_base', 1.0)\n    min_scale = hyperparams.get('min_scale', 0.1)\n    max_scale = hyperparams.get('max_scale', 10.0)\n    epsilon = hyperparams.get('epsilon', 1e-8)\n\n    # Step 1: Calculate log-probability and cost differences\n    delta_logp = logp_a - logp_b\n    delta_cost = cost_b - cost_a\n\n    # Step 2: Adaptive temperature for margin from cost stats (Inherited from Parent 1)\n    if delta_cost.numel() > 1:\n        adaptive_temp = delta_cost.detach().std() + epsilon\n    else:\n        adaptive_temp = torch.tensor(1.0, device=delta_cost.device)\n\n    # Step 3: Calculate cost-dependent margin\n    margin = torch.sigmoid(delta_cost / adaptive_temp)\n\n    # Step 4: New Coupling - Adaptive beta from mean |delta_logp|\n    # This is more stable than using std(delta_logp).\n    if delta_logp.numel() > 0:\n        mean_abs_logp_diff = delta_logp.abs().detach().mean()\n    else:\n        mean_abs_logp_diff = torch.tensor(1.0, device=delta_logp.device)\n    \n    # The scale is inversely related to the average preference strength.\n    scale = 1.0 / (mean_abs_logp_diff + epsilon)\n    stable_scale = torch.clamp(scale, min=min_scale, max=max_scale)\n    \n    # Beta is inversely coupled to the scale (similar to Parent 0's mechanism)\n    beta = beta_base / (stable_scale.detach() + epsilon)\n\n    # Step 5: Compute loss argument\n    loss_arg = delta_logp - margin\n\n    # Step 6: Apply negative log-sigmoid loss (core idea from both parents)\n    loss_per_pair = -torch.nn.functional.logsigmoid(beta * loss_arg)\n\n    # Step 7: Apply optional weights and compute the mean loss\n    if weights is not None and weights.sum() > 0:\n        loss = (loss_per_pair * weights).sum() / weights.sum()\n    else:\n        loss = loss_per_pair.mean()\n\n    return loss", "theoretical_basis": "A Bradley-Terry logistic preference model where the target log-odds is a dynamic margin. The margin's shape is controlled by a cost-variance-aware temperature (from Parent 1). The steepness of the logistic loss itself is controlled by an adaptive beta, which is inversely coupled to a new scale factor derived from the mean magnitude of the model's preference scores, providing a stable, confidence-aware learning rate."}, "fitness": {"hf_like_score": 11.665540827941895, "validation_objective": 11.665540827941895, "generalization_penalty": 0.0, "generalization_objectives": {"100": 11.661616734313965}, "train_score_mean": 23.987986268997194, "train_loss_mean": 9.466293950080871, "pair_count": 31679980, "early_eval": {"enabled": true, "steps": 100, "baseline_validation_objective": 8.412976063537597, "candidate_validation_objective": 11.665540827941895, "early_stopped": true}, "phases": {"f1": {"steps": 15630, "train_score_mean": 23.987986268997194, "train_loss_mean": 9.466293950080871, "pair_count": 31679980}, "f2": {"steps": 0, "train_score_mean": null, "train_loss_mean": null, "pair_count": 0}}, "config": {"hf": {"problem": "tsp", "hf_steps": 0, "hf_epochs": 10, "hf_instances_per_epoch": 100000, "train_problem_size": 100, "valid_problem_sizes": [100], "train_batch_size": 64, "pomo_size": 100, "learning_rate": 0.0003, "weight_decay": 1e-06, "alpha": 0.05, "device": "cuda:0", "seed": 1234, "num_validation_episodes": 10000, "validation_batch_size": 64, "generalization_penalty_weight": 1.0, "pool_version": "v0"}, "free_loss": {"f1_steps": 0, "total_train_steps": 15630, "f2_steps": 0, "f3_enabled": false}}, "loss_ir": {"name": "Adaptive_Temp_Sigmoid_Margin_LogSigmoid_Loss", "intuition": "Mode: combine. This loss function hybridizes the strongest adaptive mechanisms from both parents while introducing a new coupling for enhanced stability and performance. It inherits the core `logsigmoid` loss structure, which provides a smooth, probabilistic objective. From Parent 1, it adopts the innovative adaptive temperature for the cost-sigmoid (`adaptive_temp = std(delta_cost)`), making the margin's sensitivity to cost differences dependent on the diversity of costs within a batch. From Parent 0, it inherits the idea of an adaptive beta (temperature) for the main `logsigmoid` loss, which is inversely proportional to a scale factor, thereby softening the loss when the model is uncertain. The key new coupling idea is to redefine this scale factor. Instead of being based on the standard deviation of `delta_logp` (which can be noisy), the scale is now based on the *mean* of `delta_logp`, specifically `1.0 / mean(abs(delta_logp))`. This grounds the loss's temperature in the average magnitude of the model's preference scores for a batch, rather than their spread. This provides a more stable and direct measure of the model's current 'confidence scale'. The margin itself is a simple sigmoid function of the cost difference, scaled by the adaptive temperature, a proven component from the parents.", "hyperparams": {"beta_base": 1.0, "min_scale": 0.1, "max_scale": 10.0, "epsilon": 1e-08}, "operators_used": ["logsigmoid", "sigmoid", "clamp"]}}, "better_than_baseline": false}
{"generation": 3, "index": 5, "ir": {"name": "Sigmoid_Margin_Log_Loss_with_Dual_Adaptive_Temperature", "intuition": "Mode: combine. This child loss function aims to create a more robust and self-regulating preference objective by combining and refining the adaptive mechanisms from both parents. It inherits the core Bradley-Terry `logsigmoid` structure and the concept of a dynamic margin, which are common to both parents and have proven effective. From Parent 1, it inherits the use of a `sigmoid(delta_cost)` term to create a bounded, cost-sensitive margin. From Parent 2, it inherits the idea of making the temperature of this cost sigmoid adaptive, but instead of using `std(delta_cost)`, this child uses `std(logp_a)` and `std(logp_b)` to make the temperature sensitive to the overall confidence range of the model's outputs for both winning and losing candidates, not just their difference. This is the first new coupling. The second new coupling is a novel formulation for the adaptive beta (the temperature of the `logsigmoid` loss itself). Instead of being inversely proportional to `std(delta_logp)` (as seen in both parents), it is now inversely proportional to the *margin* itself. This creates a direct feedback loop: when the required margin is large (due to high model confidence or large cost gaps), the loss curve softens (beta decreases), preventing instability and overly aggressive updates. When the margin is small, the loss sharpens, allowing for finer-grained learning. This direct coupling of loss temperature to the margin target is a key innovation designed for enhanced stability and performance.", "pseudocode": "1. Calculate the difference in log-probabilities: delta_logp = logp(a) - logp(b).\n2. Calculate the difference in costs: delta_cost = cost(b) - cost(a).\n3. New Coupling 1: Compute an adaptive temperature for the cost-sigmoid. This temperature is based on the standard deviation of the raw log-probabilities of both winning and losing candidates in the batch, making it sensitive to the model's overall output variance. `adaptive_temp = (std(logp_a) + std(logp_b)) / 2 + epsilon`.\n4. Inherit from Parent 1: Calculate a cost-dependent margin component using a sigmoid function, scaled by the new adaptive temperature: `cost_margin_component = sigmoid(delta_cost / adaptive_temp)`.\n5. Compute the final dynamic margin, scaled by a hyperparameter: `margin = margin_scale * cost_margin_component`.\n6. New Coupling 2: Calculate an adaptive beta for the logsigmoid loss. This beta is inversely proportional to the mean of the calculated margin across the batch. `adaptive_beta = 1.0 / (mean(margin) + epsilon)`.\n7. Compute the argument for the logistic loss: `loss_arg = delta_logp - margin`.\n8. Apply the negative log-sigmoid function, scaled by the adaptive beta: `loss = -logsigmoid(adaptive_beta * loss_arg)`.\n9. Return the mean of the loss across the batch.", "hyperparams": {"margin_scale": 0.5, "epsilon": 1e-08}, "operators_used": ["sigmoid", "logsigmoid"], "implementation_hint": {"expects": ["cost_a", "cost_b", "logp_a", "logp_b"], "returns": "scalar"}, "code": "def generated_loss(batch, model_output, extra):\n    \"\"\"\n    A logistic preference loss with a sigmoid margin, where both the cost-sigmoid's\n    temperature and the loss's temperature (beta) are adaptive.\n    \"\"\"\n    # Read tensors from the batch\n    cost_a = batch['cost_a']\n    cost_b = batch['cost_b']\n    logp_a = batch['log_prob_w']\n    logp_b = batch['log_prob_l']\n    weights = batch.get('weight')\n\n    # Read hyperparameters\n    hyperparams = extra['hyperparams']\n    margin_scale = hyperparams.get('margin_scale', 0.5)\n    epsilon = hyperparams.get('epsilon', 1e-8)\n\n    # 1. Calculate log-probability and cost differences\n    delta_logp = logp_a - logp_b\n    delta_cost = cost_b - cost_a\n\n    # 2. New Coupling 1: Adaptive temperature from raw logp stats\n    if logp_a.numel() > 1:\n        # Use variance of both chosen and rejected responses for a more stable temp\n        logp_a_std = logp_a.detach().std()\n        logp_b_std = logp_b.detach().std()\n        adaptive_temp = (logp_a_std + logp_b_std) / 2.0 + epsilon\n    else:\n        adaptive_temp = torch.tensor(1.0, device=delta_logp.device)\n\n    # 3. Cost-dependent margin component (from Parent 1) with new adaptive temperature\n    cost_margin_component = torch.sigmoid(delta_cost / adaptive_temp)\n    \n    # 4. Compute the final dynamic margin\n    margin = margin_scale * cost_margin_component\n\n    # 5. New Coupling 2: Beta inversely coupled to the margin itself\n    # Detach margin to avoid second-order gradients through this path\n    mean_margin = margin.detach().mean()\n    adaptive_beta = 1.0 / (mean_margin + epsilon)\n\n    # 6. Compute loss argument for logistic loss\n    loss_arg = delta_logp - margin\n\n    # 7. Apply negative log-sigmoid (from both parents)\n    loss_per_pair = -torch.nn.functional.logsigmoid(adaptive_beta * loss_arg)\n\n    # 8. Apply optional weights and compute the mean loss\n    if weights is not None:\n        loss = (loss_per_pair * weights).sum() / weights.sum().clamp(min=epsilon)\n    else:\n        loss = loss_per_pair.mean()\n\n    return loss", "theoretical_basis": "A Bradley-Terry logistic preference model where the target log-odds is a dynamic margin. The margin's sensitivity to cost is regulated by a temperature adapted to the model's output variance. The steepness of the logistic loss itself is inversely coupled to the magnitude of this target margin, creating a self-regulating system that softens the loss when the learning target is aggressive."}, "fitness": {"hf_like_score": 22.393365756225585, "validation_objective": 22.388064099121095, "generalization_penalty": 0.005301657104489266, "generalization_objectives": {"100": 22.393365756225585}, "train_score_mean": 25.300205430984498, "train_loss_mean": 5.188506669998169, "pair_count": 31679977, "early_eval": {"enabled": true, "steps": 100, "baseline_validation_objective": 8.412976063537597, "candidate_validation_objective": 22.388064099121095, "early_stopped": true}, "phases": {"f1": {"steps": 15630, "train_score_mean": 25.300205430984498, "train_loss_mean": 5.188506669998169, "pair_count": 31679977}, "f2": {"steps": 0, "train_score_mean": null, "train_loss_mean": null, "pair_count": 0}}, "config": {"hf": {"problem": "tsp", "hf_steps": 0, "hf_epochs": 10, "hf_instances_per_epoch": 100000, "train_problem_size": 100, "valid_problem_sizes": [100], "train_batch_size": 64, "pomo_size": 100, "learning_rate": 0.0003, "weight_decay": 1e-06, "alpha": 0.05, "device": "cuda:1", "seed": 1234, "num_validation_episodes": 10000, "validation_batch_size": 64, "generalization_penalty_weight": 1.0, "pool_version": "v0"}, "free_loss": {"f1_steps": 0, "total_train_steps": 15630, "f2_steps": 0, "f3_enabled": false}}, "loss_ir": {"name": "Sigmoid_Margin_Log_Loss_with_Dual_Adaptive_Temperature", "intuition": "Mode: combine. This child loss function aims to create a more robust and self-regulating preference objective by combining and refining the adaptive mechanisms from both parents. It inherits the core Bradley-Terry `logsigmoid` structure and the concept of a dynamic margin, which are common to both parents and have proven effective. From Parent 1, it inherits the use of a `sigmoid(delta_cost)` term to create a bounded, cost-sensitive margin. From Parent 2, it inherits the idea of making the temperature of this cost sigmoid adaptive, but instead of using `std(delta_cost)`, this child uses `std(logp_a)` and `std(logp_b)` to make the temperature sensitive to the overall confidence range of the model's outputs for both winning and losing candidates, not just their difference. This is the first new coupling. The second new coupling is a novel formulation for the adaptive beta (the temperature of the `logsigmoid` loss itself). Instead of being inversely proportional to `std(delta_logp)` (as seen in both parents), it is now inversely proportional to the *margin* itself. This creates a direct feedback loop: when the required margin is large (due to high model confidence or large cost gaps), the loss curve softens (beta decreases), preventing instability and overly aggressive updates. When the margin is small, the loss sharpens, allowing for finer-grained learning. This direct coupling of loss temperature to the margin target is a key innovation designed for enhanced stability and performance.", "hyperparams": {"margin_scale": 0.5, "epsilon": 1e-08}, "operators_used": ["sigmoid", "logsigmoid"]}}, "better_than_baseline": false}
{"generation": 3, "index": 6, "ir": {"name": "Sigmoid_Cost_Margin_LogSigmoid_Loss_with_Dual_Adaptive_Beta", "intuition": "The candidate failed due to a compile error (`E_COMPILE_ERROR`) because it contained an `import torch` statement, which is not allowed. The core logic of the loss function, which uses adaptive scaling and a clamped beta to prevent numerical explosions, is sound. The fix is to simply remove the forbidden `import torch` line. The rest of the implementation, including the clamping of `beta_adaptive` to prevent the `E_LOSS_OUT_OF_RANGE` error it was originally designed to fix, remains unchanged.", "pseudocode": "1. Calculate the difference in log-probabilities: delta_logp = logp(a) - logp(b).\n2. Calculate the difference in costs: delta_cost = cost(b) - cost(a).\n3. Compute an adaptive scale for the margin based on the standard deviation of delta_logp, clipped for stability.\n4. Compute a cost-dependent margin component using a sigmoid function on the cost difference.\n5. Calculate the final margin by multiplying the adaptive scale and the cost-margin: margin = adaptive_scale * sigmoid(delta_cost / temperature).\n6. Compute the dual adaptive beta:\n   a. Compute beta component from logp standard deviation: beta_logp = 1.0 / (std(delta_logp) + epsilon).\n   b. Compute beta component from cost standard deviation: beta_cost = 1.0 / (std(delta_cost) + epsilon).\n   c. Combine them: beta_adaptive = beta_logp * beta_cost.\n   d. Clamp the combined beta to a maximum value for stability: beta_adaptive = clamp(beta_adaptive, 0, max_beta).\n7. Apply a `tanh` function to the log-probability difference to bound its contribution: bounded_delta_logp = tanh_scale * tanh(delta_logp).\n8. Compute the argument for the loss function: loss_arg = bounded_delta_logp - margin.\n9. Calculate the final loss using the negative log-sigmoid function, scaled by the stabilized dual adaptive beta: loss = -logsigmoid(beta_adaptive * loss_arg).\n10. Return the mean of the loss across the batch.", "hyperparams": {"temperature": 1.0, "min_scale": 0.1, "max_scale": 5.0, "tanh_scale": 2.0, "epsilon": 1e-08, "max_beta": 10.0}, "operators_used": ["logsigmoid", "sigmoid", "tanh", "clamp"], "implementation_hint": {"expects": ["PyTorch tensors `cost_a`, `cost_b`, `logp_a`, `logp_b`, and optional `weights`. The `torch` module is pre-imported."], "returns": "A single scalar tensor representing the mean loss."}, "code": "def generated_loss(batch, model_output, extra):\n    \"\"\"\n    A logistic preference loss with a sigmoid cost margin and adaptive scaling.\n    The logistic temperature (beta) is dually adapted to both logp and cost variance.\n    A tanh function clips the logp difference to stabilize gradients.\n    \"\"\"\n    # Read tensors from the batch\n    cost_a = batch['cost_a']\n    cost_b = batch['cost_b']\n    logp_a = batch['log_prob_w']\n    logp_b = batch['log_prob_l']\n    weights = batch.get('weight')\n\n    # Read hyperparameters\n    hyperparams = extra['hyperparams']\n    temperature = hyperparams.get('temperature', 1.0)\n    min_scale = hyperparams.get('min_scale', 0.1)\n    max_scale = hyperparams.get('max_scale', 5.0)\n    tanh_scale = hyperparams.get('tanh_scale', 2.0)\n    epsilon = hyperparams.get('epsilon', 1e-8)\n    max_beta = hyperparams.get('max_beta', 10.0)\n\n    # 1. Calculate log-probability and cost differences\n    delta_logp = logp_a - logp_b\n    delta_cost = cost_b - cost_a\n\n    # 2. Compute statistics for adaptive components\n    if delta_logp.numel() > 1:\n        logp_std = delta_logp.detach().std()\n        cost_std = delta_cost.detach().std()\n    else:\n        logp_std = torch.tensor(1.0, device=delta_logp.device)\n        cost_std = torch.tensor(1.0, device=delta_logp.device)\n\n    # 3. Compute adaptive margin (inherited from both parents)\n    adaptive_scale = torch.clamp(logp_std, min_scale, max_scale)\n    cost_margin_component = torch.sigmoid(delta_cost / temperature)\n    margin = adaptive_scale * cost_margin_component\n\n    # 4. New Coupling 1: Dual-component adaptive beta\n    beta_logp = 1.0 / (logp_std + epsilon)\n    beta_cost = 1.0 / (cost_std + epsilon)\n    beta_combined = beta_logp * beta_cost\n    # Clamp beta_adaptive to prevent explosion from small std dev\n    beta_adaptive = torch.clamp(beta_combined, max=max_beta).detach()\n\n    # 5. New Coupling 2: Tanh clipping on delta_logp\n    bounded_delta_logp = tanh_scale * torch.tanh(delta_logp)\n\n    # 6. Compute loss argument\n    loss_arg = bounded_delta_logp - margin\n\n    # 7. Apply negative log-sigmoid loss\n    loss_per_pair = -torch.nn.functional.logsigmoid(beta_adaptive * loss_arg)\n\n    # 8. Apply optional weights and compute the mean loss\n    if weights is not None:\n        loss = (loss_per_pair * weights).sum() / weights.sum().clamp(min=epsilon)\n    else:\n        loss = loss_per_pair.mean()\n\n    return loss", "theoretical_basis": ""}, "fitness": {"hf_like_score": 8.094621615600586, "validation_objective": 8.093932995605469, "generalization_penalty": 0.0006886199951168237, "generalization_objectives": {"100": 8.094621615600586}, "train_score_mean": 16.060526795487945, "train_loss_mean": 0.8020891982892806, "pair_count": 4951581336, "early_eval": {"enabled": true, "steps": 100, "baseline_validation_objective": 8.412976063537597, "candidate_validation_objective": 8.360379049682617, "early_stopped": false}, "phases": {"f1": {"steps": 15630, "train_score_mean": 16.060526795487945, "train_loss_mean": 0.8020891982892806, "pair_count": 4951581336}, "f2": {"steps": 0, "train_score_mean": null, "train_loss_mean": null, "pair_count": 0}}, "config": {"hf": {"problem": "tsp", "hf_steps": 0, "hf_epochs": 10, "hf_instances_per_epoch": 100000, "train_problem_size": 100, "valid_problem_sizes": [100], "train_batch_size": 64, "pomo_size": 100, "learning_rate": 0.0003, "weight_decay": 1e-06, "alpha": 0.05, "device": "cuda:2", "seed": 1234, "num_validation_episodes": 10000, "validation_batch_size": 64, "generalization_penalty_weight": 1.0, "pool_version": "v0"}, "free_loss": {"f1_steps": 0, "total_train_steps": 15630, "f2_steps": 0, "f3_enabled": false}}, "loss_ir": {"name": "Sigmoid_Cost_Margin_LogSigmoid_Loss_with_Dual_Adaptive_Beta", "intuition": "The candidate failed due to a compile error (`E_COMPILE_ERROR`) because it contained an `import torch` statement, which is not allowed. The core logic of the loss function, which uses adaptive scaling and a clamped beta to prevent numerical explosions, is sound. The fix is to simply remove the forbidden `import torch` line. The rest of the implementation, including the clamping of `beta_adaptive` to prevent the `E_LOSS_OUT_OF_RANGE` error it was originally designed to fix, remains unchanged.", "hyperparams": {"temperature": 1.0, "min_scale": 0.1, "max_scale": 5.0, "tanh_scale": 2.0, "epsilon": 1e-08, "max_beta": 10.0}, "operators_used": ["logsigmoid", "sigmoid", "tanh", "clamp"]}}, "better_than_baseline": false}
{"generation": 3, "index": 7, "ir": {"name": "Adaptive_Margin_LogSigmoid_with_Cost_Variance_Temp", "intuition": "Mode: combine. This loss function hybridizes the most successful components from both parents while introducing a new coupling for enhanced stability and adaptiveness. It inherits the core `logsigmoid` loss structure from both parents, which is a proven, smooth, and probabilistic objective. It also inherits the powerful adaptive margin concept from both parents, where the margin is dynamically calculated. Specifically, it uses the `adaptive_scale` based on `std(delta_logp)` (from both Parent 0 and 1) and a `sigmoid(delta_cost)` term (also from both) to form the margin `margin = adaptive_scale * sigmoid(delta_cost / temp)`. This ensures the margin scales with both model uncertainty and cost difference.\n\nFrom Parent 1, it specifically inherits the idea of making the temperature of the cost-sigmoid adaptive. However, instead of using a fixed hyperparameter for this temperature (like Parent 0) or deriving it from cost standard deviation (like Parent 1), it introduces a new coupling idea: the temperature `cost_temp` is set to the *variance* of the log-probability differences (`logp_var = std(delta_logp)^2`). This couples the cost sensitivity directly to the model's output variance. When the model is uncertain (high variance), the temperature increases, softening the sigmoid and making the margin less sensitive to small cost differences. When the model is confident (low variance), the temperature decreases, sharpening the sigmoid and making the margin more responsive to the cost gap. This prevents the margin from becoming too aggressive when the model is still learning.\n\nA second new coupling idea is to add a small, constant `base_margin` to the dynamic margin. This ensures that even for very small cost differences, there is still a minimal preference signal, preventing the loss from becoming zero and stalling learning in ambiguous cases. The final loss is `-logsigmoid(delta_logp - (dynamic_margin + base_margin))`, which elegantly combines these ideas.", "pseudocode": "1. Calculate the difference in log-probabilities: delta_logp = logp(a) - logp(b).\n2. Calculate the difference in costs: delta_cost = cost(b) - cost(a).\n3. Compute the standard deviation of log-probability differences in the batch: logp_std = std(delta_logp).\n4. Compute an adaptive scale based on logp_std, clipped for stability. This is inherited from both parents.\n5. Introduce the first new coupling: define an adaptive temperature for the cost-sigmoid as the variance of the log-probability differences: cost_temp = logp_std^2 + epsilon.\n6. Calculate a cost-dependent weight using a sigmoid function with the new adaptive temperature: cost_weight = sigmoid(delta_cost / cost_temp).\n7. Calculate the dynamic margin by multiplying the adaptive scale and the cost weight: dynamic_margin = adaptive_scale * cost_weight.\n8. Introduce the second new coupling: add a small, constant base_margin to the dynamic margin to ensure a minimal preference signal: margin = dynamic_margin + base_margin.\n9. Compute the argument for the loss function: loss_arg = delta_logp - margin.\n10. Apply the negative log-sigmoid function: loss = -logsigmoid(loss_arg).\n11. Return the mean of the loss across the batch.", "hyperparams": {"base_margin": 0.05, "min_scale": 0.1, "max_scale": 5.0, "epsilon": 1e-08}, "operators_used": ["logsigmoid", "sigmoid", "clamp"], "implementation_hint": {"expects": ["cost_a", "cost_b", "logp_a", "logp_b"], "returns": "scalar"}, "code": "def generated_loss(batch, model_output, extra):\n    \"\"\"\n    A logistic preference loss where the margin's scale is adapted from logp std,\n    and the cost-sigmoid's temperature is adapted from logp variance.\n    \"\"\"\n    # Read tensors from the batch\n    cost_a = batch['cost_a']\n    cost_b = batch['cost_b']\n    logp_a = batch['log_prob_w']\n    logp_b = batch['log_prob_l']\n    weights = batch.get('weight')\n\n    # Read hyperparameters\n    hyperparams = extra['hyperparams']\n    base_margin = hyperparams.get('base_margin', 0.05)\n    min_scale = hyperparams.get('min_scale', 0.1)\n    max_scale = hyperparams.get('max_scale', 5.0)\n    epsilon = hyperparams.get('epsilon', 1e-8)\n\n    # 1. Calculate log-probability and cost differences\n    delta_logp = logp_a - logp_b\n    delta_cost = cost_b - cost_a\n\n    # 2. Compute adaptive scale from logp stats (inherited from both parents)\n    if delta_logp.numel() > 1:\n        logp_std = delta_logp.detach().std()\n    else:\n        logp_std = torch.tensor(1.0, device=delta_logp.device)\n    adaptive_scale = torch.clamp(logp_std, min_scale, max_scale)\n\n    # 3. New Coupling 1: Adaptive temperature from logp variance\n    cost_temp = logp_std.pow(2) + epsilon\n\n    # 4. Cost-dependent weight using sigmoid (inherited) with new adaptive temp\n    cost_weight = torch.sigmoid(delta_cost / cost_temp)\n\n    # 5. Calculate the dynamic margin\n    dynamic_margin = adaptive_scale * cost_weight\n\n    # 6. New Coupling 2: Add a constant base margin\n    margin = dynamic_margin + base_margin\n\n    # 7. Compute loss argument for logistic loss\n    loss_arg = delta_logp - margin\n\n    # 8. Apply negative log-sigmoid (inherited from both parents)\n    loss_per_pair = -torch.nn.functional.logsigmoid(loss_arg)\n\n    # 9. Apply optional weights and compute the mean loss\n    if weights is not None:\n        loss = (loss_per_pair * weights).sum() / weights.sum().clamp(min=epsilon)\n    else:\n        loss = loss_per_pair.mean()\n\n    return loss", "theoretical_basis": "A Bradley-Terry logistic preference model where the target log-odds are shifted by a dynamic margin. This margin has two components: a dynamic part proportional to both the model's output variance and a sigmoid of the cost difference, and a small constant base. The temperature of the cost-sigmoid is coupled to the model's output variance, making the margin's sensitivity to cost adaptive to model confidence."}, "fitness": {"hf_like_score": 10.214339724731445, "validation_objective": 10.214339724731445, "generalization_penalty": 0.0, "generalization_objectives": {"100": 10.210481228637695}, "train_score_mean": 22.29931282043457, "train_loss_mean": 2.5685005044937133, "pair_count": 31679986, "early_eval": {"enabled": true, "steps": 100, "baseline_validation_objective": 8.412976063537597, "candidate_validation_objective": 10.214339724731445, "early_stopped": true}, "phases": {"f1": {"steps": 15630, "train_score_mean": 22.29931282043457, "train_loss_mean": 2.5685005044937133, "pair_count": 31679986}, "f2": {"steps": 0, "train_score_mean": null, "train_loss_mean": null, "pair_count": 0}}, "config": {"hf": {"problem": "tsp", "hf_steps": 0, "hf_epochs": 10, "hf_instances_per_epoch": 100000, "train_problem_size": 100, "valid_problem_sizes": [100], "train_batch_size": 64, "pomo_size": 100, "learning_rate": 0.0003, "weight_decay": 1e-06, "alpha": 0.05, "device": "cuda:3", "seed": 1234, "num_validation_episodes": 10000, "validation_batch_size": 64, "generalization_penalty_weight": 1.0, "pool_version": "v0"}, "free_loss": {"f1_steps": 0, "total_train_steps": 15630, "f2_steps": 0, "f3_enabled": false}}, "loss_ir": {"name": "Adaptive_Margin_LogSigmoid_with_Cost_Variance_Temp", "intuition": "Mode: combine. This loss function hybridizes the most successful components from both parents while introducing a new coupling for enhanced stability and adaptiveness. It inherits the core `logsigmoid` loss structure from both parents, which is a proven, smooth, and probabilistic objective. It also inherits the powerful adaptive margin concept from both parents, where the margin is dynamically calculated. Specifically, it uses the `adaptive_scale` based on `std(delta_logp)` (from both Parent 0 and 1) and a `sigmoid(delta_cost)` term (also from both) to form the margin `margin = adaptive_scale * sigmoid(delta_cost / temp)`. This ensures the margin scales with both model uncertainty and cost difference.\n\nFrom Parent 1, it specifically inherits the idea of making the temperature of the cost-sigmoid adaptive. However, instead of using a fixed hyperparameter for this temperature (like Parent 0) or deriving it from cost standard deviation (like Parent 1), it introduces a new coupling idea: the temperature `cost_temp` is set to the *variance* of the log-probability differences (`logp_var = std(delta_logp)^2`). This couples the cost sensitivity directly to the model's output variance. When the model is uncertain (high variance), the temperature increases, softening the sigmoid and making the margin less sensitive to small cost differences. When the model is confident (low variance), the temperature decreases, sharpening the sigmoid and making the margin more responsive to the cost gap. This prevents the margin from becoming too aggressive when the model is still learning.\n\nA second new coupling idea is to add a small, constant `base_margin` to the dynamic margin. This ensures that even for very small cost differences, there is still a minimal preference signal, preventing the loss from becoming zero and stalling learning in ambiguous cases. The final loss is `-logsigmoid(delta_logp - (dynamic_margin + base_margin))`, which elegantly combines these ideas.", "hyperparams": {"base_margin": 0.05, "min_scale": 0.1, "max_scale": 5.0, "epsilon": 1e-08}, "operators_used": ["logsigmoid", "sigmoid", "clamp"]}}, "better_than_baseline": false}
{"generation": 4, "index": 0, "ir": {"name": "Blended_Adaptive_Margin_LogSigmoid", "intuition": "Mode: combine. This loss function aims to synthesize the best adaptive mechanisms from both parents into a more robust and flexible objective. It inherits the core Bradley-Terry structure using a `logsigmoid` loss, a common element in both parents and many successful preference models. \n\nFrom Parent 1 (`Adaptive_LogSigmoid_with_Dual_Adaptive_Margin`), it inherits the idea of a blended margin, specifically the `alpha` parameter that interpolates between two different margin calculations. This provides a clear, tunable way to balance different adaptive strategies. From Parent 2 (`LogSigmoid_Loss_with_Adaptive_Sigmoid_Margin`), it inherits the powerful and successful core margin formulation: `margin = adaptive_scale * sigmoid(delta_cost / temp)`. This margin couples the model's output variance (`adaptive_scale` from `std(delta_logp)`) with the cost difference.\n\nThis child combines these ideas by creating a blended margin where one component is the simple, cost-proportional margin (`beta_cost * delta_cost`) and the other is the sophisticated adaptive sigmoid margin from Parent 2. The `alpha` hyperparameter, inherited from Parent 1, controls the mix. This addresses a potential weakness of sigmoid-based margins, which saturate for large cost differences. The linear term ensures the margin continues to grow, incentivizing the model to find better solutions even when cost gaps are large.\n\nNew Coupling 1: The primary innovation is the **blended margin structure**. The total margin is `margin = (1 - alpha) * margin_sigmoid + alpha * margin_linear`. `margin_sigmoid` is the adaptive sigmoid margin from Parent 2. `margin_linear` is a simple `beta_cost * delta_cost`, providing a non-saturating incentive. `alpha` balances their influence.\n\nNew Coupling 2: A **unified adaptive beta** for the `logsigmoid` loss. Both parents use an adaptive beta inversely related to `std(delta_logp)`. This design is retained, but the `adaptive_scale` used to compute it is also used within the `margin_sigmoid` component, creating a tight coupling between the loss curvature and the margin's magnitude, ensuring stability.", "pseudocode": "1. Calculate the difference in log-probabilities: delta_logp = logp(a) - logp(b).\n2. Calculate the difference in costs: delta_cost = cost(b) - cost(a).\n3. Inherit from Parent 2: Compute an adaptive scale based on the standard deviation of delta_logp, clipped for stability: `adaptive_scale = clamp(std(delta_logp), min_scale, max_scale)`.\n4. Inherit from Parent 2: Calculate the first margin component (`margin_sigmoid`) using the adaptive scale and a sigmoid of the cost difference: `margin_sigmoid = adaptive_scale * sigmoid(delta_cost / temperature)`.\n5. New Coupling 1 (Part A): Calculate a second, linear margin component (`margin_linear`) that is directly proportional to the cost difference: `margin_linear = beta_cost * delta_cost`.\n6. Inherit from Parent 1 & New Coupling 1 (Part B): Combine the two margin components using a blending factor `alpha`: `margin = (1 - alpha) * margin_sigmoid + alpha * margin_linear`.\n7. New Coupling 2: Define an adaptive beta for the logsigmoid loss as the inverse of the adaptive scale: `beta = 1.0 / adaptive_scale`.\n8. Compute the final loss argument: `loss_arg = delta_logp - margin`.\n9. Calculate the final loss using the scaled negative log-sigmoid function: `loss = -logsigmoid(beta * loss_arg)`.\n10. Return the mean of the loss across the batch.", "hyperparams": {"temperature": 1.0, "alpha": 0.1, "beta_cost": 0.1, "min_scale": 0.1, "max_scale": 5.0, "epsilon": 1e-08}, "operators_used": ["logsigmoid", "sigmoid", "clamp"], "implementation_hint": {"expects": ["cost_a", "cost_b", "logp_a", "logp_b", "weights"], "returns": "scalar"}, "code": "def generated_loss(batch, model_output, extra):\n    \"\"\"\n    A logistic preference loss with a blended margin combining an adaptive sigmoid term and a linear cost term.\n    \"\"\"\n    # Read tensors from the batch\n    cost_a = batch['cost_a']\n    cost_b = batch['cost_b']\n    logp_a = batch['log_prob_w']\n    logp_b = batch['log_prob_l']\n    weights = batch.get('weight')\n\n    # Read hyperparameters\n    hyperparams = extra['hyperparams']\n    temperature = hyperparams.get('temperature', 1.0)\n    alpha = hyperparams.get('alpha', 0.1)\n    beta_cost = hyperparams.get('beta_cost', 0.1)\n    min_scale = hyperparams.get('min_scale', 0.1)\n    max_scale = hyperparams.get('max_scale', 5.0)\n    epsilon = hyperparams.get('epsilon', 1e-8)\n\n    # 1. Calculate log-probability and cost differences\n    delta_logp = logp_a - logp_b\n    delta_cost = cost_b - cost_a\n\n    # 2. Compute adaptive scale (Inherited from Parent 2)\n    if delta_logp.numel() > 1:\n        logp_std = delta_logp.detach().std()\n    else:\n        logp_std = torch.tensor(1.0, device=delta_logp.device)\n    adaptive_scale = torch.clamp(logp_std, min_scale, max_scale)\n\n    # 3. Calculate sigmoid-based margin component (Inherited from Parent 2)\n    cost_weight = torch.sigmoid(delta_cost / temperature)\n    margin_sigmoid = adaptive_scale * cost_weight\n\n    # 4. New Coupling 1: Calculate linear margin component\n    margin_linear = beta_cost * delta_cost\n\n    # 5. Blend the two margins (Blending idea from Parent 1)\n    margin = (1.0 - alpha) * margin_sigmoid + alpha * margin_linear\n    \n    # 6. New Coupling 2: Unified adaptive beta for logsigmoid (Inherited pattern)\n    beta = 1.0 / (adaptive_scale.detach() + epsilon)\n\n    # 7. Compute the loss argument\n    loss_arg = delta_logp - margin\n\n    # 8. Apply negative log-sigmoid loss\n    loss_per_pair = -torch.nn.functional.logsigmoid(beta * loss_arg)\n\n    # 9. Apply optional weights and compute the mean loss\n    if weights is not None and weights.sum() > 0:\n        loss = (loss_per_pair * weights).sum() / weights.sum().clamp(min=epsilon)\n    else:\n        loss = loss_per_pair.mean()\n\n    return loss", "theoretical_basis": "A Bradley-Terry logistic preference model where the target log-odds is a dynamic, blended margin. The margin interpolates between a non-linear, saturating component (sensitive to model uncertainty and cost) and a linear, non-saturating component (proportional to cost). The loss function's steepness (beta) is inversely coupled to the model's output variance, creating a self-regulating system that adapts both its target and gradient strength."}, "fitness": {"hf_like_score": 7.873048870849609, "validation_objective": 7.871389374542236, "generalization_penalty": 0.001659496307373054, "generalization_objectives": {"100": 7.873048870849609}, "train_score_mean": 8.656430798589763, "train_loss_mean": 0.468506446328212, "pair_count": 4951579348, "early_eval": {"enabled": true, "steps": 100, "baseline_validation_objective": 8.412976063537597, "candidate_validation_objective": 8.34824653930664, "early_stopped": false}, "phases": {"f1": {"steps": 15630, "train_score_mean": 8.656430798589763, "train_loss_mean": 0.468506446328212, "pair_count": 4951579348}, "f2": {"steps": 0, "train_score_mean": null, "train_loss_mean": null, "pair_count": 0}}, "config": {"hf": {"problem": "tsp", "hf_steps": 0, "hf_epochs": 10, "hf_instances_per_epoch": 100000, "train_problem_size": 100, "valid_problem_sizes": [100], "train_batch_size": 64, "pomo_size": 100, "learning_rate": 0.0003, "weight_decay": 1e-06, "alpha": 0.05, "device": "cuda:0", "seed": 1234, "num_validation_episodes": 10000, "validation_batch_size": 64, "generalization_penalty_weight": 1.0, "pool_version": "v0"}, "free_loss": {"f1_steps": 0, "total_train_steps": 15630, "f2_steps": 0, "f3_enabled": false}}, "loss_ir": {"name": "Blended_Adaptive_Margin_LogSigmoid", "intuition": "Mode: combine. This loss function aims to synthesize the best adaptive mechanisms from both parents into a more robust and flexible objective. It inherits the core Bradley-Terry structure using a `logsigmoid` loss, a common element in both parents and many successful preference models. \n\nFrom Parent 1 (`Adaptive_LogSigmoid_with_Dual_Adaptive_Margin`), it inherits the idea of a blended margin, specifically the `alpha` parameter that interpolates between two different margin calculations. This provides a clear, tunable way to balance different adaptive strategies. From Parent 2 (`LogSigmoid_Loss_with_Adaptive_Sigmoid_Margin`), it inherits the powerful and successful core margin formulation: `margin = adaptive_scale * sigmoid(delta_cost / temp)`. This margin couples the model's output variance (`adaptive_scale` from `std(delta_logp)`) with the cost difference.\n\nThis child combines these ideas by creating a blended margin where one component is the simple, cost-proportional margin (`beta_cost * delta_cost`) and the other is the sophisticated adaptive sigmoid margin from Parent 2. The `alpha` hyperparameter, inherited from Parent 1, controls the mix. This addresses a potential weakness of sigmoid-based margins, which saturate for large cost differences. The linear term ensures the margin continues to grow, incentivizing the model to find better solutions even when cost gaps are large.\n\nNew Coupling 1: The primary innovation is the **blended margin structure**. The total margin is `margin = (1 - alpha) * margin_sigmoid + alpha * margin_linear`. `margin_sigmoid` is the adaptive sigmoid margin from Parent 2. `margin_linear` is a simple `beta_cost * delta_cost`, providing a non-saturating incentive. `alpha` balances their influence.\n\nNew Coupling 2: A **unified adaptive beta** for the `logsigmoid` loss. Both parents use an adaptive beta inversely related to `std(delta_logp)`. This design is retained, but the `adaptive_scale` used to compute it is also used within the `margin_sigmoid` component, creating a tight coupling between the loss curvature and the margin's magnitude, ensuring stability.", "hyperparams": {"temperature": 1.0, "alpha": 0.1, "beta_cost": 0.1, "min_scale": 0.1, "max_scale": 5.0, "epsilon": 1e-08}, "operators_used": ["logsigmoid", "sigmoid", "clamp"]}}, "better_than_baseline": true}
{"generation": 4, "index": 1, "ir": {"name": "Hybrid_Adaptive_LogSigmoid_with_Cost_Variance_Coupling", "intuition": "Mode: combine. This loss function aims to synthesize the best adaptive mechanisms from both parents while introducing a novel coupling to cost variance for enhanced stability and responsiveness. \n\nIt inherits two core ideas that are present in both parents: \n1. **`logsigmoid` Loss Basis:** It uses the standard `logsigmoid` function, providing a robust, probabilistic foundation common to Bradley-Terry models. \n2. **Adaptive Margin via `logp` Variance:** It inherits the mechanism of scaling the margin by an `adaptive_scale` derived from the standard deviation of `delta_logp`. This makes the learning target larger when the model is uncertain or its outputs are diverse, a key feature of both parents.\n\nThe first new coupling idea is to **incorporate cost variance directly into the adaptive scale**. Instead of relying solely on `std(delta_logp)`, the new `adaptive_scale` is calculated as `sqrt(std(delta_logp)^2 + std(delta_cost)^2)`. This creates a more holistic measure of batch difficulty, reacting to both model uncertainty (logp variance) and task ambiguity (cost variance). This is inspired by the `SelfBalancing_Adaptive_LogSigmoid_Loss` elite, which used a simple sum of variances.\n\nThe second new coupling is a **dynamic blending of margin types based on this new holistic scale**. It inherits the dual-margin structure from Parent 0, which blends a fixed-temperature margin (`margin_A`) and an adaptive-temperature margin (`margin_B`). However, instead of a fixed `alpha` hyperparameter for blending, the blending weight `dynamic_alpha` is now a `sigmoid` function of the `adaptive_scale`. When the scale is high (high uncertainty/difficulty), `dynamic_alpha` approaches 1, favoring the more stable, fixed-temperature margin. When the scale is low (high confidence), `dynamic_alpha` approaches 0.5, giving equal weight to both margins, allowing the adaptive-temperature component to fine-tune the preference. This creates a self-regulating mechanism that chooses the most appropriate margin type based on the current batch context.", "pseudocode": "1. Calculate the difference in log-probabilities: delta_logp = logp(a) - logp(b).\n2. Calculate the difference in costs: delta_cost = cost(b) - cost(a).\n3. Compute batch statistics: standard deviation of delta_logp (logp_std) and delta_cost (cost_std).\n4. New Coupling 1: Compute a holistic `adaptive_scale` by taking the L2-norm of the logp and cost standard deviations: `adaptive_scale = sqrt(logp_std^2 + cost_std^2)`, clipped for stability. \n5. Inherit from Parent 0: Calculate the first margin component (margin_A) using a fixed temperature: `margin_A = adaptive_scale * sigmoid(delta_cost / fixed_temp)`.\n6. Inherit from Parent 0: Calculate the second margin component (margin_B) using an adaptive cost temperature: `margin_B = adaptive_scale * sigmoid(delta_cost / (cost_std + epsilon))`.\n7. New Coupling 2: Compute a `dynamic_alpha` blending factor by applying a sigmoid function to the centered `adaptive_scale`: `dynamic_alpha = sigmoid(adaptive_scale - alpha_bias)`. This makes the blend sensitive to batch difficulty.\n8. Combine the two margin components using the dynamic blending factor: `margin = dynamic_alpha * margin_A + (1 - dynamic_alpha) * margin_B`.\n9. Inherit from both parents: Define an adaptive beta for the logsigmoid loss as the inverse of the adaptive scale: `beta = 1.0 / (adaptive_scale + epsilon)`.\n10. Compute the final loss argument: `loss_arg = delta_logp - margin`.\n11. Calculate the final loss using the scaled negative log-sigmoid function: `loss = -logsigmoid(beta * loss_arg)`.\n12. Return the mean of the loss across the batch.", "hyperparams": {"fixed_temp": 1.0, "alpha_bias": 1.0, "min_scale": 0.1, "max_scale": 10.0, "epsilon": 1e-08}, "operators_used": ["logsigmoid", "sigmoid", "clamp"], "implementation_hint": {"expects": ["cost_a", "cost_b", "logp_a", "logp_b", "weight"], "returns": "scalar"}, "code": "def generated_loss(batch, model_output, extra):\n    \"\"\"\n    A hybrid adaptive logsigmoid loss with cost variance coupling and dynamic margin blending.\n    \"\"\"\n    # Read tensors from the batch\n    cost_a = batch['cost_a']\n    cost_b = batch['cost_b']\n    logp_a = batch['log_prob_w']\n    logp_b = batch['log_prob_l']\n    weights = batch.get('weight')\n\n    # Read hyperparameters\n    hyperparams = extra['hyperparams']\n    fixed_temp = hyperparams.get('fixed_temp', 1.0)\n    alpha_bias = hyperparams.get('alpha_bias', 1.0)\n    min_scale = hyperparams.get('min_scale', 0.1)\n    max_scale = hyperparams.get('max_scale', 10.0)\n    epsilon = hyperparams.get('epsilon', 1e-8)\n\n    # 1. Calculate log-probability and cost differences\n    delta_logp = logp_a - logp_b\n    delta_cost = cost_b - cost_a\n\n    # 2. Compute batch statistics\n    if delta_logp.numel() > 1:\n        logp_std = delta_logp.detach().std()\n        cost_std = delta_cost.detach().std()\n    else:\n        logp_std = torch.tensor(1.0, device=delta_logp.device)\n        cost_std = torch.tensor(1.0, device=delta_logp.device)\n\n    # 3. New Coupling 1: Holistic adaptive scale\n    composite_variance = logp_std.pow(2) + cost_std.pow(2)\n    adaptive_scale = torch.sqrt(composite_variance).clamp(min_scale, max_scale)\n\n    # 4. Inherit from Parent 0: Margin A (fixed temp)\n    margin_A = adaptive_scale * torch.sigmoid(delta_cost / fixed_temp)\n\n    # 5. Inherit from Parent 0: Margin B (adaptive temp)\n    adaptive_cost_temp = cost_std + epsilon\n    margin_B = adaptive_scale * torch.sigmoid(delta_cost / adaptive_cost_temp)\n\n    # 6. New Coupling 2: Dynamic alpha for blending\n    # When scale is high, alpha -> 1 (favoring stable Margin A).\n    # When scale is low, alpha -> 0.5 (equal blend).\n    dynamic_alpha = torch.sigmoid(adaptive_scale.detach() - alpha_bias)\n\n    # 7. Blend the margins\n    margin = dynamic_alpha * margin_A + (1.0 - dynamic_alpha) * margin_B\n    \n    # 8. Inherit from both: Adaptive beta for logsigmoid\n    beta = 1.0 / (adaptive_scale.detach() + epsilon)\n\n    # 9. Compute the loss argument\n    loss_arg = delta_logp - margin\n\n    # 10. Apply negative log-sigmoid loss\n    loss_per_pair = -torch.nn.functional.logsigmoid(beta * loss_arg)\n\n    # 11. Apply optional weights and compute the mean loss\n    if weights is not None and weights.sum() > 0:\n        loss = (loss_per_pair * weights).sum() / weights.sum()\n    else:\n        loss = loss_per_pair.mean()\n\n    return loss", "theoretical_basis": "A Bradley-Terry logistic preference model where the target log-odds is a dynamically constructed, blended margin. The margin's overall scale is coupled to the L2-norm of log-probability and cost variance, holistically capturing batch difficulty. The blend between a fixed-temperature and an adaptive-temperature margin component is itself dynamically controlled by this scale. The loss steepness (beta) is inversely coupled to the scale, ensuring stability."}, "fitness": {"hf_like_score": 7.882705683898926, "validation_objective": 7.8811290771484375, "generalization_penalty": 0.0015766067504880965, "generalization_objectives": {"100": 7.882705683898926}, "train_score_mean": 8.11540454997516, "train_loss_mean": 0.7160844222845668, "pair_count": 4951577477, "early_eval": {"enabled": true, "steps": 100, "baseline_validation_objective": 8.412976063537597, "candidate_validation_objective": 8.366346357727052, "early_stopped": false}, "phases": {"f1": {"steps": 15630, "train_score_mean": 8.11540454997516, "train_loss_mean": 0.7160844222845668, "pair_count": 4951577477}, "f2": {"steps": 0, "train_score_mean": null, "train_loss_mean": null, "pair_count": 0}}, "config": {"hf": {"problem": "tsp", "hf_steps": 0, "hf_epochs": 10, "hf_instances_per_epoch": 100000, "train_problem_size": 100, "valid_problem_sizes": [100], "train_batch_size": 64, "pomo_size": 100, "learning_rate": 0.0003, "weight_decay": 1e-06, "alpha": 0.05, "device": "cuda:1", "seed": 1234, "num_validation_episodes": 10000, "validation_batch_size": 64, "generalization_penalty_weight": 1.0, "pool_version": "v0"}, "free_loss": {"f1_steps": 0, "total_train_steps": 15630, "f2_steps": 0, "f3_enabled": false}}, "loss_ir": {"name": "Hybrid_Adaptive_LogSigmoid_with_Cost_Variance_Coupling", "intuition": "Mode: combine. This loss function aims to synthesize the best adaptive mechanisms from both parents while introducing a novel coupling to cost variance for enhanced stability and responsiveness. \n\nIt inherits two core ideas that are present in both parents: \n1. **`logsigmoid` Loss Basis:** It uses the standard `logsigmoid` function, providing a robust, probabilistic foundation common to Bradley-Terry models. \n2. **Adaptive Margin via `logp` Variance:** It inherits the mechanism of scaling the margin by an `adaptive_scale` derived from the standard deviation of `delta_logp`. This makes the learning target larger when the model is uncertain or its outputs are diverse, a key feature of both parents.\n\nThe first new coupling idea is to **incorporate cost variance directly into the adaptive scale**. Instead of relying solely on `std(delta_logp)`, the new `adaptive_scale` is calculated as `sqrt(std(delta_logp)^2 + std(delta_cost)^2)`. This creates a more holistic measure of batch difficulty, reacting to both model uncertainty (logp variance) and task ambiguity (cost variance). This is inspired by the `SelfBalancing_Adaptive_LogSigmoid_Loss` elite, which used a simple sum of variances.\n\nThe second new coupling is a **dynamic blending of margin types based on this new holistic scale**. It inherits the dual-margin structure from Parent 0, which blends a fixed-temperature margin (`margin_A`) and an adaptive-temperature margin (`margin_B`). However, instead of a fixed `alpha` hyperparameter for blending, the blending weight `dynamic_alpha` is now a `sigmoid` function of the `adaptive_scale`. When the scale is high (high uncertainty/difficulty), `dynamic_alpha` approaches 1, favoring the more stable, fixed-temperature margin. When the scale is low (high confidence), `dynamic_alpha` approaches 0.5, giving equal weight to both margins, allowing the adaptive-temperature component to fine-tune the preference. This creates a self-regulating mechanism that chooses the most appropriate margin type based on the current batch context.", "hyperparams": {"fixed_temp": 1.0, "alpha_bias": 1.0, "min_scale": 0.1, "max_scale": 10.0, "epsilon": 1e-08}, "operators_used": ["logsigmoid", "sigmoid", "clamp"]}}, "better_than_baseline": true}
{"generation": 4, "index": 2, "ir": {"name": "Blended_Adaptive_Margin_LogSigmoid", "intuition": "Mode: combine. This loss function aims to synthesize the best adaptive mechanisms from both parents, which have demonstrated strong performance. The global feedback suggests that combining adaptive elements is a promising direction, and both parents are successful elites. \n\nInherited Ideas:\n- From both Parent 0 and Parent 1, it inherits the core `logsigmoid` loss structure, which provides a probabilistic interpretation based on the Bradley-Terry model.\n- From both parents, it also inherits the concept of an adaptive scale based on the standard deviation of `delta_logp`, which links the margin size and/or loss temperature to model uncertainty.\n- From Parent 0, it inherits the idea of a blended margin, specifically the `alpha` parameter that weights two different margin components. This allows for a more flexible and tunable margin.\n- From Parent 1, it inherits the specific formulation of a margin component: `adaptive_scale * sigmoid(delta_cost / temp)`. This has proven to be a very effective way to construct a cost-sensitive margin.\n\nNew Coupling Ideas:\n1.  **Margin Component Blending:** While Parent 0 blended two similar sigmoid-based margins with different temperatures, this child takes a more distinct approach. It blends the highly effective margin from Parent 1 (`adaptive_scale * sigmoid(delta_cost / temp)`) with a simpler, linear cost-based margin (`beta_cost * delta_cost`). This creates a hybrid margin that is non-linear and saturating for large cost differences (from the sigmoid component) but also has a direct, linear response for smaller cost differences. This might offer better calibration across a wide range of cost gaps.\n2.  **Unified Adaptive Temperature (Beta):** Both parents use an adaptive beta for the `logsigmoid` function, inversely coupled to `adaptive_scale`. This child retains this powerful stability trick, ensuring that the loss softens when the model is uncertain and the margin is large. This creates a unified self-balancing system where the margin's scale and the loss's steepness are coupled.\n\nThe resulting loss function is a sophisticated Bradley-Terry variant where the target log-odds is a blended margin, and the loss function's sensitivity adapts to the model's confidence.", "pseudocode": "1. Calculate the difference in log-probabilities: delta_logp = logp(a) - logp(b).\n2. Calculate the difference in costs: delta_cost = cost(b) - cost(a).\n3. Inherit from both parents: Compute an adaptive scale based on the standard deviation of delta_logp, clipped for stability.\n4. Inherit from Parent 1: Calculate the first margin component (margin_A) as `adaptive_scale * sigmoid(delta_cost / temperature)`.\n5. New Coupling 1 (Part 1): Define a second, simpler margin component (margin_B) that is linearly proportional to the cost difference: `margin_B = beta_cost * delta_cost`.\n6. New Coupling 1 (Part 2) & Inherit from Parent 0: Combine the two margin components using a blending factor `alpha`: `margin = alpha * margin_A + (1 - alpha) * margin_B`.\n7. New Coupling 2 & Inherit from both: Define an adaptive beta for the logsigmoid loss as the inverse of the adaptive scale: `beta = 1.0 / (adaptive_scale + epsilon)`.\n8. Compute the final loss argument: `loss_arg = delta_logp - margin`.\n9. Calculate the final loss using the scaled negative log-sigmoid function: `loss = -logsigmoid(beta * loss_arg)`.\n10. Return the mean of the loss across the batch.", "hyperparams": {"temperature": 1.0, "alpha": 0.7, "beta_cost": 0.5, "min_scale": 0.1, "max_scale": 5.0, "epsilon": 1e-08}, "operators_used": ["logsigmoid", "sigmoid", "clamp"], "implementation_hint": {"expects": ["cost_a", "cost_b", "logp_a", "logp_b"], "returns": "scalar"}, "code": "def generated_loss(batch, model_output, extra):\n    \"\"\"\n    A logistic preference loss with a blended margin combining a sigmoid-based adaptive\n    component and a linear cost component. The loss temperature is inversely coupled to\n    the adaptive scale of the margin.\n    \"\"\"\n    # Read tensors from the batch\n    cost_a = batch['cost_a']\n    cost_b = batch['cost_b']\n    logp_a = batch['log_prob_w']\n    logp_b = batch['log_prob_l']\n    weights = batch.get('weight')\n\n    # Read hyperparameters\n    hyperparams = extra['hyperparams']\n    temperature = hyperparams.get('temperature', 1.0)\n    alpha = hyperparams.get('alpha', 0.7)\n    beta_cost = hyperparams.get('beta_cost', 0.5)\n    min_scale = hyperparams.get('min_scale', 0.1)\n    max_scale = hyperparams.get('max_scale', 5.0)\n    epsilon = hyperparams.get('epsilon', 1e-8)\n\n    # 1. Calculate log-probability and cost differences\n    delta_logp = logp_a - logp_b\n    delta_cost = cost_b - cost_a\n\n    # 2. Inherit: Compute adaptive scale from logp stats\n    if delta_logp.numel() > 1:\n        logp_std = delta_logp.detach().std()\n    else:\n        logp_std = torch.tensor(1.0, device=delta_logp.device)\n    adaptive_scale = torch.clamp(logp_std, min_scale, max_scale)\n\n    # 3. Inherit from Parent 1: Calculate sigmoid-based margin component\n    cost_weight = torch.sigmoid(delta_cost / temperature)\n    margin_A = adaptive_scale * cost_weight\n\n    # 4. New Coupling 1: Define a linear cost margin component\n    margin_B = beta_cost * delta_cost\n\n    # 5. Inherit from Parent 0 & New Coupling 1: Blend the two margins\n    margin = alpha * margin_A + (1.0 - alpha) * margin_B\n    \n    # 6. Inherit from both & New Coupling 2: Adaptive beta for logsigmoid\n    beta = 1.0 / (adaptive_scale.detach() + epsilon)\n\n    # 7. Compute the loss argument\n    loss_arg = delta_logp - margin\n\n    # 8. Apply negative log-sigmoid loss\n    loss_per_pair = -torch.nn.functional.logsigmoid(beta * loss_arg)\n\n    # 9. Apply optional weights and compute the mean loss\n    if weights is not None:\n        loss = (loss_per_pair * weights).sum() / weights.sum().clamp(min=epsilon)\n    else:\n        loss = loss_per_pair.mean()\n\n    return loss", "theoretical_basis": "A Bradley-Terry logistic preference model where the target log-odds is a dynamically constructed, blended margin. The margin combines a non-linear, sigmoid-based component sensitive to cost differences with a simpler linear cost term. The steepness of the logistic loss (beta) is inversely coupled to the model's output variance, creating a self-regulating system that adapts both the learning target and gradient strength."}, "fitness": {"hf_like_score": 7.87582088470459, "validation_objective": 7.874695249938965, "generalization_penalty": 0.0011256347656249943, "generalization_objectives": {"100": 7.87582088470459}, "train_score_mean": 8.758926171701228, "train_loss_mean": 0.43281928479328263, "pair_count": 4951579500, "early_eval": {"enabled": true, "steps": 100, "baseline_validation_objective": 8.412976063537597, "candidate_validation_objective": 8.38914084777832, "early_stopped": false}, "phases": {"f1": {"steps": 15630, "train_score_mean": 8.758926171701228, "train_loss_mean": 0.43281928479328263, "pair_count": 4951579500}, "f2": {"steps": 0, "train_score_mean": null, "train_loss_mean": null, "pair_count": 0}}, "config": {"hf": {"problem": "tsp", "hf_steps": 0, "hf_epochs": 10, "hf_instances_per_epoch": 100000, "train_problem_size": 100, "valid_problem_sizes": [100], "train_batch_size": 64, "pomo_size": 100, "learning_rate": 0.0003, "weight_decay": 1e-06, "alpha": 0.05, "device": "cuda:2", "seed": 1234, "num_validation_episodes": 10000, "validation_batch_size": 64, "generalization_penalty_weight": 1.0, "pool_version": "v0"}, "free_loss": {"f1_steps": 0, "total_train_steps": 15630, "f2_steps": 0, "f3_enabled": false}}, "loss_ir": {"name": "Blended_Adaptive_Margin_LogSigmoid", "intuition": "Mode: combine. This loss function aims to synthesize the best adaptive mechanisms from both parents, which have demonstrated strong performance. The global feedback suggests that combining adaptive elements is a promising direction, and both parents are successful elites. \n\nInherited Ideas:\n- From both Parent 0 and Parent 1, it inherits the core `logsigmoid` loss structure, which provides a probabilistic interpretation based on the Bradley-Terry model.\n- From both parents, it also inherits the concept of an adaptive scale based on the standard deviation of `delta_logp`, which links the margin size and/or loss temperature to model uncertainty.\n- From Parent 0, it inherits the idea of a blended margin, specifically the `alpha` parameter that weights two different margin components. This allows for a more flexible and tunable margin.\n- From Parent 1, it inherits the specific formulation of a margin component: `adaptive_scale * sigmoid(delta_cost / temp)`. This has proven to be a very effective way to construct a cost-sensitive margin.\n\nNew Coupling Ideas:\n1.  **Margin Component Blending:** While Parent 0 blended two similar sigmoid-based margins with different temperatures, this child takes a more distinct approach. It blends the highly effective margin from Parent 1 (`adaptive_scale * sigmoid(delta_cost / temp)`) with a simpler, linear cost-based margin (`beta_cost * delta_cost`). This creates a hybrid margin that is non-linear and saturating for large cost differences (from the sigmoid component) but also has a direct, linear response for smaller cost differences. This might offer better calibration across a wide range of cost gaps.\n2.  **Unified Adaptive Temperature (Beta):** Both parents use an adaptive beta for the `logsigmoid` function, inversely coupled to `adaptive_scale`. This child retains this powerful stability trick, ensuring that the loss softens when the model is uncertain and the margin is large. This creates a unified self-balancing system where the margin's scale and the loss's steepness are coupled.\n\nThe resulting loss function is a sophisticated Bradley-Terry variant where the target log-odds is a blended margin, and the loss function's sensitivity adapts to the model's confidence.", "hyperparams": {"temperature": 1.0, "alpha": 0.7, "beta_cost": 0.5, "min_scale": 0.1, "max_scale": 5.0, "epsilon": 1e-08}, "operators_used": ["logsigmoid", "sigmoid", "clamp"]}}, "better_than_baseline": true}
{"generation": 4, "index": 3, "ir": {"name": "Hybrid_Adaptive_Margin_LogSigmoid_Loss", "intuition": "Mode: combine. This loss function synthesizes the most effective adaptive components from both parents to create a highly responsive and stable learning objective. It inherits the core `logsigmoid(beta * (delta_logp - margin))` structure, which is common to both parents and forms a solid probabilistic foundation. From Parent 1, it inherits the idea of a dual-component, blended margin (`alpha * margin_A + (1-alpha) * margin_B`), where one part uses a fixed temperature and the other an adaptive temperature based on `std(delta_cost)`. This allows the margin to capture both global and batch-specific cost sensitivities. From Parent 0, it inherits the powerful coupling of the loss temperature (`beta`) being inversely proportional to an adaptive scale (`beta = 1 / adaptive_scale`).\n\nThe first new coupling is to make the `adaptive_scale` itself a hybrid. Instead of just using `std(delta_logp)` as in both parents, the new scale is a weighted sum of the standard deviations of both `delta_logp` and `delta_cost`: `adaptive_scale = w_logp * std(delta_logp) + w_cost * std(delta_cost)`. This makes the margin's magnitude and the loss's softness responsive to variance in both the model's outputs and the cost landscape, providing a more holistic measure of batch difficulty. The second new coupling is a **dynamic alpha** for blending the margin components. Instead of a fixed hyperparameter, `alpha` is now derived from the relative variance of costs in the batch: `alpha = sigmoid((std(delta_cost) - mean(delta_cost)) / temperature)`. This allows the loss to automatically favor the fixed-temperature margin when cost differences are uniform (low std dev) and shift towards the adaptive-temperature margin when cost differences are highly varied (high std dev), making the margin composition self-tuning.", "pseudocode": "1. Calculate the difference in log-probabilities: delta_logp = logp(a) - logp(b).\n2. Calculate the difference in costs: delta_cost = cost(b) - cost(a).\n3. New Coupling 1: Compute a hybrid adaptive scale. Calculate the standard deviations of delta_logp and delta_cost. The adaptive_scale is a weighted sum: `w_logp * std(delta_logp) + w_cost * std(delta_cost)`, clipped for stability.\n4. Inherit from Parent 0: Compute an adaptive beta for the loss, inversely proportional to the hybrid adaptive scale: `beta = 1.0 / adaptive_scale`.\n5. Inherit from Parent 1: Calculate the two base margin components. `margin_A` uses a fixed temperature for its cost sigmoid. `margin_B` uses an adaptive temperature based on `std(delta_cost)`.\n6. New Coupling 2: Compute a dynamic blending factor `alpha`. This is calculated as `sigmoid((std(delta_cost) - mean(delta_cost)) / temp)`. This makes the blend sensitive to the distribution of costs in the batch.\n7. Inherit from Parent 1: Combine the two margin components using the dynamic alpha: `margin = alpha * margin_A + (1 - alpha) * margin_B`.\n8. Compute the final loss argument: `loss_arg = delta_logp - margin`.\n9. Calculate the final loss using the scaled negative log-sigmoid function: `loss = -logsigmoid(beta * loss_arg)`.\n10. Return the mean of the loss across the batch.", "hyperparams": {"fixed_temp": 1.0, "w_logp": 0.5, "w_cost": 0.5, "min_scale": 0.1, "max_scale": 5.0, "epsilon": 1e-08}, "operators_used": ["logsigmoid", "sigmoid", "clamp"], "implementation_hint": {"expects": ["cost_a", "cost_b", "logp_a", "logp_b"], "returns": "scalar"}, "code": "def generated_loss(batch, model_output, extra):\n    \"\"\"\n    A logistic loss with a hybrid adaptive margin and temperature.\n    The margin scale depends on both logp and cost variance.\n    The margin blend adapts to cost distribution.\n    \"\"\"\n    # Read tensors from the batch\n    cost_a = batch['cost_a']\n    cost_b = batch['cost_b']\n    logp_a = batch['log_prob_w']\n    logp_b = batch['log_prob_l']\n    weights = batch.get('weight')\n\n    # Read hyperparameters\n    hyperparams = extra['hyperparams']\n    fixed_temp = hyperparams.get('fixed_temp', 1.0)\n    w_logp = hyperparams.get('w_logp', 0.5)\n    w_cost = hyperparams.get('w_cost', 0.5)\n    min_scale = hyperparams.get('min_scale', 0.1)\n    max_scale = hyperparams.get('max_scale', 5.0)\n    epsilon = hyperparams.get('epsilon', 1e-8)\n\n    # 1. Calculate log-probability and cost differences\n    delta_logp = logp_a - logp_b\n    delta_cost = cost_b - cost_a\n\n    # 2. Compute batch statistics (detached for stability)\n    if delta_logp.numel() > 1:\n        logp_std = delta_logp.detach().std()\n        cost_std = delta_cost.detach().std()\n        cost_mean = delta_cost.detach().mean()\n    else:\n        logp_std = torch.tensor(1.0, device=delta_logp.device)\n        cost_std = torch.tensor(1.0, device=delta_cost.device)\n        cost_mean = torch.tensor(0.0, device=delta_cost.device)\n\n    # 3. New Coupling 1: Hybrid adaptive scale\n    hybrid_scale = w_logp * logp_std + w_cost * cost_std\n    adaptive_scale = torch.clamp(hybrid_scale, min_scale, max_scale)\n\n    # 4. Inherit from Parent 0: Adaptive beta from scale\n    beta = 1.0 / (adaptive_scale.detach() + epsilon)\n\n    # 5. Inherit from Parent 1: Margin components\n    # margin_A with fixed temperature\n    cost_weight_A = torch.sigmoid(delta_cost / fixed_temp)\n    margin_A = adaptive_scale * cost_weight_A\n    \n    # margin_B with adaptive temperature\n    adaptive_cost_temp = cost_std + epsilon\n    cost_weight_B = torch.sigmoid(delta_cost / adaptive_cost_temp)\n    margin_B = adaptive_scale * cost_weight_B\n\n    # 6. New Coupling 2: Dynamic alpha for blending\n    # Alpha increases as cost variance grows relative to its mean\n    alpha = torch.sigmoid((cost_std - cost_mean) / fixed_temp) # Use fixed_temp for stability\n\n    # 7. Inherit from Parent 1: Blended margin with dynamic alpha\n    margin = alpha * margin_A + (1.0 - alpha) * margin_B\n\n    # 8. Compute the loss argument\n    loss_arg = delta_logp - margin\n\n    # 9. Apply negative log-sigmoid loss\n    loss_per_pair = -torch.nn.functional.logsigmoid(beta * loss_arg)\n\n    # 10. Apply optional weights and compute the mean loss\n    if weights is not None:\n        loss = (loss_per_pair * weights).sum() / weights.sum().clamp(min=epsilon)\n    else:\n        loss = loss_per_pair.mean()\n\n    return loss", "theoretical_basis": "A Bradley-Terry logistic preference model where the target log-odds is a dynamically constructed and blended margin. The margin's overall scale and the loss's temperature are coupled to a hybrid measure of batch difficulty (variance of both log-probabilities and costs). The blend itself is dynamic, adapting to the distribution of costs within the batch, creating a multi-layered, self-regulating learning system."}, "fitness": {"hf_like_score": 7.870991123962402, "validation_objective": 7.869340533447265, "generalization_penalty": 0.0016505905151369404, "generalization_objectives": {"100": 7.870991123962402}, "train_score_mean": 8.63767128124957, "train_loss_mean": 0.4939244474002511, "pair_count": 4951579372, "early_eval": {"enabled": true, "steps": 100, "baseline_validation_objective": 8.412976063537597, "candidate_validation_objective": 8.35185686340332, "early_stopped": false}, "phases": {"f1": {"steps": 15630, "train_score_mean": 8.63767128124957, "train_loss_mean": 0.4939244474002511, "pair_count": 4951579372}, "f2": {"steps": 0, "train_score_mean": null, "train_loss_mean": null, "pair_count": 0}}, "config": {"hf": {"problem": "tsp", "hf_steps": 0, "hf_epochs": 10, "hf_instances_per_epoch": 100000, "train_problem_size": 100, "valid_problem_sizes": [100], "train_batch_size": 64, "pomo_size": 100, "learning_rate": 0.0003, "weight_decay": 1e-06, "alpha": 0.05, "device": "cuda:3", "seed": 1234, "num_validation_episodes": 10000, "validation_batch_size": 64, "generalization_penalty_weight": 1.0, "pool_version": "v0"}, "free_loss": {"f1_steps": 0, "total_train_steps": 15630, "f2_steps": 0, "f3_enabled": false}}, "loss_ir": {"name": "Hybrid_Adaptive_Margin_LogSigmoid_Loss", "intuition": "Mode: combine. This loss function synthesizes the most effective adaptive components from both parents to create a highly responsive and stable learning objective. It inherits the core `logsigmoid(beta * (delta_logp - margin))` structure, which is common to both parents and forms a solid probabilistic foundation. From Parent 1, it inherits the idea of a dual-component, blended margin (`alpha * margin_A + (1-alpha) * margin_B`), where one part uses a fixed temperature and the other an adaptive temperature based on `std(delta_cost)`. This allows the margin to capture both global and batch-specific cost sensitivities. From Parent 0, it inherits the powerful coupling of the loss temperature (`beta`) being inversely proportional to an adaptive scale (`beta = 1 / adaptive_scale`).\n\nThe first new coupling is to make the `adaptive_scale` itself a hybrid. Instead of just using `std(delta_logp)` as in both parents, the new scale is a weighted sum of the standard deviations of both `delta_logp` and `delta_cost`: `adaptive_scale = w_logp * std(delta_logp) + w_cost * std(delta_cost)`. This makes the margin's magnitude and the loss's softness responsive to variance in both the model's outputs and the cost landscape, providing a more holistic measure of batch difficulty. The second new coupling is a **dynamic alpha** for blending the margin components. Instead of a fixed hyperparameter, `alpha` is now derived from the relative variance of costs in the batch: `alpha = sigmoid((std(delta_cost) - mean(delta_cost)) / temperature)`. This allows the loss to automatically favor the fixed-temperature margin when cost differences are uniform (low std dev) and shift towards the adaptive-temperature margin when cost differences are highly varied (high std dev), making the margin composition self-tuning.", "hyperparams": {"fixed_temp": 1.0, "w_logp": 0.5, "w_cost": 0.5, "min_scale": 0.1, "max_scale": 5.0, "epsilon": 1e-08}, "operators_used": ["logsigmoid", "sigmoid", "clamp"]}}, "better_than_baseline": true}
{"generation": 4, "index": 5, "ir": {"name": "Adaptive_LogSigmoid_with_Logp_Normalized_Margin", "intuition": "Mode: combine. This loss function aims to create a more stable and principled preference learning objective by refining the adaptive mechanisms of its parents. It inherits the core `logsigmoid(beta * (delta_logp - margin))` structure, which provides a solid probabilistic foundation (Parent 0 & 1). It also inherits the adaptive beta, `beta = 1.0 / adaptive_scale`, where `adaptive_scale` is based on `std(delta_logp)`. This crucial coupling softens the loss when the model is uncertain, preventing overly large gradients (Parent 0 & 1).\n\nThe first new coupling idea is a **log-probability normalized margin**. Instead of the margin being `adaptive_scale * cost_weight`, it is now `adaptive_scale * cost_weight / (abs(logp_a) + abs(logp_b) + 1.0)`. This normalization down-weights the margin for pairs where the model assigns very high or very low absolute log-probabilities. The intuition is that when the model is already extremely confident (very large or very small logp), the margin target should be relaxed, preventing the loss from forcing the model to push already extreme values even further, which can cause instability. This focuses the learning signal on pairs where the model's log-probabilities are in a more contestable range.\n\nThe second new coupling is a **dynamic temperature for the cost-sigmoid**, inherited from Parent 0's second margin component. The temperature `cost_temp` is set to `std(delta_cost)`, making the margin's sensitivity to cost differences dependent on the diversity of costs in the batch. This prevents the sigmoid from saturating when cost differences are either very large or very small across a batch.\n\nThe overall design refines the successful adaptive margin concept by adding a stabilizing normalization term and making the cost-weighting more responsive to batch statistics.", "pseudocode": "1. Calculate the difference in log-probabilities: delta_logp = logp_a - logp_b.\n2. Calculate the difference in costs: delta_cost = cost(b) - cost(a).\n3. Inherit from Parents: Compute an adaptive scale for the margin and an inverse beta for the loss based on the standard deviation of delta_logp: `adaptive_scale = clamp(std(delta_logp), min_scale, max_scale)` and `beta = 1.0 / adaptive_scale`.\n4. New Coupling 2: Compute an adaptive temperature for the cost-sigmoid based on the standard deviation of delta_cost: `cost_temp = std(delta_cost) + epsilon`.\n5. Inherit from Parents: Create a cost-dependent weight using the adaptive temperature: `cost_weight = sigmoid(delta_cost / cost_temp)`.\n6. Compute the base margin: `base_margin = adaptive_scale * cost_weight`.\n7. New Coupling 1: Introduce a normalization factor based on the absolute magnitude of the input log-probabilities: `logp_norm = 1.0 / (abs(logp_a) + abs(logp_b) + 1.0)`.\n8. Calculate the final, normalized margin: `margin = base_margin * logp_norm`.\n9. Compute the final loss argument: `loss_arg = delta_logp - margin`.\n10. Calculate the loss using the scaled negative log-sigmoid function: `loss = -logsigmoid(beta * loss_arg)`.\n11. Return the mean of the loss across the batch.", "hyperparams": {"min_scale": 0.1, "max_scale": 5.0, "epsilon": 1e-08}, "operators_used": ["logsigmoid", "sigmoid", "clamp"], "implementation_hint": {"expects": ["cost_a", "cost_b", "logp_a", "logp_b"], "returns": "scalar"}, "code": "def generated_loss(batch, model_output, extra):\n    \"\"\"\n    A logistic preference loss with an adaptive margin that is normalized by the magnitude of the input log-probabilities.\n    \"\"\"\n    # Read tensors from the batch\n    cost_a = batch['cost_a']\n    cost_b = batch['cost_b']\n    logp_a = batch['log_prob_w']\n    logp_b = batch['log_prob_l']\n    weights = batch.get('weight')\n\n    # Read hyperparameters\n    hyperparams = extra['hyperparams']\n    min_scale = hyperparams.get('min_scale', 0.1)\n    max_scale = hyperparams.get('max_scale', 5.0)\n    epsilon = hyperparams.get('epsilon', 1e-8)\n\n    # 1. Calculate log-probability and cost differences\n    delta_logp = logp_a - logp_b\n    delta_cost = cost_b - cost_a\n\n    # 2. Inherit: Compute adaptive scale and beta from logp stats\n    if delta_logp.numel() > 1:\n        logp_std = delta_logp.detach().std()\n    else:\n        logp_std = torch.tensor(1.0, device=delta_logp.device)\n    adaptive_scale = torch.clamp(logp_std, min_scale, max_scale)\n    beta = 1.0 / (adaptive_scale.detach() + epsilon)\n\n    # 3. New Coupling 2: Adaptive temperature for cost-sigmoid\n    if delta_cost.numel() > 1:\n        cost_temp = delta_cost.detach().std() + epsilon\n    else:\n        cost_temp = torch.tensor(1.0, device=delta_cost.device)\n\n    # 4. Inherit: Create cost-dependent weight\n    cost_weight = torch.sigmoid(delta_cost / cost_temp)\n\n    # 5. Calculate base margin\n    base_margin = adaptive_scale * cost_weight\n\n    # 6. New Coupling 1: Normalize margin by absolute logp magnitude\n    logp_norm_factor = 1.0 / (torch.abs(logp_a.detach()) + torch.abs(logp_b.detach()) + 1.0)\n    margin = base_margin * logp_norm_factor\n\n    # 7. Compute the loss argument\n    loss_arg = delta_logp - margin\n\n    # 8. Apply negative log-sigmoid loss\n    loss_per_pair = -torch.nn.functional.logsigmoid(beta * loss_arg)\n\n    # 9. Apply optional weights and compute the mean loss\n    if weights is not None:\n        loss = (loss_per_pair * weights).sum() / weights.sum().clamp(min=epsilon)\n    else:\n        loss = loss_per_pair.mean()\n\n    return loss", "theoretical_basis": "A Bradley-Terry logistic preference model where the target log-odds is a dynamic, cost-sensitive margin. The margin's scale is coupled with model uncertainty (logp variance), and its sensitivity to cost is adapted to batch-wise cost variance. Crucially, the margin is normalized by the absolute magnitude of the input log-probabilities to prevent instability and focus learning on less extreme-probability samples. The loss function's steepness (beta) is inversely coupled to the model's output variance, creating a self-regulating system."}, "fitness": {"hf_like_score": 7.892045904541016, "validation_objective": 7.891027560424805, "generalization_penalty": 0.0010183441162112672, "generalization_objectives": {"100": 7.892045904541016}, "train_score_mean": 9.157261626718903, "train_loss_mean": 0.2986652573598019, "pair_count": 4951580182, "early_eval": {"enabled": true, "steps": 100, "baseline_validation_objective": 8.412976063537597, "candidate_validation_objective": 8.361580012512206, "early_stopped": false}, "phases": {"f1": {"steps": 15630, "train_score_mean": 9.157261626718903, "train_loss_mean": 0.2986652573598019, "pair_count": 4951580182}, "f2": {"steps": 0, "train_score_mean": null, "train_loss_mean": null, "pair_count": 0}}, "config": {"hf": {"problem": "tsp", "hf_steps": 0, "hf_epochs": 10, "hf_instances_per_epoch": 100000, "train_problem_size": 100, "valid_problem_sizes": [100], "train_batch_size": 64, "pomo_size": 100, "learning_rate": 0.0003, "weight_decay": 1e-06, "alpha": 0.05, "device": "cuda:0", "seed": 1234, "num_validation_episodes": 10000, "validation_batch_size": 64, "generalization_penalty_weight": 1.0, "pool_version": "v0"}, "free_loss": {"f1_steps": 0, "total_train_steps": 15630, "f2_steps": 0, "f3_enabled": false}}, "loss_ir": {"name": "Adaptive_LogSigmoid_with_Logp_Normalized_Margin", "intuition": "Mode: combine. This loss function aims to create a more stable and principled preference learning objective by refining the adaptive mechanisms of its parents. It inherits the core `logsigmoid(beta * (delta_logp - margin))` structure, which provides a solid probabilistic foundation (Parent 0 & 1). It also inherits the adaptive beta, `beta = 1.0 / adaptive_scale`, where `adaptive_scale` is based on `std(delta_logp)`. This crucial coupling softens the loss when the model is uncertain, preventing overly large gradients (Parent 0 & 1).\n\nThe first new coupling idea is a **log-probability normalized margin**. Instead of the margin being `adaptive_scale * cost_weight`, it is now `adaptive_scale * cost_weight / (abs(logp_a) + abs(logp_b) + 1.0)`. This normalization down-weights the margin for pairs where the model assigns very high or very low absolute log-probabilities. The intuition is that when the model is already extremely confident (very large or very small logp), the margin target should be relaxed, preventing the loss from forcing the model to push already extreme values even further, which can cause instability. This focuses the learning signal on pairs where the model's log-probabilities are in a more contestable range.\n\nThe second new coupling is a **dynamic temperature for the cost-sigmoid**, inherited from Parent 0's second margin component. The temperature `cost_temp` is set to `std(delta_cost)`, making the margin's sensitivity to cost differences dependent on the diversity of costs in the batch. This prevents the sigmoid from saturating when cost differences are either very large or very small across a batch.\n\nThe overall design refines the successful adaptive margin concept by adding a stabilizing normalization term and making the cost-weighting more responsive to batch statistics.", "hyperparams": {"min_scale": 0.1, "max_scale": 5.0, "epsilon": 1e-08}, "operators_used": ["logsigmoid", "sigmoid", "clamp"]}}, "better_than_baseline": true}
{"generation": 4, "index": 6, "ir": {"name": "Composite_Adaptive_Margin_LogSigmoid", "intuition": "Mode: combine. This loss function synergizes the adaptive mechanisms of its parents, which have shown strong performance, and introduces a more holistic batch-level difficulty assessment. It inherits the foundational `logsigmoid(beta * (delta_logp - margin))` structure common to both parents. \n\nFrom both parents, it inherits the core concept of an adaptive margin where `margin = adaptive_scale * cost_weight`. The `cost_weight` is a `sigmoid` function of the cost difference, and the `adaptive_scale` is based on the batch-wise standard deviation of `delta_logp`, which reflects model uncertainty.\n\nFrom Parent 0 (`Adaptive_LogSigmoid_with_Dual_Adaptive_Margin`), it inherits the idea of making the cost-weighting sensitive to the batch's cost diversity. Instead of just a fixed temperature, the cost sigmoid is scaled by an adaptive temperature based on `std(delta_cost)`.\n\nNew Coupling 1: The core innovation is a **composite adaptive scale**. Instead of using only `std(delta_logp)` for the scale, I define a new `composite_scale` as a weighted sum of the standard deviation of `delta_logp` and `delta_cost`: `composite_scale = (1 - gamma) * std(delta_logp) + gamma * std(delta_cost)`. This captures a more complete picture of batch difficulty, considering both model uncertainty (logp variance) and task ambiguity (cost variance). A `gamma` hyperparameter controls the blend.\n\nNew Coupling 2: Following a successful pattern from the parents, the `beta` (temperature) of the `logsigmoid` loss is **inversely coupled** to this new `composite_scale`. So, `beta = 1 / composite_scale`. This creates a robust self-balancing system: if the batch is difficult (high variance in either logps or costs), the `composite_scale` increases, which in turn increases the target margin but decreases `beta`, softening the loss and preventing unstable gradients. This dual-use of the composite scale for both the margin and loss temperature ensures stability while maintaining a strong learning signal.", "pseudocode": "1. Calculate the difference in log-probabilities: delta_logp = logp(a) - logp(b).\n2. Calculate the difference in costs: delta_cost = cost(b) - cost(a).\n3. Inherit from Parent 0: Compute an adaptive cost temperature based on the standard deviation of delta_cost: `adaptive_cost_temp = std(delta_cost) + epsilon`.\n4. Inherit from Parent 0/1: Calculate the cost-dependent weight using a sigmoid, but with the adaptive temperature: `cost_weight = sigmoid(delta_cost / adaptive_cost_temp)`.\n5. New Coupling 1: Compute a `composite_scale` that blends the standard deviation of `delta_logp` and `delta_cost` using a `gamma` hyperparameter: `composite_scale = (1-gamma)*std(delta_logp) + gamma*std(delta_cost)`. Clip for stability.\n6. Inherit from Parent 0/1: Calculate the dynamic margin by multiplying the composite scale and the cost weight: `margin = composite_scale * cost_weight`.\n7. New Coupling 2: Define an adaptive beta for the logistic loss as the inverse of the composite scale: `beta = 1.0 / composite_scale`.\n8. Compute the argument for the loss function: `loss_arg = delta_logp - margin`.\n9. Calculate the final loss using the scaled negative log-sigmoid function: `loss = -logsigmoid(beta * loss_arg)`.\n10. Return the mean of the loss across the batch.", "hyperparams": {"gamma": 0.25, "min_scale": 0.1, "max_scale": 10.0, "epsilon": 1e-08}, "operators_used": ["logsigmoid", "sigmoid", "clamp"], "implementation_hint": {"expects": ["cost_a", "cost_b", "logp_a", "logp_b"], "returns": "scalar"}, "code": "def generated_loss(batch, model_output, extra):\n    \"\"\"\n    A logistic loss with a margin adaptively scaled by a composite of logp and cost variance.\n    The loss temperature is inversely coupled to this composite scale.\n    \"\"\"\n    # Read tensors from the batch\n    cost_a = batch['cost_a']\n    cost_b = batch['cost_b']\n    logp_a = batch['log_prob_w']\n    logp_b = batch['log_prob_l']\n    weights = batch.get('weight')\n\n    # Read hyperparameters\n    hyperparams = extra['hyperparams']\n    gamma = hyperparams.get('gamma', 0.25)\n    min_scale = hyperparams.get('min_scale', 0.1)\n    max_scale = hyperparams.get('max_scale', 10.0)\n    epsilon = hyperparams.get('epsilon', 1e-8)\n\n    # 1. Calculate log-probability and cost differences\n    delta_logp = logp_a - logp_b\n    delta_cost = cost_b - cost_a\n\n    # 2. Inherit from Parent 0: Adaptive cost temperature\n    if delta_cost.numel() > 1:\n        cost_std = delta_cost.detach().std()\n        adaptive_cost_temp = cost_std + epsilon\n    else:\n        cost_std = torch.tensor(0.0, device=delta_cost.device)\n        adaptive_cost_temp = torch.tensor(1.0, device=delta_cost.device)\n\n    # 3. Inherit from Parents: Sigmoid cost weight with adaptive temp\n    cost_weight = torch.sigmoid(delta_cost / adaptive_cost_temp)\n\n    # 4. New Coupling 1: Composite adaptive scale\n    if delta_logp.numel() > 1:\n        logp_std = delta_logp.detach().std()\n    else:\n        logp_std = torch.tensor(0.0, device=delta_logp.device)\n    \n    composite_scale = (1.0 - gamma) * logp_std + gamma * cost_std\n    composite_scale_clamped = torch.clamp(composite_scale, min_scale, max_scale)\n\n    # 5. Inherit from Parents: Dynamic margin using composite scale\n    margin = composite_scale_clamped * cost_weight\n\n    # 6. New Coupling 2: Inverse coupling of beta to composite scale\n    beta = 1.0 / (composite_scale_clamped.detach() + epsilon)\n\n    # 7. Compute the loss argument\n    loss_arg = delta_logp - margin\n\n    # 8. Apply negative log-sigmoid loss\n    loss_per_pair = -torch.nn.functional.logsigmoid(beta * loss_arg)\n\n    # 9. Apply optional weights and compute the mean loss\n    if weights is not None:\n        loss = (loss_per_pair * weights).sum() / weights.sum().clamp(min=epsilon)\n    else:\n        loss = loss_per_pair.mean()\n\n    return loss", "theoretical_basis": "A Bradley-Terry logistic preference model where the target log-odds is a dynamic margin. This margin's magnitude is coupled to a composite scale reflecting both model uncertainty (logp variance) and task ambiguity (cost variance). The steepness of the logistic loss is inversely coupled to this same composite scale, creating a self-regulating system that adapts both the learning target and gradient strength to the overall difficulty of the preference pairs in a batch."}, "fitness": {"hf_like_score": 7.8784239440917965, "validation_objective": 7.875658169555664, "generalization_penalty": 0.0027657745361322483, "generalization_objectives": {"100": 7.8784239440917965}, "train_score_mean": 8.144238477689825, "train_loss_mean": 0.6481469348635494, "pair_count": 4951578180, "early_eval": {"enabled": true, "steps": 100, "baseline_validation_objective": 8.412976063537597, "candidate_validation_objective": 8.301969744873047, "early_stopped": false}, "phases": {"f1": {"steps": 15630, "train_score_mean": 8.144238477689825, "train_loss_mean": 0.6481469348635494, "pair_count": 4951578180}, "f2": {"steps": 0, "train_score_mean": null, "train_loss_mean": null, "pair_count": 0}}, "config": {"hf": {"problem": "tsp", "hf_steps": 0, "hf_epochs": 10, "hf_instances_per_epoch": 100000, "train_problem_size": 100, "valid_problem_sizes": [100], "train_batch_size": 64, "pomo_size": 100, "learning_rate": 0.0003, "weight_decay": 1e-06, "alpha": 0.05, "device": "cuda:1", "seed": 1234, "num_validation_episodes": 10000, "validation_batch_size": 64, "generalization_penalty_weight": 1.0, "pool_version": "v0"}, "free_loss": {"f1_steps": 0, "total_train_steps": 15630, "f2_steps": 0, "f3_enabled": false}}, "loss_ir": {"name": "Composite_Adaptive_Margin_LogSigmoid", "intuition": "Mode: combine. This loss function synergizes the adaptive mechanisms of its parents, which have shown strong performance, and introduces a more holistic batch-level difficulty assessment. It inherits the foundational `logsigmoid(beta * (delta_logp - margin))` structure common to both parents. \n\nFrom both parents, it inherits the core concept of an adaptive margin where `margin = adaptive_scale * cost_weight`. The `cost_weight` is a `sigmoid` function of the cost difference, and the `adaptive_scale` is based on the batch-wise standard deviation of `delta_logp`, which reflects model uncertainty.\n\nFrom Parent 0 (`Adaptive_LogSigmoid_with_Dual_Adaptive_Margin`), it inherits the idea of making the cost-weighting sensitive to the batch's cost diversity. Instead of just a fixed temperature, the cost sigmoid is scaled by an adaptive temperature based on `std(delta_cost)`.\n\nNew Coupling 1: The core innovation is a **composite adaptive scale**. Instead of using only `std(delta_logp)` for the scale, I define a new `composite_scale` as a weighted sum of the standard deviation of `delta_logp` and `delta_cost`: `composite_scale = (1 - gamma) * std(delta_logp) + gamma * std(delta_cost)`. This captures a more complete picture of batch difficulty, considering both model uncertainty (logp variance) and task ambiguity (cost variance). A `gamma` hyperparameter controls the blend.\n\nNew Coupling 2: Following a successful pattern from the parents, the `beta` (temperature) of the `logsigmoid` loss is **inversely coupled** to this new `composite_scale`. So, `beta = 1 / composite_scale`. This creates a robust self-balancing system: if the batch is difficult (high variance in either logps or costs), the `composite_scale` increases, which in turn increases the target margin but decreases `beta`, softening the loss and preventing unstable gradients. This dual-use of the composite scale for both the margin and loss temperature ensures stability while maintaining a strong learning signal.", "hyperparams": {"gamma": 0.25, "min_scale": 0.1, "max_scale": 10.0, "epsilon": 1e-08}, "operators_used": ["logsigmoid", "sigmoid", "clamp"]}}, "better_than_baseline": true}
{"generation": 4, "index": 7, "ir": {"name": "Progressive_Dual_Margin_LogSigmoid", "intuition": "Mode: combine. This loss function synergizes the adaptive mechanisms of its high-performing parents to create a sophisticated, self-regulating objective. It inherits the core `logsigmoid` loss structure and the concept of an adaptive `beta` inversely proportional to model uncertainty (`std(delta_logp)`) from both parents. This ensures stability by softening the loss when the model's predictions are noisy.\n\nFrom Parent 0 (`Adaptive_LogSigmoid_with_Dual_Adaptive_Margin`), it inherits the idea of a dual-component margin, allowing the loss to respond to cost differences at different scales. From Parent 1 (`LogSigmoid_Loss_with_Adaptive_Sigmoid_Margin`), it inherits the powerful `adaptive_scale * sigmoid(delta_cost)` margin structure, which couples the margin's magnitude to both model uncertainty and cost difference.\n\nThis child loss introduces two new coupling ideas:\n1.  **Progressive Margin Scaling:** Instead of blending two separate margin calculations, this loss creates a single, more principled margin. The base margin is `adaptive_scale * sigmoid(delta_cost / fixed_temp)`. A second, more adaptive component is then added: `adaptive_scale * sigmoid(delta_cost / adaptive_cost_temp)`. The key innovation is that `adaptive_cost_temp` is derived from `std(delta_cost)`, making the second part of the margin sensitive to the batch's cost diversity. The two parts are blended with a hyperparameter `alpha`, forming a single, coherent margin that is progressively refined by batch statistics.\n2.  **Normalized Cost Input:** To improve numerical stability and make the sigmoid function operate in a more consistent regime, the `delta_cost` used in the margin calculation is z-scored (`(delta_cost - mean) / std`). This normalization prevents very large or small cost differences from saturating the sigmoid, ensuring a more responsive gradient signal across a wider range of cost gaps. The original `delta_cost` is still used for the z-scoring statistics to preserve the true scale of cost differences within the batch.", "pseudocode": "1. Calculate the difference in log-probabilities: delta_logp = logp(a) - logp(b).\n2. Calculate the difference in costs: delta_cost = cost(b) - cost(a).\n3. Inherit from both parents: Compute an adaptive scale based on the standard deviation of delta_logp, clipped for stability. `adaptive_scale = clamp(std(delta_logp), min_scale, max_scale)`.\n4. Inherit from both parents: Compute an adaptive beta for the loss as the inverse of the adaptive scale. `beta = 1.0 / adaptive_scale`.\n5. New Coupling (Normalization): Compute the z-scored cost difference: `norm_delta_cost = zscore(delta_cost)`.\n6. Inherit from Parent 0: Compute an adaptive cost temperature from the standard deviation of the original `delta_cost`: `adaptive_cost_temp = std(delta_cost)`.\n7. Inherit from Parent 1: Calculate the first margin component using the normalized cost and a fixed temperature: `margin_A = adaptive_scale * sigmoid(norm_delta_cost / fixed_temp)`.\n8. New Coupling (Progressive Margin): Calculate the second margin component using the normalized cost and the adaptive cost temperature: `margin_B = adaptive_scale * sigmoid(norm_delta_cost / adaptive_cost_temp)`.\n9. Combine the two margin components using a blending factor `alpha`: `margin = alpha * margin_A + (1.0 - alpha) * margin_B`.\n10. Compute the final loss argument: `loss_arg = delta_logp - margin`.\n11. Calculate the final loss using the scaled negative log-sigmoid function: `loss = -logsigmoid(beta * loss_arg)`.\n12. Return the mean of the loss across the batch.", "hyperparams": {"fixed_temp": 1.0, "alpha": 0.5, "min_scale": 0.1, "max_scale": 5.0, "epsilon": 1e-08}, "operators_used": ["logsigmoid", "sigmoid", "clamp", "zscore"], "implementation_hint": {"expects": ["cost_a", "cost_b", "logp_a", "logp_b"], "returns": "scalar"}, "code": "def generated_loss(batch, model_output, extra):\n    \"\"\"\n    A logistic preference loss with a progressive, dual-temperature margin on z-scored costs.\n    \"\"\"\n    # Read tensors from the batch\n    cost_a = batch['cost_a']\n    cost_b = batch['cost_b']\n    logp_a = batch['log_prob_w']\n    logp_b = batch['log_prob_l']\n    weights = batch.get('weight')\n\n    # Read hyperparameters\n    hyperparams = extra['hyperparams']\n    fixed_temp = hyperparams.get('fixed_temp', 1.0)\n    alpha = hyperparams.get('alpha', 0.5)\n    min_scale = hyperparams.get('min_scale', 0.1)\n    max_scale = hyperparams.get('max_scale', 5.0)\n    epsilon = hyperparams.get('epsilon', 1e-8)\n\n    # 1. Calculate log-probability and cost differences\n    delta_logp = logp_a - logp_b\n    delta_cost = cost_b - cost_a\n\n    # 2. Inherit: Compute adaptive scale from logp stats\n    if delta_logp.numel() > 1:\n        logp_std = delta_logp.detach().std()\n    else:\n        logp_std = torch.tensor(1.0, device=delta_logp.device)\n    adaptive_scale = torch.clamp(logp_std, min_scale, max_scale)\n\n    # 3. Inherit: Adaptive beta for logsigmoid\n    beta = 1.0 / (adaptive_scale.detach() + epsilon)\n\n    # 4. New Coupling: Z-score normalization for cost differences\n    if delta_cost.numel() > 1:\n        cost_std = delta_cost.detach().std() + epsilon\n        cost_mean = delta_cost.detach().mean()\n        norm_delta_cost = (delta_cost - cost_mean) / cost_std\n        adaptive_cost_temp = cost_std\n    else:\n        norm_delta_cost = delta_cost\n        adaptive_cost_temp = torch.tensor(1.0, device=delta_cost.device)\n    \n    # 5. Inherit Parent 1 & New Coupling: Margin component with fixed temperature on normalized cost\n    cost_weight_A = torch.sigmoid(norm_delta_cost / fixed_temp)\n    margin_A = adaptive_scale * cost_weight_A\n\n    # 6. Inherit Parent 0 & New Coupling: Margin component with adaptive temperature on normalized cost\n    cost_weight_B = torch.sigmoid(norm_delta_cost / adaptive_cost_temp)\n    margin_B = adaptive_scale * cost_weight_B\n\n    # 7. Combine the two margin components\n    margin = alpha * margin_A + (1.0 - alpha) * margin_B\n\n    # 8. Compute the loss argument\n    loss_arg = delta_logp - margin.detach() # Detach margin for stability\n\n    # 9. Apply negative log-sigmoid loss\n    loss_per_pair = -torch.nn.functional.logsigmoid(beta * loss_arg)\n\n    # 10. Apply optional weights and compute the mean loss\n    if weights is not None:\n        loss = (loss_per_pair * weights).sum() / weights.sum().clamp(min=epsilon)\n    else:\n        loss = loss_per_pair.mean()\n\n    return loss", "theoretical_basis": "A Bradley-Terry logistic preference model with a dynamically constructed, cost-sensitive margin. The margin is a blended sum of two components operating on z-scored cost differences, with one component using a fixed temperature and the other an adaptive temperature based on batch cost variance. The loss function's steepness is inversely coupled to the model's output variance, creating a self-regulating system that adapts both the learning target and gradient strength to batch-specific statistics."}, "fitness": {"hf_like_score": 7.875502507019043, "validation_objective": 7.874008615112305, "generalization_penalty": 0.0014938919067377654, "generalization_objectives": {"100": 7.875502507019043}, "train_score_mean": 8.693629952172628, "train_loss_mean": 0.42706845366870905, "pair_count": 4951579414, "early_eval": {"enabled": true, "steps": 100, "baseline_validation_objective": 8.412976063537597, "candidate_validation_objective": 8.350543283081056, "early_stopped": false}, "phases": {"f1": {"steps": 15630, "train_score_mean": 8.693629952172628, "train_loss_mean": 0.42706845366870905, "pair_count": 4951579414}, "f2": {"steps": 0, "train_score_mean": null, "train_loss_mean": null, "pair_count": 0}}, "config": {"hf": {"problem": "tsp", "hf_steps": 0, "hf_epochs": 10, "hf_instances_per_epoch": 100000, "train_problem_size": 100, "valid_problem_sizes": [100], "train_batch_size": 64, "pomo_size": 100, "learning_rate": 0.0003, "weight_decay": 1e-06, "alpha": 0.05, "device": "cuda:2", "seed": 1234, "num_validation_episodes": 10000, "validation_batch_size": 64, "generalization_penalty_weight": 1.0, "pool_version": "v0"}, "free_loss": {"f1_steps": 0, "total_train_steps": 15630, "f2_steps": 0, "f3_enabled": false}}, "loss_ir": {"name": "Progressive_Dual_Margin_LogSigmoid", "intuition": "Mode: combine. This loss function synergizes the adaptive mechanisms of its high-performing parents to create a sophisticated, self-regulating objective. It inherits the core `logsigmoid` loss structure and the concept of an adaptive `beta` inversely proportional to model uncertainty (`std(delta_logp)`) from both parents. This ensures stability by softening the loss when the model's predictions are noisy.\n\nFrom Parent 0 (`Adaptive_LogSigmoid_with_Dual_Adaptive_Margin`), it inherits the idea of a dual-component margin, allowing the loss to respond to cost differences at different scales. From Parent 1 (`LogSigmoid_Loss_with_Adaptive_Sigmoid_Margin`), it inherits the powerful `adaptive_scale * sigmoid(delta_cost)` margin structure, which couples the margin's magnitude to both model uncertainty and cost difference.\n\nThis child loss introduces two new coupling ideas:\n1.  **Progressive Margin Scaling:** Instead of blending two separate margin calculations, this loss creates a single, more principled margin. The base margin is `adaptive_scale * sigmoid(delta_cost / fixed_temp)`. A second, more adaptive component is then added: `adaptive_scale * sigmoid(delta_cost / adaptive_cost_temp)`. The key innovation is that `adaptive_cost_temp` is derived from `std(delta_cost)`, making the second part of the margin sensitive to the batch's cost diversity. The two parts are blended with a hyperparameter `alpha`, forming a single, coherent margin that is progressively refined by batch statistics.\n2.  **Normalized Cost Input:** To improve numerical stability and make the sigmoid function operate in a more consistent regime, the `delta_cost` used in the margin calculation is z-scored (`(delta_cost - mean) / std`). This normalization prevents very large or small cost differences from saturating the sigmoid, ensuring a more responsive gradient signal across a wider range of cost gaps. The original `delta_cost` is still used for the z-scoring statistics to preserve the true scale of cost differences within the batch.", "hyperparams": {"fixed_temp": 1.0, "alpha": 0.5, "min_scale": 0.1, "max_scale": 5.0, "epsilon": 1e-08}, "operators_used": ["logsigmoid", "sigmoid", "clamp", "zscore"]}}, "better_than_baseline": true}
{"generation": 5, "index": 0, "ir": {"name": "Hybrid_Scale_LogSigmoid_with_Rank_Gap_Margin", "intuition": "Based on the compile error E_COMPILE_ERROR, I removed the `import torch` and `import torch.nn.functional as F` statements. The execution environment for the loss function already provides `torch` and its submodules, so explicit imports are not allowed. The core logic of the loss, which combines an adaptive scale with a rank-gap-based margin, remains unchanged.", "pseudocode": "1. Calculate the difference in log-probabilities: delta_logp = logp(a) - logp(b).\n2. Calculate batch statistics (detached for stability): standard deviation of delta_logp (logp_std), and the mean (cost_mean) and standard deviation (cost_std) of delta_cost.\n3. Inherit from Parent 1: Compute a hybrid base scale as a weighted sum of logp and cost standard deviations: `base_scale = w_logp * logp_std + w_cost * cost_std`.\n4. New Coupling 1 (Gating): Compute a gating factor based on the cost distribution: `gate = sigmoid(cost_std - cost_mean)`.\n5. Apply the gate to the base scale and clamp for stability: `adaptive_scale = clamp(gate * base_scale, min_scale, max_scale)`.\n6. Inherit from both parents: Compute an adaptive beta for the loss, inversely proportional to the final adaptive scale: `beta = 1.0 / adaptive_scale`.\n7. New Coupling 2 (Rank-Gap Margin): Calculate the normalized rank difference of the costs: `rank_difference = (rank(cost_a) - rank(cost_b)) / N`. The margin is then `margin = adaptive_scale * rank_difference`.\n8. Compute the final loss argument: `loss_arg = delta_logp - margin`.\n9. Calculate the final loss using the scaled negative log-sigmoid function: `loss = -logsigmoid(beta * loss_arg)`.\n10. Return the mean of the loss across the batch.", "hyperparams": {"w_logp": 0.5, "w_cost": 0.5, "min_scale": 0.1, "max_scale": 5.0, "epsilon": 1e-08}, "operators_used": ["logsigmoid", "sigmoid", "clamp"], "implementation_hint": {"expects": ["A batch dictionary with 'cost_a', 'cost_b', 'log_prob_w', and 'log_prob_l'. 'log_prob_w' and 'log_prob_l' correspond to 'a' and 'b' respectively. 'weight' is an optional tensor for weighted loss calculation."], "returns": "A single scalar tensor representing the mean loss over the batch."}, "code": "def generated_loss(batch, model_output, extra):\n    \"\"\"\n    A logistic loss with a hybrid adaptive scale and a rank-gap-based margin.\n    \"\"\"\n    # Read tensors from the batch\n    cost_a = batch['cost_a']\n    cost_b = batch['cost_b']\n    logp_a = batch['log_prob_w']\n    logp_b = batch['log_prob_l']\n    weights = batch.get('weight')\n\n    # Read hyperparameters\n    hyperparams = extra['hyperparams']\n    w_logp = hyperparams.get('w_logp', 0.5)\n    w_cost = hyperparams.get('w_cost', 0.5)\n    min_scale = hyperparams.get('min_scale', 0.1)\n    max_scale = hyperparams.get('max_scale', 5.0)\n    epsilon = hyperparams.get('epsilon', 1e-8)\n\n    # 1. Calculate log-probability and cost differences\n    delta_logp = logp_a - logp_b\n    delta_cost = cost_b - cost_a\n\n    # 2. Compute batch statistics (detached for stability)\n    if delta_logp.numel() > 1:\n        logp_std = delta_logp.detach().std()\n        cost_std = delta_cost.detach().std()\n        cost_mean = delta_cost.detach().mean()\n    else:\n        logp_std = torch.tensor(1.0, device=delta_logp.device)\n        cost_std = torch.tensor(1.0, device=delta_cost.device)\n        cost_mean = torch.tensor(0.0, device=delta_cost.device)\n\n    # 3. Inherit from Parent 1: Hybrid base scale\n    base_scale = w_logp * logp_std + w_cost * cost_std\n\n    # 4. New Coupling 1: Gating mechanism\n    gate = torch.sigmoid(cost_std - cost_mean)\n    adaptive_scale = torch.clamp(gate * base_scale, min_scale, max_scale)\n\n    # 5. Inherit from both: Adaptive beta from scale\n    beta = 1.0 / (adaptive_scale.detach() + epsilon)\n\n    # 6. New Coupling 2: Rank-gap based margin\n    # The rank_gap operator would compute (rank(a) - rank(b)) / N\n    # Here we implement it directly.\n    # Note: cost_a should be preferred (lower cost), so we expect rank(cost_a) < rank(cost_b)\n    # and thus a negative rank_difference, which correctly subtracts from delta_logp.\n    num_pairs = cost_a.numel()\n    if num_pairs > 1:\n        # Ranks are 1-based, so we use floats to handle ties correctly\n        ranks_a = cost_a.float().argsort().argsort().float() + 1.0\n        ranks_b = cost_b.float().argsort().argsort().float() + 1.0\n        # rank_gap expects lower cost = better, so rank_difference is rank(a) - rank(b)\n        rank_difference = (ranks_a - ranks_b) / num_pairs\n    else:\n        rank_difference = torch.zeros_like(cost_a)\n    \n    margin = adaptive_scale * rank_difference.detach()\n\n    # 7. Compute the loss argument\n    loss_arg = delta_logp - margin\n\n    # 8. Apply negative log-sigmoid loss\n    loss_per_pair = -torch.nn.functional.logsigmoid(beta * loss_arg)\n\n    # 9. Apply optional weights and compute the mean loss\n    if weights is not None:\n        loss = (loss_per_pair * weights).sum() / weights.sum().clamp(min=epsilon)\n    else:\n        loss = loss_per_pair.mean()\n\n    return loss", "theoretical_basis": ""}, "fitness": {"hf_like_score": 22.964454724121094, "validation_objective": 22.964454724121094, "generalization_penalty": 0.0, "generalization_objectives": {"100": 22.956596514892578}, "train_score_mean": 26.163383073806763, "train_loss_mean": 1.2287401920557022, "pair_count": 31679984, "early_eval": {"enabled": true, "steps": 100, "baseline_validation_objective": 8.412976063537597, "candidate_validation_objective": 22.964454724121094, "early_stopped": true}, "phases": {"f1": {"steps": 15630, "train_score_mean": 26.163383073806763, "train_loss_mean": 1.2287401920557022, "pair_count": 31679984}, "f2": {"steps": 0, "train_score_mean": null, "train_loss_mean": null, "pair_count": 0}}, "config": {"hf": {"problem": "tsp", "hf_steps": 0, "hf_epochs": 10, "hf_instances_per_epoch": 100000, "train_problem_size": 100, "valid_problem_sizes": [100], "train_batch_size": 64, "pomo_size": 100, "learning_rate": 0.0003, "weight_decay": 1e-06, "alpha": 0.05, "device": "cuda:0", "seed": 1234, "num_validation_episodes": 10000, "validation_batch_size": 64, "generalization_penalty_weight": 1.0, "pool_version": "v0"}, "free_loss": {"f1_steps": 0, "total_train_steps": 15630, "f2_steps": 0, "f3_enabled": false}}, "loss_ir": {"name": "Hybrid_Scale_LogSigmoid_with_Rank_Gap_Margin", "intuition": "Based on the compile error E_COMPILE_ERROR, I removed the `import torch` and `import torch.nn.functional as F` statements. The execution environment for the loss function already provides `torch` and its submodules, so explicit imports are not allowed. The core logic of the loss, which combines an adaptive scale with a rank-gap-based margin, remains unchanged.", "hyperparams": {"w_logp": 0.5, "w_cost": 0.5, "min_scale": 0.1, "max_scale": 5.0, "epsilon": 1e-08}, "operators_used": ["logsigmoid", "sigmoid", "clamp"]}}, "better_than_baseline": false}
{"generation": 5, "index": 2, "ir": {"name": "Progressive_Hybrid_Margin_LogSigmoid", "intuition": "I fixed a compile error (E_COMPILE_ERROR) by removing the forbidden `import torch` and `import torch.nn.functional as F` statements. The PyTorch framework and its functional API are already available in the execution environment, so these imports are unnecessary and violate the static gate checks. The core logic of the loss, which uses EMAs for stable adaptation and a z-score based dynamic alpha for margin blending, remains unchanged.", "pseudocode": "1. Calculate delta_logp = logp(a) - logp(b) and delta_cost = cost(b) - cost(a).\n2. Get the current batch's standard deviations: logp_std and cost_std.\n3. New Coupling 1: Update Exponential Moving Averages (EMAs) for logp_std and cost_std. Retrieve the updated EMA values (ema_logp_std, ema_cost_std). These EMAs are stored and updated across batches.\n4. Inherit from Parent 1: Compute a hybrid adaptive scale using the stable EMA statistics: `adaptive_scale = w_logp * ema_logp_std + w_cost * ema_cost_std`, clipped for stability.\n5. Inherit from Parent 0: Compute an adaptive beta for the loss, inversely proportional to the adaptive scale: `beta = 1.0 / adaptive_scale`.\n6. Inherit from Parent 1: Calculate the two base margin components. `margin_A` uses a fixed temperature. `margin_B` uses the stable `ema_cost_std` as its adaptive temperature.\n7. New Coupling 2: Compute a z-score based dynamic alpha. Calculate the z-score of the current `cost_std` relative to its EMA: `z = (cost_std - ema_cost_std) / (ema_cost_std + epsilon)`. The dynamic alpha is `sigmoid(z)`.\n8. Inherit from Parent 1: Combine the two margin components using the z-score based dynamic alpha: `margin = alpha * margin_A + (1 - alpha) * margin_B`.\n9. Compute the final loss argument: `loss_arg = delta_logp - margin`.\n10. Calculate the loss: `loss = -logsigmoid(beta * loss_arg)`.\n11. Return the mean loss.", "hyperparams": {"ema_decay": 0.99, "fixed_temp": 1.0, "w_logp": 0.5, "w_cost": 0.5, "min_scale": 0.1, "max_scale": 5.0, "epsilon": 1e-08}, "operators_used": ["logsigmoid", "sigmoid", "clamp"], "implementation_hint": {"expects": ["The `batch` dictionary is expected to contain 'cost_a', 'cost_b', 'log_prob_w', and 'log_prob_l' as tensors. It may optionally contain 'weight'. The `extra` dictionary is used for storing and updating EMA statistics ('ema_logp_std', 'ema_cost_std') across batches."], "returns": "The function returns a single scalar tensor representing the mean loss for the batch."}, "code": "def generated_loss(batch, model_output, extra):\n    \"\"\"\n    A logistic loss with a hybrid margin whose parameters are stabilized by EMAs.\n    The margin blend adapts based on the z-score of cost variance.\n    \"\"\"\n    # Read tensors from the batch\n    cost_a = batch['cost_a']\n    cost_b = batch['cost_b']\n    logp_a = batch['log_prob_w']\n    logp_b = batch['log_prob_l']\n    weights = batch.get('weight')\n\n    # Read hyperparameters\n    hyperparams = extra['hyperparams']\n    ema_decay = hyperparams.get('ema_decay', 0.99)\n    fixed_temp = hyperparams.get('fixed_temp', 1.0)\n    w_logp = hyperparams.get('w_logp', 0.5)\n    w_cost = hyperparams.get('w_cost', 0.5)\n    min_scale = hyperparams.get('min_scale', 0.1)\n    max_scale = hyperparams.get('max_scale', 5.0)\n    epsilon = hyperparams.get('epsilon', 1e-8)\n\n    # 1. Calculate log-probability and cost differences\n    delta_logp = logp_a - logp_b\n    delta_cost = cost_b - cost_a\n\n    # 2. Compute current batch statistics (detached for stability)\n    if delta_logp.numel() > 1:\n        logp_std = delta_logp.detach().std()\n        cost_std = delta_cost.detach().std()\n    else:\n        logp_std = torch.tensor(1.0, device=delta_logp.device)\n        cost_std = torch.tensor(1.0, device=delta_cost.device)\n\n    # 3. New Coupling 1: Update and use EMAs for statistics\n    if 'ema_logp_std' not in extra:\n        extra['ema_logp_std'] = logp_std\n        extra['ema_cost_std'] = cost_std\n    else:\n        extra['ema_logp_std'] = ema_decay * extra['ema_logp_std'] + (1 - ema_decay) * logp_std\n        extra['ema_cost_std'] = ema_decay * extra['ema_cost_std'] + (1 - ema_decay) * cost_std\n    \n    ema_logp_std = extra['ema_logp_std']\n    ema_cost_std = extra['ema_cost_std']\n\n    # 4. Inherit: Compute hybrid adaptive scale using stable EMAs\n    hybrid_scale = w_logp * ema_logp_std + w_cost * ema_cost_std\n    adaptive_scale = torch.clamp(hybrid_scale, min_scale, max_scale)\n\n    # 5. Inherit: Adaptive beta from scale\n    beta = 1.0 / (adaptive_scale.detach() + epsilon)\n\n    # 6. Inherit: Margin components, using stable EMA for adaptive temp\n    cost_weight_A = torch.sigmoid(delta_cost / fixed_temp)\n    margin_A = adaptive_scale * cost_weight_A\n    \n    adaptive_cost_temp = ema_cost_std + epsilon\n    cost_weight_B = torch.sigmoid(delta_cost / adaptive_cost_temp)\n    margin_B = adaptive_scale * cost_weight_B\n\n    # 7. New Coupling 2: Z-score based dynamic alpha\n    z_score_cost_std = (cost_std - ema_cost_std) / (ema_cost_std + epsilon)\n    alpha = torch.sigmoid(z_score_cost_std)\n\n    # 8. Inherit: Blended margin with dynamic alpha\n    margin = alpha * margin_A + (1.0 - alpha) * margin_B\n\n    # 9. Compute the loss argument\n    loss_arg = delta_logp - margin\n\n    # 10. Apply negative log-sigmoid loss\n    loss_per_pair = -F.logsigmoid(beta * loss_arg)\n\n    # 11. Apply optional weights and compute the mean loss\n    if weights is not None:\n        loss = (loss_per_pair * weights).sum() / weights.sum().clamp(min=epsilon)\n    else:\n        loss = loss_per_pair.mean()\n\n    return loss", "theoretical_basis": ""}, "fitness": {"hf_like_score": 7.8720624588012695, "validation_objective": 7.870422041320801, "generalization_penalty": 0.0016404174804689475, "generalization_objectives": {"100": 7.8720624588012695}, "train_score_mean": 8.64963461405485, "train_loss_mean": 0.5079600460565175, "pair_count": 4951579188, "early_eval": {"enabled": true, "steps": 100, "baseline_validation_objective": 8.412976063537597, "candidate_validation_objective": 8.341966032409667, "early_stopped": false}, "phases": {"f1": {"steps": 15630, "train_score_mean": 8.64963461405485, "train_loss_mean": 0.5079600460565175, "pair_count": 4951579188}, "f2": {"steps": 0, "train_score_mean": null, "train_loss_mean": null, "pair_count": 0}}, "config": {"hf": {"problem": "tsp", "hf_steps": 0, "hf_epochs": 10, "hf_instances_per_epoch": 100000, "train_problem_size": 100, "valid_problem_sizes": [100], "train_batch_size": 64, "pomo_size": 100, "learning_rate": 0.0003, "weight_decay": 1e-06, "alpha": 0.05, "device": "cuda:1", "seed": 1234, "num_validation_episodes": 10000, "validation_batch_size": 64, "generalization_penalty_weight": 1.0, "pool_version": "v0"}, "free_loss": {"f1_steps": 0, "total_train_steps": 15630, "f2_steps": 0, "f3_enabled": false}}, "loss_ir": {"name": "Progressive_Hybrid_Margin_LogSigmoid", "intuition": "I fixed a compile error (E_COMPILE_ERROR) by removing the forbidden `import torch` and `import torch.nn.functional as F` statements. The PyTorch framework and its functional API are already available in the execution environment, so these imports are unnecessary and violate the static gate checks. The core logic of the loss, which uses EMAs for stable adaptation and a z-score based dynamic alpha for margin blending, remains unchanged.", "hyperparams": {"ema_decay": 0.99, "fixed_temp": 1.0, "w_logp": 0.5, "w_cost": 0.5, "min_scale": 0.1, "max_scale": 5.0, "epsilon": 1e-08}, "operators_used": ["logsigmoid", "sigmoid", "clamp"]}}, "better_than_baseline": true}
{"generation": 5, "index": 3, "ir": {"name": "ZScore_Gated_Adaptive_Margin_Loss", "intuition": "Mode: combine. This loss function combines the adaptive margin and temperature mechanisms from both parents while introducing a novel gating mechanism to improve stability and responsiveness. It inherits the core `logsigmoid(beta * (delta_logp - margin))` structure, which is a proven foundation from both parents. From Parent 0, it adopts the inverse coupling between the loss temperature `beta` and an `adaptive_scale` derived from `std(delta_logp)`. From Parent 1, it inherits the idea of a hybrid `adaptive_scale` that considers the variance of both `delta_logp` and `delta_cost`, providing a more holistic measure of batch difficulty.\n\nThe first new coupling idea is a **Z-Score Gating Mechanism** for the margin. Instead of always applying the full margin, we compute the z-score of `delta_cost` for each pair in the batch. The margin is then multiplied by `sigmoid(z_score)`. This gate has two effects: 1) For pairs with a cost difference close to the batch average (`z_score` ~ 0), the margin is down-weighted (`sigmoid(0)`=0.5), preventing over-enforcement of average preferences. 2) For pairs with a very large cost difference (large positive `z_score`), the full margin is applied, focusing learning on the most significant preferences. This adaptively prioritizes pairs based on their cost-gap extremity within the batch. The second new coupling is the use of `softplus` instead of `sigmoid` for the cost-to-margin transformation. `softplus` is non-saturating for large cost differences, which means that unlike `sigmoid`, it will continue to increase the margin for very large `delta_cost`, ensuring that the model is always incentivized to respect significant cost gaps.\n\nThe theoretical basis remains a Bradley-Terry model, but the target log-odds (the margin) is now gated by the statistical typicality of the cost difference, making the learning signal more focused and robust to noise from pairs with average cost gaps.", "pseudocode": "1. Calculate the difference in log-probabilities: delta_logp = logp(a) - logp(b).\n2. Calculate the difference in costs: delta_cost = cost(b) - cost(a).\n3. Inherit from Parent 1: Compute a hybrid adaptive scale. Calculate the standard deviations of delta_logp and delta_cost. The adaptive_scale is a weighted sum: `w_logp * std(delta_logp) + w_cost * std(delta_cost)`, clipped for stability.\n4. Inherit from Parent 0: Compute an adaptive beta for the loss, inversely proportional to the hybrid adaptive scale: `beta = 1.0 / adaptive_scale`.\n5. New Coupling 2: Calculate a base margin using `softplus` instead of `sigmoid`. `base_margin = softplus(delta_cost / temp)`. This avoids saturation for large cost differences.\n6. New Coupling 1: Compute a Z-Score Gate. Calculate the z-score of delta_cost for the batch: `z_score = (delta_cost - mean(delta_cost)) / (std(delta_cost) + epsilon)`. The gate is `gate = sigmoid(z_score)`.\n7. Calculate the final gated margin: `margin = adaptive_scale * base_margin * gate`.\n8. Compute the final loss argument: `loss_arg = delta_logp - margin`.\n9. Calculate the final loss using the scaled negative log-sigmoid function: `loss = -logsigmoid(beta * loss_arg)`.\n10. Return the mean of the loss across the batch.", "hyperparams": {"temp": 1.0, "w_logp": 0.5, "w_cost": 0.5, "min_scale": 0.1, "max_scale": 5.0, "epsilon": 1e-08}, "operators_used": ["logsigmoid", "softplus", "sigmoid", "clamp"], "implementation_hint": {"expects": ["cost_a", "cost_b", "logp_a", "logp_b"], "returns": "scalar"}, "code": "def generated_loss(batch, model_output, extra):\n    \"\"\"\n    A logistic loss with a z-score gated, softplus-based adaptive margin.\n    \"\"\"\n    # Read tensors from the batch\n    cost_a = batch['cost_a']\n    cost_b = batch['cost_b']\n    logp_a = batch['log_prob_w']\n    logp_b = batch['log_prob_l']\n    weights = batch.get('weight')\n\n    # Read hyperparameters\n    hyperparams = extra['hyperparams']\n    temp = hyperparams.get('temp', 1.0)\n    w_logp = hyperparams.get('w_logp', 0.5)\n    w_cost = hyperparams.get('w_cost', 0.5)\n    min_scale = hyperparams.get('min_scale', 0.1)\n    max_scale = hyperparams.get('max_scale', 5.0)\n    epsilon = hyperparams.get('epsilon', 1e-8)\n\n    # 1. Calculate log-probability and cost differences\n    delta_logp = logp_a - logp_b\n    delta_cost = cost_b - cost_a\n\n    # 2. Compute batch statistics (detached for stability)\n    if delta_logp.numel() > 1:\n        logp_std = delta_logp.detach().std()\n        cost_std = delta_cost.detach().std()\n        cost_mean = delta_cost.detach().mean()\n    else:\n        logp_std = torch.tensor(1.0, device=delta_logp.device)\n        cost_std = torch.tensor(1.0, device=delta_cost.device)\n        cost_mean = torch.tensor(0.0, device=delta_cost.device)\n\n    # 3. Inherit from Parent 1: Hybrid adaptive scale\n    hybrid_scale = w_logp * logp_std + w_cost * cost_std\n    adaptive_scale = torch.clamp(hybrid_scale, min_scale, max_scale)\n\n    # 4. Inherit from Parent 0: Adaptive beta from scale\n    beta = 1.0 / (adaptive_scale.detach() + epsilon)\n\n    # 5. New Coupling 2: Base margin with softplus for non-saturation\n    base_margin = torch.nn.functional.softplus(delta_cost / temp)\n\n    # 6. New Coupling 1: Z-Score Gating Mechanism\n    cost_z_score = (delta_cost.detach() - cost_mean) / (cost_std + epsilon)\n    gate = torch.sigmoid(cost_z_score)\n\n    # 7. Calculate final gated margin\n    margin = adaptive_scale * base_margin * gate\n\n    # 8. Compute the loss argument\n    loss_arg = delta_logp - margin\n\n    # 9. Apply negative log-sigmoid loss\n    loss_per_pair = -torch.nn.functional.logsigmoid(beta * loss_arg)\n\n    # 10. Apply optional weights and compute the mean loss\n    if weights is not None:\n        loss = (loss_per_pair * weights).sum() / weights.sum().clamp(min=epsilon)\n    else:\n        loss = loss_per_pair.mean()\n\n    return loss", "theoretical_basis": "A Bradley-Terry logistic preference model where the target log-odds margin is dynamically scaled and gated. The margin's scale is coupled to a hybrid measure of batch difficulty (logp and cost variance). The margin's application is gated by the statistical typicality (z-score) of the cost difference within the batch, focusing learning on the most informative preference pairs."}, "fitness": {"hf_like_score": 7.889140964508057, "validation_objective": 7.8880969955444336, "generalization_penalty": 0.001043968963623243, "generalization_objectives": {"100": 7.889140964508057}, "train_score_mean": 8.791523804179537, "train_loss_mean": 0.41214337949941954, "pair_count": 4951579525, "early_eval": {"enabled": true, "steps": 100, "baseline_validation_objective": 8.412976063537597, "candidate_validation_objective": 8.347158346557617, "early_stopped": false}, "phases": {"f1": {"steps": 15630, "train_score_mean": 8.791523804179537, "train_loss_mean": 0.41214337949941954, "pair_count": 4951579525}, "f2": {"steps": 0, "train_score_mean": null, "train_loss_mean": null, "pair_count": 0}}, "config": {"hf": {"problem": "tsp", "hf_steps": 0, "hf_epochs": 10, "hf_instances_per_epoch": 100000, "train_problem_size": 100, "valid_problem_sizes": [100], "train_batch_size": 64, "pomo_size": 100, "learning_rate": 0.0003, "weight_decay": 1e-06, "alpha": 0.05, "device": "cuda:2", "seed": 1234, "num_validation_episodes": 10000, "validation_batch_size": 64, "generalization_penalty_weight": 1.0, "pool_version": "v0"}, "free_loss": {"f1_steps": 0, "total_train_steps": 15630, "f2_steps": 0, "f3_enabled": false}}, "loss_ir": {"name": "ZScore_Gated_Adaptive_Margin_Loss", "intuition": "Mode: combine. This loss function combines the adaptive margin and temperature mechanisms from both parents while introducing a novel gating mechanism to improve stability and responsiveness. It inherits the core `logsigmoid(beta * (delta_logp - margin))` structure, which is a proven foundation from both parents. From Parent 0, it adopts the inverse coupling between the loss temperature `beta` and an `adaptive_scale` derived from `std(delta_logp)`. From Parent 1, it inherits the idea of a hybrid `adaptive_scale` that considers the variance of both `delta_logp` and `delta_cost`, providing a more holistic measure of batch difficulty.\n\nThe first new coupling idea is a **Z-Score Gating Mechanism** for the margin. Instead of always applying the full margin, we compute the z-score of `delta_cost` for each pair in the batch. The margin is then multiplied by `sigmoid(z_score)`. This gate has two effects: 1) For pairs with a cost difference close to the batch average (`z_score` ~ 0), the margin is down-weighted (`sigmoid(0)`=0.5), preventing over-enforcement of average preferences. 2) For pairs with a very large cost difference (large positive `z_score`), the full margin is applied, focusing learning on the most significant preferences. This adaptively prioritizes pairs based on their cost-gap extremity within the batch. The second new coupling is the use of `softplus` instead of `sigmoid` for the cost-to-margin transformation. `softplus` is non-saturating for large cost differences, which means that unlike `sigmoid`, it will continue to increase the margin for very large `delta_cost`, ensuring that the model is always incentivized to respect significant cost gaps.\n\nThe theoretical basis remains a Bradley-Terry model, but the target log-odds (the margin) is now gated by the statistical typicality of the cost difference, making the learning signal more focused and robust to noise from pairs with average cost gaps.", "hyperparams": {"temp": 1.0, "w_logp": 0.5, "w_cost": 0.5, "min_scale": 0.1, "max_scale": 5.0, "epsilon": 1e-08}, "operators_used": ["logsigmoid", "softplus", "sigmoid", "clamp"]}}, "better_than_baseline": true}
{"generation": 5, "index": 4, "ir": {"name": "ZScore_Regulated_LogSigmoid_Loss", "intuition": "Mode: combine. This loss function integrates the successful adaptive margin and temperature concepts from its parents while introducing a new normalization scheme to enhance stability and responsiveness. It inherits the core Bradley-Terry structure `logsigmoid(beta * (delta_logp - margin))`. From both parents, it takes the idea of an adaptive `beta` inversely proportional to a scale factor, and a margin whose magnitude is also proportional to this scale factor. From Parent 1, it specifically inherits the blended margin structure (`alpha * margin_A + (1-alpha) * margin_B`) where `margin_A` uses a fixed temperature and `margin_B` uses an adaptive one based on `std(delta_cost)`.\n\nThe first new coupling is the use of **z-score normalization** on `delta_cost`. Instead of using raw `delta_cost` values in the margin calculation, we use `z_cost = (delta_cost - mean(delta_cost)) / std(delta_cost)`. This makes the margin's sensitivity independent of the absolute scale of costs in a batch, focusing instead on the relative cost differences. This prevents very large or small cost gaps from dominating the margin calculation and improves stability across different problem domains. The second new coupling is a **dynamic alpha** for the margin blend, similar in spirit to Parent 1's dynamic alpha, but now based on the skewness of the `delta_logp` distribution. `alpha = sigmoid(skew(delta_logp))`. This allows the loss to dynamically shift its margin composition. When the model's predictions are skewed (e.g., many easy pairs and a few very hard ones), it adjusts the blend, potentially favoring the more stable fixed-temperature margin to prevent over-correction on outliers. This makes the loss self-regulating based on the distribution of model confidence.", "pseudocode": "1. Calculate the difference in log-probabilities: delta_logp = logp(a) - logp(b).\n2. Calculate the difference in costs: delta_cost = cost(b) - cost(a).\n3. Inherit from Parent 0: Compute an adaptive scale based on the standard deviation of delta_logp, clipped for stability.\n4. Inherit from Parent 0: Compute an adaptive beta for the loss, inversely proportional to the adaptive scale: `beta = 1.0 / adaptive_scale`.\n5. New Coupling 1: Normalize delta_cost using z-scoring: `z_cost = (delta_cost - mean(delta_cost)) / (std(delta_cost) + epsilon)`.\n6. Inherit from Parent 1: Calculate the two base margin components, but using the normalized `z_cost`. `margin_A` uses a fixed temperature. `margin_B` uses an adaptive temperature (which is now implicitly 1.0 due to the z-scoring, but we keep the structure for clarity).\n7. New Coupling 2: Compute a dynamic blending factor `alpha` based on the skewness of the delta_logp distribution: `alpha = sigmoid(skew(delta_logp))`. A small constant is added to the denominator of skew to prevent division by zero.\n8. Inherit from Parent 1: Combine the two margin components using the dynamic alpha: `margin = adaptive_scale * (alpha * sigmoid(z_cost / temp_A) + (1 - alpha) * sigmoid(z_cost / temp_B))`.\n9. Compute the final loss argument: `loss_arg = delta_logp - margin`.\n10. Calculate the final loss using the scaled negative log-sigmoid function: `loss = -logsigmoid(beta * loss_arg)`.\n11. Return the mean of the loss across the batch.", "hyperparams": {"fixed_temp": 1.0, "min_scale": 0.1, "max_scale": 5.0, "epsilon": 1e-08}, "operators_used": ["logsigmoid", "sigmoid", "clamp"], "implementation_hint": {"expects": ["cost_a", "cost_b", "logp_a", "logp_b"], "returns": "scalar"}, "code": "def generated_loss(batch, model_output, extra):\n    \"\"\"\n    A logistic loss with a z-score regulated, dynamically blended margin.\n    \"\"\"\n    # Read tensors from the batch\n    cost_a = batch['cost_a']\n    cost_b = batch['cost_b']\n    logp_a = batch['log_prob_w']\n    logp_b = batch['log_prob_l']\n    weights = batch.get('weight')\n\n    # Read hyperparameters\n    hyperparams = extra['hyperparams']\n    fixed_temp = hyperparams.get('fixed_temp', 1.0)\n    min_scale = hyperparams.get('min_scale', 0.1)\n    max_scale = hyperparams.get('max_scale', 5.0)\n    epsilon = hyperparams.get('epsilon', 1e-8)\n\n    # 1. Calculate log-probability and cost differences\n    delta_logp = logp_a - logp_b\n    delta_cost = cost_b - cost_a\n\n    # 2. Compute batch statistics (detached for stability)\n    n = delta_logp.numel()\n    if n > 1:\n        logp_std = delta_logp.detach().std()\n        cost_std = delta_cost.detach().std()\n        cost_mean = delta_cost.detach().mean()\n        # For skewness calculation\n        logp_mean = delta_logp.detach().mean()\n        logp_m3 = torch.mean((delta_logp.detach() - logp_mean)**3)\n        logp_skew = logp_m3 / (logp_std**3 + epsilon)\n    else:\n        logp_std = torch.tensor(1.0, device=delta_logp.device)\n        cost_std = torch.tensor(1.0, device=delta_cost.device)\n        cost_mean = torch.tensor(0.0, device=delta_cost.device)\n        logp_skew = torch.tensor(0.0, device=delta_logp.device)\n\n    # 3. Inherit from Parent 0: Adaptive scale from logp variance\n    adaptive_scale = torch.clamp(logp_std, min_scale, max_scale)\n\n    # 4. Inherit from Parent 0: Adaptive beta from scale\n    beta = 1.0 / (adaptive_scale.detach() + epsilon)\n\n    # 5. New Coupling 1: Z-score normalization of delta_cost\n    z_cost = (delta_cost - cost_mean) / (cost_std + epsilon)\n\n    # 6. New Coupling 2: Dynamic alpha from logp skewness\n    alpha = torch.sigmoid(logp_skew)\n\n    # 7. Inherit from Parent 1: Blended margin using z_cost\n    # margin_A with fixed temperature\n    cost_weight_A = torch.sigmoid(z_cost / fixed_temp)\n    # margin_B with adaptive temperature (temp is 1.0 for z-scored data)\n    cost_weight_B = torch.sigmoid(z_cost)\n    \n    blended_cost_weight = alpha * cost_weight_A + (1.0 - alpha) * cost_weight_B\n    margin = adaptive_scale * blended_cost_weight\n\n    # 8. Compute the loss argument\n    loss_arg = delta_logp - margin.detach() # Detach margin for stability as in IPO/DPO\n\n    # 9. Apply negative log-sigmoid loss\n    loss_per_pair = -torch.nn.functional.logsigmoid(beta * loss_arg)\n\n    # 10. Apply optional weights and compute the mean loss\n    if weights is not None:\n        loss = (loss_per_pair * weights).sum() / weights.sum().clamp(min=epsilon)\n    else:\n        loss = loss_per_pair.mean()\n\n    return loss", "theoretical_basis": "A Bradley-Terry logistic preference model with a dynamically blended margin. The loss temperature and margin scale are coupled to model uncertainty (std of logp differences). The margin's sensitivity to cost is stabilized by using z-scored cost differences, making it robust to the absolute scale of costs. The margin's composition (the blend) adapts to the skewness of the model's log-probability distribution, creating a more nuanced, self-regulating objective."}, "fitness": {"hf_like_score": 7.879538204956055, "validation_objective": 7.8783921524047855, "generalization_penalty": 0.0011460525512694275, "generalization_objectives": {"100": 7.879538204956055}, "train_score_mean": 8.68976236582565, "train_loss_mean": 0.4521159295233411, "pair_count": 4951579227, "early_eval": {"enabled": true, "steps": 100, "baseline_validation_objective": 8.412976063537597, "candidate_validation_objective": 8.310545304870605, "early_stopped": false}, "phases": {"f1": {"steps": 15630, "train_score_mean": 8.68976236582565, "train_loss_mean": 0.4521159295233411, "pair_count": 4951579227}, "f2": {"steps": 0, "train_score_mean": null, "train_loss_mean": null, "pair_count": 0}}, "config": {"hf": {"problem": "tsp", "hf_steps": 0, "hf_epochs": 10, "hf_instances_per_epoch": 100000, "train_problem_size": 100, "valid_problem_sizes": [100], "train_batch_size": 64, "pomo_size": 100, "learning_rate": 0.0003, "weight_decay": 1e-06, "alpha": 0.05, "device": "cuda:3", "seed": 1234, "num_validation_episodes": 10000, "validation_batch_size": 64, "generalization_penalty_weight": 1.0, "pool_version": "v0"}, "free_loss": {"f1_steps": 0, "total_train_steps": 15630, "f2_steps": 0, "f3_enabled": false}}, "loss_ir": {"name": "ZScore_Regulated_LogSigmoid_Loss", "intuition": "Mode: combine. This loss function integrates the successful adaptive margin and temperature concepts from its parents while introducing a new normalization scheme to enhance stability and responsiveness. It inherits the core Bradley-Terry structure `logsigmoid(beta * (delta_logp - margin))`. From both parents, it takes the idea of an adaptive `beta` inversely proportional to a scale factor, and a margin whose magnitude is also proportional to this scale factor. From Parent 1, it specifically inherits the blended margin structure (`alpha * margin_A + (1-alpha) * margin_B`) where `margin_A` uses a fixed temperature and `margin_B` uses an adaptive one based on `std(delta_cost)`.\n\nThe first new coupling is the use of **z-score normalization** on `delta_cost`. Instead of using raw `delta_cost` values in the margin calculation, we use `z_cost = (delta_cost - mean(delta_cost)) / std(delta_cost)`. This makes the margin's sensitivity independent of the absolute scale of costs in a batch, focusing instead on the relative cost differences. This prevents very large or small cost gaps from dominating the margin calculation and improves stability across different problem domains. The second new coupling is a **dynamic alpha** for the margin blend, similar in spirit to Parent 1's dynamic alpha, but now based on the skewness of the `delta_logp` distribution. `alpha = sigmoid(skew(delta_logp))`. This allows the loss to dynamically shift its margin composition. When the model's predictions are skewed (e.g., many easy pairs and a few very hard ones), it adjusts the blend, potentially favoring the more stable fixed-temperature margin to prevent over-correction on outliers. This makes the loss self-regulating based on the distribution of model confidence.", "hyperparams": {"fixed_temp": 1.0, "min_scale": 0.1, "max_scale": 5.0, "epsilon": 1e-08}, "operators_used": ["logsigmoid", "sigmoid", "clamp"]}}, "better_than_baseline": true}
{"generation": 5, "index": 7, "ir": {"name": "ZScore_Regulated_Hybrid_Margin_Loss", "intuition": "Mode: combine. This loss function combines the sophisticated adaptive margin structures of both parents while introducing a novel normalization scheme to enhance stability and responsiveness. It inherits the core `logsigmoid(beta * (delta_logp - margin))` framework, a proven Bradley-Terry style foundation. From both parents, it inherits the concept of an `adaptive_scale` that modulates both the margin's magnitude and the loss's temperature (`beta = 1/adaptive_scale`). From Parent 1, it specifically inherits the idea of a blended margin (`alpha * margin_A + (1-alpha) * margin_B`), where `margin_A` uses a fixed temperature and `margin_B` uses an adaptive one based on `std(delta_cost)`. This provides a robust, dual-sensitivity margin.\n\nThe first new coupling idea is the **Z-Score Normalization of Costs**. Instead of using raw `delta_cost` in the sigmoid functions that form the margin, I use its z-score: `(delta_cost - mean(delta_cost)) / std(delta_cost)`. This makes the margin's sensitivity invariant to the absolute scale and shift of costs in a batch, focusing only on the relative cost differences. It prevents margin saturation or insensitivity when costs are all very high/low or have very large/small variance. The second new coupling is a **Softplus-based Adaptive Beta**. Instead of `beta = 1 / adaptive_scale`, I use `beta = softplus(1 / adaptive_scale - 1)`. This ensures `beta` is always positive and non-zero, preventing potential division-by-zero or instability if `adaptive_scale` becomes very large. It also provides a smoother, non-linear mapping from scale to temperature, dampening extreme beta values.", "pseudocode": "1. Calculate the difference in log-probabilities: delta_logp = logp(a) - logp(b).\n2. Calculate the difference in costs: delta_cost = cost(b) - cost(a).\n3. Compute batch statistics for delta_logp and delta_cost (mean and standard deviation) for normalization and adaptation, detaching gradients for stability.\n4. Inherit from Parents: Compute a hybrid adaptive scale based on a weighted sum of logp and cost standard deviations: `adaptive_scale = w_logp * std(delta_logp) + w_cost * std(delta_cost)`.\n5. New Coupling 1 (Z-Score Normalization): Normalize the cost differences: `z_delta_cost = (delta_cost - mean(delta_cost)) / (std(delta_cost) + epsilon)`.\n6. Inherit from Parent 1 (Blended Margin): Calculate two margin components using the normalized cost. `margin_A` uses a fixed temperature: `sigmoid(z_delta_cost / fixed_temp)`. `margin_B` uses an adaptive temperature based on `std(delta_cost)`: `sigmoid(z_delta_cost / adaptive_cost_temp)`.\n7. Scale the margin components by the adaptive scale and blend them using a fixed hyperparameter `alpha`: `margin = adaptive_scale * (alpha * margin_A_weight + (1 - alpha) * margin_B_weight)`.\n8. New Coupling 2 (Softplus Beta): Compute an adaptive beta for the loss using a softplus function for stability: `beta = softplus(1 / adaptive_scale - 1)`.\n9. Compute the final loss argument: `loss_arg = delta_logp - margin`.\n10. Calculate the final loss using the scaled negative log-sigmoid function: `loss = -logsigmoid(beta * loss_arg)`.\n11. Return the mean of the loss across the batch.", "hyperparams": {"fixed_temp": 1.0, "alpha": 0.5, "w_logp": 0.5, "w_cost": 0.5, "min_scale": 0.1, "max_scale": 5.0, "epsilon": 1e-08}, "operators_used": ["logsigmoid", "sigmoid", "softplus", "clamp"], "implementation_hint": {"expects": ["cost_a", "cost_b", "logp_a", "logp_b"], "returns": "scalar"}, "code": "def generated_loss(batch, model_output, extra):\n    \"\"\"\n    A logistic loss with a hybrid adaptive margin based on z-scored costs\n    and a softplus-stabilized adaptive beta.\n    \"\"\"\n    # Read tensors from the batch\n    cost_a = batch['cost_a']\n    cost_b = batch['cost_b']\n    logp_a = batch['log_prob_w']\n    logp_b = batch['log_prob_l']\n    weights = batch.get('weight')\n\n    # Read hyperparameters\n    hyperparams = extra['hyperparams']\n    fixed_temp = hyperparams.get('fixed_temp', 1.0)\n    alpha = hyperparams.get('alpha', 0.5)\n    w_logp = hyperparams.get('w_logp', 0.5)\n    w_cost = hyperparams.get('w_cost', 0.5)\n    min_scale = hyperparams.get('min_scale', 0.1)\n    max_scale = hyperparams.get('max_scale', 5.0)\n    epsilon = hyperparams.get('epsilon', 1e-8)\n\n    # 1. Calculate log-probability and cost differences\n    delta_logp = logp_a - logp_b\n    delta_cost = cost_b - cost_a\n\n    # 2. Compute batch statistics (detached for stability)\n    if delta_logp.numel() > 1:\n        logp_std = delta_logp.detach().std()\n        cost_std = delta_cost.detach().std()\n        cost_mean = delta_cost.detach().mean()\n    else:\n        logp_std = torch.tensor(1.0, device=delta_logp.device)\n        cost_std = torch.tensor(1.0, device=delta_cost.device)\n        cost_mean = torch.tensor(0.0, device=delta_cost.device)\n\n    # 3. Inherit: Hybrid adaptive scale\n    hybrid_scale = w_logp * logp_std + w_cost * cost_std\n    adaptive_scale = torch.clamp(hybrid_scale, min_scale, max_scale)\n\n    # 4. New Coupling 1: Z-Score Normalization of costs\n    z_delta_cost = (delta_cost - cost_mean) / (cost_std + epsilon)\n\n    # 5. Inherit: Margin components, now using z-scored costs\n    # margin_A with fixed temperature\n    cost_weight_A = torch.sigmoid(z_delta_cost / fixed_temp)\n    \n    # margin_B with adaptive temperature (still based on raw cost std)\n    adaptive_cost_temp = cost_std + epsilon\n    cost_weight_B = torch.sigmoid(z_delta_cost / adaptive_cost_temp)\n\n    # 6. Inherit: Blend the two margin weights\n    blended_cost_weight = alpha * cost_weight_A + (1.0 - alpha) * cost_weight_B\n    margin = adaptive_scale * blended_cost_weight\n\n    # 7. New Coupling 2: Softplus-based Adaptive Beta\n    # beta = softplus(1/scale - 1) ensures beta > 0 and is smoother\n    beta_arg = (1.0 / (adaptive_scale.detach() + epsilon)) - 1.0\n    beta = torch.nn.functional.softplus(beta_arg)\n\n    # 8. Compute the loss argument\n    loss_arg = delta_logp - margin\n\n    # 9. Apply negative log-sigmoid loss\n    loss_per_pair = -torch.nn.functional.logsigmoid(beta * loss_arg)\n\n    # 10. Apply optional weights and compute the mean loss\n    if weights is not None:\n        loss = (loss_per_pair * weights).sum() / weights.sum().clamp(min=epsilon)\n    else:\n        loss = loss_per_pair.mean()\n\n    return loss", "theoretical_basis": "A Bradley-Terry logistic preference model where the target log-odds is a dynamic, blended margin. The margin's sensitivity is based on the z-score of cost differences, making it robust to cost scale and shift. The margin's magnitude and the loss temperature are coupled to a hybrid measure of batch difficulty (logp and cost variance). The loss temperature (beta) is further stabilized via a softplus transformation."}, "fitness": {"hf_like_score": 8.678068133544922, "validation_objective": 8.678068133544922, "generalization_penalty": 0.0, "generalization_objectives": {"100": 8.674683822631836}, "train_score_mean": 15.020629806518555, "train_loss_mean": 0.8716503179073334, "pair_count": 31679972, "early_eval": {"enabled": true, "steps": 100, "baseline_validation_objective": 8.412976063537597, "candidate_validation_objective": 8.678068133544922, "early_stopped": true}, "phases": {"f1": {"steps": 15630, "train_score_mean": 15.020629806518555, "train_loss_mean": 0.8716503179073334, "pair_count": 31679972}, "f2": {"steps": 0, "train_score_mean": null, "train_loss_mean": null, "pair_count": 0}}, "config": {"hf": {"problem": "tsp", "hf_steps": 0, "hf_epochs": 10, "hf_instances_per_epoch": 100000, "train_problem_size": 100, "valid_problem_sizes": [100], "train_batch_size": 64, "pomo_size": 100, "learning_rate": 0.0003, "weight_decay": 1e-06, "alpha": 0.05, "device": "cuda:0", "seed": 1234, "num_validation_episodes": 10000, "validation_batch_size": 64, "generalization_penalty_weight": 1.0, "pool_version": "v0"}, "free_loss": {"f1_steps": 0, "total_train_steps": 15630, "f2_steps": 0, "f3_enabled": false}}, "loss_ir": {"name": "ZScore_Regulated_Hybrid_Margin_Loss", "intuition": "Mode: combine. This loss function combines the sophisticated adaptive margin structures of both parents while introducing a novel normalization scheme to enhance stability and responsiveness. It inherits the core `logsigmoid(beta * (delta_logp - margin))` framework, a proven Bradley-Terry style foundation. From both parents, it inherits the concept of an `adaptive_scale` that modulates both the margin's magnitude and the loss's temperature (`beta = 1/adaptive_scale`). From Parent 1, it specifically inherits the idea of a blended margin (`alpha * margin_A + (1-alpha) * margin_B`), where `margin_A` uses a fixed temperature and `margin_B` uses an adaptive one based on `std(delta_cost)`. This provides a robust, dual-sensitivity margin.\n\nThe first new coupling idea is the **Z-Score Normalization of Costs**. Instead of using raw `delta_cost` in the sigmoid functions that form the margin, I use its z-score: `(delta_cost - mean(delta_cost)) / std(delta_cost)`. This makes the margin's sensitivity invariant to the absolute scale and shift of costs in a batch, focusing only on the relative cost differences. It prevents margin saturation or insensitivity when costs are all very high/low or have very large/small variance. The second new coupling is a **Softplus-based Adaptive Beta**. Instead of `beta = 1 / adaptive_scale`, I use `beta = softplus(1 / adaptive_scale - 1)`. This ensures `beta` is always positive and non-zero, preventing potential division-by-zero or instability if `adaptive_scale` becomes very large. It also provides a smoother, non-linear mapping from scale to temperature, dampening extreme beta values.", "hyperparams": {"fixed_temp": 1.0, "alpha": 0.5, "w_logp": 0.5, "w_cost": 0.5, "min_scale": 0.1, "max_scale": 5.0, "epsilon": 1e-08}, "operators_used": ["logsigmoid", "sigmoid", "softplus", "clamp"]}}, "better_than_baseline": false}
{"generation": 6, "index": 1, "ir": {"name": "Progressive_Margin_LogSigmoid_with_Variance_Gating", "intuition": "Based on the compile error E_COMPILE_ERROR, I removed the `import torch` and `import torch.nn.functional as F` statements from the `code` section. These imports are disallowed as the execution environment provides these libraries automatically. The core logic of the loss function, which combines a progressive margin with a variance-based gating mechanism, remains unchanged.", "pseudocode": "1. Calculate the difference in log-probabilities: delta_logp = logp_a - logp_b.\n2. Calculate the difference in costs: delta_cost = cost_b - cost_a.\n3. Compute batch statistics: standard deviation of delta_logp (logp_std) and delta_cost (cost_std).\n4. Compute a hybrid adaptive scale as a weighted sum of logp_std and cost_std: `adaptive_scale = w_logp * logp_std + w_cost * cost_std`, clipped for stability.\n5. Compute an adaptive beta for the loss, inversely proportional to the adaptive scale: `beta = 1.0 / adaptive_scale`.\n6. Calculate a base margin using a centered sigmoid: `base_margin = adaptive_scale * (sigmoid(delta_cost / fixed_temp) - 0.5)`.\n7. Compute a gate value based on the relative variance of log-probabilities and costs: `gate = sigmoid((logp_std - cost_std) / gate_temp)`.\n8. Apply the gate to the base margin: `final_margin = gate * base_margin`.\n9. Compute the final loss argument: `loss_arg = delta_logp - final_margin`.\n10. Calculate the final loss using the scaled negative log-sigmoid function: `loss = -logsigmoid(beta * loss_arg)`.\n11. Return the mean of the loss across the batch.", "hyperparams": {"fixed_temp": 1.0, "gate_temp": 1.0, "w_logp": 0.5, "w_cost": 0.5, "min_scale": 0.1, "max_scale": 5.0, "epsilon": 1e-08}, "operators_used": ["logsigmoid", "sigmoid", "clamp"], "implementation_hint": {"expects": ["The `batch` dict should contain `cost_a`, `cost_b`, `log_prob_w`, `log_prob_l`, and an optional `weight` tensor. `log_prob_w` corresponds to the preferred candidate (a) and `log_prob_l` to the dispreferred one (b)."], "returns": "A single scalar tensor representing the mean loss for the batch."}, "code": "def generated_loss(batch, model_output, extra):\n    \"\"\"\n    A logistic loss with a progressive margin gated by relative variance.\n    \"\"\"\n    # The execution environment provides `torch` and `torch.nn.functional` as `F`.\n\n    # Read tensors from the batch\n    cost_a = batch['cost_a']\n    cost_b = batch['cost_b']\n    logp_a = batch['log_prob_w']\n    logp_b = batch['log_prob_l']\n    weights = batch.get('weight')\n\n    # Read hyperparameters\n    hyperparams = extra['hyperparams']\n    fixed_temp = hyperparams.get('fixed_temp', 1.0)\n    gate_temp = hyperparams.get('gate_temp', 1.0)\n    w_logp = hyperparams.get('w_logp', 0.5)\n    w_cost = hyperparams.get('w_cost', 0.5)\n    min_scale = hyperparams.get('min_scale', 0.1)\n    max_scale = hyperparams.get('max_scale', 5.0)\n    epsilon = hyperparams.get('epsilon', 1e-8)\n\n    # 1. Calculate log-probability and cost differences\n    delta_logp = logp_a - logp_b\n    delta_cost = cost_b - cost_a\n\n    # 2. Compute batch statistics (detached for stability)\n    if delta_logp.numel() > 1:\n        logp_std = delta_logp.detach().std()\n        cost_std = delta_cost.detach().std()\n    else:\n        logp_std = torch.tensor(1.0, device=delta_logp.device)\n        cost_std = torch.tensor(1.0, device=delta_cost.device)\n\n    # 3. Hybrid adaptive scale\n    hybrid_scale = w_logp * logp_std + w_cost * cost_std\n    adaptive_scale = torch.clamp(hybrid_scale, min_scale, max_scale)\n\n    # 4. Adaptive beta from scale\n    beta = 1.0 / (adaptive_scale.detach() + epsilon)\n\n    # 5. Progressive Margin Base\n    base_margin = adaptive_scale * (torch.sigmoid(delta_cost / fixed_temp) - 0.5)\n\n    # 6. Variance-based Gating\n    gate = torch.sigmoid((logp_std - cost_std) / gate_temp)\n\n    # 7. Apply gate to margin\n    final_margin = gate.detach() * base_margin\n\n    # 8. Compute the loss argument\n    loss_arg = delta_logp - final_margin\n\n    # 9. Apply negative log-sigmoid loss\n    loss_per_pair = -F.logsigmoid(beta * loss_arg)\n\n    # 10. Apply optional weights and compute the mean loss\n    if weights is not None:\n        loss = (loss_per_pair * weights).sum() / weights.sum().clamp(min=epsilon)\n    else:\n        loss = loss_per_pair.mean()\n\n    return loss", "theoretical_basis": ""}, "fitness": {"hf_like_score": 7.889989749145508, "validation_objective": 7.88870414276123, "generalization_penalty": 0.001285606384278104, "generalization_objectives": {"100": 7.889989749145508}, "train_score_mean": 9.06024690824522, "train_loss_mean": 0.3156875024162953, "pair_count": 4951580006, "early_eval": {"enabled": true, "steps": 100, "baseline_validation_objective": 8.412976063537597, "candidate_validation_objective": 8.379873875427245, "early_stopped": false}, "phases": {"f1": {"steps": 15630, "train_score_mean": 9.06024690824522, "train_loss_mean": 0.3156875024162953, "pair_count": 4951580006}, "f2": {"steps": 0, "train_score_mean": null, "train_loss_mean": null, "pair_count": 0}}, "config": {"hf": {"problem": "tsp", "hf_steps": 0, "hf_epochs": 10, "hf_instances_per_epoch": 100000, "train_problem_size": 100, "valid_problem_sizes": [100], "train_batch_size": 64, "pomo_size": 100, "learning_rate": 0.0003, "weight_decay": 1e-06, "alpha": 0.05, "device": "cuda:0", "seed": 1234, "num_validation_episodes": 10000, "validation_batch_size": 64, "generalization_penalty_weight": 1.0, "pool_version": "v0"}, "free_loss": {"f1_steps": 0, "total_train_steps": 15630, "f2_steps": 0, "f3_enabled": false}}, "loss_ir": {"name": "Progressive_Margin_LogSigmoid_with_Variance_Gating", "intuition": "Based on the compile error E_COMPILE_ERROR, I removed the `import torch` and `import torch.nn.functional as F` statements from the `code` section. These imports are disallowed as the execution environment provides these libraries automatically. The core logic of the loss function, which combines a progressive margin with a variance-based gating mechanism, remains unchanged.", "hyperparams": {"fixed_temp": 1.0, "gate_temp": 1.0, "w_logp": 0.5, "w_cost": 0.5, "min_scale": 0.1, "max_scale": 5.0, "epsilon": 1e-08}, "operators_used": ["logsigmoid", "sigmoid", "clamp"]}}, "better_than_baseline": true}
{"generation": 6, "index": 2, "ir": {"name": "ZScore_Regulated_Hybrid_Margin_LogSigmoid", "intuition": "Mode: combine. This loss function builds upon the sophisticated adaptive mechanisms of its parents, introducing a novel regularization technique to enhance stability and responsiveness. It inherits the core `logsigmoid(beta * (delta_logp - margin))` structure and the concept of an adaptive `beta` inversely proportional to a scale factor, a cornerstone of both parents. From Parent 1, it inherits the idea of a blended margin composed of two parts: one with a fixed temperature and one with an adaptive temperature based on `std(delta_cost)`. From Parent 0, it inherits the specific coupling of `beta = 1.0 / adaptive_scale` where `adaptive_scale` is based on `std(delta_logp)`. \n\nThe first new coupling is the **z-score normalization of delta_logp**. Before any other calculations, `delta_logp` is normalized to have a mean of 0 and a standard deviation of 1 (`delta_logp = (delta_logp - mean) / std`). This pre-processing step ensures that the scale of the model's log-probability differences is consistent across different batches and training stages, preventing extreme values from dominating the loss and gradients. It stabilizes the `adaptive_scale` and `beta` calculations that depend on `std(delta_logp)`, which now becomes a fixed value of 1. The second new coupling is a **cost-variance-aware margin scale**. Instead of the `adaptive_scale` being derived solely from `std(delta_logp)` (which is now 1), it is modulated by the standard deviation of the cost differences: `adaptive_scale = 1.0 + w_cost * std(delta_cost)`. This makes the margin's magnitude directly responsive to the diversity of costs in the batch. When costs are very similar (low std), the margin is small, and when they are diverse (high std), the margin becomes larger, demanding a clearer separation from the model.", "pseudocode": "1. Calculate the difference in log-probabilities: delta_logp = logp(a) - logp(b).\n2. Calculate the difference in costs: delta_cost = cost(b) - cost(a).\n3. New Coupling 1: Normalize delta_logp using z-scoring. `normalized_delta_logp = (delta_logp - mean(delta_logp)) / (std(delta_logp) + epsilon)`.\n4. New Coupling 2: Compute a cost-variance-aware adaptive scale. `adaptive_scale = 1.0 + w_cost * std(delta_cost)`. Clamp for stability.\n5. Inherit from Parent 0/1: Compute an adaptive beta inversely proportional to the scale: `beta = 1.0 / adaptive_scale`.\n6. Inherit from Parent 1: Calculate the two base margin components. `margin_A` uses a fixed temperature. `margin_B` uses an adaptive temperature based on `std(delta_cost)`.\n7. Inherit from Parent 1: Combine the two margin components using a fixed blending factor `alpha`: `margin = alpha * margin_A + (1 - alpha) * margin_B`.\n8. Compute the final loss argument using the normalized logp difference: `loss_arg = normalized_delta_logp - margin`.\n9. Calculate the final loss using the scaled negative log-sigmoid function: `loss = -logsigmoid(beta * loss_arg)`.\n10. Return the mean of the loss across the batch.", "hyperparams": {"fixed_temp": 1.0, "alpha": 0.5, "w_cost": 0.5, "min_scale": 0.1, "max_scale": 5.0, "epsilon": 1e-08}, "operators_used": ["logsigmoid", "sigmoid", "clamp", "zscore"], "implementation_hint": {"expects": ["cost_a", "cost_b", "logp_a", "logp_b"], "returns": "scalar"}, "code": "def generated_loss(batch, model_output, extra):\n    \"\"\"\n    A logistic loss with a z-score normalized logp difference and a hybrid margin.\n    The margin scale adapts to cost variance, and the loss temperature is coupled to it.\n    \"\"\"\n    # Read tensors from the batch\n    cost_a = batch['cost_a']\n    cost_b = batch['cost_b']\n    logp_a = batch['log_prob_w']\n    logp_b = batch['log_prob_l']\n    weights = batch.get('weight')\n\n    # Read hyperparameters\n    hyperparams = extra['hyperparams']\n    fixed_temp = hyperparams.get('fixed_temp', 1.0)\n    alpha = hyperparams.get('alpha', 0.5)\n    w_cost = hyperparams.get('w_cost', 0.5)\n    min_scale = hyperparams.get('min_scale', 0.1)\n    max_scale = hyperparams.get('max_scale', 5.0)\n    epsilon = hyperparams.get('epsilon', 1e-8)\n\n    # 1. Calculate log-probability and cost differences\n    delta_logp = logp_a - logp_b\n    delta_cost = cost_b - cost_a\n\n    # 2. Compute batch statistics (detached for stability)\n    if delta_logp.numel() > 1:\n        logp_mean = delta_logp.detach().mean()\n        logp_std = delta_logp.detach().std()\n        cost_std = delta_cost.detach().std()\n    else:\n        logp_mean = torch.tensor(0.0, device=delta_logp.device)\n        logp_std = torch.tensor(1.0, device=delta_logp.device)\n        cost_std = torch.tensor(1.0, device=delta_cost.device)\n\n    # 3. New Coupling 1: Z-score normalization of delta_logp\n    normalized_delta_logp = (delta_logp - logp_mean) / (logp_std + epsilon)\n\n    # 4. New Coupling 2: Cost-variance-aware adaptive scale\n    scale = 1.0 + w_cost * cost_std\n    adaptive_scale = torch.clamp(scale, min_scale, max_scale)\n\n    # 5. Inherit from Parent 0/1: Adaptive beta from scale\n    beta = 1.0 / (adaptive_scale.detach() + epsilon)\n\n    # 6. Inherit from Parent 1: Margin components\n    # margin_A with fixed temperature\n    cost_weight_A = torch.sigmoid(delta_cost / fixed_temp)\n    margin_A = adaptive_scale * cost_weight_A\n    \n    # margin_B with adaptive temperature\n    adaptive_cost_temp = cost_std + epsilon\n    cost_weight_B = torch.sigmoid(delta_cost / adaptive_cost_temp)\n    margin_B = adaptive_scale * cost_weight_B\n\n    # 7. Inherit from Parent 1: Blended margin\n    margin = alpha * margin_A + (1.0 - alpha) * margin_B\n\n    # 8. Compute the loss argument with normalized logp\n    loss_arg = normalized_delta_logp - margin\n\n    # 9. Apply negative log-sigmoid loss\n    loss_per_pair = -torch.nn.functional.logsigmoid(beta * loss_arg)\n\n    # 10. Apply optional weights and compute the mean loss\n    if weights is not None:\n        loss = (loss_per_pair * weights).sum() / weights.sum().clamp(min=epsilon)\n    else:\n        loss = loss_per_pair.mean()\n\n    return loss", "theoretical_basis": "A Bradley-Terry logistic preference model applied to z-score normalized log-probability differences. The target log-odds is a blended margin whose components' temperatures adapt to cost statistics. The overall margin magnitude and the loss temperature (beta) are coupled to the variance of costs within the batch, creating a system that is robust to the scale of model outputs while remaining sensitive to the difficulty of the preference task as measured by cost diversity."}, "fitness": {"hf_like_score": 7.9179487144470215, "validation_objective": 7.915741342163086, "generalization_penalty": 0.002207372283935527, "generalization_objectives": {"100": 7.9179487144470215}, "train_score_mean": 8.030350983974191, "train_loss_mean": 1.1254736138625703, "pair_count": 4951565217, "early_eval": {"enabled": true, "steps": 100, "baseline_validation_objective": 8.412976063537597, "candidate_validation_objective": 8.399677873229981, "early_stopped": false}, "phases": {"f1": {"steps": 15630, "train_score_mean": 8.030350983974191, "train_loss_mean": 1.1254736138625703, "pair_count": 4951565217}, "f2": {"steps": 0, "train_score_mean": null, "train_loss_mean": null, "pair_count": 0}}, "config": {"hf": {"problem": "tsp", "hf_steps": 0, "hf_epochs": 10, "hf_instances_per_epoch": 100000, "train_problem_size": 100, "valid_problem_sizes": [100], "train_batch_size": 64, "pomo_size": 100, "learning_rate": 0.0003, "weight_decay": 1e-06, "alpha": 0.05, "device": "cuda:1", "seed": 1234, "num_validation_episodes": 10000, "validation_batch_size": 64, "generalization_penalty_weight": 1.0, "pool_version": "v0"}, "free_loss": {"f1_steps": 0, "total_train_steps": 15630, "f2_steps": 0, "f3_enabled": false}}, "loss_ir": {"name": "ZScore_Regulated_Hybrid_Margin_LogSigmoid", "intuition": "Mode: combine. This loss function builds upon the sophisticated adaptive mechanisms of its parents, introducing a novel regularization technique to enhance stability and responsiveness. It inherits the core `logsigmoid(beta * (delta_logp - margin))` structure and the concept of an adaptive `beta` inversely proportional to a scale factor, a cornerstone of both parents. From Parent 1, it inherits the idea of a blended margin composed of two parts: one with a fixed temperature and one with an adaptive temperature based on `std(delta_cost)`. From Parent 0, it inherits the specific coupling of `beta = 1.0 / adaptive_scale` where `adaptive_scale` is based on `std(delta_logp)`. \n\nThe first new coupling is the **z-score normalization of delta_logp**. Before any other calculations, `delta_logp` is normalized to have a mean of 0 and a standard deviation of 1 (`delta_logp = (delta_logp - mean) / std`). This pre-processing step ensures that the scale of the model's log-probability differences is consistent across different batches and training stages, preventing extreme values from dominating the loss and gradients. It stabilizes the `adaptive_scale` and `beta` calculations that depend on `std(delta_logp)`, which now becomes a fixed value of 1. The second new coupling is a **cost-variance-aware margin scale**. Instead of the `adaptive_scale` being derived solely from `std(delta_logp)` (which is now 1), it is modulated by the standard deviation of the cost differences: `adaptive_scale = 1.0 + w_cost * std(delta_cost)`. This makes the margin's magnitude directly responsive to the diversity of costs in the batch. When costs are very similar (low std), the margin is small, and when they are diverse (high std), the margin becomes larger, demanding a clearer separation from the model.", "hyperparams": {"fixed_temp": 1.0, "alpha": 0.5, "w_cost": 0.5, "min_scale": 0.1, "max_scale": 5.0, "epsilon": 1e-08}, "operators_used": ["logsigmoid", "sigmoid", "clamp", "zscore"]}}, "better_than_baseline": false}
{"generation": 6, "index": 7, "ir": {"name": "Progressive_Hybrid_LogSigmoid", "intuition": "Based on the compile error E_COMPILE_ERROR, I have removed the `import torch` and `import torch.nn.functional as F` statements from the code. The execution environment pre-imports these libraries as `torch` and `F` respectively, so explicit imports are not allowed and were causing the failure. The core logic of the loss function, which uses EMA-smoothed statistics for adaptive margin and temperature, remains unchanged.", "pseudocode": "1. Calculate the difference in log-probabilities: delta_logp = logp(a) - logp(b).\n2. Calculate the difference in costs: delta_cost = cost(b) - cost(a).\n3. Compute the standard deviations of delta_logp and delta_cost for the current batch.\n4. Update the Exponential Moving Averages (EMAs) of logp_std and cost_std using the current batch's values and a decay factor `ema_decay`. These EMAs are stored in the `extra` dictionary to persist across steps.\n5. Calculate a hybrid adaptive scale using a weighted sum of the *EMA-smoothed* standard deviations: `adaptive_scale = w_logp * ema_logp_std + w_cost * ema_cost_std`. Clamp for stability.\n6. Compute an adaptive beta for the loss, inversely proportional to the adaptive scale: `beta = 1.0 / adaptive_scale`.\n7. Calculate two base margin components. `margin_A` uses a fixed temperature. `margin_B` uses an adaptive temperature based on the *EMA-smoothed* cost standard deviation.\n8. Combine the two margin components using a fixed blending factor `alpha`: `margin_base = alpha * margin_A + (1 - alpha) * margin_B`.\n9. Apply a stability clip to the margin: `margin = clamp(margin_base, -max_margin, max_margin)`.\n10. Compute the final loss argument: `loss_arg = delta_logp - margin`.\n11. Calculate the final loss using the scaled negative log-sigmoid function: `loss = -logsigmoid(beta * loss_arg)`.\n12. Return the mean of the loss across the batch.", "hyperparams": {"ema_decay": 0.99, "fixed_temp": 1.0, "alpha": 0.5, "w_logp": 0.5, "w_cost": 0.5, "min_scale": 0.1, "max_scale": 5.0, "max_margin": 5.0, "epsilon": 1e-08}, "operators_used": ["logsigmoid", "sigmoid", "clamp"], "implementation_hint": {"expects": ["A batch of paired samples (a, b) with their costs and log probabilities. `extra` dictionary for storing EMA state."], "returns": "A single scalar loss value."}, "code": "def generated_loss(batch, model_output, extra):\n    \"\"\"\n    A progressive, hybrid logistic loss using EMA-smoothed statistics for margin and temperature.\n    \"\"\"\n\n    # Read tensors from the batch\n    cost_a = batch['cost_a']\n    cost_b = batch['cost_b']\n    logp_a = batch['log_prob_w']\n    logp_b = batch['log_prob_l']\n    weights = batch.get('weight')\n\n    # Read hyperparameters\n    hp = extra['hyperparams']\n    ema_decay = hp.get('ema_decay', 0.99)\n    fixed_temp = hp.get('fixed_temp', 1.0)\n    alpha = hp.get('alpha', 0.5)\n    w_logp = hp.get('w_logp', 0.5)\n    w_cost = hp.get('w_cost', 0.5)\n    min_scale = hp.get('min_scale', 0.1)\n    max_scale = hp.get('max_scale', 5.0)\n    max_margin = hp.get('max_margin', 5.0)\n    epsilon = hp.get('epsilon', 1e-8)\n\n    # Initialize EMAs in the 'extra' dict if they don't exist\n    if 'ema_logp_std' not in extra:\n        extra['ema_logp_std'] = torch.tensor(1.0, device=logp_a.device)\n    if 'ema_cost_std' not in extra:\n        extra['ema_cost_std'] = torch.tensor(1.0, device=cost_a.device)\n\n    # 1. Calculate log-probability and cost differences\n    delta_logp = logp_a - logp_b\n    delta_cost = cost_b - cost_a\n\n    # 2. Compute current batch statistics (detached for stability)\n    if delta_logp.numel() > 1:\n        current_logp_std = delta_logp.detach().std()\n        current_cost_std = delta_cost.detach().std()\n    else:\n        current_logp_std = torch.tensor(1.0, device=delta_logp.device)\n        current_cost_std = torch.tensor(1.0, device=delta_cost.device)\n\n    # 3. New Coupling 1: Update EMAs of standard deviations\n    extra['ema_logp_std'] = ema_decay * extra['ema_logp_std'] + (1 - ema_decay) * current_logp_std\n    extra['ema_cost_std'] = ema_decay * extra['ema_cost_std'] + (1 - ema_decay) * current_cost_std\n    ema_logp_std = extra['ema_logp_std'].detach()\n    ema_cost_std = extra['ema_cost_std'].detach()\n\n    # 4. Inherit: Hybrid adaptive scale from EMA stats\n    hybrid_scale = w_logp * ema_logp_std + w_cost * ema_cost_std\n    adaptive_scale = torch.clamp(hybrid_scale, min_scale, max_scale)\n\n    # 5. Inherit: Adaptive beta from scale\n    beta = 1.0 / (adaptive_scale + epsilon)\n\n    # 6. Inherit: Margin components (using EMA cost std for adaptive part)\n    cost_weight_A = torch.sigmoid(delta_cost / fixed_temp)\n    margin_A = adaptive_scale * cost_weight_A\n    adaptive_cost_temp = ema_cost_std + epsilon\n    cost_weight_B = torch.sigmoid(delta_cost / adaptive_cost_temp)\n    margin_B = adaptive_scale * cost_weight_B\n\n    # 7. Inherit: Blended margin base\n    margin_base = alpha * margin_A + (1.0 - alpha) * margin_B\n\n    # 8. New Coupling 2: Clip the margin for stability\n    margin = torch.clamp(margin_base, -max_margin, max_margin)\n\n    # 9. Compute the loss argument\n    loss_arg = delta_logp - margin\n\n    # 10. Apply negative log-sigmoid loss\n    loss_per_pair = -F.logsigmoid(beta * loss_arg)\n\n    # 11. Apply optional weights and compute the mean loss\n    if weights is not None:\n        loss = (loss_per_pair * weights).sum() / weights.sum().clamp(min=epsilon)\n    else:\n        loss = loss_per_pair.mean()\n\n    return loss", "theoretical_basis": ""}, "fitness": {"hf_like_score": 19.56077458496094, "validation_objective": 19.56077458496094, "generalization_penalty": 0.0, "generalization_objectives": {"100": 19.533846505737305}, "train_score_mean": 24.654456453323363, "train_loss_mean": 1.7779040253162384, "pair_count": 31679985, "early_eval": {"enabled": true, "steps": 100, "baseline_validation_objective": 8.412976063537597, "candidate_validation_objective": 19.56077458496094, "early_stopped": true}, "phases": {"f1": {"steps": 15630, "train_score_mean": 24.654456453323363, "train_loss_mean": 1.7779040253162384, "pair_count": 31679985}, "f2": {"steps": 0, "train_score_mean": null, "train_loss_mean": null, "pair_count": 0}}, "config": {"hf": {"problem": "tsp", "hf_steps": 0, "hf_epochs": 10, "hf_instances_per_epoch": 100000, "train_problem_size": 100, "valid_problem_sizes": [100], "train_batch_size": 64, "pomo_size": 100, "learning_rate": 0.0003, "weight_decay": 1e-06, "alpha": 0.05, "device": "cuda:2", "seed": 1234, "num_validation_episodes": 10000, "validation_batch_size": 64, "generalization_penalty_weight": 1.0, "pool_version": "v0"}, "free_loss": {"f1_steps": 0, "total_train_steps": 15630, "f2_steps": 0, "f3_enabled": false}}, "loss_ir": {"name": "Progressive_Hybrid_LogSigmoid", "intuition": "Based on the compile error E_COMPILE_ERROR, I have removed the `import torch` and `import torch.nn.functional as F` statements from the code. The execution environment pre-imports these libraries as `torch` and `F` respectively, so explicit imports are not allowed and were causing the failure. The core logic of the loss function, which uses EMA-smoothed statistics for adaptive margin and temperature, remains unchanged.", "hyperparams": {"ema_decay": 0.99, "fixed_temp": 1.0, "alpha": 0.5, "w_logp": 0.5, "w_cost": 0.5, "min_scale": 0.1, "max_scale": 5.0, "max_margin": 5.0, "epsilon": 1e-08}, "operators_used": ["logsigmoid", "sigmoid", "clamp"]}}, "better_than_baseline": false}
{"generation": 7, "index": 0, "ir": {"name": "ZScore_Margin_Hybrid_LogSigmoid", "intuition": "Mode: combine. This loss function refines the successful adaptive margin concepts from both parents by introducing a more robust normalization scheme and a new way to balance margin components. It inherits the core `logsigmoid(beta * (delta_logp - margin))` structure and the idea of an adaptive `beta` inversely proportional to a scale factor, a common strength in both parents. It also inherits the blended margin concept from Parent 1 (`alpha * margin_A + (1-alpha) * margin_B`), where one part has a fixed temperature and the other is adaptive.\n\nNew Coupling 1: The key innovation is the use of z-score normalization for the cost difference (`delta_cost_z = (delta_cost - mean(delta_cost)) / std(delta_cost)`). This is a powerful stability trick that makes the margin calculation independent of the absolute scale and offset of costs in a batch, focusing only on their relative differences. Both margin components (`margin_A` and `margin_B`) now operate on this z-scored `delta_cost`, making them more stable and less prone to extreme values caused by cost outliers.\n\nNew Coupling 2: A 'meta-adaptive' `alpha` is introduced to blend the margins. Instead of being fixed (Parent 0) or based on raw cost stats (Parent 1), `alpha` is now a function of the relative variance of log-probabilities and costs: `alpha = sigmoid( (std(delta_logp) - std(delta_cost)) / temperature)`. This allows the loss to dynamically prioritize the margin component that corresponds to the less noisy signal in the current batch. If logp variance is high (model is uncertain), it down-weights the logp-scaled margin component. If cost variance is high (diverse costs), it down-weights the cost-scaled component, effectively self-balancing the sources of adaptivity.", "pseudocode": "1. Calculate the difference in log-probabilities: delta_logp = logp(a) - logp(b).\n2. Calculate the difference in costs: delta_cost = cost(b) - cost(a).\n3. Compute batch statistics: mean and standard deviation of delta_logp and delta_cost.\n4. New Coupling 1: Normalize the cost difference using z-scoring: `delta_cost_z = (delta_cost - mean(delta_cost)) / (std(delta_cost) + epsilon)`.\n5. Inherit from Parent 1: Calculate the hybrid adaptive scale based on a weighted sum of logp and cost standard deviations: `adaptive_scale = w_logp * std(delta_logp) + w_cost * std(delta_cost)`.\n6. Inherit from Parent 0/1: Compute an adaptive beta for the loss, inversely proportional to the adaptive scale: `beta = 1.0 / adaptive_scale`.\n7. Calculate the two margin components using the z-scored cost difference.\n   - `margin_A` uses a fixed temperature: `margin_A = adaptive_scale * sigmoid(delta_cost_z / fixed_temp)`.\n   - `margin_B` uses an adaptive temperature (based on std of the original delta_cost): `margin_B = adaptive_scale * sigmoid(delta_cost_z / adaptive_cost_temp)`.\n8. New Coupling 2: Calculate a meta-adaptive blending factor `alpha` based on the relative standard deviations of logp and cost: `alpha = sigmoid((std(delta_logp) - std(delta_cost)) / fixed_temp)`.\n9. Inherit from Parent 1: Combine the two margin components using the new dynamic alpha: `margin = alpha * margin_A + (1 - alpha) * margin_B`.\n10. Compute the final loss argument: `loss_arg = delta_logp - margin`.\n11. Calculate the final loss using the scaled negative log-sigmoid function: `loss = -logsigmoid(beta * loss_arg)`.\n12. Return the mean of the loss across the batch.", "hyperparams": {"fixed_temp": 1.0, "w_logp": 0.5, "w_cost": 0.5, "min_scale": 0.1, "max_scale": 5.0, "epsilon": 1e-08}, "operators_used": ["logsigmoid", "sigmoid", "clamp", "zscore"], "implementation_hint": {"expects": ["cost_a", "cost_b", "logp_a", "logp_b"], "returns": "scalar"}, "code": "def generated_loss(batch, model_output, extra):\n    \"\"\"\n    A logistic loss with a hybrid adaptive margin operating on z-scored costs.\n    The margin blend is meta-adaptive, balancing based on relative logp and cost variance.\n    \"\"\"\n    # Read tensors from the batch\n    cost_a = batch['cost_a']\n    cost_b = batch['cost_b']\n    logp_a = batch['log_prob_w']\n    logp_b = batch['log_prob_l']\n    weights = batch.get('weight')\n\n    # Read hyperparameters\n    hyperparams = extra['hyperparams']\n    fixed_temp = hyperparams.get('fixed_temp', 1.0)\n    w_logp = hyperparams.get('w_logp', 0.5)\n    w_cost = hyperparams.get('w_cost', 0.5)\n    min_scale = hyperparams.get('min_scale', 0.1)\n    max_scale = hyperparams.get('max_scale', 5.0)\n    epsilon = hyperparams.get('epsilon', 1e-8)\n\n    # 1. Calculate log-probability and cost differences\n    delta_logp = logp_a - logp_b\n    delta_cost = cost_b - cost_a\n\n    # 2. Compute batch statistics (detached for stability)\n    if delta_logp.numel() > 1:\n        logp_std = delta_logp.detach().std()\n        cost_std = delta_cost.detach().std()\n        cost_mean = delta_cost.detach().mean()\n    else:\n        logp_std = torch.tensor(1.0, device=delta_logp.device)\n        cost_std = torch.tensor(1.0, device=delta_cost.device)\n        cost_mean = torch.tensor(0.0, device=delta_cost.device)\n\n    # 3. New Coupling 1: Z-score normalization for cost difference\n    delta_cost_z = (delta_cost - cost_mean) / (cost_std + epsilon)\n\n    # 4. Inherit from Parent 1: Hybrid adaptive scale\n    hybrid_scale = w_logp * logp_std + w_cost * cost_std\n    adaptive_scale = torch.clamp(hybrid_scale, min_scale, max_scale)\n\n    # 5. Inherit from Parent 0/1: Adaptive beta from scale\n    beta = 1.0 / (adaptive_scale.detach() + epsilon)\n\n    # 6. Calculate margin components using z-scored costs\n    cost_weight_A = torch.sigmoid(delta_cost_z / fixed_temp)\n    margin_A = adaptive_scale * cost_weight_A\n    \n    adaptive_cost_temp = cost_std + epsilon # Temp is still based on original std\n    cost_weight_B = torch.sigmoid(delta_cost_z / adaptive_cost_temp)\n    margin_B = adaptive_scale * cost_weight_B\n\n    # 7. New Coupling 2: Meta-adaptive alpha for blending\n    alpha = torch.sigmoid((logp_std - cost_std) / fixed_temp) # Use fixed_temp for stability\n\n    # 8. Inherit from Parent 1: Blended margin with dynamic alpha\n    margin = alpha * margin_A + (1.0 - alpha) * margin_B\n\n    # 9. Compute the loss argument\n    loss_arg = delta_logp - margin\n\n    # 10. Apply negative log-sigmoid loss\n    loss_per_pair = -torch.nn.functional.logsigmoid(beta * loss_arg)\n\n    # 11. Apply optional weights and compute the mean loss\n    if weights is not None:\n        loss = (loss_per_pair * weights).sum() / weights.sum().clamp(min=epsilon)\n    else:\n        loss = loss_per_pair.mean()\n\n    return loss", "theoretical_basis": "A Bradley-Terry logistic preference model with a highly adaptive, blended margin. The margin is stabilized by operating on z-scored cost differences, making it robust to cost scale and shift. The blend between fixed-temperature and adaptive-temperature margin components is dynamically controlled by the relative uncertainty (variance) of the model's outputs versus the cost landscape, creating a meta-adaptive system."}, "fitness": {"hf_like_score": 7.877762509918213, "validation_objective": 7.876200506591797, "generalization_penalty": 0.001562003326416317, "generalization_objectives": {"100": 7.877762509918213}, "train_score_mean": 8.76292958378563, "train_loss_mean": 0.42043121089290064, "pair_count": 4951579514, "early_eval": {"enabled": true, "steps": 100, "baseline_validation_objective": 8.412976063537597, "candidate_validation_objective": 8.312365924072266, "early_stopped": false}, "phases": {"f1": {"steps": 15630, "train_score_mean": 8.76292958378563, "train_loss_mean": 0.42043121089290064, "pair_count": 4951579514}, "f2": {"steps": 0, "train_score_mean": null, "train_loss_mean": null, "pair_count": 0}}, "config": {"hf": {"problem": "tsp", "hf_steps": 0, "hf_epochs": 10, "hf_instances_per_epoch": 100000, "train_problem_size": 100, "valid_problem_sizes": [100], "train_batch_size": 64, "pomo_size": 100, "learning_rate": 0.0003, "weight_decay": 1e-06, "alpha": 0.05, "device": "cuda:0", "seed": 1234, "num_validation_episodes": 10000, "validation_batch_size": 64, "generalization_penalty_weight": 1.0, "pool_version": "v0"}, "free_loss": {"f1_steps": 0, "total_train_steps": 15630, "f2_steps": 0, "f3_enabled": false}}, "loss_ir": {"name": "ZScore_Margin_Hybrid_LogSigmoid", "intuition": "Mode: combine. This loss function refines the successful adaptive margin concepts from both parents by introducing a more robust normalization scheme and a new way to balance margin components. It inherits the core `logsigmoid(beta * (delta_logp - margin))` structure and the idea of an adaptive `beta` inversely proportional to a scale factor, a common strength in both parents. It also inherits the blended margin concept from Parent 1 (`alpha * margin_A + (1-alpha) * margin_B`), where one part has a fixed temperature and the other is adaptive.\n\nNew Coupling 1: The key innovation is the use of z-score normalization for the cost difference (`delta_cost_z = (delta_cost - mean(delta_cost)) / std(delta_cost)`). This is a powerful stability trick that makes the margin calculation independent of the absolute scale and offset of costs in a batch, focusing only on their relative differences. Both margin components (`margin_A` and `margin_B`) now operate on this z-scored `delta_cost`, making them more stable and less prone to extreme values caused by cost outliers.\n\nNew Coupling 2: A 'meta-adaptive' `alpha` is introduced to blend the margins. Instead of being fixed (Parent 0) or based on raw cost stats (Parent 1), `alpha` is now a function of the relative variance of log-probabilities and costs: `alpha = sigmoid( (std(delta_logp) - std(delta_cost)) / temperature)`. This allows the loss to dynamically prioritize the margin component that corresponds to the less noisy signal in the current batch. If logp variance is high (model is uncertain), it down-weights the logp-scaled margin component. If cost variance is high (diverse costs), it down-weights the cost-scaled component, effectively self-balancing the sources of adaptivity.", "hyperparams": {"fixed_temp": 1.0, "w_logp": 0.5, "w_cost": 0.5, "min_scale": 0.1, "max_scale": 5.0, "epsilon": 1e-08}, "operators_used": ["logsigmoid", "sigmoid", "clamp", "zscore"]}}, "better_than_baseline": true}
{"generation": 7, "index": 1, "ir": {"name": "Adaptive_LogSigmoid_with_Progressive_Margin", "intuition": "Mode: combine. This loss function evolves the adaptive margin concept from its parents by introducing a memory mechanism, aiming for smoother and more stable training. It inherits the core `logsigmoid(beta * (delta_logp - margin))` structure, which is a proven foundation from both parents. From Parent 1, it inherits the idea of a blended margin (`alpha * margin_A + (1-alpha) * margin_B`) where `margin_A` uses a fixed temperature and `margin_B` uses a batch-adaptive temperature. From Parent 0, it inherits the inverse coupling between the loss temperature `beta` and an `adaptive_scale` derived from `std(delta_logp)`, which helps stabilize learning by softening the loss when model uncertainty is high.\n\nThe first new coupling is the introduction of an **Exponential Moving Average (EMA) for the adaptive scale**. Instead of calculating `adaptive_scale` purely from the current batch's `std(delta_logp)`, we maintain a running EMA of this value. The margin's magnitude and the loss's temperature are then based on this smoothed, more stable `ema_adaptive_scale`. This prevents noisy batches from causing abrupt changes in the learning signal. The second new coupling is a **Z-score normalization of the cost difference** before it is passed into the sigmoid function for calculating the margin weights. We compute `z_delta_cost = (delta_cost - mean(delta_cost)) / (std(delta_cost) + epsilon)`. This makes the margin's sensitivity to cost differences relative to the distribution of costs within the batch, rather than being dependent on the absolute magnitude of `delta_cost`, which can vary wildly. This normalization improves stability and makes the `fixed_temp` hyperparameter more consistent across different datasets.", "pseudocode": "1. Calculate the difference in log-probabilities: delta_logp = logp(a) - logp(b).\n2. Calculate the difference in costs: delta_cost = cost(b) - cost(a).\n3. New Coupling 1: Calculate the current batch's logp standard deviation. Update an Exponential Moving Average (EMA) of this standard deviation to get a smoothed `ema_adaptive_scale`. Clip for stability.\n4. Inherit from Parent 0: Compute an adaptive beta for the loss, inversely proportional to the smoothed `ema_adaptive_scale`: `beta = 1.0 / ema_adaptive_scale`.\n5. New Coupling 2: Normalize the cost differences using a Z-score transformation: `z_delta_cost = (delta_cost - mean(delta_cost)) / (std(delta_cost) + epsilon)`.\n6. Inherit from Parent 1: Calculate the two base margin components using the normalized cost differences. `margin_A` uses `sigmoid(z_delta_cost / fixed_temp)`. `margin_B` uses `sigmoid(z_delta_cost / adaptive_temp)`, where `adaptive_temp` is based on the standard deviation of `z_delta_cost` (which is 1, so this simplifies).\n7. Inherit from Parent 1: Combine the two margin components using a fixed blending factor `alpha`: `margin = (alpha * margin_A + (1 - alpha) * margin_B) * ema_adaptive_scale`.\n8. Compute the final loss argument: `loss_arg = delta_logp - margin`.\n9. Calculate the final loss using the scaled negative log-sigmoid function: `loss = -logsigmoid(beta * loss_arg)`.\n10. Return the mean of the loss across the batch.", "hyperparams": {"fixed_temp": 1.0, "alpha": 0.5, "ema_decay": 0.99, "min_scale": 0.1, "max_scale": 5.0, "epsilon": 1e-08}, "operators_used": ["logsigmoid", "sigmoid", "clamp", "zscore"], "implementation_hint": {"expects": ["cost_a", "cost_b", "logp_a", "logp_b"], "returns": "scalar"}, "code": "def generated_loss(batch, model_output, extra):\n    \"\"\"\n    A logistic loss with a blended, Z-score-normalized margin and an EMA-smoothed adaptive scale.\n    \"\"\"\n    # Read tensors from the batch\n    cost_a = batch['cost_a']\n    cost_b = batch['cost_b']\n    logp_a = batch['log_prob_w']\n    logp_b = batch['log_prob_l']\n    weights = batch.get('weight')\n\n    # Read hyperparameters\n    hyperparams = extra['hyperparams']\n    fixed_temp = hyperparams.get('fixed_temp', 1.0)\n    alpha = hyperparams.get('alpha', 0.5)\n    ema_decay = hyperparams.get('ema_decay', 0.99)\n    min_scale = hyperparams.get('min_scale', 0.1)\n    max_scale = hyperparams.get('max_scale', 5.0)\n    epsilon = hyperparams.get('epsilon', 1e-8)\n\n    # 1. Calculate log-probability and cost differences\n    delta_logp = logp_a - logp_b\n    delta_cost = cost_b - cost_a\n\n    # 2. New Coupling 1: EMA-smoothed adaptive scale\n    if delta_logp.numel() > 1:\n        current_logp_std = delta_logp.detach().std()\n    else:\n        current_logp_std = torch.tensor(1.0, device=delta_logp.device)\n\n    # Initialize or update the EMA state\n    ema_state = extra.get('ema_state', {})\n    prev_ema_scale = ema_state.get('ema_adaptive_scale', current_logp_std)\n    ema_adaptive_scale = ema_decay * prev_ema_scale + (1.0 - ema_decay) * current_logp_std\n    ema_state['ema_adaptive_scale'] = ema_adaptive_scale.detach()\n    extra['ema_state'] = ema_state\n\n    adaptive_scale = torch.clamp(ema_adaptive_scale, min_scale, max_scale)\n\n    # 3. Inherit from Parent 0: Adaptive beta from smoothed scale\n    beta = 1.0 / (adaptive_scale.detach() + epsilon)\n\n    # 4. New Coupling 2: Z-score normalization of delta_cost\n    if delta_cost.numel() > 1:\n        cost_mean = delta_cost.detach().mean()\n        cost_std = delta_cost.detach().std()\n        z_delta_cost = (delta_cost - cost_mean) / (cost_std + epsilon)\n    else:\n        z_delta_cost = delta_cost\n\n    # 5. Inherit from Parent 1: Margin components with z-scored cost\n    # margin_A with fixed temperature\n    cost_weight_A = torch.sigmoid(z_delta_cost / fixed_temp)\n    \n    # margin_B with adaptive temperature (std of z-score is ~1)\n    adaptive_cost_temp = 1.0 # z_delta_cost.std() is approx 1\n    cost_weight_B = torch.sigmoid(z_delta_cost / adaptive_cost_temp)\n\n    # 6. Inherit from Parent 1: Blended margin\n    blended_cost_weight = alpha * cost_weight_A + (1.0 - alpha) * cost_weight_B\n    margin = adaptive_scale * blended_cost_weight\n\n    # 7. Compute the loss argument\n    loss_arg = delta_logp - margin\n\n    # 8. Apply negative log-sigmoid loss\n    loss_per_pair = -torch.nn.functional.logsigmoid(beta * loss_arg)\n\n    # 9. Apply optional weights and compute the mean loss\n    if weights is not None:\n        loss = (loss_per_pair * weights).sum() / weights.sum().clamp(min=epsilon)\n    else:\n        loss = loss_per_pair.mean()\n\n    return loss", "theoretical_basis": "A Bradley-Terry logistic preference model where the target log-odds is a blended, adaptive margin. The margin's scale and the loss's temperature are coupled to a time-smoothed (EMA) measure of model uncertainty, promoting stable learning dynamics. The margin's sensitivity to cost is based on the Z-scored cost difference, making it robust to variations in the absolute scale of costs and improving the consistency of the learning signal."}, "fitness": {"hf_like_score": 7.879341919708252, "validation_objective": 7.878829367065429, "generalization_penalty": 0.0005125526428226834, "generalization_objectives": {"100": 7.879341919708252}, "train_score_mean": 8.718656259550167, "train_loss_mean": 0.44460710414845594, "pair_count": 4951579226, "early_eval": {"enabled": true, "steps": 100, "baseline_validation_objective": 8.412976063537597, "candidate_validation_objective": 8.341582133483886, "early_stopped": false}, "phases": {"f1": {"steps": 15630, "train_score_mean": 8.718656259550167, "train_loss_mean": 0.44460710414845594, "pair_count": 4951579226}, "f2": {"steps": 0, "train_score_mean": null, "train_loss_mean": null, "pair_count": 0}}, "config": {"hf": {"problem": "tsp", "hf_steps": 0, "hf_epochs": 10, "hf_instances_per_epoch": 100000, "train_problem_size": 100, "valid_problem_sizes": [100], "train_batch_size": 64, "pomo_size": 100, "learning_rate": 0.0003, "weight_decay": 1e-06, "alpha": 0.05, "device": "cuda:1", "seed": 1234, "num_validation_episodes": 10000, "validation_batch_size": 64, "generalization_penalty_weight": 1.0, "pool_version": "v0"}, "free_loss": {"f1_steps": 0, "total_train_steps": 15630, "f2_steps": 0, "f3_enabled": false}}, "loss_ir": {"name": "Adaptive_LogSigmoid_with_Progressive_Margin", "intuition": "Mode: combine. This loss function evolves the adaptive margin concept from its parents by introducing a memory mechanism, aiming for smoother and more stable training. It inherits the core `logsigmoid(beta * (delta_logp - margin))` structure, which is a proven foundation from both parents. From Parent 1, it inherits the idea of a blended margin (`alpha * margin_A + (1-alpha) * margin_B`) where `margin_A` uses a fixed temperature and `margin_B` uses a batch-adaptive temperature. From Parent 0, it inherits the inverse coupling between the loss temperature `beta` and an `adaptive_scale` derived from `std(delta_logp)`, which helps stabilize learning by softening the loss when model uncertainty is high.\n\nThe first new coupling is the introduction of an **Exponential Moving Average (EMA) for the adaptive scale**. Instead of calculating `adaptive_scale` purely from the current batch's `std(delta_logp)`, we maintain a running EMA of this value. The margin's magnitude and the loss's temperature are then based on this smoothed, more stable `ema_adaptive_scale`. This prevents noisy batches from causing abrupt changes in the learning signal. The second new coupling is a **Z-score normalization of the cost difference** before it is passed into the sigmoid function for calculating the margin weights. We compute `z_delta_cost = (delta_cost - mean(delta_cost)) / (std(delta_cost) + epsilon)`. This makes the margin's sensitivity to cost differences relative to the distribution of costs within the batch, rather than being dependent on the absolute magnitude of `delta_cost`, which can vary wildly. This normalization improves stability and makes the `fixed_temp` hyperparameter more consistent across different datasets.", "hyperparams": {"fixed_temp": 1.0, "alpha": 0.5, "ema_decay": 0.99, "min_scale": 0.1, "max_scale": 5.0, "epsilon": 1e-08}, "operators_used": ["logsigmoid", "sigmoid", "clamp", "zscore"]}}, "better_than_baseline": true}
{"generation": 7, "index": 2, "ir": {"name": "Progressive_Hybrid_LogSigmoid", "intuition": "Mode: combine. This loss function evolves the adaptive mechanisms of its parents by introducing a progressive, stateful adaptation. It inherits the core `logsigmoid(beta * (delta_logp - margin))` structure and the concept of an adaptive `beta` inversely proportional to a scale factor, which are common successful patterns. It also inherits the blended margin idea from Parent 0, combining a fixed-temperature component with an adaptive one.\n\nThe first new coupling idea is to make the primary `adaptive_scale` stateful using an **Exponential Moving Average (EMA)**. Instead of calculating the scale from scratch for each batch using `std(delta_logp)` and `std(delta_cost)` (as in Parent 1), this child computes a `current_batch_scale` and updates a running EMA: `ema_scale = decay * ema_scale + (1 - decay) * current_batch_scale`. This smooths out the `adaptive_scale` and `beta` across batches, preventing volatile changes and promoting more stable, consistent training dynamics. The `beta` and `margin` are then based on this smoothed `ema_scale`.\n\nThe second new coupling is a **cost-gap-weighted margin**. Instead of a simple sigmoid weighting (`sigmoid(delta_cost / temp)`), the margin now incorporates the magnitude of the cost difference directly: `margin = ema_scale * (delta_cost / temp_cost)`. This makes the margin directly proportional to the cost gap, ensuring that larger differences in cost demand a proportionally larger `delta_logp` to satisfy the preference. This provides a stronger, more direct learning signal for pairs with significant cost disparities. A separate temperature hyperparameter (`temp_cost`) controls the sensitivity to this cost gap.", "pseudocode": "1. Initialize or retrieve the running EMA of the adaptive scale from the previous step.\n2. Calculate the difference in log-probabilities: delta_logp = logp(a) - logp(b).\n3. Calculate the difference in costs: delta_cost = cost(b) - cost(a).\n4. Inherit from Parent 1: Calculate a `current_batch_scale` as a weighted sum of the standard deviations of delta_logp and delta_cost: `current_batch_scale = w_logp * std(delta_logp) + w_cost * std(delta_cost)`.\n5. New Coupling 1: Update the running `ema_scale` using the current batch scale: `ema_scale = decay * ema_scale + (1 - decay) * current_batch_scale`. Clamp for stability.\n6. Inherit from Parents: Compute an adaptive beta for the loss, inversely proportional to the smoothed `ema_scale`: `beta = 1.0 / ema_scale`.\n7. New Coupling 2: Compute a cost-gap-weighted margin. The margin is directly proportional to the cost difference, scaled by the `ema_scale` and a temperature: `margin = ema_scale * (delta_cost / temp_cost)`.\n8. Compute the final loss argument: `loss_arg = delta_logp - margin`.\n9. Calculate the final loss using the scaled negative log-sigmoid function: `loss = -logsigmoid(beta * loss_arg)`.\n10. Return the mean of the loss and the updated `ema_scale` for the next iteration.", "hyperparams": {"ema_decay": 0.99, "temp_cost": 10.0, "w_logp": 0.5, "w_cost": 0.5, "min_scale": 0.1, "max_scale": 5.0, "epsilon": 1e-08}, "operators_used": ["logsigmoid", "clamp"], "implementation_hint": {"expects": ["cost_a", "cost_b", "logp_a", "logp_b"], "returns": "scalar"}, "code": "def generated_loss(batch, model_output, extra):\n    \"\"\"\n    A logistic loss with a stateful (EMA) adaptive scale and a cost-gap-weighted margin.\n    \"\"\"\n    # Read tensors from the batch\n    cost_a = batch['cost_a']\n    cost_b = batch['cost_b']\n    logp_a = batch['log_prob_w']\n    logp_b = batch['log_prob_l']\n    weights = batch.get('weight')\n\n    # Read hyperparameters\n    hyperparams = extra['hyperparams']\n    ema_decay = hyperparams.get('ema_decay', 0.99)\n    temp_cost = hyperparams.get('temp_cost', 10.0)\n    w_logp = hyperparams.get('w_logp', 0.5)\n    w_cost = hyperparams.get('w_cost', 0.5)\n    min_scale = hyperparams.get('min_scale', 0.1)\n    max_scale = hyperparams.get('max_scale', 5.0)\n    epsilon = hyperparams.get('epsilon', 1e-8)\n\n    # Get previous EMA scale or initialize it\n    # The runner must persist 'ema_scale' in the `extra` dict across calls.\n    prev_ema_scale = extra.get('ema_scale', torch.tensor(1.0, device=logp_a.device))\n\n    # 1. Calculate log-probability and cost differences\n    delta_logp = logp_a - logp_b\n    delta_cost = cost_b - cost_a\n\n    # 2. Compute batch statistics (detached for stability)\n    if delta_logp.numel() > 1:\n        logp_std = delta_logp.detach().std()\n        cost_std = delta_cost.detach().std()\n    else:\n        logp_std = torch.tensor(1.0, device=delta_logp.device)\n        cost_std = torch.tensor(1.0, device=delta_cost.device)\n\n    # 3. Inherit from Parent 1: Calculate current batch scale\n    current_batch_scale = w_logp * logp_std + w_cost * cost_std\n\n    # 4. New Coupling 1: Update EMA scale\n    ema_scale = ema_decay * prev_ema_scale + (1.0 - ema_decay) * current_batch_scale.detach()\n    ema_scale_clamped = torch.clamp(ema_scale, min_scale, max_scale)\n    extra['ema_scale'] = ema_scale # Persist for next batch\n\n    # 5. Inherit from Parents: Adaptive beta from smoothed scale\n    beta = 1.0 / (ema_scale_clamped.detach() + epsilon)\n\n    # 6. New Coupling 2: Cost-gap-weighted margin\n    margin = ema_scale_clamped * (delta_cost / (temp_cost + epsilon))\n\n    # 7. Compute the loss argument\n    loss_arg = delta_logp - margin\n\n    # 8. Apply negative log-sigmoid loss\n    loss_per_pair = -torch.nn.functional.logsigmoid(beta * loss_arg)\n\n    # 9. Apply optional weights and compute the mean loss\n    if weights is not None:\n        loss = (loss_per_pair * weights).sum() / weights.sum().clamp(min=epsilon)\n    else:\n        loss = loss_per_pair.mean()\n\n    return loss", "theoretical_basis": "A Bradley-Terry logistic preference model where the target log-odds (margin) is directly proportional to the cost difference. The overall scale of this margin and the steepness of the loss (beta) are coupled to a stateful, smoothed (EMA) measure of batch difficulty, which is derived from the variance of both model outputs and costs. This introduces temporal stability to the adaptive mechanisms."}, "fitness": {"hf_like_score": 22.034870169067382, "validation_objective": 22.034870169067382, "generalization_penalty": 0.0, "generalization_objectives": {"100": 22.02526346435547}, "train_score_mean": 25.164570722579956, "train_loss_mean": 1.508320415019989, "pair_count": 31679973, "early_eval": {"enabled": true, "steps": 100, "baseline_validation_objective": 8.412976063537597, "candidate_validation_objective": 22.034870169067382, "early_stopped": true}, "phases": {"f1": {"steps": 15630, "train_score_mean": 25.164570722579956, "train_loss_mean": 1.508320415019989, "pair_count": 31679973}, "f2": {"steps": 0, "train_score_mean": null, "train_loss_mean": null, "pair_count": 0}}, "config": {"hf": {"problem": "tsp", "hf_steps": 0, "hf_epochs": 10, "hf_instances_per_epoch": 100000, "train_problem_size": 100, "valid_problem_sizes": [100], "train_batch_size": 64, "pomo_size": 100, "learning_rate": 0.0003, "weight_decay": 1e-06, "alpha": 0.05, "device": "cuda:2", "seed": 1234, "num_validation_episodes": 10000, "validation_batch_size": 64, "generalization_penalty_weight": 1.0, "pool_version": "v0"}, "free_loss": {"f1_steps": 0, "total_train_steps": 15630, "f2_steps": 0, "f3_enabled": false}}, "loss_ir": {"name": "Progressive_Hybrid_LogSigmoid", "intuition": "Mode: combine. This loss function evolves the adaptive mechanisms of its parents by introducing a progressive, stateful adaptation. It inherits the core `logsigmoid(beta * (delta_logp - margin))` structure and the concept of an adaptive `beta` inversely proportional to a scale factor, which are common successful patterns. It also inherits the blended margin idea from Parent 0, combining a fixed-temperature component with an adaptive one.\n\nThe first new coupling idea is to make the primary `adaptive_scale` stateful using an **Exponential Moving Average (EMA)**. Instead of calculating the scale from scratch for each batch using `std(delta_logp)` and `std(delta_cost)` (as in Parent 1), this child computes a `current_batch_scale` and updates a running EMA: `ema_scale = decay * ema_scale + (1 - decay) * current_batch_scale`. This smooths out the `adaptive_scale` and `beta` across batches, preventing volatile changes and promoting more stable, consistent training dynamics. The `beta` and `margin` are then based on this smoothed `ema_scale`.\n\nThe second new coupling is a **cost-gap-weighted margin**. Instead of a simple sigmoid weighting (`sigmoid(delta_cost / temp)`), the margin now incorporates the magnitude of the cost difference directly: `margin = ema_scale * (delta_cost / temp_cost)`. This makes the margin directly proportional to the cost gap, ensuring that larger differences in cost demand a proportionally larger `delta_logp` to satisfy the preference. This provides a stronger, more direct learning signal for pairs with significant cost disparities. A separate temperature hyperparameter (`temp_cost`) controls the sensitivity to this cost gap.", "hyperparams": {"ema_decay": 0.99, "temp_cost": 10.0, "w_logp": 0.5, "w_cost": 0.5, "min_scale": 0.1, "max_scale": 5.0, "epsilon": 1e-08}, "operators_used": ["logsigmoid", "clamp"]}}, "better_than_baseline": false}
{"generation": 7, "index": 3, "ir": {"name": "Rank_Modulated_Hybrid_Margin_Loss", "intuition": "Mode: combine. This loss function enhances the successful hybrid adaptive margin framework of the parents by introducing a rank-based modulation, making the loss more sensitive to the relative importance of preference pairs. \n\nIt inherits the core `logsigmoid(beta * (delta_logp - margin))` structure. From both parents, it inherits the concept of an `adaptive_scale` and an `adaptive_beta` (inverse of the scale), which together regulate the magnitude of the margin and the steepness of the loss based on batch statistics. From Parent 1, it specifically inherits the idea of a blended margin, `alpha * margin_A + (1-alpha) * margin_B`, which combines a fixed-temperature response with a batch-adaptive one. From Parent 0, it inherits the coupling of `adaptive_scale` to `std(delta_logp)`, providing a measure of model uncertainty.\n\nThe first new coupling idea is to **modulate the adaptive scale with rank information**. Instead of just using `std(delta_logp)` or a hybrid, the scale is now multiplied by a term derived from the rank-normalized `delta_cost`: `adaptive_scale = base_scale * (1 + rank_weight * rank_norm(delta_cost))`. This makes the margin for pairs with larger cost differences (i.e., more important preferences) proportionally larger, while also increasing the loss's softness (via beta) for these pairs, focusing the model's capacity on getting high-stakes decisions right without becoming overconfident. The `rank_norm` function, which maps values to `[-0.5, 0.5]`, ensures this modulation is stable and bounded.\n\nThe second new coupling is a **dynamic alpha based on the correlation between log-probabilities and costs**. The blending factor `alpha` is now `sigmoid(correlation(delta_logp, delta_cost))`. When the model's probability estimates align well with cost differences (high positive correlation), `alpha` increases, favoring the more stable, fixed-temperature margin (`margin_A`). When they are misaligned (low or negative correlation), `alpha` decreases, favoring the more flexible, batch-adaptive margin (`margin_B`). This allows the loss to dynamically adjust its margin strategy based on how well the model currently understands the preference landscape.", "pseudocode": "1. Calculate the difference in log-probabilities: delta_logp = logp(a) - logp(b).\n2. Calculate the difference in costs: delta_cost = cost(b) - cost(a).\n3. Inherit from Parent 0: Compute a base adaptive scale from the standard deviation of delta_logp: `base_scale = std(delta_logp)`.\n4. New Coupling 1: Modulate the base scale with rank information. First, compute rank-normalized delta_cost, which maps costs to a stable [-0.5, 0.5] range. Then, compute the final adaptive scale: `adaptive_scale = base_scale * (1 + rank_weight * rank_norm(delta_cost))`. Clip for stability.\n5. Inherit from both: Compute the adaptive beta for the loss as the inverse of the final adaptive scale: `beta = 1.0 / adaptive_scale`.\n6. Inherit from Parent 1: Calculate the two base margin components. `margin_A` uses a fixed temperature. `margin_B` uses an adaptive temperature based on `std(delta_cost)`.\n7. New Coupling 2: Compute a dynamic blending factor `alpha` based on the correlation between `delta_logp` and `delta_cost`. `alpha = sigmoid(correlation(delta_logp, delta_cost))`.\n8. Inherit from Parent 1: Combine the two margin components using the dynamic alpha: `margin = alpha * margin_A + (1 - alpha) * margin_B`. The magnitude of both margin components is determined by the `adaptive_scale`.\n9. Compute the final loss argument: `loss_arg = delta_logp - margin`.\n10. Calculate the final loss using the scaled negative log-sigmoid function: `loss = -logsigmoid(beta * loss_arg)`.\n11. Return the mean of the loss across the batch.", "hyperparams": {"fixed_temp": 1.0, "rank_weight": 0.5, "min_scale": 0.1, "max_scale": 5.0, "epsilon": 1e-08}, "operators_used": ["logsigmoid", "sigmoid", "clamp"], "implementation_hint": {"expects": ["cost_a", "cost_b", "logp_a", "logp_b"], "returns": "scalar"}, "code": "def generated_loss(batch, model_output, extra):\n    \"\"\"\n    A logistic loss with a hybrid margin whose scale is modulated by cost rank\n    and whose blend is determined by logp-cost correlation.\n    \"\"\"\n    # Read tensors from the batch\n    cost_a = batch['cost_a']\n    cost_b = batch['cost_b']\n    logp_a = batch['log_prob_w']\n    logp_b = batch['log_prob_l']\n    weights = batch.get('weight')\n\n    # Read hyperparameters\n    hyperparams = extra['hyperparams']\n    fixed_temp = hyperparams.get('fixed_temp', 1.0)\n    rank_weight = hyperparams.get('rank_weight', 0.5)\n    min_scale = hyperparams.get('min_scale', 0.1)\n    max_scale = hyperparams.get('max_scale', 5.0)\n    epsilon = hyperparams.get('epsilon', 1e-8)\n\n    delta_logp = logp_a - logp_b\n    delta_cost = cost_b - cost_a\n\n    # Handle batch size of 1 for stability\n    if delta_logp.numel() <= 1:\n        base_scale = torch.tensor(1.0, device=delta_logp.device)\n        cost_std = torch.tensor(1.0, device=delta_cost.device)\n        correlation = torch.tensor(0.0, device=delta_logp.device)\n        ranks = torch.zeros_like(delta_cost)\n    else:\n        # Inherit from Parent 0: Base scale from logp std dev\n        base_scale = delta_logp.detach().std()\n        cost_std = delta_cost.detach().std()\n\n        # New Coupling 1 (part 1): Rank-normalize delta_cost\n        ranks = delta_cost.argsort().float()\n        ranks = (ranks / (ranks.numel() - 1)) - 0.5 # Maps to [-0.5, 0.5]\n\n        # New Coupling 2 (part 1): Calculate correlation\n        vx = delta_logp.detach() - delta_logp.detach().mean()\n        vy = delta_cost.detach() - delta_cost.detach().mean()\n        correlation = torch.sum(vx * vy) / (torch.sqrt(torch.sum(vx ** 2)) * torch.sqrt(torch.sum(vy ** 2)) + epsilon)\n\n    # New Coupling 1 (part 2): Modulate scale with rank\n    rank_modulator = 1.0 + rank_weight * ranks\n    adaptive_scale = torch.clamp(base_scale * rank_modulator, min_scale, max_scale)\n\n    # Inherit from both: Adaptive beta from scale\n    beta = 1.0 / (adaptive_scale.detach() + epsilon)\n\n    # Inherit from Parent 1: Margin components\n    cost_weight_A = torch.sigmoid(delta_cost / fixed_temp)\n    margin_A = adaptive_scale * cost_weight_A\n    \n    adaptive_cost_temp = cost_std + epsilon\n    cost_weight_B = torch.sigmoid(delta_cost / adaptive_cost_temp)\n    margin_B = adaptive_scale * cost_weight_B\n\n    # New Coupling 2 (part 2): Dynamic alpha from correlation\n    alpha = torch.sigmoid(correlation)\n\n    # Inherit from Parent 1: Blended margin with dynamic alpha\n    margin = alpha * margin_A + (1.0 - alpha) * margin_B\n\n    loss_arg = delta_logp - margin\n    loss_per_pair = -torch.nn.functional.logsigmoid(beta * loss_arg)\n\n    if weights is not None:\n        loss = (loss_per_pair * weights).sum() / weights.sum().clamp(min=epsilon)\n    else:\n        loss = loss_per_pair.mean()\n\n    return loss", "theoretical_basis": "A Bradley-Terry logistic preference model where the target log-odds is a dynamically blended margin, and the loss temperature is adaptive. The margin's magnitude is modulated by the rank-order of cost differences, focusing learning on more significant preferences. The margin's composition (the blend) adapts based on the correlation between model outputs and costs, effectively choosing a margin strategy based on the model's current alignment with the preference data."}, "fitness": {"hf_like_score": 7.878897737884522, "validation_objective": 7.877070379638672, "generalization_penalty": 0.001827358245849453, "generalization_objectives": {"100": 7.878897737884522}, "train_score_mean": 8.584742243505028, "train_loss_mean": 0.5378355892266666, "pair_count": 4951578851, "early_eval": {"enabled": true, "steps": 100, "baseline_validation_objective": 8.412976063537597, "candidate_validation_objective": 8.32727133178711, "early_stopped": false}, "phases": {"f1": {"steps": 15630, "train_score_mean": 8.584742243505028, "train_loss_mean": 0.5378355892266666, "pair_count": 4951578851}, "f2": {"steps": 0, "train_score_mean": null, "train_loss_mean": null, "pair_count": 0}}, "config": {"hf": {"problem": "tsp", "hf_steps": 0, "hf_epochs": 10, "hf_instances_per_epoch": 100000, "train_problem_size": 100, "valid_problem_sizes": [100], "train_batch_size": 64, "pomo_size": 100, "learning_rate": 0.0003, "weight_decay": 1e-06, "alpha": 0.05, "device": "cuda:3", "seed": 1234, "num_validation_episodes": 10000, "validation_batch_size": 64, "generalization_penalty_weight": 1.0, "pool_version": "v0"}, "free_loss": {"f1_steps": 0, "total_train_steps": 15630, "f2_steps": 0, "f3_enabled": false}}, "loss_ir": {"name": "Rank_Modulated_Hybrid_Margin_Loss", "intuition": "Mode: combine. This loss function enhances the successful hybrid adaptive margin framework of the parents by introducing a rank-based modulation, making the loss more sensitive to the relative importance of preference pairs. \n\nIt inherits the core `logsigmoid(beta * (delta_logp - margin))` structure. From both parents, it inherits the concept of an `adaptive_scale` and an `adaptive_beta` (inverse of the scale), which together regulate the magnitude of the margin and the steepness of the loss based on batch statistics. From Parent 1, it specifically inherits the idea of a blended margin, `alpha * margin_A + (1-alpha) * margin_B`, which combines a fixed-temperature response with a batch-adaptive one. From Parent 0, it inherits the coupling of `adaptive_scale` to `std(delta_logp)`, providing a measure of model uncertainty.\n\nThe first new coupling idea is to **modulate the adaptive scale with rank information**. Instead of just using `std(delta_logp)` or a hybrid, the scale is now multiplied by a term derived from the rank-normalized `delta_cost`: `adaptive_scale = base_scale * (1 + rank_weight * rank_norm(delta_cost))`. This makes the margin for pairs with larger cost differences (i.e., more important preferences) proportionally larger, while also increasing the loss's softness (via beta) for these pairs, focusing the model's capacity on getting high-stakes decisions right without becoming overconfident. The `rank_norm` function, which maps values to `[-0.5, 0.5]`, ensures this modulation is stable and bounded.\n\nThe second new coupling is a **dynamic alpha based on the correlation between log-probabilities and costs**. The blending factor `alpha` is now `sigmoid(correlation(delta_logp, delta_cost))`. When the model's probability estimates align well with cost differences (high positive correlation), `alpha` increases, favoring the more stable, fixed-temperature margin (`margin_A`). When they are misaligned (low or negative correlation), `alpha` decreases, favoring the more flexible, batch-adaptive margin (`margin_B`). This allows the loss to dynamically adjust its margin strategy based on how well the model currently understands the preference landscape.", "hyperparams": {"fixed_temp": 1.0, "rank_weight": 0.5, "min_scale": 0.1, "max_scale": 5.0, "epsilon": 1e-08}, "operators_used": ["logsigmoid", "sigmoid", "clamp"]}}, "better_than_baseline": true}
{"generation": 7, "index": 4, "ir": {"name": "Progressive_Hybrid_Margin_LogSigmoid", "intuition": "Mode: combine. This loss function synthesizes the most effective adaptive components from both parents and introduces a new stabilization mechanism based on exponential moving averages (EMA) of batch statistics. It inherits the core `logsigmoid(beta * (delta_logp - margin))` structure. From Parent 1, it inherits the blended margin concept (`alpha * margin_A + (1-alpha) * margin_B`) which uses both fixed and adaptive cost temperatures. From Parent 0, it inherits the inverse coupling between the loss temperature `beta` and an `adaptive_scale` that measures batch difficulty (`beta = 1 / adaptive_scale`).\n\nThe first new coupling idea is to stabilize the adaptive components using **Exponential Moving Averages (EMA)**. Instead of using the raw standard deviations of `delta_logp` and `delta_cost` from the current batch, which can be noisy, we compute EMA-smoothed versions of these statistics. This makes the `adaptive_scale` and the adaptive cost temperature less susceptible to outlier batches, promoting more stable and progressive learning. The EMA state is stored in the `extra` dictionary to persist across batches. The second new coupling is a **simplified dynamic alpha**. Instead of the complex calculation in Parent 1, this version's `alpha` is a simple `sigmoid` of the EMA-smoothed `cost_std`. When cost variance is low, `alpha` is near 0.5, blending the margins equally. As cost variance increases, `alpha` approaches 1, favoring the more stable fixed-temperature margin (`margin_A`) to prevent erratic gradients from large, unusual cost differences.", "pseudocode": "1. Calculate the difference in log-probabilities: delta_logp = logp(a) - logp(b).\n2. Calculate the difference in costs: delta_cost = cost(b) - cost(a).\n3. New Coupling 1: Update Exponential Moving Averages (EMAs) of the standard deviations of delta_logp and delta_cost. Retrieve the current EMA values for `logp_std_ema` and `cost_std_ema`.\n4. Inherit from Parent 1: Compute a hybrid adaptive scale using a weighted sum of the EMA-smoothed statistics: `adaptive_scale = w_logp * logp_std_ema + w_cost * cost_std_ema`, clipped for stability.\n5. Inherit from Parent 0: Compute an adaptive beta for the loss, inversely proportional to the hybrid adaptive scale: `beta = 1.0 / adaptive_scale`.\n6. Inherit from Parent 1: Calculate the two base margin components. `margin_A` uses a fixed temperature. `margin_B` uses an adaptive temperature based on the EMA-smoothed `cost_std_ema`.\n7. New Coupling 2: Compute a simplified dynamic blending factor `alpha` as `sigmoid(cost_std_ema)`. This shifts the blend towards the fixed-temperature margin as cost variance increases.\n8. Inherit from Parent 1: Combine the two margin components using the dynamic alpha: `margin = alpha * margin_A + (1 - alpha) * margin_B`.\n9. Compute the final loss argument: `loss_arg = delta_logp - margin`.\n10. Calculate the final loss using the scaled negative log-sigmoid function: `loss = -logsigmoid(beta * loss_arg)`.\n11. Return the mean of the loss across the batch.", "hyperparams": {"ema_decay": 0.99, "fixed_temp": 1.0, "w_logp": 0.5, "w_cost": 0.5, "min_scale": 0.1, "max_scale": 5.0, "epsilon": 1e-08}, "operators_used": ["logsigmoid", "sigmoid", "clamp"], "implementation_hint": {"expects": ["cost_a", "cost_b", "logp_a", "logp_b"], "returns": "scalar"}, "code": "def generated_loss(batch, model_output, extra):\n    \"\"\"\n    A logistic loss with a hybrid adaptive margin and temperature, stabilized with EMAs.\n    \"\"\"\n    # Read tensors from the batch\n    cost_a = batch['cost_a']\n    cost_b = batch['cost_b']\n    logp_a = batch['log_prob_w']\n    logp_b = batch['log_prob_l']\n    weights = batch.get('weight')\n\n    # Read hyperparameters\n    hyperparams = extra['hyperparams']\n    ema_decay = hyperparams.get('ema_decay', 0.99)\n    fixed_temp = hyperparams.get('fixed_temp', 1.0)\n    w_logp = hyperparams.get('w_logp', 0.5)\n    w_cost = hyperparams.get('w_cost', 0.5)\n    min_scale = hyperparams.get('min_scale', 0.1)\n    max_scale = hyperparams.get('max_scale', 5.0)\n    epsilon = hyperparams.get('epsilon', 1e-8)\n\n    # Initialize EMA state if not present\n    if 'ema_stats' not in extra:\n        extra['ema_stats'] = {'logp_std': torch.tensor(1.0, device=logp_a.device), 'cost_std': torch.tensor(1.0, device=cost_a.device)}\n\n    # 1. Calculate log-probability and cost differences\n    delta_logp = logp_a - logp_b\n    delta_cost = cost_b - cost_a\n\n    # 2. New Coupling 1: Update and use EMA-smoothed statistics\n    if delta_logp.numel() > 1:\n        current_logp_std = delta_logp.detach().std()\n        current_cost_std = delta_cost.detach().std()\n        # Update EMAs\n        extra['ema_stats']['logp_std'] = ema_decay * extra['ema_stats']['logp_std'] + (1 - ema_decay) * current_logp_std\n        extra['ema_stats']['cost_std'] = ema_decay * extra['ema_stats']['cost_std'] + (1 - ema_decay) * current_cost_std\n    \n    logp_std_ema = extra['ema_stats']['logp_std']\n    cost_std_ema = extra['ema_stats']['cost_std']\n\n    # 3. Inherit from Parent 1: Hybrid adaptive scale using EMA stats\n    hybrid_scale = w_logp * logp_std_ema + w_cost * cost_std_ema\n    adaptive_scale = torch.clamp(hybrid_scale, min_scale, max_scale)\n\n    # 4. Inherit from Parent 0: Adaptive beta from scale\n    beta = 1.0 / (adaptive_scale.detach() + epsilon)\n\n    # 5. Inherit from Parent 1: Margin components using EMA stats\n    cost_weight_A = torch.sigmoid(delta_cost / fixed_temp)\n    margin_A = adaptive_scale * cost_weight_A\n    \n    adaptive_cost_temp = cost_std_ema + epsilon\n    cost_weight_B = torch.sigmoid(delta_cost / adaptive_cost_temp)\n    margin_B = adaptive_scale * cost_weight_B\n\n    # 6. New Coupling 2: Simplified dynamic alpha from EMA cost std\n    alpha = torch.sigmoid(cost_std_ema) # Simple, stable adaptation\n\n    # 7. Inherit from Parent 1: Blended margin with dynamic alpha\n    margin = alpha * margin_A + (1.0 - alpha) * margin_B\n\n    # 8. Compute the loss argument\n    loss_arg = delta_logp - margin\n\n    # 9. Apply negative log-sigmoid loss\n    loss_per_pair = -torch.nn.functional.logsigmoid(beta * loss_arg)\n\n    # 10. Apply optional weights and compute the mean loss\n    if weights is not None:\n        loss = (loss_per_pair * weights).sum() / weights.sum().clamp(min=epsilon)\n    else:\n        loss = loss_per_pair.mean()\n\n    return loss", "theoretical_basis": "A Bradley-Terry logistic preference model where the target log-odds is a dynamically blended margin, and the loss temperature is adaptive. The system's adaptivity is stabilized by using Exponential Moving Averages of batch statistics (logp and cost variance), preventing noisy updates and promoting smoother convergence. The margin's blend ratio also adapts to cost variance, favoring stability when costs are diverse."}, "fitness": {"hf_like_score": 19.542795407104492, "validation_objective": 19.542795407104492, "generalization_penalty": 0.0, "generalization_objectives": {"100": 19.51875413208008}, "train_score_mean": 24.665953311920166, "train_loss_mean": 1.7804569840431212, "pair_count": 31679979, "early_eval": {"enabled": true, "steps": 100, "baseline_validation_objective": 8.412976063537597, "candidate_validation_objective": 19.542795407104492, "early_stopped": true}, "phases": {"f1": {"steps": 15630, "train_score_mean": 24.665953311920166, "train_loss_mean": 1.7804569840431212, "pair_count": 31679979}, "f2": {"steps": 0, "train_score_mean": null, "train_loss_mean": null, "pair_count": 0}}, "config": {"hf": {"problem": "tsp", "hf_steps": 0, "hf_epochs": 10, "hf_instances_per_epoch": 100000, "train_problem_size": 100, "valid_problem_sizes": [100], "train_batch_size": 64, "pomo_size": 100, "learning_rate": 0.0003, "weight_decay": 1e-06, "alpha": 0.05, "device": "cuda:0", "seed": 1234, "num_validation_episodes": 10000, "validation_batch_size": 64, "generalization_penalty_weight": 1.0, "pool_version": "v0"}, "free_loss": {"f1_steps": 0, "total_train_steps": 15630, "f2_steps": 0, "f3_enabled": false}}, "loss_ir": {"name": "Progressive_Hybrid_Margin_LogSigmoid", "intuition": "Mode: combine. This loss function synthesizes the most effective adaptive components from both parents and introduces a new stabilization mechanism based on exponential moving averages (EMA) of batch statistics. It inherits the core `logsigmoid(beta * (delta_logp - margin))` structure. From Parent 1, it inherits the blended margin concept (`alpha * margin_A + (1-alpha) * margin_B`) which uses both fixed and adaptive cost temperatures. From Parent 0, it inherits the inverse coupling between the loss temperature `beta` and an `adaptive_scale` that measures batch difficulty (`beta = 1 / adaptive_scale`).\n\nThe first new coupling idea is to stabilize the adaptive components using **Exponential Moving Averages (EMA)**. Instead of using the raw standard deviations of `delta_logp` and `delta_cost` from the current batch, which can be noisy, we compute EMA-smoothed versions of these statistics. This makes the `adaptive_scale` and the adaptive cost temperature less susceptible to outlier batches, promoting more stable and progressive learning. The EMA state is stored in the `extra` dictionary to persist across batches. The second new coupling is a **simplified dynamic alpha**. Instead of the complex calculation in Parent 1, this version's `alpha` is a simple `sigmoid` of the EMA-smoothed `cost_std`. When cost variance is low, `alpha` is near 0.5, blending the margins equally. As cost variance increases, `alpha` approaches 1, favoring the more stable fixed-temperature margin (`margin_A`) to prevent erratic gradients from large, unusual cost differences.", "hyperparams": {"ema_decay": 0.99, "fixed_temp": 1.0, "w_logp": 0.5, "w_cost": 0.5, "min_scale": 0.1, "max_scale": 5.0, "epsilon": 1e-08}, "operators_used": ["logsigmoid", "sigmoid", "clamp"]}}, "better_than_baseline": false}
{"generation": 7, "index": 5, "ir": {"name": "Normalized_Hybrid_Margin_LogSigmoid", "intuition": "Mode: combine. This loss function refines the successful hybrid adaptive margin concept from the parents by introducing normalization for enhanced stability and predictability. It inherits the core `logsigmoid(beta * (delta_logp - margin))` structure. From Parent 1, it inherits the idea of a blended margin using a fixed-temperature component (`margin_A`) and an adaptive-temperature component (`margin_B`). From Parent 0, it inherits the coupling where the loss temperature `beta` is inversely proportional to an adaptive scale (`beta = 1 / adaptive_scale`).\n\nThe first new coupling idea is the **z-score normalization of `delta_cost` before it is used in the margin calculation**. That is, `normalized_delta_cost = (delta_cost - mean(delta_cost)) / std(delta_cost)`. The margin components (`margin_A`, `margin_B`) now use this normalized cost difference in their sigmoid functions. This makes the margin's response to cost differences independent of the absolute scale and shift of costs in a batch, focusing only on the relative ranking of cost differences. This should improve robustness across different reward models and problem scales. The second new coupling is a **dynamic margin scale based on the correlation between log-probabilities and costs**. The `adaptive_scale` is now multiplied by `1 + rho`, where `rho = correlation(delta_logp, delta_cost)`. This 'agreement factor' increases the margin (and thus the learning signal) when the model's log-probabilities are already aligned with the cost differences (high correlation), encouraging it to push further. Conversely, it reduces the margin when the model disagrees with the costs, preventing overly aggressive updates on confusing pairs.", "pseudocode": "1. Calculate the difference in log-probabilities: delta_logp = logp(a) - logp(b).\n2. Calculate the difference in costs: delta_cost = cost(b) - cost(a).\n3. Compute batch statistics for delta_logp and delta_cost (mean, std dev).\n4. New Coupling 1: Normalize the cost differences using z-scoring: `normalized_delta_cost = (delta_cost - mean(delta_cost)) / (std(delta_cost) + epsilon)`.\n5. Inherit from Parent 0: Compute a base adaptive scale from the standard deviation of delta_logp: `base_scale = std(delta_logp)`.\n6. New Coupling 2: Calculate a correlation-based 'agreement factor': `rho = correlation(delta_logp, delta_cost)`. The final adaptive scale is `adaptive_scale = clamp(base_scale * (1 + rho), min_scale, max_scale)`.\n7. Inherit from Parent 0: Compute an adaptive beta for the loss, inversely proportional to the final adaptive scale: `beta = 1.0 / adaptive_scale`.\n8. Inherit from Parent 1: Calculate the two base margin components using the `normalized_delta_cost`. `margin_A` uses a fixed temperature. `margin_B` uses an adaptive temperature (which can be set to 1.0 since its input is already normalized).\n9. Inherit from Parent 1: Combine the two margin components using a fixed blending factor `alpha`: `margin = alpha * margin_A + (1 - alpha) * margin_B`.\n10. Compute the final loss argument: `loss_arg = delta_logp - margin`.\n11. Calculate the final loss using the scaled negative log-sigmoid function: `loss = -logsigmoid(beta * loss_arg)`.\n12. Return the mean of the loss across the batch.", "hyperparams": {"fixed_temp": 1.0, "alpha": 0.5, "min_scale": 0.1, "max_scale": 5.0, "epsilon": 1e-08}, "operators_used": ["logsigmoid", "sigmoid", "clamp"], "implementation_hint": {"expects": ["cost_a", "cost_b", "logp_a", "logp_b"], "returns": "scalar"}, "code": "def generated_loss(batch, model_output, extra):\n    \"\"\"\n    A logistic loss with a hybrid margin operating on normalized cost differences,\n    and a scale modulated by logp-cost correlation.\n    \"\"\"\n    # Read tensors from the batch\n    cost_a = batch['cost_a']\n    cost_b = batch['cost_b']\n    logp_a = batch['log_prob_w']\n    logp_b = batch['log_prob_l']\n    weights = batch.get('weight')\n\n    # Read hyperparameters\n    hyperparams = extra['hyperparams']\n    fixed_temp = hyperparams.get('fixed_temp', 1.0)\n    alpha = hyperparams.get('alpha', 0.5)\n    min_scale = hyperparams.get('min_scale', 0.1)\n    max_scale = hyperparams.get('max_scale', 5.0)\n    epsilon = hyperparams.get('epsilon', 1e-8)\n\n    # 1. Calculate log-probability and cost differences\n    delta_logp = logp_a - logp_b\n    delta_cost = cost_b - cost_a\n\n    # 2. Compute batch statistics (detached for stability)\n    if delta_logp.numel() > 1:\n        logp_mean = delta_logp.detach().mean()\n        logp_std = delta_logp.detach().std()\n        cost_mean = delta_cost.detach().mean()\n        cost_std = delta_cost.detach().std()\n    else:\n        logp_mean = torch.tensor(0.0, device=delta_logp.device)\n        logp_std = torch.tensor(1.0, device=delta_logp.device)\n        cost_mean = torch.tensor(0.0, device=delta_cost.device)\n        cost_std = torch.tensor(1.0, device=delta_cost.device)\n\n    # 3. New Coupling 1: Z-score normalization of delta_cost\n    normalized_delta_cost = (delta_cost - cost_mean) / (cost_std + epsilon)\n\n    # 4. Inherit from Parent 0: Base adaptive scale from logp std dev\n    base_scale = logp_std\n\n    # 5. New Coupling 2: Correlation-based agreement factor\n    if delta_logp.numel() > 1:\n        vx = delta_logp.detach() - logp_mean\n        vy = delta_cost.detach() - cost_mean\n        rho = torch.sum(vx * vy) / (torch.sqrt(torch.sum(vx ** 2)) * torch.sqrt(torch.sum(vy ** 2)) + epsilon)\n    else:\n        rho = torch.tensor(0.0, device=delta_logp.device)\n    \n    agreement_factor = 1.0 + rho\n    adaptive_scale = torch.clamp(base_scale * agreement_factor, min_scale, max_scale)\n\n    # 6. Inherit from Parent 0: Adaptive beta from scale\n    beta = 1.0 / (adaptive_scale.detach() + epsilon)\n\n    # 7. Inherit from Parent 1: Margin components on normalized cost\n    # margin_A with fixed temperature\n    cost_weight_A = torch.sigmoid(normalized_delta_cost / fixed_temp)\n    margin_A = adaptive_scale * cost_weight_A\n    \n    # margin_B with adaptive temperature (can be 1.0 due to normalization)\n    adaptive_cost_temp = 1.0 # std of normalized_delta_cost is 1\n    cost_weight_B = torch.sigmoid(normalized_delta_cost / adaptive_cost_temp)\n    margin_B = adaptive_scale * cost_weight_B\n\n    # 8. Inherit from Parent 1: Blended margin with fixed alpha\n    margin = alpha * margin_A + (1.0 - alpha) * margin_B\n\n    # 9. Compute the loss argument\n    loss_arg = delta_logp - margin\n\n    # 10. Apply negative log-sigmoid loss\n    loss_per_pair = -torch.nn.functional.logsigmoid(beta * loss_arg)\n\n    # 11. Apply optional weights and compute the mean loss\n    if weights is not None:\n        loss = (loss_per_pair * weights).sum() / weights.sum().clamp(min=epsilon)\n    else:\n        loss = loss_per_pair.mean()\n\n    return loss", "theoretical_basis": "A Bradley-Terry logistic preference model with a dynamically scaled, blended margin. The margin's sensitivity to cost differences is stabilized by operating on z-scored cost gaps. The overall magnitude of the margin and the steepness of the loss are modulated by the correlation between model log-probabilities and cost differences, creating an 'agreement-aware' learning rate that rewards correct model trends and dampens updates on conflicting signals."}, "fitness": {"hf_like_score": 7.8777853240966795, "validation_objective": 7.875363345336914, "generalization_penalty": 0.002421978759765686, "generalization_objectives": {"100": 7.8777853240966795}, "train_score_mean": 8.701691520908133, "train_loss_mean": 0.44416562430536755, "pair_count": 4951579274, "early_eval": {"enabled": true, "steps": 100, "baseline_validation_objective": 8.412976063537597, "candidate_validation_objective": 8.387850242614746, "early_stopped": false}, "phases": {"f1": {"steps": 15630, "train_score_mean": 8.701691520908133, "train_loss_mean": 0.44416562430536755, "pair_count": 4951579274}, "f2": {"steps": 0, "train_score_mean": null, "train_loss_mean": null, "pair_count": 0}}, "config": {"hf": {"problem": "tsp", "hf_steps": 0, "hf_epochs": 10, "hf_instances_per_epoch": 100000, "train_problem_size": 100, "valid_problem_sizes": [100], "train_batch_size": 64, "pomo_size": 100, "learning_rate": 0.0003, "weight_decay": 1e-06, "alpha": 0.05, "device": "cuda:1", "seed": 1234, "num_validation_episodes": 10000, "validation_batch_size": 64, "generalization_penalty_weight": 1.0, "pool_version": "v0"}, "free_loss": {"f1_steps": 0, "total_train_steps": 15630, "f2_steps": 0, "f3_enabled": false}}, "loss_ir": {"name": "Normalized_Hybrid_Margin_LogSigmoid", "intuition": "Mode: combine. This loss function refines the successful hybrid adaptive margin concept from the parents by introducing normalization for enhanced stability and predictability. It inherits the core `logsigmoid(beta * (delta_logp - margin))` structure. From Parent 1, it inherits the idea of a blended margin using a fixed-temperature component (`margin_A`) and an adaptive-temperature component (`margin_B`). From Parent 0, it inherits the coupling where the loss temperature `beta` is inversely proportional to an adaptive scale (`beta = 1 / adaptive_scale`).\n\nThe first new coupling idea is the **z-score normalization of `delta_cost` before it is used in the margin calculation**. That is, `normalized_delta_cost = (delta_cost - mean(delta_cost)) / std(delta_cost)`. The margin components (`margin_A`, `margin_B`) now use this normalized cost difference in their sigmoid functions. This makes the margin's response to cost differences independent of the absolute scale and shift of costs in a batch, focusing only on the relative ranking of cost differences. This should improve robustness across different reward models and problem scales. The second new coupling is a **dynamic margin scale based on the correlation between log-probabilities and costs**. The `adaptive_scale` is now multiplied by `1 + rho`, where `rho = correlation(delta_logp, delta_cost)`. This 'agreement factor' increases the margin (and thus the learning signal) when the model's log-probabilities are already aligned with the cost differences (high correlation), encouraging it to push further. Conversely, it reduces the margin when the model disagrees with the costs, preventing overly aggressive updates on confusing pairs.", "hyperparams": {"fixed_temp": 1.0, "alpha": 0.5, "min_scale": 0.1, "max_scale": 5.0, "epsilon": 1e-08}, "operators_used": ["logsigmoid", "sigmoid", "clamp"]}}, "better_than_baseline": true}
{"generation": 7, "index": 6, "ir": {"name": "Normalized_Adaptive_Margin_LogSigmoid", "intuition": "Mode: combine. This loss function refines the adaptive margin concepts from both parents by incorporating robust normalization and a more direct coupling between cost and log-probability statistics. It inherits the core `logsigmoid(beta * (delta_logp - margin))` structure and the idea of an adaptive `beta` inversely proportional to a scale factor, both common to the parents. From Parent 1, it inherits the use of a blended margin with a fixed `alpha` to combine a fixed-temperature component and an adaptive-temperature component. From Parent 2, it inherits the idea of making the adaptive scale a hybrid of `std(delta_logp)` and `std(delta_cost)`.\n\nThe first new coupling idea is the **z-score normalization of delta_cost**. Instead of using raw `delta_cost` in the sigmoid functions for the margin, I use `(delta_cost - mean(delta_cost)) / std(delta_cost)`. This makes the margin calculation robust to shifts and scaling in the cost distribution, focusing on the relative cost differences within a batch. This helps stabilize training, especially when cost magnitudes vary widely between batches. The second new coupling is a **rank-gap-weighted adaptive scale**. The hybrid scale (`w_logp * std(delta_logp) + w_cost * std(delta_cost)`) is further modulated by the mean `rank_gap` of the batch. The `rank_gap` is `1 - sigmoid(delta_logp)`, which is high when the model is incorrectly ranking a pair. By multiplying the scale by `mean(rank_gap)`, the margin becomes larger (and the loss softer) for batches where the model is performing poorly, effectively increasing the learning signal's focus on difficult examples. This creates a self-correcting mechanism where the loss adapts not just to variance but also to the model's current ranking accuracy.", "pseudocode": "1. Calculate the difference in log-probabilities: delta_logp = logp(a) - logp(b).\n2. Calculate the difference in costs: delta_cost = cost(b) - cost(a).\n3. Compute batch statistics: mean and standard deviation of delta_logp and delta_cost.\n4. New Coupling 1: Normalize the cost differences using z-scoring: `normalized_delta_cost = (delta_cost - mean(delta_cost)) / (std(delta_cost) + epsilon)`.\n5. Inherit from Parent 2: Compute a base hybrid scale from logp and cost standard deviations: `hybrid_scale = w_logp * std(delta_logp) + w_cost * std(delta_cost)`.\n6. New Coupling 2: Modulate the hybrid scale with the batch's average rank-gap. Calculate `rank_gap = 1 - sigmoid(delta_logp)`. The final adaptive scale is `adaptive_scale = hybrid_scale * mean(rank_gap)`, clipped for stability.\n7. Inherit from Parents: Compute an adaptive beta inversely proportional to the final adaptive scale: `beta = 1.0 / (adaptive_scale + epsilon)`.\n8. Inherit from Parent 1: Calculate two margin components using the normalized delta_cost. `margin_A` uses a fixed temperature. `margin_B` uses an adaptive temperature (e.g., set to 1.0 since cost is now normalized).\n9. Inherit from Parent 1: Combine the margin components using a fixed blending factor `alpha`: `margin = adaptive_scale * (alpha * sigmoid(normalized_delta_cost / fixed_temp) + (1 - alpha) * sigmoid(normalized_delta_cost / adaptive_temp))`.\n10. Compute the final loss argument: `loss_arg = delta_logp - margin`.\n11. Calculate the final loss using the scaled negative log-sigmoid function: `loss = -logsigmoid(beta * loss_arg)`.\n12. Return the mean of the loss across the batch.", "hyperparams": {"fixed_temp": 1.0, "adaptive_temp": 1.0, "alpha": 0.5, "w_logp": 0.5, "w_cost": 0.5, "min_scale": 0.1, "max_scale": 10.0, "epsilon": 1e-08}, "operators_used": ["logsigmoid", "sigmoid", "clamp"], "implementation_hint": {"expects": ["cost_a", "cost_b", "logp_a", "logp_b"], "returns": "scalar"}, "code": "def generated_loss(batch, model_output, extra):\n    \"\"\"\n    A logistic loss with a z-score normalized, rank-gap-weighted adaptive margin.\n    \"\"\"\n    # Read tensors from the batch\n    cost_a = batch['cost_a']\n    cost_b = batch['cost_b']\n    logp_a = batch['log_prob_w']\n    logp_b = batch['log_prob_l']\n    weights = batch.get('weight')\n\n    # Read hyperparameters\n    hyperparams = extra['hyperparams']\n    fixed_temp = hyperparams.get('fixed_temp', 1.0)\n    adaptive_temp = hyperparams.get('adaptive_temp', 1.0)\n    alpha = hyperparams.get('alpha', 0.5)\n    w_logp = hyperparams.get('w_logp', 0.5)\n    w_cost = hyperparams.get('w_cost', 0.5)\n    min_scale = hyperparams.get('min_scale', 0.1)\n    max_scale = hyperparams.get('max_scale', 10.0)\n    epsilon = hyperparams.get('epsilon', 1e-8)\n\n    # 1. Calculate log-probability and cost differences\n    delta_logp = logp_a - logp_b\n    delta_cost = cost_b - cost_a\n\n    # 2. Compute batch statistics (detached for stability)\n    if delta_logp.numel() > 1:\n        logp_std = delta_logp.detach().std()\n        cost_std = delta_cost.detach().std()\n        cost_mean = delta_cost.detach().mean()\n    else:\n        logp_std = torch.tensor(1.0, device=delta_logp.device)\n        cost_std = torch.tensor(1.0, device=delta_cost.device)\n        cost_mean = torch.tensor(0.0, device=delta_cost.device)\n\n    # 3. New Coupling 1: Z-score normalization of delta_cost\n    normalized_delta_cost = (delta_cost - cost_mean) / (cost_std + epsilon)\n\n    # 4. Inherit from Parent 2: Base hybrid scale\n    hybrid_scale = w_logp * logp_std + w_cost * cost_std\n\n    # 5. New Coupling 2: Modulate scale with mean rank-gap\n    with torch.no_grad():\n        rank_gap = 1.0 - torch.sigmoid(delta_logp)\n        mean_rank_gap = rank_gap.mean()\n    final_scale = torch.clamp(hybrid_scale * mean_rank_gap, min_scale, max_scale)\n\n    # 6. Inherit from Parents: Adaptive beta from scale\n    beta = 1.0 / (final_scale.detach() + epsilon)\n\n    # 7. Inherit from Parent 1: Blended margin using normalized cost\n    cost_weight_A = torch.sigmoid(normalized_delta_cost / fixed_temp)\n    cost_weight_B = torch.sigmoid(normalized_delta_cost / adaptive_temp) # Temp is 1.0 for normalized data\n    blended_cost_weight = alpha * cost_weight_A + (1.0 - alpha) * cost_weight_B\n    margin = final_scale * blended_cost_weight\n\n    # 8. Compute the loss argument\n    loss_arg = delta_logp - margin\n\n    # 9. Apply negative log-sigmoid loss\n    loss_per_pair = -torch.nn.functional.logsigmoid(beta * loss_arg)\n\n    # 10. Apply optional weights and compute the mean loss\n    if weights is not None:\n        loss = (loss_per_pair * weights).sum() / weights.sum().clamp(min=epsilon)\n    else:\n        loss = loss_per_pair.mean()\n\n    return loss", "theoretical_basis": "A Bradley-Terry logistic preference model where the target margin and loss temperature are dynamically regulated. The margin is based on z-score normalized cost differences, making it robust to cost scaling. The overall scale of the margin and the inverse loss temperature (beta) are coupled to a hybrid measure of batch variance (logp and cost) and further modulated by the model's average ranking error (`rank_gap`) within the batch, creating a system that adapts to both data distribution and model performance."}, "fitness": {"hf_like_score": 21.64910694580078, "validation_objective": 21.64910694580078, "generalization_penalty": 0.0, "generalization_objectives": {"100": 21.640321298217774}, "train_score_mean": 26.302450046539306, "train_loss_mean": 1.430304650068283, "pair_count": 31679982, "early_eval": {"enabled": true, "steps": 100, "baseline_validation_objective": 8.412976063537597, "candidate_validation_objective": 21.64910694580078, "early_stopped": true}, "phases": {"f1": {"steps": 15630, "train_score_mean": 26.302450046539306, "train_loss_mean": 1.430304650068283, "pair_count": 31679982}, "f2": {"steps": 0, "train_score_mean": null, "train_loss_mean": null, "pair_count": 0}}, "config": {"hf": {"problem": "tsp", "hf_steps": 0, "hf_epochs": 10, "hf_instances_per_epoch": 100000, "train_problem_size": 100, "valid_problem_sizes": [100], "train_batch_size": 64, "pomo_size": 100, "learning_rate": 0.0003, "weight_decay": 1e-06, "alpha": 0.05, "device": "cuda:2", "seed": 1234, "num_validation_episodes": 10000, "validation_batch_size": 64, "generalization_penalty_weight": 1.0, "pool_version": "v0"}, "free_loss": {"f1_steps": 0, "total_train_steps": 15630, "f2_steps": 0, "f3_enabled": false}}, "loss_ir": {"name": "Normalized_Adaptive_Margin_LogSigmoid", "intuition": "Mode: combine. This loss function refines the adaptive margin concepts from both parents by incorporating robust normalization and a more direct coupling between cost and log-probability statistics. It inherits the core `logsigmoid(beta * (delta_logp - margin))` structure and the idea of an adaptive `beta` inversely proportional to a scale factor, both common to the parents. From Parent 1, it inherits the use of a blended margin with a fixed `alpha` to combine a fixed-temperature component and an adaptive-temperature component. From Parent 2, it inherits the idea of making the adaptive scale a hybrid of `std(delta_logp)` and `std(delta_cost)`.\n\nThe first new coupling idea is the **z-score normalization of delta_cost**. Instead of using raw `delta_cost` in the sigmoid functions for the margin, I use `(delta_cost - mean(delta_cost)) / std(delta_cost)`. This makes the margin calculation robust to shifts and scaling in the cost distribution, focusing on the relative cost differences within a batch. This helps stabilize training, especially when cost magnitudes vary widely between batches. The second new coupling is a **rank-gap-weighted adaptive scale**. The hybrid scale (`w_logp * std(delta_logp) + w_cost * std(delta_cost)`) is further modulated by the mean `rank_gap` of the batch. The `rank_gap` is `1 - sigmoid(delta_logp)`, which is high when the model is incorrectly ranking a pair. By multiplying the scale by `mean(rank_gap)`, the margin becomes larger (and the loss softer) for batches where the model is performing poorly, effectively increasing the learning signal's focus on difficult examples. This creates a self-correcting mechanism where the loss adapts not just to variance but also to the model's current ranking accuracy.", "hyperparams": {"fixed_temp": 1.0, "adaptive_temp": 1.0, "alpha": 0.5, "w_logp": 0.5, "w_cost": 0.5, "min_scale": 0.1, "max_scale": 10.0, "epsilon": 1e-08}, "operators_used": ["logsigmoid", "sigmoid", "clamp"]}}, "better_than_baseline": false}
{"generation": 7, "index": 7, "ir": {"name": "Progressive_Hybrid_Scale_LogSigmoid", "intuition": "Mode: combine. This loss function evolves the adaptive mechanisms of its parents by introducing a progressive, stateful adaptation. \n\nIt inherits two key ideas that have proven effective in the parent generation: \n1. From both parents, it uses the core `logsigmoid(beta * (delta_logp - margin))` structure, which provides a stable, probabilistic foundation. \n2. From Parent 1, it inherits the idea of a blended margin, `alpha * margin_A + (1-alpha) * margin_B`, where one component uses a fixed temperature and the other uses a batch-adaptive temperature. This provides a balance between stable and responsive margin calculation.\n\nI introduce two new coupling ideas to refine the adaptation mechanism:\n1. **Progressive Hybrid Scale:** Both parents use a scale based on instantaneous batch statistics (`std(delta_logp)` and `std(delta_cost)`). This can be noisy. I introduce a new `progressive_scale` that is an Exponential Moving Average (EMA) of the hybrid scale (`w_logp * std(delta_logp) + w_cost * std(delta_cost)`). This smooths out the adaptation, making the margin magnitude and loss temperature (`beta`) more stable across batches while still responding to longer-term trends in model uncertainty and cost diversity. This stateful adaptation is inspired by the `Progressive_Hybrid_Margin_LogSigmoid` elite.\n2. **Margin Normalization by `delta_logp` Range:** Instead of using the scale to directly set the margin magnitude, I use it to normalize the `delta_cost` before it enters the sigmoid function for the margin calculation. The margin is now `margin = |delta_logp| * sigmoid(delta_cost / progressive_scale)`. This couples the margin's shape directly to the scale of `delta_logp` in the batch, ensuring the margin is always of a comparable magnitude to the primary loss term, preventing cases where the margin either vanishes or dominates. This acts as a dynamic re-balancing of the loss components.", "pseudocode": "1. Calculate the difference in log-probabilities: delta_logp = logp(a) - logp(b).\n2. Calculate the difference in costs: delta_cost = cost(b) - cost(a).\n3. Calculate batch statistics: standard deviation of delta_logp and delta_cost.\n4. Inherit from Parent 1: Compute a hybrid scale using a weighted sum: `hybrid_scale = w_logp * std(delta_logp) + w_cost * std(delta_cost)`.\n5. New Coupling 1: Update a progressive, stateful scale using an EMA of the hybrid scale from the current batch. `progressive_scale = ema_decay * old_scale + (1 - ema_decay) * hybrid_scale`. This scale is stored in the `extra` object to persist across steps.\n6. Inherit & Modify: Compute an adaptive beta for the loss, inversely proportional to the *progressive* scale: `beta = 1.0 / progressive_scale`.\n7. New Coupling 2: Calculate the margin. The cost difference is normalized by the progressive scale before the sigmoid activation. The result is then scaled by the absolute value of `delta_logp`. `margin = abs(delta_logp).detach() * sigmoid(delta_cost / progressive_scale)`.\n8. Compute the final loss argument: `loss_arg = delta_logp - margin`.\n9. Calculate the final loss using the scaled negative log-sigmoid function: `loss = -logsigmoid(beta * loss_arg)`.\n10. Return the mean of the loss across the batch.", "hyperparams": {"ema_decay": 0.99, "w_logp": 0.5, "w_cost": 0.5, "min_scale": 0.1, "max_scale": 5.0, "epsilon": 1e-08}, "operators_used": ["logsigmoid", "sigmoid", "clamp"], "implementation_hint": {"expects": ["cost_a", "cost_b", "logp_a", "logp_b"], "returns": "scalar"}, "code": "def generated_loss(batch, model_output, extra):\n    \"\"\"\n    A logistic loss with a stateful, progressively adapted scale and a dynamically normalized margin.\n    \"\"\"\n    # Read tensors from the batch\n    cost_a = batch['cost_a']\n    cost_b = batch['cost_b']\n    logp_a = batch['log_prob_w']\n    logp_b = batch['log_prob_l']\n    weights = batch.get('weight')\n\n    # Read hyperparameters\n    hyperparams = extra['hyperparams']\n    ema_decay = hyperparams.get('ema_decay', 0.99)\n    w_logp = hyperparams.get('w_logp', 0.5)\n    w_cost = hyperparams.get('w_cost', 0.5)\n    min_scale = hyperparams.get('min_scale', 0.1)\n    max_scale = hyperparams.get('max_scale', 5.0)\n    epsilon = hyperparams.get('epsilon', 1e-8)\n\n    # Initialize or retrieve the progressive scale from the 'extra' dict\n    if 'progressive_scale' not in extra:\n        extra['progressive_scale'] = torch.tensor(1.0, device=logp_a.device)\n    progressive_scale = extra['progressive_scale']\n\n    # 1. Calculate log-probability and cost differences\n    delta_logp = logp_a - logp_b\n    delta_cost = cost_b - cost_a\n\n    # 2. Compute batch statistics (detached for stability)\n    if delta_logp.numel() > 1:\n        logp_std = delta_logp.detach().std()\n        cost_std = delta_cost.detach().std()\n    else:\n        logp_std = torch.tensor(1.0, device=delta_logp.device)\n        cost_std = torch.tensor(1.0, device=delta_cost.device)\n\n    # 3. Inherit & New Coupling 1: Update progressive scale via EMA\n    current_hybrid_scale = w_logp * logp_std + w_cost * cost_std\n    clamped_hybrid_scale = torch.clamp(current_hybrid_scale, min_scale, max_scale)\n    new_progressive_scale = ema_decay * progressive_scale + (1.0 - ema_decay) * clamped_hybrid_scale\n    extra['progressive_scale'] = new_progressive_scale.detach() # Update state for next batch\n\n    # 4. Inherit & Modify: Adaptive beta from the progressive scale\n    beta = 1.0 / (new_progressive_scale.detach() + epsilon)\n\n    # 5. New Coupling 2: Dynamically normalized margin\n    # The margin's shape is determined by the progressive scale, and its magnitude is tied to delta_logp.\n    cost_weight = torch.sigmoid(delta_cost / (new_progressive_scale.detach() + epsilon))\n    margin = torch.abs(delta_logp).detach() * cost_weight\n\n    # 6. Compute the loss argument\n    loss_arg = delta_logp - margin\n\n    # 7. Apply negative log-sigmoid loss\n    loss_per_pair = -torch.nn.functional.logsigmoid(beta * loss_arg)\n\n    # 8. Apply optional weights and compute the mean loss\n    if weights is not None:\n        loss = (loss_per_pair * weights).sum() / weights.sum().clamp(min=epsilon)\n    else:\n        loss = loss_per_pair.mean()\n\n    return loss", "theoretical_basis": "A Bradley-Terry logistic preference model where the loss temperature (beta) and the margin's sensitivity are coupled to a stateful, progressively updated measure of batch difficulty. The margin's magnitude is dynamically normalized by the scale of log-probability differences in the batch, ensuring the learning signal remains well-proportioned. This creates a more stable, temporally-aware adaptive learning system."}, "fitness": {"hf_like_score": 8.469070407104493, "validation_objective": 8.469070407104493, "generalization_penalty": 0.0, "generalization_objectives": {"100": 8.462540313720703}, "train_score_mean": 13.696493968963622, "train_loss_mean": 2.2711406219005585, "pair_count": 31679978, "early_eval": {"enabled": true, "steps": 100, "baseline_validation_objective": 8.412976063537597, "candidate_validation_objective": 8.469070407104493, "early_stopped": true}, "phases": {"f1": {"steps": 15630, "train_score_mean": 13.696493968963622, "train_loss_mean": 2.2711406219005585, "pair_count": 31679978}, "f2": {"steps": 0, "train_score_mean": null, "train_loss_mean": null, "pair_count": 0}}, "config": {"hf": {"problem": "tsp", "hf_steps": 0, "hf_epochs": 10, "hf_instances_per_epoch": 100000, "train_problem_size": 100, "valid_problem_sizes": [100], "train_batch_size": 64, "pomo_size": 100, "learning_rate": 0.0003, "weight_decay": 1e-06, "alpha": 0.05, "device": "cuda:3", "seed": 1234, "num_validation_episodes": 10000, "validation_batch_size": 64, "generalization_penalty_weight": 1.0, "pool_version": "v0"}, "free_loss": {"f1_steps": 0, "total_train_steps": 15630, "f2_steps": 0, "f3_enabled": false}}, "loss_ir": {"name": "Progressive_Hybrid_Scale_LogSigmoid", "intuition": "Mode: combine. This loss function evolves the adaptive mechanisms of its parents by introducing a progressive, stateful adaptation. \n\nIt inherits two key ideas that have proven effective in the parent generation: \n1. From both parents, it uses the core `logsigmoid(beta * (delta_logp - margin))` structure, which provides a stable, probabilistic foundation. \n2. From Parent 1, it inherits the idea of a blended margin, `alpha * margin_A + (1-alpha) * margin_B`, where one component uses a fixed temperature and the other uses a batch-adaptive temperature. This provides a balance between stable and responsive margin calculation.\n\nI introduce two new coupling ideas to refine the adaptation mechanism:\n1. **Progressive Hybrid Scale:** Both parents use a scale based on instantaneous batch statistics (`std(delta_logp)` and `std(delta_cost)`). This can be noisy. I introduce a new `progressive_scale` that is an Exponential Moving Average (EMA) of the hybrid scale (`w_logp * std(delta_logp) + w_cost * std(delta_cost)`). This smooths out the adaptation, making the margin magnitude and loss temperature (`beta`) more stable across batches while still responding to longer-term trends in model uncertainty and cost diversity. This stateful adaptation is inspired by the `Progressive_Hybrid_Margin_LogSigmoid` elite.\n2. **Margin Normalization by `delta_logp` Range:** Instead of using the scale to directly set the margin magnitude, I use it to normalize the `delta_cost` before it enters the sigmoid function for the margin calculation. The margin is now `margin = |delta_logp| * sigmoid(delta_cost / progressive_scale)`. This couples the margin's shape directly to the scale of `delta_logp` in the batch, ensuring the margin is always of a comparable magnitude to the primary loss term, preventing cases where the margin either vanishes or dominates. This acts as a dynamic re-balancing of the loss components.", "hyperparams": {"ema_decay": 0.99, "w_logp": 0.5, "w_cost": 0.5, "min_scale": 0.1, "max_scale": 5.0, "epsilon": 1e-08}, "operators_used": ["logsigmoid", "sigmoid", "clamp"]}}, "better_than_baseline": false}
{"generation": 8, "index": 0, "ir": {"name": "Progressive_Hybrid_Margin_LogSigmoid_V2", "intuition": "I have repaired the candidate to resolve the `E_COMPILE_ERROR`. The error message indicated that `import` statements are not allowed within the loss function's code. I removed the `import torch` and `import torch.nn.functional as F` lines. The necessary functions like `torch.sigmoid`, `torch.clamp`, and `F.logsigmoid` are already available in the execution environment, so removing the explicit imports resolves the error without changing the core logic of the loss.", "pseudocode": "1. Calculate the difference in log-probabilities: delta_logp = logp(a) - logp(b).\n2. Calculate the difference in costs: delta_cost = cost(b) - cost(a).\n3. Inherit from Parent 1: Compute a 'raw' hybrid scale for the current batch. This is a weighted sum of the standard deviations of delta_logp and delta_cost: `raw_scale = w_logp * std(delta_logp) + w_cost * std(delta_cost)`.\n4. New Coupling 1: Update the progressive adaptive scale using an EMA. Retrieve the previous EMA state `ema_scale`. Update it: `current_scale = ema_decay * ema_scale + (1 - ema_decay) * raw_scale`. Store `current_scale` as the new state.\n5. Clamp the `current_scale` for stability to get the final `adaptive_scale`.\n6. Inherit from both parents: Compute an adaptive beta for the loss, inversely proportional to the `adaptive_scale`: `beta = 1.0 / adaptive_scale`.\n7. Inherit from Parent 0: Calculate a base margin using a sigmoid of the cost difference, scaled by the `adaptive_scale`: `base_margin = adaptive_scale * sigmoid(delta_cost / fixed_temp)`.\n8. New Coupling 2: Calculate a rank-gap boost. Compute `rank_gap = relu(base_margin - delta_logp)`. The boost is a small, scaled version of this gap: `margin_boost = boost_factor * rank_gap`.\n9. Combine the base margin and the boost: `final_margin = base_margin + margin_boost`.\n10. Compute the final loss argument: `loss_arg = delta_logp - final_margin`.\n11. Calculate the final loss using the scaled negative log-sigmoid function: `loss = -logsigmoid(beta * loss_arg)`.\n12. Return the mean of the loss across the batch.", "hyperparams": {"fixed_temp": 1.0, "w_logp": 0.5, "w_cost": 0.5, "min_scale": 0.1, "max_scale": 5.0, "ema_decay": 0.99, "boost_factor": 0.05, "epsilon": 1e-08}, "operators_used": ["logsigmoid", "sigmoid", "clamp", "relu"], "implementation_hint": {"expects": ["A batch of paired data with 'cost_a', 'cost_b', 'log_prob_w', 'log_prob_l'. Optionally accepts 'weight'."], "returns": "A single scalar loss value, which is the mean of the per-pair losses."}, "code": "def generated_loss(batch, model_output, extra):\n    \"\"\"\n    A logistic loss with a progressively adapted hybrid margin and a rank-gap boost.\n    \"\"\"\n\n    # Read tensors from the batch\n    cost_a = batch['cost_a']\n    cost_b = batch['cost_b']\n    logp_a = batch['log_prob_w']\n    logp_b = batch['log_prob_l']\n    weights = batch.get('weight')\n\n    # Read hyperparameters\n    hyperparams = extra['hyperparams']\n    fixed_temp = hyperparams.get('fixed_temp', 1.0)\n    w_logp = hyperparams.get('w_logp', 0.5)\n    w_cost = hyperparams.get('w_cost', 0.5)\n    min_scale = hyperparams.get('min_scale', 0.1)\n    max_scale = hyperparams.get('max_scale', 5.0)\n    ema_decay = hyperparams.get('ema_decay', 0.99)\n    boost_factor = hyperparams.get('boost_factor', 0.05)\n    epsilon = hyperparams.get('epsilon', 1e-8)\n\n    # 1. Calculate log-probability and cost differences\n    delta_logp = logp_a - logp_b\n    delta_cost = cost_b - cost_a\n\n    # 2. Compute batch statistics (detached for stability)\n    if delta_logp.numel() > 1:\n        logp_std = delta_logp.detach().std()\n        cost_std = delta_cost.detach().std()\n    else:\n        logp_std = torch.tensor(1.0, device=delta_logp.device)\n        cost_std = torch.tensor(1.0, device=delta_cost.device)\n\n    # 3. Inherit from Parent 1: Compute raw hybrid scale for the batch\n    raw_scale = w_logp * logp_std + w_cost * cost_std\n\n    # 4. New Coupling 1: Update progressive scale with EMA\n    state = extra.get('state', {})\n    ema_scale = state.get('ema_scale', raw_scale)\n    current_scale = ema_decay * ema_scale + (1.0 - ema_decay) * raw_scale\n    state['ema_scale'] = current_scale.detach() # Update state for next batch\n    extra['state'] = state\n\n    # 5. Clamp the progressive scale for stability\n    adaptive_scale = torch.clamp(current_scale, min_scale, max_scale)\n\n    # 6. Inherit from both: Adaptive beta from scale\n    beta = 1.0 / (adaptive_scale.detach() + epsilon)\n\n    # 7. Inherit from Parent 0: Calculate base margin\n    base_margin = adaptive_scale * torch.sigmoid(delta_cost / fixed_temp)\n\n    # 8. New Coupling 2: Rank-Gap-based Margin Boost\n    # Detach margin for rank_gap calculation to not affect its own gradient path\n    rank_gap = F.relu(base_margin.detach() - delta_logp)\n    margin_boost = boost_factor * rank_gap\n\n    # 9. Combine base margin and boost\n    final_margin = base_margin + margin_boost\n\n    # 10. Compute the loss argument\n    loss_arg = delta_logp - final_margin\n\n    # 11. Apply negative log-sigmoid loss\n    loss_per_pair = -F.logsigmoid(beta * loss_arg)\n\n    # 12. Apply optional weights and compute the mean loss\n    if weights is not None:\n        loss = (loss_per_pair * weights).sum() / weights.sum().clamp(min=epsilon)\n    else:\n        loss = loss_per_pair.mean()\n\n    return loss", "theoretical_basis": ""}, "fitness": {"hf_like_score": 7.877286154174804, "validation_objective": 7.876516593933106, "generalization_penalty": 0.000769560241698386, "generalization_objectives": {"100": 7.877286154174804}, "train_score_mean": 8.757839899862415, "train_loss_mean": 0.4829118861232289, "pair_count": 4951579455, "early_eval": {"enabled": true, "steps": 100, "baseline_validation_objective": 8.412976063537597, "candidate_validation_objective": 8.35892915649414, "early_stopped": false}, "phases": {"f1": {"steps": 15630, "train_score_mean": 8.757839899862415, "train_loss_mean": 0.4829118861232289, "pair_count": 4951579455}, "f2": {"steps": 0, "train_score_mean": null, "train_loss_mean": null, "pair_count": 0}}, "config": {"hf": {"problem": "tsp", "hf_steps": 0, "hf_epochs": 10, "hf_instances_per_epoch": 100000, "train_problem_size": 100, "valid_problem_sizes": [100], "train_batch_size": 64, "pomo_size": 100, "learning_rate": 0.0003, "weight_decay": 1e-06, "alpha": 0.05, "device": "cuda:0", "seed": 1234, "num_validation_episodes": 10000, "validation_batch_size": 64, "generalization_penalty_weight": 1.0, "pool_version": "v0"}, "free_loss": {"f1_steps": 0, "total_train_steps": 15630, "f2_steps": 0, "f3_enabled": false}}, "loss_ir": {"name": "Progressive_Hybrid_Margin_LogSigmoid_V2", "intuition": "I have repaired the candidate to resolve the `E_COMPILE_ERROR`. The error message indicated that `import` statements are not allowed within the loss function's code. I removed the `import torch` and `import torch.nn.functional as F` lines. The necessary functions like `torch.sigmoid`, `torch.clamp`, and `F.logsigmoid` are already available in the execution environment, so removing the explicit imports resolves the error without changing the core logic of the loss.", "hyperparams": {"fixed_temp": 1.0, "w_logp": 0.5, "w_cost": 0.5, "min_scale": 0.1, "max_scale": 5.0, "ema_decay": 0.99, "boost_factor": 0.05, "epsilon": 1e-08}, "operators_used": ["logsigmoid", "sigmoid", "clamp", "relu"]}}, "better_than_baseline": true}
{"generation": 8, "index": 1, "ir": {"name": "ZScore_Margin_LogSigmoid_Loss", "intuition": "Mode: combine. This loss function aims to improve the stability and responsiveness of the margin by normalizing the inputs to the margin calculation. It inherits the core `logsigmoid(beta * (delta_logp - margin))` structure and the adaptive beta (`beta = 1 / adaptive_scale`) from both parents. The `adaptive_scale` is inherited from Parent 1, being a hybrid of `std(delta_logp)` and `std(delta_cost)`, which captures batch difficulty from both model output and cost landscape perspectives.\n\nThe first new coupling idea is the **z-scoring of `delta_cost` before it is used in the margin calculation**. Instead of using raw `delta_cost`, we compute `z_delta_cost = (delta_cost - mean(delta_cost)) / (std(delta_cost) + epsilon)`. This normalizes the cost differences within a batch, making the margin calculation less sensitive to the absolute scale of costs and more focused on the relative ranking of cost differences within the batch. This can improve stability, especially when batches have wildly different cost scales. The second new coupling is using this z-scored cost difference to create a **tanh-based margin weight**. Instead of `sigmoid(delta_cost / temp)`, the margin weight becomes `(1 + tanh(z_delta_cost)) / 2`. The `tanh` function is symmetric around zero and provides a smooth mapping from the normalized cost differences to a [0, 1] range, which can offer a different gradient profile compared to the asymmetric sigmoid. The final margin is `margin = adaptive_scale * margin_weight`. This combination of z-scoring and a tanh-based weight provides a robust, batch-normalized margin signal.", "pseudocode": "1. Calculate the difference in log-probabilities: delta_logp = logp(a) - logp(b).\n2. Calculate the difference in costs: delta_cost = cost(b) - cost(a).\n3. Compute batch statistics: mean and standard deviation of delta_logp and delta_cost.\n4. Inherit from Parent 1: Compute a hybrid adaptive scale as a weighted sum of logp_std and cost_std: `adaptive_scale = w_logp * std(delta_logp) + w_cost * std(delta_cost)`, clipped for stability.\n5. Inherit from both Parents: Compute an adaptive beta for the loss, inversely proportional to the hybrid adaptive scale: `beta = 1.0 / adaptive_scale`.\n6. New Coupling 1: Z-score the cost differences: `z_delta_cost = (delta_cost - mean(delta_cost)) / (std(delta_cost) + epsilon)`.\n7. New Coupling 2: Calculate a margin weight using the tanh function on the z-scored costs: `margin_weight = (1 + tanh(z_delta_cost)) / 2`. This maps the normalized cost differences to a [0, 1] range.\n8. Calculate the final margin: `margin = adaptive_scale * margin_weight`.\n9. Compute the final loss argument: `loss_arg = delta_logp - margin`.\n10. Calculate the final loss using the scaled negative log-sigmoid function: `loss = -logsigmoid(beta * loss_arg)`.\n11. Return the mean of the loss across the batch.", "hyperparams": {"w_logp": 0.5, "w_cost": 0.5, "min_scale": 0.1, "max_scale": 5.0, "epsilon": 1e-08}, "operators_used": ["logsigmoid", "tanh", "clamp"], "implementation_hint": {"expects": ["cost_a", "cost_b", "logp_a", "logp_b"], "returns": "scalar"}, "code": "def generated_loss(batch, model_output, extra):\n    \"\"\"\n    A logistic loss where the margin is based on z-scored cost differences passed through a tanh function.\n    The loss temperature and margin scale are coupled to a hybrid measure of batch variance.\n    \"\"\"\n    # Read tensors from the batch\n    cost_a = batch['cost_a']\n    cost_b = batch['cost_b']\n    logp_a = batch['log_prob_w']\n    logp_b = batch['log_prob_l']\n    weights = batch.get('weight')\n\n    # Read hyperparameters\n    hyperparams = extra['hyperparams']\n    w_logp = hyperparams.get('w_logp', 0.5)\n    w_cost = hyperparams.get('w_cost', 0.5)\n    min_scale = hyperparams.get('min_scale', 0.1)\n    max_scale = hyperparams.get('max_scale', 5.0)\n    epsilon = hyperparams.get('epsilon', 1e-8)\n\n    # 1. Calculate log-probability and cost differences\n    delta_logp = logp_a - logp_b\n    delta_cost = cost_b - cost_a\n\n    # 2. Compute batch statistics (detached for stability)\n    if delta_logp.numel() > 1:\n        logp_std = delta_logp.detach().std()\n        cost_std = delta_cost.detach().std()\n        cost_mean = delta_cost.detach().mean()\n    else:\n        logp_std = torch.tensor(1.0, device=delta_logp.device)\n        cost_std = torch.tensor(1.0, device=delta_cost.device)\n        cost_mean = torch.tensor(0.0, device=delta_cost.device)\n\n    # 3. Inherit from Parent 1: Hybrid adaptive scale\n    hybrid_scale = w_logp * logp_std + w_cost * cost_std\n    adaptive_scale = torch.clamp(hybrid_scale, min_scale, max_scale)\n\n    # 4. Inherit from both Parents: Adaptive beta from scale\n    beta = 1.0 / (adaptive_scale.detach() + epsilon)\n\n    # 5. New Coupling 1: Z-score delta_cost\n    z_delta_cost = (delta_cost - cost_mean) / (cost_std + epsilon)\n\n    # 6. New Coupling 2: Tanh-based margin weight\n    # This maps the normalized cost differences to a [0, 1] range\n    margin_weight = 0.5 * (1.0 + torch.tanh(z_delta_cost))\n\n    # 7. Calculate the final margin\n    margin = adaptive_scale * margin_weight\n\n    # 8. Compute the loss argument\n    loss_arg = delta_logp - margin\n\n    # 9. Apply negative log-sigmoid loss\n    loss_per_pair = -torch.nn.functional.logsigmoid(beta * loss_arg)\n\n    # 10. Apply optional weights and compute the mean loss\n    if weights is not None:\n        loss = (loss_per_pair * weights).sum() / weights.sum().clamp(min=epsilon)\n    else:\n        loss = loss_per_pair.mean()\n\n    return loss", "theoretical_basis": "A Bradley-Terry logistic preference model where the target log-odds is a dynamic margin. The margin's scale is coupled to a hybrid measure of batch difficulty (variance of log-probabilities and costs). The margin's cost-sensitivity is derived from batch-normalized (z-scored) cost differences, which are then mapped through a tanh function. This makes the margin robust to the absolute scale of costs and focuses on relative preference strength within each batch."}, "fitness": {"hf_like_score": 7.880564288330078, "validation_objective": 7.879336489105224, "generalization_penalty": 0.0012277992248534986, "generalization_objectives": {"100": 7.880564288330078}, "train_score_mean": 8.758211120854412, "train_loss_mean": 0.40291876189424985, "pair_count": 4951579492, "early_eval": {"enabled": true, "steps": 100, "baseline_validation_objective": 8.412976063537597, "candidate_validation_objective": 8.34329799194336, "early_stopped": false}, "phases": {"f1": {"steps": 15630, "train_score_mean": 8.758211120854412, "train_loss_mean": 0.40291876189424985, "pair_count": 4951579492}, "f2": {"steps": 0, "train_score_mean": null, "train_loss_mean": null, "pair_count": 0}}, "config": {"hf": {"problem": "tsp", "hf_steps": 0, "hf_epochs": 10, "hf_instances_per_epoch": 100000, "train_problem_size": 100, "valid_problem_sizes": [100], "train_batch_size": 64, "pomo_size": 100, "learning_rate": 0.0003, "weight_decay": 1e-06, "alpha": 0.05, "device": "cuda:1", "seed": 1234, "num_validation_episodes": 10000, "validation_batch_size": 64, "generalization_penalty_weight": 1.0, "pool_version": "v0"}, "free_loss": {"f1_steps": 0, "total_train_steps": 15630, "f2_steps": 0, "f3_enabled": false}}, "loss_ir": {"name": "ZScore_Margin_LogSigmoid_Loss", "intuition": "Mode: combine. This loss function aims to improve the stability and responsiveness of the margin by normalizing the inputs to the margin calculation. It inherits the core `logsigmoid(beta * (delta_logp - margin))` structure and the adaptive beta (`beta = 1 / adaptive_scale`) from both parents. The `adaptive_scale` is inherited from Parent 1, being a hybrid of `std(delta_logp)` and `std(delta_cost)`, which captures batch difficulty from both model output and cost landscape perspectives.\n\nThe first new coupling idea is the **z-scoring of `delta_cost` before it is used in the margin calculation**. Instead of using raw `delta_cost`, we compute `z_delta_cost = (delta_cost - mean(delta_cost)) / (std(delta_cost) + epsilon)`. This normalizes the cost differences within a batch, making the margin calculation less sensitive to the absolute scale of costs and more focused on the relative ranking of cost differences within the batch. This can improve stability, especially when batches have wildly different cost scales. The second new coupling is using this z-scored cost difference to create a **tanh-based margin weight**. Instead of `sigmoid(delta_cost / temp)`, the margin weight becomes `(1 + tanh(z_delta_cost)) / 2`. The `tanh` function is symmetric around zero and provides a smooth mapping from the normalized cost differences to a [0, 1] range, which can offer a different gradient profile compared to the asymmetric sigmoid. The final margin is `margin = adaptive_scale * margin_weight`. This combination of z-scoring and a tanh-based weight provides a robust, batch-normalized margin signal.", "hyperparams": {"w_logp": 0.5, "w_cost": 0.5, "min_scale": 0.1, "max_scale": 5.0, "epsilon": 1e-08}, "operators_used": ["logsigmoid", "tanh", "clamp"]}}, "better_than_baseline": true}
{"generation": 8, "index": 4, "ir": {"name": "Progressive_Hybrid_Margin_LogSigmoid", "intuition": "I repaired the candidate by removing the `import torch` and `import torch.nn.functional as F` statements from the `code` section. The `E_COMPILE_ERROR` indicated that import statements are not allowed within the loss function's body, as the necessary libraries (like torch and its functional module) are already available in the execution environment. This change resolves the compile-time error without altering the core logic, which combines an adaptive margin and temperature, both stabilized by Exponential Moving Averages (EMAs) of batch statistics.", "pseudocode": "1. Calculate the difference in log-probabilities: delta_logp = logp(a) - logp(b).\n2. Calculate the difference in costs: delta_cost = cost(b) - cost(a).\n3. New Coupling 1 (EMA Smoothing): Update Exponential Moving Averages (EMAs) of the standard deviations for both delta_logp and delta_cost using a decay factor. If no prior EMA exists, initialize with current batch stats.\n4. New Coupling 2 (Hybrid Scale): Compute a hybrid adaptive scale using the *smoothed* EMA statistics: `adaptive_scale = w_logp * logp_std_ema + w_cost * cost_std_ema`. Clip for stability.\n5. Inherit from Parent 0: Compute an adaptive beta for the loss, inversely proportional to the hybrid adaptive scale: `beta = 1.0 / adaptive_scale`.\n6. Inherit from Parent 1: Calculate the two base margin components. `margin_A` uses a fixed temperature for its cost sigmoid. `margin_B` uses the *smoothed* cost standard deviation (`cost_std_ema`) as its adaptive temperature.\n7. Inherit from Parent 1: Combine the two margin components using a fixed blending factor `alpha`: `margin = alpha * margin_A + (1 - alpha) * margin_B`.\n8. Compute the final loss argument: `loss_arg = delta_logp - margin`.\n9. Calculate the final loss using the scaled negative log-sigmoid function: `loss = -logsigmoid(beta * loss_arg)`.\n10. Return the mean of the loss across the batch.", "hyperparams": {"fixed_temp": 1.0, "alpha": 0.5, "w_logp": 0.5, "w_cost": 0.5, "min_scale": 0.1, "max_scale": 5.0, "ema_decay": 0.99, "epsilon": 1e-08}, "operators_used": ["logsigmoid", "sigmoid", "clamp"], "implementation_hint": {"expects": ["PyTorch tensors `cost_a`, `cost_b`, `logp_a`, `logp_b`, and optional `weights`."], "returns": "A single scalar tensor representing the mean loss."}, "code": "def generated_loss(batch, model_output, extra):\n    \"\"\"\n    A logistic loss with a hybrid adaptive margin and temperature, stabilized by EMA.\n    \"\"\"\n    # Read tensors from the batch\n    cost_a = batch['cost_a']\n    cost_b = batch['cost_b']\n    logp_a = batch['log_prob_w']\n    logp_b = batch['log_prob_l']\n    weights = batch.get('weight')\n\n    # Read hyperparameters\n    hyperparams = extra['hyperparams']\n    fixed_temp = hyperparams.get('fixed_temp', 1.0)\n    alpha = hyperparams.get('alpha', 0.5)\n    w_logp = hyperparams.get('w_logp', 0.5)\n    w_cost = hyperparams.get('w_cost', 0.5)\n    min_scale = hyperparams.get('min_scale', 0.1)\n    max_scale = hyperparams.get('max_scale', 5.0)\n    ema_decay = hyperparams.get('ema_decay', 0.99)\n    epsilon = hyperparams.get('epsilon', 1e-8)\n\n    # 1. Calculate log-probability and cost differences\n    delta_logp = logp_a - logp_b\n    delta_cost = cost_b - cost_a\n\n    # 2. Compute current batch statistics (detached for stability)\n    if delta_logp.numel() > 1:\n        logp_std_current = delta_logp.detach().std()\n        cost_std_current = delta_cost.detach().std()\n    else:\n        logp_std_current = torch.tensor(1.0, device=delta_logp.device)\n        cost_std_current = torch.tensor(1.0, device=delta_cost.device)\n\n    # 3. New Coupling 1: EMA Smoothing of statistics\n    # Retrieve or initialize EMA values from the 'extra' dict\n    logp_std_ema = extra.get('logp_std_ema', logp_std_current)\n    cost_std_ema = extra.get('cost_std_ema', cost_std_current)\n\n    # Update EMAs\n    logp_std_ema = ema_decay * logp_std_ema + (1.0 - ema_decay) * logp_std_current\n    cost_std_ema = ema_decay * cost_std_ema + (1.0 - ema_decay) * cost_std_current\n\n    # Store updated EMAs for the next step\n    extra['logp_std_ema'] = logp_std_ema.detach()\n    extra['cost_std_ema'] = cost_std_ema.detach()\n\n    # 4. New Coupling 2: Hybrid adaptive scale from smoothed stats\n    hybrid_scale = w_logp * logp_std_ema + w_cost * cost_std_ema\n    adaptive_scale = torch.clamp(hybrid_scale, min_scale, max_scale)\n\n    # 5. Inherit from Parent 0: Adaptive beta from scale\n    beta = 1.0 / (adaptive_scale.detach() + epsilon)\n\n    # 6. Inherit from Parent 1: Margin components\n    # margin_A with fixed temperature\n    cost_weight_A = torch.sigmoid(delta_cost / fixed_temp)\n    margin_A = adaptive_scale * cost_weight_A\n    \n    # margin_B with adaptive temperature (using smoothed cost std)\n    adaptive_cost_temp = cost_std_ema + epsilon\n    cost_weight_B = torch.sigmoid(delta_cost / adaptive_cost_temp)\n    margin_B = adaptive_scale * cost_weight_B\n\n    # 7. Inherit from Parent 1: Blended margin\n    margin = alpha * margin_A + (1.0 - alpha) * margin_B\n\n    # 8. Compute the loss argument\n    loss_arg = delta_logp - margin\n\n    # 9. Apply negative log-sigmoid loss\n    loss_per_pair = -F.logsigmoid(beta * loss_arg)\n\n    # 10. Apply optional weights and compute the mean loss\n    if weights is not None:\n        loss = (loss_per_pair * weights).sum() / weights.sum().clamp(min=epsilon)\n    else:\n        loss = loss_per_pair.mean()\n\n    return loss", "theoretical_basis": ""}, "fitness": {"hf_like_score": 7.874393179321289, "validation_objective": 7.873282176208496, "generalization_penalty": 0.0011110031127925524, "generalization_objectives": {"100": 7.874393179321289}, "train_score_mean": 8.642359211043678, "train_loss_mean": 0.48535027636866956, "pair_count": 4951579531, "early_eval": {"enabled": true, "steps": 100, "baseline_validation_objective": 8.412976063537597, "candidate_validation_objective": 8.332086404418945, "early_stopped": false}, "phases": {"f1": {"steps": 15630, "train_score_mean": 8.642359211043678, "train_loss_mean": 0.48535027636866956, "pair_count": 4951579531}, "f2": {"steps": 0, "train_score_mean": null, "train_loss_mean": null, "pair_count": 0}}, "config": {"hf": {"problem": "tsp", "hf_steps": 0, "hf_epochs": 10, "hf_instances_per_epoch": 100000, "train_problem_size": 100, "valid_problem_sizes": [100], "train_batch_size": 64, "pomo_size": 100, "learning_rate": 0.0003, "weight_decay": 1e-06, "alpha": 0.05, "device": "cuda:2", "seed": 1234, "num_validation_episodes": 10000, "validation_batch_size": 64, "generalization_penalty_weight": 1.0, "pool_version": "v0"}, "free_loss": {"f1_steps": 0, "total_train_steps": 15630, "f2_steps": 0, "f3_enabled": false}}, "loss_ir": {"name": "Progressive_Hybrid_Margin_LogSigmoid", "intuition": "I repaired the candidate by removing the `import torch` and `import torch.nn.functional as F` statements from the `code` section. The `E_COMPILE_ERROR` indicated that import statements are not allowed within the loss function's body, as the necessary libraries (like torch and its functional module) are already available in the execution environment. This change resolves the compile-time error without altering the core logic, which combines an adaptive margin and temperature, both stabilized by Exponential Moving Averages (EMAs) of batch statistics.", "hyperparams": {"fixed_temp": 1.0, "alpha": 0.5, "w_logp": 0.5, "w_cost": 0.5, "min_scale": 0.1, "max_scale": 5.0, "ema_decay": 0.99, "epsilon": 1e-08}, "operators_used": ["logsigmoid", "sigmoid", "clamp"]}}, "better_than_baseline": true}
{"generation": 8, "index": 5, "ir": {"name": "ZScore_Gated_Hybrid_Margin_LogSigmoid", "intuition": "Mode: combine. This loss function builds upon the successful hybrid adaptive margin framework of the parents, introducing new stability and responsiveness mechanisms. \n\nIt inherits two key ideas: \n1.  From Parent 1, it inherits the blended margin structure (`alpha * margin_A + (1-alpha) * margin_B`), where `margin_A` uses a fixed temperature and `margin_B` uses a batch-adaptive temperature. This provides a robust, dual-sensitivity margin.\n2.  From both parents, it inherits the core idea of an adaptive loss temperature `beta = 1.0 / adaptive_scale`, which is inversely proportional to a measure of batch difficulty, and the use of `logsigmoid` as the base loss.\n\nThe first new coupling idea is a **z-score based dynamic alpha** for blending the margins. Instead of being based on the raw standard deviation of costs (as in Parent 1), the new `alpha` is a sigmoid of the z-score of `delta_cost`. This `alpha = sigmoid(z_score(delta_cost))` makes the blend weight for each individual pair dependent on how much of an outlier its cost difference is relative to the batch. Pairs with typical cost differences will have `alpha` near 0.5, while pairs with extremely large or small cost differences will have `alpha` values pushed towards 0 or 1, dynamically emphasizing either the fixed-temp or adaptive-temp margin component for that specific pair.\n\nThe second new coupling is a **gating mechanism for the adaptive scale**. The `adaptive_scale` is now a hybrid of `std(delta_logp)` and `std(delta_cost)` (inspired by Parent 1's hybrid scale), but it is multiplied by a gate: `gate = tanh(std(delta_cost))`. This gate smoothly suppresses the overall margin magnitude when the cost differences in a batch are very small (low `std(delta_cost)`), preventing the model from overfitting to tiny, noisy cost gaps. As cost variance increases, the gate approaches 1, allowing the full adaptive margin to take effect. This makes the loss more robust to batches with low-signal cost information.", "pseudocode": "1. Calculate the difference in log-probabilities: delta_logp = logp(a) - logp(b).\n2. Calculate the difference in costs: delta_cost = cost(b) - cost(a).\n3. Compute batch statistics for delta_logp and delta_cost (mean and std dev).\n4. New Coupling 1: Calculate a per-pair dynamic alpha. First, compute the z-score of delta_cost: `z_cost = (delta_cost - mean(delta_cost)) / (std(delta_cost) + epsilon)`. Then, `alpha = sigmoid(z_cost)`.\n5. New Coupling 2: Compute a gated adaptive scale. First, calculate a cost-variance gate: `gate = tanh(std(delta_cost))`. Then, compute a hybrid scale: `hybrid_scale = w_logp * std(delta_logp) + w_cost * std(delta_cost)`. The final scale is `adaptive_scale = gate * hybrid_scale`, clipped for stability.\n6. Inherit from Parents: Compute an adaptive beta for the loss, inversely proportional to the adaptive scale: `beta = 1.0 / adaptive_scale`.\n7. Inherit from Parent 0: Calculate the two base margin components. `margin_A` uses a fixed temperature. `margin_B` uses an adaptive temperature based on `std(delta_cost)`.\n8. Combine the margin components using the per-pair dynamic alpha: `margin = alpha * margin_A + (1 - alpha) * margin_B`.\n9. Compute the final loss argument: `loss_arg = delta_logp - margin`.\n10. Calculate the final loss using the scaled negative log-sigmoid function: `loss = -logsigmoid(beta * loss_arg)`.\n11. Return the mean of the loss across the batch.", "hyperparams": {"fixed_temp": 1.0, "w_logp": 0.5, "w_cost": 0.5, "min_scale": 0.1, "max_scale": 5.0, "epsilon": 1e-08}, "operators_used": ["logsigmoid", "sigmoid", "tanh", "clamp"], "implementation_hint": {"expects": ["cost_a", "cost_b", "logp_a", "logp_b"], "returns": "scalar"}, "code": "def generated_loss(batch, model_output, extra):\n    \"\"\"\n    A logistic loss with a z-score gated, hybrid adaptive margin.\n    \"\"\"\n    # Read tensors from the batch\n    cost_a = batch['cost_a']\n    cost_b = batch['cost_b']\n    logp_a = batch['log_prob_w']\n    logp_b = batch['log_prob_l']\n    weights = batch.get('weight')\n\n    # Read hyperparameters\n    hyperparams = extra['hyperparams']\n    fixed_temp = hyperparams.get('fixed_temp', 1.0)\n    w_logp = hyperparams.get('w_logp', 0.5)\n    w_cost = hyperparams.get('w_cost', 0.5)\n    min_scale = hyperparams.get('min_scale', 0.1)\n    max_scale = hyperparams.get('max_scale', 5.0)\n    epsilon = hyperparams.get('epsilon', 1e-8)\n\n    # 1. Calculate log-probability and cost differences\n    delta_logp = logp_a - logp_b\n    delta_cost = cost_b - cost_a\n\n    # 2. Compute batch statistics (detached for stability)\n    if delta_logp.numel() > 1:\n        logp_std = delta_logp.detach().std()\n        cost_std = delta_cost.detach().std()\n        cost_mean = delta_cost.detach().mean()\n    else:\n        logp_std = torch.tensor(1.0, device=delta_logp.device)\n        cost_std = torch.tensor(1.0, device=delta_cost.device)\n        cost_mean = torch.tensor(0.0, device=delta_cost.device)\n\n    # 3. New Coupling 1: Per-pair dynamic alpha from z-score\n    z_cost = (delta_cost - cost_mean) / (cost_std + epsilon)\n    alpha = torch.sigmoid(z_cost) # Per-pair alpha\n\n    # 4. New Coupling 2: Gated adaptive scale\n    cost_variance_gate = torch.tanh(cost_std)\n    hybrid_scale_base = w_logp * logp_std + w_cost * cost_std\n    adaptive_scale = torch.clamp(cost_variance_gate * hybrid_scale_base, min_scale, max_scale)\n\n    # 5. Inherit from Parents: Adaptive beta from scale\n    beta = 1.0 / (adaptive_scale.detach() + epsilon)\n\n    # 6. Inherit from Parent 0: Margin components\n    # margin_A with fixed temperature\n    cost_weight_A = torch.sigmoid(delta_cost / fixed_temp)\n    margin_A = adaptive_scale * cost_weight_A\n    \n    # margin_B with adaptive temperature\n    adaptive_cost_temp = cost_std + epsilon\n    cost_weight_B = torch.sigmoid(delta_cost / adaptive_cost_temp)\n    margin_B = adaptive_scale * cost_weight_B\n\n    # 7. Blend margin with per-pair alpha\n    margin = alpha * margin_A + (1.0 - alpha) * margin_B\n\n    # 8. Compute the loss argument\n    loss_arg = delta_logp - margin\n\n    # 9. Apply negative log-sigmoid loss\n    loss_per_pair = -torch.nn.functional.logsigmoid(beta * loss_arg)\n\n    # 10. Apply optional weights and compute the mean loss\n    if weights is not None:\n        loss = (loss_per_pair * weights).sum() / weights.sum().clamp(min=epsilon)\n    else:\n        loss = loss_per_pair.mean()\n\n    return loss", "theoretical_basis": "A Bradley-Terry logistic preference model where the target margin is a per-pair dynamic blend of two adaptive components. The blending factor is determined by the z-score of the cost difference, making the margin composition sensitive to individual preference difficulty. The overall magnitude of the margin and the loss temperature are co-regulated by a hybrid measure of batch variance, which is itself gated by the diversity of costs in the batch, preventing overfitting on low-signal pairs."}, "fitness": {"hf_like_score": 8.960275241088867, "validation_objective": 8.960275241088867, "generalization_penalty": 0.0, "generalization_objectives": {"100": 8.957302421569825}, "train_score_mean": 15.810362005233765, "train_loss_mean": 0.8448382371664047, "pair_count": 31679980, "early_eval": {"enabled": true, "steps": 100, "baseline_validation_objective": 8.412976063537597, "candidate_validation_objective": 8.960275241088867, "early_stopped": true}, "phases": {"f1": {"steps": 15630, "train_score_mean": 15.810362005233765, "train_loss_mean": 0.8448382371664047, "pair_count": 31679980}, "f2": {"steps": 0, "train_score_mean": null, "train_loss_mean": null, "pair_count": 0}}, "config": {"hf": {"problem": "tsp", "hf_steps": 0, "hf_epochs": 10, "hf_instances_per_epoch": 100000, "train_problem_size": 100, "valid_problem_sizes": [100], "train_batch_size": 64, "pomo_size": 100, "learning_rate": 0.0003, "weight_decay": 1e-06, "alpha": 0.05, "device": "cuda:3", "seed": 1234, "num_validation_episodes": 10000, "validation_batch_size": 64, "generalization_penalty_weight": 1.0, "pool_version": "v0"}, "free_loss": {"f1_steps": 0, "total_train_steps": 15630, "f2_steps": 0, "f3_enabled": false}}, "loss_ir": {"name": "ZScore_Gated_Hybrid_Margin_LogSigmoid", "intuition": "Mode: combine. This loss function builds upon the successful hybrid adaptive margin framework of the parents, introducing new stability and responsiveness mechanisms. \n\nIt inherits two key ideas: \n1.  From Parent 1, it inherits the blended margin structure (`alpha * margin_A + (1-alpha) * margin_B`), where `margin_A` uses a fixed temperature and `margin_B` uses a batch-adaptive temperature. This provides a robust, dual-sensitivity margin.\n2.  From both parents, it inherits the core idea of an adaptive loss temperature `beta = 1.0 / adaptive_scale`, which is inversely proportional to a measure of batch difficulty, and the use of `logsigmoid` as the base loss.\n\nThe first new coupling idea is a **z-score based dynamic alpha** for blending the margins. Instead of being based on the raw standard deviation of costs (as in Parent 1), the new `alpha` is a sigmoid of the z-score of `delta_cost`. This `alpha = sigmoid(z_score(delta_cost))` makes the blend weight for each individual pair dependent on how much of an outlier its cost difference is relative to the batch. Pairs with typical cost differences will have `alpha` near 0.5, while pairs with extremely large or small cost differences will have `alpha` values pushed towards 0 or 1, dynamically emphasizing either the fixed-temp or adaptive-temp margin component for that specific pair.\n\nThe second new coupling is a **gating mechanism for the adaptive scale**. The `adaptive_scale` is now a hybrid of `std(delta_logp)` and `std(delta_cost)` (inspired by Parent 1's hybrid scale), but it is multiplied by a gate: `gate = tanh(std(delta_cost))`. This gate smoothly suppresses the overall margin magnitude when the cost differences in a batch are very small (low `std(delta_cost)`), preventing the model from overfitting to tiny, noisy cost gaps. As cost variance increases, the gate approaches 1, allowing the full adaptive margin to take effect. This makes the loss more robust to batches with low-signal cost information.", "hyperparams": {"fixed_temp": 1.0, "w_logp": 0.5, "w_cost": 0.5, "min_scale": 0.1, "max_scale": 5.0, "epsilon": 1e-08}, "operators_used": ["logsigmoid", "sigmoid", "tanh", "clamp"]}}, "better_than_baseline": false}
{"generation": 9, "index": 0, "ir": {"name": "Gated_Hybrid_Adaptive_Loss", "intuition": "Mode: combine. This loss function refines the successful hybrid adaptive strategies of its parents by introducing a gating mechanism to control the influence of different adaptive components. It inherits the core `logsigmoid(beta * (delta_logp - margin))` structure and the concept of a blended margin from both parents. From Parent 1, it takes the idea of a dual-component margin, and from Parent 0, the coupling of `beta` to an adaptive scale derived from `std(delta_logp)`. From Parent 1, it also inherits the 'hybrid' adaptive scale, which is sensitive to both `std(delta_logp)` and `std(delta_cost)`.\n\nThe first new coupling idea is a **gating mechanism for the adaptive scale**. Instead of a simple weighted sum of `std(delta_logp)` and `std(delta_cost)` (as in Parent 1), the contribution of `std(delta_cost)` is now gated. The gate is a `tanh` function of the normalized cost variance: `gate = tanh(std(delta_cost) / mean(delta_cost))`. The scale becomes `scale = std(logp) + w_cost * gate * std(cost)`. This means the cost variance only influences the margin's magnitude and the loss's temperature (`beta`) when the cost distribution is highly variable relative to its mean, preventing it from having an undue effect on batches with uniform costs. The second new coupling is a **stabilized dynamic alpha** for blending the margin components. Parent 1's dynamic alpha `sigmoid((std-mean)/temp)` could be unstable. This child uses a more stable formulation: `alpha = 0.5 * (1 + tanh(z_score(delta_cost)))`, where `z_score` normalizes the cost difference. This smoothly transitions the margin blend from 0 to 1 based on how many standard deviations a pair's cost gap is from the batch mean, making the blend adaptive on a per-pair basis rather than a per-batch basis.", "pseudocode": "1. Calculate the difference in log-probabilities: delta_logp = logp(a) - logp(b).\n2. Calculate the difference in costs: delta_cost = cost(b) - cost(a).\n3. Calculate batch statistics (mean and std) for delta_logp and delta_cost.\n4. Inherit from Parent 1: Calculate the base for the hybrid adaptive scale using `std(delta_logp)`.\n5. New Coupling 1: Compute a cost variance gate: `gate = tanh(std(delta_cost) / (mean(delta_cost) + epsilon))`. Apply this gate to the cost variance contribution to the scale: `adaptive_scale = std(delta_logp) + w_cost * gate * std(delta_cost)`. Clip for stability.\n6. Inherit from Parent 0: Compute an adaptive beta for the loss, inversely proportional to the new gated adaptive scale: `beta = 1.0 / adaptive_scale`.\n7. Inherit from Parent 1: Calculate the two base margin components. `margin_A` uses a fixed temperature. `margin_B` uses an adaptive temperature `std(delta_cost)`.\n8. New Coupling 2: Compute a stabilized, per-pair dynamic alpha. Z-score the `delta_cost` tensor. `alpha = 0.5 * (1 + tanh(z_scored_delta_cost))`. This makes alpha a tensor of the same shape as delta_cost.\n9. Inherit from Parent 1: Combine the margin components using the per-pair dynamic alpha: `margin = alpha * margin_A + (1 - alpha) * margin_B`.\n10. Compute the final loss argument: `loss_arg = delta_logp - margin`.\n11. Calculate the final loss using the scaled negative log-sigmoid function: `loss = -logsigmoid(beta * loss_arg)`.\n12. Return the mean of the loss across the batch.", "hyperparams": {"fixed_temp": 1.0, "w_cost": 0.5, "min_scale": 0.1, "max_scale": 5.0, "epsilon": 1e-08}, "operators_used": ["logsigmoid", "sigmoid", "tanh", "clamp"], "implementation_hint": {"expects": ["cost_a", "cost_b", "logp_a", "logp_b"], "returns": "scalar"}, "code": "def generated_loss(batch, model_output, extra):\n    \"\"\"\n    A logistic loss with a gated hybrid adaptive scale and a per-pair dynamic margin blend.\n    \"\"\"\n    # Read tensors from the batch\n    cost_a = batch['cost_a']\n    cost_b = batch['cost_b']\n    logp_a = batch['log_prob_w']\n    logp_b = batch['log_prob_l']\n    weights = batch.get('weight')\n\n    # Read hyperparameters\n    hyperparams = extra['hyperparams']\n    fixed_temp = hyperparams.get('fixed_temp', 1.0)\n    w_cost = hyperparams.get('w_cost', 0.5)\n    min_scale = hyperparams.get('min_scale', 0.1)\n    max_scale = hyperparams.get('max_scale', 5.0)\n    epsilon = hyperparams.get('epsilon', 1e-8)\n\n    # 1. Calculate log-probability and cost differences\n    delta_logp = logp_a - logp_b\n    delta_cost = cost_b - cost_a\n\n    # 2. Compute batch statistics (detached for stability)\n    if delta_logp.numel() > 1:\n        logp_std = delta_logp.detach().std()\n        cost_std = delta_cost.detach().std()\n        cost_mean = delta_cost.detach().mean()\n    else:\n        logp_std = torch.tensor(1.0, device=delta_logp.device)\n        cost_std = torch.tensor(1.0, device=delta_cost.device)\n        cost_mean = torch.tensor(0.0, device=delta_cost.device)\n\n    # 3. New Coupling 1: Gated hybrid adaptive scale\n    cost_variance_gate = torch.tanh(cost_std / (torch.abs(cost_mean) + epsilon))\n    hybrid_scale = logp_std + w_cost * cost_variance_gate * cost_std\n    adaptive_scale = torch.clamp(hybrid_scale, min_scale, max_scale)\n\n    # 4. Inherit from Parent 0: Adaptive beta from scale\n    beta = 1.0 / (adaptive_scale.detach() + epsilon)\n\n    # 5. Inherit from Parent 1: Margin components\n    # margin_A with fixed temperature\n    cost_weight_A = torch.sigmoid(delta_cost / fixed_temp)\n    margin_A = adaptive_scale * cost_weight_A\n    \n    # margin_B with adaptive temperature\n    adaptive_cost_temp = cost_std + epsilon\n    cost_weight_B = torch.sigmoid(delta_cost / adaptive_cost_temp)\n    margin_B = adaptive_scale * cost_weight_B\n\n    # 6. New Coupling 2: Per-pair dynamic alpha via z-scoring\n    z_scored_delta_cost = (delta_cost - cost_mean) / (cost_std + epsilon)\n    alpha = 0.5 * (1.0 + torch.tanh(z_scored_delta_cost))\n\n    # 7. Inherit from Parent 1: Blended margin with per-pair dynamic alpha\n    margin = alpha * margin_A + (1.0 - alpha) * margin_B\n\n    # 8. Compute the loss argument\n    loss_arg = delta_logp - margin\n\n    # 9. Apply negative log-sigmoid loss\n    loss_per_pair = -torch.nn.functional.logsigmoid(beta * loss_arg)\n\n    # 10. Apply optional weights and compute the mean loss\n    if weights is not None:\n        loss = (loss_per_pair * weights).sum() / weights.sum().clamp(min=epsilon)\n    else:\n        loss = loss_per_pair.mean()\n\n    return loss", "theoretical_basis": "A Bradley-Terry logistic preference model with a per-pair, dynamically blended margin. The loss temperature and margin scale are coupled to a gated measure of batch difficulty, which adaptively incorporates cost variance only when it is significant. The margin's composition is determined for each pair by its cost difference relative to the batch distribution, allowing for fine-grained, instance-specific learning targets."}, "fitness": {"hf_like_score": 7.8724992515563965, "validation_objective": 7.872013209533692, "generalization_penalty": 0.00048604202270485786, "generalization_objectives": {"100": 7.8724992515563965}, "train_score_mean": 8.609249454054098, "train_loss_mean": 0.5408047505852814, "pair_count": 4951578982, "early_eval": {"enabled": true, "steps": 100, "baseline_validation_objective": 8.412976063537597, "candidate_validation_objective": 8.32618621673584, "early_stopped": false}, "phases": {"f1": {"steps": 15630, "train_score_mean": 8.609249454054098, "train_loss_mean": 0.5408047505852814, "pair_count": 4951578982}, "f2": {"steps": 0, "train_score_mean": null, "train_loss_mean": null, "pair_count": 0}}, "config": {"hf": {"problem": "tsp", "hf_steps": 0, "hf_epochs": 10, "hf_instances_per_epoch": 100000, "train_problem_size": 100, "valid_problem_sizes": [100], "train_batch_size": 64, "pomo_size": 100, "learning_rate": 0.0003, "weight_decay": 1e-06, "alpha": 0.05, "device": "cuda:0", "seed": 1234, "num_validation_episodes": 10000, "validation_batch_size": 64, "generalization_penalty_weight": 1.0, "pool_version": "v0"}, "free_loss": {"f1_steps": 0, "total_train_steps": 15630, "f2_steps": 0, "f3_enabled": false}}, "loss_ir": {"name": "Gated_Hybrid_Adaptive_Loss", "intuition": "Mode: combine. This loss function refines the successful hybrid adaptive strategies of its parents by introducing a gating mechanism to control the influence of different adaptive components. It inherits the core `logsigmoid(beta * (delta_logp - margin))` structure and the concept of a blended margin from both parents. From Parent 1, it takes the idea of a dual-component margin, and from Parent 0, the coupling of `beta` to an adaptive scale derived from `std(delta_logp)`. From Parent 1, it also inherits the 'hybrid' adaptive scale, which is sensitive to both `std(delta_logp)` and `std(delta_cost)`.\n\nThe first new coupling idea is a **gating mechanism for the adaptive scale**. Instead of a simple weighted sum of `std(delta_logp)` and `std(delta_cost)` (as in Parent 1), the contribution of `std(delta_cost)` is now gated. The gate is a `tanh` function of the normalized cost variance: `gate = tanh(std(delta_cost) / mean(delta_cost))`. The scale becomes `scale = std(logp) + w_cost * gate * std(cost)`. This means the cost variance only influences the margin's magnitude and the loss's temperature (`beta`) when the cost distribution is highly variable relative to its mean, preventing it from having an undue effect on batches with uniform costs. The second new coupling is a **stabilized dynamic alpha** for blending the margin components. Parent 1's dynamic alpha `sigmoid((std-mean)/temp)` could be unstable. This child uses a more stable formulation: `alpha = 0.5 * (1 + tanh(z_score(delta_cost)))`, where `z_score` normalizes the cost difference. This smoothly transitions the margin blend from 0 to 1 based on how many standard deviations a pair's cost gap is from the batch mean, making the blend adaptive on a per-pair basis rather than a per-batch basis.", "hyperparams": {"fixed_temp": 1.0, "w_cost": 0.5, "min_scale": 0.1, "max_scale": 5.0, "epsilon": 1e-08}, "operators_used": ["logsigmoid", "sigmoid", "tanh", "clamp"]}}, "better_than_baseline": true}
{"generation": 9, "index": 2, "ir": {"name": "Rank_Weighted_Hybrid_Adaptive_Loss", "intuition": "Mode: combine. This loss function advances the adaptive margin concept by introducing rank-based weighting, aiming to focus the learning signal on the most informative pairs within a batch. It inherits the core `logsigmoid(beta * (delta_logp - margin))` structure and the hybrid adaptive scale (`adaptive_scale = w_logp * std(delta_logp) + w_cost * std(delta_cost)`) from Parent 1. This ensures the loss's sensitivity and the margin's magnitude are responsive to variance in both model outputs and costs. It also inherits the dual-component blended margin (`alpha * margin_A + (1-alpha) * margin_B`) from Parent 0, which combines fixed and adaptive temperature cost sensitivities. \n\nThe first new coupling idea is a **rank-based re-weighting of the margin**. Instead of applying the same margin to all pairs, the margin's magnitude is modulated by the rank of the cost difference (`delta_cost`). Specifically, `margin = margin * rank_weights`, where `rank_weights` is a tensor of weights (e.g., from 0.5 to 1.5) based on the percentile rank of `delta_cost` in the batch. This causes the model to push for a larger `delta_logp` on pairs with larger cost gaps (more certain preferences) and a smaller one for pairs with small cost gaps (less certain preferences), effectively prioritizing the clearest signals. The second new coupling is a **dynamic alpha based on logp uncertainty**. The blending factor `alpha` for the two margin components is now `alpha = sigmoid(logp_std / logp_temp)`. When model uncertainty (`logp_std`) is high, `alpha` increases, favoring the more stable `margin_A` (with a fixed temperature). When uncertainty is low, `alpha` decreases, allowing the more dynamic `margin_B` (with adaptive temperature) to have more influence. This creates a self-regulating mechanism that prioritizes stability when the model is uncertain and dynamism when it is confident.", "pseudocode": "1. Calculate the difference in log-probabilities: delta_logp = logp(a) - logp(b).\n2. Calculate the difference in costs: delta_cost = cost(b) - cost(a).\n3. Compute batch statistics: std(delta_logp) and std(delta_cost).\n4. Inherit from Parent 1: Compute a hybrid adaptive scale: `adaptive_scale = w_logp * std(delta_logp) + w_cost * std(delta_cost)`, clipped for stability.\n5. Inherit from Parents: The loss temperature `beta` is inversely proportional to the scale: `beta = 1.0 / adaptive_scale`.\n6. Inherit from Parent 0: Calculate the two base margin components. `margin_A` uses a fixed temperature. `margin_B` uses an adaptive temperature based on `std(delta_cost)`.\n7. New Coupling 1: Compute rank-based weights. Calculate the percentile rank of each `delta_cost` in the batch. Map these ranks to a weight range (e.g., [0.5, 1.5]).\n8. New Coupling 2: Compute a dynamic blending factor `alpha` based on model uncertainty: `alpha = sigmoid(std(delta_logp) / logp_temp)`.\n9. Combine the margin components using the dynamic alpha: `blended_margin_base = alpha * margin_A + (1 - alpha) * margin_B`.\n10. Modulate the blended margin with the rank weights: `final_margin = blended_margin_base * rank_weights`.\n11. Compute the loss argument: `loss_arg = delta_logp - final_margin`.\n12. Calculate the final loss: `loss = -logsigmoid(beta * loss_arg)`.\n13. Return the mean of the loss across the batch.", "hyperparams": {"fixed_temp": 1.0, "logp_temp": 1.0, "w_logp": 0.5, "w_cost": 0.5, "min_scale": 0.1, "max_scale": 5.0, "epsilon": 1e-08, "min_rank_weight": 0.5, "max_rank_weight": 1.5}, "operators_used": ["logsigmoid", "sigmoid", "clamp"], "implementation_hint": {"expects": ["cost_a", "cost_b", "logp_a", "logp_b"], "returns": "scalar"}, "code": "def generated_loss(batch, model_output, extra):\n    \"\"\"\n    A logistic loss with a hybrid adaptive margin that is modulated by the rank of cost differences.\n    \"\"\"\n    # Read tensors from the batch\n    cost_a = batch['cost_a']\n    cost_b = batch['cost_b']\n    logp_a = batch['log_prob_w']\n    logp_b = batch['log_prob_l']\n    weights = batch.get('weight')\n\n    # Read hyperparameters\n    hyperparams = extra['hyperparams']\n    fixed_temp = hyperparams.get('fixed_temp', 1.0)\n    logp_temp = hyperparams.get('logp_temp', 1.0)\n    w_logp = hyperparams.get('w_logp', 0.5)\n    w_cost = hyperparams.get('w_cost', 0.5)\n    min_scale = hyperparams.get('min_scale', 0.1)\n    max_scale = hyperparams.get('max_scale', 5.0)\n    epsilon = hyperparams.get('epsilon', 1e-8)\n    min_rank_weight = hyperparams.get('min_rank_weight', 0.5)\n    max_rank_weight = hyperparams.get('max_rank_weight', 1.5)\n\n    # 1. Calculate log-probability and cost differences\n    delta_logp = logp_a - logp_b\n    delta_cost = cost_b - cost_a\n\n    # 2. Compute batch statistics (detached for stability)\n    n_pairs = delta_logp.numel()\n    if n_pairs > 1:\n        logp_std = delta_logp.detach().std()\n        cost_std = delta_cost.detach().std()\n    else:\n        logp_std = torch.tensor(1.0, device=delta_logp.device)\n        cost_std = torch.tensor(1.0, device=delta_cost.device)\n\n    # 3. Inherit from Parent 1: Hybrid adaptive scale\n    hybrid_scale = w_logp * logp_std + w_cost * cost_std\n    adaptive_scale = torch.clamp(hybrid_scale, min_scale, max_scale)\n\n    # 4. Inherit from Parents: Adaptive beta from scale\n    beta = 1.0 / (adaptive_scale.detach() + epsilon)\n\n    # 5. Inherit from Parent 0: Margin components\n    cost_weight_A = torch.sigmoid(delta_cost / fixed_temp)\n    margin_A = adaptive_scale * cost_weight_A\n    \n    adaptive_cost_temp = cost_std + epsilon\n    cost_weight_B = torch.sigmoid(delta_cost / adaptive_cost_temp)\n    margin_B = adaptive_scale * cost_weight_B\n\n    # 6. New Coupling 1: Dynamic alpha for blending based on logp uncertainty\n    alpha = torch.sigmoid(logp_std / logp_temp)\n\n    # 7. Blend the two margins\n    blended_margin_base = alpha * margin_A + (1.0 - alpha) * margin_B\n\n    # 8. New Coupling 2: Rank-based margin modulation\n    if n_pairs > 1:\n        # Compute percentile ranks of delta_cost\n        ranks = delta_cost.argsort().argsort().float() / (n_pairs - 1)\n        # Map ranks to the desired weight range\n        rank_weights = min_rank_weight + (max_rank_weight - min_rank_weight) * ranks\n        final_margin = blended_margin_base * rank_weights.detach()\n    else:\n        final_margin = blended_margin_base\n\n    # 9. Compute the loss argument\n    loss_arg = delta_logp - final_margin\n\n    # 10. Apply negative log-sigmoid loss\n    loss_per_pair = -torch.nn.functional.logsigmoid(beta * loss_arg)\n\n    # 11. Apply optional weights and compute the mean loss\n    if weights is not None:\n        loss = (loss_per_pair * weights).sum() / weights.sum().clamp(min=epsilon)\n    else:\n        loss = loss_per_pair.mean()\n\n    return loss", "theoretical_basis": "A Bradley-Terry logistic preference model where the target log-odds is a dynamically constructed, rank-sensitive margin. The margin's base value is a blend of two adaptive components, with the blend ratio itself adapting to model uncertainty. This base margin is then modulated by the rank of the cost difference within the batch, focusing the learning signal on pairs with more significant preference gaps. The overall loss temperature is coupled to a hybrid measure of batch difficulty."}, "fitness": {"hf_like_score": 7.878991627502441, "validation_objective": 7.8775828857421875, "generalization_penalty": 0.00140874176025374, "generalization_objectives": {"100": 7.878991627502441}, "train_score_mean": 8.693227144120522, "train_loss_mean": 0.46680149707516566, "pair_count": 4951579275, "early_eval": {"enabled": true, "steps": 100, "baseline_validation_objective": 8.412976063537597, "candidate_validation_objective": 8.32407647857666, "early_stopped": false}, "phases": {"f1": {"steps": 15630, "train_score_mean": 8.693227144120522, "train_loss_mean": 0.46680149707516566, "pair_count": 4951579275}, "f2": {"steps": 0, "train_score_mean": null, "train_loss_mean": null, "pair_count": 0}}, "config": {"hf": {"problem": "tsp", "hf_steps": 0, "hf_epochs": 10, "hf_instances_per_epoch": 100000, "train_problem_size": 100, "valid_problem_sizes": [100], "train_batch_size": 64, "pomo_size": 100, "learning_rate": 0.0003, "weight_decay": 1e-06, "alpha": 0.05, "device": "cuda:1", "seed": 1234, "num_validation_episodes": 10000, "validation_batch_size": 64, "generalization_penalty_weight": 1.0, "pool_version": "v0"}, "free_loss": {"f1_steps": 0, "total_train_steps": 15630, "f2_steps": 0, "f3_enabled": false}}, "loss_ir": {"name": "Rank_Weighted_Hybrid_Adaptive_Loss", "intuition": "Mode: combine. This loss function advances the adaptive margin concept by introducing rank-based weighting, aiming to focus the learning signal on the most informative pairs within a batch. It inherits the core `logsigmoid(beta * (delta_logp - margin))` structure and the hybrid adaptive scale (`adaptive_scale = w_logp * std(delta_logp) + w_cost * std(delta_cost)`) from Parent 1. This ensures the loss's sensitivity and the margin's magnitude are responsive to variance in both model outputs and costs. It also inherits the dual-component blended margin (`alpha * margin_A + (1-alpha) * margin_B`) from Parent 0, which combines fixed and adaptive temperature cost sensitivities. \n\nThe first new coupling idea is a **rank-based re-weighting of the margin**. Instead of applying the same margin to all pairs, the margin's magnitude is modulated by the rank of the cost difference (`delta_cost`). Specifically, `margin = margin * rank_weights`, where `rank_weights` is a tensor of weights (e.g., from 0.5 to 1.5) based on the percentile rank of `delta_cost` in the batch. This causes the model to push for a larger `delta_logp` on pairs with larger cost gaps (more certain preferences) and a smaller one for pairs with small cost gaps (less certain preferences), effectively prioritizing the clearest signals. The second new coupling is a **dynamic alpha based on logp uncertainty**. The blending factor `alpha` for the two margin components is now `alpha = sigmoid(logp_std / logp_temp)`. When model uncertainty (`logp_std`) is high, `alpha` increases, favoring the more stable `margin_A` (with a fixed temperature). When uncertainty is low, `alpha` decreases, allowing the more dynamic `margin_B` (with adaptive temperature) to have more influence. This creates a self-regulating mechanism that prioritizes stability when the model is uncertain and dynamism when it is confident.", "hyperparams": {"fixed_temp": 1.0, "logp_temp": 1.0, "w_logp": 0.5, "w_cost": 0.5, "min_scale": 0.1, "max_scale": 5.0, "epsilon": 1e-08, "min_rank_weight": 0.5, "max_rank_weight": 1.5}, "operators_used": ["logsigmoid", "sigmoid", "clamp"]}}, "better_than_baseline": true}
{"generation": 9, "index": 4, "ir": {"name": "Progressive_Hybrid_Scale_LogSigmoid", "intuition": "Mode: combine. This loss function refines the successful hybrid adaptive strategies of its parents by introducing a progressive, time-aware scaling mechanism. It inherits the core `logsigmoid(beta * (delta_logp - margin))` structure from both parents. From Parent 1, it inherits the blended margin concept (`alpha * margin_A + (1-alpha) * margin_B`), which provides a robust way to balance fixed and adaptive cost sensitivities. From Parent 0, it inherits the inverse coupling between the loss temperature `beta` and an `adaptive_scale`, which is a powerful stability mechanism.\n\nThe first new coupling idea is a **progressive adaptive scale**. Instead of calculating the `adaptive_scale` from scratch on each batch using a simple weighted sum of standard deviations (as in Parent 1), this child maintains an Exponential Moving Average (EMA) of `logp_std` and `cost_std`. The `adaptive_scale` is then computed from these smoothed statistics: `adaptive_scale = w_logp * ema_logp_std + w_cost * ema_cost_std`. This makes the scaling factor (and thus the margin magnitude and loss temperature) smoother and more stable over time, less susceptible to noise from individual batches, and more representative of the model's recent learning trajectory. The second new coupling is a **normalized dynamic alpha**. Parent 1's dynamic alpha `sigmoid((std - mean) / temp)` can be sensitive to the absolute scale of costs. This child uses a normalized version: `alpha = sigmoid((cost_std - cost_mean) / (cost_std + epsilon))`. This ratio, known as the coefficient of variation, is a dimensionless measure of dispersion. It makes the alpha blend factor invariant to the scale of costs and more reliably indicates when to trust the adaptive-temperature margin (high relative variance) versus the fixed-temperature one (low relative variance).", "pseudocode": "1. Calculate the difference in log-probabilities: delta_logp = logp(a) - logp(b).\n2. Calculate the difference in costs: delta_cost = cost(b) - cost(a).\n3. Compute standard deviations of delta_logp and delta_cost for the current batch.\n4. New Coupling 1: Update Exponential Moving Averages (EMAs) of logp_std and cost_std. These are stored in the `extra` dictionary to persist across batches.\n5. Inherit from Parent 1: Compute a progressive adaptive scale using the EMA-smoothed standard deviations: `adaptive_scale = w_logp * ema_logp_std + w_cost * ema_cost_std`. Clip for stability.\n6. Inherit from Parent 0: Compute an adaptive beta for the loss, inversely proportional to the progressive adaptive scale: `beta = 1.0 / adaptive_scale`.\n7. Inherit from Parent 1: Calculate two base margin components. `margin_A` uses a fixed temperature. `margin_B` uses an adaptive temperature based on the batch's `cost_std`.\n8. New Coupling 2: Compute a normalized dynamic blending factor `alpha`. This is calculated as `sigmoid((cost_std - cost_mean) / (cost_std + epsilon))`, making the blend robust to cost scaling.\n9. Inherit from Parent 1: Combine the two margin components using the dynamic alpha: `margin = alpha * margin_A + (1 - alpha) * margin_B`.\n10. Compute the final loss argument: `loss_arg = delta_logp - margin`.\n11. Calculate the final loss using the scaled negative log-sigmoid function: `loss = -logsigmoid(beta * loss_arg)`.\n12. Return the mean of the loss across the batch.", "hyperparams": {"fixed_temp": 1.0, "w_logp": 0.5, "w_cost": 0.5, "min_scale": 0.1, "max_scale": 5.0, "ema_decay": 0.99, "epsilon": 1e-08}, "operators_used": ["logsigmoid", "sigmoid", "clamp"], "implementation_hint": {"expects": ["cost_a", "cost_b", "logp_a", "logp_b"], "returns": "scalar"}, "code": "def generated_loss(batch, model_output, extra):\n    \"\"\"\n    A logistic loss with a progressive, EMA-smoothed hybrid scale and a normalized dynamic margin blend.\n    \"\"\"\n    # Read tensors from the batch\n    cost_a = batch['cost_a']\n    cost_b = batch['cost_b']\n    logp_a = batch['log_prob_w']\n    logp_b = batch['log_prob_l']\n    weights = batch.get('weight')\n\n    # Read hyperparameters\n    hyperparams = extra['hyperparams']\n    fixed_temp = hyperparams.get('fixed_temp', 1.0)\n    w_logp = hyperparams.get('w_logp', 0.5)\n    w_cost = hyperparams.get('w_cost', 0.5)\n    min_scale = hyperparams.get('min_scale', 0.1)\n    max_scale = hyperparams.get('max_scale', 5.0)\n    ema_decay = hyperparams.get('ema_decay', 0.99)\n    epsilon = hyperparams.get('epsilon', 1e-8)\n\n    # 1. Calculate log-probability and cost differences\n    delta_logp = logp_a - logp_b\n    delta_cost = cost_b - cost_a\n\n    # 2. Compute batch statistics (detached for stability)\n    if delta_logp.numel() > 1:\n        logp_std = delta_logp.detach().std()\n        cost_std = delta_cost.detach().std()\n        cost_mean = delta_cost.detach().mean()\n    else:\n        logp_std = torch.tensor(1.0, device=delta_logp.device)\n        cost_std = torch.tensor(1.0, device=delta_cost.device)\n        cost_mean = torch.tensor(0.0, device=delta_cost.device)\n\n    # 3. New Coupling 1: Progressive adaptive scale via EMA\n    if 'ema_logp_std' not in extra:\n        extra['ema_logp_std'] = logp_std\n        extra['ema_cost_std'] = cost_std\n    else:\n        extra['ema_logp_std'] = ema_decay * extra['ema_logp_std'] + (1 - ema_decay) * logp_std\n        extra['ema_cost_std'] = ema_decay * extra['ema_cost_std'] + (1 - ema_decay) * cost_std\n    \n    progressive_scale = w_logp * extra['ema_logp_std'] + w_cost * extra['ema_cost_std']\n    adaptive_scale = torch.clamp(progressive_scale, min_scale, max_scale)\n\n    # 4. Inherit from Parent 0: Adaptive beta from scale\n    beta = 1.0 / (adaptive_scale.detach() + epsilon)\n\n    # 5. Inherit from Parent 1: Margin components\n    cost_weight_A = torch.sigmoid(delta_cost / fixed_temp)\n    margin_A = adaptive_scale * cost_weight_A\n    \n    adaptive_cost_temp = cost_std + epsilon\n    cost_weight_B = torch.sigmoid(delta_cost / adaptive_cost_temp)\n    margin_B = adaptive_scale * cost_weight_B\n\n    # 6. New Coupling 2: Normalized dynamic alpha (coefficient of variation)\n    alpha = torch.sigmoid((cost_std - cost_mean) / (cost_std + epsilon))\n\n    # 7. Inherit from Parent 1: Blended margin with dynamic alpha\n    margin = alpha * margin_A + (1.0 - alpha) * margin_B\n\n    # 8. Compute the loss argument\n    loss_arg = delta_logp - margin\n\n    # 9. Apply negative log-sigmoid loss\n    loss_per_pair = -torch.nn.functional.logsigmoid(beta * loss_arg)\n\n    # 10. Apply optional weights and compute the mean loss\n    if weights is not None:\n        loss = (loss_per_pair * weights).sum() / weights.sum().clamp(min=epsilon)\n    else:\n        loss = loss_per_pair.mean()\n\n    return loss", "theoretical_basis": "A Bradley-Terry logistic preference model with a time-aware adaptive margin. The margin's scale and the loss temperature are coupled to an exponentially smoothed (progressive) measure of batch difficulty, providing temporal stability. The margin's internal structure dynamically adapts its sensitivity to cost differences based on a scale-invariant measure of cost dispersion within the batch, creating a robust, multi-layered, and temporally-aware learning objective."}, "fitness": {"hf_like_score": 7.877342292785644, "validation_objective": 7.876253685760498, "generalization_penalty": 0.0010886070251459046, "generalization_objectives": {"100": 7.877342292785644}, "train_score_mean": 8.66997357433756, "train_loss_mean": 0.515651078637563, "pair_count": 4951579133, "early_eval": {"enabled": true, "steps": 100, "baseline_validation_objective": 8.412976063537597, "candidate_validation_objective": 8.33961022644043, "early_stopped": false}, "phases": {"f1": {"steps": 15630, "train_score_mean": 8.66997357433756, "train_loss_mean": 0.515651078637563, "pair_count": 4951579133}, "f2": {"steps": 0, "train_score_mean": null, "train_loss_mean": null, "pair_count": 0}}, "config": {"hf": {"problem": "tsp", "hf_steps": 0, "hf_epochs": 10, "hf_instances_per_epoch": 100000, "train_problem_size": 100, "valid_problem_sizes": [100], "train_batch_size": 64, "pomo_size": 100, "learning_rate": 0.0003, "weight_decay": 1e-06, "alpha": 0.05, "device": "cuda:2", "seed": 1234, "num_validation_episodes": 10000, "validation_batch_size": 64, "generalization_penalty_weight": 1.0, "pool_version": "v0"}, "free_loss": {"f1_steps": 0, "total_train_steps": 15630, "f2_steps": 0, "f3_enabled": false}}, "loss_ir": {"name": "Progressive_Hybrid_Scale_LogSigmoid", "intuition": "Mode: combine. This loss function refines the successful hybrid adaptive strategies of its parents by introducing a progressive, time-aware scaling mechanism. It inherits the core `logsigmoid(beta * (delta_logp - margin))` structure from both parents. From Parent 1, it inherits the blended margin concept (`alpha * margin_A + (1-alpha) * margin_B`), which provides a robust way to balance fixed and adaptive cost sensitivities. From Parent 0, it inherits the inverse coupling between the loss temperature `beta` and an `adaptive_scale`, which is a powerful stability mechanism.\n\nThe first new coupling idea is a **progressive adaptive scale**. Instead of calculating the `adaptive_scale` from scratch on each batch using a simple weighted sum of standard deviations (as in Parent 1), this child maintains an Exponential Moving Average (EMA) of `logp_std` and `cost_std`. The `adaptive_scale` is then computed from these smoothed statistics: `adaptive_scale = w_logp * ema_logp_std + w_cost * ema_cost_std`. This makes the scaling factor (and thus the margin magnitude and loss temperature) smoother and more stable over time, less susceptible to noise from individual batches, and more representative of the model's recent learning trajectory. The second new coupling is a **normalized dynamic alpha**. Parent 1's dynamic alpha `sigmoid((std - mean) / temp)` can be sensitive to the absolute scale of costs. This child uses a normalized version: `alpha = sigmoid((cost_std - cost_mean) / (cost_std + epsilon))`. This ratio, known as the coefficient of variation, is a dimensionless measure of dispersion. It makes the alpha blend factor invariant to the scale of costs and more reliably indicates when to trust the adaptive-temperature margin (high relative variance) versus the fixed-temperature one (low relative variance).", "hyperparams": {"fixed_temp": 1.0, "w_logp": 0.5, "w_cost": 0.5, "min_scale": 0.1, "max_scale": 5.0, "ema_decay": 0.99, "epsilon": 1e-08}, "operators_used": ["logsigmoid", "sigmoid", "clamp"]}}, "better_than_baseline": true}
{"generation": 9, "index": 5, "ir": {"name": "Rank_Modulated_Hybrid_Margin_Loss", "intuition": "Mode: combine. This loss function refines the adaptive margin concept from both parents by introducing a novel rank-based modulation, aiming to focus the model's learning on the most informative preference pairs. It inherits the core `logsigmoid(beta * (delta_logp - margin))` structure and the idea of a blended margin (`alpha * margin_A + (1-alpha) * margin_B`) from Parent 0. It also inherits the concept of a hybrid adaptive scale (`w_logp * std(delta_logp) + w_cost * std(delta_cost)`) for both the margin magnitude and the loss temperature (`beta`) from Parent 1, which holistically measures batch difficulty.\n\nThe first new coupling idea is **rank-based margin modulation**. Instead of applying the same margin calculation to all pairs, the margin is scaled down for pairs where the model's current preference (`delta_logp`) is already strongly aligned with the true preference (`delta_cost > 0`). This is achieved by multiplying the margin by `1.0 - sigmoid(delta_logp)`. This 'correctness factor' reduces the margin for 'easy' pairs, effectively telling the model 'you already know this one, don't push the log-probabilities further just to meet a large margin'. This prevents overfitting on easy pairs and focuses gradient updates on misclassified or uncertain pairs. The second new coupling is a **cost-gap-weighted alpha** for blending the two margin components. Instead of a fixed or complex statistical alpha, the blending factor is `alpha = sigmoid(delta_cost - cost_threshold)`. This smoothly transitions from favoring the fixed-temperature margin (for small cost gaps) to the adaptive-temperature margin (for large cost gaps), making the margin's structure directly responsive to the magnitude of the preference signal for each individual pair, rather than just batch-level statistics.", "pseudocode": "1. Calculate the difference in log-probabilities: delta_logp = logp(a) - logp(b).\n2. Calculate the difference in costs: delta_cost = cost(b) - cost(a).\n3. Inherit from Parent 1: Compute a hybrid adaptive scale based on a weighted sum of the standard deviations of delta_logp and delta_cost. Clip for stability: `adaptive_scale = clip(w_logp * std(delta_logp) + w_cost * std(delta_cost))`.\n4. Inherit from Parent 0/1: Compute an adaptive beta for the loss, inversely proportional to the hybrid adaptive scale: `beta = 1.0 / adaptive_scale`.\n5. Inherit from Parent 0: Calculate the two base margin components. `margin_A` uses a sigmoid of `delta_cost` with a fixed temperature. `margin_B` uses an adaptive temperature based on `std(delta_cost)`. Both are scaled by `adaptive_scale`.\n6. New Coupling 1 (Cost-Gap-Weighted Alpha): Compute a per-pair dynamic blending factor `alpha = sigmoid(delta_cost - cost_threshold)`. This gives more weight to the adaptive-temp margin for pairs with larger cost differences.\n7. Inherit from Parent 0: Combine the two margin components using the new dynamic alpha: `blended_margin = alpha * margin_B + (1 - alpha) * margin_A`.\n8. New Coupling 2 (Rank-Based Modulation): Compute a 'correctness factor' `correctness = 1.0 - sigmoid(delta_logp)`. Scale the blended margin by this factor: `final_margin = blended_margin * correctness.detach()`. This reduces the margin for pairs the model already ranks correctly, focusing on errors.\n9. Compute the final loss argument: `loss_arg = delta_logp - final_margin`.\n10. Calculate the final loss using the scaled negative log-sigmoid function: `loss = -logsigmoid(beta * loss_arg)`.\n11. Return the mean of the loss across the batch.", "hyperparams": {"fixed_temp": 1.0, "cost_threshold": 0.5, "w_logp": 0.5, "w_cost": 0.5, "min_scale": 0.1, "max_scale": 5.0, "epsilon": 1e-08}, "operators_used": ["logsigmoid", "sigmoid", "clamp"], "implementation_hint": {"expects": ["cost_a", "cost_b", "logp_a", "logp_b"], "returns": "scalar"}, "code": "def generated_loss(batch, model_output, extra):\n    \"\"\"\n    A logistic loss with a hybrid margin that is modulated by rank-correctness.\n    \"\"\"\n    # Read tensors from the batch\n    cost_a = batch['cost_a']\n    cost_b = batch['cost_b']\n    logp_a = batch['log_prob_w']\n    logp_b = batch['log_prob_l']\n    weights = batch.get('weight')\n\n    # Read hyperparameters\n    hyperparams = extra['hyperparams']\n    fixed_temp = hyperparams.get('fixed_temp', 1.0)\n    cost_threshold = hyperparams.get('cost_threshold', 0.5)\n    w_logp = hyperparams.get('w_logp', 0.5)\n    w_cost = hyperparams.get('w_cost', 0.5)\n    min_scale = hyperparams.get('min_scale', 0.1)\n    max_scale = hyperparams.get('max_scale', 5.0)\n    epsilon = hyperparams.get('epsilon', 1e-8)\n\n    # 1. Calculate log-probability and cost differences\n    delta_logp = logp_a - logp_b\n    delta_cost = cost_b - cost_a\n\n    # 2. Compute batch statistics (detached for stability)\n    if delta_logp.numel() > 1:\n        logp_std = delta_logp.detach().std()\n        cost_std = delta_cost.detach().std()\n    else:\n        logp_std = torch.tensor(1.0, device=delta_logp.device)\n        cost_std = torch.tensor(1.0, device=delta_cost.device)\n\n    # 3. Inherit from P1: Hybrid adaptive scale\n    hybrid_scale = w_logp * logp_std + w_cost * cost_std\n    adaptive_scale = torch.clamp(hybrid_scale, min_scale, max_scale)\n\n    # 4. Inherit from P0/P1: Adaptive beta from scale\n    beta = 1.0 / (adaptive_scale.detach() + epsilon)\n\n    # 5. Inherit from P0: Base margin components\n    cost_weight_A = torch.sigmoid(delta_cost / fixed_temp)\n    margin_A = adaptive_scale * cost_weight_A\n    \n    adaptive_cost_temp = cost_std + epsilon\n    cost_weight_B = torch.sigmoid(delta_cost / adaptive_cost_temp)\n    margin_B = adaptive_scale * cost_weight_B\n\n    # 6. New Coupling 1: Cost-gap-weighted alpha for blending\n    # Alpha increases for pairs with larger cost differences\n    alpha = torch.sigmoid(delta_cost - cost_threshold)\n\n    # 7. Inherit from P0: Blend margins (favoring adaptive-temp margin_B for high cost gaps)\n    blended_margin = alpha * margin_B + (1.0 - alpha) * margin_A\n\n    # 8. New Coupling 2: Rank-based margin modulation\n    # Reduce margin for pairs the model already gets right (delta_logp > 0)\n    correctness_factor = (1.0 - torch.sigmoid(delta_logp)).detach()\n    final_margin = blended_margin * correctness_factor\n\n    # 9. Compute the loss argument\n    loss_arg = delta_logp - final_margin\n\n    # 10. Apply negative log-sigmoid loss\n    loss_per_pair = -torch.nn.functional.logsigmoid(beta * loss_arg)\n\n    # 11. Apply optional weights and compute the mean loss\n    if weights is not None:\n        loss = (loss_per_pair * weights).sum() / weights.sum().clamp(min=epsilon)\n    else:\n        loss = loss_per_pair.mean()\n\n    return loss", "theoretical_basis": "A Bradley-Terry logistic preference model with a highly adaptive, per-pair margin. The margin's structure (blend of fixed vs. adaptive temperature) is determined by the cost-gap magnitude, and its overall scale is modulated by the model's existing rank-correctness for that pair. This focuses learning on misranked pairs while preventing over-confident predictions on easy pairs. The loss temperature is inversely coupled to a hybrid measure of batch variance (in both costs and log-probs) for stability."}, "fitness": {"hf_like_score": 7.896653826904297, "validation_objective": 7.89591928100586, "generalization_penalty": 0.0007345458984371334, "generalization_objectives": {"100": 7.896653826904297}, "train_score_mean": 9.335634522550928, "train_loss_mean": 0.3353193214514739, "pair_count": 4951580539, "early_eval": {"enabled": true, "steps": 100, "baseline_validation_objective": 8.412976063537597, "candidate_validation_objective": 8.35409621887207, "early_stopped": false}, "phases": {"f1": {"steps": 15630, "train_score_mean": 9.335634522550928, "train_loss_mean": 0.3353193214514739, "pair_count": 4951580539}, "f2": {"steps": 0, "train_score_mean": null, "train_loss_mean": null, "pair_count": 0}}, "config": {"hf": {"problem": "tsp", "hf_steps": 0, "hf_epochs": 10, "hf_instances_per_epoch": 100000, "train_problem_size": 100, "valid_problem_sizes": [100], "train_batch_size": 64, "pomo_size": 100, "learning_rate": 0.0003, "weight_decay": 1e-06, "alpha": 0.05, "device": "cuda:3", "seed": 1234, "num_validation_episodes": 10000, "validation_batch_size": 64, "generalization_penalty_weight": 1.0, "pool_version": "v0"}, "free_loss": {"f1_steps": 0, "total_train_steps": 15630, "f2_steps": 0, "f3_enabled": false}}, "loss_ir": {"name": "Rank_Modulated_Hybrid_Margin_Loss", "intuition": "Mode: combine. This loss function refines the adaptive margin concept from both parents by introducing a novel rank-based modulation, aiming to focus the model's learning on the most informative preference pairs. It inherits the core `logsigmoid(beta * (delta_logp - margin))` structure and the idea of a blended margin (`alpha * margin_A + (1-alpha) * margin_B`) from Parent 0. It also inherits the concept of a hybrid adaptive scale (`w_logp * std(delta_logp) + w_cost * std(delta_cost)`) for both the margin magnitude and the loss temperature (`beta`) from Parent 1, which holistically measures batch difficulty.\n\nThe first new coupling idea is **rank-based margin modulation**. Instead of applying the same margin calculation to all pairs, the margin is scaled down for pairs where the model's current preference (`delta_logp`) is already strongly aligned with the true preference (`delta_cost > 0`). This is achieved by multiplying the margin by `1.0 - sigmoid(delta_logp)`. This 'correctness factor' reduces the margin for 'easy' pairs, effectively telling the model 'you already know this one, don't push the log-probabilities further just to meet a large margin'. This prevents overfitting on easy pairs and focuses gradient updates on misclassified or uncertain pairs. The second new coupling is a **cost-gap-weighted alpha** for blending the two margin components. Instead of a fixed or complex statistical alpha, the blending factor is `alpha = sigmoid(delta_cost - cost_threshold)`. This smoothly transitions from favoring the fixed-temperature margin (for small cost gaps) to the adaptive-temperature margin (for large cost gaps), making the margin's structure directly responsive to the magnitude of the preference signal for each individual pair, rather than just batch-level statistics.", "hyperparams": {"fixed_temp": 1.0, "cost_threshold": 0.5, "w_logp": 0.5, "w_cost": 0.5, "min_scale": 0.1, "max_scale": 5.0, "epsilon": 1e-08}, "operators_used": ["logsigmoid", "sigmoid", "clamp"]}}, "better_than_baseline": true}
{"generation": 9, "index": 6, "ir": {"name": "Rank_Stabilized_Hybrid_Margin_Loss", "intuition": "Based on the error `E_FORWARD_ERROR: name 'rank_gap' is not defined`, I've fixed the implementation by inlining the `rank_gap` helper function directly into the main loss function body. The original code defined `rank_gap` as a separate function, but the execution environment for dynamic gates expects all logic to be within the `generated_loss` function. This change resolves the `NameError` without altering the core logic of using a rank-based cost gap for stabilization. The rest of the innovative logic, including the adaptive margin and correlation-aware beta, remains unchanged.", "pseudocode": "1. Calculate the difference in log-probabilities: delta_logp = logp(a) - logp(b).\n2. Calculate the difference in costs: delta_cost = cost(b) - cost(a).\n3. New Coupling 1: Compute a rank-based, normalized cost gap. Stack `cost_a` and `cost_b`, compute their ranks along the pair dimension, and find the difference. This result, `norm_cost_gap`, is used in the margin calculation instead of raw delta_cost.\n4. Inherit from Parent 0: Compute an adaptive scale based on the standard deviation of delta_logp, clipped for stability: `adaptive_scale = clamp(std(delta_logp), min_scale, max_scale)`.\n5. Inherit from Parent 1: Calculate two base margin components using the normalized cost gap. `margin_A` uses a fixed temperature: `adaptive_scale * sigmoid(norm_cost_gap / fixed_temp)`. `margin_B` uses an adaptive temperature based on `std(norm_cost_gap)`.\n6. Inherit from Parent 1: Blend the two margin components using a fixed hyperparameter `alpha`: `margin = alpha * margin_A + (1 - alpha) * margin_B`.\n7. New Coupling 2: Compute a dynamic offset for beta. Calculate the Pearson correlation coefficient between `delta_logp` and `delta_cost`. Use this correlation (scaled by a small factor `corr_factor`) as an offset.\n8. Inherit from Parent 0: Compute the adaptive beta as the inverse of the adaptive scale, and add the dynamic offset: `beta = (1.0 / adaptive_scale) + corr_offset`.\n9. Compute the final loss argument: `loss_arg = delta_logp - margin`.\n10. Calculate the final loss using the scaled negative log-sigmoid function: `loss = -logsigmoid(beta * loss_arg)`.\n11. Return the mean of the loss across the batch.", "hyperparams": {"fixed_temp": 0.1, "alpha": 0.5, "corr_factor": 0.1, "min_scale": 0.1, "max_scale": 5.0, "epsilon": 1e-08}, "operators_used": ["logsigmoid", "sigmoid", "clamp", "rank_gap"], "implementation_hint": {"expects": ["A batch containing 'cost_a', 'cost_b', 'log_prob_w', and 'log_prob_l' tensors. 'log_prob_w' corresponds to the preferred response 'a', and 'log_prob_l' to the dispreferred response 'b'."], "returns": "A single scalar tensor representing the mean loss over the batch."}, "code": "def generated_loss(batch, model_output, extra):\n    \"\"\"\n    A logistic loss with a rank-stabilized margin and correlation-aware beta.\n    \"\"\"\n    # Read tensors from the batch\n    cost_a = batch['cost_a']\n    cost_b = batch['cost_b']\n    logp_a = batch['log_prob_w']\n    logp_b = batch['log_prob_l']\n    weights = batch.get('weight')\n\n    # Read hyperparameters\n    hyperparams = extra['hyperparams']\n    fixed_temp = hyperparams.get('fixed_temp', 0.1)\n    alpha = hyperparams.get('alpha', 0.5)\n    corr_factor = hyperparams.get('corr_factor', 0.1)\n    min_scale = hyperparams.get('min_scale', 0.1)\n    max_scale = hyperparams.get('max_scale', 5.0)\n    epsilon = hyperparams.get('epsilon', 1e-8)\n\n    # 1. Calculate log-probability and cost differences\n    delta_logp = logp_a - logp_b\n    delta_cost = cost_b - cost_a\n\n    # 2. New Coupling 1: Rank-based cost gap\n    # Inlined rank_gap logic to resolve NameError\n    costs = torch.stack([cost_a, cost_b], dim=1)\n    ranks = costs.argsort(dim=1).argsort(dim=1).float()\n    rank_a, rank_b = ranks[:, 0], ranks[:, 1]\n    norm_cost_gap = (rank_a - rank_b) # Range [-1, 1]\n\n    # 3. Inherit from Parent 0: Adaptive scale from logp variance\n    if delta_logp.numel() > 1:\n        logp_std = delta_logp.detach().std()\n    else:\n        logp_std = torch.tensor(1.0, device=delta_logp.device)\n    adaptive_scale = torch.clamp(logp_std, min_scale, max_scale)\n\n    # 4. Inherit from Parent 1: Margin components with rank-based cost\n    cost_weight_A = torch.sigmoid(norm_cost_gap / fixed_temp)\n    margin_A = adaptive_scale * cost_weight_A\n\n    if norm_cost_gap.numel() > 1:\n        adaptive_cost_temp = norm_cost_gap.detach().std() + epsilon\n    else:\n        adaptive_cost_temp = torch.tensor(1.0, device=norm_cost_gap.device)\n    cost_weight_B = torch.sigmoid(norm_cost_gap / adaptive_cost_temp)\n    margin_B = adaptive_scale * cost_weight_B\n\n    # 5. Inherit from Parent 1: Blended margin\n    margin = alpha * margin_A + (1.0 - alpha) * margin_B\n\n    # 6. New Coupling 2: Dynamic beta offset via correlation\n    corr_offset = torch.tensor(0.0, device=delta_logp.device)\n    if delta_logp.numel() > 1:\n        # Centered versions for Pearson correlation\n        vx = delta_logp.detach() - delta_logp.detach().mean()\n        vy = delta_cost.detach() - delta_cost.detach().mean()\n        # Numerically stable correlation calculation\n        correlation = (vx * vy).sum() / (torch.sqrt((vx**2).sum()) * torch.sqrt((vy**2).sum()) + epsilon)\n        corr_offset = corr_factor * correlation.clamp(-1.0, 1.0)\n\n    # 7. Inherit from Parent 0: Adaptive beta + dynamic offset\n    beta = (1.0 / (adaptive_scale.detach() + epsilon)) + corr_offset\n\n    # 8. Compute the loss argument\n    loss_arg = delta_logp - margin\n\n    # 9. Apply negative log-sigmoid loss\n    loss_per_pair = -torch.nn.functional.logsigmoid(beta * loss_arg)\n\n    # 10. Apply optional weights and compute the mean loss\n    if weights is not None:\n        loss = (loss_per_pair * weights).sum() / weights.sum().clamp(min=epsilon)\n    else:\n        loss = loss_per_pair.mean()\n\n    return loss", "theoretical_basis": ""}, "fitness": {"hf_like_score": 8.667896531677247, "validation_objective": 8.667896531677247, "generalization_penalty": 0.0, "generalization_objectives": {"100": 8.660476596069335}, "train_score_mean": 16.093299388885498, "train_loss_mean": 0.5606526723504066, "pair_count": 31679974, "early_eval": {"enabled": true, "steps": 100, "baseline_validation_objective": 8.412976063537597, "candidate_validation_objective": 8.667896531677247, "early_stopped": true}, "phases": {"f1": {"steps": 15630, "train_score_mean": 16.093299388885498, "train_loss_mean": 0.5606526723504066, "pair_count": 31679974}, "f2": {"steps": 0, "train_score_mean": null, "train_loss_mean": null, "pair_count": 0}}, "config": {"hf": {"problem": "tsp", "hf_steps": 0, "hf_epochs": 10, "hf_instances_per_epoch": 100000, "train_problem_size": 100, "valid_problem_sizes": [100], "train_batch_size": 64, "pomo_size": 100, "learning_rate": 0.0003, "weight_decay": 1e-06, "alpha": 0.05, "device": "cuda:0", "seed": 1234, "num_validation_episodes": 10000, "validation_batch_size": 64, "generalization_penalty_weight": 1.0, "pool_version": "v0"}, "free_loss": {"f1_steps": 0, "total_train_steps": 15630, "f2_steps": 0, "f3_enabled": false}}, "loss_ir": {"name": "Rank_Stabilized_Hybrid_Margin_Loss", "intuition": "Based on the error `E_FORWARD_ERROR: name 'rank_gap' is not defined`, I've fixed the implementation by inlining the `rank_gap` helper function directly into the main loss function body. The original code defined `rank_gap` as a separate function, but the execution environment for dynamic gates expects all logic to be within the `generated_loss` function. This change resolves the `NameError` without altering the core logic of using a rank-based cost gap for stabilization. The rest of the innovative logic, including the adaptive margin and correlation-aware beta, remains unchanged.", "hyperparams": {"fixed_temp": 0.1, "alpha": 0.5, "corr_factor": 0.1, "min_scale": 0.1, "max_scale": 5.0, "epsilon": 1e-08}, "operators_used": ["logsigmoid", "sigmoid", "clamp", "rank_gap"]}}, "better_than_baseline": false}
