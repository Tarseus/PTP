{"generation": 0, "index": 0, "ir": {"name": "Adaptive Margin Sigmoid Loss", "intuition": "The loss is designed to be robust to varying scales of cost differences. It computes a cost gap and normalizes it using a softplus function, creating a 'soft rank' between 0 and 1 that represents the certainty of preference based on cost. This normalized gap is then used as an adaptive margin. The loss is a sigmoid function applied to the difference between the log-probability gap and this adaptive margin, scaled by a temperature parameter. This structure ensures that the loss is bounded, stable, and adapts the learning signal's strength based on how much better one solution is than the other, preventing extreme gradients from large cost differences while still providing a strong signal for clear preferences.", "pseudocode": "1. Compute the cost difference: cost_diff = cost_b - cost_a.\n2. Calculate an adaptive, bounded margin based on the cost difference. The softplus function ensures the margin is always positive and grows smoothly, while the tanh function bounds it between 0 and 1, representing the 'preference strength': margin = tanh(softplus(cost_diff * scale)).\n3. Compute the log-probability difference: logp_diff = logp_a - logp_b.\n4. Calculate the core term of the loss: the difference between the log-probability gap and the adaptive margin, scaled by a temperature parameter: core_term = (logp_diff - margin) * temperature.\n5. Apply the logsigmoid function to the negative core term to obtain a stable, bounded loss. This is equivalent to softplus(-core_term), which penalizes cases where logp_diff is smaller than the adaptive margin: loss = softplus(-core_term).", "hyperparams": {"temperature": 10.0, "scale": 0.1}, "operators_used": ["softplus", "tanh"], "implementation_hint": {"expects": ["cost_a", "cost_b", "logp_a", "logp_b"], "returns": "scalar"}}, "fitness": {"hf_like_score": 3.885378360748291, "validation_objective": 3.885378360748291, "generalization_penalty": 0.0, "generalization_objectives": {"20": 3.852010130882263}, "train_score_mean": 4.210027851164341, "train_loss_mean": 0.6914556585252285, "pair_count": 4111252, "config": {"hf": {"problem": "tsp", "hf_steps": 32, "train_problem_size": 20, "valid_problem_sizes": [20], "train_batch_size": 64, "pomo_size": 64, "learning_rate": 0.0003, "weight_decay": 1e-06, "alpha": 0.05, "device": "cuda", "seed": 1234, "num_validation_episodes": 128, "validation_batch_size": 64, "generalization_penalty_weight": 1.0, "pool_version": "v0"}, "free_loss": {"f1_steps": 32, "f2_steps": 100, "f3_enabled": false}}, "loss_ir": {"name": "Adaptive Margin Sigmoid Loss", "intuition": "The loss is designed to be robust to varying scales of cost differences. It computes a cost gap and normalizes it using a softplus function, creating a 'soft rank' between 0 and 1 that represents the certainty of preference based on cost. This normalized gap is then used as an adaptive margin. The loss is a sigmoid function applied to the difference between the log-probability gap and this adaptive margin, scaled by a temperature parameter. This structure ensures that the loss is bounded, stable, and adapts the learning signal's strength based on how much better one solution is than the other, preventing extreme gradients from large cost differences while still providing a strong signal for clear preferences.", "hyperparams": {"temperature": 10.0, "scale": 0.1}, "operators_used": ["softplus", "tanh"]}}}
{"generation": 0, "index": 1, "ir": {"name": "Adaptive Margin Sigmoid Loss", "intuition": "This loss function uses the relative cost difference between two solutions to dynamically set a target margin for their log-probability difference. The core idea is that a larger cost gap should demand a larger log-probability gap. A sigmoid function scales the cost difference into a bounded, normalized margin between 0 and a maximum value (`max_margin`). This adaptive margin is then used within a softplus function, which acts like a smooth hinge loss. This structure ensures that the loss is high when the model's preference contradicts a large cost difference, but saturates gracefully to prevent extreme gradients from very large cost or log-probability differences, ensuring numerical stability.", "pseudocode": "1. Compute the cost difference: cost_diff = cost_b - cost_a.\n2. Compute the log-probability difference: logp_diff = logp_a - logp_b.\n3. Create an adaptive margin based on the cost difference. Scale the cost_diff by a hyperparameter `beta` and pass it through a sigmoid function. This maps the cost difference to a value between 0 and 1. Multiply by `max_margin` to set the target separation.\n   adaptive_margin = max_margin * sigmoid(beta * cost_diff).\n4. The loss is then calculated as the softplus of the adaptive margin minus the log-probability difference. This penalizes cases where logp_diff is smaller than the target adaptive_margin.\n   loss = softplus(adaptive_margin - logp_diff).", "hyperparams": {"beta": 0.1, "max_margin": 5.0}, "operators_used": ["sigmoid", "softplus"], "implementation_hint": {"expects": ["cost_a", "cost_b", "logp_a", "logp_b"], "returns": "scalar"}}, "fitness": {"hf_like_score": 3.881171464920044, "validation_objective": 3.881171464920044, "generalization_penalty": 0.0, "generalization_objectives": {"20": 3.8458696603775024}, "train_score_mean": 4.211596377193928, "train_loss_mean": 0.6782676354050636, "pair_count": 4112320, "config": {"hf": {"problem": "tsp", "hf_steps": 32, "train_problem_size": 20, "valid_problem_sizes": [20], "train_batch_size": 64, "pomo_size": 64, "learning_rate": 0.0003, "weight_decay": 1e-06, "alpha": 0.05, "device": "cuda", "seed": 1234, "num_validation_episodes": 128, "validation_batch_size": 64, "generalization_penalty_weight": 1.0, "pool_version": "v0"}, "free_loss": {"f1_steps": 32, "f2_steps": 100, "f3_enabled": false}}, "loss_ir": {"name": "Adaptive Margin Sigmoid Loss", "intuition": "This loss function uses the relative cost difference between two solutions to dynamically set a target margin for their log-probability difference. The core idea is that a larger cost gap should demand a larger log-probability gap. A sigmoid function scales the cost difference into a bounded, normalized margin between 0 and a maximum value (`max_margin`). This adaptive margin is then used within a softplus function, which acts like a smooth hinge loss. This structure ensures that the loss is high when the model's preference contradicts a large cost difference, but saturates gracefully to prevent extreme gradients from very large cost or log-probability differences, ensuring numerical stability.", "hyperparams": {"beta": 0.1, "max_margin": 5.0}, "operators_used": ["sigmoid", "softplus"]}}}
{"generation": 0, "index": 2, "ir": {"name": "Adaptive Margin Sigmoid Loss", "intuition": "This loss function dynamically adapts the preference margin based on the magnitude of the cost difference. For pairs with a large cost difference, it enforces a stronger preference (larger margin), encouraging the model to be more confident. For pairs with a small, almost negligible cost difference, it uses a smaller margin, preventing the model from overfitting to noisy or insignificant preferences. The cost difference is normalized using a hyperbolic tangent function (tanh) to keep it bounded and stable, which then scales a base margin. The final loss is computed using a softplus function, which is a smooth and numerically stable alternative to a hinge loss, ensuring gradients are always well-behaved.", "pseudocode": "1. Compute the cost difference: delta_cost = cost_a - cost_b.\n2. Compute the log-probability difference: delta_logp = logp_a - logp_b.\n3. Create a signed preference indicator: pref_indicator = rank_gap(cost_a, cost_b), which is -1 if a is better, +1 if b is better.\n4. Calculate an adaptive margin: adaptive_margin = base_margin + margin_scale * tanh(abs(delta_cost) * cost_sensitivity).\n5. Combine the log-probability difference with the adaptive margin, guided by the preference indicator: argument = pref_indicator * delta_logp + adaptive_margin.\n6. Compute the final loss using the softplus function for stability: loss = softplus(-argument). This penalizes cases where the argument is negative (i.e., when the model's preference contradicts the ground truth preference by more than the adaptive margin).", "hyperparams": {"base_margin": 0.1, "margin_scale": 2.0, "cost_sensitivity": 0.05}, "operators_used": ["rank_gap", "tanh", "softplus"], "implementation_hint": {"expects": ["cost_a", "cost_b", "logp_a", "logp_b"], "returns": "scalar"}}, "fitness": {"hf_like_score": 3.8897804021835327, "validation_objective": 3.8897804021835327, "generalization_penalty": 0.0, "generalization_objectives": {"20": 3.8502399921417236}, "train_score_mean": 4.2085650116205215, "train_loss_mean": 0.6781906466931105, "pair_count": 4111497, "config": {"hf": {"problem": "tsp", "hf_steps": 32, "train_problem_size": 20, "valid_problem_sizes": [20], "train_batch_size": 64, "pomo_size": 64, "learning_rate": 0.0003, "weight_decay": 1e-06, "alpha": 0.05, "device": "cuda", "seed": 1234, "num_validation_episodes": 128, "validation_batch_size": 64, "generalization_penalty_weight": 1.0, "pool_version": "v0"}, "free_loss": {"f1_steps": 32, "f2_steps": 100, "f3_enabled": false}}, "loss_ir": {"name": "Adaptive Margin Sigmoid Loss", "intuition": "This loss function dynamically adapts the preference margin based on the magnitude of the cost difference. For pairs with a large cost difference, it enforces a stronger preference (larger margin), encouraging the model to be more confident. For pairs with a small, almost negligible cost difference, it uses a smaller margin, preventing the model from overfitting to noisy or insignificant preferences. The cost difference is normalized using a hyperbolic tangent function (tanh) to keep it bounded and stable, which then scales a base margin. The final loss is computed using a softplus function, which is a smooth and numerically stable alternative to a hinge loss, ensuring gradients are always well-behaved.", "hyperparams": {"base_margin": 0.1, "margin_scale": 2.0, "cost_sensitivity": 0.05}, "operators_used": ["rank_gap", "tanh", "softplus"]}}}
{"generation": 1, "index": 0, "ir": {"name": "Rank-Normalized Margin Loss", "intuition": "This loss function combines the adaptive margin concepts from both parents with a novel rank-based normalization scheme for enhanced stability and context-awareness. \n\nInherited Ideas:\n- From Parent 0, we inherit the core structure of `softplus(margin - logp_diff)`, which provides a smooth, hinge-like penalty when the model's preference (`logp_diff`) does not meet the target `margin`.\n- From Parent 1, we inherit the use of `tanh(softplus(...))` to create a bounded, normalized representation of preference strength based on cost differences. This ensures the margin is always between 0 and 1, making it robust to the scale of the costs.\n\nNew Coupling Ideas:\n1.  **Rank-Based Normalization:** Instead of using the raw `cost_b - cost_a`, we first compute a `rank_gap` of the costs within a batch. This makes the margin adaptive not only to the magnitude of the difference but also to its relative ranking within the current data distribution. This helps the model focus on meaningful distinctions rather than being swayed by absolute cost outliers.\n2.  **Margin Scaling:** The `tanh(softplus(...))` output (from 0 to 1) is then scaled by a `max_margin` hyperparameter. This allows explicit control over the maximum desired separation in log-probability space, combining the bounded nature of Parent 1's margin with the explicit scale control of Parent 0.", "pseudocode": "1. For a batch of preferences, compute the rank-normalized cost gap: normalized_cost_gap = rank_gap(cost_b, cost_a).\n2. Compute the log-probability difference: logp_diff = logp_a - logp_b.\n3. Create a bounded preference strength using the normalized cost gap. The softplus ensures positivity, and tanh bounds it between 0 and 1: preference_strength = tanh(softplus(beta * normalized_cost_gap)).\n4. Scale this preference strength by a maximum margin to define the target separation for log probabilities: adaptive_margin = max_margin * preference_strength.\n5. The loss is calculated as the softplus of the adaptive margin minus the log-probability difference. This penalizes cases where logp_diff is smaller than the target adaptive_margin in a smooth and stable manner.\n   loss = softplus(adaptive_margin - logp_diff).", "hyperparams": {"beta": 1.0, "max_margin": 5.0}, "operators_used": ["rank_gap", "softplus", "tanh"], "implementation_hint": {"expects": ["cost_a", "cost_b", "logp_a", "logp_b"], "returns": "scalar"}}, "fitness": {"hf_like_score": 3.8854143619537354, "validation_objective": 3.8854143619537354, "generalization_penalty": 0.0, "generalization_objectives": {"20": 3.8561620712280273}, "train_score_mean": 4.210797928273678, "train_loss_mean": 0.6780349910259247, "pair_count": 4112164, "config": {"hf": {"problem": "tsp", "hf_steps": 32, "train_problem_size": 20, "valid_problem_sizes": [20], "train_batch_size": 64, "pomo_size": 64, "learning_rate": 0.0003, "weight_decay": 1e-06, "alpha": 0.05, "device": "cuda", "seed": 1234, "num_validation_episodes": 128, "validation_batch_size": 64, "generalization_penalty_weight": 1.0, "pool_version": "v0"}, "free_loss": {"f1_steps": 32, "f2_steps": 100, "f3_enabled": false}}, "loss_ir": {"name": "Rank-Normalized Margin Loss", "intuition": "This loss function combines the adaptive margin concepts from both parents with a novel rank-based normalization scheme for enhanced stability and context-awareness. \n\nInherited Ideas:\n- From Parent 0, we inherit the core structure of `softplus(margin - logp_diff)`, which provides a smooth, hinge-like penalty when the model's preference (`logp_diff`) does not meet the target `margin`.\n- From Parent 1, we inherit the use of `tanh(softplus(...))` to create a bounded, normalized representation of preference strength based on cost differences. This ensures the margin is always between 0 and 1, making it robust to the scale of the costs.\n\nNew Coupling Ideas:\n1.  **Rank-Based Normalization:** Instead of using the raw `cost_b - cost_a`, we first compute a `rank_gap` of the costs within a batch. This makes the margin adaptive not only to the magnitude of the difference but also to its relative ranking within the current data distribution. This helps the model focus on meaningful distinctions rather than being swayed by absolute cost outliers.\n2.  **Margin Scaling:** The `tanh(softplus(...))` output (from 0 to 1) is then scaled by a `max_margin` hyperparameter. This allows explicit control over the maximum desired separation in log-probability space, combining the bounded nature of Parent 1's margin with the explicit scale control of Parent 0.", "hyperparams": {"beta": 1.0, "max_margin": 5.0}, "operators_used": ["rank_gap", "softplus", "tanh"]}}}
{"generation": 1, "index": 1, "ir": {"name": "Rank-Gapped Sigmoid Margin Loss", "intuition": "This loss function combines an adaptive margin with a rank-based stability mechanism. It inherits the core concept of an adaptive margin from both parents, where the difference in cost between two solutions dictates the desired separation in their log-probabilities. Specifically, it inherits the use of a sigmoid function to create a bounded margin from Parent 0, ensuring the target separation scales smoothly with the cost difference. From Parent 1, it inherits the idea of scaling the log-probability difference by a temperature parameter, which controls the sharpness of the loss landscape.\n\nA new coupling idea is introduced: the `rank_gap` operator. Before calculating the adaptive margin, the raw cost difference is replaced by its rank-gap. This means we consider the relative ranking of the current cost difference among all cost differences in the batch, rather than its absolute value. This makes the loss invariant to the scale of the costs and more robust to outliers with extreme cost differences, preventing a single pair from dominating the gradient signal. A second new idea is using `logsigmoid` directly on the scaled difference between the log-probability gap and the margin. This is a common and stable formulation for binary cross-entropy-style losses, ensuring the output is always negative and well-behaved.", "pseudocode": "1. Compute the cost difference for the current pair: cost_diff = cost_b - cost_a.\n2. Compute the rank-gap of the cost difference across the entire batch. This replaces the raw cost_diff with a value that reflects its relative magnitude within the batch, e.g., a value between -1 and 1.\n   ranked_cost_diff = rank_gap(cost_diff, all_cost_diffs_in_batch).\n3. Create an adaptive margin using the ranked cost difference. The sigmoid function maps the rank-gap to a value between 0 and 1, which is then scaled by `max_margin` to set the target separation.\n   adaptive_margin = max_margin * sigmoid(ranked_cost_diff).\n4. Compute the log-probability difference: logp_diff = logp_a - logp_b.\n5. Calculate the core loss term. This is the difference between the adaptive margin and the log-probability difference, scaled by a temperature parameter.\n   core_term = temperature * (adaptive_margin - logp_diff).\n6. The final loss is the negative logsigmoid of the core term. This penalizes cases where the log-probability difference is smaller than the target adaptive margin in a numerically stable way.\n   loss = -logsigmoid(core_term).", "hyperparams": {"temperature": 1.0, "max_margin": 5.0}, "operators_used": ["rank_gap", "sigmoid", "logsigmoid"], "implementation_hint": {"expects": ["cost_a", "cost_b", "logp_a", "logp_b"], "returns": "scalar"}}, "fitness": {"hf_like_score": 3.8836371898651123, "validation_objective": 3.8836371898651123, "generalization_penalty": 0.0, "generalization_objectives": {"20": 3.8602771759033203}, "train_score_mean": 4.209405399858952, "train_loss_mean": 0.6780776362866163, "pair_count": 4111571, "config": {"hf": {"problem": "tsp", "hf_steps": 32, "train_problem_size": 20, "valid_problem_sizes": [20], "train_batch_size": 64, "pomo_size": 64, "learning_rate": 0.0003, "weight_decay": 1e-06, "alpha": 0.05, "device": "cuda", "seed": 1234, "num_validation_episodes": 128, "validation_batch_size": 64, "generalization_penalty_weight": 1.0, "pool_version": "v0"}, "free_loss": {"f1_steps": 32, "f2_steps": 100, "f3_enabled": false}}, "loss_ir": {"name": "Rank-Gapped Sigmoid Margin Loss", "intuition": "This loss function combines an adaptive margin with a rank-based stability mechanism. It inherits the core concept of an adaptive margin from both parents, where the difference in cost between two solutions dictates the desired separation in their log-probabilities. Specifically, it inherits the use of a sigmoid function to create a bounded margin from Parent 0, ensuring the target separation scales smoothly with the cost difference. From Parent 1, it inherits the idea of scaling the log-probability difference by a temperature parameter, which controls the sharpness of the loss landscape.\n\nA new coupling idea is introduced: the `rank_gap` operator. Before calculating the adaptive margin, the raw cost difference is replaced by its rank-gap. This means we consider the relative ranking of the current cost difference among all cost differences in the batch, rather than its absolute value. This makes the loss invariant to the scale of the costs and more robust to outliers with extreme cost differences, preventing a single pair from dominating the gradient signal. A second new idea is using `logsigmoid` directly on the scaled difference between the log-probability gap and the margin. This is a common and stable formulation for binary cross-entropy-style losses, ensuring the output is always negative and well-behaved.", "hyperparams": {"temperature": 1.0, "max_margin": 5.0}, "operators_used": ["rank_gap", "sigmoid", "logsigmoid"]}}}
{"generation": 1, "index": 2, "ir": {"name": "Rank-Normalized Margin Loss", "intuition": "This loss function combines the adaptive margin concept from its parents with a novel rank-based normalization to improve stability and robustness to outliers. \n\nInherited Ideas:\n- From Parent 0: It inherits the core structure of `softplus(margin - logp_diff)`, which creates a smooth hinge loss that penalizes the model when the log-probability difference (`logp_a - logp_b`) does not meet the target margin.\n- From Parent 1: It inherits the idea of scaling the cost difference to control its influence on the margin, using a hyperparameter (`beta`) similar to Parent 1's `scale`.\n\nNew Coupling Ideas:\n1. **Rank-based Normalization**: Instead of using the raw cost difference `cost_b - cost_a`, which can be sensitive to outliers and varying scales, this loss uses the `rank_gap` between `cost_a` and `cost_b`. This operator normalizes the cost difference based on its rank within a batch, making the margin less susceptible to extreme cost values and more focused on the relative ordering of preferences. This ensures the margin is always within a predictable range (e.g., -1 to 1).\n2. **Margin Scaling with `tanh`**: The normalized rank gap is passed through a `tanh` function. This creates a smooth, bounded (-1 to 1) signal that saturates for very large rank differences, preventing the margin from growing indefinitely and contributing to numerical stability. The result is then scaled by `max_margin` to set the final target separation for the log-probabilities.", "pseudocode": "1. Compute the log-probability difference: logp_diff = logp_a - logp_b.\n2. Calculate the rank-normalized cost gap using the `rank_gap` operator on the costs of the current batch. This transforms the raw cost difference into a stable, rank-based value (e.g., in [-1, 1]).\n   normalized_cost_gap = rank_gap(cost_a, cost_b).\n3. Create an adaptive margin. Apply a `tanh` function to the scaled normalized cost gap. This produces a smooth, bounded value which is then scaled by `max_margin` to define the target separation.\n   adaptive_margin = max_margin * tanh(beta * normalized_cost_gap).\n4. The final loss is calculated as the softplus of the difference between the adaptive margin and the log-probability difference. This penalizes cases where `logp_diff` is smaller than the target `adaptive_margin` in a smooth and stable manner.\n   loss = softplus(adaptive_margin - logp_diff).", "hyperparams": {"beta": 1.0, "max_margin": 5.0}, "operators_used": ["softplus", "tanh", "rank_gap"], "implementation_hint": {"expects": ["cost_a", "cost_b", "logp_a", "logp_b"], "returns": "scalar"}}, "fitness": {"hf_like_score": 3.8781912326812744, "validation_objective": 3.8781912326812744, "generalization_penalty": 0.0, "generalization_objectives": {"20": 3.8498014211654663}, "train_score_mean": 4.2100652903318405, "train_loss_mean": 0.6781818065792322, "pair_count": 4111573, "config": {"hf": {"problem": "tsp", "hf_steps": 32, "train_problem_size": 20, "valid_problem_sizes": [20], "train_batch_size": 64, "pomo_size": 64, "learning_rate": 0.0003, "weight_decay": 1e-06, "alpha": 0.05, "device": "cuda", "seed": 1234, "num_validation_episodes": 128, "validation_batch_size": 64, "generalization_penalty_weight": 1.0, "pool_version": "v0"}, "free_loss": {"f1_steps": 32, "f2_steps": 100, "f3_enabled": false}}, "loss_ir": {"name": "Rank-Normalized Margin Loss", "intuition": "This loss function combines the adaptive margin concept from its parents with a novel rank-based normalization to improve stability and robustness to outliers. \n\nInherited Ideas:\n- From Parent 0: It inherits the core structure of `softplus(margin - logp_diff)`, which creates a smooth hinge loss that penalizes the model when the log-probability difference (`logp_a - logp_b`) does not meet the target margin.\n- From Parent 1: It inherits the idea of scaling the cost difference to control its influence on the margin, using a hyperparameter (`beta`) similar to Parent 1's `scale`.\n\nNew Coupling Ideas:\n1. **Rank-based Normalization**: Instead of using the raw cost difference `cost_b - cost_a`, which can be sensitive to outliers and varying scales, this loss uses the `rank_gap` between `cost_a` and `cost_b`. This operator normalizes the cost difference based on its rank within a batch, making the margin less susceptible to extreme cost values and more focused on the relative ordering of preferences. This ensures the margin is always within a predictable range (e.g., -1 to 1).\n2. **Margin Scaling with `tanh`**: The normalized rank gap is passed through a `tanh` function. This creates a smooth, bounded (-1 to 1) signal that saturates for very large rank differences, preventing the margin from growing indefinitely and contributing to numerical stability. The result is then scaled by `max_margin` to set the final target separation for the log-probabilities.", "hyperparams": {"beta": 1.0, "max_margin": 5.0}, "operators_used": ["softplus", "tanh", "rank_gap"]}}}
{"generation": 1, "index": 3, "ir": {"name": "Rank-Gap Scaled Adaptive Margin Loss", "intuition": "This loss function combines an adaptive margin with batch-level normalization to create a robust and stable preference learning signal. \n\nInherited ideas:\n- From Parent 0, it inherits the core structure of `softplus(margin - logp_diff)`, which provides a smooth, hinge-like penalty when the model's preference (`logp_diff`) does not meet the target `margin`.\n- From both parents, it inherits the concept of an `adaptive_margin` that increases with the cost difference (`cost_b - cost_a`), ensuring that larger cost gaps demand stronger model preference.\n\nNew coupling ideas:\n1. **Rank-Gap Scaling:** Instead of using the raw cost difference, we use a `rank_gap` operator. This operator normalizes the cost difference based on its rank within the current batch, making the margin less sensitive to the absolute scale of costs and more robust to outliers. This idea is a form of batch-level normalization applied to the margin calculation.\n2. **Dynamic Margin Scaling:** The `rank_gap` is then scaled by a `tanh` function, which bounds the margin between 0 and a `max_margin`. This prevents the margin from growing uncontrollably while still allowing it to adapt, combining the stability of a bounded margin (inspired by Parent 1) with the adaptive nature of Parent 0.", "pseudocode": "1. Compute the cost difference: cost_diff = cost_b - cost_a.\n2. Compute the log-probability difference: logp_diff = logp_a - logp_b.\n3. Normalize the cost difference using its rank within the batch to get a value between -1 and 1. This is the `rank_gap` of the cost difference.\n   normalized_cost_gap = rank_gap(cost_diff).\n4. Create an adaptive, bounded margin. The `tanh` function smoothly maps the normalized cost gap to a value that scales with the `max_margin`. We use `relu` to ensure the margin is non-negative, as we only care about cases where cost_b > cost_a.\n   adaptive_margin = max_margin * tanh(beta * relu(normalized_cost_gap)).\n5. The final loss is calculated by penalizing cases where the log-probability difference is smaller than the target adaptive margin, using a softplus function for stability.\n   loss = softplus(adaptive_margin - logp_diff).", "hyperparams": {"max_margin": 5.0, "beta": 2.0}, "operators_used": ["rank_gap", "tanh", "relu", "softplus"], "implementation_hint": {"expects": ["cost_a", "cost_b", "logp_a", "logp_b"], "returns": "scalar"}}, "fitness": {"hf_like_score": 3.8796855211257935, "validation_objective": 3.8796855211257935, "generalization_penalty": 0.0, "generalization_objectives": {"20": 3.8520355224609375}, "train_score_mean": 4.20790995657444, "train_loss_mean": 0.6782151311635971, "pair_count": 4111223, "config": {"hf": {"problem": "tsp", "hf_steps": 32, "train_problem_size": 20, "valid_problem_sizes": [20], "train_batch_size": 64, "pomo_size": 64, "learning_rate": 0.0003, "weight_decay": 1e-06, "alpha": 0.05, "device": "cuda", "seed": 1234, "num_validation_episodes": 128, "validation_batch_size": 64, "generalization_penalty_weight": 1.0, "pool_version": "v0"}, "free_loss": {"f1_steps": 32, "f2_steps": 100, "f3_enabled": false}}, "loss_ir": {"name": "Rank-Gap Scaled Adaptive Margin Loss", "intuition": "This loss function combines an adaptive margin with batch-level normalization to create a robust and stable preference learning signal. \n\nInherited ideas:\n- From Parent 0, it inherits the core structure of `softplus(margin - logp_diff)`, which provides a smooth, hinge-like penalty when the model's preference (`logp_diff`) does not meet the target `margin`.\n- From both parents, it inherits the concept of an `adaptive_margin` that increases with the cost difference (`cost_b - cost_a`), ensuring that larger cost gaps demand stronger model preference.\n\nNew coupling ideas:\n1. **Rank-Gap Scaling:** Instead of using the raw cost difference, we use a `rank_gap` operator. This operator normalizes the cost difference based on its rank within the current batch, making the margin less sensitive to the absolute scale of costs and more robust to outliers. This idea is a form of batch-level normalization applied to the margin calculation.\n2. **Dynamic Margin Scaling:** The `rank_gap` is then scaled by a `tanh` function, which bounds the margin between 0 and a `max_margin`. This prevents the margin from growing uncontrollably while still allowing it to adapt, combining the stability of a bounded margin (inspired by Parent 1) with the adaptive nature of Parent 0.", "hyperparams": {"max_margin": 5.0, "beta": 2.0}, "operators_used": ["rank_gap", "tanh", "relu", "softplus"]}}}
{"generation": 1, "index": 4, "ir": {"name": "Rank-Gap Scaled Adaptive Margin Loss", "intuition": "This loss function synthesizes ideas from its parents to create a stable and adaptive preference learning objective. From Parent 0, it inherits the core structure of `softplus(margin - logp_diff)`, which provides a smooth, hinge-like penalty when the model's log-probability difference (`logp_diff`) does not meet a target margin. From Parent 1, it inherits the use of `tanh(softplus(...))` to create a bounded, normalized, and adaptive margin based on the cost difference, ensuring the target separation scales with the perceived preference strength. \n\nThe novel coupling idea is the introduction of a `rank_gap` term to dynamically scale the influence of the loss. The `rank_gap` operator computes a normalized, batch-aware measure of how significant the `cost_b - cost_a` difference is relative to other cost differences in the batch. This scalar, `rank_gap_scale`, is then multiplied by the core loss. This has two benefits: 1) It acts as a stability trick by down-weighting the loss contribution from pairs with very small or insignificant cost differences, which might be noisy. 2) It focuses the training signal on pairs where the cost-based preference is clear and unambiguous within the context of the current batch, promoting more efficient learning.", "pseudocode": "1. Compute the cost and log-probability differences: cost_diff = cost_b - cost_a and logp_diff = logp_a - logp_b.\n2. (Inherited from Parent 1) Create a bounded, adaptive margin. The `softplus` ensures positivity, and `tanh` bounds the result between 0 and `max_margin`, representing the target preference strength.\n   adaptive_margin = max_margin * tanh(softplus(beta * cost_diff)).\n3. (Inherited from Parent 0) Calculate the base hinge-like loss, penalizing cases where the log-probability difference is smaller than the adaptive margin.\n   base_loss = softplus(adaptive_margin - logp_diff).\n4. (New Coupling Idea) Compute a batch-aware scaling factor using the `rank_gap` operator on the cost difference. This factor is close to 1 for large cost gaps (relative to the batch) and close to 0 for small ones.\n   rank_gap_scale = rank_gap(cost_diff).\n5. (New Coupling Idea) Modulate the base loss with the rank-gap scale. This focuses training on pairs with a clear cost preference within the batch context.\n   loss = rank_gap_scale * base_loss.", "hyperparams": {"beta": 0.1, "max_margin": 5.0}, "operators_used": ["softplus", "tanh", "rank_gap"], "implementation_hint": {"expects": ["cost_a", "cost_b", "logp_a", "logp_b"], "returns": "scalar"}}, "fitness": {"hf_like_score": 3.8813148736953735, "validation_objective": 3.8813148736953735, "generalization_penalty": 0.0, "generalization_objectives": {"20": 3.8531097173690796}, "train_score_mean": 4.20842219889164, "train_loss_mean": 0.6784002054482698, "pair_count": 4110952, "config": {"hf": {"problem": "tsp", "hf_steps": 32, "train_problem_size": 20, "valid_problem_sizes": [20], "train_batch_size": 64, "pomo_size": 64, "learning_rate": 0.0003, "weight_decay": 1e-06, "alpha": 0.05, "device": "cuda", "seed": 1234, "num_validation_episodes": 128, "validation_batch_size": 64, "generalization_penalty_weight": 1.0, "pool_version": "v0"}, "free_loss": {"f1_steps": 32, "f2_steps": 100, "f3_enabled": false}}, "loss_ir": {"name": "Rank-Gap Scaled Adaptive Margin Loss", "intuition": "This loss function synthesizes ideas from its parents to create a stable and adaptive preference learning objective. From Parent 0, it inherits the core structure of `softplus(margin - logp_diff)`, which provides a smooth, hinge-like penalty when the model's log-probability difference (`logp_diff`) does not meet a target margin. From Parent 1, it inherits the use of `tanh(softplus(...))` to create a bounded, normalized, and adaptive margin based on the cost difference, ensuring the target separation scales with the perceived preference strength. \n\nThe novel coupling idea is the introduction of a `rank_gap` term to dynamically scale the influence of the loss. The `rank_gap` operator computes a normalized, batch-aware measure of how significant the `cost_b - cost_a` difference is relative to other cost differences in the batch. This scalar, `rank_gap_scale`, is then multiplied by the core loss. This has two benefits: 1) It acts as a stability trick by down-weighting the loss contribution from pairs with very small or insignificant cost differences, which might be noisy. 2) It focuses the training signal on pairs where the cost-based preference is clear and unambiguous within the context of the current batch, promoting more efficient learning.", "hyperparams": {"beta": 0.1, "max_margin": 5.0}, "operators_used": ["softplus", "tanh", "rank_gap"]}}}
{"generation": 1, "index": 5, "ir": {"name": "Rank-Gap Scaled Sigmoid-Margin Loss", "intuition": "This loss function combines the adaptive margin concept from both parents with a novel batch-aware scaling mechanism to improve robustness and stability. \n\nInherited Ideas:\n- From Parent 0, we inherit the core structure of `softplus(margin - logp_diff)`, which provides a smooth, hinge-like penalty when the model's preference (`logp_diff`) does not meet the target margin.\n- From Parent 1, we inherit the use of a `tanh` function to create a bounded, normalized margin from the cost difference. This ensures the target margin is always between 0 and a maximum value, preventing extreme targets from large cost differences.\n\nNew Coupling Ideas:\n1. **Rank-Gap Scaling:** A new coupling idea is introduced where the `beta` hyperparameter, which controls the steepness of the margin function, is dynamically scaled by the `rank_gap` of the cost difference within the current batch. The `rank_gap` measures the percentile difference between `cost_a` and `cost_b` in the batch. This makes the loss less sensitive to the absolute scale of costs and more attuned to their relative ranking, providing a more stable learning signal across different problem instances or training stages.\n2. **Margin Clipping:** To further enhance stability, the computed adaptive margin is explicitly clamped to a safe range `[0, max_margin]`. This acts as a safeguard against potential numerical issues from the dynamic scaling, ensuring the margin remains well-behaved before being used in the final loss calculation.", "pseudocode": "1. Compute the cost difference: cost_diff = cost_b - cost_a.\n2. Compute the log-probability difference: logp_diff = logp_a - logp_b.\n3. Calculate a dynamic beta scale based on the relative ranking of costs in the batch. The rank_gap(cost_b, cost_a) function returns a value between 0 and 1 representing the percentile gap between the two costs.\n   dynamic_beta = beta * rank_gap(cost_b, cost_a).\n4. Create a bounded, adaptive margin. Apply the dynamic beta to the cost difference and pass it through a tanh function. This creates a value between -1 and 1. Scale it by max_margin.\n   raw_margin = max_margin * tanh(dynamic_beta * cost_diff).\n5. Clamp the margin to ensure it is non-negative and does not exceed the maximum, guaranteeing stability.\n   adaptive_margin = clamp(raw_margin, min=0.0, max=max_margin).\n6. The loss is the softplus of the margin minus the log-probability difference. This penalizes cases where the model's preference for 'a' is less than the target adaptive margin.\n   loss = softplus(adaptive_margin - logp_diff).", "hyperparams": {"beta": 1.0, "max_margin": 5.0}, "operators_used": ["rank_gap", "tanh", "clamp", "softplus"], "implementation_hint": {"expects": ["cost_a", "cost_b", "logp_a", "logp_b"], "returns": "scalar"}}, "fitness": {"hf_like_score": 3.8846170902252197, "validation_objective": 3.8846170902252197, "generalization_penalty": 0.0, "generalization_objectives": {"20": 3.851141571998596}, "train_score_mean": 4.208311483263969, "train_loss_mean": 0.6784999035298824, "pair_count": 4111865, "config": {"hf": {"problem": "tsp", "hf_steps": 32, "train_problem_size": 20, "valid_problem_sizes": [20], "train_batch_size": 64, "pomo_size": 64, "learning_rate": 0.0003, "weight_decay": 1e-06, "alpha": 0.05, "device": "cuda", "seed": 1234, "num_validation_episodes": 128, "validation_batch_size": 64, "generalization_penalty_weight": 1.0, "pool_version": "v0"}, "free_loss": {"f1_steps": 32, "f2_steps": 100, "f3_enabled": false}}, "loss_ir": {"name": "Rank-Gap Scaled Sigmoid-Margin Loss", "intuition": "This loss function combines the adaptive margin concept from both parents with a novel batch-aware scaling mechanism to improve robustness and stability. \n\nInherited Ideas:\n- From Parent 0, we inherit the core structure of `softplus(margin - logp_diff)`, which provides a smooth, hinge-like penalty when the model's preference (`logp_diff`) does not meet the target margin.\n- From Parent 1, we inherit the use of a `tanh` function to create a bounded, normalized margin from the cost difference. This ensures the target margin is always between 0 and a maximum value, preventing extreme targets from large cost differences.\n\nNew Coupling Ideas:\n1. **Rank-Gap Scaling:** A new coupling idea is introduced where the `beta` hyperparameter, which controls the steepness of the margin function, is dynamically scaled by the `rank_gap` of the cost difference within the current batch. The `rank_gap` measures the percentile difference between `cost_a` and `cost_b` in the batch. This makes the loss less sensitive to the absolute scale of costs and more attuned to their relative ranking, providing a more stable learning signal across different problem instances or training stages.\n2. **Margin Clipping:** To further enhance stability, the computed adaptive margin is explicitly clamped to a safe range `[0, max_margin]`. This acts as a safeguard against potential numerical issues from the dynamic scaling, ensuring the margin remains well-behaved before being used in the final loss calculation.", "hyperparams": {"beta": 1.0, "max_margin": 5.0}, "operators_used": ["rank_gap", "tanh", "clamp", "softplus"]}}}
{"generation": 1, "index": 6, "ir": {"name": "Rank-Normalized Margin Loss", "intuition": "This loss function combines the adaptive margin concepts from both parents with a novel rank-based normalization for robustness. \n\nInherited Ideas:\n- From Parent 0, it inherits the idea of using a sigmoid-scaled cost difference to create a bounded, adaptive margin. This ensures that the target separation between log-probabilities grows with the cost difference but saturates gracefully.\n- From Parent 1, it inherits the use of a temperature-scaled difference between the log-probability gap and the margin, which is then passed to a `logsigmoid` function. This provides a stable, bounded loss value that is sensitive to the magnitude of preference violations.\n\nNew Coupling Ideas:\n1.  **Rank-Gap Normalization:** Instead of using the raw cost difference, the function first normalizes the costs using `rank_gap`. This operator computes `rank_gap(cost_a, cost_b)`, which is the difference in the ranks of `cost_a` and `cost_b` within the batch, normalized by the batch size. This makes the margin robust to the absolute scale and distribution of costs, focusing instead on the relative ordering of solutions within the current batch. This prevents outliers with huge cost differences from dominating the loss signal.\n2.  **Margin Combination:** The final adaptive margin is a linear combination of this rank-gap-based margin and a small, fixed baseline margin (`min_margin`). This ensures that even for pairs with a zero rank gap (e.g., in a small batch), there is still a minimum separation target, preventing the loss from becoming zero when it shouldn't.", "pseudocode": "1. Given a batch of costs, compute the normalized rank gap between `cost_a` and `cost_b`. This value, `rank_diff`, is between -1 and 1 and represents the relative preference strength within the batch.\n   rank_diff = rank_gap(cost_a, cost_b).\n2. Create a rank-based adaptive margin. Pass the `rank_diff` through a sigmoid function to map it to a value between 0 and 1, then scale it by `max_margin`.\n   rank_margin = max_margin * sigmoid(rank_diff).\n3. Combine this with a small baseline margin to ensure a minimum separation target.\n   adaptive_margin = min_margin + rank_margin.\n4. Compute the log-probability difference: logp_diff = logp_a - logp_b.\n5. Calculate the core loss term. This is the difference between the log-probability gap and the adaptive margin, scaled by a temperature parameter.\n   core_term = (logp_diff - adaptive_margin) * temperature.\n6. The final loss is the logsigmoid of the negative core term. This is equivalent to `softplus(-core_term)` and penalizes cases where the `logp_diff` is smaller than the target `adaptive_margin`.\n   loss = -logsigmoid(core_term).", "hyperparams": {"temperature": 10.0, "max_margin": 5.0, "min_margin": 0.1}, "operators_used": ["rank_gap", "sigmoid", "logsigmoid"], "implementation_hint": {"expects": ["cost_a", "cost_b", "logp_a", "logp_b"], "returns": "scalar"}}, "fitness": {"hf_like_score": 3.8765450716018677, "validation_objective": 3.8765450716018677, "generalization_penalty": 0.0, "generalization_objectives": {"20": 3.849939465522766}, "train_score_mean": 4.2086643278598785, "train_loss_mean": 0.6782502476125956, "pair_count": 4111690, "config": {"hf": {"problem": "tsp", "hf_steps": 32, "train_problem_size": 20, "valid_problem_sizes": [20], "train_batch_size": 64, "pomo_size": 64, "learning_rate": 0.0003, "weight_decay": 1e-06, "alpha": 0.05, "device": "cuda", "seed": 1234, "num_validation_episodes": 128, "validation_batch_size": 64, "generalization_penalty_weight": 1.0, "pool_version": "v0"}, "free_loss": {"f1_steps": 32, "f2_steps": 100, "f3_enabled": false}}, "loss_ir": {"name": "Rank-Normalized Margin Loss", "intuition": "This loss function combines the adaptive margin concepts from both parents with a novel rank-based normalization for robustness. \n\nInherited Ideas:\n- From Parent 0, it inherits the idea of using a sigmoid-scaled cost difference to create a bounded, adaptive margin. This ensures that the target separation between log-probabilities grows with the cost difference but saturates gracefully.\n- From Parent 1, it inherits the use of a temperature-scaled difference between the log-probability gap and the margin, which is then passed to a `logsigmoid` function. This provides a stable, bounded loss value that is sensitive to the magnitude of preference violations.\n\nNew Coupling Ideas:\n1.  **Rank-Gap Normalization:** Instead of using the raw cost difference, the function first normalizes the costs using `rank_gap`. This operator computes `rank_gap(cost_a, cost_b)`, which is the difference in the ranks of `cost_a` and `cost_b` within the batch, normalized by the batch size. This makes the margin robust to the absolute scale and distribution of costs, focusing instead on the relative ordering of solutions within the current batch. This prevents outliers with huge cost differences from dominating the loss signal.\n2.  **Margin Combination:** The final adaptive margin is a linear combination of this rank-gap-based margin and a small, fixed baseline margin (`min_margin`). This ensures that even for pairs with a zero rank gap (e.g., in a small batch), there is still a minimum separation target, preventing the loss from becoming zero when it shouldn't.", "hyperparams": {"temperature": 10.0, "max_margin": 5.0, "min_margin": 0.1}, "operators_used": ["rank_gap", "sigmoid", "logsigmoid"]}}}
{"generation": 1, "index": 7, "ir": {"name": "Rank-Normalized Margin Loss", "intuition": "This loss function combines the adaptive margin concepts from both parents with a new rank-based normalization scheme for enhanced stability and batch-awareness. It inherits the core structure of `softplus(margin - logp_diff)` from Parent 0, which provides a smooth, hinge-like penalty. From Parent 1, it inherits the idea of using `tanh(softplus(...))` to create a bounded, normalized representation of the cost difference, which prevents extreme cost values from dominating the loss signal. The key innovation is a new coupling idea: the cost difference (`cost_b - cost_a`) is first normalized within a batch using `rank_gap`. This `rank_gap` operator transforms the raw cost differences into a stable, zero-centered distribution based on their relative ranks, making the margin less sensitive to the absolute scale of costs in a given batch. This rank-normalized value is then passed through the inherited `tanh(softplus(...))` structure to form a robust, adaptive margin. This design ensures that the model learns from the relative preference ordering within a batch, while the softplus and tanh functions guarantee numerical stability and a well-behaved loss landscape.", "pseudocode": "1. For a batch of preference pairs, compute the cost difference for each pair: cost_diff = cost_b - cost_a.\n2. Inherited Idea (Parent 1, modified): Normalize the batch of `cost_diff` values using rank-based scaling. The `rank_gap` operator computes the percentile difference for each `cost_diff`, resulting in a stable, zero-centered distribution. This is the new coupling idea for batch-aware normalization.\n   ranked_cost_diff = rank_gap(cost_diff).\n3. Inherited Idea (Parent 1): Transform the normalized cost difference into a bounded value between 0 and 1. This represents the 'preference strength' and is used to scale the maximum margin.\n   preference_strength = tanh(softplus(ranked_cost_diff)).\n4. Inherited Idea (Parent 0): Create the final adaptive margin by scaling a maximum margin value by the computed preference strength.\n   adaptive_margin = max_margin * preference_strength.\n5. Compute the log-probability difference: logp_diff = logp_a - logp_b.\n6. Inherited Idea (Parent 0): Calculate the final loss using a softplus function, which penalizes the model when the log-probability difference is smaller than the target adaptive margin. The loss is averaged over the batch.\n   loss = softplus(adaptive_margin - logp_diff).", "hyperparams": {"max_margin": 5.0}, "operators_used": ["rank_gap", "softplus", "tanh"], "implementation_hint": {"expects": ["cost_a", "cost_b", "logp_a", "logp_b"], "returns": "scalar"}}, "fitness": {"hf_like_score": 3.884966254234314, "validation_objective": 3.884966254234314, "generalization_penalty": 0.0, "generalization_objectives": {"20": 3.853423237800598}, "train_score_mean": 4.209609217941761, "train_loss_mean": 0.6782292146235704, "pair_count": 4112172, "config": {"hf": {"problem": "tsp", "hf_steps": 32, "train_problem_size": 20, "valid_problem_sizes": [20], "train_batch_size": 64, "pomo_size": 64, "learning_rate": 0.0003, "weight_decay": 1e-06, "alpha": 0.05, "device": "cuda", "seed": 1234, "num_validation_episodes": 128, "validation_batch_size": 64, "generalization_penalty_weight": 1.0, "pool_version": "v0"}, "free_loss": {"f1_steps": 32, "f2_steps": 100, "f3_enabled": false}}, "loss_ir": {"name": "Rank-Normalized Margin Loss", "intuition": "This loss function combines the adaptive margin concepts from both parents with a new rank-based normalization scheme for enhanced stability and batch-awareness. It inherits the core structure of `softplus(margin - logp_diff)` from Parent 0, which provides a smooth, hinge-like penalty. From Parent 1, it inherits the idea of using `tanh(softplus(...))` to create a bounded, normalized representation of the cost difference, which prevents extreme cost values from dominating the loss signal. The key innovation is a new coupling idea: the cost difference (`cost_b - cost_a`) is first normalized within a batch using `rank_gap`. This `rank_gap` operator transforms the raw cost differences into a stable, zero-centered distribution based on their relative ranks, making the margin less sensitive to the absolute scale of costs in a given batch. This rank-normalized value is then passed through the inherited `tanh(softplus(...))` structure to form a robust, adaptive margin. This design ensures that the model learns from the relative preference ordering within a batch, while the softplus and tanh functions guarantee numerical stability and a well-behaved loss landscape.", "hyperparams": {"max_margin": 5.0}, "operators_used": ["rank_gap", "softplus", "tanh"]}}}
{"generation": 2, "index": 0, "ir": {"name": "Rank-Gated Margin Loss", "intuition": "This loss function creates a dynamic, adaptive margin that is gated by the significance of the cost difference within a batch, combining robustness from rank normalization with a stable loss formulation.\n\nInherited Ideas:\n- From Parent 0, it inherits the core loss structure `-logsigmoid(temperature * (logp_diff - adaptive_margin))`. This provides a stable, bounded loss that penalizes the model when the log-probability difference (`logp_a - logp_b`) is less than the target margin.\n- From Parent 1, it inherits the idea of using `tanh` to create a smoothly saturating margin based on a normalized cost difference. This prevents the margin from growing uncontrollably and adds stability.\n\nNew Coupling Ideas:\n1. **Rank-Gated Margin**: Instead of directly using the rank-normalized cost difference to set the margin, it is first used to compute a 'gate' value via `sigmoid(beta * rank_gap(cost_a, cost_b))`. This gate, ranging from 0 to 1, represents the confidence or significance of the preference. A small rank difference results in a gate value near 0.5, while a large difference pushes it towards 1. This gate then multiplicatively scales a fixed `max_margin`. The result is that pairs with a negligible cost difference (small rank gap) are assigned a small margin, preventing the model from being forced to distinguish between them, while pairs with a clear cost difference are assigned a large margin.\n2. **Dynamic Temperature Scaling**: The temperature parameter, which controls the steepness of the loss, is dynamically scaled by the same rank-based gate. When the rank gap is small, the temperature is reduced, softening the penalty for small preference violations. When the rank gap is large and the preference is clear, the temperature is increased, creating a sharper, more confident loss signal. This couples the loss's sensitivity directly to the significance of the preference itself.", "pseudocode": "1. Compute the log-probability difference: logp_diff = logp_a - logp_b.\n2. Calculate the rank-normalized cost gap using the `rank_gap` operator on the costs of the current batch. This value, `rank_diff`, is typically in [-1, 1].\n   rank_diff = rank_gap(cost_a, cost_b).\n3. Compute a 'preference significance' gate using the sigmoid function. This gate value approaches 1 for large rank differences and is near 0.5 for small ones.\n   gate = sigmoid(beta * rank_diff).\n4. Calculate the adaptive margin by scaling `max_margin` by the gate. This makes the margin proportional to the significance of the preference.\n   adaptive_margin = max_margin * gate.\n5. Compute the dynamic temperature by scaling the base `temperature` by the same gate. This makes the loss steeper for more significant preferences.\n   dynamic_temp = temperature * gate.\n6. Calculate the core loss term using the adaptive margin and dynamic temperature.\n   core_term = dynamic_temp * (logp_diff - adaptive_margin).\n7. The final loss is the negative logsigmoid of the core term, which is a stable way to penalize cases where `logp_diff` is smaller than the target `adaptive_margin`.\n   loss = -logsigmoid(core_term).", "hyperparams": {"beta": 2.0, "max_margin": 5.0, "temperature": 1.0}, "operators_used": ["rank_gap", "sigmoid", "logsigmoid"], "implementation_hint": {"expects": ["cost_a", "cost_b", "logp_a", "logp_b"], "returns": "scalar"}}, "fitness": {"hf_like_score": 3.8841025829315186, "validation_objective": 3.8841025829315186, "generalization_penalty": 0.0, "generalization_objectives": {"20": 3.8554853200912476}, "train_score_mean": 4.2090586721897125, "train_loss_mean": 0.6782295871526003, "pair_count": 4111371, "config": {"hf": {"problem": "tsp", "hf_steps": 32, "train_problem_size": 20, "valid_problem_sizes": [20], "train_batch_size": 64, "pomo_size": 64, "learning_rate": 0.0003, "weight_decay": 1e-06, "alpha": 0.05, "device": "cuda", "seed": 1234, "num_validation_episodes": 128, "validation_batch_size": 64, "generalization_penalty_weight": 1.0, "pool_version": "v0"}, "free_loss": {"f1_steps": 32, "f2_steps": 100, "f3_enabled": false}}, "loss_ir": {"name": "Rank-Gated Margin Loss", "intuition": "This loss function creates a dynamic, adaptive margin that is gated by the significance of the cost difference within a batch, combining robustness from rank normalization with a stable loss formulation.\n\nInherited Ideas:\n- From Parent 0, it inherits the core loss structure `-logsigmoid(temperature * (logp_diff - adaptive_margin))`. This provides a stable, bounded loss that penalizes the model when the log-probability difference (`logp_a - logp_b`) is less than the target margin.\n- From Parent 1, it inherits the idea of using `tanh` to create a smoothly saturating margin based on a normalized cost difference. This prevents the margin from growing uncontrollably and adds stability.\n\nNew Coupling Ideas:\n1. **Rank-Gated Margin**: Instead of directly using the rank-normalized cost difference to set the margin, it is first used to compute a 'gate' value via `sigmoid(beta * rank_gap(cost_a, cost_b))`. This gate, ranging from 0 to 1, represents the confidence or significance of the preference. A small rank difference results in a gate value near 0.5, while a large difference pushes it towards 1. This gate then multiplicatively scales a fixed `max_margin`. The result is that pairs with a negligible cost difference (small rank gap) are assigned a small margin, preventing the model from being forced to distinguish between them, while pairs with a clear cost difference are assigned a large margin.\n2. **Dynamic Temperature Scaling**: The temperature parameter, which controls the steepness of the loss, is dynamically scaled by the same rank-based gate. When the rank gap is small, the temperature is reduced, softening the penalty for small preference violations. When the rank gap is large and the preference is clear, the temperature is increased, creating a sharper, more confident loss signal. This couples the loss's sensitivity directly to the significance of the preference itself.", "hyperparams": {"beta": 2.0, "max_margin": 5.0, "temperature": 1.0}, "operators_used": ["rank_gap", "sigmoid", "logsigmoid"]}}}
{"generation": 2, "index": 1, "ir": {"name": "Z-Scored Rank-Modulated Hinge Loss", "intuition": "This loss function creates a robust preference learning signal by combining rank-based margins with z-score normalization for stability.\n\nInherited Ideas:\n- From Parent 0, it inherits the use of a temperature-scaled difference between the log-probability gap and a margin, which is then passed to a `logsigmoid` function. This provides a stable, bounded loss value that is sensitive to the magnitude of preference violations.\n- From Parent 1, it inherits the idea of using `softplus` to create a smooth hinge loss, but it's applied to the margin itself rather than the final loss. It also inherits the concept of scaling a normalized cost difference to form an adaptive margin.\n\nNew Coupling Ideas:\n1. **Z-Score Normalization of Log-Probabilities**: Before computing the log-probability difference, the log-probabilities of all items in the batch (`logp_a` and `logp_b`) are standardized using `zscore`. This centers the log-probability distribution around zero with a standard deviation of one. This normalization makes the loss less sensitive to the absolute scale or shift of the model's output logits, focusing purely on the relative separation and preventing large logit values from causing numerical instability.\n2. **Softplus-Smoothed Rank Margin**: The margin is constructed by first calculating the `rank_gap` between `cost_a` and `cost_b`. Instead of using this directly or with a `tanh`/`sigmoid` function, it is passed through a `softplus` function. This creates a margin that is always non-negative and grows smoothly (but non-linearly) with the rank difference, providing a stronger separation signal for clearer preferences while compressing the signal for small rank differences.", "pseudocode": "1. Given the batch of log-probabilities, compute the z-score normalized values for `logp_a` and `logp_b`.\n   z_logp_a = zscore(logp_a)\n   z_logp_b = zscore(logp_b)\n2. Compute the normalized log-probability difference.\n   logp_diff = z_logp_a - z_logp_b.\n3. Calculate the rank-normalized cost gap using the `rank_gap` operator on the costs of the current batch. This transforms the raw cost difference into a stable, rank-based value.\n   normalized_cost_gap = rank_gap(cost_a, cost_b).\n4. Create a smooth, non-negative adaptive margin. Apply the `softplus` function to the scaled normalized cost gap.\n   adaptive_margin = softplus(beta * normalized_cost_gap).\n5. Calculate the core loss term. This is the difference between the log-probability gap and the adaptive margin, scaled by a temperature parameter.\n   core_term = (logp_diff - adaptive_margin) * temperature.\n6. The final loss is the logsigmoid of the negative core term, which is equivalent to `softplus(-core_term)`. This penalizes cases where the normalized `logp_diff` is smaller than the target `adaptive_margin`.\n   loss = -logsigmoid(core_term).", "hyperparams": {"beta": 1.0, "temperature": 1.0}, "operators_used": ["zscore", "rank_gap", "softplus", "logsigmoid"], "implementation_hint": {"expects": ["cost_a", "cost_b", "logp_a", "logp_b"], "returns": "scalar"}}, "fitness": {"hf_like_score": 3.878625750541687, "validation_objective": 3.878625750541687, "generalization_penalty": 0.0, "generalization_objectives": {"20": 3.847885847091675}, "train_score_mean": 4.2101586908102036, "train_loss_mean": 0.6783098597079515, "pair_count": 4112001, "config": {"hf": {"problem": "tsp", "hf_steps": 32, "train_problem_size": 20, "valid_problem_sizes": [20], "train_batch_size": 64, "pomo_size": 64, "learning_rate": 0.0003, "weight_decay": 1e-06, "alpha": 0.05, "device": "cuda", "seed": 1234, "num_validation_episodes": 128, "validation_batch_size": 64, "generalization_penalty_weight": 1.0, "pool_version": "v0"}, "free_loss": {"f1_steps": 32, "f2_steps": 100, "f3_enabled": false}}, "loss_ir": {"name": "Z-Scored Rank-Modulated Hinge Loss", "intuition": "This loss function creates a robust preference learning signal by combining rank-based margins with z-score normalization for stability.\n\nInherited Ideas:\n- From Parent 0, it inherits the use of a temperature-scaled difference between the log-probability gap and a margin, which is then passed to a `logsigmoid` function. This provides a stable, bounded loss value that is sensitive to the magnitude of preference violations.\n- From Parent 1, it inherits the idea of using `softplus` to create a smooth hinge loss, but it's applied to the margin itself rather than the final loss. It also inherits the concept of scaling a normalized cost difference to form an adaptive margin.\n\nNew Coupling Ideas:\n1. **Z-Score Normalization of Log-Probabilities**: Before computing the log-probability difference, the log-probabilities of all items in the batch (`logp_a` and `logp_b`) are standardized using `zscore`. This centers the log-probability distribution around zero with a standard deviation of one. This normalization makes the loss less sensitive to the absolute scale or shift of the model's output logits, focusing purely on the relative separation and preventing large logit values from causing numerical instability.\n2. **Softplus-Smoothed Rank Margin**: The margin is constructed by first calculating the `rank_gap` between `cost_a` and `cost_b`. Instead of using this directly or with a `tanh`/`sigmoid` function, it is passed through a `softplus` function. This creates a margin that is always non-negative and grows smoothly (but non-linearly) with the rank difference, providing a stronger separation signal for clearer preferences while compressing the signal for small rank differences.", "hyperparams": {"beta": 1.0, "temperature": 1.0}, "operators_used": ["zscore", "rank_gap", "softplus", "logsigmoid"]}}}
{"generation": 2, "index": 2, "ir": {"name": "Soft-Clipped Rank-Gap Loss", "intuition": "This loss function creates a dynamic margin based on both relative rank and log-probability differences, aiming for a stable and adaptive preference signal.\n\nInherited Ideas:\n- From both parents (Parent 0 and 1), it inherits the core idea of using `rank_gap` to normalize the cost difference. This makes the margin robust to the absolute scale of costs and focuses on the relative preference ordering within a batch.\n- From Parent 1, it inherits the use of `softplus` to create a smooth, one-sided penalty. The structure `softplus(margin - logp_diff)` is used to penalize cases where the log-probability difference does not meet the target margin.\n\nNew Coupling Ideas:\n1. **Log-Probability Clipping:** Instead of using the raw log-probability difference (`logp_a - logp_b`), it is first clipped to a symmetric range `[-clip_val, clip_val]` using the `clamp` operator. This prevents extremely confident or unconfident predictions from creating excessively large or small `logp_diff` values, which can lead to vanishing or exploding gradients and destabilize training. This acts as a gradient stabilization trick.\n2. **Soft-Margin Coupling:** The margin is dynamically calculated as the `softplus` of the scaled rank-gap. This creates a margin that is always positive and grows non-linearly with the rank difference, but does so smoothly without a hard upper bound like `tanh` or `sigmoid`. This allows the margin to adapt to the strength of preference while avoiding the hard saturation points that can stop learning for well-separated pairs.", "pseudocode": "1. Compute the log-probability difference: logp_diff_raw = logp_a - logp_b.\n2. Stabilize the log-probability difference by clipping it to a predefined range, preventing extreme values.\n   logp_diff = clamp(logp_diff_raw, -clip_val, clip_val).\n3. Calculate the rank-normalized cost gap using the `rank_gap` operator. This provides a stable measure of preference strength based on batch-wise ordering.\n   normalized_cost_gap = rank_gap(cost_a, cost_b).\n4. Create a dynamic, non-negative margin. Apply the `softplus` function to the rank gap, scaled by a temperature parameter `beta`. This ensures the margin is always positive and grows smoothly with the rank difference.\n   adaptive_margin = softplus(beta * normalized_cost_gap).\n5. The final loss is the softplus of the difference between the adaptive margin and the clipped log-probability difference. This penalizes the model when its (stabilized) log-probability gap is smaller than the target margin.\n   loss = softplus(adaptive_margin - logp_diff).", "hyperparams": {"beta": 1.0, "clip_val": 10.0}, "operators_used": ["clamp", "rank_gap", "softplus"], "implementation_hint": {"expects": ["cost_a", "cost_b", "logp_a", "logp_b"], "returns": "scalar"}}, "fitness": {"hf_like_score": 3.878672242164612, "validation_objective": 3.878672242164612, "generalization_penalty": 0.0, "generalization_objectives": {"20": 3.849644184112549}, "train_score_mean": 4.2082496881484985, "train_loss_mean": 0.678372947499156, "pair_count": 4110485, "config": {"hf": {"problem": "tsp", "hf_steps": 32, "train_problem_size": 20, "valid_problem_sizes": [20], "train_batch_size": 64, "pomo_size": 64, "learning_rate": 0.0003, "weight_decay": 1e-06, "alpha": 0.05, "device": "cuda", "seed": 1234, "num_validation_episodes": 128, "validation_batch_size": 64, "generalization_penalty_weight": 1.0, "pool_version": "v0"}, "free_loss": {"f1_steps": 32, "f2_steps": 100, "f3_enabled": false}}, "loss_ir": {"name": "Soft-Clipped Rank-Gap Loss", "intuition": "This loss function creates a dynamic margin based on both relative rank and log-probability differences, aiming for a stable and adaptive preference signal.\n\nInherited Ideas:\n- From both parents (Parent 0 and 1), it inherits the core idea of using `rank_gap` to normalize the cost difference. This makes the margin robust to the absolute scale of costs and focuses on the relative preference ordering within a batch.\n- From Parent 1, it inherits the use of `softplus` to create a smooth, one-sided penalty. The structure `softplus(margin - logp_diff)` is used to penalize cases where the log-probability difference does not meet the target margin.\n\nNew Coupling Ideas:\n1. **Log-Probability Clipping:** Instead of using the raw log-probability difference (`logp_a - logp_b`), it is first clipped to a symmetric range `[-clip_val, clip_val]` using the `clamp` operator. This prevents extremely confident or unconfident predictions from creating excessively large or small `logp_diff` values, which can lead to vanishing or exploding gradients and destabilize training. This acts as a gradient stabilization trick.\n2. **Soft-Margin Coupling:** The margin is dynamically calculated as the `softplus` of the scaled rank-gap. This creates a margin that is always positive and grows non-linearly with the rank difference, but does so smoothly without a hard upper bound like `tanh` or `sigmoid`. This allows the margin to adapt to the strength of preference while avoiding the hard saturation points that can stop learning for well-separated pairs.", "hyperparams": {"beta": 1.0, "clip_val": 10.0}, "operators_used": ["clamp", "rank_gap", "softplus"]}}}
{"generation": 2, "index": 3, "ir": {"name": "Adaptive Rank-Gap Hinge Loss", "intuition": "This loss function creates a dynamic, hinge-like penalty that is robust to cost outliers and sensitive to both relative preference strength and the magnitude of preference violation.\n\nInherited Ideas:\n- From Parent 0, it inherits the core structure of using `logsigmoid` applied to a scaled difference term (`-logsigmoid(temperature * (logp_diff - margin))`). This provides a stable, bounded loss that penalizes the model when the log-probability difference (`logp_a - logp_b`) is less than the target margin.\n- From Parent 1, it inherits the use of `tanh` to create a smooth, bounded, and saturating margin from a normalized cost difference. This prevents the margin from growing indefinitely and adds stability.\n\nNew Coupling Ideas:\n1. **Z-Score Rank-Gap Coupling**: Instead of using the raw `rank_gap` directly, this loss first computes the `rank_gap` for all pairs in the batch and then standardizes these values using `zscore`. This `zscore(rank_gap(cost_a, cost_b))` operation centers the rank-based preference signal around zero with a standard deviation of one across the batch. This coupling makes the subsequent `tanh` activation more effective by ensuring its input is well-distributed, preventing it from consistently operating in its saturated regions, thereby maintaining a responsive gradient signal.\n2. **Dynamic Temperature Scaling**: The `temperature` parameter, which controls the steepness of the loss curve, is made dynamic. It is scaled by the absolute value of the log-probability difference, `abs(logp_a - logp_b)`. This means that when the model is very uncertain (log-probabilities are close), the temperature is low, resulting in a gentler loss. Conversely, when the model makes a confident but incorrect prediction, the temperature is high, creating a stronger corrective penalty. This focuses the training signal where it is most needed.", "pseudocode": "1. For each pair in the batch, compute the log-probability difference: logp_diff = logp_a - logp_b.\n2. Compute the rank-based cost gap for all pairs in the batch: batch_rank_gaps = rank_gap(all_costs_a, all_costs_b).\n3. Standardize the rank gaps across the batch using z-score normalization to get a well-distributed signal: z_scored_gap = zscore(batch_rank_gaps).\n4. Create a robust, adaptive margin by passing the z-scored gap through a `tanh` function and scaling it. This creates a smooth, bounded target separation.\n   adaptive_margin = max_margin * tanh(beta * z_scored_gap).\n5. Create a dynamic temperature that scales with the magnitude of the log-probability difference. A small offset `epsilon` ensures stability.\n   dynamic_temp = base_temperature * (epsilon + abs(logp_diff)).\n6. Calculate the core loss term. This is the difference between the log-probability gap and the adaptive margin, scaled by the dynamic temperature.\n   core_term = (logp_diff - adaptive_margin) * dynamic_temp.\n7. The final loss is the negative logsigmoid of the core term. This is a numerically stable way of computing `softplus(-core_term)` and penalizes cases where `logp_diff` is smaller than the `adaptive_margin`.\n   loss = -logsigmoid(core_term).", "hyperparams": {"beta": 1.0, "max_margin": 3.0, "base_temperature": 5.0, "epsilon": 0.01}, "operators_used": ["logsigmoid", "tanh", "rank_gap", "zscore"], "implementation_hint": {"expects": ["cost_a", "cost_b", "logp_a", "logp_b"], "returns": "scalar"}}, "fitness": {"hf_like_score": 3.877833127975464, "validation_objective": 3.877833127975464, "generalization_penalty": 0.0, "generalization_objectives": {"20": 3.8514294624328613}, "train_score_mean": 4.208394035696983, "train_loss_mean": 0.6781632918864489, "pair_count": 4111224, "config": {"hf": {"problem": "tsp", "hf_steps": 32, "train_problem_size": 20, "valid_problem_sizes": [20], "train_batch_size": 64, "pomo_size": 64, "learning_rate": 0.0003, "weight_decay": 1e-06, "alpha": 0.05, "device": "cuda", "seed": 1234, "num_validation_episodes": 128, "validation_batch_size": 64, "generalization_penalty_weight": 1.0, "pool_version": "v0"}, "free_loss": {"f1_steps": 32, "f2_steps": 100, "f3_enabled": false}}, "loss_ir": {"name": "Adaptive Rank-Gap Hinge Loss", "intuition": "This loss function creates a dynamic, hinge-like penalty that is robust to cost outliers and sensitive to both relative preference strength and the magnitude of preference violation.\n\nInherited Ideas:\n- From Parent 0, it inherits the core structure of using `logsigmoid` applied to a scaled difference term (`-logsigmoid(temperature * (logp_diff - margin))`). This provides a stable, bounded loss that penalizes the model when the log-probability difference (`logp_a - logp_b`) is less than the target margin.\n- From Parent 1, it inherits the use of `tanh` to create a smooth, bounded, and saturating margin from a normalized cost difference. This prevents the margin from growing indefinitely and adds stability.\n\nNew Coupling Ideas:\n1. **Z-Score Rank-Gap Coupling**: Instead of using the raw `rank_gap` directly, this loss first computes the `rank_gap` for all pairs in the batch and then standardizes these values using `zscore`. This `zscore(rank_gap(cost_a, cost_b))` operation centers the rank-based preference signal around zero with a standard deviation of one across the batch. This coupling makes the subsequent `tanh` activation more effective by ensuring its input is well-distributed, preventing it from consistently operating in its saturated regions, thereby maintaining a responsive gradient signal.\n2. **Dynamic Temperature Scaling**: The `temperature` parameter, which controls the steepness of the loss curve, is made dynamic. It is scaled by the absolute value of the log-probability difference, `abs(logp_a - logp_b)`. This means that when the model is very uncertain (log-probabilities are close), the temperature is low, resulting in a gentler loss. Conversely, when the model makes a confident but incorrect prediction, the temperature is high, creating a stronger corrective penalty. This focuses the training signal where it is most needed.", "hyperparams": {"beta": 1.0, "max_margin": 3.0, "base_temperature": 5.0, "epsilon": 0.01}, "operators_used": ["logsigmoid", "tanh", "rank_gap", "zscore"]}}}
{"generation": 2, "index": 4, "ir": {"name": "Adaptive Rank-Gap Hinge Loss", "intuition": "This loss function creates a robust preference learning objective by combining rank-based normalization with a dynamic, self-adjusting margin, ensuring stability and sensitivity to preference violations.\n\nInherited Ideas:\n- From Parent 0, it inherits the core structure of `-logsigmoid((logp_diff - margin) * temperature)`, which provides a stable, bounded loss that strongly penalizes cases where the log-probability difference (`logp_a - logp_b`) is smaller than the target margin.\n- From Parent 1, it inherits the idea of using `tanh` to create a smooth, bounded, and saturating margin from a normalized cost difference. This prevents the margin from growing uncontrollably.\n- From both parents, it inherits the use of `rank_gap` to normalize the cost difference. This makes the margin robust to outliers and varying cost scales by focusing on the relative ranking of solutions within a batch.\n\nNew Coupling Ideas:\n1. **Dynamic Margin Scaling:** Instead of a fixed `max_margin` hyperparameter, the margin is scaled by the standard deviation of the log-probability differences within the batch, plus a small constant `epsilon` for stability (`std_logp_diff`). This allows the margin's scale to dynamically adapt to the current model's confidence. If the model is producing highly varied log-probabilities (high `std_logp_diff`), the target margin increases, pushing for greater separation. If the model is less certain (low `std_logp_diff`), the margin shrinks, providing a gentler learning signal.\n2. **Margin Baseline with `softplus`:** A small baseline margin is added, computed as `softplus(min_margin)`. This ensures the margin is always strictly positive and avoids potential issues with a zero or negative margin, while maintaining differentiability and smoothness.", "pseudocode": "1. For a batch of pairs, compute the log-probability difference for each pair: logp_diff = logp_a - logp_b.\n2. Calculate the rank-normalized cost gap using the `rank_gap` operator on the costs of the current batch. This transforms the raw cost difference into a stable, rank-based value.\n   normalized_cost_gap = rank_gap(cost_a, cost_b).\n3. Compute the standard deviation of all `logp_diff` values in the batch: std_logp_diff.\n4. Create a dynamic scale for the margin by adding a small constant `epsilon` to the standard deviation for numerical stability.\n   dynamic_scale = std_logp_diff + epsilon.\n5. Construct the adaptive margin. First, pass the normalized cost gap through a `tanh` function. Then, scale this by the `dynamic_scale` and add a small, smooth baseline margin derived from `softplus`.\n   adaptive_margin = (dynamic_scale * tanh(normalized_cost_gap)) + softplus(min_margin).\n6. Calculate the core loss term. This is the difference between the log-probability gap and the adaptive margin, scaled by a temperature parameter.\n   core_term = (logp_diff - adaptive_margin) * temperature.\n7. The final loss is the negative logsigmoid of the core term. This penalizes cases where `logp_diff` is smaller than the target `adaptive_margin`.\n   loss = -logsigmoid(core_term).", "hyperparams": {"temperature": 10.0, "min_margin": 0.01, "epsilon": 1e-06}, "operators_used": ["logsigmoid", "tanh", "rank_gap", "softplus"], "implementation_hint": {"expects": ["cost_a", "cost_b", "logp_a", "logp_b"], "returns": "scalar"}}, "fitness": {"hf_like_score": 3.8782838582992554, "validation_objective": 3.8782838582992554, "generalization_penalty": 0.0, "generalization_objectives": {"20": 3.8495839834213257}, "train_score_mean": 4.209284916520119, "train_loss_mean": 0.6781736817210913, "pair_count": 4112055, "config": {"hf": {"problem": "tsp", "hf_steps": 32, "train_problem_size": 20, "valid_problem_sizes": [20], "train_batch_size": 64, "pomo_size": 64, "learning_rate": 0.0003, "weight_decay": 1e-06, "alpha": 0.05, "device": "cuda", "seed": 1234, "num_validation_episodes": 128, "validation_batch_size": 64, "generalization_penalty_weight": 1.0, "pool_version": "v0"}, "free_loss": {"f1_steps": 32, "f2_steps": 100, "f3_enabled": false}}, "loss_ir": {"name": "Adaptive Rank-Gap Hinge Loss", "intuition": "This loss function creates a robust preference learning objective by combining rank-based normalization with a dynamic, self-adjusting margin, ensuring stability and sensitivity to preference violations.\n\nInherited Ideas:\n- From Parent 0, it inherits the core structure of `-logsigmoid((logp_diff - margin) * temperature)`, which provides a stable, bounded loss that strongly penalizes cases where the log-probability difference (`logp_a - logp_b`) is smaller than the target margin.\n- From Parent 1, it inherits the idea of using `tanh` to create a smooth, bounded, and saturating margin from a normalized cost difference. This prevents the margin from growing uncontrollably.\n- From both parents, it inherits the use of `rank_gap` to normalize the cost difference. This makes the margin robust to outliers and varying cost scales by focusing on the relative ranking of solutions within a batch.\n\nNew Coupling Ideas:\n1. **Dynamic Margin Scaling:** Instead of a fixed `max_margin` hyperparameter, the margin is scaled by the standard deviation of the log-probability differences within the batch, plus a small constant `epsilon` for stability (`std_logp_diff`). This allows the margin's scale to dynamically adapt to the current model's confidence. If the model is producing highly varied log-probabilities (high `std_logp_diff`), the target margin increases, pushing for greater separation. If the model is less certain (low `std_logp_diff`), the margin shrinks, providing a gentler learning signal.\n2. **Margin Baseline with `softplus`:** A small baseline margin is added, computed as `softplus(min_margin)`. This ensures the margin is always strictly positive and avoids potential issues with a zero or negative margin, while maintaining differentiability and smoothness.", "hyperparams": {"temperature": 10.0, "min_margin": 0.01, "epsilon": 1e-06}, "operators_used": ["logsigmoid", "tanh", "rank_gap", "softplus"]}}}
{"generation": 2, "index": 5, "ir": {"name": "Z-Scored Rank-Adaptive Hinge Loss", "intuition": "This loss function creates a robust, adaptive margin by combining rank-based and distribution-based normalization, and applies it within a stable hinge-loss framework.\n\nInherited Ideas:\n- From Parent 0, it inherits the core structure of using `logsigmoid` on a temperature-scaled term. Specifically, the loss is `-logsigmoid(temperature * (logp_diff - margin))`, which is equivalent to `softplus(temperature * (margin - logp_diff))`. This provides a smooth, bounded, and stable hinge-like loss.\n- From Parent 1, it inherits the use of `tanh` to create a smoothly saturating margin from a normalized cost signal. This prevents the target margin from growing uncontrollably, which enhances numerical stability, especially when scaled by `max_margin`.\n\nNew Coupling Ideas:\n1. **Z-Score Cost Normalization**: Before computing the rank gap, the costs for the entire batch are first normalized using a z-score transformation (`zscore(costs)`). This centers the cost distribution around zero and scales it by its standard deviation. This pre-processing step makes the subsequent rank-gap calculation more stable and less sensitive to the absolute scale or shifts in the cost distribution, focusing purely on the relative differences within the batch's statistical profile.\n2. **Hybrid Margin Signal**: The final margin is derived from a combination of two normalized signals. The primary signal comes from the `rank_gap` of the z-scored costs, which captures the relative ordering. This is then passed through `tanh` as in Parent 1. This ensures the margin is robust to cost outliers and focuses on preference order. The result is a highly stabilized margin that adapts to both the rank and the statistical distribution of costs in the batch.", "pseudocode": "1. Given costs for a batch, normalize them using z-scoring: z_cost_a = zscore(cost_a), z_cost_b = zscore(cost_b).\n2. Compute the rank-normalized gap between the z-scored costs. This provides a stable signal of relative preference within the batch, typically in [-1, 1].\n   rank_diff = rank_gap(z_cost_a, z_cost_b).\n3. Create an adaptive margin by passing the rank difference through a `tanh` function and scaling it. This creates a smooth, bounded target separation for the log-probabilities.\n   adaptive_margin = max_margin * tanh(beta * rank_diff).\n4. Compute the log-probability difference: logp_diff = logp_a - logp_b.\n5. Calculate the core loss term. This is the difference between the log-probability gap and the adaptive margin, scaled by a temperature parameter.\n   core_term = temperature * (logp_diff - adaptive_margin).\n6. The final loss is the negative logsigmoid of the core term. This penalizes cases where `logp_diff` is smaller than the target `adaptive_margin` and is equivalent to `softplus(-core_term)` for numerical stability.\n   loss = -logsigmoid(core_term).", "hyperparams": {"beta": 1.0, "max_margin": 3.0, "temperature": 5.0}, "operators_used": ["zscore", "rank_gap", "tanh", "logsigmoid"], "implementation_hint": {"expects": ["cost_a", "cost_b", "logp_a", "logp_b"], "returns": "scalar"}}, "fitness": {"hf_like_score": 3.8758625984191895, "validation_objective": 3.8758625984191895, "generalization_penalty": 0.0, "generalization_objectives": {"20": 3.857212781906128}, "train_score_mean": 4.207844980061054, "train_loss_mean": 0.6783118043094873, "pair_count": 4112569, "config": {"hf": {"problem": "tsp", "hf_steps": 32, "train_problem_size": 20, "valid_problem_sizes": [20], "train_batch_size": 64, "pomo_size": 64, "learning_rate": 0.0003, "weight_decay": 1e-06, "alpha": 0.05, "device": "cuda", "seed": 1234, "num_validation_episodes": 128, "validation_batch_size": 64, "generalization_penalty_weight": 1.0, "pool_version": "v0"}, "free_loss": {"f1_steps": 32, "f2_steps": 100, "f3_enabled": false}}, "loss_ir": {"name": "Z-Scored Rank-Adaptive Hinge Loss", "intuition": "This loss function creates a robust, adaptive margin by combining rank-based and distribution-based normalization, and applies it within a stable hinge-loss framework.\n\nInherited Ideas:\n- From Parent 0, it inherits the core structure of using `logsigmoid` on a temperature-scaled term. Specifically, the loss is `-logsigmoid(temperature * (logp_diff - margin))`, which is equivalent to `softplus(temperature * (margin - logp_diff))`. This provides a smooth, bounded, and stable hinge-like loss.\n- From Parent 1, it inherits the use of `tanh` to create a smoothly saturating margin from a normalized cost signal. This prevents the target margin from growing uncontrollably, which enhances numerical stability, especially when scaled by `max_margin`.\n\nNew Coupling Ideas:\n1. **Z-Score Cost Normalization**: Before computing the rank gap, the costs for the entire batch are first normalized using a z-score transformation (`zscore(costs)`). This centers the cost distribution around zero and scales it by its standard deviation. This pre-processing step makes the subsequent rank-gap calculation more stable and less sensitive to the absolute scale or shifts in the cost distribution, focusing purely on the relative differences within the batch's statistical profile.\n2. **Hybrid Margin Signal**: The final margin is derived from a combination of two normalized signals. The primary signal comes from the `rank_gap` of the z-scored costs, which captures the relative ordering. This is then passed through `tanh` as in Parent 1. This ensures the margin is robust to cost outliers and focuses on preference order. The result is a highly stabilized margin that adapts to both the rank and the statistical distribution of costs in the batch.", "hyperparams": {"beta": 1.0, "max_margin": 3.0, "temperature": 5.0}, "operators_used": ["zscore", "rank_gap", "tanh", "logsigmoid"]}}}
{"generation": 2, "index": 6, "ir": {"name": "Z-Score Scaled Rank-Margin Loss", "intuition": "This loss function creates a robust, adaptive margin by combining rank-based and distribution-aware normalization techniques.\n\nInherited Ideas:\n- From Parent 0, it inherits the core loss structure `-logsigmoid(temperature * (logp_diff - margin))`. This provides a stable, bounded loss that is sensitive to the magnitude of preference violations.\n- From both parents (Parent 0 and Parent 1), it inherits the concept of using a rank-based normalization (`rank_gap`) of the costs. This makes the margin robust to the absolute scale and distribution of costs, focusing instead on the relative ordering of solutions within the current batch.\n\nNew Coupling Ideas:\n1. **Z-Score Normalization of Log-Probabilities**: Before computing the log-probability difference, the log-probabilities of all items in the batch (`logp_a` and `logp_b`) are z-score normalized. This stabilizes the `logp_diff` term by centering it around zero and scaling it by the standard deviation of log-probabilities in the batch. This prevents exploding gradients and makes the loss less sensitive to the overall scale of log-probabilities, which can vary during training.\n2. **Hybrid Margin with `softplus`**: The adaptive margin is created by first calculating a rank-based gap (`rank_gap(cost_a, cost_b)`). Instead of using `sigmoid` or `tanh` which saturate, this rank gap is passed through a `softplus` function. This creates a non-negative, non-saturating margin that grows smoothly with the rank difference, ensuring that larger differences in preference are always encouraged. This is then scaled by `margin_scale` to control its magnitude.", "pseudocode": "1. Given a batch of log-probabilities, compute their z-score normalization across the entire batch.\n   logp_a_norm = zscore(logp_a)\n   logp_b_norm = zscore(logp_b)\n2. Compute the normalized log-probability difference.\n   logp_diff = logp_a_norm - logp_b_norm.\n3. Compute the normalized rank gap between `cost_a` and `cost_b` based on their ranks within the batch.\n   rank_diff = rank_gap(cost_a, cost_b).\n4. Create an adaptive margin using the `softplus` function on the rank gap, scaled by `margin_scale`. This ensures the margin is always non-negative and grows smoothly with the rank difference.\n   adaptive_margin = margin_scale * softplus(rank_diff).\n5. Calculate the core loss term. This is the difference between the normalized log-probability gap and the adaptive margin, scaled by a temperature parameter.\n   core_term = (logp_diff - adaptive_margin) * temperature.\n6. The final loss is the negative logsigmoid of the core term. This penalizes cases where the normalized `logp_diff` is smaller than the target `adaptive_margin`.\n   loss = -logsigmoid(core_term).", "hyperparams": {"temperature": 1.0, "margin_scale": 2.0}, "operators_used": ["zscore", "rank_gap", "softplus", "logsigmoid"], "implementation_hint": {"expects": ["cost_a", "cost_b", "logp_a", "logp_b"], "returns": "scalar"}}, "fitness": {"hf_like_score": 3.8854281902313232, "validation_objective": 3.8854281902313232, "generalization_penalty": 0.0, "generalization_objectives": {"20": 3.844605326652527}, "train_score_mean": 4.209985800087452, "train_loss_mean": 0.6781978737562895, "pair_count": 4112563, "config": {"hf": {"problem": "tsp", "hf_steps": 32, "train_problem_size": 20, "valid_problem_sizes": [20], "train_batch_size": 64, "pomo_size": 64, "learning_rate": 0.0003, "weight_decay": 1e-06, "alpha": 0.05, "device": "cuda", "seed": 1234, "num_validation_episodes": 128, "validation_batch_size": 64, "generalization_penalty_weight": 1.0, "pool_version": "v0"}, "free_loss": {"f1_steps": 32, "f2_steps": 100, "f3_enabled": false}}, "loss_ir": {"name": "Z-Score Scaled Rank-Margin Loss", "intuition": "This loss function creates a robust, adaptive margin by combining rank-based and distribution-aware normalization techniques.\n\nInherited Ideas:\n- From Parent 0, it inherits the core loss structure `-logsigmoid(temperature * (logp_diff - margin))`. This provides a stable, bounded loss that is sensitive to the magnitude of preference violations.\n- From both parents (Parent 0 and Parent 1), it inherits the concept of using a rank-based normalization (`rank_gap`) of the costs. This makes the margin robust to the absolute scale and distribution of costs, focusing instead on the relative ordering of solutions within the current batch.\n\nNew Coupling Ideas:\n1. **Z-Score Normalization of Log-Probabilities**: Before computing the log-probability difference, the log-probabilities of all items in the batch (`logp_a` and `logp_b`) are z-score normalized. This stabilizes the `logp_diff` term by centering it around zero and scaling it by the standard deviation of log-probabilities in the batch. This prevents exploding gradients and makes the loss less sensitive to the overall scale of log-probabilities, which can vary during training.\n2. **Hybrid Margin with `softplus`**: The adaptive margin is created by first calculating a rank-based gap (`rank_gap(cost_a, cost_b)`). Instead of using `sigmoid` or `tanh` which saturate, this rank gap is passed through a `softplus` function. This creates a non-negative, non-saturating margin that grows smoothly with the rank difference, ensuring that larger differences in preference are always encouraged. This is then scaled by `margin_scale` to control its magnitude.", "hyperparams": {"temperature": 1.0, "margin_scale": 2.0}, "operators_used": ["zscore", "rank_gap", "softplus", "logsigmoid"]}}}
{"generation": 2, "index": 7, "ir": {"name": "Z-Scored Rank-Modulated Margin Loss", "intuition": "This loss function creates a highly adaptive and stable margin by combining rank-based preference strength with batch-level statistical normalization.\n\nInherited Ideas:\n- From both parents (Parent 0 and Parent 1), it inherits the core concept of using a rank-based normalization (`rank_gap`) of costs to define the margin. This makes the target log-probability separation robust to the scale and distribution of raw cost values, focusing instead on the relative preference ordering within a batch.\n- From Parent 0, it inherits the use of `logsigmoid` to frame the loss, which provides a probabilistically interpretable and numerically stable loss value. The structure `-logsigmoid(temperature * (logp_diff - margin))` is adapted.\n- From Parent 1, it inherits the use of `tanh` to create a smooth, bounded, and saturating margin from the processed cost difference, preventing the margin from growing indefinitely.\n\nNew Coupling Ideas:\n1. **Z-Score Normalization of Log-Probabilities**: Before computing the log-probability difference, the individual log-probabilities (`logp_a` and `logp_b`) are first normalized across the batch using `zscore`. This centers the log-probability distribution around zero with a standard deviation of one. This decouples the loss from the absolute magnitude of the model's log-probabilities, focusing entirely on the *relative* log-probability difference in the context of the current batch's output distribution. This enhances stability, especially early in training or when the model's output scale shifts.\n2. **Rank-Modulated Margin Scaling**: The margin is not just based on the rank gap but is modulated by it. The base margin is defined by `max_margin * tanh(beta)`, creating a consistent target. This target is then scaled down by `(1 - rank_gap(cost_a, cost_b))`, where `rank_gap` is between 0 and 1. For pairs with a large rank difference (strong preference), `rank_gap` is close to 1, and the scaling factor is small, leading to a small margin. For pairs with a small rank difference (weak preference), `rank_gap` is close to 0, and the margin is close to its maximum. This counter-intuitive design encourages the model to focus its capacity on distinguishing between closely ranked pairs, which are often the hardest and most informative samples, rather than wasting effort on easily separable pairs.", "pseudocode": "1. Given batch log-probabilities, normalize them using z-scoring.\n   norm_logp_a = zscore(logp_a)\n   norm_logp_b = zscore(logp_b)\n2. Compute the normalized log-probability difference.\n   logp_diff = norm_logp_a - norm_logp_b\n3. Calculate the rank-normalized cost gap. This operator returns a value in [0, 1] representing the normalized rank difference, where 1 means `cost_a` is much better than `cost_b`.\n   rank_diff = rank_gap(cost_a, cost_b)\n4. Calculate the base margin using `tanh`. This creates a stable, bounded target.\n   base_margin = max_margin * tanh(beta)\n5. Modulate the base margin using the rank difference. The margin is largest for similarly ranked pairs and smallest for distant pairs.\n   adaptive_margin = base_margin * (1.0 - rank_diff)\n6. Compute the core loss term, scaled by temperature. This measures the signed deviation of the log-probability difference from the target margin.\n   core_term = (logp_diff - adaptive_margin) * temperature\n7. The final loss is the negative logsigmoid of the core term. This penalizes cases where the normalized `logp_diff` is less than the `adaptive_margin`.\n   loss = -logsigmoid(core_term)", "hyperparams": {"temperature": 5.0, "max_margin": 2.0, "beta": 1.0}, "operators_used": ["zscore", "rank_gap", "tanh", "logsigmoid"], "implementation_hint": {"expects": ["cost_a", "cost_b", "logp_a", "logp_b"], "returns": "scalar"}}, "fitness": {"hf_like_score": 3.880527377128601, "validation_objective": 3.880527377128601, "generalization_penalty": 0.0, "generalization_objectives": {"20": 3.8454225063323975}, "train_score_mean": 4.206778556108475, "train_loss_mean": 0.6783958822488785, "pair_count": 4110936, "config": {"hf": {"problem": "tsp", "hf_steps": 32, "train_problem_size": 20, "valid_problem_sizes": [20], "train_batch_size": 64, "pomo_size": 64, "learning_rate": 0.0003, "weight_decay": 1e-06, "alpha": 0.05, "device": "cuda", "seed": 1234, "num_validation_episodes": 128, "validation_batch_size": 64, "generalization_penalty_weight": 1.0, "pool_version": "v0"}, "free_loss": {"f1_steps": 32, "f2_steps": 100, "f3_enabled": false}}, "loss_ir": {"name": "Z-Scored Rank-Modulated Margin Loss", "intuition": "This loss function creates a highly adaptive and stable margin by combining rank-based preference strength with batch-level statistical normalization.\n\nInherited Ideas:\n- From both parents (Parent 0 and Parent 1), it inherits the core concept of using a rank-based normalization (`rank_gap`) of costs to define the margin. This makes the target log-probability separation robust to the scale and distribution of raw cost values, focusing instead on the relative preference ordering within a batch.\n- From Parent 0, it inherits the use of `logsigmoid` to frame the loss, which provides a probabilistically interpretable and numerically stable loss value. The structure `-logsigmoid(temperature * (logp_diff - margin))` is adapted.\n- From Parent 1, it inherits the use of `tanh` to create a smooth, bounded, and saturating margin from the processed cost difference, preventing the margin from growing indefinitely.\n\nNew Coupling Ideas:\n1. **Z-Score Normalization of Log-Probabilities**: Before computing the log-probability difference, the individual log-probabilities (`logp_a` and `logp_b`) are first normalized across the batch using `zscore`. This centers the log-probability distribution around zero with a standard deviation of one. This decouples the loss from the absolute magnitude of the model's log-probabilities, focusing entirely on the *relative* log-probability difference in the context of the current batch's output distribution. This enhances stability, especially early in training or when the model's output scale shifts.\n2. **Rank-Modulated Margin Scaling**: The margin is not just based on the rank gap but is modulated by it. The base margin is defined by `max_margin * tanh(beta)`, creating a consistent target. This target is then scaled down by `(1 - rank_gap(cost_a, cost_b))`, where `rank_gap` is between 0 and 1. For pairs with a large rank difference (strong preference), `rank_gap` is close to 1, and the scaling factor is small, leading to a small margin. For pairs with a small rank difference (weak preference), `rank_gap` is close to 0, and the margin is close to its maximum. This counter-intuitive design encourages the model to focus its capacity on distinguishing between closely ranked pairs, which are often the hardest and most informative samples, rather than wasting effort on easily separable pairs.", "hyperparams": {"temperature": 5.0, "max_margin": 2.0, "beta": 1.0}, "operators_used": ["zscore", "rank_gap", "tanh", "logsigmoid"]}}}
{"generation": 3, "index": 0, "ir": {"name": "Adaptive Rank-Gap Hinge Loss with Z-Score Normalization", "intuition": "This loss function creates a highly adaptive and robust margin by combining distribution-based and rank-based normalization, and applies it within a stable hinge-loss framework.\n\nInherited Ideas:\n- From Parent 0, it inherits the use of `zscore` to normalize costs before further processing. This makes the loss invariant to the absolute scale and shift of the cost distribution, focusing on relative differences within the batch.\n- From Parent 1, it inherits the use of `rank_gap` to compute a normalized difference based on the relative ordering of costs. This makes the margin robust to outliers and focuses on preference strength within the batch.\n\nNew Coupling Ideas:\n1. **Dual-Normalized Margin Signal**: The core innovation is to compute the `rank_gap` *on top of* the z-scored costs (`rank_gap(zscore(costs))`). This creates a doubly-normalized signal that is insensitive to both the scale/shift of the cost distribution (from `zscore`) and the specific magnitude of cost differences (from `rank_gap`). This hybrid signal robustly captures the preference strength purely based on statistical and ordinal position within the batch.\n2. **Saturating Margin with Baseline**: The doubly-normalized signal is passed through a `tanh` function to create a smooth, saturating margin, preventing extreme cost differences from creating an unstable target. A small, fixed `min_margin` is then added. This ensures that even for pairs with a very small rank-gap, a minimal separation is still enforced, preventing the loss from collapsing to zero inappropriately and providing a stable learning signal.", "pseudocode": "1. For the entire batch of costs, normalize them using a z-score transformation: z_cost_a = zscore(cost_a), z_cost_b = zscore(cost_b).\n2. Compute the rank-normalized gap between the z-scored costs. This provides a robust signal of relative preference within the batch, typically in [-1, 1].\n   rank_diff = rank_gap(z_cost_a, z_cost_b).\n3. Create a saturating margin by passing the rank difference through a `tanh` function and scaling it. This creates a smooth, bounded target separation.\n   saturating_margin = max_margin * tanh(beta * rank_diff).\n4. Combine this with a small baseline margin to ensure a minimum separation target.\n   adaptive_margin = min_margin + saturating_margin.\n5. Compute the log-probability difference: logp_diff = logp_a - logp_b.\n6. Calculate the core loss term, which is the difference between the log-probability gap and the adaptive margin, scaled by a temperature parameter.\n   core_term = temperature * (logp_diff - adaptive_margin).\n7. The final loss is the negative logsigmoid of the core term, which is equivalent to `softplus(-core_term)`. This penalizes cases where `logp_diff` is smaller than the target `adaptive_margin` in a numerically stable way.\n   loss = -logsigmoid(core_term).", "hyperparams": {"temperature": 5.0, "max_margin": 3.0, "min_margin": 0.05, "beta": 1.0}, "operators_used": ["zscore", "rank_gap", "tanh", "logsigmoid"], "implementation_hint": {"expects": ["cost_a", "cost_b", "logp_a", "logp_b"], "returns": "scalar"}}, "fitness": {"hf_like_score": 3.8802311420440674, "validation_objective": 3.8802311420440674, "generalization_penalty": 0.0, "generalization_objectives": {"20": 3.8502085208892822}, "train_score_mean": 4.209545344114304, "train_loss_mean": 0.6782267428934574, "pair_count": 4111670, "config": {"hf": {"problem": "tsp", "hf_steps": 32, "train_problem_size": 20, "valid_problem_sizes": [20], "train_batch_size": 64, "pomo_size": 64, "learning_rate": 0.0003, "weight_decay": 1e-06, "alpha": 0.05, "device": "cuda", "seed": 1234, "num_validation_episodes": 128, "validation_batch_size": 64, "generalization_penalty_weight": 1.0, "pool_version": "v0"}, "free_loss": {"f1_steps": 32, "f2_steps": 100, "f3_enabled": false}}, "loss_ir": {"name": "Adaptive Rank-Gap Hinge Loss with Z-Score Normalization", "intuition": "This loss function creates a highly adaptive and robust margin by combining distribution-based and rank-based normalization, and applies it within a stable hinge-loss framework.\n\nInherited Ideas:\n- From Parent 0, it inherits the use of `zscore` to normalize costs before further processing. This makes the loss invariant to the absolute scale and shift of the cost distribution, focusing on relative differences within the batch.\n- From Parent 1, it inherits the use of `rank_gap` to compute a normalized difference based on the relative ordering of costs. This makes the margin robust to outliers and focuses on preference strength within the batch.\n\nNew Coupling Ideas:\n1. **Dual-Normalized Margin Signal**: The core innovation is to compute the `rank_gap` *on top of* the z-scored costs (`rank_gap(zscore(costs))`). This creates a doubly-normalized signal that is insensitive to both the scale/shift of the cost distribution (from `zscore`) and the specific magnitude of cost differences (from `rank_gap`). This hybrid signal robustly captures the preference strength purely based on statistical and ordinal position within the batch.\n2. **Saturating Margin with Baseline**: The doubly-normalized signal is passed through a `tanh` function to create a smooth, saturating margin, preventing extreme cost differences from creating an unstable target. A small, fixed `min_margin` is then added. This ensures that even for pairs with a very small rank-gap, a minimal separation is still enforced, preventing the loss from collapsing to zero inappropriately and providing a stable learning signal.", "hyperparams": {"temperature": 5.0, "max_margin": 3.0, "min_margin": 0.05, "beta": 1.0}, "operators_used": ["zscore", "rank_gap", "tanh", "logsigmoid"]}}}
{"generation": 3, "index": 1, "ir": {"name": "Adaptive Rank-Gap Hinge Loss with Z-Score Normalization", "intuition": "This loss function creates a robust, adaptive margin by combining rank-based and distribution-based normalization, and applies it within a stable hinge-loss framework.\n\nInherited Ideas:\n- From Parent 0, it inherits the use of `zscore` to normalize costs before further processing. This makes the margin signal robust to the absolute scale and distribution of costs, focusing on relative differences within the batch's statistical profile.\n- From Parent 1, it inherits the core idea of using `rank_gap` on the costs to create a margin that is based on the relative ordering of solutions in the batch, making it insensitive to outlier cost values.\n- From both parents, it inherits the general structure of `loss = -logsigmoid(temperature * (logp_diff - adaptive_margin))`, which is a numerically stable hinge-like loss (equivalent to `softplus(temperature * (margin - logp_diff))`).\n\nNew Coupling Ideas:\n1. **Sequential Normalization (Z-Score then Rank-Gap):** The loss first applies `zscore` to the batch costs and then computes the `rank_gap` on these z-scored values. This two-step normalization creates an exceptionally stable signal. The z-scoring centers and scales the distribution, and the subsequent rank-gap calculation operates on this standardized space, making the resulting margin highly robust to both distributional shifts and individual outliers.\n2. **Dynamic Temperature Scaling:** The temperature, which controls the steepness of the loss, is made dynamic. It is scaled by the standard deviation of the log-probability differences (`logp_a - logp_b`) across the batch. This allows the loss to self-adjust its sensitivity: for batches where log-probability differences are very small (indicating model uncertainty), the temperature increases to create a stronger gradient signal. Conversely, if differences are large, the temperature decreases to prevent overly aggressive updates and potential instability.", "pseudocode": "1. Given costs for a batch, normalize them using z-scoring: z_cost_a = zscore(cost_a), z_cost_b = zscore(cost_b).\n2. Compute the rank-normalized gap on the z-scored costs. This provides a highly stable signal of relative preference within the batch, typically in [-1, 1].\n   rank_diff = rank_gap(z_cost_a, z_cost_b).\n3. Create an adaptive margin by scaling the rank difference. This creates a target separation for the log-probabilities that is proportional to the relative rank.\n   adaptive_margin = max_margin * rank_diff.\n4. Compute the log-probability difference: logp_diff = logp_a - logp_b.\n5. Compute a dynamic temperature. Get the standard deviation of all `logp_diff` values in the batch and use its inverse to scale a base temperature. Add a small epsilon for stability.\n   dynamic_temp = base_temp / (std_dev(all_logp_diffs) + epsilon).\n6. Calculate the core loss term. This is the difference between the log-probability gap and the adaptive margin, scaled by the dynamic temperature.\n   core_term = dynamic_temp * (logp_diff - adaptive_margin).\n7. The final loss is the negative logsigmoid of the core term. This penalizes cases where `logp_diff` is smaller than the target `adaptive_margin` and is numerically stable.\n   loss = -logsigmoid(core_term).", "hyperparams": {"base_temp": 1.0, "max_margin": 3.0, "epsilon": 1e-06}, "operators_used": ["zscore", "rank_gap", "logsigmoid"], "implementation_hint": {"expects": ["cost_a", "cost_b", "logp_a", "logp_b"], "returns": "scalar"}}, "fitness": {"hf_like_score": 3.8811455965042114, "validation_objective": 3.8811455965042114, "generalization_penalty": 0.0, "generalization_objectives": {"20": 3.855204939842224}, "train_score_mean": 4.209508642554283, "train_loss_mean": 0.6781058516353369, "pair_count": 4111708, "config": {"hf": {"problem": "tsp", "hf_steps": 32, "train_problem_size": 20, "valid_problem_sizes": [20], "train_batch_size": 64, "pomo_size": 64, "learning_rate": 0.0003, "weight_decay": 1e-06, "alpha": 0.05, "device": "cuda", "seed": 1234, "num_validation_episodes": 128, "validation_batch_size": 64, "generalization_penalty_weight": 1.0, "pool_version": "v0"}, "free_loss": {"f1_steps": 32, "f2_steps": 100, "f3_enabled": false}}, "loss_ir": {"name": "Adaptive Rank-Gap Hinge Loss with Z-Score Normalization", "intuition": "This loss function creates a robust, adaptive margin by combining rank-based and distribution-based normalization, and applies it within a stable hinge-loss framework.\n\nInherited Ideas:\n- From Parent 0, it inherits the use of `zscore` to normalize costs before further processing. This makes the margin signal robust to the absolute scale and distribution of costs, focusing on relative differences within the batch's statistical profile.\n- From Parent 1, it inherits the core idea of using `rank_gap` on the costs to create a margin that is based on the relative ordering of solutions in the batch, making it insensitive to outlier cost values.\n- From both parents, it inherits the general structure of `loss = -logsigmoid(temperature * (logp_diff - adaptive_margin))`, which is a numerically stable hinge-like loss (equivalent to `softplus(temperature * (margin - logp_diff))`).\n\nNew Coupling Ideas:\n1. **Sequential Normalization (Z-Score then Rank-Gap):** The loss first applies `zscore` to the batch costs and then computes the `rank_gap` on these z-scored values. This two-step normalization creates an exceptionally stable signal. The z-scoring centers and scales the distribution, and the subsequent rank-gap calculation operates on this standardized space, making the resulting margin highly robust to both distributional shifts and individual outliers.\n2. **Dynamic Temperature Scaling:** The temperature, which controls the steepness of the loss, is made dynamic. It is scaled by the standard deviation of the log-probability differences (`logp_a - logp_b`) across the batch. This allows the loss to self-adjust its sensitivity: for batches where log-probability differences are very small (indicating model uncertainty), the temperature increases to create a stronger gradient signal. Conversely, if differences are large, the temperature decreases to prevent overly aggressive updates and potential instability.", "hyperparams": {"base_temp": 1.0, "max_margin": 3.0, "epsilon": 1e-06}, "operators_used": ["zscore", "rank_gap", "logsigmoid"]}}}
{"generation": 3, "index": 2, "ir": {"name": "Rank-Gated Sigmoid Hinge Loss", "intuition": "This loss function creates a dynamically gated margin that adapts to both the rank-order and statistical distribution of costs within a batch, and applies it within a numerically stable hinge-loss framework.\n\nInherited Ideas:\n- From Parent 0 (Z-Scored Rank-Adaptive Hinge Loss), it inherits the idea of using z-score normalization on the costs (`zscore(cost)`) before any margin calculation. This makes the margin signal robust to the absolute scale and shift of the cost distribution, focusing on relative differences within the batch's statistical profile.\n- From Parent 1 (Rank-Normalized Margin Loss), it inherits the core structure of using `logsigmoid` on a temperature-scaled difference between the log-probability gap and an adaptive margin: `-logsigmoid(temperature * (logp_diff - adaptive_margin))`. This provides a stable, bounded loss that penalizes preference violations.\n\nNew Coupling Ideas:\n1. **Rank-Gated Margin**: Instead of directly using a rank-based signal to define the margin, we use it as a 'gate'. The margin is computed from the z-scored cost difference (`z_cost_b - z_cost_a`), but this margin is only 'activated' or scaled by a sigmoid function of the rank gap (`sigmoid(rank_gap)`). This coupling ensures the margin's magnitude is proportional to the statistical cost difference, but its influence is smoothly scaled by how confident the preference is in terms of batch-wise rank. If the rank-gap is small or negative, the sigmoid gate approaches 0.5 or less, effectively reducing or nullifying the margin, preventing noisy or ambiguous pairs from enforcing a large separation.\n2. **Margin Clamping for Stability**: The raw z-scored cost difference can still be large for outliers, even after z-scoring. To enhance stability, the computed margin is clamped to a maximum value (`max_margin`) before being applied. This prevents extreme cost pairs from creating an excessively large margin target, which could dominate the loss and destabilize training.", "pseudocode": "1. Given costs for a batch, normalize them using z-scoring: z_cost_a = zscore(cost_a), z_cost_b = zscore(cost_b).\n2. Compute the normalized rank gap between the costs. This captures the relative preference strength within the batch.\n   rank_diff = rank_gap(cost_a, cost_b).\n3. Create a 'gate' signal from the rank gap by passing it through a sigmoid function. This gate will be close to 1 for strong preferences (large positive rank gap) and close to 0.5 for weak or uncertain preferences.\n   rank_gate = sigmoid(rank_diff).\n4. Compute a base margin from the difference in z-scored costs. This reflects the preference magnitude in terms of standard deviations.\n   base_margin = z_cost_b - z_cost_a.\n5. Apply the rank gate to the base margin and clamp the result to a maximum value for stability. This creates the final adaptive margin.\n   adaptive_margin = clamp(rank_gate * base_margin, min=0, max=max_margin).\n6. Compute the log-probability difference: logp_diff = logp_a - logp_b.\n7. Calculate the core loss term, which is the difference between the log-probability gap and the adaptive margin, scaled by temperature.\n   core_term = temperature * (logp_diff - adaptive_margin).\n8. The final loss is the negative logsigmoid of the core term, which is equivalent to a softplus and robustly penalizes cases where `logp_diff` is smaller than the target `adaptive_margin`.\n   loss = -logsigmoid(core_term).", "hyperparams": {"temperature": 5.0, "max_margin": 4.0}, "operators_used": ["zscore", "rank_gap", "sigmoid", "clamp", "logsigmoid"], "implementation_hint": {"expects": ["cost_a", "cost_b", "logp_a", "logp_b"], "returns": "scalar"}}, "fitness": {"hf_like_score": 3.8824360370635986, "validation_objective": 3.8824360370635986, "generalization_penalty": 0.0, "generalization_objectives": {"20": 3.8478691577911377}, "train_score_mean": 4.210881732404232, "train_loss_mean": 0.6782612130045891, "pair_count": 4112361, "config": {"hf": {"problem": "tsp", "hf_steps": 32, "train_problem_size": 20, "valid_problem_sizes": [20], "train_batch_size": 64, "pomo_size": 64, "learning_rate": 0.0003, "weight_decay": 1e-06, "alpha": 0.05, "device": "cuda", "seed": 1234, "num_validation_episodes": 128, "validation_batch_size": 64, "generalization_penalty_weight": 1.0, "pool_version": "v0"}, "free_loss": {"f1_steps": 32, "f2_steps": 100, "f3_enabled": false}}, "loss_ir": {"name": "Rank-Gated Sigmoid Hinge Loss", "intuition": "This loss function creates a dynamically gated margin that adapts to both the rank-order and statistical distribution of costs within a batch, and applies it within a numerically stable hinge-loss framework.\n\nInherited Ideas:\n- From Parent 0 (Z-Scored Rank-Adaptive Hinge Loss), it inherits the idea of using z-score normalization on the costs (`zscore(cost)`) before any margin calculation. This makes the margin signal robust to the absolute scale and shift of the cost distribution, focusing on relative differences within the batch's statistical profile.\n- From Parent 1 (Rank-Normalized Margin Loss), it inherits the core structure of using `logsigmoid` on a temperature-scaled difference between the log-probability gap and an adaptive margin: `-logsigmoid(temperature * (logp_diff - adaptive_margin))`. This provides a stable, bounded loss that penalizes preference violations.\n\nNew Coupling Ideas:\n1. **Rank-Gated Margin**: Instead of directly using a rank-based signal to define the margin, we use it as a 'gate'. The margin is computed from the z-scored cost difference (`z_cost_b - z_cost_a`), but this margin is only 'activated' or scaled by a sigmoid function of the rank gap (`sigmoid(rank_gap)`). This coupling ensures the margin's magnitude is proportional to the statistical cost difference, but its influence is smoothly scaled by how confident the preference is in terms of batch-wise rank. If the rank-gap is small or negative, the sigmoid gate approaches 0.5 or less, effectively reducing or nullifying the margin, preventing noisy or ambiguous pairs from enforcing a large separation.\n2. **Margin Clamping for Stability**: The raw z-scored cost difference can still be large for outliers, even after z-scoring. To enhance stability, the computed margin is clamped to a maximum value (`max_margin`) before being applied. This prevents extreme cost pairs from creating an excessively large margin target, which could dominate the loss and destabilize training.", "hyperparams": {"temperature": 5.0, "max_margin": 4.0}, "operators_used": ["zscore", "rank_gap", "sigmoid", "clamp", "logsigmoid"]}}}
{"generation": 3, "index": 3, "ir": {"name": "Adaptive Rank-Gap Hinge Loss with Z-Score Normalization", "intuition": "This loss function creates a robust, multi-faceted margin by combining rank-based and distribution-based normalization, and applies it within a stable hinge-loss framework.\n\nInherited Ideas:\n- From Parent 0 (Z-Scored Rank-Adaptive Hinge Loss), it inherits the use of a `zscore` transformation on the batch costs. This pre-processing step centers the cost distribution and scales it by its standard deviation, making the subsequent margin calculation robust to shifts and scaling in the absolute cost values.\n- From Parent 1 (Rank-Normalized Margin Loss), it inherits the core idea of using `rank_gap` on the costs to create a margin signal. This captures the relative ordering of preferences within the batch, providing a signal that is inherently normalized and robust to outliers.\n\nNew Coupling Ideas:\n1. **Dual-Signal Margin Construction**: The margin is constructed from two complementary signals derived from the z-scored costs. The first signal is the `rank_gap` of the z-scored costs, capturing relative ordering. The second is the direct difference of the z-scored costs (`z_cost_b - z_cost_a`), capturing the magnitude of the preference after distributional normalization. These are combined linearly, allowing the margin to be sensitive to both the rank and the standardized magnitude of the cost difference.\n2. **Softplus-based Hinge Loss**: Instead of using `logsigmoid`, the loss uses its numerically stable equivalent, `softplus`, applied to the inverted core term. The core term itself is a simple difference between the margin and the log-probability gap (`margin - logp_diff`), without a temperature scale. This simplifies the formulation to a classic hinge loss (`max(0, margin - logp_diff)`) but in a smooth, differentiable form, directly penalizing the model when the log probability gap fails to meet the target margin.", "pseudocode": "1. Given costs for a batch, normalize them using z-scoring: z_cost_a = zscore(cost_a), z_cost_b = zscore(cost_b).\n2. Compute the rank-normalized gap between the z-scored costs. This provides a stable signal of relative preference order within the batch.\n   rank_diff = rank_gap(z_cost_a, z_cost_b).\n3. Compute the direct difference of the z-scored costs. This captures the preference magnitude, standardized across the batch.\n   z_cost_diff = z_cost_b - z_cost_a.\n4. Create an adaptive margin by linearly combining the rank-based and magnitude-based signals. The `beta` hyperparameter controls the balance between them.\n   adaptive_margin = rank_diff + beta * z_cost_diff.\n5. Compute the log-probability difference: logp_diff = logp_a - logp_b.\n6. Calculate the core loss term, which represents the violation of the margin. This is the difference between the target margin and the achieved log-probability gap.\n   margin_violation = adaptive_margin - logp_diff.\n7. The final loss is the `softplus` of the margin violation. This acts as a smooth, differentiable hinge loss, penalizing cases where `logp_diff` is smaller than the `adaptive_margin`.\n   loss = softplus(margin_violation).", "hyperparams": {"beta": 0.5}, "operators_used": ["zscore", "rank_gap", "softplus"], "implementation_hint": {"expects": ["cost_a", "cost_b", "logp_a", "logp_b"], "returns": "scalar"}}, "fitness": {"hf_like_score": 3.8857998847961426, "validation_objective": 3.8857998847961426, "generalization_penalty": 0.0, "generalization_objectives": {"20": 3.848265051841736}, "train_score_mean": 4.209740288555622, "train_loss_mean": 0.67832101136446, "pair_count": 4111503, "config": {"hf": {"problem": "tsp", "hf_steps": 32, "train_problem_size": 20, "valid_problem_sizes": [20], "train_batch_size": 64, "pomo_size": 64, "learning_rate": 0.0003, "weight_decay": 1e-06, "alpha": 0.05, "device": "cuda", "seed": 1234, "num_validation_episodes": 128, "validation_batch_size": 64, "generalization_penalty_weight": 1.0, "pool_version": "v0"}, "free_loss": {"f1_steps": 32, "f2_steps": 100, "f3_enabled": false}}, "loss_ir": {"name": "Adaptive Rank-Gap Hinge Loss with Z-Score Normalization", "intuition": "This loss function creates a robust, multi-faceted margin by combining rank-based and distribution-based normalization, and applies it within a stable hinge-loss framework.\n\nInherited Ideas:\n- From Parent 0 (Z-Scored Rank-Adaptive Hinge Loss), it inherits the use of a `zscore` transformation on the batch costs. This pre-processing step centers the cost distribution and scales it by its standard deviation, making the subsequent margin calculation robust to shifts and scaling in the absolute cost values.\n- From Parent 1 (Rank-Normalized Margin Loss), it inherits the core idea of using `rank_gap` on the costs to create a margin signal. This captures the relative ordering of preferences within the batch, providing a signal that is inherently normalized and robust to outliers.\n\nNew Coupling Ideas:\n1. **Dual-Signal Margin Construction**: The margin is constructed from two complementary signals derived from the z-scored costs. The first signal is the `rank_gap` of the z-scored costs, capturing relative ordering. The second is the direct difference of the z-scored costs (`z_cost_b - z_cost_a`), capturing the magnitude of the preference after distributional normalization. These are combined linearly, allowing the margin to be sensitive to both the rank and the standardized magnitude of the cost difference.\n2. **Softplus-based Hinge Loss**: Instead of using `logsigmoid`, the loss uses its numerically stable equivalent, `softplus`, applied to the inverted core term. The core term itself is a simple difference between the margin and the log-probability gap (`margin - logp_diff`), without a temperature scale. This simplifies the formulation to a classic hinge loss (`max(0, margin - logp_diff)`) but in a smooth, differentiable form, directly penalizing the model when the log probability gap fails to meet the target margin.", "hyperparams": {"beta": 0.5}, "operators_used": ["zscore", "rank_gap", "softplus"]}}}
{"generation": 3, "index": 4, "ir": {"name": "Rank-Adaptive Sigmoid Hinge Loss", "intuition": "This loss function creates a robust, adaptive margin by combining rank-based normalization with a sigmoid-based hinge loss, stabilized by a tanh transformation.\n\nInherited Ideas:\n- From Parent 0 (Z-Scored Rank-Adaptive Hinge Loss), it inherits the idea of creating a smooth, saturating margin by passing a normalized cost signal through a `tanh` function. This prevents the margin from growing uncontrollably, enhancing stability.\n- From Parent 1 (Rank-Normalized Margin Loss), it inherits the core idea of using `rank_gap` to normalize the cost difference. This makes the margin robust to the absolute scale and distribution of costs, focusing on the relative ordering of solutions within the current batch.\n\nNew Coupling Ideas:\n1. **Sigmoid Hinge Loss Structure**: Instead of the common `logsigmoid` structure, the loss is framed as `sigmoid(-temperature * (logp_diff - adaptive_margin))`. This directly maps the preference violation (the extent to which `logp_diff` is less than the `adaptive_margin`) to a loss value between 0 and 1. It provides a bounded, intuitive loss where 0 means the preference is strongly satisfied and 1 means it's strongly violated.\n2. **Margin Baseline from Log-Probabilities**: A small baseline margin is derived directly from the log-probabilities of the less-preferred item (`logp_b`). Specifically, `min_margin * sigmoid(-logp_b)` is added to the main adaptive margin. This introduces a subtle regularization: when the model is already very certain that `b` is a bad choice (large negative `logp_b`), the minimum margin increases slightly, demanding a clearer preference for `a`. This encourages the model to not only prefer `a` but also to be confident in its dismissal of `b`.", "pseudocode": "1. Given costs for a batch, compute the normalized rank gap between `cost_a` and `cost_b`. This provides a stable signal of relative preference within the batch, typically in [-1, 1].\n   rank_diff = rank_gap(cost_a, cost_b).\n2. Create the primary adaptive margin. Pass the `rank_diff` through a `tanh` function to create a smooth, bounded signal, then scale it by `max_margin`.\n   tanh_margin = max_margin * tanh(beta * rank_diff).\n3. Calculate a dynamic baseline margin based on the log-probability of the less-preferred item `b`. This encourages stronger separation when `b` is already deemed unlikely.\n   baseline_margin = min_margin * sigmoid(-logp_b).\n4. Combine the margins to get the final adaptive target for the log-probability difference.\n   adaptive_margin = tanh_margin + baseline_margin.\n5. Compute the log-probability difference: logp_diff = logp_a - logp_b.\n6. Calculate the core loss term, which represents the preference violation. This is the difference between the log-probability gap and the target margin.\n   violation = logp_diff - adaptive_margin.\n7. The final loss is the sigmoid of the scaled, negated violation. This smoothly maps the preference violation to a loss between 0 and 1.\n   loss = sigmoid(-temperature * violation).", "hyperparams": {"temperature": 5.0, "max_margin": 3.0, "min_margin": 0.05, "beta": 1.0}, "operators_used": ["rank_gap", "tanh", "sigmoid"], "implementation_hint": {"expects": ["cost_a", "cost_b", "logp_a", "logp_b"], "returns": "scalar"}}, "fitness": {"hf_like_score": 3.887428045272827, "validation_objective": 3.887428045272827, "generalization_penalty": 0.0, "generalization_objectives": {"20": 3.8502471446990967}, "train_score_mean": 4.209363855421543, "train_loss_mean": 0.6781328581273556, "pair_count": 4111815, "config": {"hf": {"problem": "tsp", "hf_steps": 32, "train_problem_size": 20, "valid_problem_sizes": [20], "train_batch_size": 64, "pomo_size": 64, "learning_rate": 0.0003, "weight_decay": 1e-06, "alpha": 0.05, "device": "cuda", "seed": 1234, "num_validation_episodes": 128, "validation_batch_size": 64, "generalization_penalty_weight": 1.0, "pool_version": "v0"}, "free_loss": {"f1_steps": 32, "f2_steps": 100, "f3_enabled": false}}, "loss_ir": {"name": "Rank-Adaptive Sigmoid Hinge Loss", "intuition": "This loss function creates a robust, adaptive margin by combining rank-based normalization with a sigmoid-based hinge loss, stabilized by a tanh transformation.\n\nInherited Ideas:\n- From Parent 0 (Z-Scored Rank-Adaptive Hinge Loss), it inherits the idea of creating a smooth, saturating margin by passing a normalized cost signal through a `tanh` function. This prevents the margin from growing uncontrollably, enhancing stability.\n- From Parent 1 (Rank-Normalized Margin Loss), it inherits the core idea of using `rank_gap` to normalize the cost difference. This makes the margin robust to the absolute scale and distribution of costs, focusing on the relative ordering of solutions within the current batch.\n\nNew Coupling Ideas:\n1. **Sigmoid Hinge Loss Structure**: Instead of the common `logsigmoid` structure, the loss is framed as `sigmoid(-temperature * (logp_diff - adaptive_margin))`. This directly maps the preference violation (the extent to which `logp_diff` is less than the `adaptive_margin`) to a loss value between 0 and 1. It provides a bounded, intuitive loss where 0 means the preference is strongly satisfied and 1 means it's strongly violated.\n2. **Margin Baseline from Log-Probabilities**: A small baseline margin is derived directly from the log-probabilities of the less-preferred item (`logp_b`). Specifically, `min_margin * sigmoid(-logp_b)` is added to the main adaptive margin. This introduces a subtle regularization: when the model is already very certain that `b` is a bad choice (large negative `logp_b`), the minimum margin increases slightly, demanding a clearer preference for `a`. This encourages the model to not only prefer `a` but also to be confident in its dismissal of `b`.", "hyperparams": {"temperature": 5.0, "max_margin": 3.0, "min_margin": 0.05, "beta": 1.0}, "operators_used": ["rank_gap", "tanh", "sigmoid"]}}}
{"generation": 3, "index": 5, "ir": {"name": "Z-Normalized Sigmoid-Hinge Loss", "intuition": "This loss function creates a robust, adaptive margin by normalizing both the cost and log-probability signals before combining them in a stable hinge-loss framework.\n\nInherited Ideas:\n- From Parent 0 (Z-Scored Rank-Adaptive Hinge Loss), it inherits the idea of using `zscore` to normalize the costs within a batch. This makes the margin calculation independent of the absolute scale of the costs, focusing on their statistical distribution.\n- From Parent 1 (Rank-Normalized Margin Loss), it inherits the use of `sigmoid` to transform the normalized cost difference into a bounded margin between 0 and a maximum value (`max_margin`). This prevents the margin from growing uncontrollably.\n\nNew Coupling Ideas:\n1. **Z-Score Log-Probability Normalization**: In addition to normalizing costs, the log-probabilities (`logp_a`, `logp_b`) are also normalized via `zscore` across the batch. This stabilizes the `logp_diff` term, making the loss less sensitive to shifts or scaling in the model's raw output logits, and focuses the optimization pressure on the relative differences in log-probabilities within the current batch context.\n2. **Direct Cost Difference Usage**: Instead of using `rank_gap` which only considers order, this loss directly uses the difference of the z-scored costs (`z_cost_b - z_cost_a`). This retains more granular information about the magnitude of preference after the costs have been standardized, allowing the sigmoid-based margin to be more sensitive to the size of the cost gap.", "pseudocode": "1. Given costs and log-probabilities for a batch, normalize them using z-scoring: z_cost_a, z_cost_b = zscore(cost_a), zscore(cost_b) and z_logp_a, z_logp_b = zscore(logp_a), zscore(logp_b).\n2. Compute the difference of the z-scored costs. This represents the preference strength, standardized against the batch distribution.\n   z_cost_diff = z_cost_b - z_cost_a.\n3. Create an adaptive margin by passing the z-scored cost difference through a `sigmoid` function and scaling it by `max_margin`. The margin is bounded between 0 and `max_margin`.\n   adaptive_margin = max_margin * sigmoid(beta * z_cost_diff).\n4. Compute the difference of the z-scored log-probabilities: z_logp_diff = z_logp_a - z_logp_b.\n5. Calculate the core loss term, which is the difference between the normalized log-probability gap and the adaptive margin, scaled by a temperature parameter.\n   core_term = temperature * (z_logp_diff - adaptive_margin).\n6. The final loss is the negative logsigmoid of the core term. This is equivalent to `softplus(-core_term)` and provides a stable, smooth hinge-like penalty when the model's preference (`z_logp_diff`) does not meet the target `adaptive_margin`.", "hyperparams": {"temperature": 5.0, "max_margin": 2.0, "beta": 1.0}, "operators_used": ["zscore", "sigmoid", "logsigmoid"], "implementation_hint": {"expects": ["cost_a", "cost_b", "logp_a", "logp_b"], "returns": "scalar"}}, "fitness": {"hf_like_score": 3.8830677270889282, "validation_objective": 3.8830677270889282, "generalization_penalty": 0.0, "generalization_objectives": {"20": 3.8513026237487793}, "train_score_mean": 4.209479704499245, "train_loss_mean": 0.67849101126194, "pair_count": 4110990, "config": {"hf": {"problem": "tsp", "hf_steps": 32, "train_problem_size": 20, "valid_problem_sizes": [20], "train_batch_size": 64, "pomo_size": 64, "learning_rate": 0.0003, "weight_decay": 1e-06, "alpha": 0.05, "device": "cuda", "seed": 1234, "num_validation_episodes": 128, "validation_batch_size": 64, "generalization_penalty_weight": 1.0, "pool_version": "v0"}, "free_loss": {"f1_steps": 32, "f2_steps": 100, "f3_enabled": false}}, "loss_ir": {"name": "Z-Normalized Sigmoid-Hinge Loss", "intuition": "This loss function creates a robust, adaptive margin by normalizing both the cost and log-probability signals before combining them in a stable hinge-loss framework.\n\nInherited Ideas:\n- From Parent 0 (Z-Scored Rank-Adaptive Hinge Loss), it inherits the idea of using `zscore` to normalize the costs within a batch. This makes the margin calculation independent of the absolute scale of the costs, focusing on their statistical distribution.\n- From Parent 1 (Rank-Normalized Margin Loss), it inherits the use of `sigmoid` to transform the normalized cost difference into a bounded margin between 0 and a maximum value (`max_margin`). This prevents the margin from growing uncontrollably.\n\nNew Coupling Ideas:\n1. **Z-Score Log-Probability Normalization**: In addition to normalizing costs, the log-probabilities (`logp_a`, `logp_b`) are also normalized via `zscore` across the batch. This stabilizes the `logp_diff` term, making the loss less sensitive to shifts or scaling in the model's raw output logits, and focuses the optimization pressure on the relative differences in log-probabilities within the current batch context.\n2. **Direct Cost Difference Usage**: Instead of using `rank_gap` which only considers order, this loss directly uses the difference of the z-scored costs (`z_cost_b - z_cost_a`). This retains more granular information about the magnitude of preference after the costs have been standardized, allowing the sigmoid-based margin to be more sensitive to the size of the cost gap.", "hyperparams": {"temperature": 5.0, "max_margin": 2.0, "beta": 1.0}, "operators_used": ["zscore", "sigmoid", "logsigmoid"]}}}
{"generation": 3, "index": 6, "ir": {"name": "Exponential Rank-Adaptive Sigmoid Loss", "intuition": "This loss function creates a dynamically scaled, adaptive margin that is sensitive to both the relative rank and the statistical distribution of costs, and applies it within a stable sigmoid-based loss framework.\n\nInherited Ideas:\n- From Parent 0 (Z-Scored Rank-Adaptive Hinge Loss), it inherits the idea of using `zscore` to normalize the costs before further processing. This makes the margin computation robust to the absolute scale and shift of the cost distribution, focusing on the relative statistical properties of the batch.\n- From Parent 1 (Rank-Normalized Margin Loss), it inherits the idea of using the `rank_gap` of costs as a primary signal for the margin. This ensures the margin is directly tied to the relative ordering of preferences within the batch, making it robust to cost outliers.\n\nNew Coupling Ideas:\n1. **Exponential Rank Scaling**: Instead of using a linear or `tanh` scaling on the rank difference, this loss applies an `exp` function to the rank-gap of the z-scored costs (`exp(beta * rank_gap(zscore(cost_a), zscore(cost_b)))`). This creates an exponentially growing margin. The intuition is to very strongly separate pairs that have a large rank difference, while treating pairs with small rank differences more similarly. This provides a powerful signal to prioritize learning clear preferences first.\n2. **Sigmoid-based Loss Structure**: Instead of the common `logsigmoid` (hinge-like) structure, this loss uses a sigmoid function directly on the difference between the log-probability gap and the margin: `sigmoid(margin - logp_diff)`. This frames the problem as predicting the probability that the preference is incorrect, given the model's log-probabilities. The loss value is bounded between 0 and 1, providing inherent stability and a clear probabilistic interpretation. The `temperature` scales the entire term, controlling the steepness of the sigmoid curve and thus the sensitivity to preference violations.", "pseudocode": "1. Given costs for a batch, normalize them using z-scoring: z_cost_a = zscore(cost_a), z_cost_b = zscore(cost_b).\n2. Compute the rank-normalized gap between the z-scored costs. This provides a stable signal of relative preference within the batch, typically in [-1, 1].\n   rank_diff = rank_gap(z_cost_a, z_cost_b).\n3. Create an exponentially scaled adaptive margin. Apply the `exp` function to the rank difference, scaled by `beta`. This makes the target separation grow exponentially with the rank difference.\n   adaptive_margin = exp(beta * rank_diff).\n4. Compute the log-probability difference: logp_diff = logp_a - logp_b.\n5. Calculate the core loss term. This is the difference between the adaptive margin and the log-probability gap, scaled by a temperature parameter.\n   core_term = temperature * (adaptive_margin - logp_diff).\n6. The final loss is the sigmoid of the core term. This yields a value between 0 and 1, representing the predicted probability of an incorrect preference. The loss is high when `logp_diff` is much smaller than the target `adaptive_margin`.\n   loss = sigmoid(core_term).", "hyperparams": {"temperature": 1.0, "beta": 2.0}, "operators_used": ["zscore", "rank_gap", "exp", "sigmoid"], "implementation_hint": {"expects": ["cost_a", "cost_b", "logp_a", "logp_b"], "returns": "scalar"}}, "fitness": {"hf_like_score": 3.8812326192855835, "validation_objective": 3.8812326192855835, "generalization_penalty": 0.0, "generalization_objectives": {"20": 3.8490967750549316}, "train_score_mean": 4.210669867694378, "train_loss_mean": 0.6784641947597265, "pair_count": 4111605, "config": {"hf": {"problem": "tsp", "hf_steps": 32, "train_problem_size": 20, "valid_problem_sizes": [20], "train_batch_size": 64, "pomo_size": 64, "learning_rate": 0.0003, "weight_decay": 1e-06, "alpha": 0.05, "device": "cuda", "seed": 1234, "num_validation_episodes": 128, "validation_batch_size": 64, "generalization_penalty_weight": 1.0, "pool_version": "v0"}, "free_loss": {"f1_steps": 32, "f2_steps": 100, "f3_enabled": false}}, "loss_ir": {"name": "Exponential Rank-Adaptive Sigmoid Loss", "intuition": "This loss function creates a dynamically scaled, adaptive margin that is sensitive to both the relative rank and the statistical distribution of costs, and applies it within a stable sigmoid-based loss framework.\n\nInherited Ideas:\n- From Parent 0 (Z-Scored Rank-Adaptive Hinge Loss), it inherits the idea of using `zscore` to normalize the costs before further processing. This makes the margin computation robust to the absolute scale and shift of the cost distribution, focusing on the relative statistical properties of the batch.\n- From Parent 1 (Rank-Normalized Margin Loss), it inherits the idea of using the `rank_gap` of costs as a primary signal for the margin. This ensures the margin is directly tied to the relative ordering of preferences within the batch, making it robust to cost outliers.\n\nNew Coupling Ideas:\n1. **Exponential Rank Scaling**: Instead of using a linear or `tanh` scaling on the rank difference, this loss applies an `exp` function to the rank-gap of the z-scored costs (`exp(beta * rank_gap(zscore(cost_a), zscore(cost_b)))`). This creates an exponentially growing margin. The intuition is to very strongly separate pairs that have a large rank difference, while treating pairs with small rank differences more similarly. This provides a powerful signal to prioritize learning clear preferences first.\n2. **Sigmoid-based Loss Structure**: Instead of the common `logsigmoid` (hinge-like) structure, this loss uses a sigmoid function directly on the difference between the log-probability gap and the margin: `sigmoid(margin - logp_diff)`. This frames the problem as predicting the probability that the preference is incorrect, given the model's log-probabilities. The loss value is bounded between 0 and 1, providing inherent stability and a clear probabilistic interpretation. The `temperature` scales the entire term, controlling the steepness of the sigmoid curve and thus the sensitivity to preference violations.", "hyperparams": {"temperature": 1.0, "beta": 2.0}, "operators_used": ["zscore", "rank_gap", "exp", "sigmoid"]}}}
{"generation": 3, "index": 7, "ir": {"name": "Z-Clamped Rank-Adaptive Sigmoid Loss", "intuition": "This loss function creates a highly stable and adaptive preference learning signal by blending rank-based normalization with distributional awareness and a bounded activation function.\n\nInherited Ideas:\n- From Parent 0 (Z-Scored Rank-Adaptive Hinge Loss), it inherits the idea of using z-score normalization (`zscore`) on the batch costs. This pre-processing step centers the cost distribution and scales it by its standard deviation, making the subsequent margin calculation robust to shifts and scaling in the absolute cost values.\n- From Parent 1 (Rank-Normalized Margin Loss), it inherits the use of `rank_gap` to derive a margin signal. This focuses the loss on the relative ordering of preferences within the batch, making it robust to cost outliers.\n\nNew Coupling Ideas:\n1. **Hybrid Normalized Signal**: The core innovation is to combine the z-scored costs and the rank-gap signal *before* creating the margin. The z-scored cost difference (`z_cost_b - z_cost_a`) provides a measure of distributional distance, while the `rank_gap` provides a measure of ordinal distance. These two signals are summed to create a hybrid signal that is sensitive to both the statistical magnitude and the relative rank of the preference.\n2. **Clamped Sigmoid Margin**: The hybrid signal is then passed through a `sigmoid` function, similar to Parent 1, but with a crucial stability modification. Before the sigmoid, the signal is clamped to a range `[-clamp_val, clamp_val]`. This prevents extremely large or small inputs to the sigmoid function, which can lead to vanishing gradients in its saturated regions. This clamping ensures the margin remains responsive and gradients remain healthy, while still benefiting from the smooth, bounded nature of the sigmoid function. The final loss applies this margin within the stable `-logsigmoid` framework common to both parents.", "pseudocode": "1. For the entire batch, normalize costs using z-scoring: z_cost_a = zscore(cost_a), z_cost_b = zscore(cost_b).\n2. Compute the difference in z-scored costs: z_cost_diff = z_cost_b - z_cost_a.\n3. Compute the rank-normalized gap between the original costs: rank_diff = rank_gap(cost_a, cost_b).\n4. Create a hybrid signal by summing the z-score difference and the rank difference. This combines distributional and ordinal preference information.\n   hybrid_signal = z_cost_diff + rank_diff.\n5. Clamp the hybrid signal to a symmetric range to prevent extreme values and ensure numerical stability for the subsequent sigmoid activation.\n   clamped_signal = clamp(hybrid_signal, -clamp_val, clamp_val).\n6. Create an adaptive margin by applying a sigmoid function to the clamped signal and scaling it by `max_margin`.\n   adaptive_margin = max_margin * sigmoid(clamped_signal).\n7. Compute the log-probability difference: logp_diff = logp_a - logp_b.\n8. Calculate the final loss using a temperature-scaled difference between the log-probability gap and the adaptive margin, passed through a negative logsigmoid for a stable hinge-like penalty.\n   loss = -logsigmoid(temperature * (logp_diff - adaptive_margin)).", "hyperparams": {"temperature": 5.0, "max_margin": 4.0, "clamp_val": 5.0}, "operators_used": ["zscore", "rank_gap", "clamp", "sigmoid", "logsigmoid"], "implementation_hint": {"expects": ["cost_a", "cost_b", "logp_a", "logp_b"], "returns": "scalar"}}, "fitness": {"hf_like_score": 3.883683681488037, "validation_objective": 3.883683681488037, "generalization_penalty": 0.0, "generalization_objectives": {"20": 3.853463053703308}, "train_score_mean": 4.210158295929432, "train_loss_mean": 0.6783256903290749, "pair_count": 4111209, "config": {"hf": {"problem": "tsp", "hf_steps": 32, "train_problem_size": 20, "valid_problem_sizes": [20], "train_batch_size": 64, "pomo_size": 64, "learning_rate": 0.0003, "weight_decay": 1e-06, "alpha": 0.05, "device": "cuda", "seed": 1234, "num_validation_episodes": 128, "validation_batch_size": 64, "generalization_penalty_weight": 1.0, "pool_version": "v0"}, "free_loss": {"f1_steps": 32, "f2_steps": 100, "f3_enabled": false}}, "loss_ir": {"name": "Z-Clamped Rank-Adaptive Sigmoid Loss", "intuition": "This loss function creates a highly stable and adaptive preference learning signal by blending rank-based normalization with distributional awareness and a bounded activation function.\n\nInherited Ideas:\n- From Parent 0 (Z-Scored Rank-Adaptive Hinge Loss), it inherits the idea of using z-score normalization (`zscore`) on the batch costs. This pre-processing step centers the cost distribution and scales it by its standard deviation, making the subsequent margin calculation robust to shifts and scaling in the absolute cost values.\n- From Parent 1 (Rank-Normalized Margin Loss), it inherits the use of `rank_gap` to derive a margin signal. This focuses the loss on the relative ordering of preferences within the batch, making it robust to cost outliers.\n\nNew Coupling Ideas:\n1. **Hybrid Normalized Signal**: The core innovation is to combine the z-scored costs and the rank-gap signal *before* creating the margin. The z-scored cost difference (`z_cost_b - z_cost_a`) provides a measure of distributional distance, while the `rank_gap` provides a measure of ordinal distance. These two signals are summed to create a hybrid signal that is sensitive to both the statistical magnitude and the relative rank of the preference.\n2. **Clamped Sigmoid Margin**: The hybrid signal is then passed through a `sigmoid` function, similar to Parent 1, but with a crucial stability modification. Before the sigmoid, the signal is clamped to a range `[-clamp_val, clamp_val]`. This prevents extremely large or small inputs to the sigmoid function, which can lead to vanishing gradients in its saturated regions. This clamping ensures the margin remains responsive and gradients remain healthy, while still benefiting from the smooth, bounded nature of the sigmoid function. The final loss applies this margin within the stable `-logsigmoid` framework common to both parents.", "hyperparams": {"temperature": 5.0, "max_margin": 4.0, "clamp_val": 5.0}, "operators_used": ["zscore", "rank_gap", "clamp", "sigmoid", "logsigmoid"]}}}
{"generation": 4, "index": 0, "ir": {"name": "Adaptive Rank-Gap Hinge Loss with Z-Scored Log-Probabilities", "intuition": "This loss function creates a robust, adaptive margin based on the relative ranking of costs within a batch, and then applies this margin to a z-score normalized log-probability difference. The design aims to stabilize training by normalizing both the cost signal (for the margin) and the model's output signal (log-probabilities) before comparing them.\n\nInherited Ideas:\n- From Parent 0, it inherits the use of `zscore` for normalization and the `tanh` function to create a smoothly saturating margin from a normalized signal. This prevents the margin from growing uncontrollably.\n- From Parent 1, it inherits the use of `rank_gap` to derive a preference signal from costs that is robust to outliers and independent of the absolute cost scale. It also inherits the core loss structure of `-logsigmoid(temperature * (normalized_logp_diff - margin))`, which provides a stable, bounded, hinge-like loss.\n\nNew Coupling Ideas:\n1. **Z-Score Normalization of Log-Probabilities**: Instead of using the raw log-probability difference (`logp_a - logp_b`), the loss first computes this difference for the entire batch and then normalizes it using a z-score transformation. This `zscore(logp_a - logp_b)` step centers the distribution of log-probability differences around zero and scales it by its standard deviation. This decouples the loss from the absolute magnitude of the model's log-probabilities, focusing instead on the relative confidence of its predictions within the current batch. This can improve stability, especially if the model's logits drift or scale significantly during training.\n2. **Rank-Gap Driven Margin**: The adaptive margin is created by first computing the `rank_gap(cost_a, cost_b)` and then passing this rank-based signal through a `tanh` function (inherited from Parent 0). This combines the rank-based robustness of Parent 1 with the smooth saturation of Parent 0, creating a margin that is highly stable and directly reflects the ordinal preference strength within the batch.", "pseudocode": "1. For the entire batch, compute the log-probability differences: logp_diffs = logp_a - logp_b.\n2. Normalize these differences using z-scoring to get a batch-wise standardized signal: z_logp_diff = zscore(logp_diffs).\n3. Compute the normalized rank gap between `cost_a` and `cost_b`. This value, `rank_diff`, is typically in [-1, 1] and represents the relative preference strength.\n   rank_diff = rank_gap(cost_a, cost_b).\n4. Create a smooth, bounded, adaptive margin by passing the `rank_diff` through a `tanh` function and scaling it by `max_margin`. This ensures the margin is sensitive to rank but capped.\n   adaptive_margin = max_margin * tanh(beta * rank_diff).\n5. Calculate the core loss term. This is the difference between the z-scored log-probability gap and the adaptive margin, scaled by a temperature parameter.\n   core_term = temperature * (z_logp_diff - adaptive_margin).\n6. The final loss is the negative logsigmoid of the core term. This is equivalent to `softplus(-core_term)` and penalizes cases where the normalized log-probability difference is smaller than the target margin.\n   loss = -logsigmoid(core_term).", "hyperparams": {"temperature": 1.0, "max_margin": 2.0, "beta": 1.5}, "operators_used": ["zscore", "rank_gap", "tanh", "logsigmoid"], "implementation_hint": {"expects": ["cost_a", "cost_b", "logp_a", "logp_b"], "returns": "scalar"}}, "fitness": {"hf_like_score": 3.8805209398269653, "validation_objective": 3.8805209398269653, "generalization_penalty": 0.0, "generalization_objectives": {"20": 3.851863741874695}, "train_score_mean": 4.210540883243084, "train_loss_mean": 0.6782624833285809, "pair_count": 4112300, "config": {"hf": {"problem": "tsp", "hf_steps": 32, "train_problem_size": 20, "valid_problem_sizes": [20], "train_batch_size": 64, "pomo_size": 64, "learning_rate": 0.0003, "weight_decay": 1e-06, "alpha": 0.05, "device": "cuda", "seed": 1234, "num_validation_episodes": 128, "validation_batch_size": 64, "generalization_penalty_weight": 1.0, "pool_version": "v0"}, "free_loss": {"f1_steps": 32, "f2_steps": 100, "f3_enabled": false}}, "loss_ir": {"name": "Adaptive Rank-Gap Hinge Loss with Z-Scored Log-Probabilities", "intuition": "This loss function creates a robust, adaptive margin based on the relative ranking of costs within a batch, and then applies this margin to a z-score normalized log-probability difference. The design aims to stabilize training by normalizing both the cost signal (for the margin) and the model's output signal (log-probabilities) before comparing them.\n\nInherited Ideas:\n- From Parent 0, it inherits the use of `zscore` for normalization and the `tanh` function to create a smoothly saturating margin from a normalized signal. This prevents the margin from growing uncontrollably.\n- From Parent 1, it inherits the use of `rank_gap` to derive a preference signal from costs that is robust to outliers and independent of the absolute cost scale. It also inherits the core loss structure of `-logsigmoid(temperature * (normalized_logp_diff - margin))`, which provides a stable, bounded, hinge-like loss.\n\nNew Coupling Ideas:\n1. **Z-Score Normalization of Log-Probabilities**: Instead of using the raw log-probability difference (`logp_a - logp_b`), the loss first computes this difference for the entire batch and then normalizes it using a z-score transformation. This `zscore(logp_a - logp_b)` step centers the distribution of log-probability differences around zero and scales it by its standard deviation. This decouples the loss from the absolute magnitude of the model's log-probabilities, focusing instead on the relative confidence of its predictions within the current batch. This can improve stability, especially if the model's logits drift or scale significantly during training.\n2. **Rank-Gap Driven Margin**: The adaptive margin is created by first computing the `rank_gap(cost_a, cost_b)` and then passing this rank-based signal through a `tanh` function (inherited from Parent 0). This combines the rank-based robustness of Parent 1 with the smooth saturation of Parent 0, creating a margin that is highly stable and directly reflects the ordinal preference strength within the batch.", "hyperparams": {"temperature": 1.0, "max_margin": 2.0, "beta": 1.5}, "operators_used": ["zscore", "rank_gap", "tanh", "logsigmoid"]}}}
{"generation": 4, "index": 1, "ir": {"name": "Z-Scaled Rank-Modulated Hinge Loss", "intuition": "This loss function creates a robust, dual-signal adaptive margin by blending rank-based and distribution-based cost information, and applies it within a stable hinge-loss framework.\n\nInherited Ideas:\n- From Parent 0 (Z-Scored Rank-Adaptive Hinge Loss), it inherits the idea of using z-score normalization on the batch costs (`zscore(costs)`) before any further processing. This stabilizes the cost signal against shifts and scaling in the cost distribution, focusing on relative differences.\n- From Parent 1 (Rank-Normalized Margin Loss), it inherits the core structure of using a sigmoid-transformed signal to create a bounded, adaptive margin (`max_margin * sigmoid(...)`). This ensures the margin grows with preference strength but saturates gracefully.\n- From both parents, it inherits the final loss formulation: `-logsigmoid(temperature * (logp_diff - margin))`, which is a numerically stable hinge-like loss equivalent to `softplus(temperature * (margin - logp_diff))`. It also inherits the use of `rank_gap` to capture relative ordering within the batch.\n\nNew Coupling Ideas:\n1. **Blended Margin Signal**: The margin is derived from a blend of two distinct normalized cost signals. First, the z-scored cost difference (`z_cost_a - z_cost_b`) captures the magnitude of preference based on the batch's statistical distribution. Second, the rank-gap (`rank_gap(cost_a, cost_b)`) captures the ordinal relationship. These two signals are added together *before* being passed to the sigmoid function. This coupling creates a richer margin that is sensitive to both the statistical extremity and the relative rank of the costs, making it more informative than either signal alone.\n2. **Adaptive Temperature Scaling**: The temperature, which controls the sharpness of the loss, is made adaptive. It is scaled by the absolute value of the z-scored cost difference. The term `exp(alpha * abs(z_cost_a - z_cost_b))` acts as a dynamic temperature multiplier. For pairs with a large cost difference (high confidence preference), the temperature increases, creating a sharper, more confident loss penalty. For pairs with a small cost difference (low confidence), the temperature is lower, making the loss function flatter and more forgiving. This focuses the training gradient on clear-cut preferences.", "pseudocode": "1. Given costs for a batch, normalize them using z-scoring: z_cost_a = zscore(cost_a), z_cost_b = zscore(cost_b).\n2. Compute the difference of the z-scored costs: z_diff = z_cost_a - z_cost_b.\n3. Compute the normalized rank gap between the original costs: rank_diff = rank_gap(cost_a, cost_b).\n4. Create a blended signal by adding the z-score difference and the rank gap. This combines distributional and ordinal information.\n   blended_signal = z_diff + rank_diff.\n5. Compute the adaptive margin by passing the blended signal through a sigmoid function and scaling it by `max_margin`.\n   adaptive_margin = max_margin * sigmoid(blended_signal).\n6. Compute an adaptive temperature. Start with a base temperature and scale it exponentially based on the magnitude of the z-scored cost difference.\n   adaptive_temp = base_temperature * exp(alpha * abs(z_diff)).\n7. Compute the log-probability difference: logp_diff = logp_a - logp_b.\n8. Calculate the core loss term using the adaptive margin and adaptive temperature.\n   core_term = adaptive_temp * (logp_diff - adaptive_margin).\n9. The final loss is the negative logsigmoid of the core term, which penalizes cases where `logp_diff` is smaller than the target `adaptive_margin`.\n   loss = -logsigmoid(core_term).", "hyperparams": {"base_temperature": 1.0, "max_margin": 4.0, "alpha": 0.5}, "operators_used": ["zscore", "rank_gap", "sigmoid", "exp", "logsigmoid"], "implementation_hint": {"expects": ["cost_a", "cost_b", "logp_a", "logp_b"], "returns": "scalar"}}, "fitness": {"hf_like_score": 3.899120330810547, "validation_objective": 3.899120330810547, "generalization_penalty": 0.0, "generalization_objectives": {"20": 3.873674750328064}, "train_score_mean": 4.286870583891869, "train_loss_mean": 0.6600311174988747, "pair_count": 4127570, "config": {"hf": {"problem": "tsp", "hf_steps": 32, "train_problem_size": 20, "valid_problem_sizes": [20], "train_batch_size": 64, "pomo_size": 64, "learning_rate": 0.0003, "weight_decay": 1e-06, "alpha": 0.05, "device": "cuda", "seed": 1234, "num_validation_episodes": 128, "validation_batch_size": 64, "generalization_penalty_weight": 1.0, "pool_version": "v0"}, "free_loss": {"f1_steps": 32, "f2_steps": 100, "f3_enabled": false}}, "loss_ir": {"name": "Z-Scaled Rank-Modulated Hinge Loss", "intuition": "This loss function creates a robust, dual-signal adaptive margin by blending rank-based and distribution-based cost information, and applies it within a stable hinge-loss framework.\n\nInherited Ideas:\n- From Parent 0 (Z-Scored Rank-Adaptive Hinge Loss), it inherits the idea of using z-score normalization on the batch costs (`zscore(costs)`) before any further processing. This stabilizes the cost signal against shifts and scaling in the cost distribution, focusing on relative differences.\n- From Parent 1 (Rank-Normalized Margin Loss), it inherits the core structure of using a sigmoid-transformed signal to create a bounded, adaptive margin (`max_margin * sigmoid(...)`). This ensures the margin grows with preference strength but saturates gracefully.\n- From both parents, it inherits the final loss formulation: `-logsigmoid(temperature * (logp_diff - margin))`, which is a numerically stable hinge-like loss equivalent to `softplus(temperature * (margin - logp_diff))`. It also inherits the use of `rank_gap` to capture relative ordering within the batch.\n\nNew Coupling Ideas:\n1. **Blended Margin Signal**: The margin is derived from a blend of two distinct normalized cost signals. First, the z-scored cost difference (`z_cost_a - z_cost_b`) captures the magnitude of preference based on the batch's statistical distribution. Second, the rank-gap (`rank_gap(cost_a, cost_b)`) captures the ordinal relationship. These two signals are added together *before* being passed to the sigmoid function. This coupling creates a richer margin that is sensitive to both the statistical extremity and the relative rank of the costs, making it more informative than either signal alone.\n2. **Adaptive Temperature Scaling**: The temperature, which controls the sharpness of the loss, is made adaptive. It is scaled by the absolute value of the z-scored cost difference. The term `exp(alpha * abs(z_cost_a - z_cost_b))` acts as a dynamic temperature multiplier. For pairs with a large cost difference (high confidence preference), the temperature increases, creating a sharper, more confident loss penalty. For pairs with a small cost difference (low confidence), the temperature is lower, making the loss function flatter and more forgiving. This focuses the training gradient on clear-cut preferences.", "hyperparams": {"base_temperature": 1.0, "max_margin": 4.0, "alpha": 0.5}, "operators_used": ["zscore", "rank_gap", "sigmoid", "exp", "logsigmoid"]}}}
{"generation": 4, "index": 2, "ir": {"name": "Adaptive Rank-Gap Hinge Loss with Z-Score Scaling", "intuition": "This loss function creates a highly adaptive and stable margin by combining rank-based and distribution-based normalization, and applies it within a hinge-loss framework. The goal is to create a margin that is sensitive to both the relative ordering of costs and their statistical significance within the batch.\n\nInherited Ideas:\n- From Parent 0 (Z-Scored Rank-Adaptive Hinge Loss), it inherits the idea of using `zscore` to normalize the costs before further processing. This makes the loss robust to the absolute scale and shift of the cost distribution.\n- From Parent 1 (Rank-Normalized Margin Loss), it inherits the core structure of using `rank_gap` on costs to determine the magnitude of the margin. This focuses the margin on the relative ordering of preferences within the batch, making it robust to outliers.\n- From both parents, it inherits the use of a temperature-scaled difference between the log-probability gap and an adaptive margin, which is then passed into a `-logsigmoid` function (equivalent to `softplus`) to form the final loss.\n\nNew Coupling Ideas:\n1. **Z-Score Scaled Rank Gap**: Instead of using the raw `rank_gap` or the `rank_gap` of z-scored costs directly, this loss computes the `rank_gap` of the raw costs first, and then multiplies this rank-based signal by the difference of the z-scored costs (`z_cost_a - z_cost_b`). This coupling ensures the margin is primarily driven by the rank difference but is scaled by the statistical significance (in standard deviations) of the cost difference. A large rank gap with a small z-score difference will result in a smaller margin than the same rank gap with a large z-score difference.\n2. **Clamped Margin Signal**: The resulting margin signal is passed through a `clamp` function before being scaled by `max_margin`. This acts as a hard gate, preventing extremely large or negative margin signals from destabilizing the training, providing a robust alternative to the smooth saturation of `tanh` or `sigmoid`.", "pseudocode": "1. For the entire batch, normalize costs using z-scoring to get z_cost_a and z_cost_b.\n2. Compute the normalized rank gap between the original costs: rank_diff = rank_gap(cost_a, cost_b). This captures the relative ordering.\n3. Calculate the difference in z-scored costs: z_cost_diff = z_cost_a - z_cost_b. This captures the statistical magnitude of the difference.\n4. Create a coupled margin signal by multiplying the rank difference by the z-score difference. This scales the rank-based preference by its statistical significance.\n   coupled_signal = rank_diff * z_cost_diff.\n5. Clamp the coupled signal to a reasonable range (e.g., [0, 5]) to ensure stability and prevent extreme margin values. Then scale by max_margin to control the overall magnitude.\n   adaptive_margin = max_margin * clamp(coupled_signal, 0, 5.0).\n6. Compute the log-probability difference: logp_diff = logp_a - logp_b.\n7. Calculate the core loss term, which is the temperature-scaled difference between the log-probability gap and the adaptive margin.\n   core_term = temperature * (logp_diff - adaptive_margin).\n8. The final loss is the negative logsigmoid of the core term, which is numerically stable and penalizes cases where logp_diff is smaller than the target margin.\n   loss = -logsigmoid(core_term).", "hyperparams": {"temperature": 1.0, "max_margin": 2.0}, "operators_used": ["zscore", "rank_gap", "clamp", "logsigmoid"], "implementation_hint": {"expects": ["cost_a", "cost_b", "logp_a", "logp_b"], "returns": "scalar"}}, "fitness": {"hf_like_score": 3.8796780109405518, "validation_objective": 3.8796780109405518, "generalization_penalty": 0.0, "generalization_objectives": {"20": 3.8477851152420044}, "train_score_mean": 4.209790125489235, "train_loss_mean": 0.6781718470156193, "pair_count": 4111513, "config": {"hf": {"problem": "tsp", "hf_steps": 32, "train_problem_size": 20, "valid_problem_sizes": [20], "train_batch_size": 64, "pomo_size": 64, "learning_rate": 0.0003, "weight_decay": 1e-06, "alpha": 0.05, "device": "cuda", "seed": 1234, "num_validation_episodes": 128, "validation_batch_size": 64, "generalization_penalty_weight": 1.0, "pool_version": "v0"}, "free_loss": {"f1_steps": 32, "f2_steps": 100, "f3_enabled": false}}, "loss_ir": {"name": "Adaptive Rank-Gap Hinge Loss with Z-Score Scaling", "intuition": "This loss function creates a highly adaptive and stable margin by combining rank-based and distribution-based normalization, and applies it within a hinge-loss framework. The goal is to create a margin that is sensitive to both the relative ordering of costs and their statistical significance within the batch.\n\nInherited Ideas:\n- From Parent 0 (Z-Scored Rank-Adaptive Hinge Loss), it inherits the idea of using `zscore` to normalize the costs before further processing. This makes the loss robust to the absolute scale and shift of the cost distribution.\n- From Parent 1 (Rank-Normalized Margin Loss), it inherits the core structure of using `rank_gap` on costs to determine the magnitude of the margin. This focuses the margin on the relative ordering of preferences within the batch, making it robust to outliers.\n- From both parents, it inherits the use of a temperature-scaled difference between the log-probability gap and an adaptive margin, which is then passed into a `-logsigmoid` function (equivalent to `softplus`) to form the final loss.\n\nNew Coupling Ideas:\n1. **Z-Score Scaled Rank Gap**: Instead of using the raw `rank_gap` or the `rank_gap` of z-scored costs directly, this loss computes the `rank_gap` of the raw costs first, and then multiplies this rank-based signal by the difference of the z-scored costs (`z_cost_a - z_cost_b`). This coupling ensures the margin is primarily driven by the rank difference but is scaled by the statistical significance (in standard deviations) of the cost difference. A large rank gap with a small z-score difference will result in a smaller margin than the same rank gap with a large z-score difference.\n2. **Clamped Margin Signal**: The resulting margin signal is passed through a `clamp` function before being scaled by `max_margin`. This acts as a hard gate, preventing extremely large or negative margin signals from destabilizing the training, providing a robust alternative to the smooth saturation of `tanh` or `sigmoid`.", "hyperparams": {"temperature": 1.0, "max_margin": 2.0}, "operators_used": ["zscore", "rank_gap", "clamp", "logsigmoid"]}}}
{"generation": 4, "index": 3, "ir": {"name": "Adaptive Rank-Normalized Sigmoid Loss", "intuition": "This loss function creates a robust, adaptive margin by combining rank-based normalization with a sigmoid-scaled cost signal, and applies it within a stable logsigmoid framework. The margin's sensitivity is dynamically adjusted based on the batch's cost distribution.\n\nInherited Ideas:\n- From Parent 0 (Z-Scored Rank-Adaptive Hinge Loss), it inherits the use of `zscore` to normalize the costs before further processing. This makes the loss invariant to the absolute scale and shift of the cost values in a batch, focusing on the relative differences.\n- From Parent 1 (Rank-Normalized Margin Loss), it inherits the core structure of creating an adaptive margin by passing a cost-derived signal through a `sigmoid` function and scaling it by a `max_margin`. This creates a bounded, smoothly saturating margin.\n\nNew Coupling Ideas:\n1. **Dynamic Sigmoid Scaling (Beta Coupling):** Instead of using a fixed `beta` hyperparameter to scale the input to the sigmoid, this child loss computes a dynamic `beta` based on the standard deviation of the costs in the batch. Specifically, `beta = 1.0 / (std_dev(batch_costs) + epsilon)`. When cost variance is high, `beta` is small, desensitizing the margin to large cost differences and preventing saturation. When cost variance is low, `beta` is large, making the margin more sensitive to small cost differences. This couples the margin's responsiveness directly to the statistical properties of the current batch.\n2. **Rank-Gap Weighted Cost Difference:** The input to the scaled sigmoid function is a product of the z-scored cost difference and the `rank_gap`. This combines two signals: the magnitude of the preference (from the z-scored cost difference) and its ordinal importance within the batch (from `rank_gap`). This ensures that pairs with a large rank difference contribute more significantly to the margin, even if their z-scored cost difference is moderate, providing a more nuanced preference signal.", "pseudocode": "1. For the entire batch, compute the mean and standard deviation of the costs. Normalize costs using z-scoring: z_cost_a = (cost_a - mean) / std_dev, z_cost_b = (cost_b - mean) / std_dev.\n2. Compute the normalized rank gap between the original costs: rank_diff = rank_gap(cost_a, cost_b). This captures the ordinal preference strength.\n3. Compute a dynamic scaling factor `beta` based on the batch's cost standard deviation: beta = 1.0 / (std_dev(all_costs) + epsilon). This adapts the margin's sensitivity.\n4. Create a combined preference signal by multiplying the z-scored cost difference with the rank gap: preference_signal = (z_cost_b - z_cost_a) * rank_diff.\n5. Calculate the adaptive margin by passing the preference signal through a dynamically scaled sigmoid: margin = max_margin * sigmoid(beta * preference_signal).\n6. Compute the log-probability difference: logp_diff = logp_a - logp_b.\n7. Calculate the core loss term, scaled by temperature: core_term = temperature * (logp_diff - margin).\n8. The final loss is the negative logsigmoid of the core term, which is equivalent to `softplus(-core_term)` for numerical stability. This penalizes cases where `logp_diff` is smaller than the target `margin`.\n   loss = -logsigmoid(core_term).", "hyperparams": {"temperature": 5.0, "max_margin": 4.0, "epsilon": 1e-06}, "operators_used": ["zscore", "rank_gap", "sigmoid", "logsigmoid"], "implementation_hint": {"expects": ["cost_a", "cost_b", "logp_a", "logp_b"], "returns": "scalar"}}, "fitness": {"hf_like_score": 3.8854223489761353, "validation_objective": 3.8854223489761353, "generalization_penalty": 0.0, "generalization_objectives": {"20": 3.850292921066284}, "train_score_mean": 4.207598939538002, "train_loss_mean": 0.678059620782733, "pair_count": 4111331, "config": {"hf": {"problem": "tsp", "hf_steps": 32, "train_problem_size": 20, "valid_problem_sizes": [20], "train_batch_size": 64, "pomo_size": 64, "learning_rate": 0.0003, "weight_decay": 1e-06, "alpha": 0.05, "device": "cuda", "seed": 1234, "num_validation_episodes": 128, "validation_batch_size": 64, "generalization_penalty_weight": 1.0, "pool_version": "v0"}, "free_loss": {"f1_steps": 32, "f2_steps": 100, "f3_enabled": false}}, "loss_ir": {"name": "Adaptive Rank-Normalized Sigmoid Loss", "intuition": "This loss function creates a robust, adaptive margin by combining rank-based normalization with a sigmoid-scaled cost signal, and applies it within a stable logsigmoid framework. The margin's sensitivity is dynamically adjusted based on the batch's cost distribution.\n\nInherited Ideas:\n- From Parent 0 (Z-Scored Rank-Adaptive Hinge Loss), it inherits the use of `zscore` to normalize the costs before further processing. This makes the loss invariant to the absolute scale and shift of the cost values in a batch, focusing on the relative differences.\n- From Parent 1 (Rank-Normalized Margin Loss), it inherits the core structure of creating an adaptive margin by passing a cost-derived signal through a `sigmoid` function and scaling it by a `max_margin`. This creates a bounded, smoothly saturating margin.\n\nNew Coupling Ideas:\n1. **Dynamic Sigmoid Scaling (Beta Coupling):** Instead of using a fixed `beta` hyperparameter to scale the input to the sigmoid, this child loss computes a dynamic `beta` based on the standard deviation of the costs in the batch. Specifically, `beta = 1.0 / (std_dev(batch_costs) + epsilon)`. When cost variance is high, `beta` is small, desensitizing the margin to large cost differences and preventing saturation. When cost variance is low, `beta` is large, making the margin more sensitive to small cost differences. This couples the margin's responsiveness directly to the statistical properties of the current batch.\n2. **Rank-Gap Weighted Cost Difference:** The input to the scaled sigmoid function is a product of the z-scored cost difference and the `rank_gap`. This combines two signals: the magnitude of the preference (from the z-scored cost difference) and its ordinal importance within the batch (from `rank_gap`). This ensures that pairs with a large rank difference contribute more significantly to the margin, even if their z-scored cost difference is moderate, providing a more nuanced preference signal.", "hyperparams": {"temperature": 5.0, "max_margin": 4.0, "epsilon": 1e-06}, "operators_used": ["zscore", "rank_gap", "sigmoid", "logsigmoid"]}}}
{"generation": 4, "index": 4, "ir": {"name": "Hybrid Z-Score and Rank-Normalized Hinge Loss", "intuition": "This loss function constructs a highly adaptive and stable margin by blending two distinct normalization strategies, and applies this margin within a robust hinge-loss framework.\n\nInherited Ideas:\n- From Parent 0, it inherits the use of `zscore` to normalize costs, making the loss insensitive to the absolute scale or shift of the cost distribution. It also inherits the use of `tanh` to create a smoothly saturating margin from a normalized signal, preventing the target margin from growing uncontrollably.\n- From Parent 1, it inherits the use of `rank_gap` to normalize costs based on their relative ordering within a batch. This makes the margin robust to cost outliers. It also inherits the core loss structure `-logsigmoid(temperature * (logp_diff - adaptive_margin))`, which provides a smooth, bounded, and numerically stable hinge-like penalty.\n\nNew Coupling Ideas:\n1. **Blended Normalized Signal**: Instead of choosing between z-score normalization (from Parent 0) and rank-gap normalization (from Parent 1), this child loss *blends* them. It computes both the z-scored difference and the rank-gap difference from the costs. These two signals are then combined via a weighted average controlled by a new hyperparameter, `blend_ratio`. This allows the margin to be sensitive to both the statistical magnitude of the cost difference (via z-score) and the pure ordinal ranking (via rank-gap), creating a more nuanced and robust preference signal.\n2. **Log-Probability Clamping**: Before computing the final loss, the log-probability difference (`logp_diff`) is clamped to a reasonable range, for example, `[-10, 10]`. This is a stability trick that prevents extremely large or small log-probability differences (which can occur early in training or due to numerical imprecision) from causing gradient explosions or vanishing gradients when multiplied by the temperature, further stabilizing the training process.", "pseudocode": "1. Given costs for a batch, compute two normalized difference signals.\n   a. Z-score the costs and find their difference: z_diff = zscore(cost_a) - zscore(cost_b).\n   b. Compute the normalized rank gap: rank_diff = rank_gap(cost_a, cost_b).\n2. Blend the two normalized signals using a weighted average to create a hybrid signal. The `blend_ratio` controls the contribution of each normalization method.\n   blended_signal = (blend_ratio * z_diff) + ((1 - blend_ratio) * rank_diff).\n3. Create an adaptive margin by passing the blended signal through a `tanh` function and scaling it. This creates a smooth, bounded target separation for the log-probabilities.\n   adaptive_margin = max_margin * tanh(beta * blended_signal).\n4. Compute the log-probability difference: logp_diff = logp_a - logp_b.\n5. As a stability trick, clamp the log-probability difference to a reasonable range.\n   clamped_logp_diff = clamp(logp_diff, min=-10, max=10).\n6. Calculate the core loss term, which is the difference between the clamped log-probability gap and the adaptive margin, scaled by temperature.\n   core_term = temperature * (clamped_logp_diff - adaptive_margin).\n7. The final loss is the negative logsigmoid of the core term. This penalizes cases where the model's preference (`logp_diff`) is smaller than the target `adaptive_margin`.\n   loss = -logsigmoid(core_term).", "hyperparams": {"temperature": 5.0, "max_margin": 4.0, "beta": 1.0, "blend_ratio": 0.5}, "operators_used": ["zscore", "rank_gap", "tanh", "clamp", "logsigmoid"], "implementation_hint": {"expects": ["cost_a", "cost_b", "logp_a", "logp_b"], "returns": "scalar"}}, "fitness": {"hf_like_score": 3.881518602371216, "validation_objective": 3.881518602371216, "generalization_penalty": 0.0, "generalization_objectives": {"20": 3.8556878566741943}, "train_score_mean": 4.209603734314442, "train_loss_mean": 0.6783926393836737, "pair_count": 4111923, "config": {"hf": {"problem": "tsp", "hf_steps": 32, "train_problem_size": 20, "valid_problem_sizes": [20], "train_batch_size": 64, "pomo_size": 64, "learning_rate": 0.0003, "weight_decay": 1e-06, "alpha": 0.05, "device": "cuda", "seed": 1234, "num_validation_episodes": 128, "validation_batch_size": 64, "generalization_penalty_weight": 1.0, "pool_version": "v0"}, "free_loss": {"f1_steps": 32, "f2_steps": 100, "f3_enabled": false}}, "loss_ir": {"name": "Hybrid Z-Score and Rank-Normalized Hinge Loss", "intuition": "This loss function constructs a highly adaptive and stable margin by blending two distinct normalization strategies, and applies this margin within a robust hinge-loss framework.\n\nInherited Ideas:\n- From Parent 0, it inherits the use of `zscore` to normalize costs, making the loss insensitive to the absolute scale or shift of the cost distribution. It also inherits the use of `tanh` to create a smoothly saturating margin from a normalized signal, preventing the target margin from growing uncontrollably.\n- From Parent 1, it inherits the use of `rank_gap` to normalize costs based on their relative ordering within a batch. This makes the margin robust to cost outliers. It also inherits the core loss structure `-logsigmoid(temperature * (logp_diff - adaptive_margin))`, which provides a smooth, bounded, and numerically stable hinge-like penalty.\n\nNew Coupling Ideas:\n1. **Blended Normalized Signal**: Instead of choosing between z-score normalization (from Parent 0) and rank-gap normalization (from Parent 1), this child loss *blends* them. It computes both the z-scored difference and the rank-gap difference from the costs. These two signals are then combined via a weighted average controlled by a new hyperparameter, `blend_ratio`. This allows the margin to be sensitive to both the statistical magnitude of the cost difference (via z-score) and the pure ordinal ranking (via rank-gap), creating a more nuanced and robust preference signal.\n2. **Log-Probability Clamping**: Before computing the final loss, the log-probability difference (`logp_diff`) is clamped to a reasonable range, for example, `[-10, 10]`. This is a stability trick that prevents extremely large or small log-probability differences (which can occur early in training or due to numerical imprecision) from causing gradient explosions or vanishing gradients when multiplied by the temperature, further stabilizing the training process.", "hyperparams": {"temperature": 5.0, "max_margin": 4.0, "beta": 1.0, "blend_ratio": 0.5}, "operators_used": ["zscore", "rank_gap", "tanh", "clamp", "logsigmoid"]}}}
{"generation": 4, "index": 5, "ir": {"name": "Z-Scored Dynamic Margin Loss with Rank-Gap Modulation", "intuition": "This loss function creates a highly adaptive and stable margin by combining distribution-aware and rank-aware normalization, and then dynamically modulating it based on the model's current confidence.\n\nInherited Ideas:\n- From Parent 0, it inherits the use of z-score normalization (`zscore`) on the batch costs. This pre-processing step centers the cost distribution, making the margin calculation robust to shifts and scaling in the absolute cost values.\n- From Parent 1, it inherits the use of `rank_gap` to derive a margin signal. This captures the relative preference ordering within the batch, making the margin robust to cost outliers.\n\nNew Coupling Ideas:\n1. **Dual-Signal Margin Construction**: The margin is constructed from two distinct normalized signals. First, the z-scored costs are used to compute a rank gap (`rank_gap(z_cost_a, z_cost_b)`), which provides a stable, rank-ordered preference signal. This signal is then passed through a `tanh` function to create a smoothly saturating base margin, preventing it from growing uncontrollably.\n2. **Confidence-Based Margin Modulation**: The base margin is dynamically modulated by a factor derived from the model's own log-probabilities. Specifically, `sigmoid(logp_b - logp_a)` is used. When the model is confident in the wrong preference (i.e., `logp_b` is much larger than `logp_a`), this sigmoid term approaches 1, applying the full margin. When the model is already correct (`logp_a` > `logp_b`), the term approaches 0, reducing the margin and thus the penalty. This focuses the training signal on correcting clear mistakes rather than over-optimizing already correct preferences.", "pseudocode": "1. For the entire batch, normalize costs using z-scoring: z_cost_a = zscore(cost_a), z_cost_b = zscore(cost_b).\n2. Compute the normalized rank gap between the z-scored costs to get a stable preference signal in [-1, 1]: rank_diff = rank_gap(z_cost_a, z_cost_b).\n3. Create a smooth, bounded base margin by passing the rank difference through a `tanh` function and scaling it: base_margin = max_margin * tanh(beta * rank_diff).\n4. Compute a confidence modulation factor using the model's log-probabilities. This factor is close to 1 for incorrect preferences and close to 0 for correct ones: confidence_mod = sigmoid(logp_b - logp_a).\n5. Calculate the final dynamic margin by multiplying the base margin with the confidence modulation factor: dynamic_margin = base_margin * confidence_mod.\n6. Compute the log-probability difference: logp_diff = logp_a - logp_b.\n7. Calculate the core term for the loss function, scaled by temperature: core_term = temperature * (logp_diff - dynamic_margin).\n8. The final loss is the negative logsigmoid of the core term. This is a numerically stable hinge-like loss that penalizes cases where `logp_diff` is smaller than the target `dynamic_margin`: loss = -logsigmoid(core_term).", "hyperparams": {"temperature": 5.0, "max_margin": 3.0, "beta": 1.0}, "operators_used": ["zscore", "rank_gap", "tanh", "sigmoid", "logsigmoid"], "implementation_hint": {"expects": ["cost_a", "cost_b", "logp_a", "logp_b"], "returns": "scalar"}}, "fitness": {"hf_like_score": 3.8838576078414917, "validation_objective": 3.8838576078414917, "generalization_penalty": 0.0, "generalization_objectives": {"20": 3.8530778884887695}, "train_score_mean": 4.210277751088142, "train_loss_mean": 0.6783622819930315, "pair_count": 4111622, "config": {"hf": {"problem": "tsp", "hf_steps": 32, "train_problem_size": 20, "valid_problem_sizes": [20], "train_batch_size": 64, "pomo_size": 64, "learning_rate": 0.0003, "weight_decay": 1e-06, "alpha": 0.05, "device": "cuda", "seed": 1234, "num_validation_episodes": 128, "validation_batch_size": 64, "generalization_penalty_weight": 1.0, "pool_version": "v0"}, "free_loss": {"f1_steps": 32, "f2_steps": 100, "f3_enabled": false}}, "loss_ir": {"name": "Z-Scored Dynamic Margin Loss with Rank-Gap Modulation", "intuition": "This loss function creates a highly adaptive and stable margin by combining distribution-aware and rank-aware normalization, and then dynamically modulating it based on the model's current confidence.\n\nInherited Ideas:\n- From Parent 0, it inherits the use of z-score normalization (`zscore`) on the batch costs. This pre-processing step centers the cost distribution, making the margin calculation robust to shifts and scaling in the absolute cost values.\n- From Parent 1, it inherits the use of `rank_gap` to derive a margin signal. This captures the relative preference ordering within the batch, making the margin robust to cost outliers.\n\nNew Coupling Ideas:\n1. **Dual-Signal Margin Construction**: The margin is constructed from two distinct normalized signals. First, the z-scored costs are used to compute a rank gap (`rank_gap(z_cost_a, z_cost_b)`), which provides a stable, rank-ordered preference signal. This signal is then passed through a `tanh` function to create a smoothly saturating base margin, preventing it from growing uncontrollably.\n2. **Confidence-Based Margin Modulation**: The base margin is dynamically modulated by a factor derived from the model's own log-probabilities. Specifically, `sigmoid(logp_b - logp_a)` is used. When the model is confident in the wrong preference (i.e., `logp_b` is much larger than `logp_a`), this sigmoid term approaches 1, applying the full margin. When the model is already correct (`logp_a` > `logp_b`), the term approaches 0, reducing the margin and thus the penalty. This focuses the training signal on correcting clear mistakes rather than over-optimizing already correct preferences.", "hyperparams": {"temperature": 5.0, "max_margin": 3.0, "beta": 1.0}, "operators_used": ["zscore", "rank_gap", "tanh", "sigmoid", "logsigmoid"]}}}
{"generation": 4, "index": 6, "ir": {"name": "Z-Score Saturated Rank-Gap Loss", "intuition": "This loss function creates a highly robust and adaptive preference signal by combining distribution-based normalization, rank-based preference strength, and a saturating margin within a stable hinge-loss framework.\n\nInherited Ideas:\n- From Parent 0, it inherits the idea of normalizing costs using z-scoring (`zscore`) before any other processing. This centers the cost distribution and makes the loss less sensitive to the absolute scale or shifts in cost values, focusing on the relative differences within a batch.\n- From Parent 1, it inherits the use of `sigmoid` to transform a normalized difference signal into a bounded margin between 0 and a maximum value. This ensures the target separation for log-probabilities saturates gracefully and does not grow uncontrollably.\n\nNew Coupling Ideas:\n1. **Hybrid Cost Signal (Z-Score + Rank-Gap):** Instead of using either z-scored costs or rank-gap directly, this loss couples them. First, it z-scores the costs. Then, it computes the `rank_gap` of these z-scored costs. This two-stage normalization makes the signal extremely robust to both outliers (via `rank_gap`) and overall distribution shifts (via `zscore`), capturing a pure, relative preference strength.\n2. **Saturating Margin with Baseline:** The adaptive margin is created by applying a `sigmoid` to the hybrid cost signal (`rank_gap(zscore(costs))`), scaling it by `max_margin`, and adding a small `min_margin`. This ensures that even for pairs with a very small rank gap, there is still a minimal target separation, preventing the loss from becoming zero when a preference still exists.", "pseudocode": "1. For the entire batch, normalize the costs `cost_a` and `cost_b` using a z-score transformation: z_cost_a = zscore(cost_a), z_cost_b = zscore(cost_b).\n2. Compute the normalized rank gap of these z-scored costs. This creates a highly stable preference signal, `rank_diff`, typically between -1 and 1.\n   rank_diff = rank_gap(z_cost_a, z_cost_b).\n3. Create an adaptive margin. Pass the `rank_diff` through a `sigmoid` function to map it to a value between 0 and 1. Scale this by `max_margin` and add a `min_margin` baseline.\n   adaptive_margin = min_margin + max_margin * sigmoid(rank_diff).\n4. Compute the difference in log-probabilities: logp_diff = logp_a - logp_b.\n5. Calculate the core loss term, which is the difference between the log-probability gap and the adaptive margin, scaled by a temperature parameter.\n   core_term = temperature * (logp_diff - adaptive_margin).\n6. The final loss is the negative logsigmoid of the core term. This is equivalent to `softplus(-core_term)`, providing a numerically stable hinge-like loss that penalizes cases where `logp_diff` is less than the target `adaptive_margin`.\n   loss = -logsigmoid(core_term).", "hyperparams": {"temperature": 10.0, "max_margin": 4.0, "min_margin": 0.05}, "operators_used": ["zscore", "rank_gap", "sigmoid", "logsigmoid"], "implementation_hint": {"expects": ["cost_a", "cost_b", "logp_a", "logp_b"], "returns": "scalar"}}, "fitness": {"hf_like_score": 3.8858999013900757, "validation_objective": 3.8858999013900757, "generalization_penalty": 0.0, "generalization_objectives": {"20": 3.849687099456787}, "train_score_mean": 4.208429113030434, "train_loss_mean": 0.6779971178621054, "pair_count": 4111897, "config": {"hf": {"problem": "tsp", "hf_steps": 32, "train_problem_size": 20, "valid_problem_sizes": [20], "train_batch_size": 64, "pomo_size": 64, "learning_rate": 0.0003, "weight_decay": 1e-06, "alpha": 0.05, "device": "cuda", "seed": 1234, "num_validation_episodes": 128, "validation_batch_size": 64, "generalization_penalty_weight": 1.0, "pool_version": "v0"}, "free_loss": {"f1_steps": 32, "f2_steps": 100, "f3_enabled": false}}, "loss_ir": {"name": "Z-Score Saturated Rank-Gap Loss", "intuition": "This loss function creates a highly robust and adaptive preference signal by combining distribution-based normalization, rank-based preference strength, and a saturating margin within a stable hinge-loss framework.\n\nInherited Ideas:\n- From Parent 0, it inherits the idea of normalizing costs using z-scoring (`zscore`) before any other processing. This centers the cost distribution and makes the loss less sensitive to the absolute scale or shifts in cost values, focusing on the relative differences within a batch.\n- From Parent 1, it inherits the use of `sigmoid` to transform a normalized difference signal into a bounded margin between 0 and a maximum value. This ensures the target separation for log-probabilities saturates gracefully and does not grow uncontrollably.\n\nNew Coupling Ideas:\n1. **Hybrid Cost Signal (Z-Score + Rank-Gap):** Instead of using either z-scored costs or rank-gap directly, this loss couples them. First, it z-scores the costs. Then, it computes the `rank_gap` of these z-scored costs. This two-stage normalization makes the signal extremely robust to both outliers (via `rank_gap`) and overall distribution shifts (via `zscore`), capturing a pure, relative preference strength.\n2. **Saturating Margin with Baseline:** The adaptive margin is created by applying a `sigmoid` to the hybrid cost signal (`rank_gap(zscore(costs))`), scaling it by `max_margin`, and adding a small `min_margin`. This ensures that even for pairs with a very small rank gap, there is still a minimal target separation, preventing the loss from becoming zero when a preference still exists.", "hyperparams": {"temperature": 10.0, "max_margin": 4.0, "min_margin": 0.05}, "operators_used": ["zscore", "rank_gap", "sigmoid", "logsigmoid"]}}}
{"generation": 4, "index": 7, "ir": {"name": "Z-Score Modulated Rank-Hinge Loss", "intuition": "This loss function creates a highly adaptive and stable preference signal by modulating a rank-based margin with a z-score of the raw cost difference, all within a numerically stable hinge-loss framework.\n\nInherited Ideas:\n- From Parent 0, it inherits the use of `tanh` to create a smooth, saturating margin from a normalized signal. This prevents the margin from growing uncontrollably.\n- From Parent 1, it inherits the use of `rank_gap` to normalize the cost difference based on its relative ordering within the batch. This provides robustness to outliers and the absolute scale of costs.\n- From both parents, it inherits the core structure of `softplus(margin - logp_diff)` (which is equivalent to `-logsigmoid(logp_diff - margin)`), providing a stable, one-sided penalty.\n\nNew Coupling Ideas:\n1. **Hybrid Margin Signal**: The adaptive margin is a product of two distinct normalized signals. The first is a rank-based signal (`tanh(rank_gap)`), which captures the relative preference strength. The second is a distribution-based signal (`sigmoid(zscore(cost_b - cost_a))`), which captures the magnitude of the cost difference relative to the batch's statistics. Multiplying them couples rank and magnitude: a large margin is only created when a preference is both high-ranking *and* has a statistically significant cost difference. This prevents pairs with small but high-ranking cost differences from creating an undeservedly large margin.\n2. **Temperature as a Scaling Factor on Margin**: Instead of scaling the entire loss term `(logp_diff - margin)`, the `temperature` hyperparameter is used to directly scale the `adaptive_margin` itself. This reinterprets temperature as a control over the desired separation in log-probability space, making its effect more direct and interpretable. The final loss is `softplus(temperature * adaptive_margin - logp_diff)`, which encourages `logp_a - logp_b` to be greater than a temperature-scaled adaptive margin.", "pseudocode": "1. For the entire batch, compute the z-score of the cost difference for each pair: z_diff = zscore(cost_b - cost_a).\n2. For the entire batch, compute the normalized rank gap: rank_diff = rank_gap(cost_a, cost_b).\n3. Create a magnitude-aware signal by passing the z-scored difference through a sigmoid function: magnitude_signal = sigmoid(z_diff).\n4. Create a rank-aware signal by passing the rank gap through a tanh function: rank_signal = tanh(rank_diff).\n5. Combine these two signals multiplicatively and scale by a maximum margin to form the adaptive margin. This couples the rank and magnitude information.\n   adaptive_margin = max_margin * rank_signal * magnitude_signal.\n6. Scale the adaptive margin by the temperature parameter to set the target log-probability separation.\n   target_separation = temperature * adaptive_margin.\n7. Compute the log-probability difference: logp_diff = logp_a - logp_b.\n8. Calculate the final loss using a softplus function, which penalizes cases where the log-probability difference is smaller than the target separation. This is numerically stable.\n   loss = softplus(target_separation - logp_diff).", "hyperparams": {"max_margin": 5.0, "temperature": 1.0}, "operators_used": ["zscore", "rank_gap", "sigmoid", "tanh", "softplus"], "implementation_hint": {"expects": ["cost_a", "cost_b", "logp_a", "logp_b"], "returns": "scalar"}}, "fitness": {"hf_like_score": 3.887444257736206, "validation_objective": 3.887444257736206, "generalization_penalty": 0.0, "generalization_objectives": {"20": 3.849720597267151}, "train_score_mean": 4.209002189338207, "train_loss_mean": 0.6782727595418692, "pair_count": 4111746, "config": {"hf": {"problem": "tsp", "hf_steps": 32, "train_problem_size": 20, "valid_problem_sizes": [20], "train_batch_size": 64, "pomo_size": 64, "learning_rate": 0.0003, "weight_decay": 1e-06, "alpha": 0.05, "device": "cuda", "seed": 1234, "num_validation_episodes": 128, "validation_batch_size": 64, "generalization_penalty_weight": 1.0, "pool_version": "v0"}, "free_loss": {"f1_steps": 32, "f2_steps": 100, "f3_enabled": false}}, "loss_ir": {"name": "Z-Score Modulated Rank-Hinge Loss", "intuition": "This loss function creates a highly adaptive and stable preference signal by modulating a rank-based margin with a z-score of the raw cost difference, all within a numerically stable hinge-loss framework.\n\nInherited Ideas:\n- From Parent 0, it inherits the use of `tanh` to create a smooth, saturating margin from a normalized signal. This prevents the margin from growing uncontrollably.\n- From Parent 1, it inherits the use of `rank_gap` to normalize the cost difference based on its relative ordering within the batch. This provides robustness to outliers and the absolute scale of costs.\n- From both parents, it inherits the core structure of `softplus(margin - logp_diff)` (which is equivalent to `-logsigmoid(logp_diff - margin)`), providing a stable, one-sided penalty.\n\nNew Coupling Ideas:\n1. **Hybrid Margin Signal**: The adaptive margin is a product of two distinct normalized signals. The first is a rank-based signal (`tanh(rank_gap)`), which captures the relative preference strength. The second is a distribution-based signal (`sigmoid(zscore(cost_b - cost_a))`), which captures the magnitude of the cost difference relative to the batch's statistics. Multiplying them couples rank and magnitude: a large margin is only created when a preference is both high-ranking *and* has a statistically significant cost difference. This prevents pairs with small but high-ranking cost differences from creating an undeservedly large margin.\n2. **Temperature as a Scaling Factor on Margin**: Instead of scaling the entire loss term `(logp_diff - margin)`, the `temperature` hyperparameter is used to directly scale the `adaptive_margin` itself. This reinterprets temperature as a control over the desired separation in log-probability space, making its effect more direct and interpretable. The final loss is `softplus(temperature * adaptive_margin - logp_diff)`, which encourages `logp_a - logp_b` to be greater than a temperature-scaled adaptive margin.", "hyperparams": {"max_margin": 5.0, "temperature": 1.0}, "operators_used": ["zscore", "rank_gap", "sigmoid", "tanh", "softplus"]}}}
{"generation": 5, "index": 0, "ir": {"name": "Hybrid Z-Score and Rank-Adaptive Sigmoid Loss", "intuition": "This loss function creates a highly adaptive and robust margin by synthesizing rank-based and distribution-based normalization, then applies it within a stable sigmoid-based loss framework.\n\nInherited Ideas:\n- From Parent 0, it inherits the idea of pre-normalizing costs using a z-score transformation (`zscore(costs)`). This centers the cost distribution and makes the subsequent margin calculation less sensitive to the absolute scale of costs, focusing on the batch's statistical properties.\n- From Parent 1, it inherits the core structure of creating an adaptive margin by passing a normalized cost signal through a `sigmoid` function and scaling it. This ensures the margin is bounded (between 0 and `max_margin`) and grows monotonically with the preference strength.\n\nNew Coupling Ideas:\n1. **Hybrid Normalization Signal**: The signal fed into the sigmoid function is a hybrid of two normalization techniques. First, costs are z-scored as in Parent 0. Then, the `rank_gap` is computed on these z-scored costs. This double-normalization step ensures the margin is robust to both the overall statistical distribution (via z-score) and local rank ordering (via rank-gap), effectively isolating the pure relative preference signal from distributional artifacts.\n2. **Margin Baseline from Log-Probabilities**: Instead of a fixed minimum margin, the baseline for the target separation is derived from the log-probability difference itself, scaled by a factor `alpha`. The final target margin becomes `alpha * logp_diff + adaptive_margin`. This couples the margin directly to the model's current confidence. If the model is already very confident (`logp_diff` is large), the margin target is increased, pushing for even better separation. If it is uncertain (`logp_diff` is small), the margin relies more on the cost-derived `adaptive_margin`. This creates a dynamic target that adapts to both the ground-truth cost difference and the model's own state.", "pseudocode": "1. Given costs for a batch, normalize them using z-scoring: z_cost_a = zscore(cost_a), z_cost_b = zscore(cost_b).\n2. Compute the rank-normalized gap between the z-scored costs to get a robust preference signal, `rank_diff`.\n   rank_diff = rank_gap(z_cost_a, z_cost_b).\n3. Create the primary adaptive margin by passing `rank_diff` through a sigmoid function and scaling it. This component is based purely on the normalized cost difference.\n   adaptive_margin = max_margin * sigmoid(beta * rank_diff).\n4. Compute the log-probability difference: logp_diff = logp_a - logp_b.\n5. Create the final dynamic margin by adding a term proportional to the log-probability difference itself. This couples the target to the model's confidence.\n   dynamic_margin = alpha * logp_diff + adaptive_margin.\n6. Calculate the core loss term. This is the difference between the log-probability gap and the dynamic margin, scaled by a temperature parameter.\n   core_term = temperature * (logp_diff - dynamic_margin).\n7. The final loss is the negative logsigmoid of the core term. This penalizes cases where `logp_diff` is smaller than the target `dynamic_margin` and is equivalent to `softplus(-core_term)` for numerical stability.\n   loss = -logsigmoid(core_term).", "hyperparams": {"temperature": 5.0, "max_margin": 4.0, "beta": 1.0, "alpha": 0.1}, "operators_used": ["zscore", "rank_gap", "sigmoid", "logsigmoid"], "implementation_hint": {"expects": ["cost_a", "cost_b", "logp_a", "logp_b"], "returns": "scalar"}}, "fitness": {"hf_like_score": 3.8755193948745728, "validation_objective": 3.8755193948745728, "generalization_penalty": 0.0, "generalization_objectives": {"20": 3.847528576850891}, "train_score_mean": 4.203195676207542, "train_loss_mean": 0.6676357798278332, "pair_count": 4112838, "config": {"hf": {"problem": "tsp", "hf_steps": 32, "train_problem_size": 20, "valid_problem_sizes": [20], "train_batch_size": 64, "pomo_size": 64, "learning_rate": 0.0003, "weight_decay": 1e-06, "alpha": 0.05, "device": "cuda", "seed": 1234, "num_validation_episodes": 128, "validation_batch_size": 64, "generalization_penalty_weight": 1.0, "pool_version": "v0"}, "free_loss": {"f1_steps": 32, "f2_steps": 100, "f3_enabled": false}}, "loss_ir": {"name": "Hybrid Z-Score and Rank-Adaptive Sigmoid Loss", "intuition": "This loss function creates a highly adaptive and robust margin by synthesizing rank-based and distribution-based normalization, then applies it within a stable sigmoid-based loss framework.\n\nInherited Ideas:\n- From Parent 0, it inherits the idea of pre-normalizing costs using a z-score transformation (`zscore(costs)`). This centers the cost distribution and makes the subsequent margin calculation less sensitive to the absolute scale of costs, focusing on the batch's statistical properties.\n- From Parent 1, it inherits the core structure of creating an adaptive margin by passing a normalized cost signal through a `sigmoid` function and scaling it. This ensures the margin is bounded (between 0 and `max_margin`) and grows monotonically with the preference strength.\n\nNew Coupling Ideas:\n1. **Hybrid Normalization Signal**: The signal fed into the sigmoid function is a hybrid of two normalization techniques. First, costs are z-scored as in Parent 0. Then, the `rank_gap` is computed on these z-scored costs. This double-normalization step ensures the margin is robust to both the overall statistical distribution (via z-score) and local rank ordering (via rank-gap), effectively isolating the pure relative preference signal from distributional artifacts.\n2. **Margin Baseline from Log-Probabilities**: Instead of a fixed minimum margin, the baseline for the target separation is derived from the log-probability difference itself, scaled by a factor `alpha`. The final target margin becomes `alpha * logp_diff + adaptive_margin`. This couples the margin directly to the model's current confidence. If the model is already very confident (`logp_diff` is large), the margin target is increased, pushing for even better separation. If it is uncertain (`logp_diff` is small), the margin relies more on the cost-derived `adaptive_margin`. This creates a dynamic target that adapts to both the ground-truth cost difference and the model's own state.", "hyperparams": {"temperature": 5.0, "max_margin": 4.0, "beta": 1.0, "alpha": 0.1}, "operators_used": ["zscore", "rank_gap", "sigmoid", "logsigmoid"]}}}
{"generation": 5, "index": 1, "ir": {"name": "Softplus-Clipped Rank-Gap Loss", "intuition": "This loss function creates a stable and adaptive preference learning signal by combining rank-based normalization with a clipped, non-linear margin.\n\nInherited Ideas:\n- From Parent 0 (Z-Scored Rank-Adaptive Hinge Loss), it inherits the use of `tanh` to create a smoothly saturating margin from a normalized signal. This prevents the target margin from growing uncontrollably, which enhances numerical stability.\n- From Parent 1 (Rank-Normalized Margin Loss), it inherits the core idea of using `rank_gap` to normalize the cost difference. This makes the margin robust to the absolute scale and distribution of costs, focusing instead on the relative ordering of solutions within the current batch.\n\nNew Coupling Ideas:\n1. **Softplus-Clipped Margin**: Instead of directly using the `tanh`-scaled rank gap as the margin, the margin signal first passes through a `softplus` function. This non-linearly amplifies smaller rank gaps while maintaining a smooth, non-negative output. The result is then `clamp`ed to a maximum value (`max_margin`). This coupling creates a margin that is more sensitive to subtle rank differences while preventing it from becoming excessively large for strongly separated pairs, combining the benefits of non-linear amplification and hard clipping for stability.\n2. **Direct Log-Probability Difference**: The loss is calculated directly on the difference between the log-probability gap and the adaptive margin, `logp_diff - adaptive_margin`. This is a simpler structure than the `logsigmoid` used by both parents, acting as a hinge loss. The `relu` operator ensures that the loss is zero when the log-probability difference already exceeds the target margin, focusing the training effort only on preference violations.", "pseudocode": "1. Given costs for a batch, compute the normalized rank gap between `cost_a` and `cost_b`. This provides a stable signal of relative preference strength within the batch.\n   rank_diff = rank_gap(cost_a, cost_b).\n2. Create a preliminary margin signal by passing the rank difference through a `tanh` function and scaling it by `beta`. This creates a smooth, bounded signal centered around zero.\n   tanh_signal = tanh(beta * rank_diff).\n3. Transform this signal into a non-negative, non-linear margin using `softplus`. This amplifies smaller positive signals more than a linear function would.\n   softplus_margin = softplus(tanh_signal).\n4. Clip the resulting margin to a maximum value to ensure stability and prevent outlier pairs from dominating the loss.\n   adaptive_margin = clamp(softplus_margin, min=0, max=max_margin).\n5. Compute the log-probability difference: logp_diff = logp_a - logp_b.\n6. Calculate the final loss using a hinge-like structure. The loss is the amount by which the `logp_diff` fails to meet the `adaptive_margin`, scaled by a temperature. The `relu` ensures the loss is non-negative.\n   loss = relu(temperature * (adaptive_margin - logp_diff)).", "hyperparams": {"beta": 2.0, "max_margin": 4.0, "temperature": 1.0}, "operators_used": ["rank_gap", "tanh", "softplus", "clamp", "relu"], "implementation_hint": {"expects": ["cost_a", "cost_b", "logp_a", "logp_b"], "returns": "scalar"}}, "fitness": {"hf_like_score": 3.8804441690444946, "validation_objective": 3.8804441690444946, "generalization_penalty": 0.0, "generalization_objectives": {"20": 3.850175619125366}, "train_score_mean": 4.20798647403717, "train_loss_mean": 0.6783912051469088, "pair_count": 4112014, "config": {"hf": {"problem": "tsp", "hf_steps": 32, "train_problem_size": 20, "valid_problem_sizes": [20], "train_batch_size": 64, "pomo_size": 64, "learning_rate": 0.0003, "weight_decay": 1e-06, "alpha": 0.05, "device": "cuda", "seed": 1234, "num_validation_episodes": 128, "validation_batch_size": 64, "generalization_penalty_weight": 1.0, "pool_version": "v0"}, "free_loss": {"f1_steps": 32, "f2_steps": 100, "f3_enabled": false}}, "loss_ir": {"name": "Softplus-Clipped Rank-Gap Loss", "intuition": "This loss function creates a stable and adaptive preference learning signal by combining rank-based normalization with a clipped, non-linear margin.\n\nInherited Ideas:\n- From Parent 0 (Z-Scored Rank-Adaptive Hinge Loss), it inherits the use of `tanh` to create a smoothly saturating margin from a normalized signal. This prevents the target margin from growing uncontrollably, which enhances numerical stability.\n- From Parent 1 (Rank-Normalized Margin Loss), it inherits the core idea of using `rank_gap` to normalize the cost difference. This makes the margin robust to the absolute scale and distribution of costs, focusing instead on the relative ordering of solutions within the current batch.\n\nNew Coupling Ideas:\n1. **Softplus-Clipped Margin**: Instead of directly using the `tanh`-scaled rank gap as the margin, the margin signal first passes through a `softplus` function. This non-linearly amplifies smaller rank gaps while maintaining a smooth, non-negative output. The result is then `clamp`ed to a maximum value (`max_margin`). This coupling creates a margin that is more sensitive to subtle rank differences while preventing it from becoming excessively large for strongly separated pairs, combining the benefits of non-linear amplification and hard clipping for stability.\n2. **Direct Log-Probability Difference**: The loss is calculated directly on the difference between the log-probability gap and the adaptive margin, `logp_diff - adaptive_margin`. This is a simpler structure than the `logsigmoid` used by both parents, acting as a hinge loss. The `relu` operator ensures that the loss is zero when the log-probability difference already exceeds the target margin, focusing the training effort only on preference violations.", "hyperparams": {"beta": 2.0, "max_margin": 4.0, "temperature": 1.0}, "operators_used": ["rank_gap", "tanh", "softplus", "clamp", "relu"]}}}
{"generation": 5, "index": 2, "ir": {"name": "Rank-Clipped Sigmoid-Margin Loss", "intuition": "This loss function creates a robust, adaptive margin by combining z-score normalization with a sigmoid-based margin, and introduces a clipping mechanism based on rank to prevent noisy updates from pairs with insignificant cost differences.\n\nInherited Ideas:\n- From Parent 0 (Z-Scored Rank-Adaptive Hinge Loss), it inherits the use of `zscore` to normalize costs before any other processing. This makes the loss signal robust to the absolute scale and distribution of costs in a batch, focusing on relative differences.\n- From Parent 1 (Rank-Normalized Margin Loss), it inherits the core structure of using `sigmoid` on a cost-derived signal to create a bounded, adaptive margin. It also inherits the final loss formulation: `-logsigmoid(temperature * (logp_diff - margin))`, which is a stable and smooth way to penalize preference violations.\n\nNew Coupling Ideas:\n1. **Z-Score + Sigmoid Margin**: Instead of using `rank_gap` as the input to the margin function, this child loss uses the difference of z-scored costs (`z_cost_b - z_cost_a`). This signal is then passed through a `sigmoid` function. This coupling creates a margin that is sensitive to the statistical magnitude of the cost difference within the batch, while still being bounded and smooth, preventing outliers from creating excessively large margins.\n2. **Rank-Based Loss Clipping**: The loss for a given pair is multiplied by a binary mask derived from `rank_gap`. The loss is only applied (`mask=1`) if the absolute rank difference between the two items is above a certain threshold (`rank_clip_threshold`). If the rank difference is too small, the pair is considered noisy or ambiguous, and its contribution to the loss is zeroed out (`mask=0`). This prevents the model from being updated on pairs with very similar costs, focusing training on clear preferences and improving stability.", "pseudocode": "1. For the entire batch, normalize costs using z-scoring: z_cost_a = zscore(cost_a), z_cost_b = zscore(cost_b).\n2. Compute the difference in z-scored costs: z_cost_diff = z_cost_b - z_cost_a.\n3. Create an adaptive margin by passing the z-scored cost difference through a `sigmoid` function and scaling it. This ensures the margin is bounded between 0 and `max_margin`.\n   adaptive_margin = max_margin * sigmoid(z_cost_diff).\n4. Compute the log-probability difference: logp_diff = logp_a - logp_b.\n5. Compute the normalized rank gap between the original costs: rank_diff = rank_gap(cost_a, cost_b).\n6. Create a binary clipping mask. The mask is 1 if the absolute rank difference is greater than a threshold, and 0 otherwise. This filters out pairs with ambiguous or noisy preference signals.\n   rank_mask = relu(sign(abs(rank_diff) - rank_clip_threshold)).\n7. Calculate the core loss term, which is the difference between the log-probability gap and the adaptive margin, scaled by temperature.\n   core_term = temperature * (logp_diff - adaptive_margin).\n8. The final loss is the negative logsigmoid of the core term, multiplied by the rank-based clipping mask. This selectively applies the loss only to pairs with a clear rank separation.\n   loss = rank_mask * -logsigmoid(core_term).", "hyperparams": {"temperature": 5.0, "max_margin": 4.0, "rank_clip_threshold": 0.05}, "operators_used": ["zscore", "sigmoid", "rank_gap", "relu", "logsigmoid"], "implementation_hint": {"expects": ["cost_a", "cost_b", "logp_a", "logp_b"], "returns": "scalar"}}, "fitness": {"hf_like_score": 3.885901689529419, "validation_objective": 3.885901689529419, "generalization_penalty": 0.0, "generalization_objectives": {"20": 3.8443753719329834}, "train_score_mean": 4.2095707431435585, "train_loss_mean": 0.6782175693660975, "pair_count": 4111663, "config": {"hf": {"problem": "tsp", "hf_steps": 32, "train_problem_size": 20, "valid_problem_sizes": [20], "train_batch_size": 64, "pomo_size": 64, "learning_rate": 0.0003, "weight_decay": 1e-06, "alpha": 0.05, "device": "cuda", "seed": 1234, "num_validation_episodes": 128, "validation_batch_size": 64, "generalization_penalty_weight": 1.0, "pool_version": "v0"}, "free_loss": {"f1_steps": 32, "f2_steps": 100, "f3_enabled": false}}, "loss_ir": {"name": "Rank-Clipped Sigmoid-Margin Loss", "intuition": "This loss function creates a robust, adaptive margin by combining z-score normalization with a sigmoid-based margin, and introduces a clipping mechanism based on rank to prevent noisy updates from pairs with insignificant cost differences.\n\nInherited Ideas:\n- From Parent 0 (Z-Scored Rank-Adaptive Hinge Loss), it inherits the use of `zscore` to normalize costs before any other processing. This makes the loss signal robust to the absolute scale and distribution of costs in a batch, focusing on relative differences.\n- From Parent 1 (Rank-Normalized Margin Loss), it inherits the core structure of using `sigmoid` on a cost-derived signal to create a bounded, adaptive margin. It also inherits the final loss formulation: `-logsigmoid(temperature * (logp_diff - margin))`, which is a stable and smooth way to penalize preference violations.\n\nNew Coupling Ideas:\n1. **Z-Score + Sigmoid Margin**: Instead of using `rank_gap` as the input to the margin function, this child loss uses the difference of z-scored costs (`z_cost_b - z_cost_a`). This signal is then passed through a `sigmoid` function. This coupling creates a margin that is sensitive to the statistical magnitude of the cost difference within the batch, while still being bounded and smooth, preventing outliers from creating excessively large margins.\n2. **Rank-Based Loss Clipping**: The loss for a given pair is multiplied by a binary mask derived from `rank_gap`. The loss is only applied (`mask=1`) if the absolute rank difference between the two items is above a certain threshold (`rank_clip_threshold`). If the rank difference is too small, the pair is considered noisy or ambiguous, and its contribution to the loss is zeroed out (`mask=0`). This prevents the model from being updated on pairs with very similar costs, focusing training on clear preferences and improving stability.", "hyperparams": {"temperature": 5.0, "max_margin": 4.0, "rank_clip_threshold": 0.05}, "operators_used": ["zscore", "sigmoid", "rank_gap", "relu", "logsigmoid"]}}}
{"generation": 5, "index": 3, "ir": {"name": "Z-Clipped Rank-Margin Loss", "intuition": "This loss function creates a highly stable and adaptive margin by combining rank-based and distribution-based normalization, and then applies it within a classic logsigmoid framework. It is designed to be robust to outliers in both cost and log-probability differences.\n\nInherited Ideas:\n- From Parent 0 (Z-Scored Rank-Adaptive Hinge Loss), it inherits the idea of using a z-score normalization on the costs (`zscore(cost)`) as a pre-processing step. This centers the cost distribution and makes the subsequent margin calculation less sensitive to the absolute scale of costs.\n- From Parent 1 (Rank-Normalized Margin Loss), it inherits the core concept of using `rank_gap` on the costs to create a margin that is based on relative ordering within the batch, making it robust to cost outliers.\n\nNew Coupling Ideas:\n1. **Sequential Normalization for Margin**: The margin calculation combines the inherited ideas in a novel sequence. First, costs are z-scored. Then, the `rank_gap` is computed on these z-scored costs. This hybrid approach ensures the margin is sensitive to both the statistical distribution (via z-score) and the relative ordering (via rank_gap) of the costs, creating a very stable signal. The margin is then formed by scaling this rank-gap by `max_margin`.\n2. **Log-Probability Clipping**: To prevent the loss from being dominated by pairs with extremely large log-probability differences (which can cause gradient instability), the `logp_diff` is symmetrically clipped using `clamp`. This ensures that the input to the logsigmoid function remains within a controlled range, improving numerical stability without sacrificing the core preference signal for most pairs.", "pseudocode": "1. Given costs for a batch, normalize them using z-scoring: z_cost_a = zscore(cost_a), z_cost_b = zscore(cost_b).\n2. Compute the rank-normalized gap between the z-scored costs. This provides a stable signal of relative preference strength, typically in [-1, 1].\n   rank_diff = rank_gap(z_cost_a, z_cost_b).\n3. Create an adaptive margin by scaling this rank difference. This creates a target separation for the log-probabilities that is robust to cost outliers and scale.\n   adaptive_margin = max_margin * rank_diff.\n4. Compute the log-probability difference: logp_diff = logp_a - logp_b.\n5. To improve numerical stability, clip the log-probability difference to a reasonable range. This prevents extreme values from causing gradient explosions.\n   clipped_logp_diff = clamp(logp_diff, min=-clip_value, max=clip_value).\n6. Calculate the core loss term. This is the difference between the clipped log-probability gap and the adaptive margin, scaled by a temperature parameter.\n   core_term = temperature * (clipped_logp_diff - adaptive_margin).\n7. The final loss is the negative logsigmoid of the core term. This is equivalent to `softplus(-core_term)` and penalizes cases where the model's log-probability difference does not meet the target margin.\n   loss = -logsigmoid(core_term).", "hyperparams": {"temperature": 5.0, "max_margin": 3.0, "clip_value": 10.0}, "operators_used": ["zscore", "rank_gap", "clamp", "logsigmoid"], "implementation_hint": {"expects": ["cost_a", "cost_b", "logp_a", "logp_b"], "returns": "scalar"}}, "fitness": {"hf_like_score": 3.884917974472046, "validation_objective": 3.884917974472046, "generalization_penalty": 0.0, "generalization_objectives": {"20": 3.8538684844970703}, "train_score_mean": 4.209076553583145, "train_loss_mean": 0.6783359088003635, "pair_count": 4111278, "config": {"hf": {"problem": "tsp", "hf_steps": 32, "train_problem_size": 20, "valid_problem_sizes": [20], "train_batch_size": 64, "pomo_size": 64, "learning_rate": 0.0003, "weight_decay": 1e-06, "alpha": 0.05, "device": "cuda", "seed": 1234, "num_validation_episodes": 128, "validation_batch_size": 64, "generalization_penalty_weight": 1.0, "pool_version": "v0"}, "free_loss": {"f1_steps": 32, "f2_steps": 100, "f3_enabled": false}}, "loss_ir": {"name": "Z-Clipped Rank-Margin Loss", "intuition": "This loss function creates a highly stable and adaptive margin by combining rank-based and distribution-based normalization, and then applies it within a classic logsigmoid framework. It is designed to be robust to outliers in both cost and log-probability differences.\n\nInherited Ideas:\n- From Parent 0 (Z-Scored Rank-Adaptive Hinge Loss), it inherits the idea of using a z-score normalization on the costs (`zscore(cost)`) as a pre-processing step. This centers the cost distribution and makes the subsequent margin calculation less sensitive to the absolute scale of costs.\n- From Parent 1 (Rank-Normalized Margin Loss), it inherits the core concept of using `rank_gap` on the costs to create a margin that is based on relative ordering within the batch, making it robust to cost outliers.\n\nNew Coupling Ideas:\n1. **Sequential Normalization for Margin**: The margin calculation combines the inherited ideas in a novel sequence. First, costs are z-scored. Then, the `rank_gap` is computed on these z-scored costs. This hybrid approach ensures the margin is sensitive to both the statistical distribution (via z-score) and the relative ordering (via rank_gap) of the costs, creating a very stable signal. The margin is then formed by scaling this rank-gap by `max_margin`.\n2. **Log-Probability Clipping**: To prevent the loss from being dominated by pairs with extremely large log-probability differences (which can cause gradient instability), the `logp_diff` is symmetrically clipped using `clamp`. This ensures that the input to the logsigmoid function remains within a controlled range, improving numerical stability without sacrificing the core preference signal for most pairs.", "hyperparams": {"temperature": 5.0, "max_margin": 3.0, "clip_value": 10.0}, "operators_used": ["zscore", "rank_gap", "clamp", "logsigmoid"]}}}
{"generation": 5, "index": 4, "ir": {"name": "Adaptive Beta Log-Sigmoid Loss with Rank-Gap Normalization", "intuition": "This loss function creates a highly adaptive margin by dynamically adjusting its sensitivity based on the magnitude of cost differences within a batch, while using rank-based normalization for robustness against outliers.\n\nInherited Ideas:\n- From Parent 1, it inherits the use of `rank_gap` to normalize the cost difference. This makes the margin robust to the absolute scale of costs, focusing on the relative preference ordering within the batch (`rank_diff = rank_gap(cost_a, cost_b)`).\n- From Parent 0, it inherits the use of `tanh` to create a smoothly saturating margin from the normalized cost signal (`margin = max_margin * tanh(...)`). This prevents the target margin from growing uncontrollably, enhancing numerical stability.\n- From both parents, it inherits the core loss structure of `-logsigmoid(temperature * (logp_diff - margin))`, which provides a smooth, bounded, and stable hinge-like loss.\n\nNew Coupling Ideas:\n1. **Dynamic Beta Scaling**: The sensitivity of the `tanh` function is controlled by a dynamic `beta` parameter. This `beta` is scaled by the standard deviation of the batch costs. When cost variance is high (indicating a wide spread of qualities), `beta` is increased, making the `tanh` margin more sensitive to smaller rank differences. When variance is low (solutions are similar), `beta` is decreased, making the margin less aggressive. This allows the loss to adapt its focus based on the batch's diversity.\n2. **Margin Clamping**: A `clamp` operation is applied to the final margin. This establishes a hard minimum (`min_margin`) and maximum (`max_margin`) value for the target log-probability separation. The minimum margin ensures a non-zero loss even for pairs with a small rank gap, while the maximum prevents the margin from fully saturating `max_margin` unless the preference is strong, adding another layer of stability.", "pseudocode": "1. Given costs for a batch, compute the normalized rank gap between `cost_a` and `cost_b`. This provides a stable signal of relative preference strength, `rank_diff`, typically in [-1, 1].\n   rank_diff = rank_gap(cost_a, cost_b).\n2. Compute the standard deviation of all costs in the batch. Use this to create a dynamic `beta` that adapts to the cost distribution's spread.\n   batch_cost_std = std(all_costs_in_batch).\n   dynamic_beta = base_beta * batch_cost_std.\n3. Calculate the core of the margin by passing the rank difference through a `tanh` function, with its sensitivity controlled by `dynamic_beta`.\n   tanh_term = tanh(dynamic_beta * rank_diff).\n4. Scale the result by `max_margin` and then apply a clamp to enforce a strict minimum and maximum separation target.\n   adaptive_margin = clamp(max_margin * tanh_term, min_margin, max_margin).\n5. Compute the log-probability difference: logp_diff = logp_a - logp_b.\n6. Calculate the final loss using a stable hinge-like structure. This penalizes cases where `logp_diff` is smaller than the `adaptive_margin`.\n   loss = -logsigmoid(temperature * (logp_diff - adaptive_margin)).", "hyperparams": {"temperature": 5.0, "max_margin": 4.0, "min_margin": 0.05, "base_beta": 1.5}, "operators_used": ["rank_gap", "tanh", "clamp", "logsigmoid"], "implementation_hint": {"expects": ["cost_a", "cost_b", "logp_a", "logp_b", "all_costs_in_batch"], "returns": "scalar"}}, "fitness": {"hf_like_score": 3.886611223220825, "validation_objective": 3.886611223220825, "generalization_penalty": 0.0, "generalization_objectives": {"20": 3.8511611223220825}, "train_score_mean": 4.210513308644295, "train_loss_mean": 0.6781019624322653, "pair_count": 4112918, "config": {"hf": {"problem": "tsp", "hf_steps": 32, "train_problem_size": 20, "valid_problem_sizes": [20], "train_batch_size": 64, "pomo_size": 64, "learning_rate": 0.0003, "weight_decay": 1e-06, "alpha": 0.05, "device": "cuda", "seed": 1234, "num_validation_episodes": 128, "validation_batch_size": 64, "generalization_penalty_weight": 1.0, "pool_version": "v0"}, "free_loss": {"f1_steps": 32, "f2_steps": 100, "f3_enabled": false}}, "loss_ir": {"name": "Adaptive Beta Log-Sigmoid Loss with Rank-Gap Normalization", "intuition": "This loss function creates a highly adaptive margin by dynamically adjusting its sensitivity based on the magnitude of cost differences within a batch, while using rank-based normalization for robustness against outliers.\n\nInherited Ideas:\n- From Parent 1, it inherits the use of `rank_gap` to normalize the cost difference. This makes the margin robust to the absolute scale of costs, focusing on the relative preference ordering within the batch (`rank_diff = rank_gap(cost_a, cost_b)`).\n- From Parent 0, it inherits the use of `tanh` to create a smoothly saturating margin from the normalized cost signal (`margin = max_margin * tanh(...)`). This prevents the target margin from growing uncontrollably, enhancing numerical stability.\n- From both parents, it inherits the core loss structure of `-logsigmoid(temperature * (logp_diff - margin))`, which provides a smooth, bounded, and stable hinge-like loss.\n\nNew Coupling Ideas:\n1. **Dynamic Beta Scaling**: The sensitivity of the `tanh` function is controlled by a dynamic `beta` parameter. This `beta` is scaled by the standard deviation of the batch costs. When cost variance is high (indicating a wide spread of qualities), `beta` is increased, making the `tanh` margin more sensitive to smaller rank differences. When variance is low (solutions are similar), `beta` is decreased, making the margin less aggressive. This allows the loss to adapt its focus based on the batch's diversity.\n2. **Margin Clamping**: A `clamp` operation is applied to the final margin. This establishes a hard minimum (`min_margin`) and maximum (`max_margin`) value for the target log-probability separation. The minimum margin ensures a non-zero loss even for pairs with a small rank gap, while the maximum prevents the margin from fully saturating `max_margin` unless the preference is strong, adding another layer of stability.", "hyperparams": {"temperature": 5.0, "max_margin": 4.0, "min_margin": 0.05, "base_beta": 1.5}, "operators_used": ["rank_gap", "tanh", "clamp", "logsigmoid"]}}}
{"generation": 5, "index": 5, "ir": {"name": "Adaptive Rank-Gap Hinge Loss with Z-Score Normalization", "intuition": "This loss function creates a robust, doubly-normalized margin that adapts to both the statistical distribution and the relative ranking of costs within a batch. It then applies this margin within a stable hinge-loss framework.\n\nInherited Ideas:\n- From Parent 0, it inherits the use of `zscore` to normalize the costs before further processing. This makes the margin signal robust to shifts and scaling in the absolute cost values, focusing on the batch's internal statistical properties.\n- From Parent 1, it inherits the core idea of using `rank_gap` on the costs to derive a margin signal. This captures the relative ordering of preferences within the batch, making the margin robust to outliers.\n- From both parents, it inherits the general structure of a hinge-like loss, `softplus(-temperature * (logp_diff - adaptive_margin))`, which is a numerically stable equivalent to `-logsigmoid(temperature * (logp_diff - adaptive_margin))`. This penalizes the model when the log-probability difference `logp_diff` is smaller than the target `adaptive_margin`.\n\nNew Coupling Ideas:\n1. **Sequential Normalization (Z-Score then Rank-Gap)**: The key innovation is the sequential application of normalization operators. First, the batch costs are centered and scaled using `zscore`. Then, `rank_gap` is applied to these z-scored costs. This coupling ensures the margin is derived from a signal that is independent of the original cost scale (due to `zscore`) and simultaneously robust to outliers in the normalized distribution (due to `rank_gap`). This creates a highly stable and relative preference signal.\n2. **ReLU-based Margin Scaling**: Instead of using `tanh` or `sigmoid` to shape the margin, this loss uses a simple `relu` function on the rank-gap signal before scaling. The margin is calculated as `max_margin * relu(rank_gap(z_cost_b, z_cost_a))`. Using `relu` enforces a strict one-sided penalty: a margin is only created when `cost(a) < cost(b)`. If `cost(a) >= cost(b)`, the rank gap is non-positive, and the `relu` function zeros out the margin, effectively turning off the loss for correctly ordered or tied pairs. This provides a cleaner, more direct implementation of the hinge-loss concept.", "pseudocode": "1. For a given batch, normalize all costs using z-scoring: z_cost_a = zscore(cost_a), z_cost_b = zscore(cost_b).\n2. Compute the normalized rank gap on the z-scored costs. Note the order: `rank_gap(z_cost_b, z_cost_a)`. This will be positive when `cost(a) < cost(b)`.\n   rank_diff = rank_gap(z_cost_b, z_cost_a).\n3. Create an adaptive margin by applying a `relu` function to the rank difference and scaling by `max_margin`. The `relu` ensures the margin is zero for incorrectly ordered or tied pairs, enforcing a one-sided penalty.\n   adaptive_margin = max_margin * relu(rank_diff).\n4. Compute the log-probability difference: logp_diff = logp_a - logp_b.\n5. Calculate the core term for the loss. This is the difference between the log-probability gap and the adaptive margin, scaled by a temperature parameter.\n   core_term = temperature * (logp_diff - adaptive_margin).\n6. The final loss is the `softplus` of the negative core term. This is a numerically stable hinge-like loss that penalizes cases where `logp_diff` is smaller than the target `adaptive_margin`.\n   loss = softplus(-core_term).", "hyperparams": {"temperature": 5.0, "max_margin": 4.0}, "operators_used": ["zscore", "rank_gap", "relu", "softplus"], "implementation_hint": {"expects": ["cost_a", "cost_b", "logp_a", "logp_b"], "returns": "scalar"}}, "fitness": {"hf_like_score": 3.880636692047119, "validation_objective": 3.880636692047119, "generalization_penalty": 0.0, "generalization_objectives": {"20": 3.8565642833709717}, "train_score_mean": 4.209675006568432, "train_loss_mean": 0.6782405562698841, "pair_count": 4111880, "config": {"hf": {"problem": "tsp", "hf_steps": 32, "train_problem_size": 20, "valid_problem_sizes": [20], "train_batch_size": 64, "pomo_size": 64, "learning_rate": 0.0003, "weight_decay": 1e-06, "alpha": 0.05, "device": "cuda", "seed": 1234, "num_validation_episodes": 128, "validation_batch_size": 64, "generalization_penalty_weight": 1.0, "pool_version": "v0"}, "free_loss": {"f1_steps": 32, "f2_steps": 100, "f3_enabled": false}}, "loss_ir": {"name": "Adaptive Rank-Gap Hinge Loss with Z-Score Normalization", "intuition": "This loss function creates a robust, doubly-normalized margin that adapts to both the statistical distribution and the relative ranking of costs within a batch. It then applies this margin within a stable hinge-loss framework.\n\nInherited Ideas:\n- From Parent 0, it inherits the use of `zscore` to normalize the costs before further processing. This makes the margin signal robust to shifts and scaling in the absolute cost values, focusing on the batch's internal statistical properties.\n- From Parent 1, it inherits the core idea of using `rank_gap` on the costs to derive a margin signal. This captures the relative ordering of preferences within the batch, making the margin robust to outliers.\n- From both parents, it inherits the general structure of a hinge-like loss, `softplus(-temperature * (logp_diff - adaptive_margin))`, which is a numerically stable equivalent to `-logsigmoid(temperature * (logp_diff - adaptive_margin))`. This penalizes the model when the log-probability difference `logp_diff` is smaller than the target `adaptive_margin`.\n\nNew Coupling Ideas:\n1. **Sequential Normalization (Z-Score then Rank-Gap)**: The key innovation is the sequential application of normalization operators. First, the batch costs are centered and scaled using `zscore`. Then, `rank_gap` is applied to these z-scored costs. This coupling ensures the margin is derived from a signal that is independent of the original cost scale (due to `zscore`) and simultaneously robust to outliers in the normalized distribution (due to `rank_gap`). This creates a highly stable and relative preference signal.\n2. **ReLU-based Margin Scaling**: Instead of using `tanh` or `sigmoid` to shape the margin, this loss uses a simple `relu` function on the rank-gap signal before scaling. The margin is calculated as `max_margin * relu(rank_gap(z_cost_b, z_cost_a))`. Using `relu` enforces a strict one-sided penalty: a margin is only created when `cost(a) < cost(b)`. If `cost(a) >= cost(b)`, the rank gap is non-positive, and the `relu` function zeros out the margin, effectively turning off the loss for correctly ordered or tied pairs. This provides a cleaner, more direct implementation of the hinge-loss concept.", "hyperparams": {"temperature": 5.0, "max_margin": 4.0}, "operators_used": ["zscore", "rank_gap", "relu", "softplus"]}}}
{"generation": 5, "index": 6, "ir": {"name": "Z-Scored Sigmoid-Mapped Bradley-Terry Loss", "intuition": "This loss function creates a robust, distribution-aware preference signal by blending the statistical properties of the Bradley-Terry model with batch-level normalization. It is designed to be sensitive to the magnitude of cost differences while remaining stable against outliers and shifts in the cost scale.\n\nInherited Ideas:\n- From Parent 0 (Z-Scored Rank-Adaptive Hinge Loss), it inherits the idea of using `zscore` to normalize the costs for the entire batch. This crucial first step makes the loss invariant to the absolute scale and offset of the costs, focusing only on their relative differences within the current batch's statistical distribution.\n- From Parent 1 (Rank-Normalized Margin Loss), it inherits the use of the `sigmoid` function to map a difference into a bounded probability-like space (0 to 1). This is applied not to a margin, but directly to the normalized cost difference to create a target preference probability.\n\nNew Coupling Ideas:\n1.  **Bradley-Terry Target Probability**: The core innovation is to frame the problem in a Bradley-Terry style. Instead of creating an adaptive margin for the log-probability difference, we create a target probability, `p_target`, that `a` is preferred over `b`. This target is derived by applying a `sigmoid` function to the z-scored cost difference (`z_cost_b - z_cost_a`), which elegantly maps a potentially unbounded difference onto a (0, 1) scale. This directly represents the desired strength of preference.\n2.  **Cross-Entropy Formulation**: The loss is then formulated as the cross-entropy between the model's implied preference probability and this `p_target`. The model's probability is calculated as `sigmoid(logp_a - logp_b)`. The final loss, `-p_target * logsigmoid(logp_a - logp_b) - (1 - p_target) * logsigmoid(logp_b - logp_a)`, directly minimizes the divergence between the model's preference distribution and the target distribution derived from the normalized costs. This provides a statistically grounded and stable learning objective.", "pseudocode": "1. Given costs for a batch, normalize them using z-scoring: z_cost_a = zscore(cost_a), z_cost_b = zscore(cost_b).\n2. Compute the difference in z-scored costs. This represents the normalized preference strength. We use `z_cost_b - z_cost_a` so that a better `cost_a` (lower) results in a larger positive value.\n   z_cost_diff = z_cost_b - z_cost_a.\n3. Compute the target preference probability, `p_target`, by applying a temperature-scaled sigmoid to the z-scored cost difference. This maps the normalized cost gap to a target probability in (0, 1) that `a` should be preferred over `b`.\n   p_target = sigmoid(temperature * z_cost_diff).\n4. Compute the log-probability difference from the model: logp_diff = logp_a - logp_b.\n5. Calculate the final loss using a cross-entropy formulation. This measures the divergence between the model's implied preference probability (`sigmoid(logp_diff)`) and the target probability (`p_target`).\n   loss = -p_target * logsigmoid(logp_diff) - (1 - p_target) * logsigmoid(-logp_diff).", "hyperparams": {"temperature": 1.0}, "operators_used": ["zscore", "sigmoid", "logsigmoid"], "implementation_hint": {"expects": ["cost_a", "cost_b", "logp_a", "logp_b"], "returns": "scalar"}}, "fitness": {"hf_like_score": 3.885485291481018, "validation_objective": 3.885485291481018, "generalization_penalty": 0.0, "generalization_objectives": {"20": 3.8488227128982544}, "train_score_mean": 4.20692753046751, "train_loss_mean": 0.6784215215593576, "pair_count": 4111481, "config": {"hf": {"problem": "tsp", "hf_steps": 32, "train_problem_size": 20, "valid_problem_sizes": [20], "train_batch_size": 64, "pomo_size": 64, "learning_rate": 0.0003, "weight_decay": 1e-06, "alpha": 0.05, "device": "cuda", "seed": 1234, "num_validation_episodes": 128, "validation_batch_size": 64, "generalization_penalty_weight": 1.0, "pool_version": "v0"}, "free_loss": {"f1_steps": 32, "f2_steps": 100, "f3_enabled": false}}, "loss_ir": {"name": "Z-Scored Sigmoid-Mapped Bradley-Terry Loss", "intuition": "This loss function creates a robust, distribution-aware preference signal by blending the statistical properties of the Bradley-Terry model with batch-level normalization. It is designed to be sensitive to the magnitude of cost differences while remaining stable against outliers and shifts in the cost scale.\n\nInherited Ideas:\n- From Parent 0 (Z-Scored Rank-Adaptive Hinge Loss), it inherits the idea of using `zscore` to normalize the costs for the entire batch. This crucial first step makes the loss invariant to the absolute scale and offset of the costs, focusing only on their relative differences within the current batch's statistical distribution.\n- From Parent 1 (Rank-Normalized Margin Loss), it inherits the use of the `sigmoid` function to map a difference into a bounded probability-like space (0 to 1). This is applied not to a margin, but directly to the normalized cost difference to create a target preference probability.\n\nNew Coupling Ideas:\n1.  **Bradley-Terry Target Probability**: The core innovation is to frame the problem in a Bradley-Terry style. Instead of creating an adaptive margin for the log-probability difference, we create a target probability, `p_target`, that `a` is preferred over `b`. This target is derived by applying a `sigmoid` function to the z-scored cost difference (`z_cost_b - z_cost_a`), which elegantly maps a potentially unbounded difference onto a (0, 1) scale. This directly represents the desired strength of preference.\n2.  **Cross-Entropy Formulation**: The loss is then formulated as the cross-entropy between the model's implied preference probability and this `p_target`. The model's probability is calculated as `sigmoid(logp_a - logp_b)`. The final loss, `-p_target * logsigmoid(logp_a - logp_b) - (1 - p_target) * logsigmoid(logp_b - logp_a)`, directly minimizes the divergence between the model's preference distribution and the target distribution derived from the normalized costs. This provides a statistically grounded and stable learning objective.", "hyperparams": {"temperature": 1.0}, "operators_used": ["zscore", "sigmoid", "logsigmoid"]}}}
{"generation": 5, "index": 7, "ir": {"name": "Z-Score Attenuated Rank-Adaptive Loss", "intuition": "This loss function creates a highly adaptive margin by combining rank-based and distribution-based cost normalization. It penalizes preference violations using a stable, hinge-like loss structure.\n\nInherited Ideas:\n- From Parent 0 (Z-Scored Rank-Adaptive Hinge Loss), it inherits the idea of using a z-score normalization (`zscore`) on the batch costs. This pre-processing step centers the cost distribution and scales it by its standard deviation, making the subsequent margin calculation robust to shifts and scaling in the absolute cost values.\n- From Parent 1 (Rank-Normalized Margin Loss), it inherits the core concept of creating an adaptive margin using the `rank_gap` of the costs. This focuses the loss on the relative ordering of solutions within the batch, preventing cost outliers from dominating the training signal.\n\nNew Coupling Ideas:\n1. **Dual-Signal Margin Construction**: The margin is constructed from two coupled signals derived from the z-scored costs. The primary signal is the `rank_gap` of the z-scored costs, which captures the relative ordering. This is passed through `tanh` to create a smoothly saturating base margin. The second signal is the raw difference of the z-scored costs (`z_cost_a - z_cost_b`), which captures the magnitude of preference. These two signals are multiplied together. This coupling ensures the margin is largest when both the rank difference and the standardized cost difference are high, while being attenuated if either is small. For instance, a large rank gap with a small z-score difference (e.g., tightly clustered costs) will result in a smaller margin than if the costs were far apart.\n2. **Log-Probability Clipping for Stability**: Before computing the final loss, the log-probability difference (`logp_diff`) is clamped to a symmetric range `[-clip_range, clip_range]`. This stability trick prevents extremely large log-probability differences, which can arise early in training or from model pathologies, from creating explosive gradients. It ensures the loss remains well-behaved and focuses the model on learning from reasonable preference signals.", "pseudocode": "1. Given costs for a batch, normalize them using z-scoring: z_cost_a = zscore(cost_a), z_cost_b = zscore(cost_b).\n2. Compute the rank-normalized gap between the z-scored costs. This provides a stable signal of relative preference order.\n   rank_diff = rank_gap(z_cost_a, z_cost_b).\n3. Compute the difference in the z-scored costs to capture the magnitude of preference.\n   z_cost_diff = z_cost_a - z_cost_b.\n4. Create the adaptive margin. First, pass the rank difference through a `tanh` function. Then, multiply this by the z-scored cost difference and a scaling factor `beta`. This couples the rank-based and magnitude-based signals.\n   adaptive_margin = beta * tanh(rank_diff) * z_cost_diff.\n5. Compute the log-probability difference: logp_diff = logp_a - logp_b.\n6. For numerical stability, clip the log-probability difference to a predefined range.\n   clipped_logp_diff = clamp(logp_diff, min=-clip_range, max=clip_range).\n7. Calculate the core loss term, scaled by a temperature parameter. This is the difference between the clipped log-probability gap and the adaptive margin.\n   core_term = temperature * (clipped_logp_diff - adaptive_margin).\n8. The final loss is the negative logsigmoid of the core term, which is a stable hinge-like loss that penalizes cases where the log-probability difference is smaller than the target margin.\n   loss = -logsigmoid(core_term).", "hyperparams": {"temperature": 5.0, "beta": 1.0, "clip_range": 10.0}, "operators_used": ["zscore", "rank_gap", "tanh", "clamp", "logsigmoid"], "implementation_hint": {"expects": ["cost_a", "cost_b", "logp_a", "logp_b"], "returns": "scalar"}}, "fitness": {"hf_like_score": 3.8848187923431396, "validation_objective": 3.8848187923431396, "generalization_penalty": 0.0, "generalization_objectives": {"20": 3.8558483123779297}, "train_score_mean": 4.210562393069267, "train_loss_mean": 0.6780359111726284, "pair_count": 4112077, "config": {"hf": {"problem": "tsp", "hf_steps": 32, "train_problem_size": 20, "valid_problem_sizes": [20], "train_batch_size": 64, "pomo_size": 64, "learning_rate": 0.0003, "weight_decay": 1e-06, "alpha": 0.05, "device": "cuda", "seed": 1234, "num_validation_episodes": 128, "validation_batch_size": 64, "generalization_penalty_weight": 1.0, "pool_version": "v0"}, "free_loss": {"f1_steps": 32, "f2_steps": 100, "f3_enabled": false}}, "loss_ir": {"name": "Z-Score Attenuated Rank-Adaptive Loss", "intuition": "This loss function creates a highly adaptive margin by combining rank-based and distribution-based cost normalization. It penalizes preference violations using a stable, hinge-like loss structure.\n\nInherited Ideas:\n- From Parent 0 (Z-Scored Rank-Adaptive Hinge Loss), it inherits the idea of using a z-score normalization (`zscore`) on the batch costs. This pre-processing step centers the cost distribution and scales it by its standard deviation, making the subsequent margin calculation robust to shifts and scaling in the absolute cost values.\n- From Parent 1 (Rank-Normalized Margin Loss), it inherits the core concept of creating an adaptive margin using the `rank_gap` of the costs. This focuses the loss on the relative ordering of solutions within the batch, preventing cost outliers from dominating the training signal.\n\nNew Coupling Ideas:\n1. **Dual-Signal Margin Construction**: The margin is constructed from two coupled signals derived from the z-scored costs. The primary signal is the `rank_gap` of the z-scored costs, which captures the relative ordering. This is passed through `tanh` to create a smoothly saturating base margin. The second signal is the raw difference of the z-scored costs (`z_cost_a - z_cost_b`), which captures the magnitude of preference. These two signals are multiplied together. This coupling ensures the margin is largest when both the rank difference and the standardized cost difference are high, while being attenuated if either is small. For instance, a large rank gap with a small z-score difference (e.g., tightly clustered costs) will result in a smaller margin than if the costs were far apart.\n2. **Log-Probability Clipping for Stability**: Before computing the final loss, the log-probability difference (`logp_diff`) is clamped to a symmetric range `[-clip_range, clip_range]`. This stability trick prevents extremely large log-probability differences, which can arise early in training or from model pathologies, from creating explosive gradients. It ensures the loss remains well-behaved and focuses the model on learning from reasonable preference signals.", "hyperparams": {"temperature": 5.0, "beta": 1.0, "clip_range": 10.0}, "operators_used": ["zscore", "rank_gap", "tanh", "clamp", "logsigmoid"]}}}
{"generation": 6, "index": 0, "ir": {"name": "Confidence-Coupled and Rank-Normalized Hinge Loss", "intuition": "This loss function creates a highly adaptive and robust margin by combining distribution-based normalization with a confidence-aware target, then applies it within a stable hinge-loss framework.\n\nInherited Ideas:\n- From Parent 0, it inherits the idea of coupling the margin to the model's confidence by adding a term proportional to the log-probability difference (`alpha * logp_diff`). This makes the target margin dynamic, increasing the pressure for separation when the model is already confident and relying on the cost-based margin when it is not.\n- From Parent 1, it inherits the core structure of a hinge-like loss: `-logsigmoid(temperature * (logp_diff - margin))`, which is numerically stable and equivalent to `softplus(temperature * (margin - logp_diff))`. It also inherits the use of `zscore` to normalize costs, making the margin calculation robust to the scale and shift of the input cost distribution.\n\nNew Coupling Ideas:\n1. **Tanh-Gated Confidence Coupling**: Instead of directly adding the `logp_diff` term to the margin, it is first passed through a `tanh` function. The resulting term, `alpha * tanh(logp_diff)`, is then added to the cost-derived margin. This gates the confidence contribution, ensuring it provides a bounded, smooth signal. It prevents the model's confidence from making the target margin explode, which could lead to instability, while still allowing the margin to adapt to the model's state in a controlled manner.\n2. **Margin Baseline from Softplus**: A small, constant baseline margin is created using `softplus(min_margin)`. This ensures that even when the cost difference is zero, there is always a small, non-negative target separation, preventing the margin from collapsing. Using `softplus` guarantees this baseline is strictly positive and smooth, unlike a hard `relu` or `clamp`.", "pseudocode": "1. Given costs for a batch, normalize them using z-scoring to make the signal robust to scale and shift: z_cost_a = zscore(cost_a), z_cost_b = zscore(cost_b).\n2. Calculate the difference between the z-scored costs: z_cost_diff = z_cost_b - z_cost_a. This represents the preference strength.\n3. Create the base adaptive margin by scaling the z-scored cost difference. This component is based purely on the normalized cost difference.\n   adaptive_margin = beta * z_cost_diff.\n4. Compute the log-probability difference: logp_diff = logp_a - logp_b.\n5. Create a bounded, confidence-aware term by passing the log-probability difference through `tanh` and scaling it.\n   confidence_term = alpha * tanh(logp_diff).\n6. Create a small, positive baseline margin using `softplus`.\n   baseline_margin = softplus(min_margin).\n7. Combine the components to form the final dynamic margin. It adapts to cost difference and model confidence, with a guaranteed positive baseline.\n   dynamic_margin = baseline_margin + adaptive_margin + confidence_term.\n8. Calculate the core loss term, scaled by temperature.\n   core_term = temperature * (logp_diff - dynamic_margin).\n9. The final loss is the negative logsigmoid of the core term, which implements a smooth hinge loss.\n   loss = -logsigmoid(core_term).", "hyperparams": {"temperature": 5.0, "beta": 1.0, "alpha": 0.2, "min_margin": 0.05}, "operators_used": ["zscore", "tanh", "softplus", "logsigmoid"], "implementation_hint": {"expects": ["cost_a", "cost_b", "logp_a", "logp_b"], "returns": "scalar"}}, "fitness": {"hf_like_score": 3.8785383701324463, "validation_objective": 3.8785383701324463, "generalization_penalty": 0.0, "generalization_objectives": {"20": 3.846335768699646}, "train_score_mean": 4.209012106060982, "train_loss_mean": 0.6536440346390009, "pair_count": 4116702, "config": {"hf": {"problem": "tsp", "hf_steps": 32, "train_problem_size": 20, "valid_problem_sizes": [20], "train_batch_size": 64, "pomo_size": 64, "learning_rate": 0.0003, "weight_decay": 1e-06, "alpha": 0.05, "device": "cuda", "seed": 1234, "num_validation_episodes": 128, "validation_batch_size": 64, "generalization_penalty_weight": 1.0, "pool_version": "v0"}, "free_loss": {"f1_steps": 32, "f2_steps": 100, "f3_enabled": false}}, "loss_ir": {"name": "Confidence-Coupled and Rank-Normalized Hinge Loss", "intuition": "This loss function creates a highly adaptive and robust margin by combining distribution-based normalization with a confidence-aware target, then applies it within a stable hinge-loss framework.\n\nInherited Ideas:\n- From Parent 0, it inherits the idea of coupling the margin to the model's confidence by adding a term proportional to the log-probability difference (`alpha * logp_diff`). This makes the target margin dynamic, increasing the pressure for separation when the model is already confident and relying on the cost-based margin when it is not.\n- From Parent 1, it inherits the core structure of a hinge-like loss: `-logsigmoid(temperature * (logp_diff - margin))`, which is numerically stable and equivalent to `softplus(temperature * (margin - logp_diff))`. It also inherits the use of `zscore` to normalize costs, making the margin calculation robust to the scale and shift of the input cost distribution.\n\nNew Coupling Ideas:\n1. **Tanh-Gated Confidence Coupling**: Instead of directly adding the `logp_diff` term to the margin, it is first passed through a `tanh` function. The resulting term, `alpha * tanh(logp_diff)`, is then added to the cost-derived margin. This gates the confidence contribution, ensuring it provides a bounded, smooth signal. It prevents the model's confidence from making the target margin explode, which could lead to instability, while still allowing the margin to adapt to the model's state in a controlled manner.\n2. **Margin Baseline from Softplus**: A small, constant baseline margin is created using `softplus(min_margin)`. This ensures that even when the cost difference is zero, there is always a small, non-negative target separation, preventing the margin from collapsing. Using `softplus` guarantees this baseline is strictly positive and smooth, unlike a hard `relu` or `clamp`.", "hyperparams": {"temperature": 5.0, "beta": 1.0, "alpha": 0.2, "min_margin": 0.05}, "operators_used": ["zscore", "tanh", "softplus", "logsigmoid"]}}}
{"generation": 6, "index": 1, "ir": {"name": "Softplus-Coupled Z-Score and Rank Hinge Loss", "intuition": "This loss function creates a highly adaptive and robust margin by synthesizing rank-based and distribution-based normalization, and then dynamically couples this margin to the model's own confidence using a softplus transformation for stability.\n\nInherited Ideas:\n- From both parents (Parent 0 and Parent 1), it inherits the foundational two-step normalization process: first applying `zscore` to the batch costs to stabilize against scale and shift, followed by `rank_gap` on these z-scored costs to get a robust, rank-ordered preference signal. This combines distributional and ordinal normalization.\n- From Parent 1, it inherits the use of `tanh` to create a smooth, bounded, and saturating adaptive margin from the normalized cost signal (`max_margin * tanh(...)`). This prevents the margin from growing uncontrollably.\n\nNew Coupling Ideas:\n1. **Softplus-Coupled Dynamic Margin**: The final target margin is made dynamic by coupling it to the model's confidence, represented by the log-probability difference (`logp_diff`). Specifically, the margin is `adaptive_margin + alpha * softplus(logp_diff)`. The `softplus` function ensures that this confidence-based adjustment is always non-negative and smooth. This prevents the target margin from collapsing or becoming negative, which could happen if a raw `logp_diff` was used (as it can be negative). This creates a stable 'ratcheting' effect: as the model becomes more confident (larger `logp_diff`), the target margin increases, pushing for even better separation. If the model is unconfident, the margin gracefully defaults towards the cost-based `adaptive_margin`.\n2. **ReLU-Gated Loss Activation**: The core loss term `(dynamic_margin - logp_diff)` is passed through a `relu` function before being scaled by `temperature` and fed into `softplus`. This acts as a strict hinge mechanism. If the model's log-probability difference `logp_diff` already exceeds the dynamic target margin, the `relu` output is zero, and the loss becomes zero (`softplus(0)` is non-zero but constant, so its gradient is zero). This means the model is not penalized for 'over-achieving', focusing training capacity only on pairs that fail to meet the dynamic margin.", "pseudocode": "1. Given costs for a batch, normalize them using z-scoring: z_cost_a = zscore(cost_a), z_cost_b = zscore(cost_b).\n2. Compute the rank-normalized gap between the z-scored costs to get a robust preference signal.\n   rank_diff = rank_gap(z_cost_a, z_cost_b).\n3. Create the base adaptive margin by passing `rank_diff` through a `tanh` function and scaling it. This component is bounded and based purely on the normalized cost difference.\n   adaptive_margin = max_margin * tanh(beta * rank_diff).\n4. Compute the log-probability difference: logp_diff = logp_a - logp_b.\n5. Create the final dynamic margin by adding a non-negative, smooth term proportional to the model's confidence. The `softplus` ensures this adjustment is always positive and stable.\n   dynamic_margin = adaptive_margin + alpha * softplus(logp_diff).\n6. Calculate the hinge error, which is the amount by which the log-probability difference falls short of the dynamic margin. The `relu` function ensures this is zero if the margin is met or exceeded.\n   hinge_error = relu(dynamic_margin - logp_diff).\n7. The final loss is a smooth, non-negative penalty on the hinge error, scaled by temperature. Using `softplus` is a stable alternative to `log(1 + exp(...))`.\n   loss = softplus(temperature * hinge_error).", "hyperparams": {"temperature": 5.0, "max_margin": 3.0, "beta": 1.0, "alpha": 0.1}, "operators_used": ["zscore", "rank_gap", "tanh", "softplus", "relu"], "implementation_hint": {"expects": ["cost_a", "cost_b", "logp_a", "logp_b"], "returns": "scalar"}}, "fitness": {"hf_like_score": 3.8881462812423706, "validation_objective": 3.8881462812423706, "generalization_penalty": 0.0, "generalization_objectives": {"20": 3.849940538406372}, "train_score_mean": 4.205077275633812, "train_loss_mean": 0.6677900888025761, "pair_count": 4113200, "config": {"hf": {"problem": "tsp", "hf_steps": 32, "train_problem_size": 20, "valid_problem_sizes": [20], "train_batch_size": 64, "pomo_size": 64, "learning_rate": 0.0003, "weight_decay": 1e-06, "alpha": 0.05, "device": "cuda", "seed": 1234, "num_validation_episodes": 128, "validation_batch_size": 64, "generalization_penalty_weight": 1.0, "pool_version": "v0"}, "free_loss": {"f1_steps": 32, "f2_steps": 100, "f3_enabled": false}}, "loss_ir": {"name": "Softplus-Coupled Z-Score and Rank Hinge Loss", "intuition": "This loss function creates a highly adaptive and robust margin by synthesizing rank-based and distribution-based normalization, and then dynamically couples this margin to the model's own confidence using a softplus transformation for stability.\n\nInherited Ideas:\n- From both parents (Parent 0 and Parent 1), it inherits the foundational two-step normalization process: first applying `zscore` to the batch costs to stabilize against scale and shift, followed by `rank_gap` on these z-scored costs to get a robust, rank-ordered preference signal. This combines distributional and ordinal normalization.\n- From Parent 1, it inherits the use of `tanh` to create a smooth, bounded, and saturating adaptive margin from the normalized cost signal (`max_margin * tanh(...)`). This prevents the margin from growing uncontrollably.\n\nNew Coupling Ideas:\n1. **Softplus-Coupled Dynamic Margin**: The final target margin is made dynamic by coupling it to the model's confidence, represented by the log-probability difference (`logp_diff`). Specifically, the margin is `adaptive_margin + alpha * softplus(logp_diff)`. The `softplus` function ensures that this confidence-based adjustment is always non-negative and smooth. This prevents the target margin from collapsing or becoming negative, which could happen if a raw `logp_diff` was used (as it can be negative). This creates a stable 'ratcheting' effect: as the model becomes more confident (larger `logp_diff`), the target margin increases, pushing for even better separation. If the model is unconfident, the margin gracefully defaults towards the cost-based `adaptive_margin`.\n2. **ReLU-Gated Loss Activation**: The core loss term `(dynamic_margin - logp_diff)` is passed through a `relu` function before being scaled by `temperature` and fed into `softplus`. This acts as a strict hinge mechanism. If the model's log-probability difference `logp_diff` already exceeds the dynamic target margin, the `relu` output is zero, and the loss becomes zero (`softplus(0)` is non-zero but constant, so its gradient is zero). This means the model is not penalized for 'over-achieving', focusing training capacity only on pairs that fail to meet the dynamic margin.", "hyperparams": {"temperature": 5.0, "max_margin": 3.0, "beta": 1.0, "alpha": 0.1}, "operators_used": ["zscore", "rank_gap", "tanh", "softplus", "relu"]}}}
{"generation": 6, "index": 2, "ir": {"name": "Softplus-Clipped Rank-Adaptive Log-Probability Loss", "intuition": "This loss function constructs a highly stable and adaptive margin by synthesizing distribution-based normalization with a smooth, non-saturating activation, and then couples this margin directly to the model's own confidence level.\n\nInherited Ideas:\n- From Parent 0, it inherits the idea of creating a dynamic margin by coupling a cost-derived adaptive margin with a term proportional to the model's log-probability difference (`alpha * logp_diff`). This makes the target separation dependent on both the ground-truth cost difference and the model's current confidence.\n- From Parent 1, it inherits the use of z-score normalization (`zscore(costs)`) as a pre-processing step. This centers the cost distribution and makes the subsequent margin calculation robust to the absolute scale and shift of the costs, focusing on the batch's statistical properties.\n\nNew Coupling Ideas:\n1. **Softplus for Non-Saturating Margin**: Instead of using saturating functions like `sigmoid` (Parent 0) or `tanh` (Parent 1), this loss uses `softplus(beta * rank_diff)` to create the adaptive margin. `softplus` is a smooth, non-negative, and non-saturating function. This allows the margin to grow unboundedly (but smoothly) as the rank-based preference signal becomes stronger, which can be beneficial for pairs with very clear preference, without the hard cap imposed by `sigmoid` or `tanh`.\n2. **Margin Stability via Clipping**: The `softplus` function can grow large, which might lead to an excessively large `dynamic_margin` and potential instability. To counteract this, the final `dynamic_margin` is clipped to a maximum value (`max_margin`). This `clamp` operation acts as a stability trick, reaping the benefits of the smooth, non-saturating `softplus` for most of the range while preventing extreme values from destabilizing the training process.", "pseudocode": "1. Given costs for a batch, normalize them using z-scoring: z_cost_a = zscore(cost_a), z_cost_b = zscore(cost_b).\n2. Compute the rank-normalized gap between the z-scored costs to get a robust preference signal.\n   rank_diff = rank_gap(z_cost_a, z_cost_b).\n3. Create a non-saturating adaptive margin using the `softplus` function. This allows the margin to grow smoothly with the strength of the preference signal.\n   adaptive_margin = softplus(beta * rank_diff).\n4. Compute the log-probability difference: logp_diff = logp_a - logp_b.\n5. Create a dynamic margin by adding a term proportional to the log-probability difference itself. This couples the target to the model's confidence.\n   dynamic_margin_unclipped = alpha * logp_diff + adaptive_margin.\n6. For stability, clip the dynamic margin to a maximum value, preventing it from becoming excessively large.\n   dynamic_margin = clamp(dynamic_margin_unclipped, min=-inf, max=max_margin).\n7. Calculate the core loss term, scaled by a temperature parameter. This measures the shortfall of the log-probability difference compared to the target margin.\n   core_term = temperature * (logp_diff - dynamic_margin).\n8. The final loss is the negative logsigmoid of the core term. This provides a smooth, hinge-like penalty.\n   loss = -logsigmoid(core_term).", "hyperparams": {"temperature": 5.0, "beta": 1.0, "alpha": 0.1, "max_margin": 10.0}, "operators_used": ["zscore", "rank_gap", "softplus", "clamp", "logsigmoid"], "implementation_hint": {"expects": ["cost_a", "cost_b", "logp_a", "logp_b"], "returns": "scalar"}}, "fitness": {"hf_like_score": 3.883304715156555, "validation_objective": 3.883304715156555, "generalization_penalty": 0.0, "generalization_objectives": {"20": 3.8509840965270996}, "train_score_mean": 4.203894630074501, "train_loss_mean": 0.6677600871771574, "pair_count": 4113220, "config": {"hf": {"problem": "tsp", "hf_steps": 32, "train_problem_size": 20, "valid_problem_sizes": [20], "train_batch_size": 64, "pomo_size": 64, "learning_rate": 0.0003, "weight_decay": 1e-06, "alpha": 0.05, "device": "cuda", "seed": 1234, "num_validation_episodes": 128, "validation_batch_size": 64, "generalization_penalty_weight": 1.0, "pool_version": "v0"}, "free_loss": {"f1_steps": 32, "f2_steps": 100, "f3_enabled": false}}, "loss_ir": {"name": "Softplus-Clipped Rank-Adaptive Log-Probability Loss", "intuition": "This loss function constructs a highly stable and adaptive margin by synthesizing distribution-based normalization with a smooth, non-saturating activation, and then couples this margin directly to the model's own confidence level.\n\nInherited Ideas:\n- From Parent 0, it inherits the idea of creating a dynamic margin by coupling a cost-derived adaptive margin with a term proportional to the model's log-probability difference (`alpha * logp_diff`). This makes the target separation dependent on both the ground-truth cost difference and the model's current confidence.\n- From Parent 1, it inherits the use of z-score normalization (`zscore(costs)`) as a pre-processing step. This centers the cost distribution and makes the subsequent margin calculation robust to the absolute scale and shift of the costs, focusing on the batch's statistical properties.\n\nNew Coupling Ideas:\n1. **Softplus for Non-Saturating Margin**: Instead of using saturating functions like `sigmoid` (Parent 0) or `tanh` (Parent 1), this loss uses `softplus(beta * rank_diff)` to create the adaptive margin. `softplus` is a smooth, non-negative, and non-saturating function. This allows the margin to grow unboundedly (but smoothly) as the rank-based preference signal becomes stronger, which can be beneficial for pairs with very clear preference, without the hard cap imposed by `sigmoid` or `tanh`.\n2. **Margin Stability via Clipping**: The `softplus` function can grow large, which might lead to an excessively large `dynamic_margin` and potential instability. To counteract this, the final `dynamic_margin` is clipped to a maximum value (`max_margin`). This `clamp` operation acts as a stability trick, reaping the benefits of the smooth, non-saturating `softplus` for most of the range while preventing extreme values from destabilizing the training process.", "hyperparams": {"temperature": 5.0, "beta": 1.0, "alpha": 0.1, "max_margin": 10.0}, "operators_used": ["zscore", "rank_gap", "softplus", "clamp", "logsigmoid"]}}}
{"generation": 6, "index": 3, "ir": {"name": "Confidence-Coupled Z-Score Sigmoid Hinge Loss", "intuition": "This loss function creates a dynamic and robust margin by combining distribution-based normalization with a confidence-aware target, applied within a stable sigmoid-based hinge loss framework.\n\nInherited Ideas:\n- From Parent 1 (Z-Scored Rank-Adaptive Hinge Loss), it inherits the idea of pre-normalizing costs using a `zscore` transformation. This crucial first step centers the cost distribution and makes the subsequent margin calculation robust to the absolute scale or shift of costs in a batch.\n- From Parent 0 (Hybrid Z-Score and Rank-Adaptive Sigmoid Loss), it inherits the idea of coupling the target margin directly to the model's confidence. The margin is dynamically adjusted using a term proportional to the log-probability difference (`alpha * logp_diff`), making the training target more ambitious when the model is already confident and more lenient when it is uncertain.\n\nNew Coupling Ideas:\n1. **Simplified Margin Signal**: Instead of using `rank_gap` after z-scoring, this loss directly uses the difference of the z-scored costs (`z_cost_b - z_cost_a`) as the input to the margin-shaping function. This simplifies the computation while still providing a normalized signal of preference strength that is sensitive to the magnitude of the difference within the batch's distribution, not just the rank.\n2. **Sigmoid-Shaped Margin**: The normalized cost difference is passed through a `sigmoid` function (as seen in Parent 0's margin construction) instead of `tanh` (from Parent 1). Using `sigmoid` on the difference `z_cost_b - z_cost_a` is a natural choice, as this difference is positive when `a` is preferred. The sigmoid function smoothly maps this positive-unbounded signal to a bounded (0, 1) range, which is then scaled to create a well-behaved adaptive margin.", "pseudocode": "1. Given costs for a batch, normalize them using z-scoring: z_cost_a = zscore(cost_a), z_cost_b = zscore(cost_b).\n2. Compute the difference of the z-scored costs. This represents the preference strength, normalized by the batch's statistics. We expect z_cost_b > z_cost_a when a is preferred.\n   z_diff = z_cost_b - z_cost_a.\n3. Create the cost-based adaptive margin by passing `z_diff` through a sigmoid function and scaling it. This ensures the margin is bounded and grows smoothly with preference strength.\n   adaptive_margin = max_margin * sigmoid(beta * z_diff).\n4. Compute the log-probability difference: logp_diff = logp_a - logp_b.\n5. Create the final dynamic margin by adding a term proportional to the log-probability difference itself. This couples the target to the model's current confidence.\n   dynamic_margin = alpha * logp_diff + adaptive_margin.\n6. Calculate the core term, which represents how much the model's prediction (`logp_diff`) falls short of the target (`dynamic_margin`).\n   core_term = temperature * (logp_diff - dynamic_margin).\n7. The final loss is the negative logsigmoid of the core term. This applies a smooth, hinge-like penalty that is numerically stable and encourages `logp_diff` to exceed `dynamic_margin`.\n   loss = -logsigmoid(core_term).", "hyperparams": {"temperature": 5.0, "max_margin": 4.0, "beta": 1.0, "alpha": 0.1}, "operators_used": ["zscore", "sigmoid", "logsigmoid"], "implementation_hint": {"expects": ["cost_a", "cost_b", "logp_a", "logp_b"], "returns": "scalar"}}, "fitness": {"hf_like_score": 3.8821520805358887, "validation_objective": 3.8821520805358887, "generalization_penalty": 0.0, "generalization_objectives": {"20": 3.8484264612197876}, "train_score_mean": 4.203710429370403, "train_loss_mean": 0.6678484305739403, "pair_count": 4113135, "config": {"hf": {"problem": "tsp", "hf_steps": 32, "train_problem_size": 20, "valid_problem_sizes": [20], "train_batch_size": 64, "pomo_size": 64, "learning_rate": 0.0003, "weight_decay": 1e-06, "alpha": 0.05, "device": "cuda", "seed": 1234, "num_validation_episodes": 128, "validation_batch_size": 64, "generalization_penalty_weight": 1.0, "pool_version": "v0"}, "free_loss": {"f1_steps": 32, "f2_steps": 100, "f3_enabled": false}}, "loss_ir": {"name": "Confidence-Coupled Z-Score Sigmoid Hinge Loss", "intuition": "This loss function creates a dynamic and robust margin by combining distribution-based normalization with a confidence-aware target, applied within a stable sigmoid-based hinge loss framework.\n\nInherited Ideas:\n- From Parent 1 (Z-Scored Rank-Adaptive Hinge Loss), it inherits the idea of pre-normalizing costs using a `zscore` transformation. This crucial first step centers the cost distribution and makes the subsequent margin calculation robust to the absolute scale or shift of costs in a batch.\n- From Parent 0 (Hybrid Z-Score and Rank-Adaptive Sigmoid Loss), it inherits the idea of coupling the target margin directly to the model's confidence. The margin is dynamically adjusted using a term proportional to the log-probability difference (`alpha * logp_diff`), making the training target more ambitious when the model is already confident and more lenient when it is uncertain.\n\nNew Coupling Ideas:\n1. **Simplified Margin Signal**: Instead of using `rank_gap` after z-scoring, this loss directly uses the difference of the z-scored costs (`z_cost_b - z_cost_a`) as the input to the margin-shaping function. This simplifies the computation while still providing a normalized signal of preference strength that is sensitive to the magnitude of the difference within the batch's distribution, not just the rank.\n2. **Sigmoid-Shaped Margin**: The normalized cost difference is passed through a `sigmoid` function (as seen in Parent 0's margin construction) instead of `tanh` (from Parent 1). Using `sigmoid` on the difference `z_cost_b - z_cost_a` is a natural choice, as this difference is positive when `a` is preferred. The sigmoid function smoothly maps this positive-unbounded signal to a bounded (0, 1) range, which is then scaled to create a well-behaved adaptive margin.", "hyperparams": {"temperature": 5.0, "max_margin": 4.0, "beta": 1.0, "alpha": 0.1}, "operators_used": ["zscore", "sigmoid", "logsigmoid"]}}}
{"generation": 6, "index": 4, "ir": {"name": "Probabilistic-Gated Adaptive Margin Loss", "intuition": "This loss function creates a dynamic margin that smoothly interpolates between a cost-based margin and a model-confidence-based margin, using the model's own confidence as the gating mechanism. The final loss is computed using a stable hinge-like structure.\n\nInherited Ideas:\n- From Parent 0, it inherits the idea of coupling the margin directly to the model's log-probability difference (`logp_diff`). This makes the target separation dependent on the model's current confidence, pushing for greater separation when the model is already confident.\n- From Parent 1, it inherits the use of `tanh` to create a smooth, bounded, and saturating margin from a normalized cost signal (`tanh(beta * rank_diff)`). This ensures the cost-derived part of the margin is stable and not overly influenced by extreme cost differences.\n\nNew Coupling Ideas:\n1. **Probabilistic Gating Mechanism**: A gating coefficient, `gate`, is created by applying a `sigmoid` function to the log-probability difference (`logp_diff`). This gate represents the model's normalized confidence (from 0 to 1). When the model is very confident (`logp_diff` is large), the gate approaches 1. When it's uncertain (`logp_diff` is near zero), the gate is near 0.5. When it's confidently wrong (`logp_diff` is very negative), the gate approaches 0.\n2. **Gated Margin Interpolation**: The final margin is a convex combination of two components, interpolated using the probabilistic gate. The first component is the cost-based adaptive margin from Parent 1 (`cost_margin`). The second is a confidence-based margin from Parent 0 (`confidence_margin`). The final margin is `gate * confidence_margin + (1 - gate) * cost_margin`. This means when the model is confident, it relies more on a self-improving confidence target. When it is uncertain or wrong, it falls back to the more stable, ground-truth-derived cost margin. This coupling creates a curriculum where the loss adapts its objective based on the model's learning state.", "pseudocode": "1. Given costs for a batch, normalize them using z-scoring: z_cost_a = zscore(cost_a), z_cost_b = zscore(cost_b).\n2. Compute the rank-normalized gap between the z-scored costs to get a robust preference signal.\n   rank_diff = rank_gap(z_cost_a, z_cost_b).\n3. Calculate the cost-based adaptive margin using a scaled tanh function, which provides a smooth and bounded target. This is the 'ground-truth' margin.\n   cost_margin = max_margin * tanh(beta * rank_diff).\n4. Compute the log-probability difference: logp_diff = logp_a - logp_b.\n5. Calculate the confidence-based margin, which is directly proportional to the model's log-probability difference. This is the 'self-improving' margin.\n   confidence_margin = alpha * logp_diff.\n6. Create a probabilistic gate based on the model's confidence by passing the log-probability difference through a sigmoid function.\n   gate = sigmoid(logp_diff).\n7. Compute the final dynamic margin by interpolating between the cost-based and confidence-based margins using the gate.\n   dynamic_margin = gate * confidence_margin + (1.0 - gate) * cost_margin.\n8. Calculate the core loss term, scaled by temperature.\n   core_term = temperature * (logp_diff - dynamic_margin).\n9. The final loss is the negative logsigmoid of the core term, which is numerically stable and penalizes cases where logp_diff is smaller than the dynamic margin.\n   loss = -logsigmoid(core_term).", "hyperparams": {"temperature": 5.0, "max_margin": 3.0, "beta": 1.0, "alpha": 0.2}, "operators_used": ["zscore", "rank_gap", "tanh", "sigmoid", "logsigmoid"], "implementation_hint": {"expects": ["cost_a", "cost_b", "logp_a", "logp_b"], "returns": "scalar"}}, "fitness": {"hf_like_score": 3.8790072202682495, "validation_objective": 3.8790072202682495, "generalization_penalty": 0.0, "generalization_objectives": {"20": 3.847203850746155}, "train_score_mean": 4.210196644067764, "train_loss_mean": 0.6523390132933855, "pair_count": 4117442, "config": {"hf": {"problem": "tsp", "hf_steps": 32, "train_problem_size": 20, "valid_problem_sizes": [20], "train_batch_size": 64, "pomo_size": 64, "learning_rate": 0.0003, "weight_decay": 1e-06, "alpha": 0.05, "device": "cuda", "seed": 1234, "num_validation_episodes": 128, "validation_batch_size": 64, "generalization_penalty_weight": 1.0, "pool_version": "v0"}, "free_loss": {"f1_steps": 32, "f2_steps": 100, "f3_enabled": false}}, "loss_ir": {"name": "Probabilistic-Gated Adaptive Margin Loss", "intuition": "This loss function creates a dynamic margin that smoothly interpolates between a cost-based margin and a model-confidence-based margin, using the model's own confidence as the gating mechanism. The final loss is computed using a stable hinge-like structure.\n\nInherited Ideas:\n- From Parent 0, it inherits the idea of coupling the margin directly to the model's log-probability difference (`logp_diff`). This makes the target separation dependent on the model's current confidence, pushing for greater separation when the model is already confident.\n- From Parent 1, it inherits the use of `tanh` to create a smooth, bounded, and saturating margin from a normalized cost signal (`tanh(beta * rank_diff)`). This ensures the cost-derived part of the margin is stable and not overly influenced by extreme cost differences.\n\nNew Coupling Ideas:\n1. **Probabilistic Gating Mechanism**: A gating coefficient, `gate`, is created by applying a `sigmoid` function to the log-probability difference (`logp_diff`). This gate represents the model's normalized confidence (from 0 to 1). When the model is very confident (`logp_diff` is large), the gate approaches 1. When it's uncertain (`logp_diff` is near zero), the gate is near 0.5. When it's confidently wrong (`logp_diff` is very negative), the gate approaches 0.\n2. **Gated Margin Interpolation**: The final margin is a convex combination of two components, interpolated using the probabilistic gate. The first component is the cost-based adaptive margin from Parent 1 (`cost_margin`). The second is a confidence-based margin from Parent 0 (`confidence_margin`). The final margin is `gate * confidence_margin + (1 - gate) * cost_margin`. This means when the model is confident, it relies more on a self-improving confidence target. When it is uncertain or wrong, it falls back to the more stable, ground-truth-derived cost margin. This coupling creates a curriculum where the loss adapts its objective based on the model's learning state.", "hyperparams": {"temperature": 5.0, "max_margin": 3.0, "beta": 1.0, "alpha": 0.2}, "operators_used": ["zscore", "rank_gap", "tanh", "sigmoid", "logsigmoid"]}}}
{"generation": 6, "index": 5, "ir": {"name": "Confidence-Scaled Rank-Adaptive Hinge Loss", "intuition": "This loss function creates a robust, adaptive margin by combining rank-based normalization with a confidence-aware scaling mechanism, and applies it within a stable hinge-loss framework.\n\nInherited Ideas:\n- From Parent 1 (Z-Scored Rank-Adaptive Hinge Loss), it inherits the core structure of creating a bounded, adaptive margin using `max_margin * tanh(beta * rank_diff)`. This ensures the margin is smooth, saturates, and adapts to the relative preference strength indicated by the rank-normalized cost difference.\n- From both parents, it inherits the final loss formulation of `-logsigmoid(temperature * (logp_diff - margin))`. This is equivalent to `softplus(temperature * (margin - logp_diff))`, providing a smooth and numerically stable hinge-like penalty.\n\nNew Coupling Ideas:\n1. **Confidence-Based Margin Scaling**: The `adaptive_margin` derived from the cost difference is dynamically scaled based on the model's own confidence. The scaling factor, `exp(-alpha * abs(logp_diff))`, reduces the target margin when the model is already very confident (either correctly or incorrectly). This prevents the loss from pushing for an unnecessarily large margin on already well-separated pairs, allowing the model to focus its capacity on more ambiguous or difficult preference pairs where `logp_diff` is small. This coupling makes the training process more efficient by focusing on the most informative examples.\n2. **Rank-Gap on Raw Costs**: Instead of pre-normalizing costs with a z-score, this loss applies `rank_gap` directly to the raw costs. This simplifies the pipeline and relies solely on the robust nature of rank-ordering to handle outliers and scale variance, making the margin dependent only on the relative ordering within the batch.", "pseudocode": "1. Given costs for a batch, compute the rank-normalized gap directly on the raw costs. This provides a stable signal of relative preference, typically in [-1, 1].\n   rank_diff = rank_gap(cost_a, cost_b).\n2. Create a base adaptive margin by passing the rank difference through a `tanh` function and scaling it. This creates a smooth, bounded target separation based on the cost difference.\n   base_margin = max_margin * tanh(beta * rank_diff).\n3. Compute the log-probability difference: logp_diff = logp_a - logp_b.\n4. Compute a confidence-based scaling factor. This factor approaches 1 when the model is uncertain (logp_diff is near 0) and approaches 0 when the model is very confident (abs(logp_diff) is large).\n   confidence_scale = exp(-alpha * clamp(abs(logp_diff), min=0, max=10)). The clamp adds stability for extreme logp values.\n5. Calculate the final, dynamically scaled margin by multiplying the base margin with the confidence scale. This couples the margin target to the model's current state.\n   dynamic_margin = confidence_scale * base_margin.\n6. Calculate the core loss term, which is the difference between the log-probability gap and the dynamic margin, scaled by temperature.\n   core_term = temperature * (logp_diff - dynamic_margin).\n7. The final loss is the negative logsigmoid of the core term. This penalizes cases where `logp_diff` is smaller than the `dynamic_margin`.\n   loss = -logsigmoid(core_term).", "hyperparams": {"beta": 1.0, "max_margin": 3.0, "temperature": 5.0, "alpha": 0.2}, "operators_used": ["rank_gap", "tanh", "exp", "clamp", "logsigmoid"], "implementation_hint": {"expects": ["cost_a", "cost_b", "logp_a", "logp_b"], "returns": "scalar"}}, "fitness": {"hf_like_score": 3.8838878870010376, "validation_objective": 3.8838878870010376, "generalization_penalty": 0.0, "generalization_objectives": {"20": 3.847526431083679}, "train_score_mean": 4.208515249192715, "train_loss_mean": 0.6523947194218636, "pair_count": 4116712, "config": {"hf": {"problem": "tsp", "hf_steps": 32, "train_problem_size": 20, "valid_problem_sizes": [20], "train_batch_size": 64, "pomo_size": 64, "learning_rate": 0.0003, "weight_decay": 1e-06, "alpha": 0.05, "device": "cuda", "seed": 1234, "num_validation_episodes": 128, "validation_batch_size": 64, "generalization_penalty_weight": 1.0, "pool_version": "v0"}, "free_loss": {"f1_steps": 32, "f2_steps": 100, "f3_enabled": false}}, "loss_ir": {"name": "Confidence-Scaled Rank-Adaptive Hinge Loss", "intuition": "This loss function creates a robust, adaptive margin by combining rank-based normalization with a confidence-aware scaling mechanism, and applies it within a stable hinge-loss framework.\n\nInherited Ideas:\n- From Parent 1 (Z-Scored Rank-Adaptive Hinge Loss), it inherits the core structure of creating a bounded, adaptive margin using `max_margin * tanh(beta * rank_diff)`. This ensures the margin is smooth, saturates, and adapts to the relative preference strength indicated by the rank-normalized cost difference.\n- From both parents, it inherits the final loss formulation of `-logsigmoid(temperature * (logp_diff - margin))`. This is equivalent to `softplus(temperature * (margin - logp_diff))`, providing a smooth and numerically stable hinge-like penalty.\n\nNew Coupling Ideas:\n1. **Confidence-Based Margin Scaling**: The `adaptive_margin` derived from the cost difference is dynamically scaled based on the model's own confidence. The scaling factor, `exp(-alpha * abs(logp_diff))`, reduces the target margin when the model is already very confident (either correctly or incorrectly). This prevents the loss from pushing for an unnecessarily large margin on already well-separated pairs, allowing the model to focus its capacity on more ambiguous or difficult preference pairs where `logp_diff` is small. This coupling makes the training process more efficient by focusing on the most informative examples.\n2. **Rank-Gap on Raw Costs**: Instead of pre-normalizing costs with a z-score, this loss applies `rank_gap` directly to the raw costs. This simplifies the pipeline and relies solely on the robust nature of rank-ordering to handle outliers and scale variance, making the margin dependent only on the relative ordering within the batch.", "hyperparams": {"beta": 1.0, "max_margin": 3.0, "temperature": 5.0, "alpha": 0.2}, "operators_used": ["rank_gap", "tanh", "exp", "clamp", "logsigmoid"]}}}
{"generation": 6, "index": 6, "ir": {"name": "Confidence-Weighted Rank-Hinge Loss", "intuition": "This loss function constructs a highly robust, dual-component margin that adapts to both the rank-based cost difference and the model's own confidence, then applies it within a stable hinge-loss framework. The goal is to create a more nuanced training signal that pushes harder when the model is confident but wrong, and less so when it is uncertain.\n\nInherited Ideas:\n- From Parent 0, it inherits the idea of a dynamic margin that incorporates the model's log-probability difference (`logp_diff`). Specifically, it uses `alpha * logp_diff` as one component of the margin, coupling the target separation directly to the model's current confidence state.\n- From Parent 1, it inherits the use of `tanh` to create a smooth, bounded, and saturating margin from a normalized cost signal. This prevents the margin from growing uncontrollably and enhances numerical stability.\n\nNew Coupling Ideas:\n1. **Rank-Gap on Raw Costs**: Unlike the parents, which use z-scored costs, this loss computes the `rank_gap` directly on the raw costs. This simplifies the pipeline and focuses the `tanh` component purely on the relative ordinal preference, assuming that the `tanh` function's saturating nature already provides sufficient robustness against the scale of the costs.\n2. **Confidence-Weighted Hinge Margin**: The final margin is a weighted sum of the cost-derived signal and the model's confidence. The core term `(logp_diff - margin)` is rearranged to `(logp_diff - (cost_margin + confidence_margin))`, which simplifies to `(logp_diff * (1 - alpha)) - cost_margin`. This reframing acts as a confidence-based weighting on the log-probability difference itself. When the model is confident and correct (large positive `logp_diff`), the loss gradient is down-weighted by `(1 - alpha)`. Conversely, if the model is confident but wrong (large negative `logp_diff`), the penalty is amplified. This creates a dynamic, confidence-aware hinge loss.", "pseudocode": "1. Given costs for a batch, compute the rank-normalized gap directly on the raw costs. This captures the relative preference ordering.\n   rank_diff = rank_gap(cost_a, cost_b).\n2. Create the cost-based component of the margin by passing `rank_diff` through a `tanh` function and scaling it. This provides a smooth, bounded signal based on preference strength.\n   cost_margin = max_margin * tanh(beta * rank_diff).\n3. Compute the log-probability difference: logp_diff = logp_a - logp_b.\n4. Calculate the core term for the loss. This term re-weights the log-probability difference by `(1 - alpha)` before subtracting the cost-based margin. This couples the loss directly to model confidence.\n   core_term = temperature * ((1 - alpha) * logp_diff - cost_margin).\n5. The final loss is the negative logsigmoid of the core term. This implements a smooth and numerically stable hinge-like loss, penalizing cases where the confidence-weighted log-probability difference is smaller than the target cost-based margin. This is equivalent to `softplus(-core_term)`.\n   loss = -logsigmoid(core_term).", "hyperparams": {"temperature": 5.0, "max_margin": 3.0, "beta": 1.0, "alpha": 0.1}, "operators_used": ["rank_gap", "tanh", "logsigmoid"], "implementation_hint": {"expects": ["cost_a", "cost_b", "logp_a", "logp_b"], "returns": "scalar"}}, "fitness": {"hf_like_score": 3.8823657035827637, "validation_objective": 3.8823657035827637, "generalization_penalty": 0.0, "generalization_objectives": {"20": 3.853273630142212}, "train_score_mean": 4.206592001020908, "train_loss_mean": 0.6676453445106745, "pair_count": 4113180, "config": {"hf": {"problem": "tsp", "hf_steps": 32, "train_problem_size": 20, "valid_problem_sizes": [20], "train_batch_size": 64, "pomo_size": 64, "learning_rate": 0.0003, "weight_decay": 1e-06, "alpha": 0.05, "device": "cuda", "seed": 1234, "num_validation_episodes": 128, "validation_batch_size": 64, "generalization_penalty_weight": 1.0, "pool_version": "v0"}, "free_loss": {"f1_steps": 32, "f2_steps": 100, "f3_enabled": false}}, "loss_ir": {"name": "Confidence-Weighted Rank-Hinge Loss", "intuition": "This loss function constructs a highly robust, dual-component margin that adapts to both the rank-based cost difference and the model's own confidence, then applies it within a stable hinge-loss framework. The goal is to create a more nuanced training signal that pushes harder when the model is confident but wrong, and less so when it is uncertain.\n\nInherited Ideas:\n- From Parent 0, it inherits the idea of a dynamic margin that incorporates the model's log-probability difference (`logp_diff`). Specifically, it uses `alpha * logp_diff` as one component of the margin, coupling the target separation directly to the model's current confidence state.\n- From Parent 1, it inherits the use of `tanh` to create a smooth, bounded, and saturating margin from a normalized cost signal. This prevents the margin from growing uncontrollably and enhances numerical stability.\n\nNew Coupling Ideas:\n1. **Rank-Gap on Raw Costs**: Unlike the parents, which use z-scored costs, this loss computes the `rank_gap` directly on the raw costs. This simplifies the pipeline and focuses the `tanh` component purely on the relative ordinal preference, assuming that the `tanh` function's saturating nature already provides sufficient robustness against the scale of the costs.\n2. **Confidence-Weighted Hinge Margin**: The final margin is a weighted sum of the cost-derived signal and the model's confidence. The core term `(logp_diff - margin)` is rearranged to `(logp_diff - (cost_margin + confidence_margin))`, which simplifies to `(logp_diff * (1 - alpha)) - cost_margin`. This reframing acts as a confidence-based weighting on the log-probability difference itself. When the model is confident and correct (large positive `logp_diff`), the loss gradient is down-weighted by `(1 - alpha)`. Conversely, if the model is confident but wrong (large negative `logp_diff`), the penalty is amplified. This creates a dynamic, confidence-aware hinge loss.", "hyperparams": {"temperature": 5.0, "max_margin": 3.0, "beta": 1.0, "alpha": 0.1}, "operators_used": ["rank_gap", "tanh", "logsigmoid"]}}}
{"generation": 6, "index": 7, "ir": {"name": "Confidence-Weighted Rank-Gap Hinge Loss", "intuition": "This loss function creates a highly adaptive margin by combining rank-based normalization with a dynamic, confidence-aware target. It penalizes the model when its log-probability difference for a preferred option falls short of this adaptive margin, using a stable hinge-like loss.\n\nInherited Ideas:\n- From Parent 0, it inherits the idea of coupling the margin directly to the model's confidence. The final target margin includes a term proportional to the log-probability difference (`alpha * logp_diff`). This makes the margin dynamic, increasing the target separation when the model is already confident and relying on the cost-based margin when it is not.\n- From Parent 1, it inherits the use of `tanh` to create a smooth, bounded, and saturating margin from a normalized cost signal (`max_margin * tanh(...)`). This enhances stability by preventing the margin from growing uncontrollably due to extreme cost differences.\n\nNew Coupling Ideas:\n1. **Rank-Gap as the Primary Signal**: Instead of z-scoring costs first, this loss directly uses the `rank_gap` of the raw costs as the input to the `tanh` function. This simplifies the normalization pipeline while still capturing the crucial relative ordering of preferences, making the margin robust to outliers and independent of the absolute scale of costs.\n2. **Softplus-Based Hinge Loss**: The core loss is formulated as `softplus(temperature * (dynamic_margin - logp_diff))`. This is a numerically stable implementation of a hinge loss, which is equivalent to `-logsigmoid(temperature * (logp_diff - dynamic_margin))` used by both parents. It directly penalizes the model only when the log-probability difference (`logp_diff`) is less than the target `dynamic_margin`, creating a one-sided penalty that encourages meeting but not excessively exceeding the margin.", "pseudocode": "1. Compute the rank-normalized gap between the raw costs. This provides a stable signal of relative preference, typically in [-1, 1].\n   rank_diff = rank_gap(cost_a, cost_b).\n2. Create a base adaptive margin by passing the rank difference through a `tanh` function and scaling it. This creates a smooth, bounded target based on cost ordering.\n   base_margin = max_margin * tanh(beta * rank_diff).\n3. Compute the log-probability difference: logp_diff = logp_a - logp_b.\n4. Create the final dynamic margin by adding a term proportional to the log-probability difference itself. This couples the target to the model's current confidence.\n   dynamic_margin = alpha * logp_diff + base_margin.\n5. Calculate the core difference between the target margin and the model's log-probability difference, scaled by temperature.\n   core_diff = temperature * (dynamic_margin - logp_diff).\n6. The final loss is the `softplus` of the core difference. This acts as a smooth hinge loss, penalizing the model only when `logp_diff` is less than `dynamic_margin`.\n   loss = softplus(core_diff).", "hyperparams": {"max_margin": 3.5, "beta": 1.0, "alpha": 0.1, "temperature": 5.0}, "operators_used": ["rank_gap", "tanh", "softplus"], "implementation_hint": {"expects": ["cost_a", "cost_b", "logp_a", "logp_b"], "returns": "scalar"}}, "fitness": {"hf_like_score": 3.8807135820388794, "validation_objective": 3.8807135820388794, "generalization_penalty": 0.0, "generalization_objectives": {"20": 3.850472569465637}, "train_score_mean": 4.203796282410622, "train_loss_mean": 0.6671806368976831, "pair_count": 4113767, "config": {"hf": {"problem": "tsp", "hf_steps": 32, "train_problem_size": 20, "valid_problem_sizes": [20], "train_batch_size": 64, "pomo_size": 64, "learning_rate": 0.0003, "weight_decay": 1e-06, "alpha": 0.05, "device": "cuda", "seed": 1234, "num_validation_episodes": 128, "validation_batch_size": 64, "generalization_penalty_weight": 1.0, "pool_version": "v0"}, "free_loss": {"f1_steps": 32, "f2_steps": 100, "f3_enabled": false}}, "loss_ir": {"name": "Confidence-Weighted Rank-Gap Hinge Loss", "intuition": "This loss function creates a highly adaptive margin by combining rank-based normalization with a dynamic, confidence-aware target. It penalizes the model when its log-probability difference for a preferred option falls short of this adaptive margin, using a stable hinge-like loss.\n\nInherited Ideas:\n- From Parent 0, it inherits the idea of coupling the margin directly to the model's confidence. The final target margin includes a term proportional to the log-probability difference (`alpha * logp_diff`). This makes the margin dynamic, increasing the target separation when the model is already confident and relying on the cost-based margin when it is not.\n- From Parent 1, it inherits the use of `tanh` to create a smooth, bounded, and saturating margin from a normalized cost signal (`max_margin * tanh(...)`). This enhances stability by preventing the margin from growing uncontrollably due to extreme cost differences.\n\nNew Coupling Ideas:\n1. **Rank-Gap as the Primary Signal**: Instead of z-scoring costs first, this loss directly uses the `rank_gap` of the raw costs as the input to the `tanh` function. This simplifies the normalization pipeline while still capturing the crucial relative ordering of preferences, making the margin robust to outliers and independent of the absolute scale of costs.\n2. **Softplus-Based Hinge Loss**: The core loss is formulated as `softplus(temperature * (dynamic_margin - logp_diff))`. This is a numerically stable implementation of a hinge loss, which is equivalent to `-logsigmoid(temperature * (logp_diff - dynamic_margin))` used by both parents. It directly penalizes the model only when the log-probability difference (`logp_diff`) is less than the target `dynamic_margin`, creating a one-sided penalty that encourages meeting but not excessively exceeding the margin.", "hyperparams": {"max_margin": 3.5, "beta": 1.0, "alpha": 0.1, "temperature": 5.0}, "operators_used": ["rank_gap", "tanh", "softplus"]}}}
{"generation": 7, "index": 0, "ir": {"name": "Confidence-Coupled Log-Ratio Hinge Loss", "intuition": "This loss function combines a robust, distribution-aware margin with a novel log-ratio preference signal that is dynamically coupled to the model's confidence.\n\nInherited Ideas:\n- From Parent 1 (Z-Scored Rank-Adaptive Hinge Loss), it inherits the idea of creating a robust, adaptive margin by first normalizing costs with a `zscore` and then using `rank_gap` on the result. This dual normalization (`zscore` then `rank_gap`) makes the margin highly stable and insensitive to the absolute scale or distribution of costs, focusing purely on relative preference within the batch.\n- From Parent 0 (Hybrid Z-Score and Rank-Adaptive Sigmoid Loss), it inherits the concept of coupling the loss target directly to the model's confidence, represented by the log-probability difference. However, instead of adding a confidence term to the margin, this child loss uses it to scale the entire preference signal, creating a different form of coupling.\n\nNew Coupling Ideas:\n1. **Log-Ratio Preference Signal**: Instead of the standard log-probability difference `logp_a - logp_b`, this loss uses a log-ratio formulation: `log(sigmoid(logp_a)) - log(sigmoid(logp_b))`. Since `sigmoid(x)` approximates the probability, this term represents the log-ratio of the winning and losing probabilities. This signal is more sensitive when probabilities are near 0 or 1 and less sensitive in the uncertain middle range (around 0.5), focusing the gradient on high-confidence errors.\n2. **Confidence-Scaled Margin**: The robust, rank-based adaptive margin is not used as a static target. Instead, it is scaled by the absolute value of the log-probability difference, `abs(logp_a - logp_b)`, plus a small stability constant `epsilon`. The final margin becomes `adaptive_margin * (epsilon + abs(logp_a - logp_b))`. This means when the model is very confident (large `abs(logp_diff)`), it must satisfy a much larger margin, pushing for greater separation. When the model is uncertain, the margin requirement is relaxed, preventing over-penalization for small, uncertain preferences.", "pseudocode": "1. Given costs for a batch, normalize them using z-scoring: z_cost_a = zscore(cost_a), z_cost_b = zscore(cost_b).\n2. Compute the rank-normalized gap between the z-scored costs to create a robust preference signal.\n   rank_diff = rank_gap(z_cost_a, z_cost_b).\n3. Create a base adaptive margin by passing the rank difference through a `tanh` function and scaling it. This creates a smooth, bounded value.\n   base_adaptive_margin = max_margin * tanh(beta * rank_diff).\n4. Compute the log-probability difference: logp_diff = logp_a - logp_b.\n5. Calculate the final, confidence-coupled margin by scaling the base margin with the model's confidence (absolute log-probability difference).\n   confidence_scaled_margin = base_adaptive_margin * (epsilon + abs(logp_diff)).\n6. Compute the log-ratio preference signal. This replaces the standard `logp_diff` in the loss calculation.\n   log_ratio_signal = log(sigmoid(logp_a)) - log(sigmoid(logp_b)).\n7. Calculate the core loss term, which represents the violation of the confidence-scaled margin by the log-ratio signal.\n   core_term = temperature * (log_ratio_signal - confidence_scaled_margin).\n8. The final loss is the negative logsigmoid of the core term, which is equivalent to `softplus(-core_term)`. This applies a smooth hinge-like penalty when the preference signal does not meet the dynamic margin target.", "hyperparams": {"beta": 1.0, "max_margin": 2.0, "temperature": 5.0, "epsilon": 0.01}, "operators_used": ["zscore", "rank_gap", "tanh", "sigmoid", "log", "logsigmoid"], "implementation_hint": {"expects": ["cost_a", "cost_b", "logp_a", "logp_b"], "returns": "scalar"}}, "fitness": {"hf_like_score": 3.8851099014282227, "validation_objective": 3.8851099014282227, "generalization_penalty": 0.0, "generalization_objectives": {"20": 3.8600406646728516}, "train_score_mean": 4.2080796137452126, "train_loss_mean": 0.678494842723012, "pair_count": 4112313, "config": {"hf": {"problem": "tsp", "hf_steps": 32, "train_problem_size": 20, "valid_problem_sizes": [20], "train_batch_size": 64, "pomo_size": 64, "learning_rate": 0.0003, "weight_decay": 1e-06, "alpha": 0.05, "device": "cuda", "seed": 1234, "num_validation_episodes": 128, "validation_batch_size": 64, "generalization_penalty_weight": 1.0, "pool_version": "v0"}, "free_loss": {"f1_steps": 32, "f2_steps": 100, "f3_enabled": false}}, "loss_ir": {"name": "Confidence-Coupled Log-Ratio Hinge Loss", "intuition": "This loss function combines a robust, distribution-aware margin with a novel log-ratio preference signal that is dynamically coupled to the model's confidence.\n\nInherited Ideas:\n- From Parent 1 (Z-Scored Rank-Adaptive Hinge Loss), it inherits the idea of creating a robust, adaptive margin by first normalizing costs with a `zscore` and then using `rank_gap` on the result. This dual normalization (`zscore` then `rank_gap`) makes the margin highly stable and insensitive to the absolute scale or distribution of costs, focusing purely on relative preference within the batch.\n- From Parent 0 (Hybrid Z-Score and Rank-Adaptive Sigmoid Loss), it inherits the concept of coupling the loss target directly to the model's confidence, represented by the log-probability difference. However, instead of adding a confidence term to the margin, this child loss uses it to scale the entire preference signal, creating a different form of coupling.\n\nNew Coupling Ideas:\n1. **Log-Ratio Preference Signal**: Instead of the standard log-probability difference `logp_a - logp_b`, this loss uses a log-ratio formulation: `log(sigmoid(logp_a)) - log(sigmoid(logp_b))`. Since `sigmoid(x)` approximates the probability, this term represents the log-ratio of the winning and losing probabilities. This signal is more sensitive when probabilities are near 0 or 1 and less sensitive in the uncertain middle range (around 0.5), focusing the gradient on high-confidence errors.\n2. **Confidence-Scaled Margin**: The robust, rank-based adaptive margin is not used as a static target. Instead, it is scaled by the absolute value of the log-probability difference, `abs(logp_a - logp_b)`, plus a small stability constant `epsilon`. The final margin becomes `adaptive_margin * (epsilon + abs(logp_a - logp_b))`. This means when the model is very confident (large `abs(logp_diff)`), it must satisfy a much larger margin, pushing for greater separation. When the model is uncertain, the margin requirement is relaxed, preventing over-penalization for small, uncertain preferences.", "hyperparams": {"beta": 1.0, "max_margin": 2.0, "temperature": 5.0, "epsilon": 0.01}, "operators_used": ["zscore", "rank_gap", "tanh", "sigmoid", "log", "logsigmoid"]}}}
{"generation": 7, "index": 1, "ir": {"name": "Confidence-Weighted Z-Rank Hinge Loss", "intuition": "This loss function combines robust cost normalization with a dynamic margin that is also influenced by the model's confidence, all within a stable hinge-loss framework. It aims to create a loss signal that is sensitive to both the ground-truth cost difference and the model's current belief state.\n\nInherited Ideas:\n- From Parent 0, it inherits the idea of coupling the margin directly to the model's confidence. Specifically, the loss target is influenced by the log-probability difference (`logp_diff`), making the margin dynamic. When the model is confident in the correct direction, the target margin increases, pushing for even better separation.\n- From Parent 1, it inherits the core structure of a z-scored, rank-adaptive hinge loss. This involves first normalizing costs with `zscore`, then computing the `rank_gap`, and finally using a `tanh` function to create a smooth, bounded margin. This multi-step normalization makes the margin robust to both the statistical distribution and the local ordering of costs.\n\nNew Coupling Ideas:\n1. **Confidence-Weighted Margin Scaling**: Instead of simply adding a log-probability term to the margin (as in Parent 0), this loss uses the log-probability difference to *scale* the cost-derived margin. The core margin is calculated from z-scored, rank-gapped costs. This margin is then multiplied by `exp(alpha * logp_diff)`. This creates a powerful coupling: if the model is already confident in the correct preference (`logp_diff` is large and positive), the target margin is amplified, demanding even greater separation. Conversely, if the model is confident in the *wrong* direction (`logp_diff` is large and negative), the target margin is drastically reduced, focusing the gradient on simply correcting the preference direction rather than enforcing a large, unattainable margin.\n2. **Margin Clamping for Stability**: To prevent the `exp` term from causing numerical instability or creating excessively large margins when the model is extremely confident, the final dynamic margin is clamped to a maximum value (`max_margin`). This ensures the loss remains bounded and stable, providing the benefits of confidence-weighting without the risk of gradient explosion.", "pseudocode": "1. Given costs for a batch, normalize them using z-scoring: z_cost_a = zscore(cost_a), z_cost_b = zscore(cost_b).\n2. Compute the rank-normalized gap between the z-scored costs to get a robust preference signal.\n   rank_diff = rank_gap(z_cost_a, z_cost_b).\n3. Create a base adaptive margin by passing the rank difference through a `tanh` function. This creates a smooth, bounded signal based on cost.\n   base_margin = beta * tanh(rank_diff).\n4. Compute the log-probability difference: logp_diff = logp_a - logp_b.\n5. Calculate a confidence-based scaling factor using the log-probability difference.\n   confidence_scaler = exp(alpha * logp_diff).\n6. Create the final dynamic margin by scaling the base margin with the confidence factor, and then clamp it to prevent instability.\n   dynamic_margin = clamp(base_margin * confidence_scaler, min=0.0, max=max_margin).\n7. Calculate the core loss term, which is the difference between the log-probability gap and the dynamic margin, scaled by temperature.\n   core_term = temperature * (logp_diff - dynamic_margin).\n8. The final loss is the negative logsigmoid of the core term, which is a numerically stable hinge-like loss.\n   loss = -logsigmoid(core_term).", "hyperparams": {"beta": 2.0, "max_margin": 4.0, "temperature": 5.0, "alpha": 0.2}, "operators_used": ["zscore", "rank_gap", "tanh", "exp", "clamp", "logsigmoid"], "implementation_hint": {"expects": ["cost_a", "cost_b", "logp_a", "logp_b"], "returns": "scalar"}}, "fitness": {"hf_like_score": 3.879119634628296, "validation_objective": 3.879119634628296, "generalization_penalty": 0.0, "generalization_objectives": {"20": 3.8500542640686035}, "train_score_mean": 4.210158795118332, "train_loss_mean": 0.6522805951535702, "pair_count": 4117637, "config": {"hf": {"problem": "tsp", "hf_steps": 32, "train_problem_size": 20, "valid_problem_sizes": [20], "train_batch_size": 64, "pomo_size": 64, "learning_rate": 0.0003, "weight_decay": 1e-06, "alpha": 0.05, "device": "cuda", "seed": 1234, "num_validation_episodes": 128, "validation_batch_size": 64, "generalization_penalty_weight": 1.0, "pool_version": "v0"}, "free_loss": {"f1_steps": 32, "f2_steps": 100, "f3_enabled": false}}, "loss_ir": {"name": "Confidence-Weighted Z-Rank Hinge Loss", "intuition": "This loss function combines robust cost normalization with a dynamic margin that is also influenced by the model's confidence, all within a stable hinge-loss framework. It aims to create a loss signal that is sensitive to both the ground-truth cost difference and the model's current belief state.\n\nInherited Ideas:\n- From Parent 0, it inherits the idea of coupling the margin directly to the model's confidence. Specifically, the loss target is influenced by the log-probability difference (`logp_diff`), making the margin dynamic. When the model is confident in the correct direction, the target margin increases, pushing for even better separation.\n- From Parent 1, it inherits the core structure of a z-scored, rank-adaptive hinge loss. This involves first normalizing costs with `zscore`, then computing the `rank_gap`, and finally using a `tanh` function to create a smooth, bounded margin. This multi-step normalization makes the margin robust to both the statistical distribution and the local ordering of costs.\n\nNew Coupling Ideas:\n1. **Confidence-Weighted Margin Scaling**: Instead of simply adding a log-probability term to the margin (as in Parent 0), this loss uses the log-probability difference to *scale* the cost-derived margin. The core margin is calculated from z-scored, rank-gapped costs. This margin is then multiplied by `exp(alpha * logp_diff)`. This creates a powerful coupling: if the model is already confident in the correct preference (`logp_diff` is large and positive), the target margin is amplified, demanding even greater separation. Conversely, if the model is confident in the *wrong* direction (`logp_diff` is large and negative), the target margin is drastically reduced, focusing the gradient on simply correcting the preference direction rather than enforcing a large, unattainable margin.\n2. **Margin Clamping for Stability**: To prevent the `exp` term from causing numerical instability or creating excessively large margins when the model is extremely confident, the final dynamic margin is clamped to a maximum value (`max_margin`). This ensures the loss remains bounded and stable, providing the benefits of confidence-weighting without the risk of gradient explosion.", "hyperparams": {"beta": 2.0, "max_margin": 4.0, "temperature": 5.0, "alpha": 0.2}, "operators_used": ["zscore", "rank_gap", "tanh", "exp", "clamp", "logsigmoid"]}}}
{"generation": 7, "index": 2, "ir": {"name": "Softplus-Coupled Z-Score and Rank-Adaptive Loss", "intuition": "This loss function creates a highly adaptive and robust margin by synthesizing rank-based and distribution-based normalization, and then couples this margin directly with the model's confidence to create a dynamic learning target.\n\nInherited Ideas:\n- From Parent 0, it inherits the idea of creating a dynamic margin that includes a term proportional to the log-probability difference (`alpha * logp_diff`). This couples the target separation to the model's own confidence, making the loss more demanding when the model is already confident.\n- From Parent 1, it inherits the use of `zscore` to pre-normalize costs before further processing. This stabilizes the margin calculation against shifts and scaling in the cost distribution, focusing on relative differences within the batch's statistical profile.\n\nNew Coupling Ideas:\n1. **Rank-Adaptive Softplus Margin:** Instead of using a bounded function like `sigmoid` or `tanh` to create the margin, we use `softplus` on the rank-normalized difference of z-scored costs (`rank_gap(zscore(cost_b), zscore(cost_a))`). `softplus` is a smooth, non-negative, and monotonically increasing function that acts like a `relu` but is differentiable everywhere. This allows the margin to grow without a hard upper bound for stronger preferences, providing a more expressive signal, while still being zero for non-preferred items.\n2. **Confidence-Modulated Temperature:** The temperature parameter, which controls the sharpness of the loss, is dynamically modulated by the model's current confidence. A `sigmoid` function is applied to the log-probability difference (`logp_diff`). This means that when the model is highly confident (large `logp_diff`), the temperature approaches `max_temp`. When it is uncertain or incorrect (small or negative `logp_diff`), the temperature is reduced, approaching `max_temp / 2`. This coupling softens the loss gradient when the model is struggling, preventing overly aggressive updates and improving stability.", "pseudocode": "1. Given costs for a batch, normalize them using z-scoring: z_cost_a = zscore(cost_a), z_cost_b = zscore(cost_b).\n2. Compute the rank-normalized gap. Note the order: `rank_gap(z_cost_b, z_cost_a)` ensures the result is positive when `cost_a` is preferred over `cost_b`.\n   rank_diff = rank_gap(z_cost_b, z_cost_a).\n3. Create the primary adaptive margin using `softplus` on the scaled rank difference. This creates a smooth, non-negative, and unbounded margin.\n   adaptive_margin = softplus(beta * rank_diff).\n4. Compute the log-probability difference: logp_diff = logp_a - logp_b.\n5. Create the final dynamic margin by adding a term proportional to the log-probability difference itself.\n   dynamic_margin = adaptive_margin + alpha * logp_diff.\n6. Calculate a dynamic temperature based on the model's confidence. The temperature increases as the model becomes more confident about the correct preference.\n   dynamic_temp = max_temp * sigmoid(logp_diff).\n7. Calculate the core loss term, which is the difference between the log-probability gap and the dynamic margin, scaled by the dynamic temperature.\n   core_term = dynamic_temp * (logp_diff - dynamic_margin).\n8. The final loss is the negative logsigmoid of the core term, which is numerically stable and equivalent to `softplus(-core_term)`.\n   loss = -logsigmoid(core_term).", "hyperparams": {"beta": 1.0, "alpha": 0.1, "max_temp": 5.0}, "operators_used": ["zscore", "rank_gap", "softplus", "sigmoid", "logsigmoid"], "implementation_hint": {"expects": ["cost_a", "cost_b", "logp_a", "logp_b"], "returns": "scalar"}}, "fitness": {"hf_like_score": 3.887633204460144, "validation_objective": 3.887633204460144, "generalization_penalty": 0.0, "generalization_objectives": {"20": 3.8501096963882446}, "train_score_mean": 4.205016404390335, "train_loss_mean": 0.6677992567420006, "pair_count": 4113352, "config": {"hf": {"problem": "tsp", "hf_steps": 32, "train_problem_size": 20, "valid_problem_sizes": [20], "train_batch_size": 64, "pomo_size": 64, "learning_rate": 0.0003, "weight_decay": 1e-06, "alpha": 0.05, "device": "cuda", "seed": 1234, "num_validation_episodes": 128, "validation_batch_size": 64, "generalization_penalty_weight": 1.0, "pool_version": "v0"}, "free_loss": {"f1_steps": 32, "f2_steps": 100, "f3_enabled": false}}, "loss_ir": {"name": "Softplus-Coupled Z-Score and Rank-Adaptive Loss", "intuition": "This loss function creates a highly adaptive and robust margin by synthesizing rank-based and distribution-based normalization, and then couples this margin directly with the model's confidence to create a dynamic learning target.\n\nInherited Ideas:\n- From Parent 0, it inherits the idea of creating a dynamic margin that includes a term proportional to the log-probability difference (`alpha * logp_diff`). This couples the target separation to the model's own confidence, making the loss more demanding when the model is already confident.\n- From Parent 1, it inherits the use of `zscore` to pre-normalize costs before further processing. This stabilizes the margin calculation against shifts and scaling in the cost distribution, focusing on relative differences within the batch's statistical profile.\n\nNew Coupling Ideas:\n1. **Rank-Adaptive Softplus Margin:** Instead of using a bounded function like `sigmoid` or `tanh` to create the margin, we use `softplus` on the rank-normalized difference of z-scored costs (`rank_gap(zscore(cost_b), zscore(cost_a))`). `softplus` is a smooth, non-negative, and monotonically increasing function that acts like a `relu` but is differentiable everywhere. This allows the margin to grow without a hard upper bound for stronger preferences, providing a more expressive signal, while still being zero for non-preferred items.\n2. **Confidence-Modulated Temperature:** The temperature parameter, which controls the sharpness of the loss, is dynamically modulated by the model's current confidence. A `sigmoid` function is applied to the log-probability difference (`logp_diff`). This means that when the model is highly confident (large `logp_diff`), the temperature approaches `max_temp`. When it is uncertain or incorrect (small or negative `logp_diff`), the temperature is reduced, approaching `max_temp / 2`. This coupling softens the loss gradient when the model is struggling, preventing overly aggressive updates and improving stability.", "hyperparams": {"beta": 1.0, "alpha": 0.1, "max_temp": 5.0}, "operators_used": ["zscore", "rank_gap", "softplus", "sigmoid", "logsigmoid"]}}}
{"generation": 7, "index": 3, "ir": {"name": "Confidence-Coupled Rank-Adaptive Softplus Loss", "intuition": "This loss function constructs a highly adaptive target margin by blending cost-based rank signals with model confidence, and then applies this margin within a stable softplus-based hinge loss framework.\n\nInherited Ideas:\n- From Parent 1 (Z-Scored Rank-Adaptive Hinge Loss), it inherits the core structure of creating a bounded, adaptive margin from a normalized cost difference. Specifically, it uses `zscore` to normalize batch costs, then `rank_gap` to get a relative preference signal, and finally `tanh` to smoothly saturate this signal. This ensures the margin is robust to cost scale and outliers. The `tanh` function is preferred over `sigmoid` from Parent 0 as it provides a symmetric (-1, 1) range before scaling.\n- From Parent 0 (Hybrid Z-Score and Rank-Adaptive Sigmoid Loss), it inherits the idea of coupling the margin directly to the model's confidence. The final margin includes a term proportional to the log-probability difference (`logp_diff`), making the target separation dynamic. This forces the model to achieve a greater separation when it is already confident, encouraging it to further refine its predictions.\n\nNew Coupling Ideas:\n1. **Softplus Hinge Formulation**: Instead of the common `logsigmoid` formulation, this loss uses `softplus(margin - logp_diff)`. While mathematically equivalent to `-logsigmoid(logp_diff - margin)`, this form explicitly frames the problem as minimizing a smoothed hinge loss on the violation `margin - logp_diff`, which can be more intuitive. It directly penalizes the model when `logp_diff` falls short of the target `margin`.\n2. **Margin Clamping for Stability**: A `clamp` operation is introduced to the final dynamic margin before it is used in the loss. The margin is clamped to a minimum of zero. This prevents the margin from becoming negative, which could happen if `logp_diff` is very negative and the `alpha` term dominates. A negative margin would incorrectly reward the model for preferring the worse candidate, so clamping it at zero ensures the loss always encourages the correct preference direction.", "pseudocode": "1. Given costs for a batch, normalize them using z-scoring to make them scale-invariant: z_cost_a = zscore(cost_a), z_cost_b = zscore(cost_b).\n2. Compute the rank-normalized gap between the z-scored costs. This provides a robust relative preference signal, typically in [-1, 1].\n   rank_diff = rank_gap(z_cost_a, z_cost_b).\n3. Create the cost-based adaptive margin by passing `rank_diff` through a `tanh` function and scaling it. This creates a smooth, bounded margin based on preference strength.\n   cost_margin = max_margin * tanh(beta * rank_diff).\n4. Compute the log-probability difference: logp_diff = logp_a - logp_b.\n5. Create the final dynamic margin by adding a term proportional to the log-probability difference itself. This couples the target to the model's confidence.\n   dynamic_margin = cost_margin + alpha * logp_diff.\n6. For stability, clamp the dynamic margin to be non-negative. This prevents the loss from rewarding the model for preferring the wrong candidate if `logp_diff` is very negative.\n   stable_margin = clamp(dynamic_margin, min=0).\n7. Calculate the final loss using the softplus function, which acts as a smooth hinge loss. The loss is scaled by a temperature parameter.\n   loss = softplus(temperature * (stable_margin - logp_diff)).", "hyperparams": {"temperature": 5.0, "max_margin": 3.5, "beta": 1.0, "alpha": 0.1}, "operators_used": ["zscore", "rank_gap", "tanh", "clamp", "softplus"], "implementation_hint": {"expects": ["cost_a", "cost_b", "logp_a", "logp_b"], "returns": "scalar"}}, "fitness": {"hf_like_score": 3.8821581602096558, "validation_objective": 3.8821581602096558, "generalization_penalty": 0.0, "generalization_objectives": {"20": 3.851639986038208}, "train_score_mean": 4.203304313123226, "train_loss_mean": 0.6676076911389828, "pair_count": 4112792, "config": {"hf": {"problem": "tsp", "hf_steps": 32, "train_problem_size": 20, "valid_problem_sizes": [20], "train_batch_size": 64, "pomo_size": 64, "learning_rate": 0.0003, "weight_decay": 1e-06, "alpha": 0.05, "device": "cuda", "seed": 1234, "num_validation_episodes": 128, "validation_batch_size": 64, "generalization_penalty_weight": 1.0, "pool_version": "v0"}, "free_loss": {"f1_steps": 32, "f2_steps": 100, "f3_enabled": false}}, "loss_ir": {"name": "Confidence-Coupled Rank-Adaptive Softplus Loss", "intuition": "This loss function constructs a highly adaptive target margin by blending cost-based rank signals with model confidence, and then applies this margin within a stable softplus-based hinge loss framework.\n\nInherited Ideas:\n- From Parent 1 (Z-Scored Rank-Adaptive Hinge Loss), it inherits the core structure of creating a bounded, adaptive margin from a normalized cost difference. Specifically, it uses `zscore` to normalize batch costs, then `rank_gap` to get a relative preference signal, and finally `tanh` to smoothly saturate this signal. This ensures the margin is robust to cost scale and outliers. The `tanh` function is preferred over `sigmoid` from Parent 0 as it provides a symmetric (-1, 1) range before scaling.\n- From Parent 0 (Hybrid Z-Score and Rank-Adaptive Sigmoid Loss), it inherits the idea of coupling the margin directly to the model's confidence. The final margin includes a term proportional to the log-probability difference (`logp_diff`), making the target separation dynamic. This forces the model to achieve a greater separation when it is already confident, encouraging it to further refine its predictions.\n\nNew Coupling Ideas:\n1. **Softplus Hinge Formulation**: Instead of the common `logsigmoid` formulation, this loss uses `softplus(margin - logp_diff)`. While mathematically equivalent to `-logsigmoid(logp_diff - margin)`, this form explicitly frames the problem as minimizing a smoothed hinge loss on the violation `margin - logp_diff`, which can be more intuitive. It directly penalizes the model when `logp_diff` falls short of the target `margin`.\n2. **Margin Clamping for Stability**: A `clamp` operation is introduced to the final dynamic margin before it is used in the loss. The margin is clamped to a minimum of zero. This prevents the margin from becoming negative, which could happen if `logp_diff` is very negative and the `alpha` term dominates. A negative margin would incorrectly reward the model for preferring the worse candidate, so clamping it at zero ensures the loss always encourages the correct preference direction.", "hyperparams": {"temperature": 5.0, "max_margin": 3.5, "beta": 1.0, "alpha": 0.1}, "operators_used": ["zscore", "rank_gap", "tanh", "clamp", "softplus"]}}}
{"generation": 7, "index": 4, "ir": {"name": "Confidence-Coupled Z-Scored Rank-Adaptive Loss", "intuition": "This loss function creates a highly adaptive target margin by combining rank-based and distribution-based normalization, and then dynamically couples this margin to the model's own confidence.\n\nInherited Ideas:\n- From Parent 1 (Z-Scored Rank-Adaptive Hinge Loss), it inherits the robust margin creation pipeline: first, costs are normalized using `zscore` to make the loss invariant to the scale and shift of the cost distribution. Then, `rank_gap` is computed on these z-scored costs to get a clean relative preference signal. Finally, `tanh` is used to create a smooth, bounded margin from this signal (`max_margin * tanh(beta * rank_diff)`). This entire sequence provides a stable, adaptive margin based purely on cost information.\n- From Parent 0 (Hybrid Z-Score and Rank-Adaptive Sigmoid Loss), it inherits the core idea of coupling the target margin to the model's confidence. It uses the log-probability difference (`logp_diff`) itself as a component of the final margin, creating a dynamic target that adapts to the model's current state.\n\nNew Coupling Ideas:\n1. **Confidence-Gated Margin**: Instead of simply adding the confidence term to the cost-based margin (as in Parent 0), the new approach uses the model's confidence to gate the cost-based margin. The final margin is `max_margin * tanh(beta * rank_diff) + clamp(logp_diff, min=-1, max=1)`. The `clamp` function acts as a confidence gate: if the model is already correctly confident (large positive `logp_diff`), the margin is increased, pushing for even better separation. If the model is confidently wrong (large negative `logp_diff`), the margin is decreased, making the initial learning step easier. This creates a more nuanced interaction between the cost-derived margin and the model's state.\n2. **Stable Hinge-like Formulation with Softplus**: The final loss is calculated as `softplus(temperature * (dynamic_margin - logp_diff))`. This is mathematically equivalent to the `-logsigmoid(temperature * (logp_diff - dynamic_margin))` structure used by both parents but is often implemented with `softplus` for direct numerical stability and clarity. It implements a smooth hinge loss that penalizes the model when its log-probability difference falls short of the dynamic target margin.", "pseudocode": "1. Given costs for a batch, normalize them using z-scoring: z_cost_a = zscore(cost_a), z_cost_b = zscore(cost_b).\n2. Compute the rank-normalized gap between the z-scored costs to get a robust preference signal. This signal is typically in [-1, 1].\n   rank_diff = rank_gap(z_cost_a, z_cost_b).\n3. Create the base adaptive margin by passing the rank difference through a `tanh` function and scaling it. This creates a smooth, bounded target based on cost.\n   cost_margin = max_margin * tanh(beta * rank_diff).\n4. Compute the log-probability difference: logp_diff = logp_a - logp_b.\n5. Create a confidence-gating term by clamping the log-probability difference. This prevents extreme values from dominating the margin and acts as a stable confidence signal.\n   confidence_gate = clamp(logp_diff, min=-1.0, max=1.0).\n6. Combine the cost-based margin and the confidence gate to form the final dynamic margin. The target separation is now a function of both the ground-truth costs and the model's own confidence.\n   dynamic_margin = cost_margin + confidence_gate.\n7. Calculate the final loss using `softplus`. This penalizes cases where `logp_diff` is smaller than the `dynamic_margin` in a smooth, hinge-like fashion. The `temperature` scales the penalty.\n   loss = softplus(temperature * (dynamic_margin - logp_diff)).", "hyperparams": {"beta": 1.0, "max_margin": 3.0, "temperature": 5.0}, "operators_used": ["zscore", "rank_gap", "tanh", "clamp", "softplus"], "implementation_hint": {"expects": ["cost_a", "cost_b", "logp_a", "logp_b"], "returns": "scalar"}}, "fitness": {"hf_like_score": 3.882485270500183, "validation_objective": 3.882485270500183, "generalization_penalty": 0.0, "generalization_objectives": {"20": 3.8562214374542236}, "train_score_mean": 4.207360565662384, "train_loss_mean": 0.6784335691481829, "pair_count": 4110933, "config": {"hf": {"problem": "tsp", "hf_steps": 32, "train_problem_size": 20, "valid_problem_sizes": [20], "train_batch_size": 64, "pomo_size": 64, "learning_rate": 0.0003, "weight_decay": 1e-06, "alpha": 0.05, "device": "cuda", "seed": 1234, "num_validation_episodes": 128, "validation_batch_size": 64, "generalization_penalty_weight": 1.0, "pool_version": "v0"}, "free_loss": {"f1_steps": 32, "f2_steps": 100, "f3_enabled": false}}, "loss_ir": {"name": "Confidence-Coupled Z-Scored Rank-Adaptive Loss", "intuition": "This loss function creates a highly adaptive target margin by combining rank-based and distribution-based normalization, and then dynamically couples this margin to the model's own confidence.\n\nInherited Ideas:\n- From Parent 1 (Z-Scored Rank-Adaptive Hinge Loss), it inherits the robust margin creation pipeline: first, costs are normalized using `zscore` to make the loss invariant to the scale and shift of the cost distribution. Then, `rank_gap` is computed on these z-scored costs to get a clean relative preference signal. Finally, `tanh` is used to create a smooth, bounded margin from this signal (`max_margin * tanh(beta * rank_diff)`). This entire sequence provides a stable, adaptive margin based purely on cost information.\n- From Parent 0 (Hybrid Z-Score and Rank-Adaptive Sigmoid Loss), it inherits the core idea of coupling the target margin to the model's confidence. It uses the log-probability difference (`logp_diff`) itself as a component of the final margin, creating a dynamic target that adapts to the model's current state.\n\nNew Coupling Ideas:\n1. **Confidence-Gated Margin**: Instead of simply adding the confidence term to the cost-based margin (as in Parent 0), the new approach uses the model's confidence to gate the cost-based margin. The final margin is `max_margin * tanh(beta * rank_diff) + clamp(logp_diff, min=-1, max=1)`. The `clamp` function acts as a confidence gate: if the model is already correctly confident (large positive `logp_diff`), the margin is increased, pushing for even better separation. If the model is confidently wrong (large negative `logp_diff`), the margin is decreased, making the initial learning step easier. This creates a more nuanced interaction between the cost-derived margin and the model's state.\n2. **Stable Hinge-like Formulation with Softplus**: The final loss is calculated as `softplus(temperature * (dynamic_margin - logp_diff))`. This is mathematically equivalent to the `-logsigmoid(temperature * (logp_diff - dynamic_margin))` structure used by both parents but is often implemented with `softplus` for direct numerical stability and clarity. It implements a smooth hinge loss that penalizes the model when its log-probability difference falls short of the dynamic target margin.", "hyperparams": {"beta": 1.0, "max_margin": 3.0, "temperature": 5.0}, "operators_used": ["zscore", "rank_gap", "tanh", "clamp", "softplus"]}}}
{"generation": 7, "index": 5, "ir": {"name": "Softplus-Clipped Dynamic Margin Loss with Rank-Gap", "intuition": "This loss function constructs a dynamic and robust preference margin by synthesizing ideas from both parents, then introduces a novel clipping mechanism for enhanced stability and control.\n\nInherited Ideas:\n- From Parent 0, it inherits the idea of creating a dynamic margin that is coupled to the model's own confidence. The target margin includes a term `alpha * logp_diff`, which means the loss pushes for a larger separation when the model is already confident, and a smaller one when it is uncertain.\n- From Parent 1, it inherits the use of `tanh` to create a smooth, bounded, and saturating margin from a cost-derived signal (`rank_gap(cost_a, cost_b)`). This ensures the margin is robust to cost outliers and stays within a predictable range.\n\nNew Coupling Ideas:\n1. **Softplus Clipping of the Margin**: Instead of letting the dynamic margin grow unbounded (due to the `alpha * logp_diff` term), the entire margin is passed through a `softplus` function. This acts as a smooth `ReLU`, ensuring the target margin is always non-negative. It's then clipped at `max_margin`. This coupling ensures that even if the model becomes pathologically overconfident (large `logp_diff`), the target margin remains bounded and stable, preventing gradient explosion while preserving the dynamic nature for typical `logp_diff` values.\n2. **Log-Probability Difference Scaling**: The log-probability difference `(logp_a - logp_b)` is scaled by `sigmoid(logp_a - logp_b)`. This new scaling factor acts as a dynamic temperature. When the model is very certain (large positive `logp_diff`), the sigmoid approaches 1, applying the full loss. When the model is uncertain (`logp_diff` near 0) or wrong (negative `logp_diff`), the sigmoid approaches 0.5 or less, down-weighting the loss for these less reliable predictions and focusing training on more confident errors.", "pseudocode": "1. Compute the rank-normalized gap between the costs to get a robust preference signal.\n   rank_diff = rank_gap(cost_a, cost_b).\n2. Create a base adaptive margin using `tanh`, which is bounded and smooth.\n   base_margin = beta * tanh(rank_diff).\n3. Compute the log-probability difference: logp_diff = logp_a - logp_b.\n4. Construct the dynamic margin by adding a term proportional to the model's confidence to the base margin.\n   dynamic_margin_raw = base_margin + alpha * logp_diff.\n5. Stabilize and bound the margin. First, apply `softplus` to ensure it's non-negative. Then, use `clamp` to enforce an upper bound, preventing runaway targets.\n   stable_margin = clamp(softplus(dynamic_margin_raw), 0, max_margin).\n6. Create a dynamic scaling factor from the log-probability difference using `sigmoid`.\n   dynamic_scale = sigmoid(logp_diff).\n7. Calculate the core loss term, which is the difference between the scaled log-probability gap and the stable margin.\n   core_term = dynamic_scale * logp_diff - stable_margin.\n8. The final loss is the negative logsigmoid of the core term. This is numerically stable and penalizes cases where the scaled `logp_diff` is smaller than the `stable_margin`.\n   loss = -logsigmoid(core_term).", "hyperparams": {"beta": 1.0, "alpha": 0.1, "max_margin": 3.0}, "operators_used": ["rank_gap", "tanh", "softplus", "clamp", "sigmoid", "logsigmoid"], "implementation_hint": {"expects": ["cost_a", "cost_b", "logp_a", "logp_b"], "returns": "scalar"}}, "fitness": {"hf_like_score": 3.882612943649292, "validation_objective": 3.882612943649292, "generalization_penalty": 0.0, "generalization_objectives": {"20": 3.8483688831329346}, "train_score_mean": 4.202120989561081, "train_loss_mean": 0.6673208028078079, "pair_count": 4112669, "config": {"hf": {"problem": "tsp", "hf_steps": 32, "train_problem_size": 20, "valid_problem_sizes": [20], "train_batch_size": 64, "pomo_size": 64, "learning_rate": 0.0003, "weight_decay": 1e-06, "alpha": 0.05, "device": "cuda", "seed": 1234, "num_validation_episodes": 128, "validation_batch_size": 64, "generalization_penalty_weight": 1.0, "pool_version": "v0"}, "free_loss": {"f1_steps": 32, "f2_steps": 100, "f3_enabled": false}}, "loss_ir": {"name": "Softplus-Clipped Dynamic Margin Loss with Rank-Gap", "intuition": "This loss function constructs a dynamic and robust preference margin by synthesizing ideas from both parents, then introduces a novel clipping mechanism for enhanced stability and control.\n\nInherited Ideas:\n- From Parent 0, it inherits the idea of creating a dynamic margin that is coupled to the model's own confidence. The target margin includes a term `alpha * logp_diff`, which means the loss pushes for a larger separation when the model is already confident, and a smaller one when it is uncertain.\n- From Parent 1, it inherits the use of `tanh` to create a smooth, bounded, and saturating margin from a cost-derived signal (`rank_gap(cost_a, cost_b)`). This ensures the margin is robust to cost outliers and stays within a predictable range.\n\nNew Coupling Ideas:\n1. **Softplus Clipping of the Margin**: Instead of letting the dynamic margin grow unbounded (due to the `alpha * logp_diff` term), the entire margin is passed through a `softplus` function. This acts as a smooth `ReLU`, ensuring the target margin is always non-negative. It's then clipped at `max_margin`. This coupling ensures that even if the model becomes pathologically overconfident (large `logp_diff`), the target margin remains bounded and stable, preventing gradient explosion while preserving the dynamic nature for typical `logp_diff` values.\n2. **Log-Probability Difference Scaling**: The log-probability difference `(logp_a - logp_b)` is scaled by `sigmoid(logp_a - logp_b)`. This new scaling factor acts as a dynamic temperature. When the model is very certain (large positive `logp_diff`), the sigmoid approaches 1, applying the full loss. When the model is uncertain (`logp_diff` near 0) or wrong (negative `logp_diff`), the sigmoid approaches 0.5 or less, down-weighting the loss for these less reliable predictions and focusing training on more confident errors.", "hyperparams": {"beta": 1.0, "alpha": 0.1, "max_margin": 3.0}, "operators_used": ["rank_gap", "tanh", "softplus", "clamp", "sigmoid", "logsigmoid"]}}}
{"generation": 7, "index": 6, "ir": {"name": "Confidence-Coupled Tanh-Margin Loss with Z-Score", "intuition": "This loss function creates a robust, adaptive margin that is sensitive to both the ground-truth cost difference and the model's own confidence. The margin is designed to be well-behaved and stable, while the overall loss structure pushes for more decisive preferences when the model is already confident.\n\nInherited Ideas:\n- From Parent 0, it inherits the idea of coupling the margin directly to the model's confidence (`alpha * logp_diff`). This makes the target separation dynamic: if the model is already correctly confident (`logp_diff` is large and positive), the margin increases, pushing for even better separation. This creates a self-reinforcing learning signal.\n- From Parent 1, it inherits the core mechanism of creating a smooth, bounded margin using `tanh` on a normalized cost signal, scaled by `max_margin`. This prevents the margin from growing uncontrollably and ensures stability. It also inherits the pre-normalization of costs using `zscore`, making the loss robust to the absolute scale and distribution of costs in a batch.\n\nNew Coupling Ideas:\n1. **Hybrid Margin Structure**: The final margin is a sum of two components: a cost-based component and a confidence-based component (`cost_margin + confidence_margin`). The cost-based part (`tanh`) provides a stable baseline from the ground-truth preference, while the confidence-based part (`alpha * logp_diff`) adds a dynamic target based on the model's state. This direct summation couples the two signals, allowing the model's confidence to directly augment the ground-truth margin.\n2. **ReLU-Gated Confidence Margin**: The confidence-based part of the margin (`alpha * logp_diff`) is passed through a `relu` function. This ensures that the margin is only increased when the model is already correctly confident (i.e., `logp_a > logp_b`). It prevents the margin from being reduced when the model is incorrectly confident (preferring the worse option), which would otherwise incorrectly lower the learning signal. This gating mechanism stabilizes the confidence-coupling idea from Parent 0, making it more robust.", "pseudocode": "1. Given costs for a batch, normalize them using z-scoring: z_cost_a = zscore(cost_a), z_cost_b = zscore(cost_b).\n2. Compute the difference of the z-scored costs: z_cost_diff = z_cost_a - z_cost_b.\n3. Create the cost-based margin by passing the z-scored cost difference through a `tanh` function and scaling it. This creates a smooth, bounded margin based on the normalized cost preference.\n   cost_margin = max_margin * tanh(beta * z_cost_diff).\n4. Compute the log-probability difference: logp_diff = logp_a - logp_b.\n5. Create the confidence-based margin. This term is proportional to the log-probability difference but is gated by `relu` to ensure it only adds to the margin when the model is correctly confident.\n   confidence_margin = alpha * relu(logp_diff).\n6. Combine the two components to form the final dynamic margin.\n   dynamic_margin = cost_margin + confidence_margin.\n7. Calculate the core loss term, which is the difference between the log-probability gap and the dynamic margin, scaled by temperature.\n   core_term = temperature * (logp_diff - dynamic_margin).\n8. The final loss is the negative logsigmoid of the core term. This is a stable hinge-like loss that penalizes cases where `logp_diff` is smaller than the target `dynamic_margin`.\n   loss = -logsigmoid(core_term).", "hyperparams": {"beta": 1.0, "max_margin": 3.0, "temperature": 5.0, "alpha": 0.2}, "operators_used": ["zscore", "tanh", "relu", "logsigmoid"], "implementation_hint": {"expects": ["cost_a", "cost_b", "logp_a", "logp_b"], "returns": "scalar"}}, "fitness": {"hf_like_score": 3.882133960723877, "validation_objective": 3.882133960723877, "generalization_penalty": 0.0, "generalization_objectives": {"20": 3.8509355783462524}, "train_score_mean": 4.209841266274452, "train_loss_mean": 0.65304359421134, "pair_count": 4117028, "config": {"hf": {"problem": "tsp", "hf_steps": 32, "train_problem_size": 20, "valid_problem_sizes": [20], "train_batch_size": 64, "pomo_size": 64, "learning_rate": 0.0003, "weight_decay": 1e-06, "alpha": 0.05, "device": "cuda", "seed": 1234, "num_validation_episodes": 128, "validation_batch_size": 64, "generalization_penalty_weight": 1.0, "pool_version": "v0"}, "free_loss": {"f1_steps": 32, "f2_steps": 100, "f3_enabled": false}}, "loss_ir": {"name": "Confidence-Coupled Tanh-Margin Loss with Z-Score", "intuition": "This loss function creates a robust, adaptive margin that is sensitive to both the ground-truth cost difference and the model's own confidence. The margin is designed to be well-behaved and stable, while the overall loss structure pushes for more decisive preferences when the model is already confident.\n\nInherited Ideas:\n- From Parent 0, it inherits the idea of coupling the margin directly to the model's confidence (`alpha * logp_diff`). This makes the target separation dynamic: if the model is already correctly confident (`logp_diff` is large and positive), the margin increases, pushing for even better separation. This creates a self-reinforcing learning signal.\n- From Parent 1, it inherits the core mechanism of creating a smooth, bounded margin using `tanh` on a normalized cost signal, scaled by `max_margin`. This prevents the margin from growing uncontrollably and ensures stability. It also inherits the pre-normalization of costs using `zscore`, making the loss robust to the absolute scale and distribution of costs in a batch.\n\nNew Coupling Ideas:\n1. **Hybrid Margin Structure**: The final margin is a sum of two components: a cost-based component and a confidence-based component (`cost_margin + confidence_margin`). The cost-based part (`tanh`) provides a stable baseline from the ground-truth preference, while the confidence-based part (`alpha * logp_diff`) adds a dynamic target based on the model's state. This direct summation couples the two signals, allowing the model's confidence to directly augment the ground-truth margin.\n2. **ReLU-Gated Confidence Margin**: The confidence-based part of the margin (`alpha * logp_diff`) is passed through a `relu` function. This ensures that the margin is only increased when the model is already correctly confident (i.e., `logp_a > logp_b`). It prevents the margin from being reduced when the model is incorrectly confident (preferring the worse option), which would otherwise incorrectly lower the learning signal. This gating mechanism stabilizes the confidence-coupling idea from Parent 0, making it more robust.", "hyperparams": {"beta": 1.0, "max_margin": 3.0, "temperature": 5.0, "alpha": 0.2}, "operators_used": ["zscore", "tanh", "relu", "logsigmoid"]}}}
{"generation": 7, "index": 7, "ir": {"name": "Confidence-Coupled Softplus Hinge with Hybrid Normalization", "intuition": "This loss function combines robust cost normalization with a dynamic margin that adapts to the model's own confidence, all within a stable Softplus hinge-loss framework.\n\nInherited Ideas:\n- From Parent 0, it inherits the idea of coupling the margin directly to the model's confidence. The target margin includes a term `alpha * logp_diff`, making the loss more demanding when the model is already confident and more lenient when it is uncertain.\n- From Parent 1, it inherits the use of `zscore` to normalize costs before further processing. This makes the loss robust to the absolute scale and shift of the cost distribution, focusing on relative preference within the batch's statistical context.\n- From both parents, it inherits the core structure of `logsigmoid(temperature * (logp_diff - margin))`, which is numerically equivalent to `softplus(temperature * (margin - logp_diff))`, providing a smooth and stable hinge-like loss.\n\nNew Coupling Ideas:\n1. **Hybrid Normalization Signal for Margin**: The margin is derived from a signal that is first z-scored (`zscore(cost_diff)`) and then passed through a `tanh` function. Using `tanh` (from Parent 1's margin creation) on the z-scored difference `cost_b - cost_a` creates a bounded, smooth, and outlier-resistant signal of preference strength. This is a more direct way to get a normalized signal compared to using `rank_gap`.\n2. **Dynamic Margin with Confidence and Softplus**: The final margin is a sum of the cost-derived adaptive margin and the confidence-coupled term (`alpha * logp_diff`). The loss is then formulated as `softplus(temperature * (dynamic_margin - logp_diff))`. This structure creates a dynamic target: the model is pushed to achieve a separation that is not only proportional to the normalized cost difference but also exceeds its current confidence level, encouraging continuous improvement.", "pseudocode": "1. For a batch, normalize the costs using z-scoring: z_cost_a = zscore(cost_a), z_cost_b = zscore(cost_b).\n2. Compute the difference of the z-scored costs. This represents the preference strength, centered and scaled by the batch statistics.\n   z_cost_diff = z_cost_b - z_cost_a.\n3. Create the adaptive margin component by passing the z-scored difference through a `tanh` function and scaling it. This produces a smooth, bounded margin between -max_margin and +max_margin.\n   adaptive_margin = max_margin * tanh(beta * z_cost_diff).\n4. Compute the log-probability difference: logp_diff = logp_a - logp_b.\n5. Create the final dynamic margin by adding a term proportional to the log-probability difference. This couples the target separation to the model's current confidence.\n   dynamic_margin = adaptive_margin + alpha * logp_diff.\n6. Calculate the difference between the dynamic margin and the log-probability difference. This is the core term inside the Softplus function.\n   core_term = dynamic_margin - logp_diff.\n7. The final loss is the `softplus` of the core term, scaled by a temperature. This penalizes cases where `logp_diff` is smaller than the `dynamic_margin`.\n   loss = softplus(temperature * core_term).", "hyperparams": {"temperature": 5.0, "max_margin": 3.0, "beta": 1.0, "alpha": 0.1}, "operators_used": ["zscore", "tanh", "softplus"], "implementation_hint": {"expects": ["cost_a", "cost_b", "logp_a", "logp_b"], "returns": "scalar"}}, "fitness": {"hf_like_score": 3.881010055541992, "validation_objective": 3.881010055541992, "generalization_penalty": 0.0, "generalization_objectives": {"20": 3.8455970287323}, "train_score_mean": 4.204008683562279, "train_loss_mean": 0.6672188490629196, "pair_count": 4112701, "config": {"hf": {"problem": "tsp", "hf_steps": 32, "train_problem_size": 20, "valid_problem_sizes": [20], "train_batch_size": 64, "pomo_size": 64, "learning_rate": 0.0003, "weight_decay": 1e-06, "alpha": 0.05, "device": "cuda", "seed": 1234, "num_validation_episodes": 128, "validation_batch_size": 64, "generalization_penalty_weight": 1.0, "pool_version": "v0"}, "free_loss": {"f1_steps": 32, "f2_steps": 100, "f3_enabled": false}}, "loss_ir": {"name": "Confidence-Coupled Softplus Hinge with Hybrid Normalization", "intuition": "This loss function combines robust cost normalization with a dynamic margin that adapts to the model's own confidence, all within a stable Softplus hinge-loss framework.\n\nInherited Ideas:\n- From Parent 0, it inherits the idea of coupling the margin directly to the model's confidence. The target margin includes a term `alpha * logp_diff`, making the loss more demanding when the model is already confident and more lenient when it is uncertain.\n- From Parent 1, it inherits the use of `zscore` to normalize costs before further processing. This makes the loss robust to the absolute scale and shift of the cost distribution, focusing on relative preference within the batch's statistical context.\n- From both parents, it inherits the core structure of `logsigmoid(temperature * (logp_diff - margin))`, which is numerically equivalent to `softplus(temperature * (margin - logp_diff))`, providing a smooth and stable hinge-like loss.\n\nNew Coupling Ideas:\n1. **Hybrid Normalization Signal for Margin**: The margin is derived from a signal that is first z-scored (`zscore(cost_diff)`) and then passed through a `tanh` function. Using `tanh` (from Parent 1's margin creation) on the z-scored difference `cost_b - cost_a` creates a bounded, smooth, and outlier-resistant signal of preference strength. This is a more direct way to get a normalized signal compared to using `rank_gap`.\n2. **Dynamic Margin with Confidence and Softplus**: The final margin is a sum of the cost-derived adaptive margin and the confidence-coupled term (`alpha * logp_diff`). The loss is then formulated as `softplus(temperature * (dynamic_margin - logp_diff))`. This structure creates a dynamic target: the model is pushed to achieve a separation that is not only proportional to the normalized cost difference but also exceeds its current confidence level, encouraging continuous improvement.", "hyperparams": {"temperature": 5.0, "max_margin": 3.0, "beta": 1.0, "alpha": 0.1}, "operators_used": ["zscore", "tanh", "softplus"]}}}
{"generation": 8, "index": 0, "ir": {"name": "Confidence-Weighted Z-Rank Hinge Loss", "intuition": "This loss function combines robust normalization techniques with a confidence-aware margin to create a stable and adaptive training objective. The core idea is to establish a target separation (margin) between the log-probabilities of preferred and dispreferred candidates that adapts to both the magnitude of the cost difference and the model's current confidence.\n\nInherited Ideas:\n- From both Parent 0 and Parent 1, it inherits the idea of using a two-stage normalization on the costs: first applying `zscore` to the batch of costs to make the signal robust to scale and shift, and then using `rank_gap` on these z-scored costs to focus on relative ordering. This creates a highly stable `rank_diff` signal.\n- From Parent 1, it inherits the use of `tanh` to create a smooth, bounded, and saturating margin from the normalized cost difference. This prevents the margin from growing uncontrollably and is scaled by `max_margin`.\n\nNew Coupling Ideas:\n1. **Confidence-Based Margin Attenuation**: The adaptive margin, derived from the cost difference, is attenuated by a confidence weight. This weight is calculated as `sigmoid(-abs(logp_diff))`. When the model is very confident (large positive or negative `logp_diff`), this weight approaches 0, effectively reducing the margin target. When the model is uncertain (`logp_diff` near 0), the weight approaches 1, applying the full margin. This coupling focuses the loss on correcting uncertain predictions, rather than excessively pushing on already confident ones, which can improve stability and prevent overfitting.\n2. **Log-Probability Scaling**: The entire loss term, `(margin - logp_diff)`, is passed through a `softplus` function. This is equivalent to the `-logsigmoid(logp_diff - margin)` structure seen in the parents but is expressed more directly as a smooth hinge loss. The key modification is that this hinge loss is then scaled by `exp(logp_diff)`. This scaling term, which is simply `p_a / p_b`, acts as an importance weight. It increases the loss when the model incorrectly assigns a higher probability to the worse candidate (i.e., `logp_diff` is negative), thereby prioritizing the correction of severe errors.", "pseudocode": "1. Given costs for a batch, normalize them using z-scoring: z_cost_a = zscore(cost_a), z_cost_b = zscore(cost_b).\n2. Compute the rank-normalized gap between the z-scored costs to get a robust preference signal: rank_diff = rank_gap(z_cost_a, z_cost_b).\n3. Create a base adaptive margin by passing the rank difference through a `tanh` function and scaling it. This creates a smooth, bounded target separation.\n   base_margin = max_margin * tanh(beta * rank_diff).\n4. Compute the log-probability difference: logp_diff = logp_a - logp_b.\n5. Calculate a confidence weight that is high when the model is uncertain and low when it is confident. This weight will attenuate the margin.\n   confidence_weight = sigmoid(-abs(logp_diff)).\n6. Couple the base margin with the confidence weight to get the final dynamic margin. This focuses the loss on uncertain pairs.\n   dynamic_margin = base_margin * confidence_weight.\n7. Calculate a smooth hinge loss term. This term is positive only when the log-probability difference is less than the dynamic margin.\n   hinge_term = softplus(dynamic_margin - logp_diff).\n8. Calculate a scaling factor based on the model's probability ratio. This up-weights the loss for severe mispredictions.\n   importance_weight = exp(logp_diff).\n9. The final loss is the hinge term multiplied by the importance weight. This couples the magnitude of the error with its severity.\n   loss = hinge_term * importance_weight.", "hyperparams": {"beta": 1.5, "max_margin": 3.0}, "operators_used": ["zscore", "rank_gap", "tanh", "sigmoid", "softplus", "exp"], "implementation_hint": {"expects": ["cost_a", "cost_b", "logp_a", "logp_b"], "returns": "scalar"}}, "fitness": {"hf_like_score": 3.8811153173446655, "validation_objective": 3.8811153173446655, "generalization_penalty": 0.0, "generalization_objectives": {"20": 3.8505876064300537}, "train_score_mean": 4.2099238485097885, "train_loss_mean": 0.6780871134251356, "pair_count": 4111620, "config": {"hf": {"problem": "tsp", "hf_steps": 32, "train_problem_size": 20, "valid_problem_sizes": [20], "train_batch_size": 64, "pomo_size": 64, "learning_rate": 0.0003, "weight_decay": 1e-06, "alpha": 0.05, "device": "cuda", "seed": 1234, "num_validation_episodes": 128, "validation_batch_size": 64, "generalization_penalty_weight": 1.0, "pool_version": "v0"}, "free_loss": {"f1_steps": 32, "f2_steps": 100, "f3_enabled": false}}, "loss_ir": {"name": "Confidence-Weighted Z-Rank Hinge Loss", "intuition": "This loss function combines robust normalization techniques with a confidence-aware margin to create a stable and adaptive training objective. The core idea is to establish a target separation (margin) between the log-probabilities of preferred and dispreferred candidates that adapts to both the magnitude of the cost difference and the model's current confidence.\n\nInherited Ideas:\n- From both Parent 0 and Parent 1, it inherits the idea of using a two-stage normalization on the costs: first applying `zscore` to the batch of costs to make the signal robust to scale and shift, and then using `rank_gap` on these z-scored costs to focus on relative ordering. This creates a highly stable `rank_diff` signal.\n- From Parent 1, it inherits the use of `tanh` to create a smooth, bounded, and saturating margin from the normalized cost difference. This prevents the margin from growing uncontrollably and is scaled by `max_margin`.\n\nNew Coupling Ideas:\n1. **Confidence-Based Margin Attenuation**: The adaptive margin, derived from the cost difference, is attenuated by a confidence weight. This weight is calculated as `sigmoid(-abs(logp_diff))`. When the model is very confident (large positive or negative `logp_diff`), this weight approaches 0, effectively reducing the margin target. When the model is uncertain (`logp_diff` near 0), the weight approaches 1, applying the full margin. This coupling focuses the loss on correcting uncertain predictions, rather than excessively pushing on already confident ones, which can improve stability and prevent overfitting.\n2. **Log-Probability Scaling**: The entire loss term, `(margin - logp_diff)`, is passed through a `softplus` function. This is equivalent to the `-logsigmoid(logp_diff - margin)` structure seen in the parents but is expressed more directly as a smooth hinge loss. The key modification is that this hinge loss is then scaled by `exp(logp_diff)`. This scaling term, which is simply `p_a / p_b`, acts as an importance weight. It increases the loss when the model incorrectly assigns a higher probability to the worse candidate (i.e., `logp_diff` is negative), thereby prioritizing the correction of severe errors.", "hyperparams": {"beta": 1.5, "max_margin": 3.0}, "operators_used": ["zscore", "rank_gap", "tanh", "sigmoid", "softplus", "exp"]}}}
{"generation": 8, "index": 1, "ir": {"name": "Softplus-Coupled Z-Rank Hinge Loss", "intuition": "This loss function creates a robust, adaptive margin by combining z-score and rank-gap normalization, and then couples this margin to the model's own confidence using a softplus transformation for enhanced stability and dynamic adaptation.\n\nInherited Ideas:\n- From Parent 0 and Parent 1, it inherits the idea of pre-normalizing costs using a z-score transformation (`zscore(costs)`). This centers the cost distribution and makes the margin calculation less sensitive to the absolute scale of costs.\n- From Parent 0 and Parent 1, it also inherits the use of `rank_gap` on these z-scored costs to create a highly robust, doubly-normalized preference signal `rank_diff`.\n- From both parents, it uses the core structure of a hinge-like loss: `-logsigmoid(temperature * (logp_diff - margin))`, which is numerically stable and penalizes the model when the log-probability difference falls short of the target margin.\n\nNew Coupling Ideas:\n1. **Softplus-Coupled Margin**: The final margin is not just the adaptive margin derived from costs, but is dynamically coupled with the model's log-probability difference. Specifically, `final_margin = softplus(alpha * adaptive_margin - logp_diff) + adaptive_margin`. This introduces two key behaviors: (a) When the model is already correct and confident (`logp_diff > alpha * adaptive_margin`), the `softplus` term becomes very small, and the target margin is approximately `adaptive_margin`. This prevents the loss from pushing for an unnecessarily large margin. (b) When the model is wrong or uncertain (`logp_diff` is small or negative), the `softplus` term grows, effectively increasing the target margin (`final_margin > adaptive_margin`). This adaptively increases the penalty for incorrect predictions, pushing the model harder where it struggles most.\n2. **Tanh Saturation for Stability**: The adaptive margin component is created using `tanh(beta * rank_diff)`, as seen in Parent 1. This ensures the cost-derived margin is smoothly bounded, preventing extreme cost differences from creating an unstable or exploding target margin before it is fed into the softplus coupling mechanism.", "pseudocode": "1. Given costs for a batch, normalize them using z-scoring: z_cost_a = zscore(cost_a), z_cost_b = zscore(cost_b).\n2. Compute the rank-normalized gap between the z-scored costs to get a robust preference signal, typically in [-1, 1].\n   rank_diff = rank_gap(z_cost_a, z_cost_b).\n3. Create the base adaptive margin by passing the rank difference through a `tanh` function and scaling it. This creates a smooth, bounded signal based on cost preference.\n   adaptive_margin = max_margin * tanh(beta * rank_diff).\n4. Compute the log-probability difference: logp_diff = logp_a - logp_b.\n5. Compute the final, dynamically coupled margin. The `softplus` term adjusts the target based on the model's current performance relative to the cost-based margin.\n   final_margin = softplus(alpha * adaptive_margin - logp_diff) + adaptive_margin.\n6. Calculate the core loss term, which represents the model's log-probability difference versus the dynamically coupled margin, scaled by temperature.\n   core_term = temperature * (logp_diff - final_margin).\n7. The final loss is the negative logsigmoid of the core term. This is equivalent to `softplus(-core_term)` and provides a stable, hinge-like penalty.\n   loss = -logsigmoid(core_term).", "hyperparams": {"temperature": 5.0, "max_margin": 3.0, "beta": 1.0, "alpha": 0.5}, "operators_used": ["zscore", "rank_gap", "tanh", "softplus", "logsigmoid"], "implementation_hint": {"expects": ["cost_a", "cost_b", "logp_a", "logp_b"], "returns": "scalar"}}, "fitness": {"hf_like_score": 3.909064769744873, "validation_objective": 3.909064769744873, "generalization_penalty": 0.0, "generalization_objectives": {"20": 3.871715545654297}, "train_score_mean": 4.293611370027065, "train_loss_mean": 0.6586237736046314, "pair_count": 4127580, "config": {"hf": {"problem": "tsp", "hf_steps": 32, "train_problem_size": 20, "valid_problem_sizes": [20], "train_batch_size": 64, "pomo_size": 64, "learning_rate": 0.0003, "weight_decay": 1e-06, "alpha": 0.05, "device": "cuda", "seed": 1234, "num_validation_episodes": 128, "validation_batch_size": 64, "generalization_penalty_weight": 1.0, "pool_version": "v0"}, "free_loss": {"f1_steps": 32, "f2_steps": 100, "f3_enabled": false}}, "loss_ir": {"name": "Softplus-Coupled Z-Rank Hinge Loss", "intuition": "This loss function creates a robust, adaptive margin by combining z-score and rank-gap normalization, and then couples this margin to the model's own confidence using a softplus transformation for enhanced stability and dynamic adaptation.\n\nInherited Ideas:\n- From Parent 0 and Parent 1, it inherits the idea of pre-normalizing costs using a z-score transformation (`zscore(costs)`). This centers the cost distribution and makes the margin calculation less sensitive to the absolute scale of costs.\n- From Parent 0 and Parent 1, it also inherits the use of `rank_gap` on these z-scored costs to create a highly robust, doubly-normalized preference signal `rank_diff`.\n- From both parents, it uses the core structure of a hinge-like loss: `-logsigmoid(temperature * (logp_diff - margin))`, which is numerically stable and penalizes the model when the log-probability difference falls short of the target margin.\n\nNew Coupling Ideas:\n1. **Softplus-Coupled Margin**: The final margin is not just the adaptive margin derived from costs, but is dynamically coupled with the model's log-probability difference. Specifically, `final_margin = softplus(alpha * adaptive_margin - logp_diff) + adaptive_margin`. This introduces two key behaviors: (a) When the model is already correct and confident (`logp_diff > alpha * adaptive_margin`), the `softplus` term becomes very small, and the target margin is approximately `adaptive_margin`. This prevents the loss from pushing for an unnecessarily large margin. (b) When the model is wrong or uncertain (`logp_diff` is small or negative), the `softplus` term grows, effectively increasing the target margin (`final_margin > adaptive_margin`). This adaptively increases the penalty for incorrect predictions, pushing the model harder where it struggles most.\n2. **Tanh Saturation for Stability**: The adaptive margin component is created using `tanh(beta * rank_diff)`, as seen in Parent 1. This ensures the cost-derived margin is smoothly bounded, preventing extreme cost differences from creating an unstable or exploding target margin before it is fed into the softplus coupling mechanism.", "hyperparams": {"temperature": 5.0, "max_margin": 3.0, "beta": 1.0, "alpha": 0.5}, "operators_used": ["zscore", "rank_gap", "tanh", "softplus", "logsigmoid"]}}}
{"generation": 8, "index": 2, "ir": {"name": "Confidence-Coupled Softplus-Hinge Loss with Z-Score Normalization", "intuition": "This loss function constructs a highly adaptive margin by combining z-score normalization with a confidence-aware coupling mechanism, all framed within a numerically stable softplus hinge loss structure.\n\nInherited Ideas:\n- From Parent 0 and Parent 1, it inherits the practice of pre-normalizing costs using a z-score transformation (`zscore(costs)`). This centers the cost distribution and makes the margin less sensitive to the absolute scale of costs, focusing on the batch's statistical properties.\n- From Parent 1, it inherits the core loss structure of `-logsigmoid(temperature * (logp_diff - margin))`, which is numerically equivalent to `softplus(temperature * (margin - logp_diff))`. This provides a smooth, one-sided hinge-like penalty.\n\nNew Coupling Ideas:\n1. **Direct Z-Score Margin**: Instead of using an intermediate `rank_gap`, the margin is computed directly from the difference of the z-scored costs (`z_cost_b - z_cost_a`). This signal represents how many standard deviations better `cost_a` is than `cost_b`. This creates a more granular margin that reflects the magnitude of the normalized cost difference, not just the rank.\n2. **Confidence-Coupled Margin Scaling**: The z-score difference is scaled by a factor `exp(alpha * logp_diff)`. This couples the margin directly to the model's current confidence (`logp_diff`). When the model is already confident (`logp_diff` is large and positive), the margin target is increased, pushing for even greater separation. Conversely, if the model is unconfident or wrong (`logp_diff` is small or negative), the `exp` term approaches 1 or less, reducing the target margin. This prevents the model from being overly penalized when it is far from the correct preference, allowing for a more gradual learning trajectory.", "pseudocode": "1. Given costs for an entire batch, normalize them using z-scoring: z_cost_a = zscore(cost_a), z_cost_b = zscore(cost_b).\n2. Compute the difference of the z-scored costs. This represents the preference strength in units of standard deviations.\n   z_diff = z_cost_b - z_cost_a.\n3. Compute the log-probability difference: logp_diff = logp_a - logp_b.\n4. Create the final dynamic margin. The z-score difference is scaled by a term that grows with the model's confidence in the correct preference.\n   dynamic_margin = beta * z_diff * exp(alpha * logp_diff).\n5. Calculate the core hinge term. This is the difference between the target margin and the achieved log-probability gap, scaled by a temperature parameter.\n   hinge_term = temperature * (dynamic_margin - logp_diff).\n6. The final loss is the `softplus` of the hinge term. This is equivalent to `-logsigmoid(-hinge_term)` and provides a smooth, numerically stable penalty that is only applied when the log-probability gap is smaller than the dynamic margin.\n   loss = softplus(hinge_term).", "hyperparams": {"temperature": 1.0, "beta": 1.0, "alpha": 0.1}, "operators_used": ["zscore", "exp", "softplus"], "implementation_hint": {"expects": ["cost_a", "cost_b", "logp_a", "logp_b"], "returns": "scalar"}}, "fitness": {"hf_like_score": 3.879812479019165, "validation_objective": 3.879812479019165, "generalization_penalty": 0.0, "generalization_objectives": {"20": 3.856801986694336}, "train_score_mean": 4.203946471214294, "train_loss_mean": 0.6678995788097382, "pair_count": 4112909, "config": {"hf": {"problem": "tsp", "hf_steps": 32, "train_problem_size": 20, "valid_problem_sizes": [20], "train_batch_size": 64, "pomo_size": 64, "learning_rate": 0.0003, "weight_decay": 1e-06, "alpha": 0.05, "device": "cuda", "seed": 1234, "num_validation_episodes": 128, "validation_batch_size": 64, "generalization_penalty_weight": 1.0, "pool_version": "v0"}, "free_loss": {"f1_steps": 32, "f2_steps": 100, "f3_enabled": false}}, "loss_ir": {"name": "Confidence-Coupled Softplus-Hinge Loss with Z-Score Normalization", "intuition": "This loss function constructs a highly adaptive margin by combining z-score normalization with a confidence-aware coupling mechanism, all framed within a numerically stable softplus hinge loss structure.\n\nInherited Ideas:\n- From Parent 0 and Parent 1, it inherits the practice of pre-normalizing costs using a z-score transformation (`zscore(costs)`). This centers the cost distribution and makes the margin less sensitive to the absolute scale of costs, focusing on the batch's statistical properties.\n- From Parent 1, it inherits the core loss structure of `-logsigmoid(temperature * (logp_diff - margin))`, which is numerically equivalent to `softplus(temperature * (margin - logp_diff))`. This provides a smooth, one-sided hinge-like penalty.\n\nNew Coupling Ideas:\n1. **Direct Z-Score Margin**: Instead of using an intermediate `rank_gap`, the margin is computed directly from the difference of the z-scored costs (`z_cost_b - z_cost_a`). This signal represents how many standard deviations better `cost_a` is than `cost_b`. This creates a more granular margin that reflects the magnitude of the normalized cost difference, not just the rank.\n2. **Confidence-Coupled Margin Scaling**: The z-score difference is scaled by a factor `exp(alpha * logp_diff)`. This couples the margin directly to the model's current confidence (`logp_diff`). When the model is already confident (`logp_diff` is large and positive), the margin target is increased, pushing for even greater separation. Conversely, if the model is unconfident or wrong (`logp_diff` is small or negative), the `exp` term approaches 1 or less, reducing the target margin. This prevents the model from being overly penalized when it is far from the correct preference, allowing for a more gradual learning trajectory.", "hyperparams": {"temperature": 1.0, "beta": 1.0, "alpha": 0.1}, "operators_used": ["zscore", "exp", "softplus"]}}}
{"generation": 8, "index": 3, "ir": {"name": "Z-Scored Rank-Modulated Hinge Loss with Confidence Scaling", "intuition": "This loss function creates a highly adaptive margin by combining z-score and rank normalization, and then dynamically scales this margin based on the model's own confidence in its preference.\n\nInherited Ideas:\n- From both Parent 0 and Parent 1, it inherits the idea of pre-normalizing costs using a z-score transformation (`zscore(costs)`). This centers the cost distribution and makes the subsequent margin calculation less sensitive to the absolute scale of costs.\n- From both parents, it inherits the use of `rank_gap` on these normalized costs to create a robust signal of relative preference, making the margin sensitive to the rank ordering within the batch.\n- From Parent 1, it inherits the use of `tanh` to create a smooth, bounded, and saturating margin from the normalized cost signal, which enhances numerical stability.\n- From both parents, it inherits the core `logsigmoid` loss structure: `-logsigmoid(temperature * (logp_diff - margin))`, which provides a stable, hinge-like penalty.\n\nNew Coupling Ideas:\n1. **Confidence-Scaled Margin**: The core adaptive margin, derived from the z-scored rank gap, is multiplicatively scaled by a term based on the model's own log-probability difference. The scaling factor is `softplus(logp_diff)`. This couples the target margin to the model's confidence: when the model is already confident (large positive `logp_diff`), the target margin increases, pushing for even greater separation. When the model is unconfident or wrong (negative `logp_diff`), the `softplus` function smoothly reduces the scaling factor towards zero, effectively shrinking the margin and focusing the model on just achieving the correct preference direction rather than a large, difficult separation. This prevents the model from being overly penalized on hard examples.\n2. **Dynamic Margin Baseline**: A small, constant baseline margin `min_margin` is added. This ensures that even when the model is extremely unconfident (`logp_diff` is very negative), there is still a minimal target separation to aim for, preventing the total margin from collapsing completely to zero.", "pseudocode": "1. Given costs for a batch, normalize them using z-scoring: z_cost_a = zscore(cost_a), z_cost_b = zscore(cost_b).\n2. Compute the rank-normalized gap between the z-scored costs to get a robust preference signal.\n   rank_diff = rank_gap(z_cost_a, z_cost_b).\n3. Create a base adaptive margin by passing the rank difference through a `tanh` function and scaling it. This creates a smooth, bounded signal based on cost preference.\n   base_margin = max_margin * tanh(beta * rank_diff).\n4. Compute the log-probability difference: logp_diff = logp_a - logp_b.\n5. Compute a confidence scaling factor using the `softplus` function on the log-probability difference. This factor smoothly increases as the model becomes more confident.\n   confidence_scale = softplus(logp_diff).\n6. Couple the base margin with the model's confidence by multiplying them. Add a small baseline margin `min_margin` for stability.\n   dynamic_margin = min_margin + (confidence_scale * base_margin).\n7. Calculate the core loss term, which is the difference between the log-probability gap and the dynamic margin, scaled by temperature.\n   core_term = temperature * (logp_diff - dynamic_margin).\n8. The final loss is the negative logsigmoid of the core term, which is numerically stable and penalizes cases where `logp_diff` is smaller than the `dynamic_margin`.\n   loss = -logsigmoid(core_term).", "hyperparams": {"beta": 1.0, "max_margin": 3.0, "temperature": 5.0, "min_margin": 0.05}, "operators_used": ["zscore", "rank_gap", "tanh", "softplus", "logsigmoid"], "implementation_hint": {"expects": ["cost_a", "cost_b", "logp_a", "logp_b"], "returns": "scalar"}}, "fitness": {"hf_like_score": 3.8834320306777954, "validation_objective": 3.8834320306777954, "generalization_penalty": 0.0, "generalization_objectives": {"20": 3.8488287925720215}, "train_score_mean": 4.209529832005501, "train_loss_mean": 0.6783079653978348, "pair_count": 4112128, "config": {"hf": {"problem": "tsp", "hf_steps": 32, "train_problem_size": 20, "valid_problem_sizes": [20], "train_batch_size": 64, "pomo_size": 64, "learning_rate": 0.0003, "weight_decay": 1e-06, "alpha": 0.05, "device": "cuda", "seed": 1234, "num_validation_episodes": 128, "validation_batch_size": 64, "generalization_penalty_weight": 1.0, "pool_version": "v0"}, "free_loss": {"f1_steps": 32, "f2_steps": 100, "f3_enabled": false}}, "loss_ir": {"name": "Z-Scored Rank-Modulated Hinge Loss with Confidence Scaling", "intuition": "This loss function creates a highly adaptive margin by combining z-score and rank normalization, and then dynamically scales this margin based on the model's own confidence in its preference.\n\nInherited Ideas:\n- From both Parent 0 and Parent 1, it inherits the idea of pre-normalizing costs using a z-score transformation (`zscore(costs)`). This centers the cost distribution and makes the subsequent margin calculation less sensitive to the absolute scale of costs.\n- From both parents, it inherits the use of `rank_gap` on these normalized costs to create a robust signal of relative preference, making the margin sensitive to the rank ordering within the batch.\n- From Parent 1, it inherits the use of `tanh` to create a smooth, bounded, and saturating margin from the normalized cost signal, which enhances numerical stability.\n- From both parents, it inherits the core `logsigmoid` loss structure: `-logsigmoid(temperature * (logp_diff - margin))`, which provides a stable, hinge-like penalty.\n\nNew Coupling Ideas:\n1. **Confidence-Scaled Margin**: The core adaptive margin, derived from the z-scored rank gap, is multiplicatively scaled by a term based on the model's own log-probability difference. The scaling factor is `softplus(logp_diff)`. This couples the target margin to the model's confidence: when the model is already confident (large positive `logp_diff`), the target margin increases, pushing for even greater separation. When the model is unconfident or wrong (negative `logp_diff`), the `softplus` function smoothly reduces the scaling factor towards zero, effectively shrinking the margin and focusing the model on just achieving the correct preference direction rather than a large, difficult separation. This prevents the model from being overly penalized on hard examples.\n2. **Dynamic Margin Baseline**: A small, constant baseline margin `min_margin` is added. This ensures that even when the model is extremely unconfident (`logp_diff` is very negative), there is still a minimal target separation to aim for, preventing the total margin from collapsing completely to zero.", "hyperparams": {"beta": 1.0, "max_margin": 3.0, "temperature": 5.0, "min_margin": 0.05}, "operators_used": ["zscore", "rank_gap", "tanh", "softplus", "logsigmoid"]}}}
{"generation": 8, "index": 4, "ir": {"name": "Confidence-Weighted Rank-Gap Loss with Softplus Hinge", "intuition": "This loss function constructs a highly adaptive margin by combining rank-based and distribution-based normalization, and then dynamically weights the loss based on the model's own confidence. The core loss structure is a stable hinge-like penalty.\n\nInherited Ideas:\n- From both parents, it inherits the idea of creating a robust, adaptive margin by first normalizing costs with a z-score (`zscore`) and then computing the `rank_gap` on these normalized costs. This makes the margin signal robust to both the overall statistical distribution and the local rank ordering of costs in a batch.\n- From Parent 1, it inherits the use of `tanh` to create a smooth, bounded margin from the rank-gap signal (`max_margin * tanh(beta * rank_diff)`). This prevents the margin from growing uncontrollably, enhancing numerical stability.\n\nNew Coupling Ideas:\n1. **Softplus Hinge Formulation**: Instead of the `logsigmoid` formulation used by both parents, this loss directly uses `softplus(margin - logp_diff)`. While mathematically equivalent to `-logsigmoid(logp_diff - margin)`, this formulation more explicitly frames the problem as a hinge loss, penalizing `logp_diff` only when it falls below the target `margin`. This enhances conceptual clarity.\n2. **Confidence-Based Loss Weighting**: The hinge loss is multiplied by a weight derived from the model's confidence (`logp_diff`). The weight is `exp(-alpha * relu(logp_diff))`. This means that when the model is already correctly confident (i.e., `logp_diff` is large and positive), the loss for that pair is down-weighted. This coupling focuses training on difficult pairs where the model is uncertain or wrong, preventing it from wasting capacity on pairs it has already mastered. The `relu` ensures that only correct confidence (positive `logp_diff`) leads to down-weighting.", "pseudocode": "1. Given costs for a batch, normalize them using z-scoring: z_cost_a = zscore(cost_a), z_cost_b = zscore(cost_b).\n2. Compute the rank-normalized gap between the z-scored costs. This provides a stable signal of relative preference, typically in [-1, 1].\n   rank_diff = rank_gap(z_cost_a, z_cost_b).\n3. Create an adaptive margin by passing the rank difference through a `tanh` function and scaling it. This creates a smooth, bounded target separation.\n   adaptive_margin = max_margin * tanh(beta * rank_diff).\n4. Compute the log-probability difference: logp_diff = logp_a - logp_b.\n5. Calculate the core hinge loss term using `softplus`. This penalizes cases where `logp_diff` is smaller than the `adaptive_margin`.\n   hinge_term = softplus(adaptive_margin - logp_diff).\n6. Compute a confidence weight. This weight decreases exponentially as the model's correct confidence (`logp_diff > 0`) increases, focusing training on difficult examples.\n   confidence_weight = exp(-alpha * relu(logp_diff)).\n7. The final loss is the hinge term scaled by the temperature and multiplied by the confidence weight.\n   loss = temperature * confidence_weight * hinge_term.", "hyperparams": {"beta": 1.0, "max_margin": 3.0, "temperature": 1.0, "alpha": 0.2}, "operators_used": ["zscore", "rank_gap", "tanh", "softplus", "exp", "relu"], "implementation_hint": {"expects": ["cost_a", "cost_b", "logp_a", "logp_b"], "returns": "scalar"}}, "fitness": {"hf_like_score": 3.8853652477264404, "validation_objective": 3.8853652477264404, "generalization_penalty": 0.0, "generalization_objectives": {"20": 3.854309916496277}, "train_score_mean": 4.210237428545952, "train_loss_mean": 0.6514828335493803, "pair_count": 4117455, "config": {"hf": {"problem": "tsp", "hf_steps": 32, "train_problem_size": 20, "valid_problem_sizes": [20], "train_batch_size": 64, "pomo_size": 64, "learning_rate": 0.0003, "weight_decay": 1e-06, "alpha": 0.05, "device": "cuda", "seed": 1234, "num_validation_episodes": 128, "validation_batch_size": 64, "generalization_penalty_weight": 1.0, "pool_version": "v0"}, "free_loss": {"f1_steps": 32, "f2_steps": 100, "f3_enabled": false}}, "loss_ir": {"name": "Confidence-Weighted Rank-Gap Loss with Softplus Hinge", "intuition": "This loss function constructs a highly adaptive margin by combining rank-based and distribution-based normalization, and then dynamically weights the loss based on the model's own confidence. The core loss structure is a stable hinge-like penalty.\n\nInherited Ideas:\n- From both parents, it inherits the idea of creating a robust, adaptive margin by first normalizing costs with a z-score (`zscore`) and then computing the `rank_gap` on these normalized costs. This makes the margin signal robust to both the overall statistical distribution and the local rank ordering of costs in a batch.\n- From Parent 1, it inherits the use of `tanh` to create a smooth, bounded margin from the rank-gap signal (`max_margin * tanh(beta * rank_diff)`). This prevents the margin from growing uncontrollably, enhancing numerical stability.\n\nNew Coupling Ideas:\n1. **Softplus Hinge Formulation**: Instead of the `logsigmoid` formulation used by both parents, this loss directly uses `softplus(margin - logp_diff)`. While mathematically equivalent to `-logsigmoid(logp_diff - margin)`, this formulation more explicitly frames the problem as a hinge loss, penalizing `logp_diff` only when it falls below the target `margin`. This enhances conceptual clarity.\n2. **Confidence-Based Loss Weighting**: The hinge loss is multiplied by a weight derived from the model's confidence (`logp_diff`). The weight is `exp(-alpha * relu(logp_diff))`. This means that when the model is already correctly confident (i.e., `logp_diff` is large and positive), the loss for that pair is down-weighted. This coupling focuses training on difficult pairs where the model is uncertain or wrong, preventing it from wasting capacity on pairs it has already mastered. The `relu` ensures that only correct confidence (positive `logp_diff`) leads to down-weighting.", "hyperparams": {"beta": 1.0, "max_margin": 3.0, "temperature": 1.0, "alpha": 0.2}, "operators_used": ["zscore", "rank_gap", "tanh", "softplus", "exp", "relu"]}}}
{"generation": 8, "index": 5, "ir": {"name": "Confidence-Coupled Softplus-Hinge Loss with Z-Scored Rank Gap", "intuition": "This loss function constructs a dynamic, two-part margin that adapts to both the ground-truth cost difference and the model's current confidence, then applies it within a numerically stable softplus-hinge framework.\n\nInherited Ideas:\n- From both Parent 0 and Parent 1, it inherits the idea of creating a robust preference signal by first applying a `zscore` transformation to the batch costs and then computing the `rank_gap`. This two-step normalization makes the resulting margin insensitive to the absolute scale of costs and robust to outliers.\n- From Parent 0, it inherits the concept of a dynamic margin that is coupled with the model's own log-probabilities (`logp_diff`). This makes the target separation dependent not only on the ground-truth costs but also on the model's confidence, pushing for greater separation when the model is already confident.\n\nNew Coupling Ideas:\n1. **Tanh-based Adaptive Margin:** While the z-score and rank-gap normalization is inherited, the method of creating the adaptive margin from this signal is taken from Parent 1's use of `tanh`. The `rank_gap` of z-scored costs is passed through `tanh`, creating a smooth, bounded (-1, 1) signal that is then scaled. This provides better gradient flow and saturation properties compared to `sigmoid`.\n2. **Softplus-based Confidence Coupling:** The coupling to `logp_diff` (inspired by Parent 0) is modified for stability. Instead of adding `alpha * logp_diff` directly, which can lead to instability if `logp_diff` is large and negative, we use `alpha * softplus(logp_diff)`. This ensures the confidence-based part of the margin is always non-negative and grows smoothly, preventing the total margin from collapsing or becoming negative due to low model confidence.", "pseudocode": "1. Given costs for a batch, normalize them using z-scoring: z_cost_a = zscore(cost_a), z_cost_b = zscore(cost_b).\n2. Compute the rank-normalized gap between the z-scored costs to get a robust preference signal, typically in [-1, 1].\n   rank_diff = rank_gap(z_cost_a, z_cost_b).\n3. Create the cost-based adaptive margin by passing the rank difference through a `tanh` function and scaling it.\n   cost_margin = max_margin * tanh(beta * rank_diff).\n4. Compute the log-probability difference: logp_diff = logp_a - logp_b.\n5. Create a confidence-based margin component using `softplus` for stability. This term is always non-negative.\n   confidence_margin = alpha * softplus(logp_diff).\n6. Combine the two components to form the final dynamic margin.\n   dynamic_margin = cost_margin + confidence_margin.\n7. Calculate the core loss term, which represents the extent to which the log-probability difference falls short of the dynamic margin, scaled by temperature.\n   core_term = temperature * (logp_diff - dynamic_margin).\n8. The final loss is the negative logsigmoid of the core term, which is equivalent to a softplus hinge loss: `softplus(-core_term)`. This penalizes cases where `logp_diff` is less than `dynamic_margin`.\n   loss = -logsigmoid(core_term).", "hyperparams": {"temperature": 5.0, "max_margin": 3.5, "beta": 1.0, "alpha": 0.2}, "operators_used": ["zscore", "rank_gap", "tanh", "softplus", "logsigmoid"], "implementation_hint": {"expects": ["cost_a", "cost_b", "logp_a", "logp_b"], "returns": "scalar"}}, "fitness": {"hf_like_score": 3.882810950279236, "validation_objective": 3.882810950279236, "generalization_penalty": 0.0, "generalization_objectives": {"20": 3.8484151363372803}, "train_score_mean": 4.210906602442265, "train_loss_mean": 0.6534379441291094, "pair_count": 4116787, "config": {"hf": {"problem": "tsp", "hf_steps": 32, "train_problem_size": 20, "valid_problem_sizes": [20], "train_batch_size": 64, "pomo_size": 64, "learning_rate": 0.0003, "weight_decay": 1e-06, "alpha": 0.05, "device": "cuda", "seed": 1234, "num_validation_episodes": 128, "validation_batch_size": 64, "generalization_penalty_weight": 1.0, "pool_version": "v0"}, "free_loss": {"f1_steps": 32, "f2_steps": 100, "f3_enabled": false}}, "loss_ir": {"name": "Confidence-Coupled Softplus-Hinge Loss with Z-Scored Rank Gap", "intuition": "This loss function constructs a dynamic, two-part margin that adapts to both the ground-truth cost difference and the model's current confidence, then applies it within a numerically stable softplus-hinge framework.\n\nInherited Ideas:\n- From both Parent 0 and Parent 1, it inherits the idea of creating a robust preference signal by first applying a `zscore` transformation to the batch costs and then computing the `rank_gap`. This two-step normalization makes the resulting margin insensitive to the absolute scale of costs and robust to outliers.\n- From Parent 0, it inherits the concept of a dynamic margin that is coupled with the model's own log-probabilities (`logp_diff`). This makes the target separation dependent not only on the ground-truth costs but also on the model's confidence, pushing for greater separation when the model is already confident.\n\nNew Coupling Ideas:\n1. **Tanh-based Adaptive Margin:** While the z-score and rank-gap normalization is inherited, the method of creating the adaptive margin from this signal is taken from Parent 1's use of `tanh`. The `rank_gap` of z-scored costs is passed through `tanh`, creating a smooth, bounded (-1, 1) signal that is then scaled. This provides better gradient flow and saturation properties compared to `sigmoid`.\n2. **Softplus-based Confidence Coupling:** The coupling to `logp_diff` (inspired by Parent 0) is modified for stability. Instead of adding `alpha * logp_diff` directly, which can lead to instability if `logp_diff` is large and negative, we use `alpha * softplus(logp_diff)`. This ensures the confidence-based part of the margin is always non-negative and grows smoothly, preventing the total margin from collapsing or becoming negative due to low model confidence.", "hyperparams": {"temperature": 5.0, "max_margin": 3.5, "beta": 1.0, "alpha": 0.2}, "operators_used": ["zscore", "rank_gap", "tanh", "softplus", "logsigmoid"]}}}
{"generation": 8, "index": 6, "ir": {"name": "Confidence-Coupled Hinge Loss with Hybrid Rank-Z Normalization", "intuition": "This loss function combines robust normalization techniques with a dynamic margin that adapts to the model's own confidence, all within a stable hinge-loss framework.\n\nInherited Ideas:\n- From Parent 0, it inherits the idea of a **dynamic margin** that is coupled to the model's confidence. The target separation is not fixed but includes a term proportional to the log-probability difference (`alpha * logp_diff`). This pushes more confident models to achieve even better separation, creating a self-reinforcing learning signal.\n- From Parent 1, it inherits the core structure of a stable hinge-like loss: `-logsigmoid(temperature * (logp_diff - margin))`, which is equivalent to `softplus(temperature * (margin - logp_diff))`. It also inherits the use of `tanh` to create a smooth, bounded, and saturating margin from the cost difference, which enhances stability.\n\nNew Coupling Ideas:\n1. **Hybrid Normalization Signal**: The margin is derived from a signal that is first z-scored and then rank-normalized (`rank_gap(zscore(costs))`). This double-normalization step, present in both parents, is used here to create a highly robust signal that is insensitive to both the overall cost scale/shift (from `zscore`) and the local density of costs (from `rank_gap`). This isolates a pure relative preference signal.\n2. **ReLU-Gated Margin Scaling**: The cost-derived part of the margin is passed through a `ReLU` function (`relu(tanh(...))`). Since `tanh` can be negative if `cost_a > cost_b`, this gating mechanism ensures that the margin contribution from the cost difference is always non-negative. This simplifies the learning objective by enforcing that the cost difference can only *increase* the target separation, never decrease it, which stabilizes the interaction with the confidence-coupled term.", "pseudocode": "1. For the entire batch of costs, compute their z-score normalized versions: z_cost_a = zscore(cost_a), z_cost_b = zscore(cost_b).\n2. Compute the rank-normalized gap between the z-scored costs to get a robust preference signal: rank_diff = rank_gap(z_cost_a, z_cost_b).\n3. Create a smooth, bounded, and non-negative adaptive margin component. The `tanh` creates a smooth signal, and the `relu` ensures it only contributes positively to the margin target.\n   adaptive_margin_base = max_margin * tanh(beta * rank_diff).\n   adaptive_margin = relu(adaptive_margin_base).\n4. Compute the log-probability difference: logp_diff = logp_a - logp_b.\n5. Construct the final dynamic margin by coupling the adaptive margin with a term proportional to the model's confidence (logp_diff). This creates a self-adjusting target.\n   dynamic_margin = alpha * logp_diff + adaptive_margin.\n6. Calculate the core loss term, which represents how far the model's log-probability difference is from the dynamic target margin.\n   core_term = temperature * (logp_diff - dynamic_margin).\n7. The final loss is the negative logsigmoid of the core term. This implements a smooth hinge loss that penalizes the model when `logp_diff` is less than `dynamic_margin`.\n   loss = -logsigmoid(core_term).", "hyperparams": {"temperature": 5.0, "max_margin": 3.5, "beta": 1.0, "alpha": 0.1}, "operators_used": ["zscore", "rank_gap", "tanh", "relu", "logsigmoid"], "implementation_hint": {"expects": ["cost_a", "cost_b", "logp_a", "logp_b"], "returns": "scalar"}}, "fitness": {"hf_like_score": 3.8816381692886353, "validation_objective": 3.8816381692886353, "generalization_penalty": 0.0, "generalization_objectives": {"20": 3.84884512424469}, "train_score_mean": 4.20636884868145, "train_loss_mean": 0.6670463904738426, "pair_count": 4113544, "config": {"hf": {"problem": "tsp", "hf_steps": 32, "train_problem_size": 20, "valid_problem_sizes": [20], "train_batch_size": 64, "pomo_size": 64, "learning_rate": 0.0003, "weight_decay": 1e-06, "alpha": 0.05, "device": "cuda", "seed": 1234, "num_validation_episodes": 128, "validation_batch_size": 64, "generalization_penalty_weight": 1.0, "pool_version": "v0"}, "free_loss": {"f1_steps": 32, "f2_steps": 100, "f3_enabled": false}}, "loss_ir": {"name": "Confidence-Coupled Hinge Loss with Hybrid Rank-Z Normalization", "intuition": "This loss function combines robust normalization techniques with a dynamic margin that adapts to the model's own confidence, all within a stable hinge-loss framework.\n\nInherited Ideas:\n- From Parent 0, it inherits the idea of a **dynamic margin** that is coupled to the model's confidence. The target separation is not fixed but includes a term proportional to the log-probability difference (`alpha * logp_diff`). This pushes more confident models to achieve even better separation, creating a self-reinforcing learning signal.\n- From Parent 1, it inherits the core structure of a stable hinge-like loss: `-logsigmoid(temperature * (logp_diff - margin))`, which is equivalent to `softplus(temperature * (margin - logp_diff))`. It also inherits the use of `tanh` to create a smooth, bounded, and saturating margin from the cost difference, which enhances stability.\n\nNew Coupling Ideas:\n1. **Hybrid Normalization Signal**: The margin is derived from a signal that is first z-scored and then rank-normalized (`rank_gap(zscore(costs))`). This double-normalization step, present in both parents, is used here to create a highly robust signal that is insensitive to both the overall cost scale/shift (from `zscore`) and the local density of costs (from `rank_gap`). This isolates a pure relative preference signal.\n2. **ReLU-Gated Margin Scaling**: The cost-derived part of the margin is passed through a `ReLU` function (`relu(tanh(...))`). Since `tanh` can be negative if `cost_a > cost_b`, this gating mechanism ensures that the margin contribution from the cost difference is always non-negative. This simplifies the learning objective by enforcing that the cost difference can only *increase* the target separation, never decrease it, which stabilizes the interaction with the confidence-coupled term.", "hyperparams": {"temperature": 5.0, "max_margin": 3.5, "beta": 1.0, "alpha": 0.1}, "operators_used": ["zscore", "rank_gap", "tanh", "relu", "logsigmoid"]}}}
{"generation": 8, "index": 7, "ir": {"name": "Z-Scored Rank-Adaptive Log-Prob Coupled Hinge Loss", "intuition": "This loss function creates a highly adaptive and robust margin by synthesizing rank-based and distribution-based normalization, coupling it with the model's own confidence, and applying it within a stable hinge-loss framework.\n\nInherited Ideas:\n- From Parent 0, it inherits the idea of creating a dynamic margin by adding a term proportional to the log-probability difference (`alpha * logp_diff`). This couples the target separation to the model's current confidence, pushing for greater separation when the model is already confident and relying more on cost-derived information otherwise.\n- From Parent 1, it inherits the use of `tanh` to create a smoothly saturating, bounded margin from a normalized cost signal (`max_margin * tanh(...)`). This enhances numerical stability by preventing the margin from growing uncontrollably.\n- From both parents, it inherits the core structure of pre-normalizing costs using a z-score transformation (`zscore(costs)`) and then computing a `rank_gap` on these normalized costs. This two-step normalization provides a signal that is robust to both the overall statistical distribution and local rank ordering.\n\nNew Coupling Ideas:\n1. **Log-Probabilistic Temperature Scaling**: The temperature parameter, which controls the steepness of the loss, is made dynamic. It is scaled by the sigmoid of the log-probability difference (`sigmoid(logp_diff)`). When the model is uncertain (logp_diff is near zero), the temperature is lower, making the loss gradient gentler. As the model becomes more confident (logp_diff is large), the temperature increases, sharpening the loss and pushing for more precise alignment with the target margin. This couples the loss's sensitivity directly to the model's confidence.\n2. **Margin Clamping for Stability**: The final dynamic margin, which is a sum of the cost-derived `tanh` term and the confidence-derived `logp_diff` term, is clamped to a safe range (`[0, 2 * max_margin]`). This prevents the target margin from becoming excessively large or negative, which could occur if the model's `logp_diff` is extreme. This simple trick guarantees the stability of the margin target before it is used in the final loss calculation.", "pseudocode": "1. Given costs for a batch, normalize them using z-scoring: z_cost_a = zscore(cost_a), z_cost_b = zscore(cost_b).\n2. Compute the rank-normalized gap between the z-scored costs to get a robust preference signal.\n   rank_diff = rank_gap(z_cost_a, z_cost_b).\n3. Create the cost-based adaptive margin by passing `rank_diff` through a `tanh` function and scaling it. This creates a smooth, bounded component.\n   cost_margin = max_margin * tanh(beta * rank_diff).\n4. Compute the log-probability difference: logp_diff = logp_a - logp_b.\n5. Create the final dynamic margin by adding a term proportional to the log-probability difference. Clamp the result to a safe range for stability.\n   dynamic_margin_unclamped = cost_margin + alpha * logp_diff.\n   dynamic_margin = clamp(dynamic_margin_unclamped, 0, 2 * max_margin).\n6. Compute a dynamic temperature that adapts to the model's confidence.\n   dynamic_temperature = temperature * sigmoid(logp_diff).\n7. Calculate the core loss term. This is the difference between the log-probability gap and the dynamic margin, scaled by the dynamic temperature.\n   core_term = dynamic_temperature * (logp_diff - dynamic_margin).\n8. The final loss is the negative logsigmoid of the core term, which is numerically stable and equivalent to `softplus(-core_term)`.\n   loss = -logsigmoid(core_term).", "hyperparams": {"temperature": 5.0, "max_margin": 3.5, "beta": 1.0, "alpha": 0.1}, "operators_used": ["zscore", "rank_gap", "tanh", "sigmoid", "clamp", "logsigmoid"], "implementation_hint": {"expects": ["cost_a", "cost_b", "logp_a", "logp_b"], "returns": "scalar"}}, "fitness": {"hf_like_score": 3.879427671432495, "validation_objective": 3.879427671432495, "generalization_penalty": 0.0, "generalization_objectives": {"20": 3.847743272781372}, "train_score_mean": 4.2044228464365005, "train_loss_mean": 0.667531767860055, "pair_count": 4113051, "config": {"hf": {"problem": "tsp", "hf_steps": 32, "train_problem_size": 20, "valid_problem_sizes": [20], "train_batch_size": 64, "pomo_size": 64, "learning_rate": 0.0003, "weight_decay": 1e-06, "alpha": 0.05, "device": "cuda", "seed": 1234, "num_validation_episodes": 128, "validation_batch_size": 64, "generalization_penalty_weight": 1.0, "pool_version": "v0"}, "free_loss": {"f1_steps": 32, "f2_steps": 100, "f3_enabled": false}}, "loss_ir": {"name": "Z-Scored Rank-Adaptive Log-Prob Coupled Hinge Loss", "intuition": "This loss function creates a highly adaptive and robust margin by synthesizing rank-based and distribution-based normalization, coupling it with the model's own confidence, and applying it within a stable hinge-loss framework.\n\nInherited Ideas:\n- From Parent 0, it inherits the idea of creating a dynamic margin by adding a term proportional to the log-probability difference (`alpha * logp_diff`). This couples the target separation to the model's current confidence, pushing for greater separation when the model is already confident and relying more on cost-derived information otherwise.\n- From Parent 1, it inherits the use of `tanh` to create a smoothly saturating, bounded margin from a normalized cost signal (`max_margin * tanh(...)`). This enhances numerical stability by preventing the margin from growing uncontrollably.\n- From both parents, it inherits the core structure of pre-normalizing costs using a z-score transformation (`zscore(costs)`) and then computing a `rank_gap` on these normalized costs. This two-step normalization provides a signal that is robust to both the overall statistical distribution and local rank ordering.\n\nNew Coupling Ideas:\n1. **Log-Probabilistic Temperature Scaling**: The temperature parameter, which controls the steepness of the loss, is made dynamic. It is scaled by the sigmoid of the log-probability difference (`sigmoid(logp_diff)`). When the model is uncertain (logp_diff is near zero), the temperature is lower, making the loss gradient gentler. As the model becomes more confident (logp_diff is large), the temperature increases, sharpening the loss and pushing for more precise alignment with the target margin. This couples the loss's sensitivity directly to the model's confidence.\n2. **Margin Clamping for Stability**: The final dynamic margin, which is a sum of the cost-derived `tanh` term and the confidence-derived `logp_diff` term, is clamped to a safe range (`[0, 2 * max_margin]`). This prevents the target margin from becoming excessively large or negative, which could occur if the model's `logp_diff` is extreme. This simple trick guarantees the stability of the margin target before it is used in the final loss calculation.", "hyperparams": {"temperature": 5.0, "max_margin": 3.5, "beta": 1.0, "alpha": 0.1}, "operators_used": ["zscore", "rank_gap", "tanh", "sigmoid", "clamp", "logsigmoid"]}}}
{"generation": 9, "index": 0, "ir": {"name": "Confidence-Coupled and Z-Scored Rank-Hinge Loss", "intuition": "This loss function creates a highly adaptive and robust margin by synthesizing rank-based and distribution-based normalization, coupling it with the model's own confidence, and applying it within a stable hinge-loss framework.\n\nInherited Ideas:\n- From Parent 0, it inherits the idea of creating a dynamic margin by adding a term proportional to the log-probability difference (`alpha * logp_diff`). This couples the margin target to the model's current confidence, pushing for greater separation when the model is already confident and relying more on cost-based signals when it is not.\n- From Parent 1, it inherits the core structure of the loss: `-logsigmoid(temperature * (logp_diff - margin))`, which is a stable, hinge-like loss. It also inherits the use of `tanh` to create a smoothly saturating, bounded margin from a normalized cost signal, preventing the margin from growing uncontrollably.\n\nNew Coupling Ideas:\n1. **Z-Score Pre-Normalization**: Before computing the rank gap, the costs for the entire batch are first normalized using a z-score transformation. This centers the cost distribution and scales it by its standard deviation. This pre-processing step, inspired by both parents' use of `zscore`, makes the subsequent rank-gap calculation more stable and less sensitive to the absolute scale or shifts in the cost distribution.\n2. **Hybrid Dynamic Margin**: The final target margin is a hybrid of three components. First, a cost-based margin is computed using `tanh(beta * rank_gap(zscore(costs)))`. This provides a stable, bounded signal based on relative preference. Second, this is added to a model-confidence term `alpha * logp_diff`. The combination `cost_margin + confidence_margin` forms a dynamic target that adapts to both ground-truth preference and model state. This directly couples the stable `tanh`-based margin from Parent 1 with the confidence-coupling mechanism from Parent 0.", "pseudocode": "1. Given costs for a batch, normalize them using z-scoring: z_cost_a = zscore(cost_a), z_cost_b = zscore(cost_b).\n2. Compute the rank-normalized gap between the z-scored costs. This provides a stable signal of relative preference, typically in [-1, 1].\n   rank_diff = rank_gap(z_cost_a, z_cost_b).\n3. Create the cost-based adaptive margin by passing the rank difference through a `tanh` function and scaling it. This creates a smooth, bounded component for the target separation.\n   cost_margin = max_margin * tanh(beta * rank_diff).\n4. Compute the log-probability difference: logp_diff = logp_a - logp_b.\n5. Create the final dynamic margin by adding a term proportional to the log-probability difference itself. This couples the target to the model's confidence.\n   dynamic_margin = cost_margin + alpha * logp_diff.\n6. Calculate the core loss term. This is the difference between the log-probability gap and the dynamic margin, scaled by a temperature parameter.\n   core_term = temperature * (logp_diff - dynamic_margin).\n7. The final loss is the negative logsigmoid of the core term. This penalizes cases where `logp_diff` is smaller than the target `dynamic_margin` and is equivalent to `softplus(-core_term)` for numerical stability.\n   loss = -logsigmoid(core_term).", "hyperparams": {"temperature": 5.0, "max_margin": 3.5, "beta": 1.0, "alpha": 0.1}, "operators_used": ["zscore", "rank_gap", "tanh", "logsigmoid"], "implementation_hint": {"expects": ["cost_a", "cost_b", "logp_a", "logp_b"], "returns": "scalar"}}, "fitness": {"hf_like_score": 3.8758974075317383, "validation_objective": 3.8758974075317383, "generalization_penalty": 0.0, "generalization_objectives": {"20": 3.844630718231201}, "train_score_mean": 4.202833868563175, "train_loss_mean": 0.6675978191196918, "pair_count": 4112530, "config": {"hf": {"problem": "tsp", "hf_steps": 32, "train_problem_size": 20, "valid_problem_sizes": [20], "train_batch_size": 64, "pomo_size": 64, "learning_rate": 0.0003, "weight_decay": 1e-06, "alpha": 0.05, "device": "cuda", "seed": 1234, "num_validation_episodes": 128, "validation_batch_size": 64, "generalization_penalty_weight": 1.0, "pool_version": "v0"}, "free_loss": {"f1_steps": 32, "f2_steps": 100, "f3_enabled": false}}, "loss_ir": {"name": "Confidence-Coupled and Z-Scored Rank-Hinge Loss", "intuition": "This loss function creates a highly adaptive and robust margin by synthesizing rank-based and distribution-based normalization, coupling it with the model's own confidence, and applying it within a stable hinge-loss framework.\n\nInherited Ideas:\n- From Parent 0, it inherits the idea of creating a dynamic margin by adding a term proportional to the log-probability difference (`alpha * logp_diff`). This couples the margin target to the model's current confidence, pushing for greater separation when the model is already confident and relying more on cost-based signals when it is not.\n- From Parent 1, it inherits the core structure of the loss: `-logsigmoid(temperature * (logp_diff - margin))`, which is a stable, hinge-like loss. It also inherits the use of `tanh` to create a smoothly saturating, bounded margin from a normalized cost signal, preventing the margin from growing uncontrollably.\n\nNew Coupling Ideas:\n1. **Z-Score Pre-Normalization**: Before computing the rank gap, the costs for the entire batch are first normalized using a z-score transformation. This centers the cost distribution and scales it by its standard deviation. This pre-processing step, inspired by both parents' use of `zscore`, makes the subsequent rank-gap calculation more stable and less sensitive to the absolute scale or shifts in the cost distribution.\n2. **Hybrid Dynamic Margin**: The final target margin is a hybrid of three components. First, a cost-based margin is computed using `tanh(beta * rank_gap(zscore(costs)))`. This provides a stable, bounded signal based on relative preference. Second, this is added to a model-confidence term `alpha * logp_diff`. The combination `cost_margin + confidence_margin` forms a dynamic target that adapts to both ground-truth preference and model state. This directly couples the stable `tanh`-based margin from Parent 1 with the confidence-coupling mechanism from Parent 0.", "hyperparams": {"temperature": 5.0, "max_margin": 3.5, "beta": 1.0, "alpha": 0.1}, "operators_used": ["zscore", "rank_gap", "tanh", "logsigmoid"]}}}
{"generation": 9, "index": 1, "ir": {"name": "Confidence-Weighted Z-Rank Softplus Loss", "intuition": "This loss function constructs a highly adaptive margin by combining rank and distributional normalization, and then dynamically weights the loss based on the model's confidence, effectively focusing on correcting high-confidence errors.\n\nInherited Ideas:\n- From Parent 0 and Parent 1, it inherits the core idea of creating a robust, adaptive margin by first normalizing costs with a z-score (`zscore(costs)`) and then computing the `rank_gap` on these normalized costs. This two-step normalization makes the margin signal robust to both the overall statistical distribution and the local rank ordering of costs.\n- From Parent 1, it inherits the use of `tanh` to create a smoothly saturating, bounded margin (`max_margin * tanh(beta * rank_diff)`). This prevents the target margin from growing uncontrollably, which enhances numerical stability.\n\nNew Coupling Ideas:\n1. **Softplus Hinge Formulation**: Instead of the `logsigmoid` formulation, the loss directly uses `softplus(margin - logp_diff)`. This is mathematically equivalent to `-logsigmoid(logp_diff - margin)` but frames the objective more explicitly as a smooth hinge loss, penalizing cases where `logp_diff` is less than the target `margin`.\n2. **Confidence-Based Loss Weighting**: The entire softplus term is multiplied by a weight derived from the sigmoid of the log-probability difference (`sigmoid(logp_diff)`). This new coupling idea means that the loss is amplified for pairs where the model is already confidently predicting the correct preference (large positive `logp_diff`). This forces the model to push for even greater separation on 'easy' pairs it already understands, refining its calibration and sharpening its decision boundary, rather than just focusing on pairs where it is uncertain or wrong.", "pseudocode": "1. Given costs for a batch, normalize them using z-scoring: z_cost_a = zscore(cost_a), z_cost_b = zscore(cost_b).\n2. Compute the rank-normalized gap between the z-scored costs to get a robust preference signal. \n   rank_diff = rank_gap(z_cost_a, z_cost_b).\n3. Create an adaptive margin by passing the rank difference through a `tanh` function and scaling it. This creates a smooth, bounded target separation.\n   margin = max_margin * tanh(beta * rank_diff).\n4. Compute the log-probability difference: logp_diff = logp_a - logp_b.\n5. Calculate a confidence weight based on the model's current prediction. The weight is close to 1.0 when the model is confident in the correct preference and close to 0.5 when uncertain.\n   confidence_weight = sigmoid(logp_diff).\n6. Calculate the core hinge loss term using `softplus`. This penalizes cases where the log-probability difference is smaller than the target margin.\n   hinge_term = softplus(margin - logp_diff).\n7. The final loss is the hinge term multiplied by the confidence weight. This scales the loss to focus more on pairs where the model is already confident.\n   loss = confidence_weight * hinge_term.", "hyperparams": {"beta": 1.0, "max_margin": 3.0}, "operators_used": ["zscore", "rank_gap", "tanh", "softplus", "sigmoid"], "implementation_hint": {"expects": ["cost_a", "cost_b", "logp_a", "logp_b"], "returns": "scalar"}}, "fitness": {"hf_like_score": 3.8830149173736572, "validation_objective": 3.8830149173736572, "generalization_penalty": 0.0, "generalization_objectives": {"20": 3.8485554456710815}, "train_score_mean": 4.209236092865467, "train_loss_mean": 0.6786436624825001, "pair_count": 4111687, "config": {"hf": {"problem": "tsp", "hf_steps": 32, "train_problem_size": 20, "valid_problem_sizes": [20], "train_batch_size": 64, "pomo_size": 64, "learning_rate": 0.0003, "weight_decay": 1e-06, "alpha": 0.05, "device": "cuda", "seed": 1234, "num_validation_episodes": 128, "validation_batch_size": 64, "generalization_penalty_weight": 1.0, "pool_version": "v0"}, "free_loss": {"f1_steps": 32, "f2_steps": 100, "f3_enabled": false}}, "loss_ir": {"name": "Confidence-Weighted Z-Rank Softplus Loss", "intuition": "This loss function constructs a highly adaptive margin by combining rank and distributional normalization, and then dynamically weights the loss based on the model's confidence, effectively focusing on correcting high-confidence errors.\n\nInherited Ideas:\n- From Parent 0 and Parent 1, it inherits the core idea of creating a robust, adaptive margin by first normalizing costs with a z-score (`zscore(costs)`) and then computing the `rank_gap` on these normalized costs. This two-step normalization makes the margin signal robust to both the overall statistical distribution and the local rank ordering of costs.\n- From Parent 1, it inherits the use of `tanh` to create a smoothly saturating, bounded margin (`max_margin * tanh(beta * rank_diff)`). This prevents the target margin from growing uncontrollably, which enhances numerical stability.\n\nNew Coupling Ideas:\n1. **Softplus Hinge Formulation**: Instead of the `logsigmoid` formulation, the loss directly uses `softplus(margin - logp_diff)`. This is mathematically equivalent to `-logsigmoid(logp_diff - margin)` but frames the objective more explicitly as a smooth hinge loss, penalizing cases where `logp_diff` is less than the target `margin`.\n2. **Confidence-Based Loss Weighting**: The entire softplus term is multiplied by a weight derived from the sigmoid of the log-probability difference (`sigmoid(logp_diff)`). This new coupling idea means that the loss is amplified for pairs where the model is already confidently predicting the correct preference (large positive `logp_diff`). This forces the model to push for even greater separation on 'easy' pairs it already understands, refining its calibration and sharpening its decision boundary, rather than just focusing on pairs where it is uncertain or wrong.", "hyperparams": {"beta": 1.0, "max_margin": 3.0}, "operators_used": ["zscore", "rank_gap", "tanh", "softplus", "sigmoid"]}}}
{"generation": 9, "index": 2, "ir": {"name": "Confidence-Weighted Rank and Z-Score Hinge Loss", "intuition": "This loss function constructs a highly adaptive margin by combining rank-based and distribution-based normalization, and then dynamically weights the loss based on the model's own confidence, creating a self-correcting learning signal.\n\nInherited Ideas:\n- From Parent 0 (Hybrid Z-Score and Rank-Adaptive Sigmoid Loss) and Parent 1 (Z-Scored Rank-Adaptive Hinge Loss), it inherits the powerful pre-processing step of normalizing costs using `zscore` before computing the `rank_gap`. This makes the margin robust to both the scale and distribution of costs.\n- From Parent 1 (Z-Scored Rank-Adaptive Hinge Loss), it inherits the use of `tanh` to create a smooth, bounded, and saturating adaptive margin from the normalized cost difference. This prevents the target margin from growing uncontrollably.\n\nNew Coupling Ideas:\n1. **Dynamic Temperature from Log-Probability Difference**: The loss's temperature, which controls the steepness of the penalty, is not fixed. Instead, it is dynamically computed from the log-probability difference (`logp_diff`). The `softplus` function is used on `-logp_diff` to create a temperature that is high when the model is wrongly confident (i.e., `logp_a - logp_b` is large and negative) and low when the model is correctly confident. This coupling forces the model to pay more attention to its most egregious errors.\n2. **Confidence-Based Loss Attenuation**: The final loss is attenuated by a sigmoid function of the log-probability difference (`sigmoid(logp_diff)`). This means that pairs where the model is already very confident in the correct preference (large positive `logp_diff`) contribute less to the total loss. This acts as a stability trick, focusing the training effort on uncertain or incorrectly classified pairs and preventing the model from over-optimizing on easy examples.", "pseudocode": "1. Given costs for a batch, normalize them using z-scoring: z_cost_a = zscore(cost_a), z_cost_b = zscore(cost_b).\n2. Compute the rank-normalized gap between the z-scored costs. This provides a stable signal of relative preference.\n   rank_diff = rank_gap(z_cost_a, z_cost_b).\n3. Create a smooth, adaptive margin by passing the rank difference through a `tanh` function and scaling it.\n   adaptive_margin = max_margin * tanh(beta * rank_diff).\n4. Compute the log-probability difference: logp_diff = logp_a - logp_b.\n5. Compute a dynamic temperature that increases when the model is wrongly confident. A base temperature is added for stability.\n   dynamic_temperature = base_temperature + softplus(-logp_diff).\n6. Calculate the core hinge loss term using the `softplus` function, which is a numerically stable equivalent of `-logsigmoid` on the inverted argument. This penalizes cases where `logp_diff` is less than the target margin.\n   hinge_term = softplus(dynamic_temperature * (adaptive_margin - logp_diff)).\n7. Compute a confidence weight that attenuates the loss for pairs where the model is already correctly confident.\n   confidence_weight = sigmoid(logp_diff).\n8. The final loss is the hinge term multiplied by the confidence weight. This focuses training on difficult pairs.\n   loss = confidence_weight * hinge_term.", "hyperparams": {"beta": 1.0, "max_margin": 3.0, "base_temperature": 1.0}, "operators_used": ["zscore", "rank_gap", "tanh", "softplus", "sigmoid"], "implementation_hint": {"expects": ["cost_a", "cost_b", "logp_a", "logp_b"], "returns": "scalar"}}, "fitness": {"hf_like_score": 3.887705445289612, "validation_objective": 3.887705445289612, "generalization_penalty": 0.0, "generalization_objectives": {"20": 3.847819447517395}, "train_score_mean": 4.210088036954403, "train_loss_mean": 0.678058348596096, "pair_count": 4111165, "config": {"hf": {"problem": "tsp", "hf_steps": 32, "train_problem_size": 20, "valid_problem_sizes": [20], "train_batch_size": 64, "pomo_size": 64, "learning_rate": 0.0003, "weight_decay": 1e-06, "alpha": 0.05, "device": "cuda", "seed": 1234, "num_validation_episodes": 128, "validation_batch_size": 64, "generalization_penalty_weight": 1.0, "pool_version": "v0"}, "free_loss": {"f1_steps": 32, "f2_steps": 100, "f3_enabled": false}}, "loss_ir": {"name": "Confidence-Weighted Rank and Z-Score Hinge Loss", "intuition": "This loss function constructs a highly adaptive margin by combining rank-based and distribution-based normalization, and then dynamically weights the loss based on the model's own confidence, creating a self-correcting learning signal.\n\nInherited Ideas:\n- From Parent 0 (Hybrid Z-Score and Rank-Adaptive Sigmoid Loss) and Parent 1 (Z-Scored Rank-Adaptive Hinge Loss), it inherits the powerful pre-processing step of normalizing costs using `zscore` before computing the `rank_gap`. This makes the margin robust to both the scale and distribution of costs.\n- From Parent 1 (Z-Scored Rank-Adaptive Hinge Loss), it inherits the use of `tanh` to create a smooth, bounded, and saturating adaptive margin from the normalized cost difference. This prevents the target margin from growing uncontrollably.\n\nNew Coupling Ideas:\n1. **Dynamic Temperature from Log-Probability Difference**: The loss's temperature, which controls the steepness of the penalty, is not fixed. Instead, it is dynamically computed from the log-probability difference (`logp_diff`). The `softplus` function is used on `-logp_diff` to create a temperature that is high when the model is wrongly confident (i.e., `logp_a - logp_b` is large and negative) and low when the model is correctly confident. This coupling forces the model to pay more attention to its most egregious errors.\n2. **Confidence-Based Loss Attenuation**: The final loss is attenuated by a sigmoid function of the log-probability difference (`sigmoid(logp_diff)`). This means that pairs where the model is already very confident in the correct preference (large positive `logp_diff`) contribute less to the total loss. This acts as a stability trick, focusing the training effort on uncertain or incorrectly classified pairs and preventing the model from over-optimizing on easy examples.", "hyperparams": {"beta": 1.0, "max_margin": 3.0, "base_temperature": 1.0}, "operators_used": ["zscore", "rank_gap", "tanh", "softplus", "sigmoid"]}}}
{"generation": 9, "index": 3, "ir": {"name": "Confidence-Coupled Rank-Adaptive Softplus Loss", "intuition": "This loss function constructs a highly adaptive and robust margin by combining distribution-based and rank-based normalization, and further couples this margin to the model's own confidence before applying it within a stable softplus-based loss structure.\n\nInherited Ideas:\n- From Parent 0 and Parent 1, it inherits the idea of creating a robust preference signal by first applying `zscore` to the costs and then computing the `rank_gap`. This two-step normalization makes the margin signal insensitive to the absolute scale of costs and robust to outliers.\n- From Parent 0, it inherits the concept of a 'dynamic margin' that is coupled to the model's current state. The margin is not just a function of costs but also incorporates the log-probability difference (`logp_diff`), making the training target adjust to the model's confidence.\n- From Parent 1, it inherits the use of `tanh` to create a smoothly saturating, bounded margin from the normalized cost signal. This enhances numerical stability by preventing the margin from growing uncontrollably.\n\nNew Coupling Ideas:\n1. **Dual-Signal Margin Construction**: The final margin is a sum of two distinct components. The first is a cost-based adaptive margin (`max_margin * tanh(...)`) which captures the ground-truth preference strength. The second is a model-confidence-based term (`alpha * relu(logp_diff)`) which adds an extra push for separation only when the model is already correctly confident (`logp_diff > 0`). This `relu` coupling prevents the loss from being reduced when the model is wrong (i.e., it doesn't reward confident but incorrect predictions).\n2. **Direct Softplus Application**: Instead of the `-logsigmoid(temperature * (logp_diff - margin))` formulation, the loss directly uses `softplus(temperature * (margin - logp_diff))`. While mathematically equivalent, this formulation is more explicit about its nature as a smoothed hinge loss. It penalizes the model when `logp_diff` is less than the target `dynamic_margin`, with the `temperature` parameter controlling the sharpness of this penalty.", "pseudocode": "1. Given costs for a batch, normalize them using z-scoring: z_cost_a = zscore(cost_a), z_cost_b = zscore(cost_b).\n2. Compute the rank-normalized gap between the z-scored costs to get a robust preference signal.\n   rank_diff = rank_gap(z_cost_a, z_cost_b).\n3. Create the cost-based adaptive margin by passing `rank_diff` through a `tanh` function and scaling it. This creates a smooth, bounded target based on preference strength.\n   cost_margin = max_margin * tanh(beta * rank_diff).\n4. Compute the log-probability difference: logp_diff = logp_a - logp_b.\n5. Create the final dynamic margin by adding a term proportional to the model's confidence. The `relu` ensures this term only applies when the model's preference aligns with the ground truth (logp_diff > 0).\n   dynamic_margin = cost_margin + alpha * relu(logp_diff).\n6. Calculate the core difference between the target margin and the model's log-probability difference.\n   margin_shortfall = dynamic_margin - logp_diff.\n7. The final loss is the `softplus` of the scaled margin shortfall. This applies a smooth penalty when the model's log-probability difference fails to meet the dynamic margin target.\n   loss = softplus(temperature * margin_shortfall).", "hyperparams": {"temperature": 5.0, "max_margin": 3.5, "beta": 1.0, "alpha": 0.2}, "operators_used": ["zscore", "rank_gap", "tanh", "relu", "softplus"], "implementation_hint": {"expects": ["cost_a", "cost_b", "logp_a", "logp_b"], "returns": "scalar"}}, "fitness": {"hf_like_score": 3.879699468612671, "validation_objective": 3.879699468612671, "generalization_penalty": 0.0, "generalization_objectives": {"20": 3.8483781814575195}, "train_score_mean": 4.210460402071476, "train_loss_mean": 0.6517876517027617, "pair_count": 4117463, "config": {"hf": {"problem": "tsp", "hf_steps": 32, "train_problem_size": 20, "valid_problem_sizes": [20], "train_batch_size": 64, "pomo_size": 64, "learning_rate": 0.0003, "weight_decay": 1e-06, "alpha": 0.05, "device": "cuda", "seed": 1234, "num_validation_episodes": 128, "validation_batch_size": 64, "generalization_penalty_weight": 1.0, "pool_version": "v0"}, "free_loss": {"f1_steps": 32, "f2_steps": 100, "f3_enabled": false}}, "loss_ir": {"name": "Confidence-Coupled Rank-Adaptive Softplus Loss", "intuition": "This loss function constructs a highly adaptive and robust margin by combining distribution-based and rank-based normalization, and further couples this margin to the model's own confidence before applying it within a stable softplus-based loss structure.\n\nInherited Ideas:\n- From Parent 0 and Parent 1, it inherits the idea of creating a robust preference signal by first applying `zscore` to the costs and then computing the `rank_gap`. This two-step normalization makes the margin signal insensitive to the absolute scale of costs and robust to outliers.\n- From Parent 0, it inherits the concept of a 'dynamic margin' that is coupled to the model's current state. The margin is not just a function of costs but also incorporates the log-probability difference (`logp_diff`), making the training target adjust to the model's confidence.\n- From Parent 1, it inherits the use of `tanh` to create a smoothly saturating, bounded margin from the normalized cost signal. This enhances numerical stability by preventing the margin from growing uncontrollably.\n\nNew Coupling Ideas:\n1. **Dual-Signal Margin Construction**: The final margin is a sum of two distinct components. The first is a cost-based adaptive margin (`max_margin * tanh(...)`) which captures the ground-truth preference strength. The second is a model-confidence-based term (`alpha * relu(logp_diff)`) which adds an extra push for separation only when the model is already correctly confident (`logp_diff > 0`). This `relu` coupling prevents the loss from being reduced when the model is wrong (i.e., it doesn't reward confident but incorrect predictions).\n2. **Direct Softplus Application**: Instead of the `-logsigmoid(temperature * (logp_diff - margin))` formulation, the loss directly uses `softplus(temperature * (margin - logp_diff))`. While mathematically equivalent, this formulation is more explicit about its nature as a smoothed hinge loss. It penalizes the model when `logp_diff` is less than the target `dynamic_margin`, with the `temperature` parameter controlling the sharpness of this penalty.", "hyperparams": {"temperature": 5.0, "max_margin": 3.5, "beta": 1.0, "alpha": 0.2}, "operators_used": ["zscore", "rank_gap", "tanh", "relu", "softplus"]}}}
{"generation": 9, "index": 4, "ir": {"name": "Z-Score and Rank-Gap Modulated Hinge Loss", "intuition": "This loss function constructs a highly adaptive margin by combining z-score and rank-based normalization, and then dynamically modulates this margin based on the model's own confidence before applying it within a stable, hinge-like loss framework.\n\nInherited Ideas:\n- From both Parent 0 and Parent 1, it inherits the use of a two-stage normalization process: first applying `zscore` to the batch costs to stabilize against scale and shift, and then using `rank_gap` on these z-scored costs to get a robust, outlier-resistant measure of relative preference.\n- From Parent 1, it inherits the use of `tanh` to create a smooth, bounded, and saturating adaptive margin from the normalized cost difference. This ensures the margin remains within a controlled range (`-max_margin` to `+max_margin`).\n- From both parents, it inherits the final loss structure of `-logsigmoid(temperature * (logp_diff - margin))`, which is numerically equivalent to `softplus(temperature * (margin - logp_diff))` and behaves like a smoothed hinge loss.\n\nNew Coupling Ideas:\n1. **Confidence-Modulated Margin**: The core adaptive margin, derived from the normalized cost difference, is dynamically modulated by a factor based on the model's own confidence. This factor, `sigmoid(alpha * logp_diff)`, scales the margin. When the model is already very confident and correct (large positive `logp_diff`), the sigmoid approaches 1, applying the full margin and pushing for even better separation. When the model is unconfident or wrong (small or negative `logp_diff`), the sigmoid approaches 0.5 or less, effectively reducing the target margin. This coupling prevents the model from being overly penalized on difficult examples where it is uncertain, allowing it to focus its capacity on clearer preferences first.\n2. **Margin Baseline from Log-Probabilities**: It introduces a small, secondary margin component directly proportional to the log-probability difference (`gamma * logp_diff`), similar to an idea in Parent 0. This term provides a gentle, corrective baseline. When the model is wrong (`logp_diff < 0`), this term becomes negative, slightly increasing the loss and encouraging a correction. This combines with the primary confidence-modulated margin to create a nuanced and responsive target.", "pseudocode": "1. Given costs for a batch, normalize them using z-scoring: z_cost_a = zscore(cost_a), z_cost_b = zscore(cost_b).\n2. Compute the rank-normalized gap between the z-scored costs. This provides a stable signal of relative preference, typically in [-1, 1].\n   rank_diff = rank_gap(z_cost_a, z_cost_b).\n3. Create the primary adaptive margin by passing the rank difference through a `tanh` function and scaling it. This creates a smooth, bounded target separation.\n   base_adaptive_margin = max_margin * tanh(beta * rank_diff).\n4. Compute the log-probability difference: logp_diff = logp_a - logp_b.\n5. Calculate a confidence modulation factor using a sigmoid function on the log-probability difference. This factor scales the margin based on the model's current belief.\n   confidence_modulator = sigmoid(alpha * logp_diff).\n6. Apply the modulation to the base margin. This couples the cost-based margin to the model's confidence.\n   modulated_margin = confidence_modulator * base_adaptive_margin.\n7. Create the final dynamic margin by adding a small term proportional to the log-probability difference itself. This acts as a gentle corrective baseline.\n   dynamic_margin = modulated_margin + gamma * logp_diff.\n8. Calculate the core loss term by scaling the difference between the log-probability gap and the dynamic margin.\n   core_term = temperature * (logp_diff - dynamic_margin).\n9. The final loss is the negative logsigmoid of the core term, which is numerically stable and penalizes cases where `logp_diff` is smaller than the target `dynamic_margin`.\n   loss = -logsigmoid(core_term).", "hyperparams": {"temperature": 5.0, "max_margin": 3.0, "beta": 1.0, "alpha": 0.2, "gamma": 0.05}, "operators_used": ["zscore", "rank_gap", "tanh", "sigmoid", "logsigmoid"], "implementation_hint": {"expects": ["cost_a", "cost_b", "logp_a", "logp_b"], "returns": "scalar"}}, "fitness": {"hf_like_score": 3.876646876335144, "validation_objective": 3.876646876335144, "generalization_penalty": 0.0, "generalization_objectives": {"20": 3.849900722503662}, "train_score_mean": 4.209473080933094, "train_loss_mean": 0.6532747931778431, "pair_count": 4117437, "config": {"hf": {"problem": "tsp", "hf_steps": 32, "train_problem_size": 20, "valid_problem_sizes": [20], "train_batch_size": 64, "pomo_size": 64, "learning_rate": 0.0003, "weight_decay": 1e-06, "alpha": 0.05, "device": "cuda", "seed": 1234, "num_validation_episodes": 128, "validation_batch_size": 64, "generalization_penalty_weight": 1.0, "pool_version": "v0"}, "free_loss": {"f1_steps": 32, "f2_steps": 100, "f3_enabled": false}}, "loss_ir": {"name": "Z-Score and Rank-Gap Modulated Hinge Loss", "intuition": "This loss function constructs a highly adaptive margin by combining z-score and rank-based normalization, and then dynamically modulates this margin based on the model's own confidence before applying it within a stable, hinge-like loss framework.\n\nInherited Ideas:\n- From both Parent 0 and Parent 1, it inherits the use of a two-stage normalization process: first applying `zscore` to the batch costs to stabilize against scale and shift, and then using `rank_gap` on these z-scored costs to get a robust, outlier-resistant measure of relative preference.\n- From Parent 1, it inherits the use of `tanh` to create a smooth, bounded, and saturating adaptive margin from the normalized cost difference. This ensures the margin remains within a controlled range (`-max_margin` to `+max_margin`).\n- From both parents, it inherits the final loss structure of `-logsigmoid(temperature * (logp_diff - margin))`, which is numerically equivalent to `softplus(temperature * (margin - logp_diff))` and behaves like a smoothed hinge loss.\n\nNew Coupling Ideas:\n1. **Confidence-Modulated Margin**: The core adaptive margin, derived from the normalized cost difference, is dynamically modulated by a factor based on the model's own confidence. This factor, `sigmoid(alpha * logp_diff)`, scales the margin. When the model is already very confident and correct (large positive `logp_diff`), the sigmoid approaches 1, applying the full margin and pushing for even better separation. When the model is unconfident or wrong (small or negative `logp_diff`), the sigmoid approaches 0.5 or less, effectively reducing the target margin. This coupling prevents the model from being overly penalized on difficult examples where it is uncertain, allowing it to focus its capacity on clearer preferences first.\n2. **Margin Baseline from Log-Probabilities**: It introduces a small, secondary margin component directly proportional to the log-probability difference (`gamma * logp_diff`), similar to an idea in Parent 0. This term provides a gentle, corrective baseline. When the model is wrong (`logp_diff < 0`), this term becomes negative, slightly increasing the loss and encouraging a correction. This combines with the primary confidence-modulated margin to create a nuanced and responsive target.", "hyperparams": {"temperature": 5.0, "max_margin": 3.0, "beta": 1.0, "alpha": 0.2, "gamma": 0.05}, "operators_used": ["zscore", "rank_gap", "tanh", "sigmoid", "logsigmoid"]}}}
{"generation": 9, "index": 5, "ir": {"name": "Confidence-Coupled Rank-Hinge Loss", "intuition": "This loss function creates a highly adaptive and robust margin by synthesizing rank-based normalization with a confidence-aware target, then applies it within a stable hinge-loss framework.\n\nInherited Ideas:\n- From Parent 0, it inherits the idea of coupling the target margin directly to the model's confidence. The margin is dynamically adjusted by adding a term proportional to the log-probability difference (`alpha * logp_diff`), making the training target more aggressive when the model is already confident and more lenient when it is not.\n- From Parent 1, it inherits the core structure of creating a smooth, bounded margin by passing a normalized cost signal through a `tanh` function and scaling it (`max_margin * tanh(...)`). It also inherits the final loss formulation of `-logsigmoid(temperature * (logp_diff - margin))`, which is a numerically stable, smooth hinge-like loss.\n\nNew Coupling Ideas:\n1. **Rank-Gap before Tanh**: Instead of z-scoring, this loss directly uses the `rank_gap` of the raw costs as the input to the `tanh` function. This simplifies the normalization pipeline while still capturing the crucial relative ordering of preferences in the batch. The `tanh` then smoothly squashes this rank-based signal into a bounded [-1, 1] range, making the margin robust to outliers in rank differences.\n2. **Margin Clamping for Stability**: A `clamp` operation is introduced on the final dynamic margin. The margin is clamped to a minimum of zero to prevent it from becoming negative, which could happen if `logp_diff` is large and negative (i.e., the model is confidently wrong) and `alpha` is non-trivial. This ensures the loss always encourages `logp_a > logp_b` when `cost_a < cost_b` and prevents the optimization target from flipping, enhancing training stability.", "pseudocode": "1. Given costs for a batch, compute the rank-normalized gap between them. This provides a stable signal of relative preference, typically in [-1, 1].\n   rank_diff = rank_gap(cost_a, cost_b).\n2. Create the cost-based adaptive margin by passing the rank difference through a `tanh` function and scaling it. This creates a smooth, bounded component of the target separation.\n   cost_margin = max_margin * tanh(beta * rank_diff).\n3. Compute the log-probability difference: logp_diff = logp_a - logp_b.\n4. Create the final dynamic margin by adding a term proportional to the log-probability difference itself. This couples the target to the model's confidence.\n   dynamic_margin = cost_margin + alpha * logp_diff.\n5. For stability, clamp the dynamic margin to be non-negative. This prevents the loss from rewarding incorrect predictions when the model is confidently wrong.\n   clamped_margin = clamp(dynamic_margin, min=0.0).\n6. Calculate the core loss term. This is the difference between the log-probability gap and the clamped margin, scaled by a temperature parameter.\n   core_term = temperature * (logp_diff - clamped_margin).\n7. The final loss is the negative logsigmoid of the core term. This penalizes cases where `logp_diff` is smaller than the target `clamped_margin` and is equivalent to `softplus(-core_term)` for numerical stability.\n   loss = -logsigmoid(core_term).", "hyperparams": {"beta": 1.0, "max_margin": 2.0, "temperature": 5.0, "alpha": 0.1}, "operators_used": ["rank_gap", "tanh", "clamp", "logsigmoid"], "implementation_hint": {"expects": ["cost_a", "cost_b", "logp_a", "logp_b"], "returns": "scalar"}}, "fitness": {"hf_like_score": 3.8813345432281494, "validation_objective": 3.8813345432281494, "generalization_penalty": 0.0, "generalization_objectives": {"20": 3.8478649854660034}, "train_score_mean": 4.203583329916, "train_loss_mean": 0.667473528534174, "pair_count": 4113503, "config": {"hf": {"problem": "tsp", "hf_steps": 32, "train_problem_size": 20, "valid_problem_sizes": [20], "train_batch_size": 64, "pomo_size": 64, "learning_rate": 0.0003, "weight_decay": 1e-06, "alpha": 0.05, "device": "cuda", "seed": 1234, "num_validation_episodes": 128, "validation_batch_size": 64, "generalization_penalty_weight": 1.0, "pool_version": "v0"}, "free_loss": {"f1_steps": 32, "f2_steps": 100, "f3_enabled": false}}, "loss_ir": {"name": "Confidence-Coupled Rank-Hinge Loss", "intuition": "This loss function creates a highly adaptive and robust margin by synthesizing rank-based normalization with a confidence-aware target, then applies it within a stable hinge-loss framework.\n\nInherited Ideas:\n- From Parent 0, it inherits the idea of coupling the target margin directly to the model's confidence. The margin is dynamically adjusted by adding a term proportional to the log-probability difference (`alpha * logp_diff`), making the training target more aggressive when the model is already confident and more lenient when it is not.\n- From Parent 1, it inherits the core structure of creating a smooth, bounded margin by passing a normalized cost signal through a `tanh` function and scaling it (`max_margin * tanh(...)`). It also inherits the final loss formulation of `-logsigmoid(temperature * (logp_diff - margin))`, which is a numerically stable, smooth hinge-like loss.\n\nNew Coupling Ideas:\n1. **Rank-Gap before Tanh**: Instead of z-scoring, this loss directly uses the `rank_gap` of the raw costs as the input to the `tanh` function. This simplifies the normalization pipeline while still capturing the crucial relative ordering of preferences in the batch. The `tanh` then smoothly squashes this rank-based signal into a bounded [-1, 1] range, making the margin robust to outliers in rank differences.\n2. **Margin Clamping for Stability**: A `clamp` operation is introduced on the final dynamic margin. The margin is clamped to a minimum of zero to prevent it from becoming negative, which could happen if `logp_diff` is large and negative (i.e., the model is confidently wrong) and `alpha` is non-trivial. This ensures the loss always encourages `logp_a > logp_b` when `cost_a < cost_b` and prevents the optimization target from flipping, enhancing training stability.", "hyperparams": {"beta": 1.0, "max_margin": 2.0, "temperature": 5.0, "alpha": 0.1}, "operators_used": ["rank_gap", "tanh", "clamp", "logsigmoid"]}}}
{"generation": 9, "index": 6, "ir": {"name": "Confidence-Weighted Softplus-Hinge with Hybrid Normalization", "intuition": "This loss function combines the stability of z-score and rank-gap normalization with a dynamically weighted hinge-like loss. The margin is adaptive, based on the normalized cost difference, while the loss's penalty is scaled by the model's own confidence, focusing training on uncertain or misclassified pairs.\n\nInherited Ideas:\n- From both parents (Parent 0 and Parent 1), it inherits the robust pre-processing pipeline of first applying `zscore` to the batch costs and then computing the `rank_gap` on these normalized costs. This creates a highly stable signal of relative preference that is invariant to the scale, shift, and outliers in the original cost distribution.\n- From Parent 1, it inherits the use of `tanh` to create a smooth, bounded, and saturating adaptive margin from the normalized cost signal. This ensures the target separation (`adaptive_margin`) remains within a controlled range (`[-max_margin, max_margin]`).\n\nNew Coupling Ideas:\n1. **Softplus Hinge Loss Structure**: Instead of the `logsigmoid` formulation, this loss uses a direct `softplus` hinge-like structure: `softplus(adaptive_margin - logp_diff)`. This is mathematically equivalent to `-logsigmoid(logp_diff - adaptive_margin)` but frames the objective more directly as minimizing the violation of the margin `adaptive_margin` by the log-probability difference `logp_diff`.\n2. **Confidence-Based Loss Weighting**: The core `softplus` term is multiplied by a weight derived from the model's own confidence. The weight is calculated as `exp(-gamma * abs(logp_diff))`. This couples the loss magnitude to the model's certainty. When the model is very confident (large `abs(logp_diff)`), the weight becomes small, effectively down-weighting pairs where the model already has a strong opinion. Conversely, when the model is uncertain (small `abs(logp_diff)`), the weight is close to 1, focusing the training updates on these ambiguous cases. This acts as a form of self-paced learning.", "pseudocode": "1. Given costs for a batch, normalize them using z-scoring: z_cost_a = zscore(cost_a), z_cost_b = zscore(cost_b).\n2. Compute the rank-normalized gap between the z-scored costs to get a robust preference signal, typically in [-1, 1].\n   rank_diff = rank_gap(z_cost_a, z_cost_b).\n3. Create a smooth, bounded adaptive margin by passing the rank difference through a `tanh` function and scaling it.\n   adaptive_margin = max_margin * tanh(beta * rank_diff).\n4. Compute the log-probability difference: logp_diff = logp_a - logp_b.\n5. Calculate a confidence weight based on the absolute log-probability difference. The weight decreases as the model's confidence increases.\n   confidence_weight = exp(-gamma * abs(logp_diff)).\n6. Calculate the core hinge-like loss term using `softplus`. This term is positive only when the log-probability difference fails to meet the target adaptive margin.\n   hinge_term = softplus(adaptive_margin - logp_diff).\n7. The final loss is the hinge term multiplied by the confidence weight. This focuses training on pairs where the model is uncertain and the preference margin is violated.\n   loss = confidence_weight * hinge_term.", "hyperparams": {"beta": 1.0, "max_margin": 3.0, "gamma": 0.2}, "operators_used": ["zscore", "rank_gap", "tanh", "exp", "softplus"], "implementation_hint": {"expects": ["cost_a", "cost_b", "logp_a", "logp_b"], "returns": "scalar"}}, "fitness": {"hf_like_score": 3.8873223066329956, "validation_objective": 3.8873223066329956, "generalization_penalty": 0.0, "generalization_objectives": {"20": 3.8503847122192383}, "train_score_mean": 4.2101926654577255, "train_loss_mean": 0.677979975938797, "pair_count": 4112247, "config": {"hf": {"problem": "tsp", "hf_steps": 32, "train_problem_size": 20, "valid_problem_sizes": [20], "train_batch_size": 64, "pomo_size": 64, "learning_rate": 0.0003, "weight_decay": 1e-06, "alpha": 0.05, "device": "cuda", "seed": 1234, "num_validation_episodes": 128, "validation_batch_size": 64, "generalization_penalty_weight": 1.0, "pool_version": "v0"}, "free_loss": {"f1_steps": 32, "f2_steps": 100, "f3_enabled": false}}, "loss_ir": {"name": "Confidence-Weighted Softplus-Hinge with Hybrid Normalization", "intuition": "This loss function combines the stability of z-score and rank-gap normalization with a dynamically weighted hinge-like loss. The margin is adaptive, based on the normalized cost difference, while the loss's penalty is scaled by the model's own confidence, focusing training on uncertain or misclassified pairs.\n\nInherited Ideas:\n- From both parents (Parent 0 and Parent 1), it inherits the robust pre-processing pipeline of first applying `zscore` to the batch costs and then computing the `rank_gap` on these normalized costs. This creates a highly stable signal of relative preference that is invariant to the scale, shift, and outliers in the original cost distribution.\n- From Parent 1, it inherits the use of `tanh` to create a smooth, bounded, and saturating adaptive margin from the normalized cost signal. This ensures the target separation (`adaptive_margin`) remains within a controlled range (`[-max_margin, max_margin]`).\n\nNew Coupling Ideas:\n1. **Softplus Hinge Loss Structure**: Instead of the `logsigmoid` formulation, this loss uses a direct `softplus` hinge-like structure: `softplus(adaptive_margin - logp_diff)`. This is mathematically equivalent to `-logsigmoid(logp_diff - adaptive_margin)` but frames the objective more directly as minimizing the violation of the margin `adaptive_margin` by the log-probability difference `logp_diff`.\n2. **Confidence-Based Loss Weighting**: The core `softplus` term is multiplied by a weight derived from the model's own confidence. The weight is calculated as `exp(-gamma * abs(logp_diff))`. This couples the loss magnitude to the model's certainty. When the model is very confident (large `abs(logp_diff)`), the weight becomes small, effectively down-weighting pairs where the model already has a strong opinion. Conversely, when the model is uncertain (small `abs(logp_diff)`), the weight is close to 1, focusing the training updates on these ambiguous cases. This acts as a form of self-paced learning.", "hyperparams": {"beta": 1.0, "max_margin": 3.0, "gamma": 0.2}, "operators_used": ["zscore", "rank_gap", "tanh", "exp", "softplus"]}}}
{"generation": 9, "index": 7, "ir": {"name": "Confidence-Weighted Z-Score Tanh Loss", "intuition": "This loss function creates a robust margin by normalizing costs and then weights the loss signal based on the model's own confidence, effectively focusing updates on uncertain or misclassified pairs. The margin is designed to be stable and adaptive, while the loss weighting adds a dynamic, self-correcting element.\n\nInherited Ideas:\n- From **Parent 1 (Z-Scored Rank-Adaptive Hinge Loss)**, it inherits the core mechanism for creating a stable, adaptive margin. This involves first normalizing costs with a z-score (`zscore(costs)`) and then using the `tanh` function on the rank-gap of these normalized costs (`tanh(beta * rank_gap(z_costs))`). This combination ensures the margin is robust to the scale and distribution of costs and is smoothly bounded.\n- From both **Parent 0 and Parent 1**, it inherits the final loss structure of `-logsigmoid(temperature * (logp_diff - margin))`, which is numerically stable and behaves like a smoothed hinge loss, penalizing the model when the log-probability difference `logp_diff` fails to exceed the target `margin`.\n\nNew Coupling Ideas:\n1. **Confidence-Based Loss Attenuation**: The final loss value is multiplied by a weight derived from the model's own log-probabilities. The weight is calculated as `sigmoid(-gamma * logp_diff)`. When the model is very confident and correct (`logp_diff` is large and positive), the sigmoid term approaches 0, reducing the loss and preventing the model from becoming overconfident on easy examples. Conversely, when the model is wrong or uncertain (`logp_diff` is small or negative), the sigmoid term approaches 1, applying the full loss signal. This couples the loss magnitude directly to the model's certainty, focusing training on more informative pairs.\n2. **Dynamic Temperature Scaling**: The temperature, which controls the sharpness of the loss, is made dynamic. It is calculated as `base_temp + exp(logp_diff - adaptive_margin)`. When the model is already satisfying the margin (`logp_diff > adaptive_margin`), the exponential term is large, increasing the temperature and sharpening the loss to push for even better separation. When the model is failing the margin, the exponential term is small (approaching 0), keeping the temperature close to `base_temp`. This couples the loss's steepness to how well the model is currently performing relative to the target, creating a more aggressive gradient for well-separated pairs.", "pseudocode": "1. Given costs for a batch, normalize them using z-scoring: z_cost_a = zscore(cost_a), z_cost_b = zscore(cost_b).\n2. Compute the rank-normalized gap between the z-scored costs to get a robust preference signal: rank_diff = rank_gap(z_cost_a, z_cost_b).\n3. Create the adaptive margin by passing `rank_diff` through a tanh function and scaling it. This creates a smooth, bounded target.\n   adaptive_margin = max_margin * tanh(beta * rank_diff).\n4. Compute the log-probability difference: logp_diff = logp_a - logp_b.\n5. Calculate a dynamic temperature. It increases as the model's performance exceeds the margin.\n   dynamic_temp = base_temp + exp(logp_diff - adaptive_margin).\n6. Calculate the core loss term using the dynamic temperature.\n   core_term = dynamic_temp * (logp_diff - adaptive_margin).\n7. Compute the unweighted loss using a numerically stable logsigmoid function.\n   unweighted_loss = -logsigmoid(core_term).\n8. Calculate a confidence-based weight. The weight is close to 1 for uncertain/wrong predictions and close to 0 for confident/correct ones.\n   loss_weight = sigmoid(-gamma * logp_diff).\n9. The final loss is the unweighted loss multiplied by the confidence weight.\n   loss = loss_weight * unweighted_loss.", "hyperparams": {"beta": 1.0, "max_margin": 3.0, "base_temp": 2.0, "gamma": 0.5}, "operators_used": ["zscore", "rank_gap", "tanh", "exp", "sigmoid", "logsigmoid"], "implementation_hint": {"expects": ["cost_a", "cost_b", "logp_a", "logp_b"], "returns": "scalar"}}, "fitness": {"hf_like_score": 3.882310152053833, "validation_objective": 3.882310152053833, "generalization_penalty": 0.0, "generalization_objectives": {"20": 3.850818395614624}, "train_score_mean": 4.207873128354549, "train_loss_mean": 0.6783146187663078, "pair_count": 4111316, "config": {"hf": {"problem": "tsp", "hf_steps": 32, "train_problem_size": 20, "valid_problem_sizes": [20], "train_batch_size": 64, "pomo_size": 64, "learning_rate": 0.0003, "weight_decay": 1e-06, "alpha": 0.05, "device": "cuda", "seed": 1234, "num_validation_episodes": 128, "validation_batch_size": 64, "generalization_penalty_weight": 1.0, "pool_version": "v0"}, "free_loss": {"f1_steps": 32, "f2_steps": 100, "f3_enabled": false}}, "loss_ir": {"name": "Confidence-Weighted Z-Score Tanh Loss", "intuition": "This loss function creates a robust margin by normalizing costs and then weights the loss signal based on the model's own confidence, effectively focusing updates on uncertain or misclassified pairs. The margin is designed to be stable and adaptive, while the loss weighting adds a dynamic, self-correcting element.\n\nInherited Ideas:\n- From **Parent 1 (Z-Scored Rank-Adaptive Hinge Loss)**, it inherits the core mechanism for creating a stable, adaptive margin. This involves first normalizing costs with a z-score (`zscore(costs)`) and then using the `tanh` function on the rank-gap of these normalized costs (`tanh(beta * rank_gap(z_costs))`). This combination ensures the margin is robust to the scale and distribution of costs and is smoothly bounded.\n- From both **Parent 0 and Parent 1**, it inherits the final loss structure of `-logsigmoid(temperature * (logp_diff - margin))`, which is numerically stable and behaves like a smoothed hinge loss, penalizing the model when the log-probability difference `logp_diff` fails to exceed the target `margin`.\n\nNew Coupling Ideas:\n1. **Confidence-Based Loss Attenuation**: The final loss value is multiplied by a weight derived from the model's own log-probabilities. The weight is calculated as `sigmoid(-gamma * logp_diff)`. When the model is very confident and correct (`logp_diff` is large and positive), the sigmoid term approaches 0, reducing the loss and preventing the model from becoming overconfident on easy examples. Conversely, when the model is wrong or uncertain (`logp_diff` is small or negative), the sigmoid term approaches 1, applying the full loss signal. This couples the loss magnitude directly to the model's certainty, focusing training on more informative pairs.\n2. **Dynamic Temperature Scaling**: The temperature, which controls the sharpness of the loss, is made dynamic. It is calculated as `base_temp + exp(logp_diff - adaptive_margin)`. When the model is already satisfying the margin (`logp_diff > adaptive_margin`), the exponential term is large, increasing the temperature and sharpening the loss to push for even better separation. When the model is failing the margin, the exponential term is small (approaching 0), keeping the temperature close to `base_temp`. This couples the loss's steepness to how well the model is currently performing relative to the target, creating a more aggressive gradient for well-separated pairs.", "hyperparams": {"beta": 1.0, "max_margin": 3.0, "base_temp": 2.0, "gamma": 0.5}, "operators_used": ["zscore", "rank_gap", "tanh", "exp", "sigmoid", "logsigmoid"]}}}
