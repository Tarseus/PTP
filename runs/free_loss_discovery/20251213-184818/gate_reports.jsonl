{"generation": 0, "index": 0, "ir": {"name": "Adaptive Margin Log-Ratio Loss", "intuition": "This loss function conceptualizes the learning problem as aligning two different 'preference ratios': one derived from the model's log-probabilities and another from the ground-truth costs. It uses the log-sigmoid function to create a soft, bounded loss that measures the discrepancy between these two ratios. A key feature is an adaptive margin, which is a non-linear function of the cost difference, scaled by a hyperparameter. This margin becomes more forgiving for pairs with very similar costs (where the preference is ambiguous) and more demanding for pairs with large, clear cost differences. The use of tanh on the log-probability difference ensures that extreme model outputs don't dominate the gradient, leading to more stable training.", "pseudocode": "1. Calculate the cost difference: delta_cost = cost_b - cost_a.\n2. Calculate the log-probability difference: delta_logp = logp_a - logp_b.\n3. Compute an adaptive margin. Normalize the cost difference (e.g., using z-score or rank_gap) to get a stable scale. Pass this normalized difference through a softplus function to ensure positivity and create a non-linear margin that grows with the cost gap. Scale this by a hyperparameter 'alpha'.\n4. Stabilize the log-probability difference by applying a tanh function, scaled by a temperature 'tau'. This bounds the logit difference, preventing extreme values from causing instability.\n5. The core of the loss is log_sigmoid(margin - scaled_delta_logp). This encourages the scaled log-probability difference to be greater than the adaptive margin.\n6. Negate the result so that minimizing the loss corresponds to maximizing the argument of log_sigmoid.\n7. Apply optional sample weights and compute the mean over the batch.", "hyperparams": {"alpha": 1.0, "tau": 1.0}, "operators_used": ["logsigmoid", "softplus", "tanh", "zscore"], "implementation_hint": {"expects": ["cost_a", "cost_b", "logp_a", "logp_b"], "returns": "scalar"}, "code": "import torch\nimport torch.nn.functional as F\n\ndef zscore(x, eps=1e-8):\n    \"\"\"Numerically stable z-score normalization.\"\"\"\n    if x.numel() <= 1:\n        return torch.zeros_like(x)\n    mean = x.mean()\n    std = x.std()\n    return (x - mean) / (std + eps)\n\ndef generated_loss(batch, model_output, extra):\n    \"\"\"\n    Computes the Adaptive Margin Log-Ratio Loss.\n\n    The loss is based on aligning the model's preference (logp_a - logp_b)\n    with an adaptive margin derived from the cost difference.\n    \"\"\"\n    # Hyperparameters from the 'extra' dictionary\n    alpha = extra.get('alpha', 1.0)\n    tau = extra.get('tau', 1.0)\n\n    # Unpack tensors from the batch\n    # Note: The problem description uses (a, b) where cost(a) < cost(b) is preferred.\n    # The batch provides (log_prob_w, log_prob_l) where 'w' is the winner (lower cost)\n    # and 'l' is the loser (higher cost). So, we map:\n    # a -> w (winner), b -> l (loser)\n    # cost_a -> cost_w, cost_b -> cost_l\n    cost_w = batch['cost_a']\n    cost_l = batch['cost_b']\n    logp_w = batch['log_prob_w']\n    logp_l = batch['log_prob_l']\n\n    # 1. Calculate cost and log-probability differences\n    # delta_cost should be positive since cost_l > cost_w\n    delta_cost = cost_l - cost_w\n    # delta_logp is the model's preference score for the winner over the loser\n    delta_logp = logp_w - logp_l\n\n    # 2. Compute an adaptive margin based on the normalized cost difference\n    # zscore normalizes the cost gap across the batch, making the margin\n    # adaptive to the current batch's difficulty distribution.\n    normalized_delta_cost = zscore(delta_cost)\n    # softplus ensures the margin is non-negative and grows smoothly.\n    # alpha controls the magnitude of the margin.\n    margin = alpha * F.softplus(normalized_delta_cost)\n\n    # 3. Stabilize the log-probability difference\n    # tanh bounds the log-probability difference to [-1, 1], preventing\n    # extreme values from causing numerical instability or overly large gradients.\n    # tau acts as a temperature, controlling the sharpness of the tanh function.\n    stable_delta_logp = torch.tanh(delta_logp / tau)\n\n    # 4. Compute the core loss using log-sigmoid\n    # The loss encourages stable_delta_logp to be greater than the margin.\n    # loss = -log_sigmoid(X) is equivalent to softplus(-X).\n    # It's a convex, smooth loss that is large when X is very negative\n    # and approaches zero when X is very positive.\n    loss_per_sample = -F.logsigmoid(stable_delta_logp - margin)\n\n    # 5. Apply optional sample weights\n    weights = batch.get('weight', None)\n    if weights is not None:\n        loss_per_sample = loss_per_sample * weights\n\n    # 6. Return the mean loss over the batch\n    return loss_per_sample.mean()"}, "static_ok": true, "static_reason": "", "dynamic_ok": false, "dynamic_reason": "forward_error: name 'zscore' is not defined", "loss_value": null, "grad_norm": null}
{"generation": 0, "index": 1, "ir": {"name": "Adaptive Margin Sigmoid Loss with Cost-Gap Normalization", "intuition": "This loss adapts the learning signal based on the relative difficulty of the preference. It uses a sigmoid function to create a bounded loss, preventing extreme gradients from large log-probability differences. The margin, which determines the target separation between log-probabilities, is dynamically scaled by the normalized cost difference. This means that pairs with a large, obvious cost gap are expected to have a larger log-probability gap, while pairs with similar costs have a smaller target gap, focusing the model's capacity on resolving difficult, near-optimal trade-offs without being overly penalized for minor preference disagreements.", "pseudocode": "1. For a batch of pairs (a, b), calculate the cost difference `delta_cost = cost(b) - cost(a)`. Note: a is preferred, so delta_cost > 0.\n2. Normalize the cost differences across the batch to get a stable scaling factor, e.g., using z-score normalization followed by a sigmoid to map it to (0, 1). This gives `normalized_gap`.\n3. Compute an adaptive margin `margin = alpha * normalized_gap`, where `alpha` is a hyperparameter scaling the maximum margin.\n4. Calculate the log-probability difference `delta_logp = logp(a) - logp(b)`.\n5. The core of the loss is `logsigmoid(margin - delta_logp)`. This penalizes cases where `delta_logp` is less than the `margin`.\n6. The final loss is the negative of this value, making it a quantity to be minimized. The loss is high when `delta_logp` is much smaller than the adaptive `margin` and approaches zero as `delta_logp` surpasses it.", "hyperparams": {"alpha": {"value": 5.0, "description": "Scales the maximum margin, controlling the target separation in log-probabilities for pairs with the largest cost gaps."}, "tau": {"value": 1.0, "description": "Temperature parameter for scaling the log-probability difference before applying the loss."}, "eps": {"value": 1e-08, "description": "A small epsilon for numerical stability during normalization."}}, "operators_used": ["logsigmoid", "sigmoid", "zscore", "clamp"], "implementation_hint": {"expects": ["cost_a", "cost_b", "logp_a", "logp_b"], "returns": "scalar"}, "code": "import torch\nimport torch.nn.functional as F\n\ndef generated_loss(batch, model_output, extra):\n    \"\"\"\n    Computes the Adaptive Margin Sigmoid Loss.\n\n    This loss encourages the log-probability of the winning solution (w) to be greater\n    than that of the losing solution (l) by an adaptive margin. The margin is proportional\n    to the normalized cost difference between the two solutions.\n\n    Args:\n        batch (dict): A dictionary containing tensors.\n            - 'cost_l': Cost of the losing solution (higher is worse), shape [N].\n            - 'cost_w': Cost of the winning solution (lower is better), shape [N].\n            - 'log_prob_l': Log probability of the losing solution, shape [N].\n            - 'log_prob_w': Log probability of the winning solution, shape [N].\n            - 'weight' (optional): Per-pair loss weights, shape [N].\n        model_output (dict): Not used in this loss implementation.\n        extra (dict): A dictionary for hyperparameters, e.g., {'alpha': 5.0, 'tau': 1.0, 'eps': 1e-8}.\n\n    Returns:\n        torch.Tensor: A scalar tensor representing the mean loss.\n    \"\"\"\n    # For clarity and consistency with preference learning literature (e.g., DPO),\n    # let's define 'w' as the winner (preferred) and 'l' as the loser (not preferred).\n    # The input provides cost_a and cost_b. We must first establish which is the winner.\n    # Assume cost_a < cost_b implies a is preferred over b.\n    is_a_winner = batch['cost_a'] < batch['cost_b']\n\n    cost_w = torch.where(is_a_winner, batch['cost_a'], batch['cost_b'])\n    cost_l = torch.where(is_a_winner, batch['cost_b'], batch['cost_a'])\n    logp_w = torch.where(is_a_winner, batch['logp_a'], batch['logp_b'])\n    logp_l = torch.where(is_a_winner, batch['logp_b'], batch['logp_a'])\n\n    # Retrieve hyperparameters\n    alpha = extra.get('alpha', 5.0)\n    tau = extra.get('tau', 1.0)\n    eps = extra.get('eps', 1e-8)\n\n    # 1. Calculate the cost gap (always positive)\n    cost_gap = cost_l - cost_w\n\n    # 2. Normalize the cost gap to get a stable scaling factor.\n    # We use a simplified z-score like normalization, but without subtracting the mean\n    # to keep the gaps positive and then clamp to avoid extreme values.\n    # Using mean and std of the current batch for normalization.\n    if cost_gap.numel() > 1:\n        gap_std, gap_mean = torch.std_mean(cost_gap)\n        # z-score normalization function\n        normalized_gap = (cost_gap - gap_mean) / (gap_std + eps)\n    else:\n        # Handle batch size of 1 to avoid NaN from std=0\n        normalized_gap = torch.zeros_like(cost_gap)\n\n    # Use sigmoid to squash the normalized gap into (0, 1). This is a stable 'rank_gap' proxy.\n    # It ensures the margin is bounded and behaves well with outliers.\n    adaptive_scaler = torch.sigmoid(normalized_gap)\n\n    # 3. Compute the adaptive margin\n    margin = alpha * adaptive_scaler\n\n    # 4. Calculate the log-probability difference\n    delta_logp = logp_w - logp_l\n\n    # 5. Compute the core loss using logsigmoid for numerical stability.\n    # The argument is scaled by temperature `tau`.\n    # The loss is -log(sigmoid(delta_logp - margin)), which is equivalent to logsigmoid(margin - delta_logp).\n    # This form is numerically stable and penalizes cases where delta_logp < margin.\n    loss_per_item = -F.logsigmoid((delta_logp - margin) / tau)\n\n    # 6. Apply optional weights\n    weights = batch.get('weight', None)\n    if weights is not None:\n        loss_per_item = loss_per_item * weights\n\n    # 7. Return the mean loss\n    return loss_per_item.mean()"}, "static_ok": true, "static_reason": "", "dynamic_ok": false, "dynamic_reason": "forward_error: 'logp_a'", "loss_value": null, "grad_norm": null}
{"generation": 0, "index": 2, "ir": {"name": "Adaptive Margin Logit-Normalized Hinge Loss", "intuition": "This loss function combines a hinge-like structure with adaptive margins and logit normalization. The hinge loss focuses on misaligned preferences (when the model prefers the worse solution), creating a clear separation between 'correct' and 'incorrect' model predictions. The margin, which dictates how much better the model's preference should be, is not fixed but adapts to the normalized cost difference. This makes the learning signal stronger for pairs with large cost gaps and weaker for pairs with negligible differences. Furthermore, the log-probability difference is normalized using a tanh function, which squashes extreme values, preventing them from dominating the gradient and ensuring numerical stability. This creates a bounded, robust learning signal that focuses on correcting the most significant preference errors.", "pseudocode": "1. Calculate the cost difference: delta_cost = cost_b - cost_a. This is positive when 'a' is the better solution.\n2. Calculate the log-probability difference: delta_logp = logp_a - logp_b.\n3. Normalize the cost difference using z-scoring or another stable method to get normalized_delta_cost. This makes the margin scale-invariant.\n4. Create an adaptive margin based on the normalized cost difference. A softplus function ensures the margin is non-negative and grows smoothly: margin = softplus(normalized_delta_cost * beta).\n5. Normalize the log-probability difference using the tanh function to squash it into the [-1, 1] range: normalized_delta_logp = tanh(delta_logp * alpha).\n6. Formulate the hinge loss: loss = relu(margin - normalized_delta_logp). The loss is positive only when the normalized logp difference is smaller than the required margin.\n7. Apply optional sample weights and compute the mean over the batch.", "hyperparams": {"alpha": 1.0, "beta": 1.0}, "operators_used": ["softplus", "tanh", "relu", "zscore"], "implementation_hint": {"expects": ["cost_a", "cost_b", "logp_a", "logp_b"], "returns": "scalar"}, "code": "import torch\nimport torch.nn.functional as F\n\ndef zscore(x, eps=1e-8):\n    \"\"\"Stable Z-score normalization.\"\"\"\n    if x.numel() <= 1:\n        return torch.zeros_like(x)\n    mean = x.mean()\n    std = x.std()\n    return (x - mean) / (std + eps)\n\ndef generated_loss(batch, model_output, extra):\n    \"\"\"\n    Computes the Adaptive Margin Logit-Normalized Hinge Loss.\n\n    Args:\n        batch (dict): A dictionary containing tensors from the data loader.\n                      Expected keys: 'cost_a', 'cost_b'.\n        model_output (dict): A dictionary containing tensors from the model.\n                             Expected keys: 'log_prob_w', 'log_prob_l'.\n        extra (dict): A dictionary containing hyperparameters, e.g., 'alpha', 'beta'.\n\n    Returns:\n        torch.Tensor: A scalar tensor representing the mean loss.\n    \"\"\"\n    # Hyperparameters from the 'extra' dictionary\n    alpha = extra.get('alpha', 1.0)\n    beta = extra.get('beta', 1.0)\n\n    # For preference data, a is preferred to b. \n    # In RL settings, log_prob_w corresponds to the winner (preferred) and log_prob_l to the loser.\n    # We align this with cost: cost_a < cost_b, so 'a' is the winner.\n    cost_a = batch['cost_a']\n    cost_b = batch['cost_b']\n    logp_a = model_output['log_prob_w']\n    logp_b = model_output['log_prob_l']\n\n    # 1. Calculate cost and log-probability differences\n    # delta_cost > 0 means 'a' is better than 'b'\n    delta_cost = cost_b - cost_a\n    delta_logp = logp_a - logp_b\n\n    # 2. Normalize the cost difference for a stable adaptive margin\n    # Using z-score for normalization across the batch\n    with torch.no_grad(): # Don't backpropagate through normalization stats\n        normalized_delta_cost = zscore(delta_cost)\n\n    # 3. Compute the adaptive margin using softplus for non-negativity\n    # The margin increases with the significance of the cost difference\n    margin = F.softplus(normalized_delta_cost * beta)\n\n    # 4. Normalize the log-probability difference using tanh for stability\n    # This squashes extreme logit differences into a [-1, 1] range\n    normalized_delta_logp = torch.tanh(delta_logp * alpha)\n\n    # 5. Compute the core hinge loss\n    # Loss is positive only if normalized_delta_logp < margin\n    # This penalizes the model for not preferring the better solution 'a' by a large enough margin\n    loss_per_sample = F.relu(margin - normalized_delta_logp)\n\n    # 6. Apply optional sample weights if they exist\n    weights = batch.get('weight', None)\n    if weights is not None:\n        loss_per_sample = loss_per_sample * weights\n\n    # 7. Return the mean loss over the batch\n    return loss_per_sample.mean()"}, "static_ok": true, "static_reason": "", "dynamic_ok": false, "dynamic_reason": "forward_error: 'log_prob_w'", "loss_value": null, "grad_norm": null}
{"generation": 0, "index": 3, "ir": {"name": "Adaptive Margin Sigmoid Loss with Rank-Gap Normalization", "intuition": "This loss function uses a sigmoid-based structure, similar to a logistic or Bradley-Terry loss, but introduces an adaptive margin that is a function of the cost difference. The cost difference is normalized using a rank-gap transformation, which makes the margin robust to the scale and distribution of cost values. The rank-gap normalization converts absolute cost differences into a stable, relative measure (e.g., how many standard deviations better is solution 'a' than 'b'?). This prevents extreme cost gaps from creating excessively large margins, which could lead to exploding gradients or overly aggressive updates, while ensuring that significant cost differences still provide a strong learning signal. A softplus function is used to ensure the margin is always non-negative, and the final loss is a softplus of the margin-adjusted log-probability difference, ensuring numerical stability and a lower bound of zero.", "pseudocode": "1. For each pair (a, b), calculate the cost difference `delta_cost = cost(b) - cost(a)`. Note: A positive `delta_cost` means `a` is better.\n2. Normalize the `delta_cost` across the batch using the `rank_gap` operator. This yields a standardized, scale-invariant measure of the cost improvement.\n3. Compute an adaptive margin `m` by scaling the normalized cost difference with a hyperparameter `alpha`. Apply `softplus` to ensure the margin is non-negative and smooth.\n4. Calculate the log-probability difference `delta_logp = logp(a) - logp(b)`.\n5. The core of the loss is `m - delta_logp`. This term is positive when the model's preference `delta_logp` is smaller than the required margin `m`, indicating a need for an update.\n6. Apply a final `softplus` function to the result from step 5. This creates a smooth, non-negative loss that penalizes incorrect or weak preferences while saturating for correctly classified pairs, preventing overly confident predictions from dominating the loss.\n7. Compute the weighted mean of this value across the batch to get the final scalar loss.", "hyperparams": {"alpha": 1.0}, "operators_used": ["softplus", "rank_gap"], "implementation_hint": {"expects": ["cost_a", "cost_b", "logp_a", "logp_b"], "returns": "scalar"}, "code": "import torch\nimport torch.nn.functional as F\n\ndef rank_gap(x):\n    \"\"\"Helper function for rank-gap normalization.\"\"\"\n    # Ensure stability for constant inputs\n    if x.std() < 1e-9:\n        return torch.zeros_like(x)\n    return (x - x.mean()) / x.std()\n\ndef generated_loss(batch, model_output, extra):\n    \"\"\" \n    Adaptive Margin Sigmoid Loss with Rank-Gap Normalization.\n\n    This loss function computes a margin based on the rank-gap normalized cost\n    difference. The loss is then a softplus of the difference between this\n    adaptive margin and the log-probability difference.\n\n    Args:\n        batch (dict): A dictionary containing tensors.\n            'cost_a' (Tensor): Cost of the first solution in the pair.\n            'cost_b' (Tensor): Cost of the second solution in the pair.\n            'log_prob_w' (Tensor): Log probability of the winning solution (lower cost).\n            'log_prob_l' (Tensor): Log probability of the losing solution (higher cost).\n            'weight' (Tensor, optional): Per-sample weights.\n        model_output (dict): Not used in this loss, but part of the standard interface.\n        extra (dict): A dictionary for hyperparameters, e.g., {'alpha': 1.0}.\n\n    Returns:\n        torch.Tensor: A scalar loss value.\n    \"\"\"\n    # Hyperparameters\n    alpha = extra.get('alpha', 1.0)\n\n    # In the provided setup, (w, l) implies cost_w < cost_l.\n    # So, we map (w, l) to (a, b) where a is preferred.\n    cost_a = batch['cost_w']\n    cost_b = batch['cost_l']\n    logp_a = batch['log_prob_w']\n    logp_b = batch['log_prob_l']\n\n    # 1. Calculate the cost difference. Positive means 'a' is better than 'b'.\n    # delta_cost = cost(b) - cost(a)\n    delta_cost = cost_b - cost_a\n\n    # 2. Normalize the cost difference using rank_gap for stability.\n    # This makes the margin robust to the scale of costs.\n    normalized_delta_cost = rank_gap(delta_cost)\n\n    # 3. Compute an adaptive, non-negative margin.\n    # The softplus ensures the margin is always >= 0.\n    margin = F.softplus(alpha * normalized_delta_cost)\n\n    # 4. Calculate the log-probability difference.\n    delta_logp = logp_a - logp_b\n\n    # 5. Compute the core loss term: margin - delta_logp\n    # This encourages delta_logp to be at least as large as the margin.\n    loss_values = F.softplus(margin - delta_logp)\n\n    # 6. Apply weights if they exist.\n    weights = batch.get('weight', None)\n    if weights is not None:\n        loss_values = loss_values * weights\n\n    # 7. Return the mean loss over the batch.\n    return loss_values.mean()"}, "static_ok": true, "static_reason": "", "dynamic_ok": false, "dynamic_reason": "forward_error: 'cost_w'", "loss_value": null, "grad_norm": null}
{"generation": 0, "index": 4, "ir": {"name": "Adaptive Margin Log-Ratio Loss", "intuition": "This loss function uses a log-ratio of sigmoid-transformed probabilities, which provides a symmetric and bounded preference signal. It incorporates an adaptive margin based on the rank-normalized cost difference (rank_gap). This margin dynamically adjusts the learning target: for pairs with a large cost difference, the model is pushed more strongly to have a large log-probability difference, while for pairs with similar costs, the target difference is smaller. This avoids over-penalizing the model for small, potentially noisy cost differences. The use of tanh on the log-probability difference and softplus on the cost gap ensures numerical stability and prevents gradients from exploding with extreme inputs.", "pseudocode": "1. For each pair (a, b) in the batch, calculate the cost difference: delta_cost = cost(b) - cost(a).\n2. Calculate the log-probability difference: delta_logp = logp(a) - logp(b).\n3. Normalize the cost differences across the batch using a rank-based z-score (rank_gap) to get a stable, scaled measure of relative cost improvement.\n4. Create an adaptive margin, M, by applying a softplus function to the normalized cost gap, scaled by a hyperparameter 'beta'. This makes the margin non-negative and smooth.\n5. Compute the core preference signal as the difference between the tanh-squashed log-probability difference and the adaptive margin: signal = tanh(delta_logp / tau) - M.\n6. The final loss is the log-sigmoid of this signal, which is equivalent to a logistic loss. This penalizes cases where the signal is negative (i.e., when the model's preference does not exceed the required margin) and rewards positive signals.\n7. Apply optional sample weights and compute the mean over the batch.", "hyperparams": {"beta": 1.0, "tau": 1.0}, "operators_used": ["rank_gap", "softplus", "tanh", "logsigmoid"], "implementation_hint": {"expects": ["cost_a", "cost_b", "logp_a", "logp_b"], "returns": "scalar"}, "code": "import torch\nimport torch.nn.functional as F\n\ndef rank_gap(x, eps=1e-9):\n    \"\"\"Computes a rank-based z-score normalization for a 1D tensor.\"\"\"\n    # Sort the tensor and get the ranks\n    sorted_indices = torch.argsort(x)\n    ranks = torch.empty_like(sorted_indices, dtype=torch.float)\n    ranks[sorted_indices] = torch.arange(len(x), device=x.device, dtype=torch.float)\n    \n    # Convert ranks to a standard normal distribution (approx.)\n    ranks = (ranks / (len(x) + 1) - 0.5) * 2 # Map to [-1, 1]\n    \n    # Apply inverse error function to approximate z-scores\n    # torch.erfinv is a stable way to do this\n    z_scores = torch.erfinv(ranks) * (2**0.5)\n    \n    # Clamp to avoid potential Inf from erfinv at +/-1\n    z_scores = torch.clamp(z_scores, -5.0, 5.0)\n    return z_scores\n\ndef generated_loss(batch, model_output, extra):\n    \"\"\"\n    Computes the Adaptive Margin Log-Ratio Loss.\n\n    Args:\n        batch (dict): A dictionary containing tensors from the data loader.\n                      Expected keys: 'cost_a', 'cost_b'.\n        model_output (dict): A dictionary containing tensors from the model.\n                             Expected keys: 'log_prob_w', 'log_prob_l'.\n        extra (dict): A dictionary for hyperparameters, e.g., {'beta': 1.0, 'tau': 1.0}.\n\n    Returns:\n        torch.Tensor: A scalar tensor representing the mean loss.\n    \"\"\"\n    # Hyperparameters with default values\n    beta = extra.get('beta', 1.0)\n    tau = extra.get('tau', 1.0)\n\n    # For a pair (a,b) where cost(a) < cost(b), a is the winner (w) and b is the loser (l).\n    # The dataset provides log_prob_w and log_prob_l directly.\n    # We assume cost_w corresponds to log_prob_w and cost_l to log_prob_l.\n    # So, cost_w = batch['cost_a'] and cost_l = batch['cost_b'].\n    cost_w, cost_l = batch['cost_a'], batch['cost_b']\n    logp_w, logp_l = model_output['log_prob_w'], model_output['log_prob_l']\n\n    # Calculate cost and log-probability differences\n    # We want logp_w to be greater than logp_l, so the difference is logp_w - logp_l.\n    delta_logp = logp_w - logp_l\n    \n    # Cost difference, should be positive since cost_l > cost_w\n    delta_cost = cost_l - cost_w\n\n    # 1. Normalize the cost gap using a stable, rank-based method.\n    # This measures how significant the cost improvement is relative to other pairs in the batch.\n    # The result is centered around 0 with a standard deviation of ~1.\n    normalized_cost_gap = rank_gap(delta_cost)\n\n    # 2. Create an adaptive, non-negative margin from the normalized cost gap.\n    # softplus ensures the margin is always positive and smooth.\n    # beta scales the influence of the cost gap on the margin.\n    # A larger cost gap leads to a larger required margin for the log-probability difference.\n    margin = F.softplus(beta * normalized_cost_gap)\n\n    # 3. Squash the log-probability difference to be in [-1, 1] for stability.\n    # tau acts as a temperature parameter, controlling the sharpness of the tanh function.\n    squashed_delta_logp = torch.tanh(delta_logp / tau)\n\n    # 4. The core of the loss: compare the squashed logp difference against the adaptive margin.\n    # We want squashed_delta_logp to be greater than the margin.\n    # The loss is based on log(sigmoid(margin - squashed_delta_logp)), which is equivalent to\n    # -log(sigmoid(squashed_delta_logp - margin)) = -log_sigmoid(squashed_delta_logp - margin).\n    # This is a standard logistic loss formulation.\n    loss_per_sample = -F.logsigmoid(squashed_delta_logp - margin)\n\n    # 5. Apply optional sample weights if they are provided.\n    weights = batch.get('weight', None)\n    if weights is not None:\n        loss_per_sample = loss_per_sample * weights\n\n    # 6. Return the mean loss over the batch.\n    return loss_per_sample.mean()"}, "static_ok": true, "static_reason": "", "dynamic_ok": false, "dynamic_reason": "forward_error: 'log_prob_w'", "loss_value": null, "grad_norm": null}
{"generation": 0, "index": 5, "ir": {"name": "Adaptive Margin Hinge Loss with Rank-Gap Scaling", "intuition": "This loss function adapts the hinge loss concept for preference learning. It defines an 'ideal' log-probability difference that is proportional to the rank-normalized cost gap between two solutions. The loss penalizes the model only when its predicted log-probability difference falls short of this ideal margin. The use of rank-gap normalization makes the margin robust to the scale and distribution of costs, while a softplus function ensures the loss is smooth and non-negative. A tanh squashing function on the log-probability difference prevents extreme gradients from unstable log-probability predictions.", "pseudocode": "1. For a batch of solution pairs (a, b), calculate the cost difference `delta_cost = cost(b) - cost(a)`.\n2. Normalize `delta_cost` across the batch using a rank-gap transformation to get `normalized_delta_cost`. This represents the relative quality improvement of 'a' over 'b'.\n3. Calculate the log-probability difference `delta_logp = logp(a) - logp(b)`.\n4. Apply a tanh function to `delta_logp` to bound its value and stabilize gradients, resulting in `squashed_delta_logp`.\n5. Define an adaptive margin `margin = alpha * normalized_delta_cost`, where alpha is a hyperparameter scaling the margin's importance.\n6. The core loss is `softplus(margin - squashed_delta_logp)`. This penalizes cases where the model's preference `squashed_delta_logp` is less than the desired `margin`.\n7. If batch weights are provided, multiply the loss for each pair by its corresponding weight.\n8. The final loss is the mean of these weighted values over the batch.", "hyperparams": {"alpha": 1.0, "beta": 1.0}, "operators_used": ["rank_gap", "tanh", "softplus"], "implementation_hint": {"expects": ["cost_a", "cost_b", "logp_a", "logp_b", "weight"], "returns": "scalar"}, "code": "import torch\nimport torch.nn.functional as F\n\ndef rank_gap(x):\n    \"\"\"Numerically stable rank-based normalization for a tensor x.\"\"\"\n    if x.numel() <= 1:\n        return torch.zeros_like(x)\n    \n    # Sort the tensor and get the indices\n    sorted_x, _ = torch.sort(x)\n    \n    # Calculate ranks (average rank for ties)\n    ranks = torch.zeros_like(x)\n    unique_vals, inverse_indices, counts = torch.unique(sorted_x, return_inverse=True, return_counts=True)\n    cum_counts = torch.cat((torch.tensor([0], device=x.device), torch.cumsum(counts, 0)))\n    avg_ranks = (cum_counts[:-1] + cum_counts[1:] - 1) / 2.0\n    ranks = avg_ranks[inverse_indices]\n    \n    # Reorder ranks to match original tensor order\n    original_ranks = torch.zeros_like(x)\n    original_ranks[x.argsort()] = ranks\n\n    # Normalize ranks to [-0.5, 0.5]\n    n = x.numel()\n    normalized_ranks = (original_ranks - (n - 1) / 2.0) / n\n    return normalized_ranks\n\ndef generated_loss(batch, model_output, extra):\n    \"\"\"\n    Calculates the Adaptive Margin Hinge Loss with Rank-Gap Scaling.\n    \n    Note: The input `batch` is assumed to contain preferred (`_w` for winner) and\n    less-preferred (`_l` for loser) solutions based on cost. So, cost_w < cost_l.\n    We rename them to 'a' and 'b' for clarity, where 'a' is the better solution.\n    \"\"\"\n    # Unpack inputs, renaming for clarity (a is winner, b is loser)\n    cost_a = batch['cost_w']\n    cost_b = batch['cost_l']\n    logp_a = batch['log_prob_w']\n    logp_b = batch['log_prob_l']\n    weight = batch.get('weight', None)\n\n    # Retrieve hyperparameters\n    # alpha scales the margin, beta scales the logp difference\n    hyperparams = extra.get('hyperparams', {})\n    alpha = hyperparams.get('alpha', 1.0)\n    beta = hyperparams.get('beta', 1.0)\n\n    # 1. Calculate cost difference (should be > 0 since cost_a < cost_b)\n    delta_cost = cost_b - cost_a\n\n    # 2. Normalize the cost difference using rank-gap\n    # This makes the margin robust to the scale of costs\n    normalized_delta_cost = rank_gap(delta_cost)\n\n    # 3. Calculate log probability difference\n    delta_logp = logp_a - logp_b\n\n    # 4. Squash the log-probability difference to prevent extreme values\n    # This improves numerical stability for models producing large logits\n    squashed_delta_logp = torch.tanh(beta * delta_logp)\n\n    # 5. Define the adaptive margin\n    # The 'ideal' preference gap is proportional to the normalized cost gap\n    margin = alpha * normalized_delta_cost\n\n    # 6. Calculate the core hinge-like loss using softplus for smoothness\n    # Loss is incurred when squashed_delta_logp < margin\n    # softplus(x) = log(1 + exp(x)), which is a smooth version of relu(x)\n    losses = F.softplus(margin - squashed_delta_logp)\n\n    # 7. Apply optional instance-specific weights\n    if weight is not None:\n        losses = losses * weight.detach()\n\n    # 8. Return the mean loss over the batch\n    return losses.mean()"}, "static_ok": true, "static_reason": "", "dynamic_ok": false, "dynamic_reason": "forward_error: 'cost_w'", "loss_value": null, "grad_norm": null}
{"generation": 0, "index": 6, "ir": {"name": "Adaptive Margin Hinge Loss with Rank-Gap Scaling", "intuition": "This loss function conceptualizes preference learning as a margin-based classification task. It aims to separate preferred solutions (lower cost) from non-preferred ones in the log-probability space by a dynamic margin. The margin's size is determined by the normalized rank-gap of the cost difference, making the required separation larger for more significant differences in solution quality. This prevents the model from being overly penalized for small, noisy cost variations while enforcing strong separation for clear winners. A softplus function is used instead of a hard ReLU to create a smooth, non-zero gradient everywhere, avoiding dead neurons and promoting stable training. The loss is bounded due to the clamping of the rank-gap and the nature of softplus.", "pseudocode": "1. Calculate the cost difference: delta_cost = cost_b - cost_a.\n2. Calculate the log-probability difference: delta_logp = logp_a - logp_b.\n3. Compute a normalized rank-gap of the cost differences across the batch. Clamp this gap to a reasonable range [0, 1] to ensure stability. This becomes our adaptive scaling factor.\n4. Define an adaptive margin as a base margin hyperparameter multiplied by the scaled rank-gap. This means pairs with a larger quality gap must be separated by a larger margin in log-probability space.\n5. The core of the loss is a softplus hinge-like term: softplus(margin - delta_logp). This penalizes cases where the log-probability difference is smaller than the required adaptive margin.\n6. Take the element-wise product of this hinge loss with an optional sample weight.\n7. The final loss is the mean of these values over the batch.", "hyperparams": {"base_margin": 1.0, "margin_scale": 5.0, "rank_gap_clamp_min": 0.0, "rank_gap_clamp_max": 1.0}, "operators_used": ["softplus", "clamp", "rank_gap"], "implementation_hint": {"expects": ["cost_a", "cost_b", "logp_a", "logp_b"], "returns": "scalar"}, "code": "import torch\nimport torch.nn.functional as F\n\ndef rank_gap(x):\n    \"\"\"Calculates the rank-based gap of a tensor x.\"\"\"\n    if x.numel() <= 1:\n        return torch.zeros_like(x)\n    sorted_x, _ = torch.sort(x)\n    ranks = torch.empty_like(x)\n    ranks[torch.argsort(x)] = torch.arange(len(x), device=x.device, dtype=x.dtype)\n    \n    # Compute the average gap between sorted unique values\n    unique_sorted_x = torch.unique_consecutive(sorted_x)\n    if unique_sorted_x.numel() > 1:\n        avg_gap = (unique_sorted_x[-1] - unique_sorted_x[0]) / (unique_sorted_x.numel() - 1)\n    else:\n        avg_gap = 1.0 # Avoid division by zero if all values are the same\n        \n    # Normalize by the average gap, using a small epsilon for stability\n    normalized_gap = x / (avg_gap + 1e-8)\n    return normalized_gap\n\ndef generated_loss(batch, model_output, extra):\n    \"\"\"\n    Adaptive Margin Hinge Loss with Rank-Gap Scaling.\n\n    Assumes for each pair (a, b) in the batch, cost_a < cost_b.\n    The model should learn to make logp_a > logp_b.\n    \"\"\"\n    # Hyperparameters from the 'extra' dict, with defaults.\n    hyperparams = extra.get('hyperparams', {})\n    base_margin = hyperparams.get('base_margin', 1.0)\n    margin_scale = hyperparams.get('margin_scale', 5.0)\n    rank_gap_clamp_min = hyperparams.get('rank_gap_clamp_min', 0.0)\n    rank_gap_clamp_max = hyperparams.get('rank_gap_clamp_max', 1.0)\n\n    # Unpack data from the batch dictionary\n    # We assume the batch is structured such that cost_a is the winner (lower cost)\n    # and cost_b is the loser (higher cost).\n    cost_a, cost_b = batch['cost_a'], batch['cost_b']\n    logp_a, logp_b = batch['logp_a'], batch['logp_b']\n    weights = batch.get('weight', None)\n\n    # 1. Calculate cost and log-probability differences\n    # delta_cost should be positive since cost_b > cost_a (lower is better)\n    delta_cost = cost_b - cost_a\n    delta_logp = logp_a - logp_b\n\n    # 2. Compute a normalized, stable scaling factor based on the cost gap\n    # rank_gap normalizes the cost differences based on their ranks in the batch\n    # This makes the scaling robust to outliers in cost values.\n    with torch.no_grad(): # Don't backpropagate through the margin calculation\n        cost_gap_scaler = rank_gap(delta_cost)\n        # Clamp the scaler to prevent extreme margins\n        cost_gap_scaler = torch.clamp(\n            cost_gap_scaler,\n            min=rank_gap_clamp_min,\n            max=rank_gap_clamp_max\n        )\n\n    # 3. Define an adaptive margin\n    # The required separation in log-prob space is larger for pairs with a larger quality gap\n    adaptive_margin = base_margin + margin_scale * cost_gap_scaler\n\n    # 4. Compute the soft hinge loss\n    # The loss is softplus(margin - (logp_winner - logp_loser))\n    # This penalizes the model if logp_a is not sufficiently larger than logp_b\n    # softplus(x) = log(1 + exp(x)), which is a smooth approximation of relu(x)\n    # This ensures the gradient is always non-zero and avoids dead gradients.\n    losses = F.softplus(adaptive_margin - delta_logp)\n\n    # 5. Apply optional weights\n    if weights is not None:\n        losses = losses * weights\n\n    # 6. Return the mean loss for the batch\n    return losses.mean()"}, "static_ok": true, "static_reason": "", "dynamic_ok": false, "dynamic_reason": "forward_error: 'logp_a'", "loss_value": null, "grad_norm": null}
{"generation": 0, "index": 7, "ir": {"name": "Adaptive Sigmoid Margin Loss", "intuition": "This loss uses a sigmoid-shaped weighting function based on the normalized cost difference to adaptively scale the learning signal. For pairs with small cost differences, the loss behaves like a standard logistic loss, focusing on getting the preference direction right. For pairs with very large cost differences, the sigmoid saturates, preventing the gradient from becoming excessively large and dominating the learning process. This focuses the model on learning from 'hard' or 'subtle' pairs where the cost gap is not trivially large, while still correctly handling 'easy' pairs without being destabilized by them. An additional margin, also scaled by the cost difference, provides a clear separation target for the log-probability difference.", "pseudocode": "1. Calculate the cost difference: delta_cost = cost(b) - cost(a).\n2. Normalize the cost difference using a z-score or similar method to get 'normalized_delta_cost'.\n3. Calculate the log-probability difference: delta_logp = logp(a) - logp(b).\n4. Compute an adaptive weight using a sigmoid function on the normalized cost difference: weight = sigmoid(beta * normalized_delta_cost).\n5. Compute an adaptive margin based on the absolute cost difference, clamped for stability: margin = clamp(alpha * abs(delta_cost), 0, max_margin).\n6. The core of the loss is a softplus function (log(1+exp(x))) applied to the weighted and margined log-probability difference: loss = softplus(-sign(delta_cost) * delta_logp + margin).\n7. The final loss is the product of the adaptive weight and the core loss, averaged over the batch: mean(weight * loss).", "hyperparams": {"alpha": 0.5, "beta": 1.0, "max_margin": 5.0, "eps": 1e-08}, "operators_used": ["sigmoid", "softplus", "clamp"], "implementation_hint": {"expects": ["cost_a", "cost_b", "log_prob_w", "log_prob_l"], "returns": "scalar"}, "code": "import torch\nimport torch.nn.functional as F\n\ndef generated_loss(batch, model_output, extra):\n    \"\"\"\n    Adaptive Sigmoid Margin Loss.\n\n    This loss function applies a sigmoid weight based on the normalized cost difference\n    to modulate the learning signal. This prevents pairs with extremely large cost\n    differences from generating excessively large gradients, while still ensuring a\n    strong learning signal for pairs with meaningful cost gaps.\n\n    The preference `p(a > b)` is modeled based on `logp(a) - logp(b)`. The ground truth is that\n    the solution with the lower cost is preferred.\n\n    In the input, `log_prob_w` corresponds to the winner (lower cost) and `log_prob_l` to the loser.\n    Therefore, `cost_a` is the cost of the winner and `cost_b` is the cost of the loser, so cost_a < cost_b.\n    We want to encourage `log_prob_w > log_prob_l`.\n    \"\"\"\n    # Read hyperparameters from the 'extra' dictionary\n    # These could also be passed via a config system\n    alpha = extra.get('alpha', 0.5)\n    beta = extra.get('beta', 1.0)\n    max_margin = extra.get('max_margin', 5.0)\n    eps = extra.get('eps', 1e-8)\n\n    # Unpack tensors from the batch\n    # By convention, 'w' is winner (lower cost), 'l' is loser (higher cost)\n    cost_w = batch['cost_a']\n    cost_l = batch['cost_b']\n    logp_w = batch['log_prob_w']\n    logp_l = batch['log_prob_l']\n    \n    # Sanity check: cost_w should be less than cost_l\n    assert torch.all(cost_w <= cost_l), \"Assumption failed: cost_a (winner) must be <= cost_b (loser)\"\n\n    # 1. Calculate cost and log-probability differences\n    delta_cost = cost_l - cost_w  # This will be >= 0\n    delta_logp = logp_w - logp_l # We want this to be positive\n\n    # 2. Normalize the cost difference for stable sigmoid input\n    # z-score normalization: (x - mean) / std\n    # This makes the scaling distribution-dependent and robust to the scale of costs\n    if delta_cost.numel() > 1:\n        mean_delta_cost = delta_cost.mean()\n        std_delta_cost = delta_cost.std() + eps\n        normalized_delta_cost = (delta_cost - mean_delta_cost) / std_delta_cost\n    else:\n        normalized_delta_cost = torch.zeros_like(delta_cost)\n\n    # 3. Compute the adaptive margin\n    # The margin is proportional to the cost gap, encouraging a larger logp gap for larger cost gaps\n    # Clamping prevents the margin from becoming excessively large for outliers\n    margin = torch.clamp(alpha * delta_cost, min=0.0, max=max_margin)\n\n    # 4. Compute the core loss term (similar to a margin-based logistic loss)\n    # We want delta_logp to be positive, so the loss is high when delta_logp is negative.\n    # The term inside softplus is `margin - delta_logp`.\n    # softplus(x) = log(1 + exp(x)). This is a smooth, non-negative version of ReLU.\n    # Loss is low if delta_logp > margin, high otherwise.\n    core_loss = F.softplus(margin - delta_logp)\n\n    # 5. Compute the adaptive weight\n    # The sigmoid scales the loss. For pairs where the normalized cost difference is large and positive,\n    # the weight approaches 1. For pairs where the cost difference is near the mean, the weight is ~0.5.\n    # If a cost difference is much smaller than average, its weight is reduced.\n    adaptive_weight = torch.sigmoid(beta * normalized_delta_cost)\n\n    # 6. Combine core loss and adaptive weight\n    # The final loss for each pair is the product of its adaptive weight and its core loss.\n    # This focuses learning on pairs with significant (but not extreme) cost differences.\n    instance_loss = adaptive_weight * core_loss\n    \n    # Apply optional sample weights if provided\n    if 'weight' in batch and batch['weight'] is not None:\n        instance_loss = instance_loss * batch['weight']\n\n    # 7. Return the mean loss over the batch\n    return instance_loss.mean()"}, "static_ok": true, "static_reason": "", "dynamic_ok": true, "dynamic_reason": "ok", "loss_value": 0.4870385527610779, "grad_norm": 0.0}
{"generation": 1, "index": 0, "ir": {"name": "Adaptive Tanh Margin Loss", "intuition": "This loss function refines the parent's idea by replacing the sigmoid-based adaptive weight with a tanh-based one. The tanh function, which ranges from -1 to 1, provides a more nuanced scaling. It not only up-weights pairs with a large cost difference but also down-weights (or even negatively weights, which is clipped to zero) pairs with a cost difference smaller than the batch average. This focuses the model's attention more sharply on pairs that are 'harder' or more 'informative' according to the distribution of cost gaps in the batch. The core margin logic remains, where the target log-probability separation is proportional to the raw cost difference, ensuring that more distinct pairs are pushed further apart.", "pseudocode": "1. Calculate the cost difference: delta_cost = cost(b) - cost(a).\n2. Calculate the log-probability difference: delta_logp = logp(a) - logp(b).\n3. Normalize the cost difference using a z-score to get 'normalized_delta_cost'.\n4. Compute an adaptive weight using a scaled and shifted tanh function on the normalized cost difference: weight = relu(tanh(beta * normalized_delta_cost)). This focuses on pairs with above-average cost gaps.\n5. Compute an adaptive margin proportional to the absolute cost difference, clamped for stability: margin = clamp(alpha * delta_cost, 0, max_margin).\n6. The core loss is a softplus function applied to the margined log-probability difference: core_loss = softplus(margin - delta_logp).\n7. The final loss is the product of the adaptive weight and the core loss, averaged over the batch: mean(weight * core_loss).", "hyperparams": {"alpha": 0.5, "beta": 1.0, "max_margin": 5.0, "eps": 1e-08}, "operators_used": ["tanh", "relu", "softplus", "clamp", "zscore"], "implementation_hint": {"expects": ["cost_a", "cost_b", "logp_a", "logp_b", "weight"], "returns": "scalar"}, "code": "import torch\nimport torch.nn.functional as F\n\ndef zscore(x, eps=1e-8):\n    \"\"\"Calculates z-score for a tensor.\"\"\"\n    if x.numel() <= 1:\n        return torch.zeros_like(x)\n    mean = x.mean()\n    std = x.std() + eps\n    return (x - mean) / std\n\ndef generated_loss(batch, model_output, extra):\n    \"\"\"\n    Adaptive Tanh Margin Loss.\n\n    This loss function uses a tanh weight based on the z-scored cost difference\n    to modulate the learning signal. This focuses learning on pairs with an above-average\n    cost difference, effectively ignoring pairs that are much 'easier' than the batch average.\n    The preference `p(a > b)` is modeled based on `logp(a) - logp(b)`. The ground truth is that\n    the solution with the lower cost is preferred.\n\n    By convention, `log_prob_w` corresponds to the winner (lower cost) and `log_prob_l` to the loser.\n    Therefore, `cost_a` is the cost of the winner and `cost_b` is the cost of the loser, so cost_a < cost_b.\n    We want to encourage `log_prob_w > log_prob_l`.\n    \"\"\"\n    # Read hyperparameters from the 'extra' dictionary\n    alpha = extra.get('alpha', 0.5)\n    beta = extra.get('beta', 1.0)\n    max_margin = extra.get('max_margin', 5.0)\n    eps = extra.get('eps', 1e-8)\n\n    # Unpack tensors from the batch\n    cost_w = batch['cost_a']\n    cost_l = batch['cost_b']\n    logp_w = batch['log_prob_w']\n    logp_l = batch['log_prob_l']\n    \n    # Sanity check: cost_w should be less than cost_l\n    assert torch.all(cost_w <= cost_l), \"Assumption failed: cost_a (winner) must be <= cost_b (loser)\"\n\n    # 1. Calculate cost and log-probability differences\n    delta_cost = cost_l - cost_w  # This will be >= 0\n    delta_logp = logp_w - logp_l # We want this to be positive\n\n    # 2. Normalize the cost difference for stable tanh input\n    normalized_delta_cost = zscore(delta_cost, eps=eps)\n\n    # 3. Compute the adaptive margin\n    # The margin is proportional to the cost gap, encouraging a larger logp gap for larger cost gaps.\n    margin = torch.clamp(alpha * delta_cost, min=0.0, max=max_margin)\n\n    # 4. Compute the core loss term (margin-based logistic loss)\n    # We want delta_logp to be positive. Loss is high when delta_logp < margin.\n    core_loss = F.softplus(margin - delta_logp)\n\n    # 5. Compute the adaptive weight using tanh\n    # tanh(beta * z) scales from -1 to 1. By applying relu, we get a weight from 0 to 1.\n    # This aggressively focuses learning on pairs with an above-average cost difference (z > 0),\n    # while zeroing out the loss for pairs with a below-average cost difference (z < 0).\n    adaptive_weight = F.relu(torch.tanh(beta * normalized_delta_cost))\n\n    # 6. Combine core loss and adaptive weight\n    # The final loss for each pair is the product of its adaptive weight and its core loss.\n    instance_loss = adaptive_weight * core_loss\n    \n    # Apply optional sample weights if provided\n    if 'weight' in batch and batch['weight'] is not None:\n        instance_loss = instance_loss * batch['weight']\n\n    # 7. Return the mean loss over the batch\n    return instance_loss.mean()"}, "static_ok": true, "static_reason": "", "dynamic_ok": false, "dynamic_reason": "forward_error: name 'zscore' is not defined", "loss_value": null, "grad_norm": null}
{"generation": 1, "index": 1, "ir": {"name": "Adaptive Tanh Margin Loss", "intuition": "This loss function refines the parent's idea by replacing the sigmoid-based adaptive weight with a tanh-based one. The core remains a margin-based logistic loss. The margin is proportional to the cost difference, encouraging a larger log-probability gap for more distinct pairs. The key change is the weighting mechanism: it uses `tanh` on the normalized cost difference. `tanh` is symmetric around zero and spans from -1 to 1. By scaling and shifting it (`0.5 * (1 + tanh(...))`), we create a smooth weighting function from 0 to 1. This provides a more balanced gradient profile compared to sigmoid, as the gradients of tanh are more evenly distributed. This can lead to more stable learning by gently de-weighting pairs with cost differences far below the mean, while strongly weighting those at or above the mean.", "pseudocode": "1. Calculate the cost difference: delta_cost = cost(b) - cost(a).\n2. Normalize the cost difference using z-scoring to get 'normalized_delta_cost'.\n3. Calculate the log-probability difference: delta_logp = logp(a) - logp(b).\n4. Compute an adaptive margin proportional to the cost difference, clamped for stability: margin = clamp(alpha * delta_cost, 0, max_margin).\n5. Compute the core loss using a softplus function: core_loss = softplus(margin - delta_logp).\n6. Compute an adaptive weight using a scaled and shifted tanh function on the normalized cost difference: weight = 0.5 * (1 + tanh(beta * normalized_delta_cost)).\n7. The final loss is the product of the adaptive weight and the core loss, averaged over the batch: mean(weight * core_loss).", "hyperparams": {"alpha": 0.5, "beta": 1.0, "max_margin": 5.0, "eps": 1e-08}, "operators_used": ["tanh", "softplus", "clamp", "zscore"], "implementation_hint": {"expects": ["cost_a", "cost_b", "logp_a", "logp_b"], "returns": "scalar"}, "code": "import torch\nimport torch.nn.functional as F\n\ndef generated_loss(batch, model_output, extra):\n    \"\"\"\n    Adaptive Tanh Margin Loss.\n\n    This loss function uses a tanh-based weight on the normalized cost difference\n    to modulate the learning signal. This provides a smooth weighting that is symmetric\n    around the mean cost difference.\n\n    The preference `p(a > b)` is modeled based on `logp(a) - logp(b)`. The ground truth is that\n    the solution with the lower cost is preferred.\n\n    In the input, `log_prob_w` corresponds to the winner (lower cost) and `log_prob_l` to the loser.\n    Therefore, `cost_a` is the cost of the winner and `cost_b` is the cost of the loser, so cost_a < cost_b.\n    We want to encourage `log_prob_w > log_prob_l`.\n    \"\"\"\n    # Read hyperparameters from the 'extra' dictionary\n    alpha = extra.get('alpha', 0.5)\n    beta = extra.get('beta', 1.0)\n    max_margin = extra.get('max_margin', 5.0)\n    eps = extra.get('eps', 1e-8)\n\n    # Unpack tensors from the batch\n    # By convention, 'w' is winner (lower cost), 'l' is loser (higher cost)\n    cost_w = batch['cost_a']\n    cost_l = batch['cost_b']\n    logp_w = batch['log_prob_w']\n    logp_l = batch['log_prob_l']\n    \n    # Sanity check: cost_w should be less than or equal to cost_l\n    assert torch.all(cost_w <= cost_l), \"Assumption failed: cost_a (winner) must be <= cost_b (loser)\"\n\n    # 1. Calculate cost and log-probability differences\n    delta_cost = cost_l - cost_w  # This will be >= 0\n    delta_logp = logp_w - logp_l # We want this to be positive\n\n    # 2. Normalize the cost difference for stable tanh input (z-score)\n    if delta_cost.numel() > 1:\n        mean_delta_cost = delta_cost.mean()\n        std_delta_cost = delta_cost.std() + eps\n        normalized_delta_cost = (delta_cost - mean_delta_cost) / std_delta_cost\n    else:\n        normalized_delta_cost = torch.zeros_like(delta_cost)\n\n    # 3. Compute the adaptive margin\n    # The margin is proportional to the cost gap, encouraging a larger logp gap for larger cost gaps.\n    margin = torch.clamp(alpha * delta_cost, min=0.0, max=max_margin)\n\n    # 4. Compute the core loss term (margin-based logistic loss)\n    # The term inside softplus is `margin - delta_logp`.\n    # Loss is low if delta_logp > margin, high otherwise.\n    core_loss = F.softplus(margin - delta_logp)\n\n    # 5. Compute the adaptive weight using tanh\n    # tanh is symmetric around 0. We scale and shift it to map to a [0, 1] range.\n    # This provides a smooth weighting function similar to sigmoid but with different gradient properties.\n    adaptive_weight = 0.5 * (1.0 + torch.tanh(beta * normalized_delta_cost))\n\n    # 6. Combine core loss and adaptive weight\n    # The final loss for each pair is the product of its adaptive weight and its core loss.\n    instance_loss = adaptive_weight * core_loss\n    \n    # Apply optional sample weights if provided\n    if 'weight' in batch and batch['weight'] is not None:\n        instance_loss = instance_loss * batch['weight']\n\n    # 7. Return the mean loss over the batch\n    return instance_loss.mean()"}, "static_ok": true, "static_reason": "", "dynamic_ok": true, "dynamic_reason": "ok", "loss_value": 0.4870385527610779, "grad_norm": 0.0}
{"generation": 1, "index": 2, "ir": {"name": "Rank-Gap Scaled Sigmoid Loss", "intuition": "This loss function refines the parent's idea by replacing z-score normalization with a more direct, rank-based scaling. It uses the `rank_gap` operator to measure how much better the winning choice's cost is relative to other costs in the batch. This rank-based gap is then used to scale a sigmoid loss. The intuition is that pairs with a larger cost difference (and thus a larger rank gap) should contribute more to the loss, encouraging the model to focus on correctly ordering pairs that are clearly distinct. The sigmoid function ensures the loss remains bounded, preventing extreme outliers from dominating training. Unlike the parent, this version drops the explicit margin term, simplifying the formulation to a scaled sigmoid loss.", "pseudocode": "1. Calculate the cost difference: delta_cost = cost(b) - cost(a).\n2. Calculate the log-probability difference: delta_logp = logp(a) - logp(b).\n3. Calculate the rank-based gap for the cost difference: cost_rank_gap = rank_gap(delta_cost).\n4. Compute the core loss using a sigmoid function: loss = 2 * sigmoid(-delta_logp). This is equivalent to `1 + tanh(-delta_logp/2)`, a bounded logistic-style loss.\n5. Scale the core loss by the cost rank gap: scaled_loss = cost_rank_gap * loss.\n6. Average the scaled loss over the batch.", "hyperparams": {"eps": 1e-08}, "operators_used": ["sigmoid", "rank_gap"], "implementation_hint": {"expects": ["cost_a", "cost_b", "log_prob_w", "log_prob_l", "weight"], "returns": "scalar"}, "code": "import torch\nimport torch.nn.functional as F\n\ndef rank_gap(x, eps=1e-8):\n    \"\"\"Computes the gap between each element and the max, scaled by the total range.\n    Result is in [0, 1]. Larger values of x get smaller rank_gap.\n    Here, we want larger delta_cost to have a larger weight, so we return 1.0 - rank_gap.\n    \"\"\"\n    if x.numel() <= 1:\n        return torch.ones_like(x)\n    \n    # Sort the tensor to find ranks\n    sorted_x, _ = torch.sort(x, descending=True)\n    min_x, max_x = sorted_x[-1], sorted_x[0]\n    \n    # Calculate the gap scaled by the range\n    # This gives a value in [0, 1] where max_x maps to 0 and min_x maps to 1\n    scaled_gap = (max_x - x) / (max_x - min_x + eps)\n\n    # We want a high weight for a large cost difference (low scaled_gap), so we invert it.\n    return 1.0 - scaled_gap\n\ndef generated_loss(batch, model_output, extra):\n    \"\"\"\n    Rank-Gap Scaled Sigmoid Loss.\n\n    This loss function uses a rank-based scaling of the cost difference to modulate\n    a sigmoid loss. The `rank_gap` function provides a non-parametric, robust measure\n    of how significant the cost difference is within the current batch. This avoids\n    the need for z-score normalization and associated hyperparameters like beta.\n\n    By convention, `log_prob_w` corresponds to the winner (lower cost) and `log_prob_l` to the loser.\n    We want to encourage `log_prob_w > log_prob_l`.\n    \"\"\"\n    # Read hyperparameters\n    eps = extra.get('eps', 1e-8)\n\n    # Unpack tensors from the batch\n    cost_w = batch['cost_a']\n    cost_l = batch['cost_b']\n    logp_w = batch['log_prob_w']\n    logp_l = batch['log_prob_l']\n    \n    # Sanity check: cost_w should be less than cost_l\n    assert torch.all(cost_w <= cost_l), \"Assumption failed: cost_a (winner) must be <= cost_b (loser)\"\n\n    # 1. Calculate cost and log-probability differences\n    delta_cost = cost_l - cost_w  # This will be >= 0\n    delta_logp = logp_w - logp_l # We want this to be positive\n\n    # 2. Calculate the rank-based weight for the cost difference\n    # A larger delta_cost should result in a larger weight.\n    cost_weight = rank_gap(delta_cost, eps=eps)\n\n    # 3. Compute the core loss term using a sigmoid function\n    # This is a bounded version of the logistic loss, encouraging delta_logp to be positive.\n    # `2 * sigmoid(-x)` is a smooth loss that is 2 when x -> -inf, 1 when x = 0, and 0 when x -> +inf.\n    # It is equivalent to `1 + tanh(-x/2)`.\n    core_loss = 2 * torch.sigmoid(-delta_logp)\n\n    # 4. Combine core loss and rank-based weight\n    # The final loss for each pair is the product of its rank-based weight and its core loss.\n    instance_loss = cost_weight * core_loss\n    \n    # Apply optional sample weights if provided\n    if 'weight' in batch and batch['weight'] is not None:\n        instance_loss = instance_loss * batch['weight']\n\n    # 5. Return the mean loss over the batch\n    return instance_loss.mean()"}, "static_ok": true, "static_reason": "", "dynamic_ok": false, "dynamic_reason": "forward_error: name 'rank_gap' is not defined", "loss_value": null, "grad_norm": null}
{"generation": 1, "index": 3, "ir": {"name": "Softplus-Weighted Adaptive Margin Loss", "intuition": "This loss modifies the parent's sigmoid-based weighting to use a softplus function instead. The sigmoid function saturates for large cost differences, effectively capping the influence of 'easy' pairs. By contrast, a softplus weight (`softplus(normalized_delta_cost)`) continues to grow (though linearly for large inputs), ensuring that pairs with larger cost differences always contribute more to the loss, but without the explosive growth of a pure linear or exponential weighting. This provides a smoother, non-saturating emphasis on more significant cost gaps. The core logic of an adaptive margin proportional to the cost difference is retained to encourage a separation in log-probabilities that reflects the magnitude of the performance gap.", "pseudocode": "1. Calculate the cost difference: delta_cost = cost(b) - cost(a).\n2. Normalize the cost difference using a z-score to get 'normalized_delta_cost'.\n3. Calculate the log-probability difference: delta_logp = logp(a) - logp(b).\n4. Compute an adaptive weight using a softplus function on the normalized cost difference: weight = softplus(beta * normalized_delta_cost).\n5. Compute an adaptive margin based on the absolute cost difference, clamped for stability: margin = clamp(alpha * delta_cost, 0, max_margin).\n6. The core of the loss is a softplus function applied to the margined log-probability difference: core_loss = softplus(margin - delta_logp).\n7. The final loss is the product of the adaptive weight and the core loss, averaged over the batch: mean(weight * core_loss).", "hyperparams": {"alpha": 0.5, "beta": 1.0, "max_margin": 5.0, "eps": 1e-08}, "operators_used": ["softplus", "clamp", "zscore"], "implementation_hint": {"expects": ["cost_a", "cost_b", "logp_a", "logp_b"], "returns": "scalar"}, "code": "import torch\nimport torch.nn.functional as F\n\ndef generated_loss(batch, model_output, extra):\n    \"\"\"\n    Softplus-Weighted Adaptive Margin Loss.\n\n    This loss function uses a softplus weight based on the normalized cost difference\n    to modulate the learning signal. Unlike a sigmoid, softplus does not saturate for large\n    positive inputs, allowing pairs with larger cost differences to have a proportionally\n    larger influence on the final loss, but in a smooth, non-explosive manner.\n\n    The preference `p(a > b)` is modeled based on `logp(a) - logp(b)`. The ground truth is that\n    the solution with the lower cost is preferred.\n\n    In the input, `log_prob_w` corresponds to the winner (lower cost) and `log_prob_l` to the loser.\n    Therefore, `cost_a` is the cost of the winner and `cost_b` is the cost of the loser, so cost_a < cost_b.\n    We want to encourage `log_prob_w > log_prob_l`.\n    \"\"\"\n    # Read hyperparameters from the 'extra' dictionary\n    alpha = extra.get('alpha', 0.5)\n    beta = extra.get('beta', 1.0)\n    max_margin = extra.get('max_margin', 5.0)\n    eps = extra.get('eps', 1e-8)\n\n    # Unpack tensors from the batch\n    # By convention, 'w' is winner (lower cost), 'l' is loser (higher cost)\n    cost_w = batch['cost_a']\n    cost_l = batch['cost_b']\n    logp_w = batch['log_prob_w']\n    logp_l = batch['log_prob_l']\n    \n    # Sanity check: cost_w should be less than cost_l\n    assert torch.all(cost_w <= cost_l), \"Assumption failed: cost_a (winner) must be <= cost_b (loser)\"\n\n    # 1. Calculate cost and log-probability differences\n    delta_cost = cost_l - cost_w  # This will be >= 0\n    delta_logp = logp_w - logp_l # We want this to be positive\n\n    # 2. Normalize the cost difference for stable weighting input (z-score)\n    if delta_cost.numel() > 1:\n        mean_delta_cost = delta_cost.mean()\n        std_delta_cost = delta_cost.std() + eps\n        normalized_delta_cost = (delta_cost - mean_delta_cost) / std_delta_cost\n    else:\n        normalized_delta_cost = torch.zeros_like(delta_cost)\n\n    # 3. Compute the adaptive margin\n    # The margin is proportional to the cost gap, encouraging a larger logp gap for larger cost gaps.\n    margin = torch.clamp(alpha * delta_cost, min=0.0, max=max_margin)\n\n    # 4. Compute the core loss term (similar to a margin-based logistic loss)\n    # We want delta_logp to be positive, so the loss is high when delta_logp is negative.\n    # The term inside softplus is `margin - delta_logp`.\n    core_loss = F.softplus(margin - delta_logp)\n\n    # 5. Compute the adaptive weight using softplus instead of sigmoid\n    # The softplus function provides a non-saturating weight for larger cost differences.\n    # This gives more importance to pairs with a greater cost gap.\n    adaptive_weight = F.softplus(beta * normalized_delta_cost)\n\n    # 6. Combine core loss and adaptive weight\n    # The final loss for each pair is the product of its adaptive weight and its core loss.\n    instance_loss = adaptive_weight * core_loss\n    \n    # Apply optional sample weights if provided\n    if 'weight' in batch and batch['weight'] is not None:\n        instance_loss = instance_loss * batch['weight']\n\n    # 7. Return the mean loss over the batch\n    return instance_loss.mean()"}, "static_ok": true, "static_reason": "", "dynamic_ok": true, "dynamic_reason": "ok", "loss_value": 0.6751786470413208, "grad_norm": 0.0}
{"generation": 1, "index": 4, "ir": {"name": "Adaptive Rank-Gap Margin Loss", "intuition": "This loss function adapts the learning signal based on the rank of the cost difference within the batch, rather than its raw value. It uses a rank-based margin to ensure that pairs with a larger cost gap (relative to others in the batch) are pushed further apart in log-probability space. The core loss is a standard logistic loss on the difference between the log-probability gap and this rank-based margin. This approach is robust to the absolute scale of costs and focuses the model on preserving the relative preference ordering, preventing outliers with huge cost gaps from dominating the training process.", "pseudocode": "1. Calculate the cost difference: delta_cost = cost(b) - cost(a).\n2. Calculate the log-probability difference: delta_logp = logp(a) - logp(b).\n3. Compute a rank-based margin using the `rank_gap` operator on the cost differences. This maps the cost differences to a smooth, normalized range (e.g., [0, 1]).\n4. Scale this normalized rank by a hyperparameter `alpha` to set the target margin: margin = alpha * rank_gap(delta_cost).\n5. The loss is a softplus function (a smooth version of hinge loss) applied to the difference between the margin and the log-probability gap: loss = softplus(margin - delta_logp).\n6. The final loss is the mean of this value over the batch.", "hyperparams": {"alpha": 2.0, "eps": 1e-08}, "operators_used": ["softplus", "rank_gap"], "implementation_hint": {"expects": ["cost_a", "cost_b", "log_prob_w", "log_prob_l", "weight"], "returns": "scalar"}, "code": "import torch\nimport torch.nn.functional as F\n\ndef rank_gap(x, eps=1e-8):\n    \"\"\"Computes the rank-based gap for a batch of values.\"\"\"\n    if x.numel() <= 1:\n        return torch.zeros_like(x)\n    # Get the rank of each element\n    # The smallest element gets rank 0, the largest gets rank N-1\n    ranks = torch.argsort(torch.argsort(x)).float()\n    # Normalize ranks to be in [0, 1]\n    normalized_ranks = ranks / (x.numel() - 1 + eps)\n    return normalized_ranks\n\ndef generated_loss(batch, model_output, extra):\n    \"\"\"\n    Adaptive Rank-Gap Margin Loss.\n\n    This loss function uses a margin that is proportional to the rank of the cost\n    difference within the batch. This makes the loss robust to the scale of costs\n    and focuses on relative improvements.\n\n    The preference `p(a > b)` is modeled based on `logp(a) - logp(b)`. The ground truth is that\n    the solution with the lower cost is preferred.\n\n    In the input, `log_prob_w` corresponds to the winner (lower cost) and `log_prob_l` to the loser.\n    Therefore, `cost_a` is the cost of the winner and `cost_b` is the cost of the loser, so cost_a < cost_b.\n    We want to encourage `log_prob_w > log_prob_l`.\n    \"\"\"\n    # Read hyperparameters\n    alpha = extra.get('alpha', 2.0)\n    eps = extra.get('eps', 1e-8)\n\n    # Unpack tensors from the batch\n    # By convention, 'w' is winner (lower cost), 'l' is loser (higher cost)\n    cost_w = batch['cost_a']\n    cost_l = batch['cost_b']\n    logp_w = batch['log_prob_w']\n    logp_l = batch['log_prob_l']\n    \n    # Sanity check: cost_w should be less than or equal to cost_l\n    assert torch.all(cost_w <= cost_l), \"Assumption failed: cost_a (winner) must be <= cost_b (loser)\"\n\n    # 1. Calculate cost and log-probability differences\n    delta_cost = cost_l - cost_w  # This will be >= 0\n    delta_logp = logp_w - logp_l # We want this to be positive\n\n    # 2. Compute a rank-based margin\n    # The rank_gap function maps cost differences to a normalized scale [0, 1]\n    # based on their relative ordering in the batch. This is robust to outliers.\n    cost_rank_gap = rank_gap(delta_cost, eps=eps)\n    margin = alpha * cost_rank_gap\n\n    # 3. Compute the core loss term (logistic loss with a margin)\n    # We want delta_logp to be greater than the margin.\n    # The loss is high when delta_logp is smaller than the margin.\n    # softplus(x) = log(1 + exp(x)), a smooth version of max(0, x).\n    instance_loss = F.softplus(margin - delta_logp)\n\n    # 4. Apply optional sample weights if provided\n    if 'weight' in batch and batch['weight'] is not None:\n        instance_loss = instance_loss * batch['weight']\n\n    # 5. Return the mean loss over the batch\n    return instance_loss.mean()"}, "static_ok": true, "static_reason": "", "dynamic_ok": false, "dynamic_reason": "forward_error: name 'rank_gap' is not defined", "loss_value": null, "grad_norm": null}
{"generation": 1, "index": 5, "ir": {"name": "Adaptive Sigmoid Margin Loss with Tanh Margin", "intuition": "This loss uses a sigmoid-shaped weighting function based on the normalized cost difference to adaptively scale the learning signal, focusing on 'subtle' pairs. It introduces a margin that is a non-linear, saturating function of the cost difference, using `tanh`. This prevents the margin from growing indefinitely with the cost gap, which can cause instability or overconfidence. The `tanh` function provides a smooth, bounded margin that quickly increases for small cost gaps but then levels off, ensuring a reasonable separation target for all pairs without being overly sensitive to outliers with huge cost differences.", "pseudocode": "1. Calculate the cost difference: delta_cost = cost(b) - cost(a).\n2. Normalize the cost difference using a z-score to get 'normalized_delta_cost'.\n3. Calculate the log-probability difference: delta_logp = logp(a) - logp(b).\n4. Compute an adaptive weight using a sigmoid function on the normalized cost difference: weight = sigmoid(beta * normalized_delta_cost).\n5. Compute a bounded, adaptive margin using a tanh function on the absolute cost difference: margin = max_margin * tanh(alpha * abs(delta_cost)).\n6. The core of the loss is a softplus function applied to the margined log-probability difference: loss = softplus(margin - delta_logp).\n7. The final loss is the product of the adaptive weight and the core loss, averaged over the batch: mean(weight * loss).", "hyperparams": {"alpha": 0.5, "beta": 1.0, "max_margin": 5.0, "eps": 1e-08}, "operators_used": ["sigmoid", "softplus", "tanh"], "implementation_hint": {"expects": ["cost_a", "cost_b", "log_prob_w", "log_prob_l", "weight"], "returns": "scalar"}, "code": "import torch\nimport torch.nn.functional as F\n\ndef generated_loss(batch, model_output, extra):\n    \"\"\"\n    Adaptive Sigmoid Margin Loss with a Tanh-based margin.\n\n    This loss function applies a sigmoid weight based on the normalized cost difference\n    to modulate the learning signal. The margin is now a saturating function of the\n    cost difference, using tanh, which prevents it from growing excessively large.\n\n    The preference `p(a > b)` is modeled based on `logp(a) - logp(b)`. The ground truth is that\n    the solution with the lower cost is preferred.\n\n    In the input, `log_prob_w` corresponds to the winner (lower cost) and `log_prob_l` to the loser.\n    Therefore, `cost_a` is the cost of the winner and `cost_b` is the cost of the loser, so cost_a < cost_b.\n    We want to encourage `log_prob_w > log_prob_l`.\n    \"\"\"\n    # Read hyperparameters from the 'extra' dictionary\n    alpha = extra.get('alpha', 0.5)\n    beta = extra.get('beta', 1.0)\n    max_margin = extra.get('max_margin', 5.0)\n    eps = extra.get('eps', 1e-8)\n\n    # Unpack tensors from the batch\n    # By convention, 'w' is winner (lower cost), 'l' is loser (higher cost)\n    cost_w = batch['cost_a']\n    cost_l = batch['cost_b']\n    logp_w = batch['log_prob_w']\n    logp_l = batch['log_prob_l']\n    \n    # Sanity check: cost_w should be less than or equal to cost_l\n    assert torch.all(cost_w <= cost_l), \"Assumption failed: cost_a (winner) must be <= cost_b (loser)\"\n\n    # 1. Calculate cost and log-probability differences\n    delta_cost = cost_l - cost_w  # This will be >= 0\n    delta_logp = logp_w - logp_l # We want this to be positive\n\n    # 2. Normalize the cost difference for stable sigmoid input\n    # z-score normalization: (x - mean) / std\n    if delta_cost.numel() > 1:\n        mean_delta_cost = delta_cost.mean()\n        std_delta_cost = delta_cost.std() + eps\n        normalized_delta_cost = (delta_cost - mean_delta_cost) / std_delta_cost\n    else:\n        normalized_delta_cost = torch.zeros_like(delta_cost)\n\n    # 3. Compute the adaptive margin using tanh for smooth saturation\n    # The margin grows with the cost gap but is bounded by max_margin.\n    # tanh(x) is in [-1, 1], so tanh(alpha * delta_cost) is in [0, 1] for non-negative delta_cost.\n    margin = max_margin * torch.tanh(alpha * delta_cost)\n\n    # 4. Compute the core loss term (similar to a margin-based logistic loss)\n    # We want delta_logp to be positive, so the loss is high when delta_logp is negative.\n    # The term inside softplus is `margin - delta_logp`.\n    # softplus(x) = log(1 + exp(x)). This is a smooth, non-negative version of ReLU.\n    # Loss is low if delta_logp > margin, high otherwise.\n    core_loss = F.softplus(margin - delta_logp)\n\n    # 5. Compute the adaptive weight\n    # The sigmoid scales the loss. For pairs where the normalized cost difference is large and positive,\n    # the weight approaches 1. For pairs where the cost difference is near the mean, the weight is ~0.5.\n    adaptive_weight = torch.sigmoid(beta * normalized_delta_cost)\n\n    # 6. Combine core loss and adaptive weight\n    # The final loss for each pair is the product of its adaptive weight and its core loss.\n    instance_loss = adaptive_weight * core_loss\n    \n    # Apply optional sample weights if provided\n    if 'weight' in batch and batch['weight'] is not None:\n        instance_loss = instance_loss * batch['weight']\n\n    # 7. Return the mean loss over the batch\n    return instance_loss.mean()"}, "static_ok": true, "static_reason": "", "dynamic_ok": true, "dynamic_reason": "ok", "loss_value": 1.2025855779647827, "grad_norm": 0.0}
{"generation": 1, "index": 6, "ir": {"name": "Softplus Weighted Margin Loss", "intuition": "This loss function refines the parent's idea of adaptive weighting. Instead of using a sigmoid function on a z-scored cost difference, it uses a softplus function directly on the raw cost difference. This provides a smoother, non-saturating weight that increases monotonically with the cost gap, giving more importance to pairs that are more clearly separated in quality. The z-score normalization is removed to make the weighting more direct and less dependent on batch statistics, which can be noisy. The margin remains proportional to the cost difference, ensuring that the model is pushed to create a larger log-probability gap for pairs with a larger true quality difference.", "pseudocode": "1. Calculate the cost difference: delta_cost = cost(b) - cost(a).\n2. Calculate the log-probability difference: delta_logp = logp(a) - logp(b).\n3. Compute an adaptive weight using a softplus function on the cost difference: weight = softplus(beta * delta_cost).\n4. Compute an adaptive margin based on the absolute cost difference, clamped for stability: margin = clamp(alpha * abs(delta_cost), 0, max_margin).\n5. The core of the loss is a softplus function applied to the margined log-probability difference: loss_term = softplus(margin - delta_logp).\n6. The final loss is the product of the adaptive weight and the core loss term, averaged over the batch: mean(weight * loss_term).", "hyperparams": {"alpha": 0.5, "beta": 0.1, "max_margin": 5.0}, "operators_used": ["softplus", "clamp"], "implementation_hint": {"expects": ["cost_a", "cost_b", "log_prob_w", "log_prob_l", "weight"], "returns": "scalar"}, "code": "import torch\nimport torch.nn.functional as F\n\ndef generated_loss(batch, model_output, extra):\n    \"\"\"\n    Softplus Weighted Margin Loss.\n\n    This loss function uses a softplus weight based on the raw cost difference\n    to monotonically increase the importance of pairs with larger cost gaps.\n    This avoids the saturation of sigmoid and the batch-dependency of z-scoring.\n\n    The preference `p(a > b)` is modeled based on `logp(a) - logp(b)`. The ground truth is that\n    the solution with the lower cost is preferred.\n\n    In the input, `log_prob_w` corresponds to the winner (lower cost) and `log_prob_l` to the loser.\n    Therefore, `cost_a` is the cost of the winner and `cost_b` is the cost of the loser, so cost_a < cost_b.\n    We want to encourage `log_prob_w > log_prob_l`.\n    \"\"\"\n    # Read hyperparameters from the 'extra' dictionary\n    alpha = extra.get('alpha', 0.5)\n    beta = extra.get('beta', 0.1)\n    max_margin = extra.get('max_margin', 5.0)\n\n    # Unpack tensors from the batch\n    # By convention, 'w' is winner (lower cost), 'l' is loser (higher cost)\n    cost_w = batch['cost_a']\n    cost_l = batch['cost_b']\n    logp_w = batch['log_prob_w']\n    logp_l = batch['log_prob_l']\n    \n    # Sanity check: cost_w should be less than or equal to cost_l\n    assert torch.all(cost_w <= cost_l), \"Assumption failed: cost_a (winner) must be <= cost_b (loser)\"\n\n    # 1. Calculate cost and log-probability differences\n    delta_cost = cost_l - cost_w  # This will be >= 0\n    delta_logp = logp_w - logp_l # We want this to be positive\n\n    # 2. Compute the adaptive margin\n    # The margin is proportional to the cost gap, encouraging a larger logp gap for larger cost gaps.\n    # Clamping prevents the margin from becoming excessively large for outliers.\n    margin = torch.clamp(alpha * delta_cost, min=0.0, max=max_margin)\n\n    # 3. Compute the core loss term (similar to a margin-based logistic loss)\n    # We want delta_logp to be positive. The loss is high when delta_logp < margin.\n    # softplus(x) = log(1 + exp(x)). This is a smooth, non-negative version of ReLU.\n    core_loss = F.softplus(margin - delta_logp)\n\n    # 4. Compute the adaptive weight using softplus on the raw cost difference\n    # This weight increases smoothly and monotonically with the cost gap, emphasizing clearer pairs.\n    # The beta hyperparameter controls the steepness of this emphasis.\n    # Unlike the parent, this is not batch-dependent (no z-score) and does not saturate (no sigmoid).\n    adaptive_weight = F.softplus(beta * delta_cost)\n\n    # 5. Combine core loss and adaptive weight\n    # The final loss for each pair is the product of its adaptive weight and its core loss.\n    instance_loss = adaptive_weight * core_loss\n    \n    # Apply optional sample weights if provided\n    if 'weight' in batch and batch['weight'] is not None:\n        instance_loss = instance_loss * batch['weight']\n\n    # 6. Return the mean loss over the batch\n    return instance_loss.mean()"}, "static_ok": true, "static_reason": "", "dynamic_ok": true, "dynamic_reason": "ok", "loss_value": 0.7250996232032776, "grad_norm": 0.0}
{"generation": 1, "index": 7, "ir": {"name": "Adaptive Tanh Margin Loss", "intuition": "This loss function uses a tanh-based weighting function, derived from the z-scored cost difference, to modulate the learning signal. The tanh function provides a smooth, bounded (-1 to 1) scaling factor. By using `0.5 * (1 + tanh(...))`, the weight smoothly transitions from 0 (for pairs with much smaller-than-average cost gaps) to 1 (for pairs with much larger-than-average cost gaps). This focuses the model's attention on pairs with significant cost differences, effectively down-weighting pairs that are either too similar (noisy signal) or too different (already easy). An additional margin, scaled by the raw cost difference, provides a clear separation target for the log-probability difference, pushing the model to create a larger log-probability gap for pairs with larger cost differences.", "pseudocode": "1. Calculate the cost difference: delta_cost = cost(b) - cost(a).\n2. Normalize the cost difference using a z-score to get 'normalized_delta_cost'.\n3. Calculate the log-probability difference: delta_logp = logp(a) - logp(b).\n4. Compute an adaptive weight using a scaled tanh function on the normalized cost difference: weight = 0.5 * (1 + tanh(beta * normalized_delta_cost)).\n5. Compute an adaptive margin based on the absolute cost difference, clamped for stability: margin = clamp(alpha * delta_cost, 0, max_margin).\n6. The core of the loss is a softplus function (log(1+exp(x))) applied to the margined log-probability difference: loss = softplus(margin - delta_logp).\n7. The final loss is the product of the adaptive weight and the core loss, averaged over the batch: mean(weight * loss).", "hyperparams": {"alpha": 0.5, "beta": 1.0, "max_margin": 5.0, "eps": 1e-08}, "operators_used": ["tanh", "softplus", "clamp", "zscore"], "implementation_hint": {"expects": ["cost_a", "cost_b", "log_prob_w", "log_prob_l", "weight"], "returns": "scalar"}, "code": "import torch\nimport torch.nn.functional as F\n\ndef generated_loss(batch, model_output, extra):\n    \"\"\"\n    Adaptive Tanh Margin Loss.\n\n    This loss function applies a tanh-based weight derived from the normalized cost difference\n    to modulate the learning signal. This prevents pairs with extremely large cost\n    differences from generating excessively large gradients, while still ensuring a\n    strong learning signal for pairs with meaningful cost gaps.\n\n    The preference `p(a > b)` is modeled based on `logp(a) - logp(b)`. The ground truth is that\n    the solution with the lower cost is preferred.\n\n    In the input, `log_prob_w` corresponds to the winner (lower cost) and `log_prob_l` to the loser.\n    Therefore, `cost_a` is the cost of the winner and `cost_b` is the cost of the loser, so cost_a < cost_b.\n    We want to encourage `log_prob_w > log_prob_l`.\n    \"\"\"\n    # Read hyperparameters from the 'extra' dictionary\n    alpha = extra.get('alpha', 0.5)\n    beta = extra.get('beta', 1.0)\n    max_margin = extra.get('max_margin', 5.0)\n    eps = extra.get('eps', 1e-8)\n\n    # Unpack tensors from the batch\n    # By convention, 'w' is winner (lower cost), 'l' is loser (higher cost)\n    cost_w = batch['cost_a']\n    cost_l = batch['cost_b']\n    logp_w = batch['log_prob_w']\n    logp_l = batch['log_prob_l']\n    \n    # Sanity check: cost_w should be less than or equal to cost_l\n    assert torch.all(cost_w <= cost_l), \"Assumption failed: cost_a (winner) must be <= cost_b (loser)\"\n\n    # 1. Calculate cost and log-probability differences\n    delta_cost = cost_l - cost_w  # This will be >= 0\n    delta_logp = logp_w - logp_l # We want this to be positive\n\n    # 2. Normalize the cost difference (z-score)\n    if delta_cost.numel() > 1:\n        mean_delta_cost = delta_cost.mean()\n        std_delta_cost = delta_cost.std() + eps\n        normalized_delta_cost = (delta_cost - mean_delta_cost) / std_delta_cost\n    else:\n        normalized_delta_cost = torch.zeros_like(delta_cost)\n\n    # 3. Compute the adaptive margin\n    # The margin is proportional to the cost gap, encouraging a larger logp gap for larger cost gaps.\n    margin = torch.clamp(alpha * delta_cost, min=0.0, max=max_margin)\n\n    # 4. Compute the core loss term (margin-based logistic loss)\n    # The term inside softplus is `margin - delta_logp`.\n    # softplus(x) is a smooth, non-negative version of ReLU. Loss is low if delta_logp > margin.\n    core_loss = F.softplus(margin - delta_logp)\n\n    # 5. Compute the adaptive weight using tanh\n    # tanh scales the normalized cost diff to [-1, 1]. We remap this to [0, 1].\n    # For pairs with a large positive normalized cost difference, weight approaches 1.\n    # For pairs where the cost difference is near the mean, weight is ~0.5.\n    # For pairs with a cost difference much smaller than average, weight approaches 0.\n    adaptive_weight = 0.5 * (1.0 + torch.tanh(beta * normalized_delta_cost))\n\n    # 6. Combine core loss and adaptive weight\n    # This focuses learning on pairs with significant (but not extreme) cost differences.\n    instance_loss = adaptive_weight * core_loss\n    \n    # Apply optional sample weights if provided\n    if 'weight' in batch and batch['weight'] is not None:\n        instance_loss = instance_loss * batch['weight']\n\n    # 7. Return the mean loss over the batch\n    return instance_loss.mean()"}, "static_ok": true, "static_reason": "", "dynamic_ok": true, "dynamic_reason": "ok", "loss_value": 0.4870385527610779, "grad_norm": 0.0}
{"generation": 2, "index": 0, "ir": {"name": "Rank-Normalized Tanh Logistic Loss", "intuition": "This loss function combines a margin-based logistic loss with a novel rank-based normalization scheme. Inheriting from both parents, it uses a core logistic loss `softplus(margin - delta_logp)` where the margin is proportional to the cost difference (`delta_cost`). This encourages the model to create a log-probability gap that scales with the magnitude of the cost gap.\n\nFrom the parents, we also inherit the idea of an adaptive weight. However, instead of using z-scoring on `delta_cost`, which is sensitive to outliers and batch statistics, we introduce a new coupling idea: rank-based normalization. The cost differences within a batch are converted to ranks, and these ranks are then normalized to a [-1, 1] range. This `rank_gap` is robust to the scale and distribution of cost differences, providing a stable, non-parametric signal of relative importance within the batch. This normalized rank is then fed into a `tanh` function to compute the adaptive weight, focusing learning on pairs with a higher relative cost difference within the current batch. This avoids the potential instability of z-scoring while retaining the parents' core concept of adaptively weighting the loss signal.", "pseudocode": "1. Calculate the cost difference: delta_cost = cost(b) - cost(a).\n2. Calculate the log-probability difference: delta_logp = logp(a) - logp(b).\n3. Inherited Idea (Parents): Compute an adaptive margin proportional to the cost difference, clamped for stability: margin = clamp(alpha * delta_cost, 0, max_margin).\n4. Inherited Idea (Parents): Use a logistic-style loss as the core objective: core_loss = softplus(margin - delta_logp).\n5. New Coupling Idea: Normalize the `delta_cost` values within the batch using rank normalization to get `normalized_rank_gap`, a value from -1 to 1. This is more robust to outliers than z-scoring.\n6. Inherited Idea (Parents): Compute an adaptive weight using a `tanh` function on the normalized value. The input to `tanh` is now the `normalized_rank_gap`: adaptive_weight = 0.5 * (1 + tanh(beta * normalized_rank_gap)).\n7. The final loss is the product of the adaptive weight and the core loss, averaged over the batch: mean(adaptive_weight * core_loss).", "hyperparams": {"alpha": 0.5, "beta": 1.5, "max_margin": 5.0, "eps": 1e-08}, "operators_used": ["softplus", "clamp", "tanh", "rank_gap"], "implementation_hint": {"expects": ["cost_a", "cost_b", "log_prob_w", "log_prob_l"], "returns": "scalar"}, "code": "import torch\nimport torch.nn.functional as F\n\ndef _rank_gap(x, eps=1e-8):\n    \"\"\"\n    Computes the rank of each element in x and normalizes it to the range [-1, 1].\n    The smallest element gets rank 0, largest gets rank N-1.\n    Normalized rank is 2 * (rank / (N-1)) - 1.\n    This provides a robust, non-parametric measure of relative magnitude.\n    \"\"\"\n    if x.numel() <= 1:\n        return torch.zeros_like(x)\n    \n    ranks = x.argsort().argsort().float()\n    # Normalize ranks to [0, 1]\n    normalized_ranks = ranks / (x.numel() - 1 + eps)\n    # Scale to [-1, 1]\n    return 2.0 * normalized_ranks - 1.0\n\ndef generated_loss(batch, model_output, extra):\n    \"\"\"\n    Rank-Normalized Tanh Logistic Loss.\n\n    This loss combines a margin-based logistic loss with a novel rank-based\n    normalization for adaptive weighting. It inherits the core structure of\n    `weight * softplus(margin - delta_logp)` from its parents.\n\n    The key modification is using rank normalization on the cost difference (`delta_cost`)\n    instead of z-scoring. This makes the adaptive weight robust to outliers and batch statistics.\n    The rank-normalized `delta_cost` is passed through a tanh function to create a smooth\n    weight that emphasizes pairs with a higher relative cost difference within the batch.\n\n    By convention, `log_prob_w` is for the winner (lower cost, `cost_a`)\n    and `log_prob_l` is for the loser (higher cost, `cost_b`).\n    The loss encourages `log_prob_w - log_prob_l` to be positive.\n    \"\"\"\n    # Read hyperparameters from the 'extra' dictionary\n    alpha = extra.get('alpha', 0.5)\n    beta = extra.get('beta', 1.5)\n    max_margin = extra.get('max_margin', 5.0)\n    eps = extra.get('eps', 1e-8)\n\n    # Unpack tensors from the batch\n    cost_w = batch['cost_a']\n    cost_l = batch['cost_b']\n    logp_w = batch['log_prob_w']\n    logp_l = batch['log_prob_l']\n\n    # Sanity check: cost_w should be less than or equal to cost_l\n    assert torch.all(cost_w <= cost_l), \"Assumption failed: cost_a (winner) must be <= cost_b (loser)\"\n\n    # 1. Calculate cost and log-probability differences\n    delta_cost = cost_l - cost_w  # Should be >= 0\n    delta_logp = logp_w - logp_l # We want this to be positive\n\n    # 2. Compute the adaptive margin (Inherited from parents)\n    margin = torch.clamp(alpha * delta_cost, min=0.0, max=max_margin)\n\n    # 3. Compute the core loss term (Inherited from parents)\n    core_loss = F.softplus(margin - delta_logp)\n\n    # 4. Normalize delta_cost using rank normalization (New Coupling Idea)\n    normalized_rank_gap = _rank_gap(delta_cost, eps=eps)\n\n    # 5. Compute the adaptive weight using tanh on the rank-normalized value (Inherited + Coupled)\n    # This weight smoothly scales from 0 to 1 based on the relative importance\n    # of the cost gap within the batch.\n    adaptive_weight = 0.5 * (1.0 + torch.tanh(beta * normalized_rank_gap))\n\n    # 6. Combine core loss and adaptive weight\n    instance_loss = adaptive_weight * core_loss\n\n    # Apply optional sample weights if provided\n    if 'weight' in batch and batch['weight'] is not None:\n        instance_loss = instance_loss * batch['weight']\n\n    # 7. Return the mean loss over the batch\n    return instance_loss.mean()"}, "static_ok": true, "static_reason": "", "dynamic_ok": false, "dynamic_reason": "forward_error: name '_rank_gap' is not defined", "loss_value": null, "grad_norm": null}
{"generation": 2, "index": 1, "ir": {"name": "Rank-Gap Scaled Tanh-Logistic Loss", "intuition": "This loss function synergizes the margin-based logistic loss with an adaptive weighting scheme. \n\nInherited Ideas:\n- From both parents, it inherits the core logistic loss structure: `softplus(margin - delta_logp)`, which aims to make the log-probability difference `delta_logp` greater than a target `margin`.\n- From both parents, it also inherits the adaptive margin, `margin = alpha * delta_cost`, which scales the target separation based on the magnitude of the cost difference. This encourages the model to learn a more pronounced preference for pairs with a larger quality gap.\n\nNew Coupling Ideas:\n1.  **Rank-Gap Normalization**: Instead of z-scoring the cost differences, which is sensitive to outliers and assumes a somewhat normal distribution, this loss uses a `rank_gap` normalization. This method maps the cost differences to a uniform distribution over `[-1, 1]` based on their rank order. This is a more robust, non-parametric way to normalize, making the subsequent `tanh` weighting less susceptible to the scale and distribution of cost differences in a batch.\n2.  **Symmetric Tanh Weighting**: The inherited `tanh` weighting function `0.5 * (1 + tanh(beta * normalized_delta))` is modified. By squaring the `tanh` output (`tanh^2`), the weight becomes symmetric around the median cost gap (where the rank-normalized value is 0). This new weighting function, `tanh^2(beta * normalized_delta_cost)`, de-emphasizes pairs with both very small and very large cost differences (which are often noisy or trivially easy, respectively), focusing the training on the most informative, moderately-difficult pairs in the middle of the distribution.", "pseudocode": "1. Calculate the cost difference: delta_cost = cost(b) - cost(a).\n2. Calculate the log-probability difference: delta_logp = logp(a) - logp(b).\n3. Normalize the cost differences using rank-gap normalization to get 'normalized_delta_cost', which maps the values to the range [-1, 1] based on their rank.\n4. Compute an adaptive margin proportional to the raw cost difference, clamped for stability: margin = clamp(alpha * delta_cost, 0, max_margin).\n5. Compute the core loss using a softplus function: core_loss = softplus(margin - delta_logp).\n6. Compute a symmetric adaptive weight by applying a scaled tanh to the rank-normalized cost difference and squaring the result: weight = tanh(beta * normalized_delta_cost)^2.\n7. The final loss is the product of the adaptive weight and the core loss, averaged over the batch: mean(weight * core_loss).", "hyperparams": {"alpha": 0.5, "beta": 1.5, "max_margin": 5.0, "eps": 1e-08}, "operators_used": ["softplus", "clamp", "tanh", "rank_gap"], "implementation_hint": {"expects": ["cost_a", "cost_b", "log_prob_w", "log_prob_l"], "returns": "scalar"}, "code": "import torch\nimport torch.nn.functional as F\n\ndef rank_gap(x, eps=1e-8):\n    \"\"\"Non-parametrically transform a tensor to the interval [-1, 1] based on rank.\"\"\"\n    if x.numel() <= 1:\n        return torch.zeros_like(x)\n    ranks = x.argsort().argsort().float()\n    # Transform ranks to [-1, 1]\n    # Subtracting 0.5 centers the ranks around 0\n    # Dividing by (x.numel() / 2) scales it to approx [-1, 1]\n    # Clamping ensures the range is strictly [-1, 1]\n    scaled_ranks = (ranks - (x.numel() - 1) / 2.0) / ((x.numel() - 1) / 2.0 + eps)\n    return torch.clamp(scaled_ranks, -1.0, 1.0)\n\ndef generated_loss(batch, model_output, extra):\n    \"\"\"\n    Rank-Gap Scaled Tanh-Logistic Loss.\n\n    Inherits the margin-based logistic loss from parents. Introduces two new ideas:\n    1. Rank-Gap Normalization: A robust, non-parametric normalization for cost differences.\n    2. Symmetric Tanh Weighting: Uses tanh^2 to focus on moderately difficult examples, \n       de-emphasizing both the easiest and the hardest pairs.\n    \n    As per convention, 'w' denotes the winner (lower cost) and 'l' the loser.\n    We want to encourage log_prob_w > log_prob_l.\n    \"\"\"\n    # Read hyperparameters\n    alpha = extra.get('alpha', 0.5)\n    beta = extra.get('beta', 1.5)\n    max_margin = extra.get('max_margin', 5.0)\n    eps = extra.get('eps', 1e-8)\n\n    # Unpack tensors from batch\n    cost_w = batch['cost_a']\n    cost_l = batch['cost_b']\n    logp_w = batch['log_prob_w']\n    logp_l = batch['log_prob_l']\n\n    # Sanity check: cost_w should be less than or equal to cost_l\n    assert torch.all(cost_w <= cost_l), \"Assumption failed: cost_a (winner) must be <= cost_b (loser)\"\n\n    # 1. Calculate cost and log-probability differences\n    delta_cost = cost_l - cost_w  # Should be >= 0\n    delta_logp = logp_w - logp_l  # We want this to be positive\n\n    # 2. [NEW] Normalize cost difference using rank-gap normalization\n    normalized_delta_cost = rank_gap(delta_cost, eps=eps)\n\n    # 3. [INHERITED] Compute the adaptive margin\n    margin = torch.clamp(alpha * delta_cost, min=0.0, max=max_margin)\n\n    # 4. [INHERITED] Compute the core loss term (margin-based logistic loss)\n    core_loss = F.softplus(margin - delta_logp)\n\n    # 5. [NEW] Compute symmetric adaptive weight using squared tanh\n    # This creates a U-shaped weighting curve (if plotted against rank), focusing on the middle.\n    # tanh output is in [-1, 1]. Squaring it maps to [0, 1] and makes it symmetric around 0.\n    adaptive_weight = torch.tanh(beta * normalized_delta_cost).pow(2)\n\n    # 6. Combine core loss and adaptive weight\n    instance_loss = adaptive_weight * core_loss\n\n    # Apply optional sample weights if provided\n    if 'weight' in batch and batch['weight'] is not None:\n        instance_loss = instance_loss * batch['weight']\n\n    # 7. Return the mean loss over the batch\n    return instance_loss.mean()"}, "static_ok": true, "static_reason": "", "dynamic_ok": false, "dynamic_reason": "forward_error: name 'rank_gap' is not defined", "loss_value": null, "grad_norm": null}
{"generation": 2, "index": 2, "ir": {"name": "Adaptive Rank-Gap Tanh Loss", "intuition": "This loss function combines an adaptive margin with a novel rank-based weighting scheme. \n\nInherited Ideas:\n- From 'Adaptive Tanh Margin Loss' (both parents), we inherit the core structure of a margin-based logistic loss using `softplus(margin - delta_logp)`. This encourages the log-probability difference (`delta_logp`) to exceed a certain `margin`.\n- Also from the parents, we inherit the idea of making this `margin` adaptive and proportional to the cost difference (`delta_cost`), scaled by `alpha`. This means pairs with a larger cost gap are expected to have a larger log-probability gap.\n\nNew Coupling Ideas:\n1. **Rank-Gap Normalization**: Instead of z-scoring the cost differences, we use a more robust rank-based normalization. We calculate the percentile rank of each cost difference within the batch. This `rank_gap` is inherently normalized between 0 and 1, making it less sensitive to outliers in the cost distribution compared to z-scoring. This provides a stable, distribution-agnostic measure of a pair's relative importance.\n2. **Symmetric Tanh Weighting**: We couple this new `rank_gap` with the `tanh` function to create a symmetric weighting mechanism. The `rank_gap` is shifted by -0.5 to center it around zero, then scaled by `beta`. The `tanh` function then maps these values to a [-1, 1] range. Finally, we add 1.0 to this result, creating a weight that smoothly varies from 0 to 2. This weight is centered at 1.0 for the median-ranked cost difference. It strongly de-weights pairs with the smallest cost gaps (approaching 0) and up-weights pairs with the largest cost gaps (approaching 2), creating a more focused and dynamic learning signal.", "pseudocode": "1. Calculate the cost difference: delta_cost = cost(b) - cost(a).\n2. Calculate the log-probability difference: delta_logp = logp(a) - logp(b).\n3. Compute an adaptive margin proportional to the raw cost difference, clamped for stability: margin = clamp(alpha * delta_cost, 0, max_margin).\n4. Compute the core loss using a softplus function: core_loss = softplus(margin - delta_logp).\n5. (New Idea 1) Normalize the cost differences using rank-gap normalization to get 'rank_gap_norm', a value for each pair in [0, 1] representing its percentile rank.\n6. (New Idea 2) Compute an adaptive weight using the rank-normalized value. First, center the rank by subtracting 0.5. Then, apply a scaled tanh: weight = 1.0 + tanh(beta * (rank_gap_norm - 0.5)). This creates a weight between 0 and 2, centered at 1 for the median pair.\n7. The final loss is the product of the adaptive weight and the core loss, averaged over the batch: mean(weight * core_loss).", "hyperparams": {"alpha": 0.5, "beta": 4.0, "max_margin": 5.0, "eps": 1e-08}, "operators_used": ["softplus", "clamp", "tanh", "rank_gap"], "implementation_hint": {"expects": ["cost_a", "cost_b", "logp_a", "logp_b"], "returns": "scalar"}, "code": "import torch\nimport torch.nn.functional as F\n\ndef rank_gap(x, eps=1e-8):\n    \"\"\"Computes the percentile rank of each element in a 1D tensor.\"\"\"\n    if x.numel() <= 1:\n        return torch.ones_like(x) * 0.5\n    # argsort twice gives the rank of each element\n    ranks = x.argsort().argsort().float()\n    # Normalize to [0, 1]\n    normalized_ranks = ranks / (x.numel() - 1 + eps)\n    return normalized_ranks\n\ndef generated_loss(batch, model_output, extra):\n    \"\"\"\n    Adaptive Rank-Gap Tanh Loss.\n\n    This loss combines a cost-proportional margin with a novel rank-based weighting.\n    The weight is derived from the percentile rank of the cost difference within the batch,\n    which is more robust to outliers than z-scoring. This rank is then passed through\n    a tanh function to create a symmetric weight that emphasizes pairs with large cost\n    gaps and de-emphasizes those with small gaps.\n\n    By convention, 'w' is winner (lower cost), 'l' is loser (higher cost).\n    We want to encourage log_prob_w > log_prob_l.\n    \"\"\"\n    # Read hyperparameters from the 'extra' dictionary\n    alpha = extra.get('alpha', 0.5)\n    beta = extra.get('beta', 4.0)\n    max_margin = extra.get('max_margin', 5.0)\n    eps = extra.get('eps', 1e-8)\n\n    # Unpack tensors from the batch\n    cost_w = batch['cost_a']\n    cost_l = batch['cost_b']\n    logp_w = batch['log_prob_w']\n    logp_l = batch['log_prob_l']\n\n    # Sanity check\n    assert torch.all(cost_w <= cost_l), \"Assumption failed: cost_a (winner) must be <= cost_b (loser)\"\n\n    # 1. Calculate cost and log-probability differences\n    delta_cost = cost_l - cost_w\n    delta_logp = logp_w - logp_l\n\n    # 2. Compute the adaptive margin (inherited idea)\n    margin = torch.clamp(alpha * delta_cost, min=0.0, max=max_margin)\n\n    # 3. Compute the core loss term (inherited idea)\n    core_loss = F.softplus(margin - delta_logp)\n\n    # 4. Compute rank-based normalization (new coupling idea 1)\n    # This is more robust to outlier cost differences than z-scoring.\n    # The result `rank_gap_norm` is in the range [0, 1].\n    rank_gap_norm = rank_gap(delta_cost, eps=eps)\n\n    # 5. Compute adaptive weight with centered tanh (new coupling idea 2)\n    # We shift the rank to be in [-0.5, 0.5] to center the tanh input around zero.\n    # `tanh(beta * (rank - 0.5))` will be in [-tanh(beta*0.5), tanh(beta*0.5)].\n    # Adding 1.0 shifts the weight range. For beta=4, this is approx [0.04, 1.96].\n    # This symmetrically up-weights high-rank pairs and down-weights low-rank pairs.\n    adaptive_weight = 1.0 + torch.tanh(beta * (rank_gap_norm - 0.5))\n\n    # 6. Combine core loss and adaptive weight\n    instance_loss = adaptive_weight * core_loss\n\n    # Apply optional sample weights if provided\n    if 'weight' in batch and batch['weight'] is not None:\n        instance_loss = instance_loss * batch['weight']\n\n    # 7. Return the mean loss over the batch\n    return instance_loss.mean()"}, "static_ok": true, "static_reason": "", "dynamic_ok": false, "dynamic_reason": "forward_error: name 'rank_gap' is not defined", "loss_value": null, "grad_norm": null}
{"generation": 2, "index": 3, "ir": {"name": "Rank-Correlated Tanh Loss", "intuition": "This loss function combines an adaptive margin with a novel adaptive weighting scheme to create a robust preference learning objective. \n\nInherited ideas:\n- From both parents, it inherits the core structure of a margin-based logistic loss, `softplus(margin - delta_logp)`, where the margin is proportional to the raw cost difference `delta_cost`. This encourages the log-probability gap to be at least as large as a scaled version of the cost gap.\n- From both parents, it also inherits the use of a `tanh` function to create a smooth, bounded adaptive weight that modulates the loss for each pair.\n\nNew coupling ideas:\n1.  **Rank-Gap Normalization**: Instead of z-scoring the cost differences, which is sensitive to outliers, this loss normalizes the cost differences using their rank gap. The cost differences are ranked, and the ranks are scaled to the `[-1, 1]` range. This provides a non-parametric, robust measure of how significant a pair's cost difference is relative to others in the batch. This normalized rank gap then drives the `tanh` weighting function.\n2.  **Symmetric Loss Formulation**: The core loss is expressed as `logsigmoid(delta_logp - margin)`. Mathematically, `logsigmoid(x)` is equivalent to `-softplus(-x)`. By using `-logsigmoid(margin - delta_logp)`, we get the same loss value as `softplus(margin - delta_logp)` but frame it more directly as maximizing the log-likelihood of the preferred model (`logp_w`) correctly exceeding the log-likelihood of the less preferred model (`logp_l`) by a certain margin. This maintains the same optimization objective while offering a slightly different, but equivalent, probabilistic interpretation.", "pseudocode": "1. Calculate the cost difference for each pair: delta_cost = cost(b) - cost(a).\n2. Calculate the log-probability difference: delta_logp = logp(a) - logp(b).\n3. Compute an adaptive margin proportional to the cost difference, clamped for stability: margin = clamp(alpha * delta_cost, 0, max_margin).\n4. Normalize the cost differences using the rank_gap operator, which ranks delta_cost and scales the ranks to the range [-1, 1]. This produces 'rank_normalized_delta_cost'.\n5. Compute an adaptive weight using a scaled tanh function on the rank-normalized cost difference: weight = 0.5 * (1 + tanh(beta * rank_normalized_delta_cost)). This up-weights pairs with a higher relative cost difference within the batch.\n6. Compute the core loss using the logsigmoid function: core_loss = -logsigmoid(delta_logp - margin). This is equivalent to softplus(margin - delta_logp) but framed as a log-likelihood maximization.\n7. The final loss is the product of the adaptive weight and the core loss, averaged over the batch: mean(weight * core_loss).", "hyperparams": {"alpha": 0.5, "beta": 1.0, "max_margin": 5.0, "eps": 1e-08}, "operators_used": ["logsigmoid", "tanh", "clamp", "rank_gap"], "implementation_hint": {"expects": ["cost_a", "cost_b", "logp_a", "logp_b"], "returns": "scalar"}, "code": "import torch\nimport torch.nn.functional as F\n\ndef _rank_gap(x):\n    \"\"\"Non-differentiable rank-based normalization to [-1, 1].\"\"\"\n    if x.numel() <= 1:\n        return torch.zeros_like(x)\n    ranks = torch.empty_like(x, dtype=torch.float)\n    ranks[x.argsort()] = torch.arange(x.numel(), device=x.device, dtype=x.dtype)\n    # Scale ranks to [-1, 1]\n    ranks = 2 * (ranks / (x.numel() - 1)) - 1\n    return ranks\n\ndef generated_loss(batch, model_output, extra):\n    \"\"\"\n    Rank-Correlated Tanh Loss.\n\n    This loss combines a margin proportional to the cost difference with an adaptive\n    weight derived from the rank of the cost difference within the batch. This makes the\n    weighting robust to outliers in cost distributions.\n\n    The preference `p(a > b)` is modeled based on `logp(a) - logp(b)`. The ground truth is that\n    the solution with the lower cost is preferred.\n\n    In the input, `log_prob_w` corresponds to the winner (lower cost) and `log_prob_l` to the loser.\n    Therefore, `cost_a` is the cost of the winner and `cost_b` is the cost of the loser, so cost_a < cost_b.\n    We want to encourage `log_prob_w > log_prob_l`.\n    \"\"\"\n    # Read hyperparameters from the 'extra' dictionary\n    alpha = extra.get('alpha', 0.5)\n    beta = extra.get('beta', 1.0)\n    max_margin = extra.get('max_margin', 5.0)\n\n    # Unpack tensors from the batch\n    cost_w = batch['cost_a']\n    cost_l = batch['cost_b']\n    logp_w = batch['log_prob_w']\n    logp_l = batch['log_prob_l']\n\n    # Sanity check: cost_w should be less than or equal to cost_l\n    assert torch.all(cost_w <= cost_l), \"Assumption failed: cost_a (winner) must be <= cost_b (loser)\"\n\n    # 1. Calculate cost and log-probability differences\n    delta_cost = cost_l - cost_w  # This will be >= 0\n    delta_logp = logp_w - logp_l  # We want this to be positive\n\n    # 2. Normalize the cost difference using rank_gap for robust weighting\n    # This is a non-parametric way to assess the importance of a pair's cost gap.\n    with torch.no_grad(): # ranking is not differentiable\n        rank_normalized_delta_cost = _rank_gap(delta_cost)\n\n    # 3. Compute the adaptive margin from raw cost difference\n    margin = torch.clamp(alpha * delta_cost, min=0.0, max=max_margin)\n\n    # 4. Compute the adaptive weight using tanh on the rank-normalized cost difference\n    # This smoothly maps ranks from [0, 1] to focus learning on pairs with higher relative cost gaps.\n    adaptive_weight = 0.5 * (1.0 + torch.tanh(beta * rank_normalized_delta_cost))\n\n    # 5. Compute the core loss using a logsigmoid formulation\n    # We want to maximize the log-probability of the difference being greater than the margin.\n    # loss = -log P(correct) = -log sigmoid(delta_logp - margin)\n    # This is numerically equivalent to F.softplus(margin - delta_logp).\n    core_loss = -F.logsigmoid(delta_logp - margin)\n\n    # 6. Combine core loss and adaptive weight\n    instance_loss = adaptive_weight * core_loss\n\n    # Apply optional sample weights if provided\n    if 'weight' in batch and batch['weight'] is not None:\n        instance_loss = instance_loss * batch['weight']\n\n    # 7. Return the mean loss over the batch\n    return instance_loss.mean()"}, "static_ok": true, "static_reason": "", "dynamic_ok": false, "dynamic_reason": "forward_error: name '_rank_gap' is not defined", "loss_value": null, "grad_norm": null}
{"generation": 2, "index": 4, "ir": {"name": "Rank-Correlated Tanh Loss", "intuition": "This loss function combines a margin-based logistic loss with a dynamic weighting scheme that considers both the magnitude and the rank of cost differences within a batch. \n\nInherited ideas:\n- From both parents, it inherits the core structure of a margin-based logistic loss: `softplus(margin - delta_logp)`, where the margin is proportional to the cost difference (`alpha * delta_cost`). This encourages the model to create a log-probability gap that reflects the magnitude of the cost gap.\n- It also inherits the use of a `tanh` function to create a smooth, bounded weighting factor based on the normalized cost difference. This helps to modulate the learning signal, focusing on pairs with meaningful cost gaps.\n\nNew coupling ideas:\n1.  **Rank-based Weighting (`rank_gap`)**: A new weighting component is introduced based on the rank of the cost difference within the batch. The `rank_gap` operator maps the rank of each `delta_cost` to a value in `[-1, 1]`. This signal is then scaled and shifted to a `[0, 1]` range. This new weight emphasizes pairs that have a relatively larger cost difference compared to others in the same batch, adding a layer of competitive, rank-aware learning.\n2.  **Harmonic Mean of Weights**: The `tanh`-based weight (reflecting magnitude) and the new rank-based weight are combined using a harmonic mean. This ensures that a pair only receives a high final weight if *both* its cost difference magnitude is significant (high tanh weight) *and* its rank within the batch is high (high rank weight). This coupling prevents pairs with large absolute cost gaps in a batch of other large gaps from dominating, and also prevents high-ranking but low-magnitude pairs from getting undue attention.", "pseudocode": "1. Calculate the cost difference: delta_cost = cost(b) - cost(a).\n2. Calculate the log-probability difference: delta_logp = logp(a) - logp(b).\n3. Compute an adaptive margin proportional to the cost difference, clamped for stability: margin = clamp(alpha * delta_cost, 0, max_margin).\n4. Compute the core logistic loss term using a softplus function: core_loss = softplus(margin - delta_logp).\n5. Normalize the cost difference using z-scoring: normalized_delta_cost = zscore(delta_cost).\n6. Compute a magnitude-based weight using a scaled and shifted tanh function: weight_magnitude = 0.5 * (1 + tanh(beta * normalized_delta_cost)).\n7. Compute a rank-based weight by first calculating the rank gap of the cost differences, then scaling and shifting it to the [0, 1] range: weight_rank = 0.5 * (1 + rank_gap(delta_cost)).\n8. Combine the magnitude and rank weights using a smoothed harmonic mean: combined_weight = (2 * weight_magnitude * weight_rank) / (weight_magnitude + weight_rank + eps).\n9. The final loss is the product of the combined weight and the core loss, averaged over the batch: mean(combined_weight * core_loss).", "hyperparams": {"alpha": 0.5, "beta": 1.0, "max_margin": 5.0, "eps": 1e-08}, "operators_used": ["softplus", "clamp", "zscore", "tanh", "rank_gap"], "implementation_hint": {"expects": ["cost_a", "cost_b", "logp_a", "logp_b", "weight"], "returns": "scalar"}, "code": "import torch\nimport torch.nn.functional as F\n\ndef zscore(x, eps=1e-8):\n    \"\"\"Standard z-scoring.\"\"\"\n    if x.numel() <= 1:\n        return torch.zeros_like(x)\n    mean = x.mean()\n    std = x.std() + eps\n    return (x - mean) / std\n\ndef rank_gap(x):\n    \"\"\"Maps ranks to a [-1, 1] range, centered at 0.\"\"\"\n    if x.numel() <= 1:\n        return torch.zeros_like(x)\n    ranks = torch.argsort(torch.argsort(x).float()).float()\n    ranks = (2 * ranks / (x.numel() - 1)) - 1\n    return ranks\n\ndef generated_loss(batch, model_output, extra):\n    \"\"\"\n    Rank-Correlated Tanh Loss.\n\n    This loss combines a standard margin-based logistic loss with a dynamic weight.\n    The weight is the harmonic mean of two components:\n    1. A tanh-based weight on the z-scored cost difference (magnitude).\n    2. A rank-based weight from the rank_gap of the cost difference (relative importance).\n    This focuses learning on pairs that have both a large absolute cost gap and a high rank\n    of that gap within the batch.\n    \"\"\"\n    # Read hyperparameters from the 'extra' dictionary\n    alpha = extra.get('alpha', 0.5)\n    beta = extra.get('beta', 1.0)\n    max_margin = extra.get('max_margin', 5.0)\n    eps = extra.get('eps', 1e-8)\n\n    # Unpack tensors from the batch\n    # 'w' is winner (lower cost), 'l' is loser (higher cost)\n    cost_w = batch['cost_a']\n    cost_l = batch['cost_b']\n    logp_w = batch['log_prob_w']\n    logp_l = batch['log_prob_l']\n\n    # Sanity check: cost_w should be less than or equal to cost_l\n    assert torch.all(cost_w <= cost_l), \"Assumption failed: cost_a (winner) must be <= cost_b (loser)\"\n\n    # 1. Calculate cost and log-probability differences\n    delta_cost = cost_l - cost_w  # >= 0\n    delta_logp = logp_w - logp_l # We want this to be positive\n\n    # 2. Compute the adaptive margin (Inherited from parents)\n    margin = torch.clamp(alpha * delta_cost, min=0.0, max=max_margin)\n\n    # 3. Compute the core loss term (Inherited from parents)\n    core_loss = F.softplus(margin - delta_logp)\n\n    # 4. Compute magnitude-based weight (Inherited from parents)\n    normalized_delta_cost = zscore(delta_cost, eps=eps)\n    weight_magnitude = 0.5 * (1.0 + torch.tanh(beta * normalized_delta_cost))\n\n    # 5. Compute rank-based weight (New idea)\n    # rank_gap maps ranks to [-1, 1], we shift to [0, 1]\n    weight_rank = 0.5 * (1.0 + rank_gap(delta_cost))\n    \n    # 6. Combine weights using harmonic mean (New coupling idea)\n    # This ensures both weights must be high for the combined weight to be high.\n    # It's a soft 'AND' operation.\n    numerator = 2 * weight_magnitude * weight_rank\n    denominator = weight_magnitude + weight_rank + eps\n    combined_weight = numerator / denominator\n\n    # 7. Apply the combined weight to the core loss\n    instance_loss = combined_weight * core_loss\n\n    # Apply optional sample weights if provided\n    if 'weight' in batch and batch['weight'] is not None:\n        instance_loss = instance_loss * batch['weight']\n\n    # 8. Return the mean loss over the batch\n    return instance_loss.mean()"}, "static_ok": true, "static_reason": "", "dynamic_ok": false, "dynamic_reason": "forward_error: name 'zscore' is not defined", "loss_value": null, "grad_norm": null}
{"generation": 2, "index": 5, "ir": {"name": "Rank-Gapped Tanh-Modulated Logistic Loss", "intuition": "This loss function integrates a tanh-based adaptive weighting from its parents with a novel rank-based margin. \n\nInherited Ideas:\n- From both parents, it inherits the use of a `tanh` function on the z-scored cost difference (`delta_cost`) to create an adaptive weight. This weight, `adaptive_weight`, smoothly ranges from 0 to 1, focusing the model's attention on pairs with a cost difference that is significantly above the batch average, while de-emphasizing pairs that are too similar (low signal) or already well-separated.\n- It also inherits the fundamental Bradley-Terry / logistic loss structure, which aims to maximize `logp(winner) - logp(loser)`. Instead of using `softplus`, this child uses the mathematically equivalent `logsigmoid` formulation `logsigmoid(delta_logp - margin)`, which is often more numerically stable.\n\nNew Coupling Ideas:\n1.  **Rank-Gapped Margin**: A new margin calculation is introduced. Instead of being directly proportional to `delta_cost`, the margin is determined by the *rank* of `delta_cost` within the batch. The `rank_gap` function maps the cost differences to a uniform distribution over `[0, 1]`, and this is then scaled by a hyperparameter `gamma`. This makes the margin robust to outliers in the cost distribution and ensures that the model is incentivized to separate pairs based on their relative cost importance, not their absolute cost gap. This prevents pairs with extremely large cost differences from dominating the loss landscape.\n2.  **Modulated Logistic Loss**: The final loss is a direct product of the adaptive weight and the rank-gapped logistic loss term. This coupling ensures that the learning signal is strongest for pairs that are both relatively important (high rank gap) and have a cost difference significantly above the batch mean (high tanh weight).", "pseudocode": "1. Calculate the cost difference: delta_cost = cost(b) - cost(a).\n2. Calculate the log-probability difference: delta_logp = logp(a) - logp(b).\n3. (Inherited) Normalize delta_cost using z-scoring to get 'normalized_delta_cost'.\n4. (Inherited) Compute an adaptive weight using a scaled tanh function on the normalized cost difference: adaptive_weight = 0.5 * (1 + tanh(beta * normalized_delta_cost)).\n5. (New) Compute a rank-based margin. First, calculate the rank_gap of delta_cost, which maps the values to a [0, 1] range based on their sorted order. Then, scale this by a hyperparameter `gamma` to get the final margin: margin = gamma * rank_gap(delta_cost).\n6. Compute the core logistic loss using the rank-gapped margin: core_loss = -logsigmoid(delta_logp - margin).\n7. The final loss is the product of the adaptive weight and the core loss, averaged over the batch: mean(adaptive_weight * core_loss).", "hyperparams": {"beta": 1.5, "gamma": 2.0, "eps": 1e-08}, "operators_used": ["logsigmoid", "tanh", "zscore", "rank_gap"], "implementation_hint": {"expects": ["cost_a", "cost_b", "logp_a", "logp_b"], "returns": "scalar"}, "code": "import torch\nimport torch.nn.functional as F\n\ndef generated_loss(batch, model_output, extra):\n    \"\"\"\n    Rank-Gapped Tanh-Modulated Logistic Loss.\n\n    This loss combines a tanh-based weighting on the z-scored cost difference with a\n    novel margin based on the rank of the cost difference within the batch.\n\n    The preference `p(a > b)` is modeled based on `logp(a) - logp(b)`. The ground truth is that\n    the solution with the lower cost is preferred.\n\n    In the input, `log_prob_w` corresponds to the winner (lower cost) and `log_prob_l` to the loser.\n    Therefore, `cost_a` is the cost of the winner and `cost_b` is the cost of the loser, so cost_a < cost_b.\n    We want to encourage `log_prob_w > log_prob_l`.\n    \"\"\"\n    # Read hyperparameters from the 'extra' dictionary\n    beta = extra.get('beta', 1.5)\n    gamma = extra.get('gamma', 2.0)\n    eps = extra.get('eps', 1e-8)\n\n    # Unpack tensors from the batch\n    cost_w = batch['cost_a']\n    cost_l = batch['cost_b']\n    logp_w = batch['log_prob_w']\n    logp_l = batch['log_prob_l']\n    \n    # Sanity check: cost_w should be less than or equal to cost_l\n    assert torch.all(cost_w <= cost_l), \"Assumption failed: cost_a (winner) must be <= cost_b (loser)\"\n\n    # 1. Calculate cost and log-probability differences\n    delta_cost = cost_l - cost_w  # This will be >= 0\n    delta_logp = logp_w - logp_l # We want this to be positive\n\n    # 2. Normalize the cost difference (z-score) for the adaptive weight\n    if delta_cost.numel() > 1:\n        mean_delta_cost = delta_cost.mean()\n        std_delta_cost = delta_cost.std() + eps\n        normalized_delta_cost = (delta_cost - mean_delta_cost) / std_delta_cost\n    else:\n        normalized_delta_cost = torch.zeros_like(delta_cost)\n\n    # 3. Compute the adaptive weight using tanh (Inherited)\n    # This weight focuses on pairs with an above-average cost difference.\n    adaptive_weight = 0.5 * (1.0 + torch.tanh(beta * normalized_delta_cost))\n\n    # 4. Compute the rank-gapped margin (New Coupling)\n    # `rank_gap` maps delta_cost to [0, 1] based on its rank in the batch.\n    # This makes the margin robust to outliers and dependent on relative cost importance.\n    if delta_cost.numel() > 1:\n        ranks = delta_cost.argsort().argsort().float()\n        rank_gaps = ranks / (delta_cost.numel() - 1)\n    else:\n        rank_gaps = torch.ones_like(delta_cost) # Only one element, so it's the max rank\n    margin = gamma * rank_gaps\n\n    # 5. Compute the core logistic loss term\n    # We use -logsigmoid(x) which is equivalent to softplus(-x).\n    # The loss is low when delta_logp > margin.\n    core_loss = -F.logsigmoid(delta_logp - margin)\n\n    # 6. Combine core loss and adaptive weight\n    # The final loss is modulated by the tanh weight.\n    instance_loss = adaptive_weight * core_loss\n    \n    # Apply optional sample weights if provided\n    if 'weight' in batch and batch['weight'] is not None:\n        instance_loss = instance_loss * batch['weight']\n\n    # 7. Return the mean loss over the batch\n    return instance_loss.mean()"}, "static_ok": true, "static_reason": "", "dynamic_ok": true, "dynamic_reason": "ok", "loss_value": 0.6750055551528931, "grad_norm": 0.0}
{"generation": 2, "index": 6, "ir": {"name": "Rank-Gapped Tanh-Logistic Loss", "intuition": "This loss function combines an adaptive margin with a rank-based weighting scheme to create a robust preference learning objective. \n\nInherited Ideas:\n- From both parents, it inherits the core logistic loss structure `softplus(margin - delta_logp)`, which aims to make the log-probability difference `delta_logp` greater than a target margin.\n- From both parents, it also inherits the idea of an adaptive margin proportional to the cost difference (`alpha * delta_cost`), encouraging a larger log-probability gap for pairs with more distinct costs.\n\nNew Coupling Ideas:\n1. **Rank-based Normalization (`rank_gap`)**: Instead of z-scoring the cost differences, which is sensitive to outliers, this loss uses a rank-based normalization. `rank_gap` maps the cost differences to a uniform distribution over [-1, 1] based on their rank order. This creates a stable, outlier-resistant signal that captures the relative significance of the cost gap within the batch.\n2. **Symmetric Tanh Gating**: This rank-gapped signal is then fed into a `tanh` function. The result is used as a symmetric gate that modulates the loss. Unlike the parents' `0.5 * (1 + tanh(...))` which creates a weight from 0 to 1, this uses `tanh` directly. This means pairs with a median cost gap (rank_gap near 0) have their loss down-weighted, while pairs with very low or very high cost gaps (rank_gap near -1 or 1) receive a stronger learning signal. This focuses training on the most and least discriminative pairs in the batch, rather than just the most discriminative ones.", "pseudocode": "1. Calculate the cost difference: delta_cost = cost(b) - cost(a).\n2. Calculate the log-probability difference: delta_logp = logp(a) - logp(b).\n3. Compute an adaptive margin proportional to the cost difference, clamped for stability: margin = clamp(alpha * delta_cost, 0, max_margin).\n4. Compute the core loss using a softplus function: core_loss = softplus(margin - delta_logp).\n5. Normalize the cost differences using rank-based scaling to get 'rank_gapped_cost', which maps values to the [-1, 1] range based on their rank within the batch.\n6. Compute a symmetric gating factor using the tanh function on the rank-gapped costs: gate = tanh(beta * rank_gapped_cost).\n7. The final loss is the product of the absolute value of the gate and the core loss. Using the absolute value ensures the weight is always non-negative: final_loss = abs(gate) * core_loss.\n8. Average the final loss over the batch.", "hyperparams": {"alpha": 0.5, "beta": 2.0, "max_margin": 5.0, "eps": 1e-08}, "operators_used": ["softplus", "clamp", "tanh", "rank_gap"], "implementation_hint": {"expects": ["cost_a", "cost_b", "logp_a", "logp_b", "weight"], "returns": "scalar"}, "code": "import torch\nimport torch.nn.functional as F\n\ndef rank_gap(x, eps=1e-8):\n    \"\"\"Maps a tensor to [-1, 1] based on rank, handling ties.\"\"\"\n    if x.numel() <= 1:\n        return torch.zeros_like(x)\n    # Get ranks, average for ties\n    ranks = x.argsort().argsort().float()\n    # Scale to [0, 1]\n    ranks = ranks / (x.numel() - 1 + eps)\n    # Scale to [-1, 1]\n    return 2.0 * ranks - 1.0\n\ndef generated_loss(batch, model_output, extra):\n    \"\"\"\n    Rank-Gapped Tanh-Logistic Loss.\n\n    Inherits the adaptive margin logistic loss from parents.\n    Introduces two new couplings:\n    1. rank_gap normalization: Replaces z-score with a rank-based mapping to [-1, 1],\n       making the weighting robust to outlier cost differences.\n    2. Symmetric tanh gating: Uses tanh on the rank-gapped signal to create a symmetric\n       weighting factor. This focuses learning on pairs with the most and least significant\n       cost gaps within the batch, rather than only the most significant.\n    \"\"\"\n    # Read hyperparameters from the 'extra' dictionary\n    alpha = extra.get('alpha', 0.5)\n    beta = extra.get('beta', 2.0)\n    max_margin = extra.get('max_margin', 5.0)\n    eps = extra.get('eps', 1e-8)\n\n    # Unpack tensors from the batch\n    # By convention, 'w' is winner (lower cost), 'l' is loser (higher cost)\n    cost_w = batch['cost_a']\n    cost_l = batch['cost_b']\n    logp_w = batch['log_prob_w']\n    logp_l = batch['log_prob_l']\n\n    # Sanity check: cost_w should be less than or equal to cost_l\n    assert torch.all(cost_w <= cost_l), \"Assumption failed: cost_a (winner) must be <= cost_b (loser)\"\n\n    # 1. Calculate cost and log-probability differences\n    delta_cost = cost_l - cost_w  # This will be >= 0\n    delta_logp = logp_w - logp_l # We want this to be positive\n\n    # 2. Compute the adaptive margin (Inherited from parents)\n    margin = torch.clamp(alpha * delta_cost, min=0.0, max=max_margin)\n\n    # 3. Compute the core loss term (Inherited from parents)\n    core_loss = F.softplus(margin - delta_logp)\n\n    # 4. Compute rank-based normalization (New Coupling Idea 1)\n    # This is more robust to outliers than z-scoring.\n    rank_gapped_cost = rank_gap(delta_cost, eps=eps)\n\n    # 5. Compute symmetric tanh gating factor (New Coupling Idea 2)\n    # This creates a weight that is small for median-gap pairs and large for extreme-gap pairs.\n    # abs() ensures the final weight is non-negative.\n    symmetric_gate = torch.abs(torch.tanh(beta * rank_gapped_cost))\n\n    # 6. Combine core loss and the symmetric gate\n    instance_loss = symmetric_gate * core_loss\n\n    # Apply optional sample weights if provided\n    if 'weight' in batch and batch['weight'] is not None:\n        instance_loss = instance_loss * batch['weight']\n\n    # 7. Return the mean loss over the batch\n    return instance_loss.mean()"}, "static_ok": true, "static_reason": "", "dynamic_ok": false, "dynamic_reason": "forward_error: name 'rank_gap' is not defined", "loss_value": null, "grad_norm": null}
{"generation": 2, "index": 7, "ir": {"name": "Rank-Gap Modulated Logistic Loss", "intuition": "This loss function combines the core logistic loss structure of its parents with a novel rank-based modulation scheme. It inherits the fundamental Bradley-Terry style objective, `softplus(margin - delta_logp)`, where the margin is proportional to the cost difference. This encourages the model to create a log-probability gap `delta_logp` that is at least as large as the scaled cost gap `margin`.\n\nFrom the parents, we inherit:\n1. The `softplus(margin - delta_logp)` core, which is a smooth, stable hinge loss.\n2. The adaptive margin `clamp(alpha * delta_cost, ...)` which scales the learning target with the magnitude of the cost difference.\n\nAs a new coupling idea, we replace the parents' `tanh`-based weighting with a mechanism based on the relative ranking of cost differences. We compute a 'rank gap' for each pair, which indicates how its cost difference compares to others in the batch (e.g., a value of 0.9 means its cost gap is larger than 90% of the pairs). This rank is then used to modulate the loss in two ways:\n1. **Rank-based Margin Scaling**: The margin is scaled by `(1 + rank_gap)`, giving more challenging margins to pairs with higher relative cost differences. This focuses the model on correctly ordering the most distinct pairs.\n2. **Log-Prob Difference Clipping**: To prevent pairs that are already correctly ordered by a large margin from dominating the gradient, we clip the `delta_logp` from below using `relu`. The clipped value is then scaled by `(1 - rank_gap)`. This acts as a stability trick, gently reducing the influence of 'easy' pairs (those with low rank gaps) that already have a large positive logp difference, allowing the model to focus on more difficult cases.", "pseudocode": "1. Calculate the cost difference for each pair: delta_cost = cost(loser) - cost(winner).\n2. Calculate the log-probability difference: delta_logp = logp(winner) - logp(loser).\n3. Inherited Idea 1 (Core Loss): The base loss structure is a margin-based logistic loss.\n4. Inherited Idea 2 (Adaptive Margin): An initial margin is calculated proportionally to the raw cost difference: initial_margin = alpha * delta_cost.\n5. New Coupling 1 (Rank-based Margin Scaling): Calculate the 'rank_gap' of each delta_cost within the batch, which is its percentile rank scaled to [0, 1]. The final margin is scaled by this rank: margin = initial_margin * (1 + rank_gap). This makes the target separation larger for pairs with a higher relative cost difference.\n6. New Coupling 2 (Clipped Log-Prob Difference): To stabilize training and reduce noise from easy examples, clip the log-probability difference from below at zero using `relu`. Then, scale this clipped value by `(1 - rank_gap)` to down-weight the contribution from pairs with very small cost differences: clipped_delta_logp = relu(delta_logp) * (1 - rank_gap).\n7. Compute the final loss using the softplus operator on the difference between the rank-scaled margin and the clipped log-probability difference: loss = softplus(margin - clipped_delta_logp).\n8. Return the mean loss over the batch.", "hyperparams": {"alpha": 0.5, "eps": 1e-08}, "operators_used": ["softplus", "clamp", "relu", "rank_gap"], "implementation_hint": {"expects": ["cost_a", "cost_b", "logp_w", "logp_l", "weight"], "returns": "scalar"}, "code": "import torch\nimport torch.nn.functional as F\n\ndef _rank_gap(x):\n    \"\"\"Computes the percentile rank of each element in a 1D tensor, scaled to [0, 1].\"\"\"\n    if x.numel() <= 1:\n        return torch.zeros_like(x)\n    # Get the rank of each element, then scale to [0, numel-1]\n    ranks = x.argsort().argsort().float()\n    # Scale to [0, 1]\n    return ranks / (x.numel() - 1)\n\ndef generated_loss(batch, model_output, extra):\n    \"\"\"\n    Rank-Gap Modulated Logistic Loss.\n\n    This loss uses a logistic objective `softplus(margin - delta_logp)` where the margin is\n    dynamically scaled by the percentile rank of the cost difference within the batch.\n    It also clips the log-probability difference to stabilize training for pairs that\n    are already well-separated.\n\n    Inherits: `softplus` core and `alpha * delta_cost` margin from parents.\n    New Couplings:\n    1. Margin is scaled by `(1 + rank_gap)` to create larger targets for relatively large cost differences.\n    2. `delta_logp` is clipped and scaled by `(1 - rank_gap)` to reduce gradient noise from easy examples.\n    \"\"\"\n    # Read hyperparameters\n    alpha = extra.get('alpha', 0.5)\n    eps = extra.get('eps', 1e-8)\n\n    # Unpack tensors from the batch\n    # By convention, 'w' is winner (lower cost), 'l' is loser (higher cost)\n    cost_w = batch['cost_a']\n    cost_l = batch['cost_b']\n    logp_w = batch['log_prob_w']\n    logp_l = batch['log_prob_l']\n\n    # Sanity check\n    assert torch.all(cost_w <= cost_l), \"Assumption failed: cost_a (winner) must be <= cost_b (loser)\"\n\n    # 1. Calculate cost and log-probability differences\n    delta_cost = cost_l - cost_w  # Should be >= 0\n    delta_logp = logp_w - logp_l # We want this to be positive\n\n    # 2. New Coupling 1: Calculate rank-based modulator\n    # rank_gap is the percentile rank of each cost difference, from 0 to 1.\n    rank_gap = _rank_gap(delta_cost)\n\n    # 3. Inherited Idea: Compute an initial margin proportional to cost difference\n    initial_margin = alpha * delta_cost\n\n    # 4. New Coupling 1 (cont.): Scale the margin by the rank gap\n    # This makes the target separation larger for pairs with higher relative cost differences.\n    margin = initial_margin * (1.0 + rank_gap)\n    \n    # 5. New Coupling 2: Clip and scale the log-probability difference\n    # relu(delta_logp) clips from below at 0, focusing on misordered pairs.\n    # Scaling by (1 - rank_gap) reduces the contribution of pairs with small cost differences\n    # that might already have a large logp gap, preventing them from dominating the loss.\n    clipped_delta_logp = F.relu(delta_logp) * (1.0 - rank_gap)\n\n    # 6. Inherited Idea (Core Loss): Compute the final instance loss\n    # The core is a logistic-style loss.\n    instance_loss = F.softplus(margin - clipped_delta_logp)\n\n    # Apply optional sample weights if provided\n    if 'weight' in batch and batch['weight'] is not None:\n        instance_loss = instance_loss * batch['weight']\n\n    # 7. Return the mean loss over the batch\n    return instance_loss.mean()"}, "static_ok": true, "static_reason": "", "dynamic_ok": false, "dynamic_reason": "forward_error: name '_rank_gap' is not defined", "loss_value": null, "grad_norm": null}
{"generation": 3, "index": 0, "ir": {"name": "Dynamic Rank-Margin Hinge Loss", "intuition": "This loss function creates a dynamic, rank-aware margin that adapts to both the relative and absolute cost differences within a batch, combining ideas from both parents while introducing a novel hinge-based structure.\n\nInherited Ideas:\n- From Parent 0 (`Rank-Gapped Tanh-Modulated Logistic Loss`), it inherits the core concept of a **rank-based margin**. The margin is determined by the relative importance of a pair's cost difference (`delta_cost`) within the batch, making it robust to outliers. We use `rank_gap` to map `delta_cost` to a uniform distribution.\n- From Parent 1 (`Adaptive Tanh Margin Loss`), it inherits the use of an **adaptive weight based on a scaled tanh function** of the z-scored cost difference. This `adaptive_weight` smoothly emphasizes pairs with cost differences that are significantly above the batch average, providing a stable and balanced weighting scheme.\n\nNew Coupling Ideas:\n1.  **Dynamic Margin Scaling**: The rank-based margin is dynamically scaled by the *absolute* cost difference. The final margin is `gamma * rank_gap(delta_cost) * softplus(delta_cost)`. This coupling ensures that pairs with a high relative rank *and* a large absolute cost difference are required to have a much larger log-probability separation. The `softplus` function is used instead of the raw `delta_cost` to ensure the scaling factor is always positive and smooth, avoiding abrupt changes near zero.\n2.  **Hinge Loss Formulation**: Instead of a logistic loss (`logsigmoid` or `softplus`), the core loss is a **hinge loss** implemented with `relu(margin - delta_logp)`. This creates a 'satisfaction' condition: once `delta_logp` exceeds the dynamic margin, the loss for that pair becomes exactly zero. This focuses the model's capacity on 'difficult' pairs that have not yet met their required log-probability gap, preventing it from wasting effort on pairs that are already well-separated.", "pseudocode": "1. Calculate the cost difference: delta_cost = cost(b) - cost(a).\n2. Calculate the log-probability difference: delta_logp = logp(a) - logp(b).\n3. (Inherited from Parent 1) Normalize delta_cost using z-scoring to get 'normalized_delta_cost'.\n4. (Inherited from Parent 1) Compute an adaptive weight using a scaled tanh function on the normalized cost difference: adaptive_weight = 0.5 * (1 + tanh(beta * normalized_delta_cost)).\n5. (Inherited from Parent 0) Compute the rank gap of delta_cost, which maps values to a [0, 1] range based on their sorted order.\n6. (New Coupling) Create a dynamic margin by coupling the rank gap with the absolute cost difference: margin = gamma * rank_gap(delta_cost) * softplus(delta_cost).\n7. (New Coupling) Compute the core loss using a hinge formulation (ReLU): core_loss = relu(margin - delta_logp).\n8. The final loss is the product of the adaptive weight and the core hinge loss, averaged over the batch: mean(adaptive_weight * core_loss).", "hyperparams": {"beta": 1.0, "gamma": 0.5, "eps": 1e-08}, "operators_used": ["relu", "tanh", "softplus", "zscore", "rank_gap"], "implementation_hint": {"expects": ["cost_a", "cost_b", "log_prob_w", "log_prob_l", "weight"], "returns": "scalar"}, "code": "import torch\nimport torch.nn.functional as F\n\ndef generated_loss(batch, model_output, extra):\n    \"\"\"\n    Dynamic Rank-Margin Hinge Loss.\n\n    This loss combines a rank-based margin (from Parent 0) with a tanh-based\n    adaptive weight (from Parent 1). It introduces two new couplings: the rank-based\n    margin is dynamically scaled by the absolute cost difference, and the core loss\n    is a hinge loss (ReLU) instead of a logistic one.\n    \"\"\"\n    # Read hyperparameters from the 'extra' dictionary\n    beta = extra.get('beta', 1.0)\n    gamma = extra.get('gamma', 0.5)\n    eps = extra.get('eps', 1e-8)\n\n    # Unpack tensors from the batch\n    cost_w = batch['cost_a']\n    cost_l = batch['cost_b']\n    logp_w = batch['log_prob_w']\n    logp_l = batch['log_prob_l']\n\n    # Ensure winner has lower or equal cost\n    assert torch.all(cost_w <= cost_l), \"Assumption failed: cost_a (winner) must be <= cost_b (loser)\"\n\n    # 1. Calculate cost and log-probability differences\n    delta_cost = cost_l - cost_w  # Should be >= 0\n    delta_logp = logp_w - logp_l # We want this to be positive\n\n    # 2. (Inherited) Normalize delta_cost using z-score for stable tanh input\n    if delta_cost.numel() > 1:\n        mean_delta_cost = delta_cost.mean()\n        std_delta_cost = delta_cost.std() + eps\n        normalized_delta_cost = (delta_cost - mean_delta_cost) / std_delta_cost\n    else:\n        normalized_delta_cost = torch.zeros_like(delta_cost)\n\n    # 3. (Inherited) Compute adaptive weight using tanh\n    adaptive_weight = 0.5 * (1.0 + torch.tanh(beta * normalized_delta_cost))\n\n    # 4. (Inherited) Compute rank gap for the margin\n    if delta_cost.numel() > 1:\n        ranks = delta_cost.argsort().argsort().float()\n        rank_gaps = ranks / (delta_cost.numel() - 1)\n    else:\n        rank_gaps = torch.ones_like(delta_cost)\n\n    # 5. (New Coupling) Create a dynamic margin\n    # The margin is scaled by both its rank and its absolute magnitude (via softplus).\n    margin_scale = F.softplus(delta_cost)\n    margin = gamma * rank_gaps * margin_scale\n\n    # 6. (New Coupling) Compute the core loss using a hinge formulation (ReLU)\n    # The loss is zero if delta_logp is greater than the dynamic margin.\n    core_loss = F.relu(margin - delta_logp)\n\n    # 7. Combine the adaptive weight and the core loss\n    instance_loss = adaptive_weight * core_loss\n\n    # Apply optional sample weights if provided\n    if 'weight' in batch and batch['weight'] is not None:\n        instance_loss = instance_loss * batch['weight']\n\n    # 8. Return the mean loss over the batch\n    return instance_loss.mean()"}, "static_ok": true, "static_reason": "", "dynamic_ok": true, "dynamic_reason": "ok", "loss_value": 0.1641577035188675, "grad_norm": 0.0}
{"generation": 3, "index": 1, "ir": {"name": "Adaptive Rank-Modulated Exponential Loss", "intuition": "This loss function synthesizes the rank-based margin concept from one parent with the adaptive tanh-weighting from both, while introducing a new exponential loss formulation and a dynamic margin scaling mechanism.\n\nInherited Ideas:\n- From Parent 0 (`Rank-Gapped Tanh-Modulated Logistic Loss`), it inherits the use of a `rank_gap` function to determine the relative importance of a preference pair. This makes the loss robust to cost outliers by focusing on the percentile rank of the cost difference (`delta_cost`) within the batch, rather than its absolute magnitude.\n- From both parents, it inherits the use of a `tanh` function on the z-scored `delta_cost` to create an adaptive weight (`adaptive_weight`). This weight smoothly de-emphasizes pairs with cost differences far below the batch average, focusing learning on more informative pairs.\n\nNew Coupling Ideas:\n1. **Exponential Loss Formulation**: Instead of the common logistic loss (`logsigmoid` or `softplus`), this child uses an exponential loss: `exp(margin - delta_logp)`. This formulation more aggressively penalizes misclassifications where `delta_logp` is much smaller than the target `margin`, potentially leading to faster convergence on difficult examples.\n2. **Dynamic Margin Scaling**: A new coupling is introduced where the `rank_gap` is not the margin itself, but rather a modulator for a margin that is also proportional to the `delta_cost`. The margin is calculated as `gamma * rank_gap(delta_cost) * delta_cost`. This creates a dynamic margin that is sensitive to both the absolute cost difference (like in Parent 1) and its relative importance within the batch (like in Parent 0). This prevents pairs with high rank but tiny absolute cost gaps from demanding an overly large log-probability separation, grounding the margin in the actual cost scale.", "pseudocode": "1. Calculate the cost difference: delta_cost = cost(b) - cost(a).\n2. Calculate the log-probability difference: delta_logp = logp(a) - logp(b).\n3. (Inherited) Normalize delta_cost using z-scoring to get 'normalized_delta_cost'.\n4. (Inherited) Compute an adaptive weight using a scaled tanh function on the normalized cost difference: adaptive_weight = 0.5 * (1 + tanh(beta * normalized_delta_cost)).\n5. (New Coupling) Compute a dynamic margin. First, calculate the rank_gap of delta_cost, which maps values to a [0, 1] range. Then, compute the margin as the product of a hyperparameter `gamma`, the rank_gap, and the original delta_cost: margin = gamma * rank_gap(delta_cost) * delta_cost.\n6. (New) Compute the core loss using an exponential formulation: core_loss = exp(margin - delta_logp).\n7. The final loss is the product of the adaptive weight and the core loss, averaged over the batch: mean(adaptive_weight * core_loss).", "hyperparams": {"beta": 1.0, "gamma": 0.5, "eps": 1e-08}, "operators_used": ["exp", "tanh", "zscore", "rank_gap"], "implementation_hint": {"expects": ["cost_a", "cost_b", "logp_a", "logp_b"], "returns": "scalar"}, "code": "import torch\nimport torch.nn.functional as F\n\ndef generated_loss(batch, model_output, extra):\n    \"\"\"\n    Adaptive Rank-Modulated Exponential Loss.\n\n    This loss combines several ideas:\n    1. An exponential loss core `exp(margin - delta_logp)` for aggressive penalization.\n    2. An adaptive tanh-based weight on z-scored cost differences (from both parents).\n    3. A dynamic margin that is a product of the cost difference and its normalized rank,\n       making it sensitive to both absolute and relative cost importance.\n    \"\"\"\n    # Read hyperparameters from the 'extra' dictionary\n    beta = extra.get('beta', 1.0)\n    gamma = extra.get('gamma', 0.5)\n    eps = extra.get('eps', 1e-8)\n\n    # Unpack tensors from the batch\n    # By convention, 'w' is winner (lower cost), 'l' is loser (higher cost)\n    cost_w = batch['cost_a']\n    cost_l = batch['cost_b']\n    logp_w = batch['log_prob_w']\n    logp_l = batch['log_prob_l']\n\n    # Sanity check: cost_w should be less than or equal to cost_l\n    assert torch.all(cost_w <= cost_l), \"Assumption failed: cost_a (winner) must be <= cost_b (loser)\"\n\n    # 1. Calculate cost and log-probability differences\n    delta_cost = cost_l - cost_w  # This will be >= 0\n    delta_logp = logp_w - logp_l # We want this to be positive\n\n    # 2. (Inherited) Normalize delta_cost for stable tanh input (z-score)\n    if delta_cost.numel() > 1:\n        mean_delta_cost = delta_cost.mean()\n        std_delta_cost = delta_cost.std().clamp(min=eps)\n        normalized_delta_cost = (delta_cost - mean_delta_cost) / std_delta_cost\n    else:\n        normalized_delta_cost = torch.zeros_like(delta_cost)\n\n    # 3. (Inherited) Compute adaptive weight with tanh\n    adaptive_weight = 0.5 * (1.0 + torch.tanh(beta * normalized_delta_cost))\n\n    # 4. (New Coupling) Compute the dynamic rank-modulated margin\n    # First, get the normalized rank (rank_gap)\n    if delta_cost.numel() > 1:\n        # argsort().argsort() gives the rank of each element\n        ranks = delta_cost.argsort().argsort().float()\n        rank_gaps = ranks / (delta_cost.numel() - 1)\n    else:\n        rank_gaps = torch.ones_like(delta_cost) # Only one element, rank is 1.0\n    \n    # The margin is scaled by both its rank and its absolute value\n    margin = gamma * rank_gaps * delta_cost\n\n    # 5. (New) Compute the core exponential loss\n    # The loss is low when delta_logp > margin.\n    # Using exp() penalizes large negative values of (delta_logp - margin) more heavily.\n    core_loss = torch.exp(margin - delta_logp)\n\n    # 6. Combine core loss and adaptive weight\n    instance_loss = adaptive_weight * core_loss\n\n    # Apply optional sample weights if provided\n    if 'weight' in batch and batch['weight'] is not None:\n        instance_loss = instance_loss * batch['weight']\n\n    # 7. Return the mean loss over the batch\n    return instance_loss.mean()"}, "static_ok": true, "static_reason": "", "dynamic_ok": true, "dynamic_reason": "ok", "loss_value": 0.6496188044548035, "grad_norm": 0.0}
{"generation": 3, "index": 2, "ir": {"name": "Adaptive Rank-Modulated Hinge Loss", "intuition": "This loss function creates a hybrid objective by combining a rank-based margin from one parent with an adaptive weighting scheme from both, and then applies these to a hinge loss structure instead of a logistic one. The goal is to create a more robust loss that focuses on 'hard' examples where the model's preference is incorrect, while still being sensitive to the relative importance of preference pairs.\n\nInherited Ideas:\n- From Parent 0 (Rank-Gapped Tanh-Modulated Logistic Loss), it inherits the idea of a **rank-based margin**. The margin that the model must overcome is not proportional to the absolute cost difference but to its rank within the batch. This prevents outliers with large cost gaps from dominating the loss and makes the objective focus on the relative ordering of preferences.\n- From both Parent 0 and Parent 1, it inherits the use of a **tanh-based adaptive weight** on the z-scored cost difference. This weight (`adaptive_weight`) smoothly emphasizes pairs with an above-average cost difference, effectively focusing the training on more significant preference pairs while down-weighting pairs that are either too similar (low signal) or already well-separated.\n\nNew Coupling Ideas:\n1.  **Hinge Loss Core**: The core loss is changed from the logistic/softplus form of the parents to a hinge loss (`relu(margin - delta_logp)`). This creates a 'max-margin' objective. The loss is zero for 'easy' pairs where the log probability gap (`delta_logp`) already exceeds the rank-based margin, allowing the model to focus its capacity exclusively on 'hard' or 'misclassified' pairs where `delta_logp < margin`. This can lead to faster convergence on difficult examples.\n2.  **Modulation of Hinge Loss**: The final loss is a direct product of the inherited `adaptive_weight` and the new hinge loss core. This coupling ensures that the learning signal is only applied to pairs that are both misclassified (non-zero hinge loss) and considered important by the adaptive weighting scheme (high tanh weight). This synergy prevents the model from wasting effort on trivial misclassifications (e.g., pairs with very small cost differences that happen to fall just below the margin).", "pseudocode": "1. Calculate the cost difference: delta_cost = cost(b) - cost(a).\n2. Calculate the log-probability difference: delta_logp = logp(a) - logp(b).\n3. (Inherited) Normalize delta_cost using z-scoring to get 'normalized_delta_cost'.\n4. (Inherited) Compute an adaptive weight using a scaled tanh function on the normalized cost difference: adaptive_weight = 0.5 * (1 + tanh(beta * normalized_delta_cost)).\n5. (Inherited) Compute a rank-based margin. First, calculate the rank_gap of delta_cost, which maps values to a [0, 1] range based on their sorted order. Then, scale this by a hyperparameter `gamma`: margin = gamma * rank_gap(delta_cost).\n6. (New) Compute the core loss using a hinge (ReLU) function: core_loss = relu(margin - delta_logp).\n7. (New) The final loss is the product of the adaptive weight and the hinge loss, averaged over the batch: mean(adaptive_weight * core_loss).", "hyperparams": {"beta": 1.0, "gamma": 1.5, "eps": 1e-08}, "operators_used": ["relu", "tanh", "zscore", "rank_gap"], "implementation_hint": {"expects": ["cost_a", "cost_b", "log_prob_w", "log_prob_l"], "returns": "scalar"}, "code": "import torch\nimport torch.nn.functional as F\n\ndef generated_loss(batch, model_output, extra):\n    \"\"\"\n    Adaptive Rank-Modulated Hinge Loss.\n\n    This loss combines a rank-based margin with a tanh-based adaptive weight,\n    applied to a core hinge loss (ReLU). This focuses training on misclassified\n    pairs, weighted by their relative importance.\n\n    The preference `p(a > b)` is modeled based on `logp(a) - logp(b)`. The ground truth is that\n    the solution with the lower cost is preferred.\n\n    In the input, `log_prob_w` corresponds to the winner (lower cost) and `log_prob_l` to the loser.\n    Therefore, `cost_a` is the cost of the winner and `cost_b` is the cost of the loser, so cost_a < cost_b.\n    We want to encourage `log_prob_w > log_prob_l`.\n    \"\"\"\n    # Read hyperparameters from the 'extra' dictionary\n    beta = extra.get('beta', 1.0)\n    gamma = extra.get('gamma', 1.5)\n    eps = extra.get('eps', 1e-8)\n\n    # Unpack tensors from the batch\n    cost_w = batch['cost_a']\n    cost_l = batch['cost_b']\n    logp_w = batch['log_prob_w']\n    logp_l = batch['log_prob_l']\n    \n    # Sanity check: cost_w should be less than or equal to cost_l\n    assert torch.all(cost_w <= cost_l), \"Assumption failed: cost_a (winner) must be <= cost_b (loser)\"\n\n    # 1. Calculate cost and log-probability differences\n    delta_cost = cost_l - cost_w  # This will be >= 0\n    delta_logp = logp_w - logp_l # We want this to be positive\n\n    # 2. Normalize the cost difference (z-score) for the adaptive weight (Inherited)\n    if delta_cost.numel() > 1:\n        mean_delta_cost = delta_cost.mean()\n        std_delta_cost = delta_cost.std() + eps\n        normalized_delta_cost = (delta_cost - mean_delta_cost) / std_delta_cost\n    else:\n        normalized_delta_cost = torch.zeros_like(delta_cost)\n\n    # 3. Compute the adaptive weight using tanh (Inherited)\n    adaptive_weight = 0.5 * (1.0 + torch.tanh(beta * normalized_delta_cost))\n\n    # 4. Compute the rank-based margin (Inherited)\n    if delta_cost.numel() > 1:\n        ranks = delta_cost.argsort().argsort().float()\n        rank_gaps = ranks / (delta_cost.numel() - 1)\n    else:\n        rank_gaps = torch.ones_like(delta_cost)\n    margin = gamma * rank_gaps\n\n    # 5. Compute the core hinge loss (New Coupling)\n    # The loss is non-zero only if delta_logp is smaller than the required margin.\n    core_loss = F.relu(margin - delta_logp)\n\n    # 6. Combine core loss and adaptive weight (New Coupling)\n    # The final loss is modulated by the tanh weight.\n    instance_loss = adaptive_weight * core_loss\n    \n    # Apply optional sample weights if provided\n    if 'weight' in batch and batch['weight'] is not None:\n        instance_loss = instance_loss * batch['weight']\n\n    # 7. Return the mean loss over the batch\n    return instance_loss.mean()"}, "static_ok": true, "static_reason": "", "dynamic_ok": true, "dynamic_reason": "ok", "loss_value": 0.375, "grad_norm": 0.0}
{"generation": 3, "index": 3, "ir": {"name": "Adaptive Rank-Modulated Hinge Loss", "intuition": "This loss function creates a hybrid objective that combines a hinge-loss structure with adaptive weighting and a rank-based modulation. \n\nInherited Ideas:\n- From Parent 0 (Rank-Gapped Tanh-Modulated Logistic Loss), it inherits the concept of using the rank of the cost difference (`delta_cost`) to modulate the learning signal. Instead of using rank to define a margin, this child uses the `rank_gap` to directly scale the loss, making the penalty for mis-ordering pairs with a higher relative cost difference more severe. This provides robustness to cost outliers.\n- From Parent 1 (Adaptive Tanh Margin Loss), it inherits the use of a `tanh` function on the z-scored `delta_cost` to create an `adaptive_weight`. This weight smoothly de-emphasizes pairs with cost differences far below the batch average, focusing the model's attention on more meaningful comparisons.\n\nNew Coupling Ideas:\n1.  **Hinge Loss Core**: The central loss component is a hinge loss, `relu(margin - delta_logp)`. Unlike the logistic loss from the parents, which always provides a gradient, the hinge loss provides zero gradient once a pair is correctly classified by a sufficient margin (`delta_logp > margin`). This can lead to sparser gradients and potentially faster convergence once easy examples are learned.\n2.  **Dual Modulation**: The final loss is a product of three terms: the `adaptive_weight` (from tanh), the rank-based scaling (`rank_gap`), and the core `hinge_loss`. This dual modulation ensures that the learning signal is strongest for pairs that are both mis-ordered (`hinge_loss > 0`), have an above-average cost difference (high `adaptive_weight`), and are relatively important within the batch (high `rank_gap`). This creates a highly focused learning objective.", "pseudocode": "1. Calculate the cost difference: delta_cost = cost(b) - cost(a).\n2. Calculate the log-probability difference: delta_logp = logp(a) - logp(b).\n3. (Inherited from Parent 1) Normalize delta_cost using z-scoring to get 'normalized_delta_cost'.\n4. (Inherited from Parent 1) Compute an adaptive weight using a scaled tanh function on the normalized cost difference: adaptive_weight = 0.5 * (1 + tanh(beta * normalized_delta_cost)).\n5. (Inherited from Parent 0) Compute a rank-based scaling factor. Calculate the rank_gap of delta_cost, which maps values to a [0, 1] range based on their sorted order: rank_scale = rank_gap(delta_cost).\n6. (New) Compute a core hinge loss with a fixed margin: hinge_loss = relu(margin - delta_logp).\n7. (New Coupling) The final loss is the product of the hinge loss, the adaptive weight, and the rank-based scaling, averaged over the batch: mean(rank_scale * adaptive_weight * hinge_loss).", "hyperparams": {"margin": 1.0, "beta": 1.0, "eps": 1e-08}, "operators_used": ["relu", "tanh", "zscore", "rank_gap"], "implementation_hint": {"expects": ["cost_a", "cost_b", "logp_a", "logp_b"], "returns": "scalar"}, "code": "import torch\nimport torch.nn.functional as F\n\ndef generated_loss(batch, model_output, extra):\n    \"\"\"\n    Adaptive Rank-Modulated Hinge Loss.\n\n    This loss combines a hinge loss core with a dual modulation scheme:\n    1. An adaptive weight from tanh on z-scored cost differences (from Parent 1).\n    2. A rank-based scaling factor based on the relative importance of the cost difference\n       within the batch (inspired by Parent 0).\n    The final loss focuses on mis-ordered pairs that are both important relative to the batch\n    and have a significant absolute cost difference.\n    \"\"\"\n    # Read hyperparameters from the 'extra' dictionary\n    margin = extra.get('margin', 1.0)\n    beta = extra.get('beta', 1.0)\n    eps = extra.get('eps', 1e-8)\n\n    # Unpack tensors from the batch\n    cost_w = batch['cost_a']\n    cost_l = batch['cost_b']\n    logp_w = batch['log_prob_w']\n    logp_l = batch['log_prob_l']\n\n    # Sanity check: cost_w should be less than or equal to cost_l\n    assert torch.all(cost_w <= cost_l), \"Assumption failed: cost_a (winner) must be <= cost_b (loser)\"\n\n    # 1. Calculate cost and log-probability differences\n    delta_cost = cost_l - cost_w  # This will be >= 0\n    delta_logp = logp_w - logp_l # We want this to be positive\n\n    # 2. (Inherited from Parent 1) Normalize the cost difference for adaptive weight\n    if delta_cost.numel() > 1:\n        mean_delta_cost = delta_cost.mean()\n        std_delta_cost = delta_cost.std() + eps\n        normalized_delta_cost = (delta_cost - mean_delta_cost) / std_delta_cost\n    else:\n        normalized_delta_cost = torch.zeros_like(delta_cost)\n\n    # 3. (Inherited from Parent 1) Compute the adaptive weight using tanh\n    adaptive_weight = 0.5 * (1.0 + torch.tanh(beta * normalized_delta_cost))\n\n    # 4. (Inherited from Parent 0) Compute rank-based scaling factor\n    if delta_cost.numel() > 1:\n        ranks = delta_cost.argsort().argsort().float()\n        rank_scale = ranks / (delta_cost.numel() - 1)\n    else:\n        rank_scale = torch.ones_like(delta_cost)\n\n    # 5. (New) Compute the core hinge loss\n    # Loss is zero if delta_logp > margin, positive otherwise.\n    hinge_loss = F.relu(margin - delta_logp)\n\n    # 6. (New Coupling) Combine hinge loss with dual modulation\n    instance_loss = rank_scale * adaptive_weight * hinge_loss\n\n    # Apply optional sample weights if provided\n    if 'weight' in batch and batch['weight'] is not None:\n        instance_loss = instance_loss * batch['weight']\n\n    # 7. Return the mean loss over the batch\n    return instance_loss.mean()"}, "static_ok": true, "static_reason": "", "dynamic_ok": true, "dynamic_reason": "ok", "loss_value": 0.25, "grad_norm": 0.0}
{"generation": 3, "index": 4, "ir": {"name": "Adaptive Rank-Gapped Margin Loss", "intuition": "This loss function creates a robust learning signal by combining an adaptive margin with a cost-aware weighting scheme.\n\nInherited Ideas:\n- From Parent 0 (Rank-Gapped Tanh-Modulated Logistic Loss), it inherits the concept of a **rank-based margin**. The margin that the log-probability difference (`delta_logp`) must overcome is not based on the absolute cost difference but on its relative rank within the batch. This prevents pairs with outlier cost gaps from dominating the gradient and focuses the model on achieving the correct relative ordering of preferences.\n- From Parent 1 (Adaptive Tanh Margin Loss), it inherits the core structure of a **margin-based logistic loss** using `softplus(margin - delta_logp)`. This is a standard and stable formulation for preference learning.\n\nNew Coupling Ideas:\n1.  **Cost-Normalized Adaptive Weight**: Instead of using a `tanh` or `sigmoid` function on a z-scored cost difference, this child introduces a simpler and more direct weighting. The weight for each pair is the cost difference (`delta_cost`) normalized by the maximum cost difference in the batch. This creates a linear ramp from 0 to 1, ensuring that pairs with the largest cost separation contribute most to the loss, while pairs with very small (and potentially noisy) cost differences are down-weighted. This avoids the saturation issues of sigmoid/tanh and directly ties the learning signal's strength to the magnitude of the preference.\n2.  **Margin and Weight Synergy**: The final loss is the product of the cost-normalized weight and the rank-gapped margin loss. This coupling ensures that the learning signal is strongest for pairs that are both relatively important (high rank gap, leading to a large margin to overcome) and have a large absolute cost difference (high weight). A pair might have a high rank but a small absolute cost gap, resulting in a moderate loss. Conversely, a pair might have a large absolute cost gap but be ranked low (e.g., in a batch of all high-gap pairs), also resulting in a moderate loss. The strongest signal is reserved for pairs that are exceptional in both relative and absolute terms.", "pseudocode": "1. Calculate the cost difference: delta_cost = cost(b) - cost(a).\n2. Calculate the log-probability difference: delta_logp = logp(a) - logp(b).\n3. (Inherited from Parent 0) Compute a rank-based margin. First, calculate the rank_gap of delta_cost, which maps values to a [0, 1] range based on their sorted order. Then, scale this by a hyperparameter `gamma`: margin = gamma * rank_gap(delta_cost).\n4. (Inherited from Parent 1) Compute the core logistic loss using the rank-based margin: core_loss = softplus(margin - delta_logp).\n5. (New Coupling) Compute a cost-normalized adaptive weight. Divide each delta_cost by the maximum delta_cost in the batch (plus a small epsilon for stability). This directly scales the weight by the magnitude of the cost difference: adaptive_weight = delta_cost / max(delta_cost).\n6. The final loss is the product of the adaptive weight and the core loss, averaged over the batch: mean(adaptive_weight * core_loss).", "hyperparams": {"gamma": 2.5, "eps": 1e-08}, "operators_used": ["softplus", "rank_gap", "clamp"], "implementation_hint": {"expects": ["cost_a", "cost_b", "logp_a", "logp_b"], "returns": "scalar"}, "code": "import torch\nimport torch.nn.functional as F\n\ndef generated_loss(batch, model_output, extra):\n    \"\"\"\n    Adaptive Rank-Gapped Margin Loss.\n\n    This loss combines a margin based on the rank of the cost difference with an\n    adaptive weight directly proportional to the normalized cost difference.\n    \n    - The margin is inherited from Parent 0's rank-gap idea, making it robust to outliers.\n    - The core loss structure is a standard softplus-based margin loss, similar to Parent 1.\n    - The new coupling is a simple, direct adaptive weight: delta_cost / max(delta_cost), which\n      weights each pair's contribution by its normalized cost importance.\n    \"\"\"\n    # Read hyperparameters from the 'extra' dictionary\n    gamma = extra.get('gamma', 2.5)\n    eps = extra.get('eps', 1e-8)\n\n    # Unpack tensors from the batch\n    # By convention, 'w' is winner (lower cost), 'l' is loser (higher cost)\n    cost_w = batch['cost_a']\n    cost_l = batch['cost_b']\n    logp_w = batch['log_prob_w']\n    logp_l = batch['log_prob_l']\n\n    # Sanity check: cost_w should be less than or equal to cost_l\n    assert torch.all(cost_w <= cost_l), \"Assumption failed: cost_a (winner) must be <= cost_b (loser)\"\n\n    # 1. Calculate cost and log-probability differences\n    delta_cost = cost_l - cost_w  # This will be >= 0\n    delta_logp = logp_w - logp_l # We want this to be positive\n\n    # 2. Compute the rank-gapped margin (Inherited from Parent 0)\n    # This makes the margin robust to outliers and dependent on relative cost importance.\n    if delta_cost.numel() > 1:\n        # Calculate rank_gap: map values to [0, 1] based on sorted order\n        ranks = delta_cost.argsort().argsort().float()\n        rank_gaps = ranks / (delta_cost.numel() - 1)\n    else:\n        # Handle batch size of 1\n        rank_gaps = torch.ones_like(delta_cost)\n    margin = gamma * rank_gaps\n\n    # 3. Compute the core logistic loss (Inherited from Parent 1)\n    # The loss is low when delta_logp > margin.\n    core_loss = F.softplus(margin - delta_logp)\n\n    # 4. Compute the cost-normalized adaptive weight (New Coupling)\n    # This weight scales the loss contribution by the magnitude of the cost difference.\n    # Using clamp(min=eps) prevents division by zero if all costs are identical.\n    max_delta_cost = torch.clamp(delta_cost.max(), min=eps)\n    adaptive_weight = delta_cost / max_delta_cost\n\n    # 5. Combine core loss and adaptive weight\n    # The final loss is modulated by the cost-normalized weight.\n    instance_loss = adaptive_weight * core_loss\n\n    # Apply optional sample weights if provided\n    if 'weight' in batch and batch['weight'] is not None:\n        instance_loss = instance_loss * batch['weight']\n\n    # 6. Return the mean loss over the batch\n    return instance_loss.mean()"}, "static_ok": true, "static_reason": "", "dynamic_ok": true, "dynamic_reason": "ok", "loss_value": 1.5527472496032715, "grad_norm": 0.0}
{"generation": 3, "index": 5, "ir": {"name": "Rank-Modulated Tanh-Margin Logistic Loss", "intuition": "This loss function creates a novel synthesis between margin-based and weighting-based preference learning. It introduces a dynamic margin that adapts based on the relative importance of a preference pair within a batch, while also using a separate weighting scheme to focus on pairs with a significant cost difference.\n\nInherited Ideas:\n- From Parent 0 (`Rank-Gapped Tanh-Modulated Logistic Loss`), it inherits the concept of using the rank of the cost difference (`delta_cost`) to determine the learning signal. However, instead of using the rank to set a margin *inside* the logistic loss, this child uses it to create an external weight.\n- From Parent 1 (`Adaptive Tanh Margin Loss`), it inherits the use of a `tanh` function to create a smooth, adaptive term. However, instead of using `tanh` for weighting, it is repurposed to create a dynamic, bounded margin.\n- From both parents, it inherits the fundamental Bradley-Terry structure using `logsigmoid(delta_logp - margin)` for numerical stability and probabilistic interpretation.\n\nNew Coupling Ideas:\n1.  **Rank-Based Adaptive Weighting**: The `rank_gap` of the cost difference (`delta_cost`) is used to create a primary adaptive weight (`rank_weight`). This weight scales from 0 to 1, ensuring that pairs with a higher relative cost difference within the batch receive a proportionally larger learning signal. This makes the loss robust to the absolute scale of costs and focuses learning on the most discriminative pairs in the current batch context.\n2.  **Tanh-based Dynamic Margin**: The margin is no longer a simple function of `delta_cost`. Instead, `delta_cost` is z-scored and passed through a `tanh` function, then scaled by a hyperparameter `gamma`. This creates a margin that is sensitive to the cost difference but smoothly saturates, preventing extremely large `delta_cost` values from creating excessively large, potentially unstable margins. This couples the margin's scale to the statistical properties of the batch's cost differences.", "pseudocode": "1. Calculate the cost difference: delta_cost = cost(b) - cost(a).\n2. Calculate the log-probability difference: delta_logp = logp(a) - logp(b).\n3. (New Coupling 1 - Inspired by Parent 0) Compute a rank-based adaptive weight. First, calculate the rank_gap of delta_cost, which maps its values to a [0, 1] range based on their sorted order. This is the `rank_weight`.\n4. (Inherited from Parent 1) Normalize delta_cost using z-scoring to get 'normalized_delta_cost'.\n5. (New Coupling 2 - Inspired by Parent 1) Compute a dynamic, bounded margin. Apply a `tanh` function to the normalized_delta_cost and scale it by a hyperparameter `gamma`. This is the `margin`.\n6. (Inherited from Parents) Compute the core logistic loss using the dynamic margin: core_loss = -logsigmoid(delta_logp - margin).\n7. The final loss is the product of the rank-based adaptive weight and the core loss, averaged over the batch: mean(rank_weight * core_loss).", "hyperparams": {"gamma": 2.0, "eps": 1e-08}, "operators_used": ["logsigmoid", "tanh", "zscore", "rank_gap"], "implementation_hint": {"expects": ["cost_a", "cost_b", "logp_a", "logp_b"], "returns": "scalar"}, "code": "import torch\nimport torch.nn.functional as F\n\ndef generated_loss(batch, model_output, extra):\n    \"\"\"\n    Rank-Modulated Tanh-Margin Logistic Loss.\n\n    This loss combines a rank-based adaptive weight with a tanh-based dynamic margin.\n    The weight is determined by the rank of the cost difference, focusing on relatively important pairs.\n    The margin is a bounded function of the z-scored cost difference, preventing instability.\n\n    The preference `p(a > b)` is modeled based on `logp(a) - logp(b)`. The ground truth is that\n    the solution with the lower cost is preferred.\n\n    In the input, `log_prob_w` corresponds to the winner (lower cost) and `log_prob_l` to the loser.\n    Therefore, `cost_a` is the cost of the winner and `cost_b` is the cost of the loser, so cost_a < cost_b.\n    We want to encourage `log_prob_w > log_prob_l`.\n    \"\"\"\n    # Read hyperparameters from the 'extra' dictionary\n    gamma = extra.get('gamma', 2.0)\n    eps = extra.get('eps', 1e-8)\n\n    # Unpack tensors from the batch\n    cost_w = batch['cost_a']\n    cost_l = batch['cost_b']\n    logp_w = batch['log_prob_w']\n    logp_l = batch['log_prob_l']\n\n    # Sanity check: cost_w should be less than or equal to cost_l\n    assert torch.all(cost_w <= cost_l), \"Assumption failed: cost_a (winner) must be <= cost_b (loser)\"\n\n    # 1. Calculate cost and log-probability differences\n    delta_cost = cost_l - cost_w  # This will be >= 0\n    delta_logp = logp_w - logp_l  # We want this to be positive\n\n    # 2. (New Coupling 1) Compute rank-based adaptive weight (inspired by Parent 0)\n    # `rank_gap` maps delta_cost to [0, 1] based on its rank in the batch.\n    # This weight emphasizes pairs with a higher relative cost difference.\n    if delta_cost.numel() > 1:\n        ranks = delta_cost.argsort().argsort().float()\n        rank_weight = ranks / (delta_cost.numel() - 1)\n    else:\n        rank_weight = torch.ones_like(delta_cost)\n\n    # 3. (Inherited) Normalize the cost difference for the dynamic margin\n    if delta_cost.numel() > 1:\n        mean_delta_cost = delta_cost.mean()\n        std_delta_cost = delta_cost.std() + eps\n        normalized_delta_cost = (delta_cost - mean_delta_cost) / std_delta_cost\n    else:\n        normalized_delta_cost = torch.zeros_like(delta_cost)\n\n    # 4. (New Coupling 2) Compute tanh-based dynamic margin (inspired by Parent 1)\n    # The margin is a smooth, bounded function of the normalized cost difference.\n    margin = gamma * torch.tanh(normalized_delta_cost)\n\n    # 5. (Inherited) Compute the core logistic loss term\n    # We use -logsigmoid(x) which is equivalent to softplus(-x).\n    # The loss is low when delta_logp > margin.\n    core_loss = -F.logsigmoid(delta_logp - margin)\n\n    # 6. Combine rank weight and core loss\n    instance_loss = rank_weight * core_loss\n\n    # Apply optional sample weights if provided\n    if 'weight' in batch and batch['weight'] is not None:\n        instance_loss = instance_loss * batch['weight']\n\n    # 7. Return the mean loss over the batch\n    return instance_loss.mean()"}, "static_ok": true, "static_reason": "", "dynamic_ok": true, "dynamic_reason": "ok", "loss_value": 0.3465735912322998, "grad_norm": 0.0}
{"generation": 3, "index": 6, "ir": {"name": "Rank-Modulated Tanh-Margin Logistic Loss", "intuition": "This loss function creates a novel synthesis of margin-based and weighting-based preference learning. It inherits the core logistic loss structure and the use of a tanh function from its parents, but re-purposes the tanh function to create an adaptive margin rather than an external weight. This is coupled with a new normalization scheme for the margin input, making it robust to cost outliers.\n\nInherited Ideas:\n- From both parents, it inherits the use of a `tanh` function to adaptively scale the learning signal based on the cost difference (`delta_cost`).\n- From Parent 0 (`Rank-Gapped Tanh-Modulated Logistic Loss`), it inherits the core `logsigmoid` formulation for the Bradley-Terry objective, `logsigmoid(delta_logp - margin)`, which is numerically stable and encourages `delta_logp` to be larger than the margin.\n\nNew Coupling Ideas:\n1.  **Tanh-based Adaptive Margin**: Instead of using `tanh` to create a separate loss weight, this child loss directly incorporates the tanh output into the margin itself. The margin is defined as `margin_scale * tanh(margin_input)`. This creates a bounded, non-linear margin that saturates for very large or very small cost differences, preventing single outlier pairs from creating excessively large margin targets and dominating the gradient. The model is encouraged to create a log-probability gap that is proportional to the cost difference, but only up to a point, improving stability.\n2.  **Rank-Normalized Margin Input**: The input to the `tanh` function is not the raw or z-scored `delta_cost`, but rather its rank-normalized value. The `rank_gap` of `delta_cost` is calculated (mapping it to `[0, 1]`) and then shifted and scaled to `[-1, 1]` before being multiplied by a temperature `beta`. This makes the margin's behavior dependent on the relative importance of a pair's cost difference within the batch, not its absolute value. This coupling makes the loss robust to the scale and distribution of `delta_cost`, ensuring a consistent margin target distribution across different batches.", "pseudocode": "1. Calculate the cost difference: delta_cost = cost(b) - cost(a).\n2. Calculate the log-probability difference: delta_logp = logp(a) - logp(b).\n3. (New Coupling) Compute the input for the adaptive margin. First, find the rank_gap of delta_cost, which maps its values to a [0, 1] range based on their sorted order. Then, transform this to a [-1, 1] range: `centered_ranks = 2 * rank_gap(delta_cost) - 1`.\n4. (Inherited/Modified) Compute an adaptive margin using a scaled `tanh` function on the centered ranks: `margin = margin_scale * tanh(beta * centered_ranks)`. This creates a bounded margin that is non-linearly related to the relative rank of the cost difference.\n5. (Inherited) Compute the core logistic loss using the adaptive margin: `loss = -logsigmoid(delta_logp - margin)`.\n6. The final loss is the mean of the per-instance losses over the batch.", "hyperparams": {"margin_scale": 2.5, "beta": 1.5, "eps": 1e-08}, "operators_used": ["logsigmoid", "tanh", "rank_gap"], "implementation_hint": {"expects": ["cost_a", "cost_b", "logp_a", "logp_b"], "returns": "scalar"}, "code": "import torch\nimport torch.nn.functional as F\n\ndef generated_loss(batch, model_output, extra):\n    \"\"\"\n    Rank-Modulated Tanh-Margin Logistic Loss.\n\n    This loss uses a tanh function to create a bounded, adaptive margin. The input to the\n    tanh is derived from the rank of the cost difference within the batch, making the\n    margin robust to outliers and dependent on relative cost importance.\n\n    In the input, log_prob_w corresponds to the winner (lower cost) and log_prob_l to the loser.\n    Therefore, cost_a is the cost of the winner and cost_b is the cost of the loser.\n    We want to encourage log_prob_w > log_prob_l.\n    \"\"\"\n    # Read hyperparameters from the 'extra' dictionary\n    margin_scale = extra.get('margin_scale', 2.5)\n    beta = extra.get('beta', 1.5)\n    eps = extra.get('eps', 1e-8)\n\n    # Unpack tensors from the batch\n    cost_w = batch['cost_a']\n    cost_l = batch['cost_b']\n    logp_w = batch['log_prob_w']\n    logp_l = batch['log_prob_l']\n\n    # Sanity check: cost_w should be less than or equal to cost_l\n    assert torch.all(cost_w <= cost_l), \"Assumption failed: cost_a (winner) must be <= cost_b (loser)\"\n\n    # 1. Calculate cost and log-probability differences\n    delta_cost = cost_l - cost_w  # This will be >= 0\n    delta_logp = logp_w - logp_l # We want this to be positive\n\n    # 2. Compute the rank-normalized input for the margin (New Coupling)\n    # This makes the margin input robust to the scale and distribution of delta_cost.\n    if delta_cost.numel() > 1:\n        # `rank_gap` maps delta_cost to [0, 1] based on its sorted order.\n        ranks = delta_cost.argsort().argsort().float()\n        rank_gaps = ranks / (delta_cost.numel() - 1)\n        # Shift and scale to [-1, 1] range for tanh input\n        centered_ranks = 2.0 * rank_gaps - 1.0\n    else:\n        # For a single element, its rank is max, so rank_gap is 1. centered_rank is 1.\n        centered_ranks = torch.ones_like(delta_cost)\n    \n    margin_input = beta * centered_ranks\n\n    # 3. Compute the adaptive margin using tanh (Inherited/Modified)\n    # The margin is now a bounded, non-linear function of the rank-normalized cost difference.\n    # This prevents outlier cost pairs from creating excessively large margin targets.\n    margin = margin_scale * torch.tanh(margin_input)\n\n    # 4. Compute the core logistic loss (Inherited)\n    # The loss is low when delta_logp > margin.\n    # We use -logsigmoid(x) which is equivalent to softplus(-x).\n    instance_loss = -F.logsigmoid(delta_logp - margin)\n\n    # Apply optional sample weights if provided\n    if 'weight' in batch and batch['weight'] is not None:\n        instance_loss = instance_loss * batch['weight']\n\n    # 5. Return the mean loss over the batch\n    return instance_loss.mean()"}, "static_ok": true, "static_reason": "", "dynamic_ok": true, "dynamic_reason": "ok", "loss_value": 0.9824560880661011, "grad_norm": 0.0}
{"generation": 3, "index": 7, "ir": {"name": "Rank-Gapped Exponential Margin Loss", "intuition": "This loss function creates a dynamic margin that is sensitive to both the relative importance of a preference pair within a batch and the absolute magnitude of its cost difference, while maintaining the core logistic loss structure.\n\nInherited Ideas:\n- From Parent 0 (`Rank-Gapped Tanh-Modulated Logistic Loss`), it inherits the concept of a `rank_gap` based margin. This makes the loss robust to outliers in cost differences by considering the relative ranking of a pair's cost gap, ensuring that no single pair with an extreme cost difference dominates the training signal.\n- From Parent 1 (`Adaptive Tanh Margin Loss`), it inherits the use of an adaptive, cost-proportional margin. However, instead of a simple linear scaling, this child adapts it into an exponential form.\n\nNew Coupling Ideas:\n1. **Exponential Rank-Coupled Margin**: The core innovation is coupling the rank-based and cost-based margin ideas. The margin is defined as `gamma * rank_gap(delta_cost) * exp(alpha * normalized_delta_cost)`. This creates a margin that has a baseline determined by its relative importance (`rank_gap`) which is then exponentially scaled by its z-scored absolute cost difference. This strongly incentivizes the model to achieve a large log-probability gap for pairs that are both highly ranked *and* have a significantly above-average cost difference, creating a more nuanced and powerful learning signal than either parent's margin alone.\n2. **Margin Clamping for Stability**: To prevent the exponential term from causing numerical instability or creating excessively large targets, the final margin is clamped to a maximum value (`max_margin`). This acts as a stability trick, ensuring the learning target remains within a reasonable range, preventing exploding gradients for pairs with extremely large `delta_cost` values.", "pseudocode": "1. Calculate the cost difference: delta_cost = cost(b) - cost(a).\n2. Calculate the log-probability difference: delta_logp = logp(a) - logp(b).\n3. (Inherited) Compute the rank_gap of delta_cost, which maps the values to a [0, 1] range based on their sorted order.\n4. (New) Normalize delta_cost using z-scoring to get 'normalized_delta_cost'.\n5. (New Coupling) Compute a dynamic margin by coupling the rank gap with an exponential function of the normalized cost difference: dynamic_margin = gamma * rank_gap * exp(alpha * normalized_delta_cost).\n6. (New Stability) Clamp the dynamic margin to a maximum value to prevent instability: margin = clamp(dynamic_margin, 0, max_margin).\n7. Compute the final logistic loss using the logsigmoid function: loss = -logsigmoid(delta_logp - margin).\n8. Return the mean of the loss over the batch.", "hyperparams": {"gamma": 1.5, "alpha": 0.5, "max_margin": 10.0, "eps": 1e-08}, "operators_used": ["logsigmoid", "exp", "zscore", "rank_gap", "clamp"], "implementation_hint": {"expects": ["cost_a", "cost_b", "logp_a", "logp_b"], "returns": "scalar"}, "code": "import torch\nimport torch.nn.functional as F\n\ndef generated_loss(batch, model_output, extra):\n    \"\"\"\n    Rank-Gapped Exponential Margin Loss.\n\n    This loss combines a rank-based margin with an exponential scaling based on the\n    z-scored cost difference, creating a dynamic and powerful learning target.\n\n    In the input, `log_prob_w` corresponds to the winner (lower cost) and `log_prob_l` to the loser.\n    We want to encourage `log_prob_w > log_prob_l`.\n    \"\"\"\n    # Read hyperparameters from the 'extra' dictionary\n    gamma = extra.get('gamma', 1.5)\n    alpha = extra.get('alpha', 0.5)\n    max_margin = extra.get('max_margin', 10.0)\n    eps = extra.get('eps', 1e-8)\n\n    # Unpack tensors from the batch\n    cost_w = batch['cost_a']\n    cost_l = batch['cost_b']\n    logp_w = batch['log_prob_w']\n    logp_l = batch['log_prob_l']\n    \n    # Sanity check: cost_w should be less than or equal to cost_l\n    assert torch.all(cost_w <= cost_l), \"Assumption failed: cost_a (winner) must be <= cost_b (loser)\"\n\n    # 1. Calculate cost and log-probability differences\n    delta_cost = cost_l - cost_w  # This will be >= 0\n    delta_logp = logp_w - logp_l # We want this to be positive\n\n    # 2. Compute rank_gap (Inherited from Parent 0)\n    # This provides a baseline margin based on relative importance within the batch.\n    if delta_cost.numel() > 1:\n        ranks = delta_cost.argsort().argsort().float()\n        rank_gaps = ranks / (delta_cost.numel() - 1)\n    else:\n        rank_gaps = torch.ones_like(delta_cost)\n\n    # 3. Normalize delta_cost using z-score (New)\n    # This prepares the cost difference for the exponential scaling.\n    if delta_cost.numel() > 1:\n        mean_delta_cost = delta_cost.mean()\n        std_delta_cost = delta_cost.std() + eps\n        normalized_delta_cost = (delta_cost - mean_delta_cost) / std_delta_cost\n    else:\n        normalized_delta_cost = torch.zeros_like(delta_cost)\n\n    # 4. Compute the coupled exponential margin (New Coupling)\n    # The margin is the product of the rank-based term and an exponential\n    # scaling of the normalized cost difference.\n    dynamic_margin = gamma * rank_gaps * torch.exp(alpha * normalized_delta_cost)\n\n    # 5. Clamp the margin for stability (New Stability Trick)\n    # Prevents the exponential term from creating excessively large targets.\n    margin = torch.clamp(dynamic_margin, min=0.0, max=max_margin)\n\n    # 6. Compute the core logistic loss\n    # The loss is low when the model's logp difference exceeds the dynamic margin.\n    instance_loss = -F.logsigmoid(delta_logp - margin)\n    \n    # Apply optional sample weights if provided\n    if 'weight' in batch and batch['weight'] is not None:\n        instance_loss = instance_loss * batch['weight']\n\n    # 7. Return the mean loss over the batch\n    return instance_loss.mean()"}, "static_ok": true, "static_reason": "", "dynamic_ok": true, "dynamic_reason": "ok", "loss_value": 1.1597986221313477, "grad_norm": 0.0}
{"generation": 4, "index": 0, "ir": {"name": "Rank-Weighted, Z-Scored Margin Loss", "intuition": "This loss function synergizes batch-relative ranking with batch-normalized margins to create a robust and adaptive learning signal. It ensures that the model focuses its learning on pairs that are most discriminative within the current batch context, both in terms of relative importance (rank) and statistical significance (z-score).\n\nInherited Ideas:\n- From Parent 0 (`Adaptive Rank-Gapped Margin Loss`), it inherits the core structure of a margin-based logistic loss using `softplus(margin - delta_logp)`. This is a stable and well-understood formulation for preference learning.\n- From Parent 1 (`Rank-Modulated Tanh-Margin Logistic Loss`), it inherits the idea of using the rank of the cost difference (`delta_cost`) to create an external weight for the loss of each pair. This `rank_weight` ensures that pairs with a higher relative cost difference contribute more to the final loss, making the learning process robust to the absolute scale of costs.\n\nNew Coupling Ideas:\n1. **Z-Scored Margin**: Instead of using a complex function like `tanh` or `rank_gap` to define the margin itself, this child uses a direct, statistically normalized margin. The `delta_cost` is z-scored (`(x - mean) / std`), which centers the cost differences around zero and scales them by their batch-wise standard deviation. This z-scored value, scaled by a hyperparameter `gamma`, becomes the margin. This approach makes the margin adaptive to the current batch's cost distribution, preventing pairs with outlier cost differences from creating excessively large margins while still being sensitive to the magnitude of the cost gap.\n2. **Margin-Weight Decoupling**: The loss decouples the weighting mechanism from the margin calculation. The `rank_weight` (from Parent 1's idea) determines the *importance* of a pair based on its relative ordering, while the `z-scored margin` (new idea) sets the *difficulty* of the learning target based on its statistical deviation. This coupling ensures that the strongest learning signal (`rank_weight * loss`) is applied to pairs that are both highly ranked in importance and have a statistically significant cost difference, leading to a more focused and stable training dynamic.", "pseudocode": "1. Calculate the cost difference: delta_cost = cost(b) - cost(a).\n2. Calculate the log-probability difference: delta_logp = logp(a) - logp(b).\n3. (Inherited from Parent 1) Compute a rank-based adaptive weight. Calculate the rank_gap of delta_cost, which maps its values to a [0, 1] range based on their sorted order. This is the `rank_weight`.\n4. (New Coupling) Compute a z-scored margin. Normalize delta_cost using z-scoring (subtract mean, divide by standard deviation). Scale this normalized value by a hyperparameter `gamma` to create the `margin`.\n5. (Inherited from Parent 0) Compute the core margin loss using the z-scored margin: core_loss = softplus(margin - delta_logp).\n6. The final loss is the product of the rank-based adaptive weight and the core loss, averaged over the batch: mean(rank_weight * core_loss).", "hyperparams": {"gamma": 1.5, "eps": 1e-08}, "operators_used": ["softplus", "rank_gap", "zscore"], "implementation_hint": {"expects": ["cost_a", "cost_b", "log_prob_w", "log_prob_l", "weight"], "returns": "scalar"}, "code": "import torch\nimport torch.nn.functional as F\n\ndef generated_loss(batch, model_output, extra):\n    \"\"\"\n    Rank-Weighted, Z-Scored Margin Loss.\n\n    This loss combines an external weight based on the rank of the cost difference with an\n    internal margin based on the z-scored cost difference.\n\n    - The external `rank_weight` (inherited from Parent 1) makes the loss focus on pairs\n      with a higher relative cost gap within the batch.\n    - The `softplus` loss structure (inherited from Parent 0) provides a stable margin-based loss.\n    - The new coupling is a `z-scored margin`, which adapts the learning target based on the\n      statistical properties of the cost differences in the current batch.\n    \"\"\"\n    # Read hyperparameters from the 'extra' dictionary\n    gamma = extra.get('gamma', 1.5)\n    eps = extra.get('eps', 1e-8)\n\n    # Unpack tensors from the batch\n    # By convention, 'w' is winner (lower cost), 'l' is loser (higher cost)\n    cost_w = batch['cost_a']\n    cost_l = batch['cost_b']\n    logp_w = batch['log_prob_w']\n    logp_l = batch['log_prob_l']\n\n    # Sanity check: cost_w should be less than or equal to cost_l\n    assert torch.all(cost_w <= cost_l), \"Assumption failed: cost_a (winner) must be <= cost_b (loser)\"\n\n    # 1. Calculate cost and log-probability differences\n    delta_cost = cost_l - cost_w  # This will be >= 0\n    delta_logp = logp_w - logp_l # We want this to be positive\n\n    # 2. Compute rank-based adaptive weight (Inherited from Parent 1)\n    # This weight emphasizes pairs with a higher relative cost difference in the batch.\n    if delta_cost.numel() > 1:\n        ranks = delta_cost.argsort().argsort().float()\n        rank_weight = ranks / (delta_cost.numel() - 1)\n    else:\n        # Handle batch size of 1 where rank is trivially 1.\n        rank_weight = torch.ones_like(delta_cost)\n\n    # 3. Compute z-scored margin (New Coupling)\n    # This margin adapts to the batch's cost distribution.\n    if delta_cost.numel() > 1:\n        mean_delta_cost = delta_cost.mean()\n        std_delta_cost = delta_cost.std()\n        # Clamp std to avoid division by zero if all costs are identical.\n        normalized_delta_cost = (delta_cost - mean_delta_cost) / torch.clamp(std_delta_cost, min=eps)\n    else:\n        # Handle batch size of 1 where normalization is not meaningful.\n        normalized_delta_cost = torch.zeros_like(delta_cost)\n    margin = gamma * normalized_delta_cost\n\n    # 4. Compute the core logistic loss (Inherited from Parent 0)\n    # The loss is low when delta_logp > margin.\n    core_loss = F.softplus(margin - delta_logp)\n\n    # 5. Combine rank weight and core loss\n    instance_loss = rank_weight * core_loss\n\n    # Apply optional sample weights if provided\n    if 'weight' in batch and batch['weight'] is not None:\n        instance_loss = instance_loss * batch['weight']\n\n    # 6. Return the mean loss over the batch\n    return instance_loss.mean()"}, "static_ok": true, "static_reason": "", "dynamic_ok": true, "dynamic_reason": "ok", "loss_value": 0.3465735912322998, "grad_norm": 0.0}
{"generation": 4, "index": 1, "ir": {"name": "Rank-Gapped Sigmoid-Weighted Margin Loss", "intuition": "This loss function creates a robust learning signal by combining a rank-based margin with a sigmoid-based cost-aware weighting. The goal is to create a loss that is sensitive to the relative importance of a preference pair (via ranking) while also being responsive to the absolute magnitude of the cost difference in a controlled, non-linear way.\n\nInherited Ideas:\n- From Parent 0 (Adaptive Rank-Gapped Margin Loss), it inherits the core concept of a **rank-based margin**. The margin that the log-probability difference (`delta_logp`) must overcome is determined by the rank of the cost difference (`delta_cost`) within the batch. This makes the loss robust to outlier cost gaps and focuses learning on achieving the correct relative ordering of preferences.\n- From Parent 1 (Rank-Modulated Tanh-Margin Logistic Loss), it inherits the use of a **z-scored cost difference** to normalize the costs based on the batch's statistics. This makes the subsequent weighting scheme independent of the absolute scale of the costs.\n\nNew Coupling Ideas:\n1.  **Sigmoid-based Adaptive Weighting**: Instead of using a linear weight (Parent 0) or a rank-based weight (Parent 1), this child introduces a sigmoid weighting scheme. The z-scored `delta_cost` is passed through a `sigmoid` function. This creates a smooth, S-shaped weight between 0 and 1. Pairs with a `delta_cost` much lower than the batch average receive a low weight, pairs near the average receive a moderate weight, and pairs much higher than the average receive a high weight, with the effect saturating. This focuses the model on learning from pairs with an average or above-average cost difference, while gently de-emphasizing pairs with very small or extremely large cost differences, preventing domination by outliers.\n2.  **Harmonized Loss Structure**: The final loss is the product of this new sigmoid-based adaptive weight and the rank-gapped margin loss (`softplus(margin - delta_logp)`). This coupling ensures that the learning signal is strongest for pairs that are both relatively important (high rank gap, leading to a large margin) and have a statistically significant cost difference (high sigmoid weight). A pair with a high rank but a cost difference near the batch mean will still generate a strong signal, while a pair with a very low cost difference (and thus low sigmoid weight) will be down-weighted, even if its rank is moderate.", "pseudocode": "1. Calculate the cost difference: delta_cost = cost(b) - cost(a).\n2. Calculate the log-probability difference: delta_logp = logp(a) - logp(b).\n3. (Inherited from Parent 0) Compute a rank-based margin. First, calculate the rank_gap of delta_cost, mapping values to a [0, 1] range based on their sorted order. Then, scale this by a hyperparameter `gamma`: margin = gamma * rank_gap(delta_cost).\n4. Compute the core margin loss: core_loss = softplus(margin - delta_logp).\n5. (Inherited from Parent 1) Normalize delta_cost using z-scoring to get 'normalized_delta_cost'.\n6. (New Coupling) Compute a sigmoid-based adaptive weight from the normalized cost difference: adaptive_weight = sigmoid(normalized_delta_cost).\n7. The final loss is the product of the adaptive weight and the core loss, averaged over the batch: mean(adaptive_weight * core_loss).", "hyperparams": {"gamma": 2.0, "eps": 1e-08}, "operators_used": ["softplus", "sigmoid", "zscore", "rank_gap"], "implementation_hint": {"expects": ["cost_a", "cost_b", "logp_a", "logp_b"], "returns": "scalar"}, "code": "import torch\nimport torch.nn.functional as F\n\ndef generated_loss(batch, model_output, extra):\n    \"\"\"\n    Rank-Gapped Sigmoid-Weighted Margin Loss.\n\n    This loss combines a margin based on the rank of the cost difference with an\n    adaptive weight derived from the sigmoid of the z-scored cost difference.\n\n    - The rank-based margin (from Parent 0) makes the loss robust to the scale of cost outliers.\n    - The z-scoring of costs (from Parent 1) normalizes the cost signal based on batch statistics.\n    - The new sigmoid weighting provides a smooth, non-linear emphasis on pairs with\n      above-average cost differences, while saturating to prevent domination by extreme values.\n    \"\"\"\n    # Read hyperparameters from the 'extra' dictionary\n    gamma = extra.get('gamma', 2.0)\n    eps = extra.get('eps', 1e-8)\n\n    # Unpack tensors from the batch\n    # By convention, 'w' is winner (lower cost), 'l' is loser (higher cost)\n    cost_w = batch['cost_a']\n    cost_l = batch['cost_b']\n    logp_w = batch['log_prob_w']\n    logp_l = batch['log_prob_l']\n\n    # Sanity check: cost_w should be less than or equal to cost_l\n    assert torch.all(cost_w <= cost_l), \"Assumption failed: cost_a (winner) must be <= cost_b (loser)\"\n\n    # 1. Calculate cost and log-probability differences\n    delta_cost = cost_l - cost_w  # This will be >= 0\n    delta_logp = logp_w - logp_l # We want this to be positive\n\n    # 2. (Inherited from Parent 0) Compute the rank-gapped margin\n    # This makes the margin robust to outliers and dependent on relative cost importance.\n    if delta_cost.numel() > 1:\n        ranks = delta_cost.argsort().argsort().float()\n        rank_gaps = ranks / (delta_cost.numel() - 1)\n    else:\n        # Handle batch size of 1\n        rank_gaps = torch.ones_like(delta_cost)\n    margin = gamma * rank_gaps\n\n    # 3. Compute the core margin loss\n    # The loss is low when delta_logp > margin.\n    core_loss = F.softplus(margin - delta_logp)\n\n    # 4. (Inherited from Parent 1) Normalize the cost difference for weighting\n    if delta_cost.numel() > 1:\n        mean_delta_cost = delta_cost.mean()\n        std_delta_cost = delta_cost.std() + eps\n        normalized_delta_cost = (delta_cost - mean_delta_cost) / std_delta_cost\n    else:\n        normalized_delta_cost = torch.zeros_like(delta_cost)\n\n    # 5. (New Coupling) Compute sigmoid-based adaptive weight\n    # This creates a smooth, S-shaped weight, emphasizing pairs with above-average cost difference.\n    adaptive_weight = torch.sigmoid(normalized_delta_cost)\n\n    # 6. Combine core loss and adaptive weight\n    # The final loss is modulated by the sigmoid weight.\n    instance_loss = adaptive_weight * core_loss\n\n    # Apply optional sample weights if provided\n    if 'weight' in batch and batch['weight'] is not None:\n        instance_loss = instance_loss * batch['weight']\n\n    # 7. Return the mean loss over the batch\n    return instance_loss.mean()"}, "static_ok": true, "static_reason": "", "dynamic_ok": true, "dynamic_reason": "ok", "loss_value": 0.6750055551528931, "grad_norm": 0.0}
{"generation": 4, "index": 2, "ir": {"name": "Rank-Weighted Exponential Margin Loss", "intuition": "This loss function synergizes rank-based weighting with a non-linear, exponential margin to create a robust and adaptive learning signal. The goal is to strongly enforce preferences for pairs that are both relatively important within a batch and have a substantial absolute cost difference, while being gentle on pairs with small, potentially noisy cost gaps.\n\nInherited Ideas:\n- From Parent 1 (Rank-Modulated Tanh-Margin Logistic Loss), it inherits the idea of using the **rank of the cost difference as an external weight**. The `rank_gap` of `delta_cost` is calculated and used to modulate the final loss for each pair. This focuses the learning signal on pairs with the highest relative cost differences in the batch, making the loss robust to the absolute scale of costs.\n- From both parents, it inherits the core structure of a margin-based logistic loss, where the objective is to make `delta_logp` greater than some margin `m`. It uses the numerically stable `-logsigmoid(delta_logp - m)` formulation.\n\nNew Coupling Ideas:\n1.  **Exponential Margin from Z-Scored Cost**: Instead of a linear, tanh-bounded, or rank-based margin, this child introduces an **exponential margin**. The cost difference (`delta_cost`) is first normalized using z-scoring. The `exp` function is then applied to this normalized value and scaled by a hyperparameter `beta`. This creates a margin that grows super-linearly with the cost difference. For pairs with a `delta_cost` below the batch mean, the margin is small (approaching 0), making it easy to satisfy the preference. For pairs with a `delta_cost` significantly above the mean, the margin grows very rapidly, creating a strong learning signal to push `delta_logp` much higher.\n2.  **Synergy of Rank Weight and Exponential Margin**: The final loss is the product of the rank-based weight and the exponential-margin loss. This coupling creates a powerful dynamic. A pair only receives a large loss if it has *both* a high relative rank (large `rank_weight`) and a large absolute cost difference (leading to a large exponential `margin` that is hard to overcome). A pair with a high rank but small z-score will have a high weight but a small margin, resulting in a moderate loss. Conversely, a pair with a large z-score but low rank (e.g., in a batch of all high-gap pairs) will face a large margin but its contribution will be down-weighted. This focuses the model's capacity on the most unambiguously important preference pairs.", "pseudocode": "1. Calculate the cost difference: delta_cost = cost(b) - cost(a).\n2. Calculate the log-probability difference: delta_logp = logp(a) - logp(b).\n3. (Inherited from Parent 1) Compute a rank-based adaptive weight. Calculate the `rank_gap` of delta_cost, which maps its values to a [0, 1] range based on their sorted order. This is the `rank_weight`.\n4. (New Coupling 1) Compute an exponential margin. First, normalize delta_cost using z-scoring to get `normalized_delta_cost`. Then, compute the margin as `margin = beta * exp(normalized_delta_cost)`.\n5. (Inherited from Parents) Compute the core logistic loss using the exponential margin: `core_loss = -logsigmoid(delta_logp - margin)`.\n6. (New Coupling 2) The final loss is the product of the rank-based weight and the core loss, averaged over the batch: `mean(rank_weight * core_loss)`.", "hyperparams": {"beta": 1.0, "eps": 1e-08}, "operators_used": ["logsigmoid", "exp", "zscore", "rank_gap"], "implementation_hint": {"expects": ["cost_a", "cost_b", "logp_a", "logp_b"], "returns": "scalar"}, "code": "import torch\nimport torch.nn.functional as F\n\ndef generated_loss(batch, model_output, extra):\n    \"\"\"\n    Rank-Weighted Exponential Margin Loss.\n\n    This loss combines a rank-based external weight with a non-linear exponential margin.\n    The goal is to focus learning on pairs that are important both relatively (high rank)\n    and in terms of deviation from the batch mean (large z-score).\n\n    - Inherits rank-based weighting from Parent 1.\n    - Introduces a new exponential margin based on z-scored cost difference.\n    \"\"\"\n    # Read hyperparameters from the 'extra' dictionary\n    beta = extra.get('beta', 1.0)\n    eps = extra.get('eps', 1e-8)\n\n    # Unpack tensors from the batch\n    # By convention, 'w' is winner (lower cost), 'l' is loser (higher cost)\n    cost_w = batch['cost_a']\n    cost_l = batch['cost_b']\n    logp_w = batch['log_prob_w']\n    logp_l = batch['log_prob_l']\n\n    # Sanity check: cost_w should be less than or equal to cost_l\n    assert torch.all(cost_w <= cost_l), \"Assumption failed: cost_a (winner) must be <= cost_b (loser)\"\n\n    # 1. Calculate cost and log-probability differences\n    delta_cost = cost_l - cost_w  # This will be >= 0\n    delta_logp = logp_w - logp_l  # We want this to be positive\n\n    # 2. Compute rank-based adaptive weight (Inherited from Parent 1)\n    # `rank_gap` maps delta_cost to [0, 1] based on its rank in the batch.\n    # This weight emphasizes pairs with a higher relative cost difference.\n    if delta_cost.numel() > 1:\n        ranks = delta_cost.argsort().argsort().float()\n        rank_weight = ranks / (delta_cost.numel() - 1)\n    else:\n        # Handle batch size of 1: the single item has the highest rank.\n        rank_weight = torch.ones_like(delta_cost)\n\n    # 3. Compute z-scored cost difference for the margin calculation\n    if delta_cost.numel() > 1:\n        mean_delta_cost = delta_cost.mean()\n        std_delta_cost = delta_cost.std() + eps\n        normalized_delta_cost = (delta_cost - mean_delta_cost) / std_delta_cost\n    else:\n        # Handle batch size of 1: z-score is 0.\n        normalized_delta_cost = torch.zeros_like(delta_cost)\n\n    # 4. Compute exponential margin (New Coupling 1)\n    # The margin grows super-linearly with the normalized cost difference.\n    # This creates a very strong signal for pairs with costs far above the mean.\n    margin = beta * torch.exp(normalized_delta_cost)\n\n    # 5. Compute the core logistic loss (Inherited from Parents)\n    # We use -logsigmoid(x), which is equivalent to softplus(-x).\n    # The loss is low when delta_logp > margin.\n    core_loss = -F.logsigmoid(delta_logp - margin)\n\n    # 6. Combine rank weight and core loss (New Coupling 2)\n    # The final loss is modulated by the relative importance (rank) of the pair.\n    instance_loss = rank_weight * core_loss\n\n    # Apply optional sample weights if provided\n    if 'weight' in batch and batch['weight'] is not None:\n        instance_loss = instance_loss * batch['weight']\n\n    # 7. Return the mean loss over the batch\n    return instance_loss.mean()"}, "static_ok": true, "static_reason": "", "dynamic_ok": true, "dynamic_reason": "ok", "loss_value": 0.6566308736801147, "grad_norm": 0.0}
{"generation": 4, "index": 3, "ir": {"name": "Rank-Normalized Margin with Sigmoid Cost Modulation", "intuition": "This loss function creates a learning signal where the target margin is adaptive to the batch statistics, and the loss for each pair is modulated by the significance of its cost difference.\n\nInherited Ideas:\n- From Parent 0 (Adaptive Rank-Gapped Margin Loss), it inherits the use of `softplus` for a stable, margin-based logistic loss. This provides a robust foundation: `softplus(margin - delta_logp)`.\n- From Parent 1 (Rank-Modulated Tanh-Margin Logistic Loss), it inherits the idea of normalizing the cost difference using z-scoring (`zscore(delta_cost)`) to make the margin adaptive to the current batch's statistical properties (mean and standard deviation). This prevents the margin from being dominated by pairs with outlier cost differences.\n\nNew Coupling Ideas:\n1.  **Rank-Based Margin Normalization**: Instead of using z-scored costs directly in a `tanh` function to create the margin (as in Parent 1), this child loss first computes the `rank_gap` of the z-scored cost differences. This `rank_gap(zscore(delta_cost))` produces a value in [0, 1] that is robust to both the absolute scale of costs (due to z-scoring) and the distribution of costs (due to ranking). This normalized rank is then scaled by a hyperparameter `gamma` to set the target margin. This coupling ensures the margin is determined by a pair's relative standing within the normalized cost distribution of the batch.\n2.  **Sigmoid Cost Modulation**: The final loss is weighted by `sigmoid(zscore(delta_cost))`. This acts as a smooth, adaptive switch. For pairs with a cost difference significantly above the batch mean, the weight approaches 1. For pairs with a cost difference near or below the mean, the weight smoothly decreases towards 0. This coupling focuses the learning on pairs that are 'surprisingly' far apart in cost, given the current batch context, while gracefully down-weighting less significant or potentially noisy pairs.", "pseudocode": "1. Calculate the cost difference: delta_cost = cost(b) - cost(a).\n2. Calculate the log-probability difference: delta_logp = logp(a) - logp(b).\n3. (Inherited from Parent 1) Normalize the cost difference using z-scoring: normalized_delta_cost = zscore(delta_cost).\n4. (New Coupling 1) Compute a rank-normalized margin. First, calculate the rank_gap of the normalized_delta_cost, which maps its values to a [0, 1] range based on sorted order. Then, scale this by a hyperparameter `gamma`: margin = gamma * rank_gap(normalized_delta_cost).\n5. (Inherited from Parent 0) Compute the core logistic loss using the rank-normalized margin: core_loss = softplus(margin - delta_logp).\n6. (New Coupling 2) Compute a sigmoid-based adaptive weight. Apply a sigmoid function to the normalized_delta_cost: adaptive_weight = sigmoid(normalized_delta_cost).\n7. The final loss is the product of the adaptive weight and the core loss, averaged over the batch: mean(adaptive_weight * core_loss).", "hyperparams": {"gamma": 2.0, "eps": 1e-08}, "operators_used": ["softplus", "sigmoid", "zscore", "rank_gap"], "implementation_hint": {"expects": ["cost_a", "cost_b", "logp_a", "logp_b"], "returns": "scalar"}, "code": "import torch\nimport torch.nn.functional as F\n\ndef generated_loss(batch, model_output, extra):\n    \"\"\"\n    Rank-Normalized Margin with Sigmoid Cost Modulation.\n\n    This loss combines a margin derived from the rank of z-scored cost differences\n    with an adaptive weight based on a sigmoid of the same z-scored costs.\n\n    - The margin's adaptiveness is inherited from Parent 1's use of z-scoring.\n    - The stable `softplus` loss structure is inherited from Parent 0.\n    - New Coupling 1: The margin `gamma * rank_gap(zscore(delta_cost))` is robust to\n      both the scale and distribution of costs in the batch.\n    - New Coupling 2: The weight `sigmoid(zscore(delta_cost))` acts as a smooth switch,\n      focusing learning on pairs with a cost difference significantly above the batch mean.\n    \"\"\"\n    # Read hyperparameters from the 'extra' dictionary\n    gamma = extra.get('gamma', 2.0)\n    eps = extra.get('eps', 1e-8)\n\n    # Unpack tensors from the batch\n    cost_w = batch['cost_a']\n    cost_l = batch['cost_b']\n    logp_w = batch['log_prob_w']\n    logp_l = batch['log_prob_l']\n\n    # Sanity check: cost_w should be less than or equal to cost_l\n    assert torch.all(cost_w <= cost_l), \"Assumption failed: cost_a (winner) must be <= cost_b (loser)\"\n\n    # 1. Calculate cost and log-probability differences\n    delta_cost = cost_l - cost_w  # This will be >= 0\n    delta_logp = logp_w - logp_l  # We want this to be positive\n\n    # 2. (Inherited from Parent 1) Normalize delta_cost using z-scoring\n    if delta_cost.numel() > 1:\n        mean_delta_cost = delta_cost.mean()\n        std_delta_cost = delta_cost.std() + eps\n        normalized_delta_cost = (delta_cost - mean_delta_cost) / std_delta_cost\n    else:\n        normalized_delta_cost = torch.zeros_like(delta_cost)\n\n    # 3. (New Coupling 1) Compute rank-normalized margin\n    if normalized_delta_cost.numel() > 1:\n        ranks = normalized_delta_cost.argsort().argsort().float()\n        rank_gaps = ranks / (normalized_delta_cost.numel() - 1)\n    else:\n        rank_gaps = torch.ones_like(normalized_delta_cost)\n    margin = gamma * rank_gaps\n\n    # 4. (Inherited from Parent 0) Compute the core logistic loss\n    # The loss is low when delta_logp > margin.\n    core_loss = F.softplus(margin - delta_logp)\n\n    # 5. (New Coupling 2) Compute sigmoid-based adaptive weight\n    # This weight smoothly emphasizes pairs with above-average cost differences.\n    adaptive_weight = torch.sigmoid(normalized_delta_cost)\n\n    # 6. Combine weight and core loss\n    instance_loss = adaptive_weight * core_loss\n\n    # Apply optional sample weights if provided\n    if 'weight' in batch and batch['weight'] is not None:\n        instance_loss = instance_loss * batch['weight']\n\n    # 7. Return the mean loss over the batch\n    return instance_loss.mean()"}, "static_ok": true, "static_reason": "", "dynamic_ok": true, "dynamic_reason": "ok", "loss_value": 0.6750055551528931, "grad_norm": 0.0}
{"generation": 4, "index": 4, "ir": {"name": "Adaptive Sigmoid-Gated Rank Loss", "intuition": "This loss function creates a hybrid learning signal that smoothly transitions between a rank-based objective and a margin-based objective, gated by the magnitude of the cost difference.\n\nInherited Ideas:\n- From Parent 0 (Adaptive Rank-Gapped Margin Loss), it inherits the use of `rank_gap` on the cost difference (`delta_cost`) to create a learning target that is robust to the absolute scale of costs and focuses on relative ordering. The `rank_gap` provides a normalized, outlier-resistant signal of preference strength.\n- From Parent 1 (Rank-Modulated Tanh-Margin Logistic Loss), it inherits the use of z-scoring the `delta_cost` to create a normalized signal that reflects how unusual a given cost difference is relative to the batch statistics.\n\nNew Coupling Ideas:\n1.  **Sigmoid Gating Mechanism**: Instead of using z-scored costs to define a margin or a weight, this child uses it to create a dynamic gate via a scaled sigmoid function: `gate = sigmoid(beta * zscore(delta_cost))`. When `delta_cost` is small (average or below), the gate is close to 0. When `delta_cost` is large, the gate is close to 1. This gate acts as a switch between two different learning regimes.\n2.  **Hybrid Loss Objective**: The final loss is a linear interpolation between two distinct loss terms, controlled by the sigmoid gate: `loss = (1 - gate) * loss_rank + gate * loss_margin`. For pairs with small or average cost differences (where `gate` is near 0), the loss is dominated by `loss_rank = softplus(gamma * rank_gap(delta_cost) - delta_logp)`. This focuses the model on getting the relative ordering correct without enforcing a large margin. For pairs with a large and statistically significant cost difference (where `gate` is near 1), the loss transitions to `loss_margin = softplus(delta_cost - delta_logp)`, which is a more aggressive margin objective that pushes the log-probability difference to match the absolute cost difference. This coupling allows the model to learn fine-grained ranking for subtle preferences while aggressively separating high-confidence preferences.", "pseudocode": "1. Calculate the cost difference: delta_cost = cost(b) - cost(a).\n2. Calculate the log-probability difference: delta_logp = logp(a) - logp(b).\n3. (Inherited from Parent 1) Normalize delta_cost using z-scoring.\n4. (New Coupling 1) Compute a sigmoid gate based on the z-scored delta_cost. This gate will be close to 0 for small/average delta_cost and close to 1 for large delta_cost. The steepness is controlled by a hyperparameter `beta`: gate = sigmoid(beta * zscore(delta_cost)).\n5. (Inherited from Parent 0) Compute a rank-based loss term. This term is dominant when the gate is near 0. It uses the `rank_gap` of delta_cost to define a soft margin: loss_rank = softplus(gamma * rank_gap(delta_cost) - delta_logp).\n6. Compute a margin-based loss term. This term is dominant when the gate is near 1. It uses the raw `delta_cost` as a hard margin target: loss_margin = softplus(delta_cost - delta_logp).\n7. (New Coupling 2) Combine the two loss terms using the sigmoid gate as a linear interpolator: instance_loss = (1 - gate) * loss_rank + gate * loss_margin.\n8. Return the mean of instance_loss over the batch.", "hyperparams": {"gamma": 1.0, "beta": 1.0, "eps": 1e-08}, "operators_used": ["softplus", "sigmoid", "zscore", "rank_gap"], "implementation_hint": {"expects": ["cost_a", "cost_b", "logp_a", "logp_b"], "returns": "scalar"}, "code": "import torch\nimport torch.nn.functional as F\n\ndef generated_loss(batch, model_output, extra):\n    \"\"\"\n    Adaptive Sigmoid-Gated Rank Loss.\n\n    This loss smoothly interpolates between a rank-based objective for small cost differences\n    and a margin-based objective for large cost differences.\n\n    - Inherits `rank_gap` from Parent 0 for the rank-based objective.\n    - Inherits `zscore` from Parent 1 to normalize cost differences for gating.\n    - New Coupling 1: A sigmoid gate `sigmoid(beta * zscore(delta_cost))` determines the interpolation weight.\n    - New Coupling 2: The final loss is a mix of a rank-loss and a margin-loss, controlled by the gate.\n    \"\"\"\n    # Read hyperparameters from the 'extra' dictionary\n    gamma = extra.get('gamma', 1.0)\n    beta = extra.get('beta', 1.0)\n    eps = extra.get('eps', 1e-8)\n\n    # Unpack tensors from the batch\n    cost_w = batch['cost_a']\n    cost_l = batch['cost_b']\n    logp_w = batch['log_prob_w']\n    logp_l = batch['log_prob_l']\n\n    # Sanity check: cost_w should be less than or equal to cost_l\n    assert torch.all(cost_w <= cost_l), \"Assumption failed: cost_a (winner) must be <= cost_b (loser)\"\n\n    # 1. Calculate cost and log-probability differences\n    delta_cost = cost_l - cost_w\n    delta_logp = logp_w - logp_l\n\n    # Handle batch size of 1 to avoid NaN from zscore/rank_gap\n    if delta_cost.numel() > 1:\n        # 2. (Inherited from Parent 1) Normalize delta_cost\n        mean_delta_cost = delta_cost.mean()\n        std_delta_cost = delta_cost.std() + eps\n        zscored_delta_cost = (delta_cost - mean_delta_cost) / std_delta_cost\n\n        # 3. (Inherited from Parent 0) Compute rank_gap for the rank-based loss term\n        ranks = delta_cost.argsort().argsort().float()\n        rank_gaps = ranks / (delta_cost.numel() - 1)\n    else:\n        zscored_delta_cost = torch.zeros_like(delta_cost)\n        rank_gaps = torch.ones_like(delta_cost)\n\n    # 4. (New Coupling 1) Compute the sigmoid gate\n    # The gate smoothly transitions from 0 to 1 as delta_cost becomes large.\n    gate = torch.sigmoid(beta * zscored_delta_cost)\n\n    # 5. Compute the two loss components\n    # loss_rank: Focuses on relative ordering, dominant for small delta_cost\n    rank_margin = gamma * rank_gaps\n    loss_rank = F.softplus(rank_margin - delta_logp)\n\n    # loss_margin: Focuses on matching the absolute cost difference, dominant for large delta_cost\n    loss_margin = F.softplus(delta_cost - delta_logp)\n\n    # 6. (New Coupling 2) Combine losses using the gate\n    instance_loss = (1 - gate) * loss_rank + gate * loss_margin\n\n    # Apply optional sample weights if provided\n    if 'weight' in batch and batch['weight'] is not None:\n        instance_loss = instance_loss * batch['weight']\n\n    # 7. Return the mean loss over the batch\n    return instance_loss.mean()"}, "static_ok": true, "static_reason": "", "dynamic_ok": true, "dynamic_reason": "ok", "loss_value": 1.149186134338379, "grad_norm": 0.0}
{"generation": 4, "index": 5, "ir": {"name": "Rank-Weighted, Sigmoid-Gated Margin Loss", "intuition": "This loss function creates a robust and adaptive learning signal by combining rank-based weighting with a dynamic, cost-gated margin. It aims to focus learning on pairs with a clear and relatively significant preference, while smoothly down-weighting pairs that are either too easy (large cost gap) or potentially noisy (small cost gap).\n\nInherited Ideas:\n- From Parent 0 (Adaptive Rank-Gapped Margin Loss), it inherits the use of `softplus(margin - delta_logp)` as the core logistic loss structure. This is a stable and effective formulation for margin-based preference learning.\n- From Parent 1 (Rank-Modulated Tanh-Margin Logistic Loss), it inherits the idea of using the `rank_gap` of the cost difference as an external, modulating weight. This `rank_weight` ensures that pairs with a higher relative cost difference within the batch contribute more significantly to the final loss, making the learning process robust to the absolute scale of costs.\n\nNew Coupling Ideas:\n1.  **Sigmoid-Gated Dynamic Margin**: The margin is no longer a simple linear function of the cost difference. Instead, the z-scored `delta_cost` is passed through a `sigmoid` function, which is then scaled by a hyperparameter `gamma`. This creates a margin that is small for pairs with a low cost difference (relative to the batch average), preventing the model from being penalized for not achieving a large `delta_logp` on noisy or ambiguous pairs. The margin then smoothly increases and saturates for pairs with a high cost difference, providing a consistent and stable learning target for clear preferences without becoming excessively large and causing instability.\n2.  **Dual-Modulation of the Loss Signal**: The final loss is a product of the `rank_weight` and the core loss term, `softplus(sigmoid_margin - delta_logp)`. This coupling creates a dual-control mechanism. The `rank_weight` modulates the importance based on relative ordering, while the `sigmoid_margin` sets a dynamic, bounded target based on the statistical significance of the cost gap. The strongest learning signal is applied to pairs that have both a high rank (important ordering) and a statistically significant cost difference (leading to a non-trivial margin to overcome).", "pseudocode": "1. Calculate the cost difference: delta_cost = cost(b) - cost(a).\n2. Calculate the log-probability difference: delta_logp = logp(a) - logp(b).\n3. (Inherited from Parent 1) Compute a rank-based adaptive weight. Calculate the rank_gap of delta_cost, which maps its values to a [0, 1] range based on their sorted order. This is the `rank_weight`.\n4. (New Coupling 1) Compute a sigmoid-gated dynamic margin. First, normalize delta_cost using z-scoring. Then, apply a `sigmoid` function to the z-scored value and scale it by a hyperparameter `gamma`. The result is a smooth, bounded margin between 0 and gamma.\n5. (Inherited from Parent 0) Compute the core logistic loss using the dynamic margin: core_loss = softplus(margin - delta_logp).\n6. (New Coupling 2) The final loss is the product of the rank-based adaptive weight and the core loss, averaged over the batch: mean(rank_weight * core_loss).", "hyperparams": {"gamma": 2.0, "eps": 1e-08}, "operators_used": ["softplus", "sigmoid", "zscore", "rank_gap"], "implementation_hint": {"expects": ["cost_a", "cost_b", "log_prob_w", "log_prob_l", "weight"], "returns": "scalar"}, "code": "import torch\nimport torch.nn.functional as F\n\ndef generated_loss(batch, model_output, extra):\n    \"\"\"\n    Rank-Weighted, Sigmoid-Gated Margin Loss.\n\n    This loss combines a rank-based weight with a dynamic margin gated by a sigmoid function.\n    - The external weight is based on the rank of the cost difference (from Parent 1).\n    - The core loss uses a softplus formulation (from Parent 0).\n    - The new coupling is a sigmoid-gated margin, which creates a smooth, bounded learning\n      target based on the z-scored cost difference.\n    \"\"\"\n    # Read hyperparameters from the 'extra' dictionary\n    gamma = extra.get('gamma', 2.0)\n    eps = extra.get('eps', 1e-8)\n\n    # Unpack tensors from the batch\n    # By convention, 'w' is winner (lower cost), 'l' is loser (higher cost)\n    cost_w = batch['cost_a']\n    cost_l = batch['cost_b']\n    logp_w = batch['log_prob_w']\n    logp_l = batch['log_prob_l']\n\n    # Sanity check: cost_w should be less than or equal to cost_l\n    assert torch.all(cost_w <= cost_l), \"Assumption failed: cost_a (winner) must be <= cost_b (loser)\"\n\n    # 1. Calculate cost and log-probability differences\n    delta_cost = cost_l - cost_w  # This will be >= 0\n    delta_logp = logp_w - logp_l  # We want this to be positive\n\n    # 2. (Inherited from Parent 1) Compute rank-based adaptive weight\n    # `rank_gap` maps delta_cost to [0, 1] based on its rank in the batch.\n    # This weight emphasizes pairs with a higher relative cost difference.\n    if delta_cost.numel() > 1:\n        ranks = delta_cost.argsort().argsort().float()\n        rank_weight = ranks / (delta_cost.numel() - 1)\n    else:\n        rank_weight = torch.ones_like(delta_cost)\n\n    # 3. (New Coupling 1) Compute sigmoid-gated dynamic margin\n    # First, z-score the cost difference to normalize it.\n    if delta_cost.numel() > 1:\n        mean_delta_cost = delta_cost.mean()\n        std_delta_cost = delta_cost.std() + eps\n        zscored_delta_cost = (delta_cost - mean_delta_cost) / std_delta_cost\n    else:\n        zscored_delta_cost = torch.zeros_like(delta_cost)\n    \n    # The margin is a smooth, bounded function of the normalized cost difference.\n    # It provides a gentle target for small cost gaps and a stable one for large gaps.\n    margin = gamma * torch.sigmoid(zscored_delta_cost)\n\n    # 4. (Inherited from Parent 0) Compute the core logistic loss term\n    # The loss is low when delta_logp > margin.\n    core_loss = F.softplus(margin - delta_logp)\n\n    # 5. (New Coupling 2) Combine rank weight and core loss\n    instance_loss = rank_weight * core_loss\n\n    # Apply optional sample weights if provided\n    if 'weight' in batch and batch['weight'] is not None:\n        instance_loss = instance_loss * batch['weight']\n\n    # 6. Return the mean loss over the batch\n    return instance_loss.mean()"}, "static_ok": true, "static_reason": "", "dynamic_ok": true, "dynamic_reason": "ok", "loss_value": 0.65663081407547, "grad_norm": 0.0}
{"generation": 4, "index": 6, "ir": {"name": "Saturated Rank-Weighted Logistic Loss", "intuition": "This loss function synergistically combines rank-based weighting with a dynamic margin to create a stable and adaptive learning signal. The goal is to ensure the model prefers lower-cost solutions, where the strength of the learning signal is proportional to the relative importance of the preference pair within a batch, while preventing outlier cost differences from creating excessively large or unstable loss values.\n\nInherited Ideas:\n- From Parent 0 (`Adaptive Rank-Gapped Margin Loss`), it inherits the use of `softplus` for the core loss structure: `softplus(margin - delta_logp)`. This is a numerically stable formulation of a hinge-like loss.\n- From Parent 1 (`Rank-Modulated Tanh-Margin Logistic Loss`), it inherits the concept of using the `rank_gap` of the cost difference (`delta_cost`) as an adaptive weight. This `rank_weight` ensures that pairs with a higher relative cost difference within a batch contribute more to the loss, making the learning robust to the absolute scale of costs.\n\nNew Coupling Ideas:\n1.  **Saturated Dynamic Margin**: This is a new way to construct the margin. Instead of being directly proportional to the cost difference or its z-score, the margin is computed by passing the raw `delta_cost` through a `tanh` function and then scaling it. The formula is `margin = gamma * tanh(delta_cost)`. This creates a margin that is sensitive to small cost differences but smoothly saturates for very large ones. This prevents outlier pairs with huge cost gaps from creating an overly large margin that dominates the learning signal, thus improving stability while still being cost-aware.\n2.  **Rank-Weighted Margin Loss**: The final loss is a direct product of the `rank_weight` and the `softplus`-based loss term. This coupling ensures that the learning signal is strongest for pairs that have both high relative importance (high `rank_weight`) and where the model's log-probability difference `delta_logp` fails to clear the saturated margin. A pair with a high rank but a small `delta_cost` will have a high weight but a small margin to clear. Conversely, a pair with a large `delta_cost` (and thus a large, saturated margin) but a low rank will have its loss contribution down-weighted. This balances relative and absolute importance effectively.", "pseudocode": "1. Calculate the cost difference: delta_cost = cost(b) - cost(a).\n2. Calculate the log-probability difference: delta_logp = logp(a) - logp(b).\n3. (Inherited from Parent 1) Compute a rank-based adaptive weight. Calculate the rank_gap of delta_cost, which maps values to a [0, 1] range based on their sorted order. This is the `rank_weight`.\n4. (New Coupling 1) Compute a saturated dynamic margin. Apply a `tanh` function to the raw delta_cost and scale it by a hyperparameter `gamma`. The margin is `gamma * tanh(delta_cost)`. This makes the margin sensitive to cost differences but prevents it from growing unboundedly.\n5. (Inherited from Parent 0) Compute the core logistic loss using the saturated margin: `core_loss = softplus(margin - delta_logp)`.\n6. (New Coupling 2) The final loss is the product of the rank-based adaptive weight and the core loss, averaged over the batch: `mean(rank_weight * core_loss)`.", "hyperparams": {"gamma": 1.5}, "operators_used": ["softplus", "tanh", "rank_gap"], "implementation_hint": {"expects": ["cost_a", "cost_b", "logp_a", "logp_b"], "returns": "scalar"}, "code": "import torch\nimport torch.nn.functional as F\n\ndef generated_loss(batch, model_output, extra):\n    \"\"\"\n    Saturated Rank-Weighted Logistic Loss.\n\n    This loss combines a rank-based adaptive weight with a tanh-saturated dynamic margin.\n    The weight is determined by the rank of the cost difference, focusing on relatively important pairs.\n    The margin is a smooth, bounded function of the raw cost difference, preventing instability from outliers.\n\n    - Inherits `rank_gap` for weighting from Parent 1.\n    - Inherits `softplus` for the core loss from Parent 0.\n    - Introduces a new `tanh(delta_cost)` margin for bounded, cost-aware dynamics.\n    - Couples the rank weight with the softplus loss for a balanced signal.\n    \"\"\"\n    # Read hyperparameters from the 'extra' dictionary\n    gamma = extra.get('gamma', 1.5)\n\n    # Unpack tensors from the batch\n    # By convention, 'w' is winner (lower cost), 'l' is loser (higher cost)\n    cost_w = batch['cost_a']\n    cost_l = batch['cost_b']\n    logp_w = batch['log_prob_w']\n    logp_l = batch['log_prob_l']\n\n    # Sanity check: cost_w should be less than or equal to cost_l\n    assert torch.all(cost_w <= cost_l), \"Assumption failed: cost_a (winner) must be <= cost_b (loser)\"\n\n    # 1. Calculate cost and log-probability differences\n    delta_cost = cost_l - cost_w  # This will be >= 0\n    delta_logp = logp_w - logp_l # We want this to be positive\n\n    # 2. (Inherited from Parent 1) Compute rank-based adaptive weight.\n    # `rank_gap` maps delta_cost to [0, 1] based on its rank in the batch.\n    # This weight emphasizes pairs with a higher relative cost difference.\n    if delta_cost.numel() > 1:\n        ranks = delta_cost.argsort().argsort().float()\n        rank_weight = ranks / (delta_cost.numel() - 1)\n    else:\n        # Handle batch size of 1 gracefully\n        rank_weight = torch.ones_like(delta_cost)\n\n    # 3. (New Coupling 1) Compute a saturated dynamic margin.\n    # The margin is a smooth, bounded function of the raw cost difference.\n    # This prevents very large cost differences from creating excessively large margins.\n    margin = gamma * torch.tanh(delta_cost)\n\n    # 4. (Inherited from Parent 0) Compute the core logistic loss.\n    # The loss is low when delta_logp > margin.\n    core_loss = F.softplus(margin - delta_logp)\n\n    # 5. (New Coupling 2) Combine rank weight and core loss.\n    # The final loss is modulated by the rank-based weight.\n    instance_loss = rank_weight * core_loss\n\n    # Apply optional sample weights if provided\n    if 'weight' in batch and batch['weight'] is not None:\n        instance_loss = instance_loss * batch['weight']\n\n    # 6. Return the mean loss over the batch\n    return instance_loss.mean()"}, "static_ok": true, "static_reason": "", "dynamic_ok": true, "dynamic_reason": "ok", "loss_value": 0.7096534371376038, "grad_norm": 0.0}
{"generation": 4, "index": 7, "ir": {"name": "Normalized Rank-Gapped Sigmoid Loss", "intuition": "This loss function creates a stable and adaptive learning signal by combining rank-based margins with batch-level normalization of log-probabilities. It aims to make the model focus on the relative ordering of preferences within a batch, while being robust to the absolute scale of log-probabilities and cost differences.\n\nInherited Ideas:\n- From Parent 0 (Adaptive Rank-Gapped Margin Loss), it inherits the core concept of using `rank_gap` on the cost difference (`delta_cost`) to define a margin. This makes the learning target robust to outlier cost differences and sensitive to the relative importance of each pair within the batch.\n- From Parent 1 (Rank-Modulated Tanh-Margin Logistic Loss), it inherits the fundamental Bradley-Terry structure using `logsigmoid`. The loss is formulated as `-logsigmoid(scaled_delta_logp - margin)`, which provides a stable and probabilistically interpretable objective.\n\nNew Coupling Ideas:\n1. **Z-Score Normalization of Log-Probability Difference**: Instead of using the raw `delta_logp`, this loss first normalizes it using z-scoring across the batch. This new coupling makes the loss invariant to the mean and variance of the model's log-probability outputs for a given batch. It forces the model to learn a consistent relative ordering of log-probabilities, rather than just pushing their absolute values up or down. This can improve stability and prevent the model from becoming overconfident in its raw logp assignments.\n2. **Margin Scaling by Cost Standard Deviation**: The rank-based margin is scaled not by a fixed hyperparameter, but by the standard deviation of the batch's cost differences (`delta_cost.std()`). This makes the margin's scale adaptive to the current batch's cost distribution. If costs are tightly clustered (low std), the margin will be small, allowing the model to focus on fine-grained distinctions. If costs are widely spread (high std), the margin will be larger, demanding a more significant log-probability separation for high-cost-gap pairs. This couples the learning objective's difficulty to the statistical properties of the batch's costs.", "pseudocode": "1. Calculate the cost difference: delta_cost = cost(b) - cost(a).\n2. Calculate the log-probability difference: delta_logp = logp(a) - logp(b).\n3. (New Coupling 1) Normalize the log-probability difference using z-scoring across the batch to get `normalized_delta_logp`. This makes the learning signal robust to the scale and shift of the model's raw logp outputs.\n4. (Inherited from Parent 0) Compute a rank-based margin base. Calculate the `rank_gap` of delta_cost, which maps values to a [0, 1] range based on their sorted order.\n5. (New Coupling 2) Create an adaptive margin scale. Calculate the standard deviation of delta_cost in the batch and multiply it by a hyperparameter `gamma`. This is the `margin_scale`.\n6. Combine the rank-based term and the adaptive scale to get the final margin: margin = margin_scale * rank_gap(delta_cost).\n7. (Inherited from Parent 1) Compute the core logistic loss using the normalized logp difference and the adaptive margin: loss = -logsigmoid(normalized_delta_logp - margin).\n8. Return the mean of the instance losses over the batch.", "hyperparams": {"gamma": 1.5, "eps": 1e-08}, "operators_used": ["logsigmoid", "rank_gap", "zscore"], "implementation_hint": {"expects": ["cost_a", "cost_b", "logp_a", "logp_b"], "returns": "scalar"}, "code": "import torch\nimport torch.nn.functional as F\n\ndef generated_loss(batch, model_output, extra):\n    \"\"\"\n    Normalized Rank-Gapped Sigmoid Loss.\n\n    This loss combines a rank-based margin (from Parent 0) with a logsigmoid\n    objective (from Parent 1). It introduces two new couplings:\n    1. Z-score normalization of the log-probability difference (`delta_logp`)\n       to make the loss robust to the scale and shift of model outputs.\n    2. An adaptive margin scale based on the standard deviation of the batch's\n       cost differences, making the learning target sensitive to the cost distribution.\n    \"\"\"\n    # Read hyperparameters from the 'extra' dictionary\n    gamma = extra.get('gamma', 1.5)\n    eps = extra.get('eps', 1e-8)\n\n    # Unpack tensors from the batch\n    # By convention, 'w' is winner (lower cost), 'l' is loser (higher cost)\n    cost_w = batch['cost_a']\n    cost_l = batch['cost_b']\n    logp_w = batch['log_prob_w']\n    logp_l = batch['log_prob_l']\n\n    # Sanity check: cost_w should be less than or equal to cost_l\n    assert torch.all(cost_w <= cost_l), \"Assumption failed: cost_a (winner) must be <= cost_b (loser)\"\n\n    # 1. Calculate cost and log-probability differences\n    delta_cost = cost_l - cost_w  # This will be >= 0\n    delta_logp = logp_w - logp_l # We want this to be positive\n\n    # 2. (New Coupling 1) Z-score normalization of delta_logp\n    if delta_logp.numel() > 1:\n        mean_logp = delta_logp.mean()\n        std_logp = delta_logp.std() + eps\n        normalized_delta_logp = (delta_logp - mean_logp) / std_logp\n    else:\n        # z-score is 0 for a single element\n        normalized_delta_logp = torch.zeros_like(delta_logp)\n\n    # 3. (Inherited from Parent 0) Compute rank-based margin base\n    if delta_cost.numel() > 1:\n        ranks = delta_cost.argsort().argsort().float()\n        rank_gaps = ranks / (delta_cost.numel() - 1)\n    else:\n        rank_gaps = torch.ones_like(delta_cost)\n\n    # 4. (New Coupling 2) Compute adaptive margin scale\n    # The margin's magnitude adapts to the spread of costs in the batch.\n    margin_scale = gamma * (delta_cost.std() if delta_cost.numel() > 1 else 0.0)\n\n    # 5. Compute the final adaptive margin\n    margin = margin_scale * rank_gaps\n\n    # 6. (Inherited from Parent 1) Compute the core logistic loss\n    # The loss is low when normalized_delta_logp > margin.\n    # Using -logsigmoid(x) is equivalent to softplus(-x).\n    instance_loss = -F.logsigmoid(normalized_delta_logp - margin)\n\n    # Apply optional sample weights if provided\n    if 'weight' in batch and batch['weight'] is not None:\n        instance_loss = instance_loss * batch['weight']\n\n    # 7. Return the mean loss over the batch\n    return instance_loss.mean()"}, "static_ok": true, "static_reason": "", "dynamic_ok": true, "dynamic_reason": "ok", "loss_value": 0.6931471824645996, "grad_norm": 0.0}
{"generation": 5, "index": 0, "ir": {"name": "Rank-Clipped Tanh-Weighted Logistic Loss", "intuition": "This loss function creates a learning signal that is robust to both outlier cost differences and outlier model predictions. It achieves this by combining a rank-based margin with a tanh-based weighting scheme, and introduces a novel clipping mechanism for stability.\n\nInherited Ideas:\n- From Parent 0 (Rank-Normalized Margin with Sigmoid Cost Modulation), it inherits the idea of using the rank of a normalized cost difference to construct the margin. Specifically, it uses `rank_gap(zscore(delta_cost))` to create a margin that is adaptive to the batch's statistical properties and robust to the cost distribution.\n- From Parent 1 (Saturated Rank-Weighted Logistic Loss), it inherits the use of `tanh` to create a smooth, bounded signal from the cost difference. However, instead of using it for the margin, this child uses it as a weighting factor.\n\nNew Coupling Ideas:\n1.  **Tanh-Weighted Loss**: The final loss is weighted by `tanh(delta_cost)`. This acts as a smooth, bounded emphasis mechanism. Pairs with small cost differences receive a proportional weight, while pairs with very large cost differences have their influence gracefully saturated, preventing them from dominating the loss. This is a new way to couple the raw cost difference with the core loss.\n2.  **Rank-Clipped Margin**: The margin is constructed using the rank-gapped z-scored cost difference, inherited from Parent 0. However, a new stability trick is introduced: the margin is clamped to a maximum value, `margin_max`. The formula is `clamp(gamma * rank_gap(zscore(delta_cost)), max=margin_max)`. This coupling prevents the margin from becoming excessively large, even if the scaling factor `gamma` is set high, which can happen in small batches or with unusual data distributions. It acts as a safety valve, ensuring the target `delta_logp` remains within a reasonable, stable range.", "pseudocode": "1. Calculate the cost difference: delta_cost = cost(b) - cost(a).\n2. Calculate the log-probability difference: delta_logp = logp(a) - logp(b).\n3. (Inherited from Parent 0) Normalize the cost difference using z-scoring: normalized_delta_cost = zscore(delta_cost).\n4. (Inherited from Parent 0 & New Coupling 2) Compute a rank-clipped margin. First, calculate the rank_gap of the normalized_delta_cost. Scale this by `gamma` and then clamp the result to a maximum value `margin_max`. The margin is `clamp(gamma * rank_gap(normalized_delta_cost), max=margin_max)`.\n5. Compute the core logistic loss using the rank-clipped margin: core_loss = softplus(margin - delta_logp).\n6. (Inherited from Parent 1 & New Coupling 1) Compute a tanh-based adaptive weight. Apply a `tanh` function to the raw delta_cost: adaptive_weight = tanh(delta_cost). This smoothly saturates the weight for large cost differences.\n7. The final loss is the product of the adaptive weight and the core loss, averaged over the batch: mean(adaptive_weight * core_loss).", "hyperparams": {"gamma": 2.5, "margin_max": 5.0, "eps": 1e-08}, "operators_used": ["softplus", "tanh", "zscore", "rank_gap", "clamp"], "implementation_hint": {"expects": ["cost_a", "cost_b", "logp_a", "logp_b"], "returns": "scalar"}, "code": "import torch\nimport torch.nn.functional as F\n\ndef generated_loss(batch, model_output, extra):\n    \"\"\"\n    Rank-Clipped Tanh-Weighted Logistic Loss.\n\n    This loss combines a margin derived from the rank of z-scored cost differences with a\n    tanh-based weight on the raw cost difference. A clipping mechanism is added to the\n    margin for enhanced stability.\n\n    - Inherits `rank_gap(zscore(delta_cost))` for margin construction from Parent 0.\n    - Inherits `tanh` for signal modulation from Parent 1, but uses it for weighting.\n    - New Coupling 1: The loss is weighted by `tanh(delta_cost)`, providing a smooth,\n      bounded emphasis on pairs with larger cost differences.\n    - New Coupling 2: The margin is clipped via `clamp(..., max=margin_max)` to prevent\n      it from becoming excessively large, improving numerical stability.\n    \"\"\"\n    # Read hyperparameters from the 'extra' dictionary\n    gamma = extra.get('gamma', 2.5)\n    margin_max = extra.get('margin_max', 5.0)\n    eps = extra.get('eps', 1e-8)\n\n    # Unpack tensors from the batch\n    cost_w = batch['cost_a']\n    cost_l = batch['cost_b']\n    logp_w = batch['log_prob_w']\n    logp_l = batch['log_prob_l']\n\n    # Sanity check: cost_w should be less than or equal to cost_l\n    assert torch.all(cost_w <= cost_l), \"Assumption failed: cost_a (winner) must be <= cost_b (loser)\"\n\n    # 1. Calculate cost and log-probability differences\n    delta_cost = cost_l - cost_w  # This will be >= 0\n    delta_logp = logp_w - logp_l  # We want this to be positive\n\n    # 2. (Inherited from Parent 0) Normalize delta_cost using z-scoring\n    if delta_cost.numel() > 1:\n        mean_delta_cost = delta_cost.mean()\n        std_delta_cost = delta_cost.std() + eps\n        normalized_delta_cost = (delta_cost - mean_delta_cost) / std_delta_cost\n    else:\n        normalized_delta_cost = torch.zeros_like(delta_cost)\n\n    # 3. (Inherited from Parent 0 & New Coupling 2) Compute a rank-clipped margin\n    if normalized_delta_cost.numel() > 1:\n        ranks = normalized_delta_cost.argsort().argsort().float()\n        rank_gaps = ranks / (normalized_delta_cost.numel() - 1)\n    else:\n        rank_gaps = torch.ones_like(normalized_delta_cost)\n    \n    unclipped_margin = gamma * rank_gaps\n    margin = torch.clamp(unclipped_margin, max=margin_max)\n\n    # 4. Compute the core logistic loss\n    # The loss is low when delta_logp > margin.\n    core_loss = F.softplus(margin - delta_logp)\n\n    # 5. (Inherited from Parent 1 & New Coupling 1) Compute tanh-based adaptive weight\n    # This weight smoothly emphasizes pairs with larger cost differences, saturating for outliers.\n    adaptive_weight = torch.tanh(delta_cost)\n\n    # 6. Combine weight and core loss\n    instance_loss = adaptive_weight * core_loss\n\n    # Apply optional sample weights if provided\n    if 'weight' in batch and batch['weight'] is not None:\n        instance_loss = instance_loss * batch['weight']\n\n    # 7. Return the mean loss over the batch\n    return instance_loss.mean()"}, "static_ok": true, "static_reason": "", "dynamic_ok": true, "dynamic_reason": "ok", "loss_value": 1.182563304901123, "grad_norm": 0.0}
{"generation": 5, "index": 1, "ir": {"name": "Saturated Rank-Modulated Logistic Loss", "intuition": "This loss function creates a stable and adaptive learning signal by combining a saturating, cost-aware margin with a rank-based modulation of the log-probability difference. The goal is to focus learning on pairs with significant cost differences, as determined by their rank within the batch, while preventing outlier costs from destabilizing the training process.\n\nInherited Ideas:\n- From Parent 0 (Rank-Normalized Margin with Sigmoid Cost Modulation), it inherits the idea of normalizing the cost difference using `zscore(delta_cost)`. This makes the loss adaptive to the batch's statistical properties and robust to the absolute scale of costs.\n- From Parent 1 (Saturated Rank-Weighted Logistic Loss), it inherits the use of `tanh` to create a saturated, bounded signal from the cost difference. This prevents pairs with extremely large cost gaps from creating an unbounded learning signal, improving numerical stability.\n\nNew Coupling Ideas:\n1.  **Saturated Rank Modulation**: Instead of using rank to create a margin or a weight, this loss uses it to directly modulate the log-probability difference. The rank gap of the z-scored costs, `rank_gap(zscore(delta_cost))`, is used as a coefficient `alpha` for `delta_logp`. The term becomes `alpha * delta_logp`. This coupling forces the model to create a larger log-probability gap for pairs that have a higher relative cost difference within the batch, effectively prioritizing the learning on more significant preference pairs.\n2.  **Stable Saturating Margin**: The margin is derived from the z-scored cost, but it's passed through a `tanh` function: `margin = gamma * tanh(zscore(delta_cost))`. This combines the batch-adaptive nature of z-scoring (from Parent 0) with the saturation property of `tanh` (from Parent 1). The result is a margin that is sensitive to relative cost differences around the batch mean but saturates for extreme outliers, providing a stable yet informative target for the modulated `delta_logp` to overcome.", "pseudocode": "1. Calculate the cost difference: delta_cost = cost(b) - cost(a).\n2. Calculate the log-probability difference: delta_logp = logp(a) - logp(b).\n3. (Inherited from Parent 0) Normalize the cost difference using z-scoring: normalized_delta_cost = zscore(delta_cost).\n4. (New Coupling 1) Compute a rank-based modulation coefficient. Calculate the rank_gap of the normalized_delta_cost, which maps its values to a [0, 1] range based on sorted order. This is the coefficient `alpha`.\n5. Modulate the log-probability difference with this coefficient: modulated_delta_logp = alpha * delta_logp.\n6. (New Coupling 2 & Inherited from Parent 1) Compute a stable, saturating margin. Apply a `tanh` function to the normalized_delta_cost and scale it by a hyperparameter `gamma`. The margin is `gamma * tanh(normalized_delta_cost)`.\n7. Compute the final loss using a `softplus` structure, which is common to both parents: loss = softplus(margin - modulated_delta_logp).\n8. The final loss is the mean of the instance losses over the batch.", "hyperparams": {"gamma": 1.0, "eps": 1e-08}, "operators_used": ["softplus", "tanh", "zscore", "rank_gap"], "implementation_hint": {"expects": ["cost_a", "cost_b", "logp_a", "logp_b"], "returns": "scalar"}, "code": "import torch\nimport torch.nn.functional as F\n\ndef generated_loss(batch, model_output, extra):\n    \"\"\"\n    Saturated Rank-Modulated Logistic Loss.\n\n    This loss combines a tanh-saturated margin based on z-scored costs with a \n    rank-based modulation of the log-probability difference.\n\n    - Inherits `zscore` for batch-adaptive cost normalization from Parent 0.\n    - Inherits `tanh` for creating a stable, saturating signal from Parent 1.\n    - New Coupling 1: Modulates `delta_logp` by `rank_gap(zscore(delta_cost))`, \n      forcing a larger logp gap for pairs with higher relative cost differences.\n    - New Coupling 2: Creates a stable margin `gamma * tanh(zscore(delta_cost))` that\n      is both batch-adaptive and robust to extreme outliers.\n    \"\"\"\n    # Read hyperparameters from the 'extra' dictionary\n    gamma = extra.get('gamma', 1.0)\n    eps = extra.get('eps', 1e-8)\n\n    # Unpack tensors from the batch\n    cost_w = batch['cost_a']\n    cost_l = batch['cost_b']\n    logp_w = batch['log_prob_w']\n    logp_l = batch['log_prob_l']\n\n    # Sanity check: cost_w should be less than or equal to cost_l\n    assert torch.all(cost_w <= cost_l), \"Assumption failed: cost_a (winner) must be <= cost_b (loser)\"\n\n    # 1. Calculate cost and log-probability differences\n    delta_cost = cost_l - cost_w  # This will be >= 0\n    delta_logp = logp_w - logp_l  # We want this to be positive\n\n    # 2. (Inherited from Parent 0) Normalize delta_cost using z-scoring\n    if delta_cost.numel() > 1:\n        mean_delta_cost = delta_cost.mean()\n        std_delta_cost = delta_cost.std() + eps\n        normalized_delta_cost = (delta_cost - mean_delta_cost) / std_delta_cost\n    else:\n        normalized_delta_cost = torch.zeros_like(delta_cost)\n\n    # 3. (New Coupling 1) Compute rank-based modulation coefficient `alpha`\n    if normalized_delta_cost.numel() > 1:\n        ranks = normalized_delta_cost.argsort().argsort().float()\n        alpha = ranks / (normalized_delta_cost.numel() - 1)\n    else:\n        alpha = torch.ones_like(normalized_delta_cost)\n    \n    # Modulate the log-probability difference\n    modulated_delta_logp = alpha * delta_logp\n\n    # 4. (New Coupling 2 & Inherited from Parent 1) Compute stable, saturating margin\n    # This margin is adaptive to batch stats (z-score) and stable (tanh).\n    margin = gamma * torch.tanh(normalized_delta_cost)\n\n    # 5. Compute the core loss using a softplus structure\n    # The loss is low when modulated_delta_logp > margin.\n    instance_loss = F.softplus(margin - modulated_delta_logp)\n\n    # Apply optional sample weights if provided\n    if 'weight' in batch and batch['weight'] is not None:\n        instance_loss = instance_loss * batch['weight']\n\n    # 6. Return the mean loss over the batch\n    return instance_loss.mean()"}, "static_ok": true, "static_reason": "", "dynamic_ok": true, "dynamic_reason": "ok", "loss_value": 0.6931471824645996, "grad_norm": 0.0}
{"generation": 5, "index": 2, "ir": {"name": "Adaptive Saturated Margin Loss with Rank-Gap Modulation", "intuition": "This loss function creates a robust learning signal by combining a batch-adaptive, saturated margin with a smooth, rank-based modulation of the loss. The goal is to provide a strong learning signal for pairs with significant cost differences, while being robust to both the absolute scale of costs and their distribution within the batch.\n\nInherited Ideas:\n- From Parent 0 (Rank-Normalized Margin with Sigmoid Cost Modulation), it inherits the use of z-scoring (`zscore(delta_cost)`) to normalize cost differences. This makes the margin adaptive to the batch's statistical properties (mean and standard deviation), preventing outlier costs from dominating.\n- From Parent 1 (Saturated Rank-Weighted Logistic Loss), it inherits the use of `tanh` to create a saturated margin. This ensures the target `delta_logp` is sensitive to cost differences but does not grow unboundedly, which improves numerical stability.\n\nNew Coupling Ideas:\n1.  **Adaptive Saturated Margin**: This is a new margin formulation that combines the inherited ideas. The margin is calculated as `gamma * tanh(zscore(delta_cost))`. By applying `tanh` to the z-scored cost difference, the margin becomes both adaptive to the batch statistics (from z-scoring) and saturated (from tanh). This makes the margin robust to the scale, mean, and variance of costs within a batch, providing a stable target for `delta_logp`.\n2.  **Rank-Gap Loss Modulation**: Instead of using the rank of costs to define the margin or as a hard weight, this loss uses it to modulate the final loss value. The core loss is computed as `softplus(margin - delta_logp)`. This loss is then multiplied by `(1 + rank_gap(delta_cost))`. This smoothly scales the loss, giving more importance to pairs with a higher relative cost difference within the batch (rank_gap near 1) and less to those with a smaller relative difference (rank_gap near 0). The `1 +` term ensures that even the lowest-ranked pair (rank_gap=0) still contributes a non-zero loss, preventing gradient death for those pairs.", "pseudocode": "1. Calculate the cost difference: delta_cost = cost(b) - cost(a).\n2. Calculate the log-probability difference: delta_logp = logp(a) - logp(b).\n3. (Inherited from Parent 0) Normalize the cost difference using z-scoring to make it batch-adaptive: normalized_delta_cost = zscore(delta_cost).\n4. (New Coupling 1 - combines ideas from both parents) Compute an adaptive saturated margin. Apply a `tanh` function to the normalized_delta_cost and scale it by a hyperparameter `gamma`. The margin is `gamma * tanh(normalized_delta_cost)`.\n5. Compute the core logistic loss using this new margin: `core_loss = softplus(margin - delta_logp)`.\n6. (New Coupling 2) Compute a rank-based loss modulator. Calculate the `rank_gap` of the raw delta_cost, which maps values to a [0, 1] range based on their sorted order. Create the modulator as `(1 + rank_gap(delta_cost))`.\n7. The final loss is the product of the rank-based modulator and the core loss, averaged over the batch: `mean((1 + rank_gap(delta_cost)) * core_loss)`.", "hyperparams": {"gamma": 2.5, "eps": 1e-08}, "operators_used": ["softplus", "tanh", "zscore", "rank_gap"], "implementation_hint": {"expects": ["cost_a", "cost_b", "logp_a", "logp_b"], "returns": "scalar"}, "code": "import torch\nimport torch.nn.functional as F\n\ndef generated_loss(batch, model_output, extra):\n    \"\"\"\n    Adaptive Saturated Margin Loss with Rank-Gap Modulation.\n\n    This loss combines a margin that is both saturated (via tanh) and adaptive\n    to batch statistics (via z-scoring) with a smooth loss modulation based on\n    the rank of the cost difference.\n\n    - Inherits `zscore` from Parent 0 for batch-adaptive normalization.\n    - Inherits `tanh` from Parent 1 for a saturated, stable margin.\n    - New Coupling 1: Creates an adaptive saturated margin `gamma * tanh(zscore(delta_cost))`.\n    - New Coupling 2: Modulates the final loss by `(1 + rank_gap(delta_cost))`,\n      smoothly up-weighting pairs with higher relative cost differences.\n    \"\"\"\n    # Read hyperparameters from the 'extra' dictionary\n    gamma = extra.get('gamma', 2.5)\n    eps = extra.get('eps', 1e-8)\n\n    # Unpack tensors from the batch\n    cost_w = batch['cost_a']\n    cost_l = batch['cost_b']\n    logp_w = batch['log_prob_w']\n    logp_l = batch['log_prob_l']\n\n    # Sanity check: cost_w should be less than or equal to cost_l\n    assert torch.all(cost_w <= cost_l), \"Assumption failed: cost_a (winner) must be <= cost_b (loser)\"\n\n    # 1. Calculate cost and log-probability differences\n    delta_cost = cost_l - cost_w  # This will be >= 0\n    delta_logp = logp_w - logp_l  # We want this to be positive\n\n    # 2. (Inherited from Parent 0) Normalize delta_cost using z-scoring\n    if delta_cost.numel() > 1:\n        mean_delta_cost = delta_cost.mean()\n        std_delta_cost = delta_cost.std() + eps\n        normalized_delta_cost = (delta_cost - mean_delta_cost) / std_delta_cost\n    else:\n        normalized_delta_cost = torch.zeros_like(delta_cost)\n\n    # 3. (New Coupling 1) Compute an adaptive saturated margin\n    # Combines z-scoring (Parent 0) with tanh saturation (Parent 1).\n    margin = gamma * torch.tanh(normalized_delta_cost)\n\n    # 4. Compute the core logistic loss\n    core_loss = F.softplus(margin - delta_logp)\n\n    # 5. (New Coupling 2) Compute a rank-based loss modulator\n    if delta_cost.numel() > 1:\n        ranks = delta_cost.argsort().argsort().float()\n        rank_gaps = ranks / (delta_cost.numel() - 1)\n    else:\n        rank_gaps = torch.ones_like(delta_cost)\n    \n    # The (1 + ...) ensures even the lowest-ranked pair has a non-zero gradient path.\n    loss_modulator = 1.0 + rank_gaps\n\n    # 6. Combine modulator and core loss\n    instance_loss = loss_modulator * core_loss\n\n    # Apply optional sample weights if provided\n    if 'weight' in batch and batch['weight'] is not None:\n        instance_loss = instance_loss * batch['weight']\n\n    # 7. Return the mean loss over the batch\n    return instance_loss.mean()"}, "static_ok": true, "static_reason": "", "dynamic_ok": true, "dynamic_reason": "ok", "loss_value": 1.0397207736968994, "grad_norm": 0.0}
{"generation": 5, "index": 3, "ir": {"name": "Adaptive Saturated Margin with Z-Score Modulation", "intuition": "This loss function creates a stable and adaptive learning signal by combining a saturated, cost-aware margin with a batch-normalized modulation factor. The goal is to encourage a log-probability gap that is sensitive to the cost difference, but robust to outliers in both cost and log-probability space.\n\nInherited Ideas:\n- From Parent 1 (Saturated Rank-Weighted Logistic Loss), it inherits the idea of a saturated margin using `tanh(delta_cost)`. This ensures the target margin is sensitive to cost differences but does not grow unboundedly, preventing instability from pairs with extremely large cost gaps.\n- From Parent 0 (Rank-Normalized Margin with Sigmoid Cost Modulation), it inherits the use of `zscore` to normalize a quantity based on batch statistics. However, instead of normalizing the cost, we normalize the log-probability difference (`delta_logp`), making the loss sensitive to how 'surprising' the model's current preference is for a given pair, relative to the batch.\n\nNew Coupling Ideas:\n1. **Z-Score Modulated Tanh Margin**: The core margin is `gamma * tanh(delta_cost)`, inherited from Parent 1. This margin is then coupled with the z-scored `delta_logp`. The final loss term is `softplus(margin - delta_logp)`. This structure is standard. The novelty comes from how we use the z-score later.\n2. **Adaptive Sigmoid Modulation**: The final loss is modulated (multiplied) by an adaptive factor `sigmoid(-zscore(delta_logp))`. This factor acts as a dynamic curriculum. If a pair's `delta_logp` is already much higher than the batch average (i.e., `zscore(delta_logp)` is large and positive), the model is already confident. The sigmoid factor becomes small, reducing the loss and focusing learning elsewhere. Conversely, if the `delta_logp` is much lower than the batch average (a 'difficult' pair), the sigmoid factor approaches 1, prioritizing learning on this example. This coupling of the loss magnitude with the relative log-probability difference helps focus on the most informative pairs in the batch.", "pseudocode": "1. Calculate the cost difference: delta_cost = cost(b) - cost(a).\n2. Calculate the log-probability difference: delta_logp = logp(a) - logp(b).\n3. (Inherited from Parent 1) Compute a saturated, cost-aware margin. The margin is `gamma * tanh(delta_cost)`. This creates a target logp gap that grows with the cost difference but is bounded, preventing instability from outliers.\n4. (New Coupling 1) Compute the core loss using the saturated margin: `core_loss = softplus(margin - delta_logp)`.\n5. (Inherited from Parent 0, but applied differently) Normalize the log-probability difference using z-scoring: `normalized_delta_logp = zscore(delta_logp)`. This captures how confident the model is on a given pair relative to other pairs in the batch.\n6. (New Coupling 2) Compute an adaptive modulation factor based on the normalized logp difference. The factor is `sigmoid(-normalized_delta_logp)`. This factor is close to 1 for pairs where the model is unconfident (low delta_logp) and close to 0 for pairs where it is already confident (high delta_logp).\n7. The final loss is the product of the modulation factor and the core loss, averaged over the batch: `mean(sigmoid(-normalized_delta_logp) * core_loss)`.", "hyperparams": {"gamma": 1.0, "eps": 1e-08}, "operators_used": ["softplus", "tanh", "sigmoid", "zscore"], "implementation_hint": {"expects": ["cost_a", "cost_b", "logp_a", "logp_b", "weight"], "returns": "scalar"}, "code": "import torch\nimport torch.nn.functional as F\n\ndef generated_loss(batch, model_output, extra):\n    \"\"\"\n    Adaptive Saturated Margin with Z-Score Modulation.\n\n    This loss combines a tanh-saturated margin sensitive to cost differences with a\n    modulating factor based on the z-score of the log-probability differences.\n\n    - Inherits `tanh(delta_cost)` for a bounded margin from Parent 1.\n    - Inherits `zscore` for batch-relative normalization from Parent 0, but applies it to delta_logp.\n    - New Coupling 1: Uses a standard `softplus(margin - delta_logp)` loss structure.\n    - New Coupling 2: Modulates this loss with `sigmoid(-zscore(delta_logp))`, which acts as a\n      dynamic curriculum, focusing on pairs where the model is relatively unconfident.\n    \"\"\"\n    # Read hyperparameters from the 'extra' dictionary\n    gamma = extra.get('gamma', 1.0)\n    eps = extra.get('eps', 1e-8)\n\n    # Unpack tensors from the batch\n    cost_w = batch['cost_a']\n    cost_l = batch['cost_b']\n    logp_w = batch['log_prob_w']\n    logp_l = batch['log_prob_l']\n\n    # Sanity check: cost_w should be less than or equal to cost_l\n    assert torch.all(cost_w <= cost_l), \"Assumption failed: cost_a (winner) must be <= cost_b (loser)\"\n\n    # 1. Calculate cost and log-probability differences\n    delta_cost = cost_l - cost_w  # Should be >= 0\n    delta_logp = logp_w - logp_l  # We want this to be positive\n\n    # 2. (Inherited from Parent 1) Compute a saturated, cost-aware margin\n    margin = gamma * torch.tanh(delta_cost)\n\n    # 3. (New Coupling 1) Compute the core loss\n    core_loss = F.softplus(margin - delta_logp)\n\n    # 4. (Inherited from Parent 0, applied differently) Normalize delta_logp using z-scoring\n    if delta_logp.numel() > 1:\n        mean_delta_logp = delta_logp.mean()\n        std_delta_logp = delta_logp.std() + eps\n        normalized_delta_logp = (delta_logp - mean_delta_logp) / std_delta_logp\n    else:\n        # Handle batch size of 1 gracefully, no modulation needed.\n        normalized_delta_logp = torch.zeros_like(delta_logp)\n\n    # 5. (New Coupling 2) Compute an adaptive modulation factor\n    # This factor is high (->1) when delta_logp is below the batch average (normalized < 0)\n    # and low (->0) when delta_logp is above the batch average (normalized > 0).\n    adaptive_modulation = torch.sigmoid(-normalized_delta_logp)\n\n    # 6. Combine the modulation factor and the core loss\n    instance_loss = adaptive_modulation * core_loss\n\n    # Apply optional sample weights if provided\n    if 'weight' in batch and batch['weight'] is not None:\n        instance_loss = instance_loss * batch['weight']\n\n    # 7. Return the mean loss over the batch\n    return instance_loss.mean()"}, "static_ok": true, "static_reason": "", "dynamic_ok": true, "dynamic_reason": "ok", "loss_value": 0.5723800659179688, "grad_norm": 0.0}
{"generation": 5, "index": 4, "ir": {"name": "Adaptive Saturated Logistic Loss with Rank-Gap Modulation", "intuition": "This loss function creates a robust learning signal by combining batch-normalized cost information with a saturating margin, and then weighting the final loss by the relative importance of each pair. The goal is to be sensitive to cost differences while remaining stable against outliers in both cost and log-probability space.\n\nInherited Ideas:\n- From Parent 0 (Rank-Normalized Margin...), it inherits the idea of using z-scoring (`zscore`) on the cost difference (`delta_cost`) to create a batch-adaptive, normalized representation of cost. This makes the loss less sensitive to the absolute scale of costs.\n- From Parent 1 (Saturated Rank-Weighted...), it inherits the use of a saturating `tanh` function to create the margin (`gamma * tanh(...)`). This prevents pairs with extremely large cost differences from dominating the learning signal and creating an unstable, excessively large margin.\n\nNew Coupling Ideas:\n1.  **Coupled Saturated-Adaptive Margin**: The two inherited ideas are coupled directly. The margin is now `gamma * tanh(zscore(delta_cost))`. This creates a margin that is both adaptive to the batch's cost distribution (due to z-scoring) and saturated (due to tanh). It responds to how surprisingly large a cost gap is relative to the batch, but smoothly caps the maximum required `delta_logp` to avoid instability.\n2.  **Rank-Gap Loss Modulation**: Instead of using the rank of `delta_cost` to define the margin or as a direct weight, it is used to modulate the final loss term. The loss is computed as `rank_gap(delta_cost) * softplus(margin - delta_logp)`. This coupling ensures that pairs with a higher relative cost difference within the batch contribute more significantly to the final loss, effectively prioritizing learning on the most distinct preference pairs. This is different from Parent 1, where `rank_gap` weighted a loss with a non-z-scored margin, and different from Parent 0, where `rank_gap` was part of the margin itself.", "pseudocode": "1. Calculate the cost difference: delta_cost = cost(b) - cost(a).\n2. Calculate the log-probability difference: delta_logp = logp(a) - logp(b).\n3. (Inherited from Parent 0) Normalize the cost difference using z-scoring: normalized_delta_cost = zscore(delta_cost).\n4. (New Coupling 1 & Inherited from Parent 1) Compute a saturated, adaptive margin by applying a scaled tanh function to the normalized cost difference: margin = gamma * tanh(normalized_delta_cost).\n5. (New Coupling 2) Compute a modulating weight based on the relative importance of the cost difference within the batch. This is done by calculating the rank_gap of the raw delta_cost: rank_modulator = rank_gap(delta_cost).\n6. Compute the core logistic loss using the adaptive margin: core_loss = softplus(margin - delta_logp).\n7. The final loss for each pair is the product of the rank modulator and the core loss.\n8. Average the final loss over the entire batch: mean(rank_modulator * core_loss).", "hyperparams": {"gamma": 2.5, "eps": 1e-08}, "operators_used": ["softplus", "tanh", "zscore", "rank_gap"], "implementation_hint": {"expects": ["cost_a", "cost_b", "logp_a", "logp_b", "weight"], "returns": "scalar"}, "code": "import torch\nimport torch.nn.functional as F\n\ndef generated_loss(batch, model_output, extra):\n    \"\"\"\n    Adaptive Saturated Logistic Loss with Rank-Gap Modulation.\n\n    This loss combines a z-score normalized, tanh-saturated margin for stability\n    with a rank_gap modulation to prioritize learning on relatively important pairs.\n    \"\"\"\n    # Read hyperparameters from the 'extra' dictionary\n    gamma = extra.get('gamma', 2.5)\n    eps = extra.get('eps', 1e-8)\n\n    # Unpack tensors from the batch\n    cost_w = batch['cost_a']\n    cost_l = batch['cost_b']\n    logp_w = batch['log_prob_w']\n    logp_l = batch['log_prob_l']\n\n    # Sanity check for preference direction\n    assert torch.all(cost_w <= cost_l), \"Assumption failed: cost_a (winner) must be <= cost_b (loser)\"\n\n    # 1. Calculate cost and log-probability differences\n    delta_cost = cost_l - cost_w\n    delta_logp = logp_w - logp_l\n\n    # 2. (Inherited from Parent 0) Normalize delta_cost using z-scoring\n    if delta_cost.numel() > 1:\n        mean_delta_cost = delta_cost.mean()\n        std_delta_cost = delta_cost.std() + eps\n        normalized_delta_cost = (delta_cost - mean_delta_cost) / std_delta_cost\n    else:\n        normalized_delta_cost = torch.zeros_like(delta_cost)\n\n    # 3. (New Coupling 1 & Inherited from Parent 1) Compute a saturated, adaptive margin\n    # This margin is sensitive to the cost's standing within the batch but is capped.\n    margin = gamma * torch.tanh(normalized_delta_cost)\n\n    # 4. (New Coupling 2) Compute a rank-based loss modulator\n    # This weight emphasizes pairs with a higher relative cost difference in the batch.\n    if delta_cost.numel() > 1:\n        ranks = delta_cost.argsort().argsort().float()\n        rank_modulator = ranks / (delta_cost.numel() - 1)\n    else:\n        rank_modulator = torch.ones_like(delta_cost)\n\n    # 5. Compute the core logistic loss\n    # The loss is low when delta_logp is greater than the adaptive margin.\n    core_loss = F.softplus(margin - delta_logp)\n\n    # 6. Modulate the loss by the rank-based weight\n    instance_loss = rank_modulator * core_loss\n\n    # Apply optional sample weights if provided\n    if 'weight' in batch and batch['weight'] is not None:\n        instance_loss = instance_loss * batch['weight']\n\n    # 7. Return the mean loss over the batch\n    return instance_loss.mean()"}, "static_ok": true, "static_reason": "", "dynamic_ok": true, "dynamic_reason": "ok", "loss_value": 0.3465735912322998, "grad_norm": 0.0}
{"generation": 5, "index": 5, "ir": {"name": "Z-Scored Tanh-Saturated Margin with Rank Weighting", "intuition": "This loss function creates a robust learning signal by combining batch-normalized cost information for setting the margin with rank-based weighting to prioritize learning. The goal is to create a margin that is sensitive to the local cost landscape of a batch but saturates to prevent instability from outliers, while simultaneously ensuring that pairs with a higher relative cost difference contribute more significantly to the overall loss.\n\nInherited Ideas:\n- From Parent 0 (Rank-Normalized Margin...), it inherits the idea of normalizing the cost difference using z-scoring (`zscore(delta_cost)`). This makes the margin adaptive to the batch's statistical properties (mean and standard deviation) and robust to the absolute scale of costs.\n- From Parent 1 (Saturated Rank-Weighted...), it inherits two key concepts: \n  1. The use of a `tanh` function to create a saturated, bounded margin, which prevents outlier cost differences from creating an excessively large margin and destabilizing training.\n  2. The use of `rank_gap(delta_cost)` as an adaptive weight, ensuring that pairs with a higher relative cost difference within the batch are given more importance.\n\nNew Coupling Ideas:\n1.  **Z-Score Saturated Margin**: This is a novel way to construct the margin. It first computes the z-score of the cost differences (`zscore(delta_cost)`) and then passes this normalized value through a `tanh` function before scaling. The formula is `margin = gamma * tanh(zscore(delta_cost))`. This coupling synergizes the benefits of both parents: the z-scoring (from Parent 0) makes the input to the tanh function independent of the absolute cost scale, while the `tanh` function (from Parent 1) provides saturation. This creates a margin that is sensitive to a pair's cost difference relative to the batch average, but smoothly caps its influence for extreme values, leading to a highly stable and adaptive margin.\n2.  **Rank-Weighted Saturated Loss**: The final loss is a direct product of the `rank_weight` (from Parent 1) and the core loss term, `softplus(margin - delta_logp)`. This coupling ensures that the learning signal is strongest for pairs that are both relatively important within the batch (high rank) and where the model fails to meet the adaptively saturated margin. This design effectively balances the relative importance of a pair (via rank weight) with a stable, batch-aware target (via the z-score saturated margin).", "pseudocode": "1. Calculate the cost difference: delta_cost = cost(b) - cost(a).\n2. Calculate the log-probability difference: delta_logp = logp(a) - logp(b).\n3. (Inherited from Parent 1) Compute a rank-based adaptive weight. Calculate the rank_gap of the raw delta_cost, mapping its values to a [0, 1] range based on sorted order. This is the `rank_weight`.\n4. (Inherited from Parent 0) Normalize the cost difference using z-scoring: `normalized_delta_cost = zscore(delta_cost)`.\n5. (New Coupling 1) Compute a z-score saturated margin. Apply a `tanh` function (inherited from Parent 1) to the `normalized_delta_cost` and scale it by a hyperparameter `gamma`. The margin is `gamma * tanh(normalized_delta_cost)`. This creates a margin that is both batch-adaptive and bounded.\n6. Compute the core logistic loss using the saturated margin: `core_loss = softplus(margin - delta_logp)`.\n7. (New Coupling 2) The final loss is the product of the rank-based adaptive weight and the core loss, averaged over the batch: `mean(rank_weight * core_loss)`.", "hyperparams": {"gamma": 1.0, "eps": 1e-08}, "operators_used": ["softplus", "tanh", "zscore", "rank_gap"], "implementation_hint": {"expects": ["cost_a", "cost_b", "logp_a", "logp_b", "weight"], "returns": "scalar"}, "code": "import torch\nimport torch.nn.functional as F\n\ndef generated_loss(batch, model_output, extra):\n    \"\"\"\n    Z-Scored Tanh-Saturated Margin with Rank Weighting.\n\n    This loss combines a margin derived from the tanh of z-scored cost differences\n    with an adaptive weight based on the rank of the raw cost differences.\n\n    - Inherits `zscore(delta_cost)` from Parent 0 for batch normalization.\n    - Inherits `tanh` saturation and `rank_gap` weighting from Parent 1.\n    - New Coupling 1: The margin `gamma * tanh(zscore(delta_cost))` is robust to\n      both cost scale (z-score) and outliers (tanh), making it highly stable.\n    - New Coupling 2: The final loss `rank_weight * softplus(...)` balances the\n      relative importance of a pair (rank) with a stable, adaptive learning target (margin).\n    \"\"\"\n    # Read hyperparameters from the 'extra' dictionary\n    gamma = extra.get('gamma', 1.0)\n    eps = extra.get('eps', 1e-8)\n\n    # Unpack tensors from the batch\n    cost_w = batch['cost_a']\n    cost_l = batch['cost_b']\n    logp_w = batch['log_prob_w']\n    logp_l = batch['log_prob_l']\n\n    # Sanity check: cost_w should be less than or equal to cost_l\n    assert torch.all(cost_w <= cost_l), \"Assumption failed: cost_a (winner) must be <= cost_b (loser)\"\n\n    # 1. Calculate cost and log-probability differences\n    delta_cost = cost_l - cost_w  # This will be >= 0\n    delta_logp = logp_w - logp_l  # We want this to be positive\n\n    # 2. (Inherited from Parent 1) Compute rank-based adaptive weight from raw delta_cost.\n    if delta_cost.numel() > 1:\n        ranks = delta_cost.argsort().argsort().float()\n        rank_weight = ranks / (delta_cost.numel() - 1)\n    else:\n        rank_weight = torch.ones_like(delta_cost)\n\n    # 3. (Inherited from Parent 0) Normalize delta_cost using z-scoring\n    if delta_cost.numel() > 1:\n        mean_delta_cost = delta_cost.mean()\n        std_delta_cost = delta_cost.std() + eps\n        normalized_delta_cost = (delta_cost - mean_delta_cost) / std_delta_cost\n    else:\n        normalized_delta_cost = torch.zeros_like(delta_cost)\n\n    # 4. (New Coupling 1) Compute a z-score saturated margin.\n    # This combines z-scoring (Parent 0) with tanh saturation (Parent 1).\n    # The margin is sensitive to relative cost diffs but is bounded, preventing instability.\n    margin = gamma * torch.tanh(normalized_delta_cost)\n\n    # 5. Compute the core logistic loss.\n    # The loss is low when delta_logp > margin.\n    core_loss = F.softplus(margin - delta_logp)\n\n    # 6. (New Coupling 2) Combine rank weight and core loss.\n    # The final loss is modulated by the rank-based weight.\n    instance_loss = rank_weight * core_loss\n\n    # Apply optional sample weights if provided\n    if 'weight' in batch and batch['weight'] is not None:\n        instance_loss = instance_loss * batch['weight']\n\n    # 7. Return the mean loss over the batch\n    return instance_loss.mean()"}, "static_ok": true, "static_reason": "", "dynamic_ok": true, "dynamic_reason": "ok", "loss_value": 0.3465735912322998, "grad_norm": 0.0}
{"generation": 5, "index": 6, "ir": {"name": "Adaptive Rank-Weighted Saturated Logistic Loss", "intuition": "This loss function creates a highly adaptive learning signal by combining rank-based weighting with a saturated, z-score-normalized margin. The goal is to produce a loss that is robust to variations in the scale and distribution of costs within a batch, while focusing learning on pairs where the model's preference is misaligned with the cost-based ground truth.\n\nInherited Ideas:\n- From Parent 0 (Rank-Normalized Margin with Sigmoid Cost Modulation), it inherits the use of `zscore(delta_cost)` to normalize the cost difference. This makes the loss adaptive to the batch's statistical properties (mean and standard deviation) and robust to the absolute scale of costs.\n- From Parent 1 (Saturated Rank-Weighted Logistic Loss), it inherits the use of `rank_gap` to create an adaptive weight. This `rank_weight` ensures that pairs with a higher relative cost difference within the batch contribute more significantly to the gradient, focusing on the most important preference signals.\n\nNew Coupling Ideas:\n1.  **Saturated Z-Score Margin**: A new margin is constructed by coupling the `zscore` normalization from Parent 0 with the `tanh` saturation from Parent 1. The margin is `gamma * tanh(zscore(delta_cost))`. This creates a margin that is not only robust to the absolute scale of costs (due to z-scoring) but also saturates for pairs with extreme cost differences relative to the batch mean. This prevents outlier pairs from creating an unbounded margin, leading to greater stability.\n2.  **Rank-Weighted Loss with a Constant Offset**: The final loss is computed as `rank_weight * softplus(margin - delta_logp) + offset`. The `offset` is a small positive constant. This coupling ensures that even pairs with a `rank_weight` of zero (i.e., the pair with the smallest cost difference) still contribute a small, non-zero gradient. This prevents the model from completely ignoring these pairs, which could lead to unstable log-probabilities for the lowest-ranked pairs, and ensures a baseline learning signal for all pairs in the batch.", "pseudocode": "1. Calculate the cost difference: delta_cost = cost(b) - cost(a).\n2. Calculate the log-probability difference: delta_logp = logp(a) - logp(b).\n3. (Inherited from Parent 0) Normalize the cost difference using z-scoring: normalized_delta_cost = zscore(delta_cost).\n4. (Inherited from Parent 1) Compute a rank-based adaptive weight from the raw delta_cost: rank_weight = rank_gap(delta_cost).\n5. (New Coupling 1) Compute a saturated, z-score-normalized margin. Apply a `tanh` function to the normalized_delta_cost and scale it by a hyperparameter `gamma`: margin = gamma * tanh(normalized_delta_cost).\n6. Compute the core logistic loss: core_loss = softplus(margin - delta_logp).\n7. (New Coupling 2) The final loss for each pair is the product of the rank weight and the core loss, with a small offset added to ensure a non-zero gradient for all pairs: instance_loss = (rank_weight * core_loss) + offset.\n8. The final loss is the average of instance_loss over the batch.", "hyperparams": {"gamma": 1.5, "offset": 0.01, "eps": 1e-08}, "operators_used": ["softplus", "tanh", "zscore", "rank_gap"], "implementation_hint": {"expects": ["cost_a", "cost_b", "logp_a", "logp_b"], "returns": "scalar"}, "code": "import torch\nimport torch.nn.functional as F\n\ndef generated_loss(batch, model_output, extra):\n    \"\"\"\n    Adaptive Rank-Weighted Saturated Logistic Loss.\n\n    This loss combines rank-based weighting with a margin that is both normalized\n    by the batch statistics (z-score) and saturated (tanh).\n\n    - Inherits `zscore` normalization from Parent 0 for batch adaptiveness.\n    - Inherits `rank_gap` weighting from Parent 1 to focus on important pairs.\n    - New Coupling 1: Creates a `tanh(zscore(delta_cost))` margin, which is robust\n      to both the scale and distribution of costs, and is stable against outliers.\n    - New Coupling 2: Adds a small `offset` to the rank-weighted loss to ensure all\n      pairs, even those with zero rank weight, contribute a minimal gradient.\n    \"\"\"\n    # Read hyperparameters from the 'extra' dictionary\n    gamma = extra.get('gamma', 1.5)\n    offset = extra.get('offset', 0.01)\n    eps = extra.get('eps', 1e-8)\n\n    # Unpack tensors from the batch\n    cost_w = batch['cost_a']\n    cost_l = batch['cost_b']\n    logp_w = batch['log_prob_w']\n    logp_l = batch['log_prob_l']\n\n    # Sanity check: cost_w should be less than or equal to cost_l\n    assert torch.all(cost_w <= cost_l), \"Assumption failed: cost_a (winner) must be <= cost_b (loser)\"\n\n    # 1. Calculate cost and log-probability differences\n    delta_cost = cost_l - cost_w  # This will be >= 0\n    delta_logp = logp_w - logp_l  # We want this to be positive\n\n    # 2. (Inherited from Parent 0) Normalize delta_cost using z-scoring\n    if delta_cost.numel() > 1:\n        mean_delta_cost = delta_cost.mean()\n        std_delta_cost = delta_cost.std() + eps\n        normalized_delta_cost = (delta_cost - mean_delta_cost) / std_delta_cost\n    else:\n        normalized_delta_cost = torch.zeros_like(delta_cost)\n\n    # 3. (Inherited from Parent 1) Compute rank-based adaptive weight from raw delta_cost\n    if delta_cost.numel() > 1:\n        ranks = delta_cost.argsort().argsort().float()\n        rank_weight = ranks / (delta_cost.numel() - 1)  # Maps to [0, 1]\n    else:\n        rank_weight = torch.ones_like(delta_cost)\n\n    # 4. (New Coupling 1) Compute a saturated, z-score-normalized margin\n    # This margin is adaptive to batch stats and stable against extreme outliers.\n    margin = gamma * torch.tanh(normalized_delta_cost)\n\n    # 5. Compute the core logistic loss using the adaptive margin\n    core_loss = F.softplus(margin - delta_logp)\n\n    # 6. (New Coupling 2) Combine rank weight and core loss with an offset\n    # The offset ensures a minimal gradient even for pairs with rank_weight = 0.\n    instance_loss = (rank_weight * core_loss) + offset\n\n    # Apply optional sample weights if provided\n    if 'weight' in batch and batch['weight'] is not None:\n        instance_loss = instance_loss * batch['weight']\n\n    # 7. Return the mean loss over the batch\n    return instance_loss.mean()"}, "static_ok": true, "static_reason": "", "dynamic_ok": true, "dynamic_reason": "ok", "loss_value": 0.35657358169555664, "grad_norm": 0.0}
{"generation": 5, "index": 7, "ir": {"name": "Saturated Rank-Weighted Margin Loss with Z-Score Modulation", "intuition": "This loss function creates a stable and adaptive learning signal by combining a saturated margin, rank-based weighting, and z-score based loss modulation. The core idea is to create a cost-aware margin that doesn't explode for outliers, while ensuring the loss signal is strongest for pairs that are both relatively important within the batch and have a surprisingly large cost difference.\n\nInherited Ideas:\n- From Parent 1 (Saturated Rank-Weighted Logistic Loss), it inherits the concept of a `tanh`-saturated dynamic margin: `margin = gamma * tanh(delta_cost)`. This makes the target log-probability gap sensitive to the cost difference but prevents outlier pairs with huge cost gaps from dominating the learning signal.\n- From Parent 0 (Rank-Normalized Margin with Sigmoid Cost Modulation), it inherits the idea of using z-scoring (`zscore(delta_cost)`) to normalize the cost differences within a batch. This makes subsequent operations robust to the absolute scale of costs.\n\nNew Coupling Ideas:\n1.  **Rank-Weighted Saturated Margin**: The `tanh` function is applied to the z-scored cost difference, `zscore(delta_cost)`, instead of the raw `delta_cost`. This couples the saturation behavior with the batch's statistical properties. The margin, `gamma * tanh(zscore(delta_cost))`, now represents a target log-probability gap that is a function of how many standard deviations the pair's cost difference is from the batch mean, saturating for extreme values. This improves stability and adaptivity.\n2.  **Coupled Rank and Z-Score Modulation**: The final loss is weighted by the product of two adaptive terms: the `rank_gap` of the raw cost difference and the `sigmoid` of the z-scored cost difference. The `rank_gap(delta_cost)` term (inspired by Parent 1's weighting scheme) prioritizes pairs with a higher relative cost difference. The `sigmoid(zscore(delta_cost))` term (inspired by Parent 0's weighting scheme) acts as a smooth switch, emphasizing pairs whose cost difference is significantly above the batch mean. Multiplying them (`rank_weight * sigmoid_weight`) ensures the loss is strongest for pairs that are both relatively important (high rank) and statistically significant (high z-score).", "pseudocode": "1. Calculate the cost difference: delta_cost = cost(b) - cost(a).\n2. Calculate the log-probability difference: delta_logp = logp(a) - logp(b).\n3. (Inherited from Parent 0) Normalize the cost difference using z-scoring: normalized_delta_cost = zscore(delta_cost).\n4. (New Coupling 1) Compute a saturated, batch-adaptive margin. Apply a `tanh` function to the normalized_delta_cost and scale it by `gamma`: margin = gamma * tanh(normalized_delta_cost). This is inherited from Parent 1's saturation idea but coupled with Parent 0's z-scoring.\n5. Compute the core logistic loss using the saturated margin: core_loss = softplus(margin - delta_logp).\n6. (New Coupling 2) Compute a combined adaptive weight. First, calculate the rank_gap of the raw delta_cost to get a `rank_weight`. Second, apply a `sigmoid` to the normalized_delta_cost to get a `sigmoid_weight`. The final adaptive weight is their product: `adaptive_weight = rank_weight * sigmoid_weight`.\n7. The final loss is the product of the adaptive weight and the core loss, averaged over the batch: mean(adaptive_weight * core_loss).", "hyperparams": {"gamma": 1.5, "eps": 1e-08}, "operators_used": ["softplus", "tanh", "sigmoid", "zscore", "rank_gap"], "implementation_hint": {"expects": ["cost_a", "cost_b", "logp_a", "logp_b"], "returns": "scalar"}, "code": "import torch\nimport torch.nn.functional as F\n\ndef generated_loss(batch, model_output, extra):\n    \"\"\"\n    Saturated Rank-Weighted Margin Loss with Z-Score Modulation.\n\n    This loss combines a tanh-saturated margin based on z-scored costs with a dual\n    weighting scheme using both rank and sigmoid modulation.\n\n    - Inherits `tanh` saturation from Parent 1, but applies it to z-scored costs.\n    - Inherits `zscore` normalization and `sigmoid` modulation from Parent 0.\n    - New Coupling 1: The margin `gamma * tanh(zscore(delta_cost))` is adaptive\n      to batch statistics and robust to outliers.\n    - New Coupling 2: The final loss is weighted by the product of `rank_gap(delta_cost)`\n      and `sigmoid(zscore(delta_cost))`, focusing learning on pairs that are both\n      relatively and statistically significant.\n    \"\"\"\n    # Read hyperparameters from the 'extra' dictionary\n    gamma = extra.get('gamma', 1.5)\n    eps = extra.get('eps', 1e-8)\n\n    # Unpack tensors from the batch\n    cost_w = batch['cost_a']\n    cost_l = batch['cost_b']\n    logp_w = batch['log_prob_w']\n    logp_l = batch['log_prob_l']\n\n    # Sanity check: cost_w should be less than or equal to cost_l\n    assert torch.all(cost_w <= cost_l), \"Assumption failed: cost_a (winner) must be <= cost_b (loser)\"\n\n    # 1. Calculate cost and log-probability differences\n    delta_cost = cost_l - cost_w\n    delta_logp = logp_w - logp_l\n\n    # 2. (Inherited from Parent 0) Normalize delta_cost using z-scoring\n    if delta_cost.numel() > 1:\n        mean_delta_cost = delta_cost.mean()\n        std_delta_cost = delta_cost.std() + eps\n        normalized_delta_cost = (delta_cost - mean_delta_cost) / std_delta_cost\n    else:\n        normalized_delta_cost = torch.zeros_like(delta_cost)\n\n    # 3. (New Coupling 1) Compute a saturated, batch-adaptive margin\n    # Inherits tanh from Parent 1, but applies it to z-scored costs from Parent 0.\n    margin = gamma * torch.tanh(normalized_delta_cost)\n\n    # 4. Compute the core logistic loss (using softplus for stability)\n    core_loss = F.softplus(margin - delta_logp)\n\n    # 5. (New Coupling 2) Compute a combined adaptive weight\n    # 5a. Rank-based weight (inspired by Parent 1)\n    if delta_cost.numel() > 1:\n        ranks = delta_cost.argsort().argsort().float()\n        rank_weight = ranks / (delta_cost.numel() - 1)\n    else:\n        rank_weight = torch.ones_like(delta_cost)\n\n    # 5b. Sigmoid-based weight (inspired by Parent 0)\n    sigmoid_weight = torch.sigmoid(normalized_delta_cost)\n\n    # 5c. Combine weights\n    adaptive_weight = rank_weight * sigmoid_weight\n\n    # 6. Apply the combined weight to the core loss\n    instance_loss = adaptive_weight * core_loss\n\n    # Apply optional sample weights if provided\n    if 'weight' in batch and batch['weight'] is not None:\n        instance_loss = instance_loss * batch['weight']\n\n    # 7. Return the mean loss over the batch\n    return instance_loss.mean()"}, "static_ok": true, "static_reason": "", "dynamic_ok": true, "dynamic_reason": "ok", "loss_value": 0.1732867956161499, "grad_norm": 0.0}
{"generation": 6, "index": 0, "ir": {"name": "Hybrid Rank-Modulated Saturated Loss", "intuition": "This loss function creates a stable and adaptive learning signal by combining a saturated margin with a hybrid weighting scheme that considers both the rank and the magnitude of cost differences within a batch.\n\nInherited Ideas:\n- From Parent 0 (Adaptive Rank-Weighted Saturated Logistic Loss), it inherits the concept of a saturated margin, specifically `gamma * tanh(zscore(delta_cost))`. This makes the target `delta_logp` robust to the scale of costs and prevents outlier pairs with extremely large cost differences from creating an unbounded, destabilizing margin.\n- From Parent 1 (Rank-Normalized Margin with Sigmoid Cost Modulation), it inherits the idea of using a sigmoid-based weight, `sigmoid(zscore(delta_cost))`, to modulate the loss. This acts as a smooth switch, emphasizing pairs with cost differences significantly above the batch average.\n\nNew Coupling Ideas:\n1. **Hybrid Rank-Sigmoid Weighting**: A new adaptive weight is created by multiplying the rank-based weight from Parent 0 (`rank_gap(delta_cost)`) with the sigmoid-based weight from Parent 1 (`sigmoid(zscore(delta_cost))`). This `hybrid_weight = rank_gap(delta_cost) * sigmoid(zscore(delta_cost))` ensures that the loss focuses on pairs that are both relatively important within the batch's sorted order (high rank) AND have a cost difference that is statistically significant (high z-score). This prevents pairs with a high rank but low absolute significance from dominating, and vice versa.\n2. **Log-Barrier Regularization on Margin**: Instead of a simple `softplus` loss, a log-barrier term, `-log(margin - delta_logp + eps)`, is added when `delta_logp` is less than the margin. This creates a much steeper gradient as `delta_logp` approaches the margin from below, strongly penalizing the model for being close to violating the preference margin. For pairs that already satisfy the margin (`delta_logp >= margin`), the loss is zero. This coupling provides a stronger corrective signal for misclassified pairs compared to the gentler slope of `softplus`.", "pseudocode": "1. Calculate the cost difference: delta_cost = cost(b) - cost(a).\n2. Calculate the log-probability difference: delta_logp = logp(a) - logp(b).\n3. Normalize the cost difference using z-scoring: normalized_delta_cost = zscore(delta_cost).\n4. (Inherited from Parent 0) Compute a saturated margin using tanh on the normalized cost difference: margin = gamma * tanh(normalized_delta_cost).\n5. (New Coupling 1) Compute a hybrid adaptive weight. First, compute a rank-based weight: rank_weight = rank_gap(delta_cost). Second, compute a sigmoid-based weight from Parent 1: sigmoid_weight = sigmoid(normalized_delta_cost). The final weight is their product: hybrid_weight = rank_weight * sigmoid_weight.\n6. (New Coupling 2) Compute the core loss using a log-barrier. For each pair, if delta_logp < margin, the loss is -log(margin - delta_logp + eps). Otherwise, the loss is 0. This is implemented efficiently using `relu`.\n7. The final loss for each pair is the product of the hybrid weight and the core loss: instance_loss = hybrid_weight * core_loss.\n8. The total loss is the average of instance_loss over the batch.", "hyperparams": {"gamma": 1.0, "eps": 1e-08}, "operators_used": ["tanh", "sigmoid", "log", "relu", "zscore", "rank_gap"], "implementation_hint": {"expects": ["cost_a", "cost_b", "logp_a", "logp_b"], "returns": "scalar"}, "code": "import torch\nimport torch.nn.functional as F\n\ndef generated_loss(batch, model_output, extra):\n    \"\"\"\n    Hybrid Rank-Modulated Saturated Loss.\n\n    This loss combines a saturated margin with a hybrid weighting scheme and a log-barrier objective.\n\n    - Inherits a saturated margin `gamma * tanh(zscore(delta_cost))` from Parent 0 for stability.\n    - Inherits a sigmoid-based weight `sigmoid(zscore(delta_cost))` from Parent 1 to focus on significant pairs.\n    - New Coupling 1: Creates a `hybrid_weight` by multiplying the sigmoid weight with a rank-based weight,\n      focusing on pairs that are both high-rank and have a statistically significant cost gap.\n    - New Coupling 2: Uses a log-barrier `-log(margin - delta_logp)` for misclassified pairs, providing a strong\n      corrective gradient that intensifies as the model's prediction approaches the margin.\n    \"\"\"\n    # Read hyperparameters from the 'extra' dictionary\n    gamma = extra.get('gamma', 1.0)\n    eps = extra.get('eps', 1e-8)\n\n    # Unpack tensors from the batch\n    cost_w = batch['cost_a']\n    cost_l = batch['cost_b']\n    logp_w = batch['log_prob_w']\n    logp_l = batch['log_prob_l']\n\n    # Sanity check: cost_w should be less than or equal to cost_l\n    assert torch.all(cost_w <= cost_l), \"Assumption failed: cost_a (winner) must be <= cost_b (loser)\"\n\n    # 1. Calculate cost and log-probability differences\n    delta_cost = cost_l - cost_w\n    delta_logp = logp_w - logp_l\n\n    # 2. Normalize delta_cost using z-scoring for batch-adaptiveness\n    if delta_cost.numel() > 1:\n        mean_delta_cost = delta_cost.mean()\n        std_delta_cost = delta_cost.std() + eps\n        normalized_delta_cost = (delta_cost - mean_delta_cost) / std_delta_cost\n    else:\n        normalized_delta_cost = torch.zeros_like(delta_cost)\n\n    # 3. (Inherited from Parent 0) Compute a saturated margin\n    margin = gamma * torch.tanh(normalized_delta_cost)\n\n    # 4. (New Coupling 1) Compute a hybrid adaptive weight\n    # Combines rank-based importance with sigmoid-based significance.\n    if delta_cost.numel() > 1:\n        ranks = delta_cost.argsort().argsort().float()\n        rank_weight = ranks / (delta_cost.numel() - 1)\n    else:\n        rank_weight = torch.ones_like(delta_cost)\n    \n    sigmoid_weight = torch.sigmoid(normalized_delta_cost) # (Inherited from Parent 1)\n    hybrid_weight = rank_weight * sigmoid_weight\n\n    # 5. (New Coupling 2) Compute the core loss using a log-barrier\n    # This creates a strong penalty for pairs where delta_logp < margin.\n    margin_violation = margin - delta_logp\n    # Use relu to set the loss to 0 for pairs that satisfy the margin (margin_violation <= 0)\n    # The barrier term is -log(x) where x is the positive violation amount.\n    core_loss = -torch.log(F.relu(margin_violation) + eps)\n\n    # 6. Combine weight and core loss\n    instance_loss = hybrid_weight * core_loss\n\n    # Apply optional sample weights if provided\n    if 'weight' in batch and batch['weight'] is not None:\n        instance_loss = instance_loss * batch['weight']\n\n    # 7. Return the mean loss over the batch\n    return instance_loss.mean()"}, "static_ok": true, "static_reason": "", "dynamic_ok": true, "dynamic_reason": "ok", "loss_value": 4.605170249938965, "grad_norm": 0.0}
{"generation": 6, "index": 1, "ir": {"name": "Sigmoid-Weighted Rank-Gapped Margin Loss", "intuition": "This loss function constructs an adaptive learning signal by combining a rank-based margin with a sigmoid-based weighting scheme, both derived from batch-normalized cost differences. The goal is to create a loss that is robust to the scale and distribution of costs, while dynamically focusing learning on pairs that are both significant in terms of cost difference and where the model's preference is incorrect.\n\nInherited Ideas:\n- From Parent 0 (Adaptive Rank-Weighted Saturated Logistic Loss), it inherits the idea of using a rank-based weight, `rank_gap(delta_cost)`, to modulate the loss. This focuses learning on pairs with a higher relative cost difference within the batch.\n- From Parent 1 (Rank-Normalized Margin with Sigmoid Cost Modulation), it inherits the use of `zscore(delta_cost)` to normalize the cost differences, making the loss adaptive to the batch's statistical properties. It also inherits the idea of using `sigmoid(zscore(delta_cost))` as a weighting factor.\n\nNew Coupling Ideas:\n1.  **Rank-Gapped Margin**: A new margin is constructed by directly coupling the `rank_gap` of the z-scored cost differences with a scaling factor `gamma`. The margin is `gamma * rank_gap(zscore(delta_cost))`. This creates a margin that is determined by a pair's relative standing within the normalized cost distribution of the batch. It is robust to both absolute cost scale (due to z-scoring) and the specific distribution of costs (due to ranking), providing a stable and adaptive preference target.\n2.  **Combined Sigmoid and Rank Weighting**: The final loss is weighted by the product of two adaptive factors: `sigmoid(zscore(delta_cost))` and `rank_gap(delta_cost)`. The sigmoid weight acts as a smooth switch, emphasizing pairs with cost differences above the batch mean. The rank weight provides a linear emphasis based on relative cost difference. Multiplying them couples these two signals, heavily prioritizing pairs that are both statistically significant (high z-score) and have a large relative rank within the batch, while strongly de-emphasizing pairs that are insignificant on either metric.", "pseudocode": "1. Calculate the cost difference: delta_cost = cost_b - cost_a.\n2. Calculate the log-probability difference: delta_logp = logp_a - logp_b.\n3. (Inherited from Parent 1) Normalize the cost difference using z-scoring: normalized_delta_cost = zscore(delta_cost).\n4. (Inherited from Parent 0) Compute a rank-based adaptive weight from the raw delta_cost: rank_weight = rank_gap(delta_cost).\n5. (New Coupling 1) Compute a rank-gapped margin. Calculate the rank_gap of the normalized_delta_cost and scale it by a hyperparameter `gamma`: margin = gamma * rank_gap(normalized_delta_cost).\n6. Compute the core logistic loss using the adaptive margin: core_loss = softplus(margin - delta_logp).\n7. (New Coupling 2) Compute a combined adaptive weight. Multiply the rank_weight by a sigmoid applied to the normalized_delta_cost: combined_weight = rank_weight * sigmoid(normalized_delta_cost).\n8. The final loss for each pair is the product of the combined weight and the core loss: instance_loss = combined_weight * core_loss.\n9. The final loss is the average of instance_loss over the batch.", "hyperparams": {"gamma": 2.5, "eps": 1e-08}, "operators_used": ["softplus", "sigmoid", "zscore", "rank_gap"], "implementation_hint": {"expects": ["cost_a", "cost_b", "logp_a", "logp_b", "weight"], "returns": "scalar"}, "code": "import torch\nimport torch.nn.functional as F\n\ndef generated_loss(batch, model_output, extra):\n    \"\"\"\n    Sigmoid-Weighted Rank-Gapped Margin Loss.\n\n    This loss combines a margin derived from the rank of z-scored costs with a \n    combined weight from both sigmoid(z-scored cost) and the rank of raw costs.\n\n    - Inherits `zscore` from Parent 1 for batch-adaptive normalization.\n    - Inherits `rank_gap` weighting on raw costs from Parent 0.\n    - New Coupling 1: The margin `gamma * rank_gap(zscore(delta_cost))` is robust to\n      both the scale and distribution of costs in the batch.\n    - New Coupling 2: The weight `rank_gap(delta_cost) * sigmoid(zscore(delta_cost))`\n      couples rank-based importance with statistical significance, strongly focusing\n      learning on the most important pairs.\n    \"\"\"\n    # Read hyperparameters from the 'extra' dictionary\n    gamma = extra.get('gamma', 2.5)\n    eps = extra.get('eps', 1e-8)\n\n    # Unpack tensors from the batch\n    cost_w = batch['cost_a']\n    cost_l = batch['cost_b']\n    logp_w = batch['log_prob_w']\n    logp_l = batch['log_prob_l']\n\n    # Sanity check: cost_w should be less than or equal to cost_l\n    assert torch.all(cost_w <= cost_l), \"Assumption failed: cost_a (winner) must be <= cost_b (loser)\"\n\n    # 1. Calculate cost and log-probability differences\n    delta_cost = cost_l - cost_w  # This will be >= 0\n    delta_logp = logp_w - logp_l  # We want this to be positive\n    batch_size = delta_cost.numel()\n\n    # 2. (Inherited from Parent 1) Normalize delta_cost using z-scoring\n    if batch_size > 1:\n        mean_delta_cost = delta_cost.mean()\n        std_delta_cost = delta_cost.std() + eps\n        normalized_delta_cost = (delta_cost - mean_delta_cost) / std_delta_cost\n    else:\n        normalized_delta_cost = torch.zeros_like(delta_cost)\n\n    # 3. (New Coupling 1) Compute a rank-gapped margin from normalized costs\n    if batch_size > 1:\n        norm_ranks = normalized_delta_cost.argsort().argsort().float()\n        norm_rank_gaps = norm_ranks / (batch_size - 1) # [0, 1]\n    else:\n        norm_rank_gaps = torch.ones_like(normalized_delta_cost)\n    margin = gamma * norm_rank_gaps\n\n    # 4. Compute the core logistic loss\n    core_loss = F.softplus(margin - delta_logp)\n\n    # 5. (New Coupling 2) Compute a combined adaptive weight\n    # (Inherited from Parent 0) Compute rank weight from raw delta_cost\n    if batch_size > 1:\n        raw_ranks = delta_cost.argsort().argsort().float()\n        rank_weight = raw_ranks / (batch_size - 1) # [0, 1]\n    else:\n        rank_weight = torch.ones_like(delta_cost)\n    \n    # (Inherited from Parent 1) Compute sigmoid weight from normalized delta_cost\n    sigmoid_weight = torch.sigmoid(normalized_delta_cost)\n    \n    # Couple the two weights\n    combined_weight = rank_weight * sigmoid_weight\n\n    # 6. Apply the combined weight to the core loss\n    instance_loss = combined_weight * core_loss\n\n    # Apply optional sample weights if provided\n    if 'weight' in batch and batch['weight'] is not None:\n        instance_loss = instance_loss * batch['weight']\n\n    # 7. Return the mean loss over the batch\n    return instance_loss.mean()"}, "static_ok": true, "static_reason": "", "dynamic_ok": true, "dynamic_reason": "ok", "loss_value": 0.47808361053466797, "grad_norm": 0.0}
{"generation": 6, "index": 2, "ir": {"name": "Sigmoid-Weighted Rank-Saturated Logistic Loss", "intuition": "This loss function constructs a highly adaptive learning signal by combining rank-based saturation with sigmoid weighting. The goal is to create a loss that is robust to the distribution of costs within a batch, while dynamically focusing learning on pairs where the cost difference is statistically significant.\n\nInherited Ideas:\n- From Parent 0 (Adaptive Rank-Weighted Saturated Logistic Loss), it inherits the idea of creating a saturated margin using `tanh`. Specifically, it uses `tanh` on a normalized value to create a bounded margin, preventing outlier pairs from creating an unbounded learning target and enhancing stability.\n- From Parent 1 (Rank-Normalized Margin with Sigmoid Cost Modulation), it inherits the use of `sigmoid(zscore(delta_cost))` as an adaptive weight. This smoothly modulates the loss, focusing learning on pairs with a cost difference that is significantly above the batch mean, while down-weighting less informative pairs.\n\nNew Coupling Ideas:\n1. **Rank-Saturated Margin**: A new margin is constructed by coupling the rank-based normalization (`rank_gap`) with the saturation function (`tanh`). The margin is `gamma * tanh(beta * rank_gap(delta_cost))`. This makes the margin dependent on the relative ordering of cost differences, which is robust to distributional skew, and then saturates this rank-based signal. The `beta` hyperparameter controls the steepness of this saturation, allowing fine-tuning of how quickly the margin increases with rank. This is different from Parent 0, which saturates the z-scored cost, and Parent 1, which uses the rank-gap linearly.\n2. **Log-Prob Normalization**: The log-probability difference `delta_logp` is normalized using z-scoring before being used in the loss calculation: `normalized_delta_logp = zscore(delta_logp)`. This coupling stabilizes training by preventing the `delta_logp` term from growing uncontrollably large, which can lead to vanishing gradients in the `softplus` function. It standardizes the model's output distribution within the batch, ensuring the margin and model evidence are on a comparable scale.", "pseudocode": "1. Calculate the cost difference: delta_cost = cost(b) - cost(a).\n2. Calculate the log-probability difference: delta_logp = logp(a) - logp(b).\n3. (Inherited from Parent 1) Normalize the cost difference using z-scoring: normalized_delta_cost = zscore(delta_cost).\n4. (Inherited from Parent 1) Compute a sigmoid-based adaptive weight from the normalized cost difference: adaptive_weight = sigmoid(normalized_delta_cost).\n5. (New Coupling 1) Compute a rank-saturated margin. First, compute the rank_gap of the raw delta_cost. Then, apply a scaled tanh function: margin = gamma * tanh(beta * rank_gap(delta_cost)). This creates a margin based on relative cost importance that is also bounded and stable.\n6. (New Coupling 2) Normalize the log-probability difference using z-scoring for stability: normalized_delta_logp = zscore(delta_logp).\n7. Compute the core logistic loss using the saturated margin and normalized log-probabilities: core_loss = softplus(margin - normalized_delta_logp).\n8. The final loss for each pair is the product of the adaptive weight and the core loss: instance_loss = adaptive_weight * core_loss.\n9. The final loss is the average of instance_loss over the batch.", "hyperparams": {"gamma": 1.5, "beta": 4.0, "eps": 1e-08}, "operators_used": ["softplus", "sigmoid", "tanh", "zscore", "rank_gap"], "implementation_hint": {"expects": ["cost_a", "cost_b", "logp_a", "logp_b"], "returns": "scalar"}, "code": "import torch\nimport torch.nn.functional as F\n\ndef generated_loss(batch, model_output, extra):\n    \"\"\"\n    Sigmoid-Weighted Rank-Saturated Logistic Loss.\n\n    This loss combines a margin saturated by rank with sigmoid-based weighting\n    and z-score normalization of log-probabilities for stability.\n\n    - Inherits `tanh` saturation from Parent 0, but applies it to rank.\n    - Inherits `sigmoid(zscore(delta_cost))` weighting from Parent 1.\n    - New Coupling 1: `gamma * tanh(beta * rank_gap(delta_cost))` margin is robust\n      to cost distribution and stable against outliers.\n    - New Coupling 2: Normalizes `delta_logp` with z-scoring to prevent extreme values\n      and stabilize gradients.\n    \"\"\"\n    # Read hyperparameters\n    gamma = extra.get('gamma', 1.5)\n    beta = extra.get('beta', 4.0)\n    eps = extra.get('eps', 1e-8)\n\n    # Unpack tensors from the batch\n    cost_w = batch['cost_a']\n    cost_l = batch['cost_b']\n    logp_w = batch['log_prob_w']\n    logp_l = batch['log_prob_l']\n\n    # Sanity check: cost_w should be less than or equal to cost_l\n    assert torch.all(cost_w <= cost_l), \"Assumption failed: cost_a (winner) must be <= cost_b (loser)\"\n\n    delta_cost = cost_l - cost_w\n    delta_logp = logp_w - logp_l\n\n    batch_size = delta_cost.numel()\n    if batch_size <= 1:\n        return torch.tensor(0.0, device=delta_cost.device, requires_grad=True)\n\n    # (Inherited from Parent 1) Compute adaptive weight\n    mean_delta_cost = delta_cost.mean()\n    std_delta_cost = delta_cost.std() + eps\n    normalized_delta_cost = (delta_cost - mean_delta_cost) / std_delta_cost\n    adaptive_weight = torch.sigmoid(normalized_delta_cost)\n\n    # (New Coupling 1) Compute rank-saturated margin\n    # The idea of saturation is inspired by Parent 0's use of tanh.\n    ranks = delta_cost.argsort().argsort().float()\n    rank_gaps = ranks / (batch_size - 1) # Normalized to [0, 1]\n    margin = gamma * torch.tanh(beta * rank_gaps)\n\n    # (New Coupling 2) Normalize delta_logp for stability\n    mean_delta_logp = delta_logp.mean()\n    std_delta_logp = delta_logp.std() + eps\n    normalized_delta_logp = (delta_logp - mean_delta_logp) / std_delta_logp\n\n    # Compute the core logistic loss\n    core_loss = F.softplus(margin - normalized_delta_logp)\n\n    # Combine weight and core loss\n    instance_loss = adaptive_weight * core_loss\n\n    # Apply optional sample weights if provided\n    if 'weight' in batch and batch['weight'] is not None:\n        instance_loss = instance_loss * batch['weight']\n\n    # Return the mean loss over the batch\n    return instance_loss.mean()"}, "static_ok": true, "static_reason": "", "dynamic_ok": true, "dynamic_reason": "ok", "loss_value": 0.7441070675849915, "grad_norm": 0.0}
{"generation": 6, "index": 3, "ir": {"name": "Sigmoid-Gated Rank-Weighted Loss with Saturated Margin", "intuition": "This loss function aims for a stable and adaptive learning signal by carefully combining rank-based weighting, batch-level normalization, and smooth gating. It focuses learning on pairs where the model's preference is misaligned with the cost-based ground truth, while being robust to the scale and distribution of costs and log-probabilities within a batch.\n\nInherited Ideas:\n- From Parent 0 (Adaptive Rank-Weighted Saturated Logistic Loss), it inherits the idea of using a rank-based weight, `rank_gap(delta_cost)`. This ensures that pairs with a larger cost difference relative to others in the batch contribute more significantly to the gradient, focusing the model's attention on the most important preference signals.\n- From Parent 1 (Rank-Normalized Margin with Sigmoid Cost Modulation), it inherits the idea of using `sigmoid(zscore(delta_cost))` as an adaptive, smooth weighting or gating mechanism. This focuses learning on pairs with cost differences that are significantly above the batch mean.\n\nNew Coupling Ideas:\n1. **Saturated Margin with Log-Prob Normalization**: A new margin is constructed by coupling `tanh` saturation with z-score normalization of the *log-probability difference*, `delta_logp`. The margin is `gamma * tanh(zscore(delta_logp))`. This is a significant departure from typical preference losses that base the margin on `delta_cost`. By basing the margin on the model's current confidence (`delta_logp`), the loss becomes adaptive to the model's own output distribution. When the model is very confident (large `delta_logp`), the `tanh` saturates, preventing excessively large gradients. When the model is uncertain (small `delta_logp`), the margin is small, providing a gentle learning signal. This stabilizes training, especially when log-probabilities fluctuate wildly.\n2. **Gated Rank-Weighted Loss**: The final loss couples the rank-based weight from Parent 0 with the sigmoid-based gate from Parent 1. The loss for each instance is `sigmoid(zscore(delta_cost)) * rank_weight * softplus(...)`. This creates a two-stage filter: the `rank_weight` up-weights pairs with a large relative cost gap, and the `sigmoid` gate smoothly down-weights or 'turns off' pairs whose cost gap is not statistically significant within the batch. This dual-factor weighting ensures that the model only receives strong signals from pairs that are both relatively and statistically important, enhancing stability and focus.", "pseudocode": "1. Calculate the cost difference: delta_cost = cost(b) - cost(a).\n2. Calculate the log-probability difference: delta_logp = logp(a) - logp(b).\n3. Normalize the cost difference using z-scoring: normalized_delta_cost = zscore(delta_cost).\n4. Normalize the log-probability difference using z-scoring: normalized_delta_logp = zscore(delta_logp).\n5. (Inherited from Parent 0) Compute a rank-based adaptive weight from the raw delta_cost: rank_weight = rank_gap(delta_cost).\n6. (Inherited from Parent 1) Compute a sigmoid-based adaptive gate from the normalized cost difference: adaptive_gate = sigmoid(normalized_delta_cost).\n7. (New Coupling 1) Compute a saturated, adaptive margin based on the normalized *log-probability* difference: margin = gamma * tanh(normalized_delta_logp).\n8. Compute the core logistic loss. The loss is a function of the log-probability difference itself, regularized by the adaptive margin: core_loss = softplus(margin - delta_logp).\n9. (New Coupling 2) The final loss for each pair is the product of the adaptive gate, the rank weight, and the core loss: instance_loss = adaptive_gate * rank_weight * core_loss.\n10. The final loss is the average of instance_loss over the batch.", "hyperparams": {"gamma": 1.0, "eps": 1e-08}, "operators_used": ["softplus", "sigmoid", "tanh", "zscore", "rank_gap"], "implementation_hint": {"expects": ["cost_a", "cost_b", "logp_a", "logp_b"], "returns": "scalar"}, "code": "import torch\nimport torch.nn.functional as F\n\ndef generated_loss(batch, model_output, extra):\n    \"\"\"\n    Sigmoid-Gated Rank-Weighted Loss with Saturated Margin.\n\n    This loss combines rank-based weighting (Parent 0) with sigmoid gating (Parent 1)\n    and introduces a novel margin based on the z-scored log-probability difference.\n\n    - Inherits `rank_gap(delta_cost)` weighting from Parent 0 to focus on large cost gaps.\n    - Inherits `sigmoid(zscore(delta_cost))` from Parent 1 as a smooth gate.\n    - New Coupling 1: Creates a margin `gamma * tanh(zscore(delta_logp))` that adapts to the\n      model's own output distribution, stabilizing gradients.\n    - New Coupling 2: Couples the rank weight and sigmoid gate (`gate * rank_weight * loss`)\n      to create a two-stage filter for focusing on the most important pairs.\n    \"\"\"\n    # Read hyperparameters\n    gamma = extra.get('gamma', 1.0)\n    eps = extra.get('eps', 1e-8)\n\n    # Unpack tensors from the batch\n    cost_w = batch['cost_a']\n    cost_l = batch['cost_b']\n    logp_w = batch['log_prob_w']\n    logp_l = batch['log_prob_l']\n\n    # Sanity check\n    assert torch.all(cost_w <= cost_l), \"Assumption failed: cost_a (winner) must be <= cost_b (loser)\"\n\n    delta_cost = cost_l - cost_w\n    delta_logp = logp_w - logp_l\n\n    batch_size = delta_cost.numel()\n    if batch_size <= 1:\n        return torch.tensor(0.0, device=delta_cost.device)\n\n    # Normalize delta_cost and delta_logp using z-scoring\n    mean_cost = delta_cost.mean()\n    std_cost = delta_cost.std() + eps\n    normalized_delta_cost = (delta_cost - mean_cost) / std_cost\n\n    mean_logp = delta_logp.mean()\n    std_logp = delta_logp.std() + eps\n    normalized_delta_logp = (delta_logp - mean_logp) / std_logp\n\n    # (Inherited from Parent 0) Compute rank-based weight from raw delta_cost\n    ranks = delta_cost.argsort().argsort().float()\n    rank_weight = ranks / (batch_size - 1)  # Maps to [0, 1]\n\n    # (Inherited from Parent 1) Compute sigmoid-based adaptive gate\n    adaptive_gate = torch.sigmoid(normalized_delta_cost)\n\n    # (New Coupling 1) Compute a saturated, adaptive margin from normalized log-probability\n    margin = gamma * torch.tanh(normalized_delta_logp)\n\n    # Compute the core logistic loss\n    core_loss = F.softplus(margin - delta_logp)\n\n    # (New Coupling 2) Combine the gate, rank weight, and core loss\n    instance_loss = adaptive_gate * rank_weight * core_loss\n\n    # Apply optional sample weights if provided\n    if 'weight' in batch and batch['weight'] is not None:\n        instance_loss = instance_loss * batch['weight']\n\n    return instance_loss.mean()"}, "static_ok": true, "static_reason": "", "dynamic_ok": true, "dynamic_reason": "ok", "loss_value": 0.1732867956161499, "grad_norm": 0.0}
{"generation": 6, "index": 4, "ir": {"name": "Sigmoid-Gated Rank-Weighted Logistic Loss", "intuition": "This loss function creates an adaptive learning signal by combining rank-based weighting with a sigmoid-based gating mechanism. The goal is to focus learning on pairs where the model's preference is misaligned with the cost-based ground truth, while gracefully de-prioritizing pairs with very small cost differences.\n\nInherited Ideas:\n- From Parent 0 (Adaptive Rank-Weighted Saturated Logistic Loss), it inherits the idea of using `rank_gap(delta_cost)` to create an adaptive weight. This `rank_weight` ensures that pairs with a higher relative cost difference within the batch contribute more significantly to the gradient, focusing on the most important preference signals.\n- From Parent 1 (Rank-Normalized Margin with Sigmoid Cost Modulation), it inherits the use of `zscore(delta_cost)` to normalize the cost difference, making the loss adaptive to the batch's statistical properties (mean and standard deviation) and robust to the absolute scale of costs.\n\nNew Coupling Ideas:\n1.  **Rank-Weighted Margin**: A new margin is constructed by directly coupling the `rank_gap` with a scaling factor `gamma`. The margin is `gamma * rank_gap(delta_cost)`. This creates a margin that is determined by the relative importance of a pair within the batch, as defined by the sorted order of cost differences. It's robust to the distribution of costs because ranking is invariant to monotonic transformations.\n2.  **Sigmoid Gating on Z-Scored Costs**: The final loss is weighted by `sigmoid(zscore(delta_cost))`. This acts as a smooth, adaptive gate. For pairs with a cost difference significantly above the batch mean, the weight approaches 1, allowing the rank-weighted loss to pass through. For pairs with a cost difference near or below the mean, the weight smoothly decreases towards 0. This coupling focuses learning on pairs that are 'surprisingly' far apart in cost given the batch context, while gracefully down-weighting less significant or potentially noisy pairs, preventing them from contributing much to the loss.", "pseudocode": "1. Calculate the cost difference: delta_cost = cost(b) - cost(a).\n2. Calculate the log-probability difference: delta_logp = logp(a) - logp(b).\n3. (Inherited from Parent 1) Normalize the cost difference using z-scoring: normalized_delta_cost = zscore(delta_cost).\n4. (Inherited from Parent 0) Compute a rank-based adaptive weight from the raw delta_cost: rank_weight = rank_gap(delta_cost).\n5. (New Coupling 1) Compute a rank-weighted margin. Scale the rank_weight by a hyperparameter `gamma`: margin = gamma * rank_weight.\n6. Compute the core logistic loss using the adaptive margin: core_loss = softplus(margin - delta_logp).\n7. (New Coupling 2) Compute a sigmoid-based gating weight. Apply a sigmoid function to the normalized_delta_cost: gating_weight = sigmoid(normalized_delta_cost).\n8. The final loss for each pair is the product of the gating weight and the core loss: instance_loss = gating_weight * core_loss.\n9. The final loss is the average of instance_loss over the batch.", "hyperparams": {"gamma": 2.0, "eps": 1e-08}, "operators_used": ["softplus", "sigmoid", "zscore", "rank_gap"], "implementation_hint": {"expects": ["cost_a", "cost_b", "log_prob_w", "log_prob_l", "weight"], "returns": "scalar"}, "code": "import torch\nimport torch.nn.functional as F\n\ndef generated_loss(batch, model_output, extra):\n    \"\"\"\n    Sigmoid-Gated Rank-Weighted Logistic Loss.\n\n    This loss combines a rank-weighted margin with a sigmoid gate on z-scored costs.\n    The goal is to focus on pairs with significant cost differences relative to the batch.\n\n    - Inherits `rank_gap` from Parent 0 to determine a pair's relative importance.\n    - Inherits `zscore` normalization from Parent 1 for batch adaptiveness.\n    - New Coupling 1: The margin is `gamma * rank_gap(delta_cost)`, making it directly\n      proportional to a pair's cost-difference rank in the batch.\n    - New Coupling 2: A `sigmoid(zscore(delta_cost))` gate weights the final loss, smoothly\n      focusing learning on pairs with cost differences above the batch mean.\n    \"\"\"\n    # Read hyperparameters from the 'extra' dictionary\n    gamma = extra.get('gamma', 2.0)\n    eps = extra.get('eps', 1e-8)\n\n    # Unpack tensors from the batch\n    cost_w = batch['cost_a']\n    cost_l = batch['cost_b']\n    logp_w = batch['log_prob_w']\n    logp_l = batch['log_prob_l']\n\n    # Sanity check: cost_w should be less than or equal to cost_l\n    assert torch.all(cost_w <= cost_l), \"Assumption failed: cost_a (winner) must be <= cost_b (loser)\"\n\n    # 1. Calculate cost and log-probability differences\n    delta_cost = cost_l - cost_w  # This will be >= 0\n    delta_logp = logp_w - logp_l  # We want this to be positive\n\n    # 2. (Inherited from Parent 1) Normalize delta_cost using z-scoring\n    if delta_cost.numel() > 1:\n        mean_delta_cost = delta_cost.mean()\n        std_delta_cost = delta_cost.std() + eps\n        normalized_delta_cost = (delta_cost - mean_delta_cost) / std_delta_cost\n    else:\n        normalized_delta_cost = torch.zeros_like(delta_cost)\n\n    # 3. (Inherited from Parent 0) Compute rank-based adaptive weight from raw delta_cost\n    if delta_cost.numel() > 1:\n        ranks = delta_cost.argsort().argsort().float()\n        rank_weight = ranks / (delta_cost.numel() - 1)  # Maps to [0, 1]\n    else:\n        rank_weight = torch.ones_like(delta_cost)\n\n    # 4. (New Coupling 1) Compute a rank-weighted margin\n    # The margin is directly proportional to the rank of the cost difference.\n    margin = gamma * rank_weight\n\n    # 5. Compute the core logistic loss using the adaptive margin\n    # The loss is low when delta_logp > margin.\n    core_loss = F.softplus(margin - delta_logp)\n\n    # 6. (New Coupling 2) Compute sigmoid-based gating weight\n    # This gate smoothly emphasizes pairs with above-average cost differences.\n    gating_weight = torch.sigmoid(normalized_delta_cost)\n\n    # 7. Combine gate and core loss\n    instance_loss = gating_weight * core_loss\n\n    # Apply optional sample weights if provided\n    if 'weight' in batch and batch['weight'] is not None:\n        instance_loss = instance_loss * batch['weight']\n\n    # 8. Return the mean loss over the batch\n    return instance_loss.mean()"}, "static_ok": true, "static_reason": "", "dynamic_ok": true, "dynamic_reason": "ok", "loss_value": 0.6750055551528931, "grad_norm": 0.0}
{"generation": 6, "index": 5, "ir": {"name": "Saturated Rank-Weighted Logistic Loss", "intuition": "This loss function creates an adaptive learning signal by weighting the loss based on the relative importance of a preference pair within a batch, while using a saturated margin to prevent outlier pairs from dominating the gradient.\n\nInherited Ideas:\n- From Parent 0, it inherits the use of a `rank_gap` weight. This `rank_weight = rank_gap(delta_cost)` ensures that pairs with a larger cost difference, relative to others in the batch, contribute more significantly to the loss. This focuses the model's learning on the most clearly distinct preferences.\n- From Parent 1, it inherits the core structure of a margin-based logistic loss using `softplus`, specifically `softplus(margin - delta_logp)`. This is a standard and numerically stable way to enforce the preference that `logp(a)` should be greater than `logp(b)` by at least a certain margin.\n\nNew Coupling Ideas:\n1. **Saturated Z-Score Margin**: A new margin is constructed by coupling `zscore` normalization with a `tanh` saturation function. The margin is `gamma * tanh(zscore(delta_cost))`. This makes the target margin adaptive to the batch's statistical properties (mean and standard deviation) while also being bounded. The `tanh` function prevents pairs with extremely large cost differences (outliers) from creating an unbounded margin, which could lead to gradient explosion and instability.\n2. **Log-Probability Difference Normalization**: The log-probability difference, `delta_logp`, is normalized using z-scoring before being used in the loss calculation. This `normalized_delta_logp = zscore(delta_logp)` stabilizes training by preventing the scale of the model's log-probabilities from drastically affecting the loss magnitude. It ensures the learning signal is based on the relative differences in log-probabilities within the batch, not their absolute scale, making the optimization process more robust to variations in model confidence.", "pseudocode": "1. Calculate the cost difference: delta_cost = cost(b) - cost(a).\n2. Calculate the log-probability difference: delta_logp = logp(a) - logp(b).\n3. (Inherited from Parent 0) Compute a rank-based adaptive weight from the raw delta_cost: rank_weight = rank_gap(delta_cost).\n4. Normalize the cost difference using z-scoring: normalized_delta_cost = zscore(delta_cost).\n5. (New Coupling 1) Compute a saturated, z-score-normalized margin. Apply a `tanh` function to the normalized_delta_cost and scale it by a hyperparameter `gamma`: margin = gamma * tanh(normalized_delta_cost).\n6. (New Coupling 2) Normalize the log-probability difference using z-scoring: normalized_delta_logp = zscore(delta_logp).\n7. (Inherited from Parent 1) Compute the core logistic loss using the saturated margin and the normalized log-probability difference: core_loss = softplus(margin - normalized_delta_logp).\n8. The final loss for each pair is the product of the rank weight and the core loss: instance_loss = rank_weight * core_loss.\n9. The final loss is the average of instance_loss over the batch.", "hyperparams": {"gamma": 1.0, "eps": 1e-08}, "operators_used": ["softplus", "tanh", "zscore", "rank_gap"], "implementation_hint": {"expects": ["cost_a", "cost_b", "logp_a", "logp_b"], "returns": "scalar"}, "code": "import torch\nimport torch.nn.functional as F\n\ndef generated_loss(batch, model_output, extra):\n    \"\"\"\n    Saturated Rank-Weighted Logistic Loss.\n\n    This loss combines rank-based weighting of the loss with a margin that is\n    both normalized by batch statistics (z-score) and saturated (tanh).\n\n    - Inherits `rank_gap` weighting from Parent 0 to focus on important pairs.\n    - Inherits the `softplus` logistic loss structure from Parent 1.\n    - New Coupling 1: Creates a `tanh(zscore(delta_cost))` margin, which is robust\n      to both the scale and distribution of costs, and is stable against outliers.\n    - New Coupling 2: Normalizes `delta_logp` via z-scoring to make the loss robust\n      to the absolute scale of model log-probabilities.\n    \"\"\"\n    # Read hyperparameters from the 'extra' dictionary\n    gamma = extra.get('gamma', 1.0)\n    eps = extra.get('eps', 1e-8)\n\n    # Unpack tensors from the batch\n    cost_w = batch['cost_a']\n    cost_l = batch['cost_b']\n    logp_w = batch['log_prob_w']\n    logp_l = batch['log_prob_l']\n\n    # Sanity check: cost_w should be less than or equal to cost_l\n    assert torch.all(cost_w <= cost_l), \"Assumption failed: cost_a (winner) must be <= cost_b (loser)\"\n\n    # 1. Calculate cost and log-probability differences\n    delta_cost = cost_l - cost_w  # This will be >= 0\n    delta_logp = logp_w - logp_l  # We want this to be positive\n\n    # 2. (Inherited from Parent 0) Compute rank-based adaptive weight\n    if delta_cost.numel() > 1:\n        ranks = delta_cost.argsort().argsort().float()\n        rank_weight = ranks / (delta_cost.numel() - 1)  # Maps to [0, 1]\n    else:\n        rank_weight = torch.ones_like(delta_cost)\n\n    # 3. Z-score normalize delta_cost for the margin calculation\n    if delta_cost.numel() > 1:\n        mean_delta_cost = delta_cost.mean()\n        std_delta_cost = delta_cost.std() + eps\n        normalized_delta_cost = (delta_cost - mean_delta_cost) / std_delta_cost\n    else:\n        normalized_delta_cost = torch.zeros_like(delta_cost)\n\n    # 4. (New Coupling 1) Compute a saturated, z-score-normalized margin\n    margin = gamma * torch.tanh(normalized_delta_cost)\n\n    # 5. (New Coupling 2) Z-score normalize delta_logp for stability\n    if delta_logp.numel() > 1:\n        mean_delta_logp = delta_logp.mean()\n        std_delta_logp = delta_logp.std() + eps\n        normalized_delta_logp = (delta_logp - mean_delta_logp) / std_delta_logp\n    else:\n        normalized_delta_logp = torch.zeros_like(delta_logp)\n\n    # 6. (Inherited from Parent 1) Compute the core logistic loss\n    core_loss = F.softplus(margin - normalized_delta_logp)\n\n    # 7. Combine rank weight and core loss\n    instance_loss = rank_weight * core_loss\n\n    # Apply optional sample weights if provided\n    if 'weight' in batch and batch['weight'] is not None:\n        instance_loss = instance_loss * batch['weight']\n\n    # 8. Return the mean loss over the batch\n    return instance_loss.mean()"}, "static_ok": true, "static_reason": "", "dynamic_ok": true, "dynamic_reason": "ok", "loss_value": 0.3465735912322998, "grad_norm": 0.0}
{"generation": 6, "index": 6, "ir": {"name": "Saturated Rank-Weighted Logistic Loss", "intuition": "This loss function creates an adaptive learning signal where both the target margin and the loss weighting are determined by the relative rank of the cost difference within a batch, with a saturation mechanism to prevent extreme log-probability differences from dominating the gradient.\n\nInherited Ideas:\n- From Parent 0 (Adaptive Rank-Weighted Saturated Logistic Loss), it inherits the idea of using `rank_gap(delta_cost)` to create an adaptive weight. This focuses learning on pairs with a larger cost difference relative to others in the batch.\n- From Parent 1 (Rank-Normalized Margin with Sigmoid Cost Modulation), it inherits the core structure of using `softplus` for a stable, margin-based logistic loss: `softplus(margin - delta_logp)`.\n\nNew Coupling Ideas:\n1. **Rank-Based Margin**: A new margin is constructed directly from the rank of the cost differences: `margin = beta * rank_gap(delta_cost)`. Unlike parents that use z-scoring, this margin is determined purely by the ordinal relationship of cost gaps in the batch. This makes the target `delta_logp` robust to the distribution and scale of `delta_cost`, depending only on its relative ranking.\n2. **Saturated Log-Probability Difference**: The `delta_logp` term in the loss is saturated using `tanh`: `saturated_delta_logp = gamma * tanh(delta_logp / gamma)`. This coupling prevents pairs where the model is already extremely confident (very large `delta_logp`) from generating excessively large negative loss values (and thus, near-zero gradients). It effectively caps the 'reward' for being correct, ensuring that the model continues to learn from pairs where the margin is not yet met, rather than being overly influenced by already well-separated pairs. This improves stability, especially when `delta_logp` might become very large.", "pseudocode": "1. Calculate the cost difference: delta_cost = cost(b) - cost(a).\n2. Calculate the log-probability difference: delta_logp = logp(a) - logp(b).\n3. (Inherited from Parent 0) Compute a rank-based adaptive weight from the cost difference: rank_weight = rank_gap(delta_cost).\n4. (New Coupling 1) Compute a rank-based margin. This is calculated similarly to the weight but scaled by a hyperparameter `beta`: margin = beta * rank_gap(delta_cost).\n5. (New Coupling 2) Compute a saturated version of the log-probability difference using `tanh` to bound its value and improve stability: saturated_delta_logp = gamma * tanh(delta_logp / gamma).\n6. (Inherited from Parent 1) Compute the core logistic loss using the rank-based margin and the saturated log-probability difference: core_loss = softplus(margin - saturated_delta_logp).\n7. The final loss for each pair is the product of the rank weight and the core loss: instance_loss = rank_weight * core_loss.\n8. The final loss is the average of instance_loss over the batch.", "hyperparams": {"beta": 1.5, "gamma": 2.0, "eps": 1e-08}, "operators_used": ["softplus", "tanh", "rank_gap"], "implementation_hint": {"expects": ["cost_a", "cost_b", "logp_a", "logp_b"], "returns": "scalar"}, "code": "import torch\nimport torch.nn.functional as F\n\ndef generated_loss(batch, model_output, extra):\n    \"\"\"\n    Saturated Rank-Weighted Logistic Loss.\n\n    This loss combines a margin and a weight, both derived from the rank of cost\n    differences, with a tanh saturation on the log-probability difference for stability.\n\n    - Inherits `rank_gap` weighting from Parent 0 to focus on important pairs.\n    - Inherits the `softplus` logistic loss structure from Parent 1.\n    - New Coupling 1: The margin `beta * rank_gap(delta_cost)` is determined purely by the\n      ordinal ranking of cost differences, making it robust to cost scale and distribution.\n    - New Coupling 2: The log-probability difference is saturated `gamma * tanh(delta_logp / gamma)`,\n      preventing extremely confident predictions from causing near-zero gradients.\n    \"\"\"\n    # Read hyperparameters from the 'extra' dictionary\n    beta = extra.get('beta', 1.5)\n    gamma = extra.get('gamma', 2.0)\n    eps = extra.get('eps', 1e-8)\n\n    # Unpack tensors from the batch\n    cost_w = batch['cost_a']\n    cost_l = batch['cost_b']\n    logp_w = batch['log_prob_w']\n    logp_l = batch['log_prob_l']\n\n    # Sanity check: cost_w should be less than or equal to cost_l\n    assert torch.all(cost_w <= cost_l), \"Assumption failed: cost_a (winner) must be <= cost_b (loser)\"\n\n    # 1. Calculate cost and log-probability differences\n    delta_cost = cost_l - cost_w  # This will be >= 0\n    delta_logp = logp_w - logp_l  # We want this to be positive\n\n    # 2. (Inherited from Parent 0 & New Coupling 1) Compute rank-based weight and margin\n    if delta_cost.numel() > 1:\n        ranks = delta_cost.argsort().argsort().float()\n        # rank_gap maps ranks to a [0, 1] range\n        rank_gaps = ranks / (delta_cost.numel() - 1)\n    else:\n        rank_gaps = torch.ones_like(delta_cost)\n    \n    rank_weight = rank_gaps\n    margin = beta * rank_gaps\n\n    # 3. (New Coupling 2) Compute saturated log-probability difference\n    # This prevents very large delta_logp from dominating and causing vanishing gradients.\n    saturated_delta_logp = gamma * torch.tanh(delta_logp / (gamma + eps))\n\n    # 4. (Inherited from Parent 1) Compute the core logistic loss\n    core_loss = F.softplus(margin - saturated_delta_logp)\n\n    # 5. Combine weight and core loss\n    instance_loss = rank_weight * core_loss\n\n    # Apply optional sample weights if provided\n    if 'weight' in batch and batch['weight'] is not None:\n        instance_loss = instance_loss * batch['weight']\n\n    # 6. Return the mean loss over the batch\n    return instance_loss.mean()"}, "static_ok": true, "static_reason": "", "dynamic_ok": true, "dynamic_reason": "ok", "loss_value": 0.6754394173622131, "grad_norm": 0.0}
{"generation": 6, "index": 7, "ir": {"name": "Sigmoid-Weighted Saturated Rank-Gap Loss", "intuition": "This loss function constructs a highly adaptive learning signal by combining rank-based margins with sigmoid-based weighting. The goal is to create a loss that is robust to the distribution of cost differences within a batch, focuses learning on pairs where the cost gap is statistically significant, and remains stable against extreme outlier costs.\n\nInherited Ideas:\n- From Parent 0 (Adaptive Rank-Weighted Saturated Logistic Loss), it inherits the idea of a saturated margin using `tanh`. Specifically, it uses `tanh` on a normalized cost difference to prevent the margin from becoming unbounded, which improves stability.\n- From Parent 1 (Rank-Normalized Margin with Sigmoid Cost Modulation), it inherits the use of `sigmoid(zscore(delta_cost))` as an adaptive weight. This weight smoothly emphasizes pairs with a cost difference that is significantly above the batch mean, focusing gradients on the most informative examples.\n\nNew Coupling Ideas:\n1.  **Saturated Rank-Gap Margin**: A new margin is created by first calculating the rank-gap of the cost differences (`rank_gap(delta_cost)`), which maps them to a [0, 1] range based on their relative order. This rank-gap is then passed through a `tanh` function and scaled by `gamma`. The resulting margin, `gamma * tanh(rank_gap(delta_cost))`, is robust to the distribution of costs (due to ranking) and is also saturated, preventing a single pair with a large rank from creating an excessively large margin.\n2.  **Log-Sigmoid with Annealed Temperature**: The core loss is formulated as `-logsigmoid(temperature * (delta_logp - margin))`. Instead of `softplus`, this uses `logsigmoid`, which is a common and stable choice for logistic-style losses. A new `temperature` hyperparameter is introduced, which can be annealed over time (e.g., using `extra`). This allows for controlling the sharpness of the loss function during training. A higher temperature makes the model more sensitive to small deviations from the margin, which can be useful later in training, while a lower temperature provides smoother gradients initially.", "pseudocode": "1. Calculate the cost difference: delta_cost = cost(b) - cost(a).\n2. Calculate the log-probability difference: delta_logp = logp(a) - logp(b).\n3. (Inherited from Parent 1) Normalize the cost difference using z-scoring: normalized_delta_cost = zscore(delta_cost).\n4. (Inherited from Parent 1) Compute a sigmoid-based adaptive weight from the normalized cost difference: adaptive_weight = sigmoid(normalized_delta_cost).\n5. (New Coupling 1) Compute a saturated, rank-based margin. First, find the rank-gap of the raw delta_cost. Then, apply a `tanh` function (idea from Parent 0) to this rank-gap and scale by `gamma`: margin = gamma * tanh(rank_gap(delta_cost)).\n6. (New Coupling 2) Compute the core loss using `logsigmoid` with an annealed temperature. The loss for an instance is `-logsigmoid(temperature * (delta_logp - margin))`. This encourages `delta_logp` to be greater than the margin.\n7. The final loss for each pair is the product of the adaptive weight and the core loss: instance_loss = adaptive_weight * core_loss.\n8. The final loss is the average of instance_loss over the batch.", "hyperparams": {"gamma": 1.5, "temperature": 1.0, "eps": 1e-08}, "operators_used": ["logsigmoid", "sigmoid", "tanh", "zscore", "rank_gap"], "implementation_hint": {"expects": ["cost_a", "cost_b", "logp_a", "logp_b"], "returns": "scalar"}, "code": "import torch\nimport torch.nn.functional as F\n\ndef generated_loss(batch, model_output, extra):\n    \"\"\"\n    Sigmoid-Weighted Saturated Rank-Gap Loss.\n\n    This loss combines a saturated rank-gap margin with sigmoid-based adaptive weighting\n    and an annealed temperature for the core logsigmoid loss.\n\n    - Inherits `tanh` saturation from Parent 0 for a stable margin.\n    - Inherits `sigmoid(zscore(delta_cost))` weighting from Parent 1 to focus on\n      statistically significant pairs.\n    - New Coupling 1: Creates a `gamma * tanh(rank_gap(delta_cost))` margin, which is\n      robust to cost distribution and bounded.\n    - New Coupling 2: Uses a `-logsigmoid` loss with a `temperature` parameter, allowing\n      for annealing the sharpness of the loss function during training.\n    \"\"\"\n    # Read hyperparameters from 'extra', with defaults\n    gamma = extra.get('gamma', 1.5)\n    temperature = extra.get('temperature', 1.0)\n    eps = extra.get('eps', 1e-8)\n\n    # Unpack tensors from the batch\n    cost_w = batch['cost_a']\n    cost_l = batch['cost_b']\n    logp_w = batch['log_prob_w']\n    logp_l = batch['log_prob_l']\n\n    # Sanity check: cost_w should be less than or equal to cost_l\n    assert torch.all(cost_w <= cost_l), \"Assumption failed: cost_a (winner) must be <= cost_b (loser)\"\n\n    # 1. Calculate cost and log-probability differences\n    delta_cost = cost_l - cost_w  # This will be >= 0\n    delta_logp = logp_w - logp_l  # We want this to be positive\n\n    # 2. (Inherited from Parent 1) Compute z-scored cost and adaptive weight\n    if delta_cost.numel() > 1:\n        mean_delta_cost = delta_cost.mean()\n        std_delta_cost = delta_cost.std() + eps\n        normalized_delta_cost = (delta_cost - mean_delta_cost) / std_delta_cost\n    else:\n        normalized_delta_cost = torch.zeros_like(delta_cost)\n    adaptive_weight = torch.sigmoid(normalized_delta_cost)\n\n    # 3. (New Coupling 1) Compute saturated, rank-based margin\n    if delta_cost.numel() > 1:\n        # rank_gap implementation\n        ranks = delta_cost.argsort().argsort().float()\n        rank_gaps = ranks / (delta_cost.numel() - 1)  # Maps to [0, 1]\n    else:\n        rank_gaps = torch.ones_like(delta_cost)\n    # Inherits tanh saturation from Parent 0, applied to rank_gaps\n    margin = gamma * torch.tanh(rank_gaps)\n\n    # 4. (New Coupling 2) Compute core loss with annealed temperature\n    # This is equivalent to softplus(-(temperature * (delta_logp - margin)))\n    core_loss = -F.logsigmoid(temperature * (delta_logp - margin))\n\n    # 5. Combine weight and core loss\n    instance_loss = adaptive_weight * core_loss\n\n    # Apply optional sample weights if provided\n    if 'weight' in batch and batch['weight'] is not None:\n        instance_loss = instance_loss * batch['weight']\n\n    # 6. Return the mean loss over the batch\n    return instance_loss.mean()"}, "static_ok": true, "static_reason": "", "dynamic_ok": true, "dynamic_reason": "ok", "loss_value": 0.5407459735870361, "grad_norm": 0.0}
{"generation": 7, "index": 0, "ir": {"name": "Rank-Gated Saturated Logistic Loss", "intuition": "This loss function creates a stable and adaptive learning signal by combining a saturated margin with a rank-based gating mechanism. The goal is to produce a loss that is robust to variations in the scale of costs, while focusing learning on pairs with a significant cost difference relative to others in the batch.\n\nInherited Ideas:\n- From Parent 1 (Adaptive Rank-Weighted Saturated Logistic Loss), it inherits the idea of a saturated, z-score-normalized margin: `margin = gamma * tanh(zscore(delta_cost))`. This creates a margin that adapts to the batch's statistical properties and is robust to extreme outlier costs, preventing them from dominating the learning signal.\n- From Parent 0 (Sigmoid-Gated Rank-Weighted Logistic Loss), it inherits the idea of using `rank_gap(delta_cost)` to modulate the loss. However, instead of using it as a direct weight or part of the margin, it's used to form a smooth gating function.\n\nNew Coupling Ideas:\n1.  **Rank-Based Gating**: A new gating mechanism is introduced by coupling the `rank_gap` with a `sigmoid` function. The gate is computed as `sigmoid(alpha * (rank_gap(delta_cost) - beta))`. This creates a smooth, step-like function that activates the loss primarily for pairs whose cost-difference rank is above a certain threshold (`beta`). The `alpha` parameter controls the steepness of this activation, allowing for a soft or hard transition. This focuses learning on the top fraction of most important pairs in the batch, as determined by their rank, while gracefully down-weighting less significant pairs.\n2.  **Log-Probability Difference Clipping**: A stability trick is introduced by clipping the log-probability difference `delta_logp` within a symmetric range `[-clip_value, clip_value]`. This prevents extremely confident but incorrect model predictions from generating excessively large gradients, which can destabilize training. It helps to regularize the model's confidence and ensures a more stable learning process, especially early in training or when encountering noisy data.", "pseudocode": "1. Calculate the cost difference: delta_cost = cost(b) - cost(a).\n2. Calculate the log-probability difference: delta_logp = logp(a) - logp(b).\n3. (New Coupling 2) Apply clipping for stability: clipped_delta_logp = clamp(delta_logp, min=-clip_value, max=clip_value).\n4. (Inherited from Parent 1) Normalize the cost difference using z-scoring: normalized_delta_cost = zscore(delta_cost).\n5. (Inherited from Parent 1) Compute a saturated, z-score-normalized margin: margin = gamma * tanh(normalized_delta_cost).\n6. (Inherited from Parent 0) Compute the rank of the cost difference, normalized to [0, 1]: rank = rank_gap(delta_cost).\n7. (New Coupling 1) Compute a rank-based gating weight. This gate activates for pairs with a rank above a soft threshold `beta`: gating_weight = sigmoid(alpha * (rank - beta)).\n8. Compute the core logistic loss using the saturated margin and clipped log-probability difference: core_loss = softplus(margin - clipped_delta_logp).\n9. The final loss for each pair is the product of the gating weight and the core loss: instance_loss = gating_weight * core_loss.\n10. The final loss is the average of instance_loss over the batch.", "hyperparams": {"gamma": 1.5, "alpha": 10.0, "beta": 0.5, "clip_value": 5.0, "eps": 1e-08}, "operators_used": ["softplus", "tanh", "sigmoid", "zscore", "rank_gap", "clamp"], "implementation_hint": {"expects": ["cost_a", "cost_b", "logp_a", "logp_b"], "returns": "scalar"}, "code": "import torch\nimport torch.nn.functional as F\n\ndef generated_loss(batch, model_output, extra):\n    \"\"\"\n    Rank-Gated Saturated Logistic Loss.\n\n    This loss combines a saturated margin (robust to cost scale) with a rank-based\n    gating mechanism to focus learning on the most significant pairs in a batch.\n\n    - Inherits `tanh(zscore(delta_cost))` margin from Parent 1 for a stable, adaptive margin.\n    - Inherits the use of `rank_gap` from Parent 0 to determine relative importance.\n    - New Coupling 1: A rank-based gate `sigmoid(alpha * (rank_gap - beta))` smoothly\n      activates the loss for pairs with a cost-difference rank above a threshold `beta`.\n    - New Coupling 2: Clips `delta_logp` to prevent extreme gradients and improve stability.\n    \"\"\"\n    # Read hyperparameters from the 'extra' dictionary\n    gamma = extra.get('gamma', 1.5)\n    alpha = extra.get('alpha', 10.0)\n    beta = extra.get('beta', 0.5)\n    clip_value = extra.get('clip_value', 5.0)\n    eps = extra.get('eps', 1e-8)\n\n    # Unpack tensors from the batch\n    cost_w = batch['cost_a']\n    cost_l = batch['cost_b']\n    logp_w = batch['log_prob_w']\n    logp_l = batch['log_prob_l']\n\n    # Sanity check: cost_w should be less than or equal to cost_l\n    assert torch.all(cost_w <= cost_l), \"Assumption failed: cost_a (winner) must be <= cost_b (loser)\"\n\n    # 1. Calculate cost and log-probability differences\n    delta_cost = cost_l - cost_w\n    delta_logp = logp_w - logp_l\n\n    # 2. (New Coupling 2) Clip delta_logp for stability\n    clipped_delta_logp = torch.clamp(delta_logp, min=-clip_value, max=clip_value)\n\n    # 3. (Inherited from Parent 1) Normalize delta_cost using z-scoring\n    if delta_cost.numel() > 1:\n        mean_delta_cost = delta_cost.mean()\n        std_delta_cost = delta_cost.std() + eps\n        normalized_delta_cost = (delta_cost - mean_delta_cost) / std_delta_cost\n    else:\n        normalized_delta_cost = torch.zeros_like(delta_cost)\n\n    # 4. (Inherited from Parent 1) Compute a saturated, z-score-normalized margin\n    margin = gamma * torch.tanh(normalized_delta_cost)\n\n    # 5. (Inherited from Parent 0) Compute rank-based adaptive weight from raw delta_cost\n    if delta_cost.numel() > 1:\n        ranks = delta_cost.argsort().argsort().float()\n        rank_norm = ranks / (delta_cost.numel() - 1)  # Maps to [0, 1]\n    else:\n        rank_norm = torch.ones_like(delta_cost)\n\n    # 6. (New Coupling 1) Compute a rank-based gating weight\n    # This gate smoothly emphasizes pairs with rank > beta.\n    gating_weight = torch.sigmoid(alpha * (rank_norm - beta))\n\n    # 7. Compute the core logistic loss with the adaptive margin and clipped logp\n    core_loss = F.softplus(margin - clipped_delta_logp)\n\n    # 8. Combine gate and core loss\n    instance_loss = gating_weight * core_loss\n\n    # Apply optional sample weights if provided\n    if 'weight' in batch and batch['weight'] is not None:\n        instance_loss = instance_loss * batch['weight']\n\n    # 9. Return the mean loss over the batch\n    return instance_loss.mean()"}, "static_ok": true, "static_reason": "", "dynamic_ok": true, "dynamic_reason": "ok", "loss_value": 0.3465736210346222, "grad_norm": 0.0}
{"generation": 7, "index": 1, "ir": {"name": "Saturated Rank-Gated Logistic Loss", "intuition": "This loss function creates a highly adaptive and stable learning signal by gating the standard logistic loss with a saturated, rank-based weight. The objective is to focus learning on pairs with a meaningful cost difference relative to others in the batch, while gracefully ignoring pairs with negligible or identical cost differences and preventing extreme outliers from dominating the gradient.\n\nInherited Ideas:\n- From Parent 0 (Sigmoid-Gated Rank-Weighted Logistic Loss), it inherits the idea of using a gating mechanism to modulate the loss. Instead of a sigmoid gate on z-scored costs, this child uses a rank-based gate, but the principle of selectively weighting the loss based on `delta_cost` is preserved.\n- From Parent 1 (Adaptive Rank-Weighted Saturated Logistic Loss), it inherits the use of `tanh` to create a saturated, bounded signal. Here, `tanh` is applied to the rank-based weight rather than a z-scored margin, but the goal of preventing extreme values and ensuring stability is the same.\n\nNew Coupling Ideas:\n1.  **Saturated Rank Gating**: The core logistic loss is multiplied by `tanh(beta * rank_gap(delta_cost))`. This creates a smooth gate that is 0 for the pair with the smallest cost difference (if `beta` is large) and smoothly increases towards 1 for pairs with larger cost differences. The `tanh` function ensures the gate is bounded, preventing pairs with extremely high ranks from having an outsized influence compared to other high-ranked pairs. This focuses the model's capacity on learning meaningful preferences.\n2.  **Log-Probability Difference Clipping**: The log-probability difference, `delta_logp`, is clamped to a maximum value `clip_val`. This is a stability trick that prevents the loss from becoming excessively small (approaching zero) when the model is overly confident (`delta_logp` is very large). By clipping `delta_logp`, we ensure that even correctly classified pairs contribute a minimal, non-zero gradient, preventing the model's log-probabilities from drifting to infinity and improving overall training stability.", "pseudocode": "1. Calculate the cost difference: delta_cost = cost(b) - cost(a).\n2. Calculate the log-probability difference: delta_logp = logp(a) - logp(b).\n3. (Inherited from Parent 1) Compute a rank-based value from the raw delta_cost: rank_value = rank_gap(delta_cost).\n4. (New Coupling 1) Create a saturated rank-based gate. Apply a scaled `tanh` function to the rank_value: gating_weight = tanh(beta * rank_value). This smoothly gates the loss, focusing on pairs with higher relative cost differences.\n5. (New Coupling 2) Clip the log-probability difference for stability. Clamp delta_logp to a maximum value: clipped_delta_logp = clamp(delta_logp, max=clip_val).\n6. Compute the core logistic loss using the clipped log-probability difference. The loss is based on the Bradley-Terry model: core_loss = softplus(-clipped_delta_logp).\n7. The final loss for each pair is the product of the gating weight and the core loss: instance_loss = gating_weight * core_loss.\n8. The final loss is the average of instance_loss over the batch.", "hyperparams": {"beta": 4.0, "clip_val": 5.0, "eps": 1e-08}, "operators_used": ["softplus", "tanh", "rank_gap", "clamp"], "implementation_hint": {"expects": ["cost_a", "cost_b", "logp_a", "logp_b"], "returns": "scalar"}, "code": "import torch\nimport torch.nn.functional as F\n\ndef generated_loss(batch, model_output, extra):\n    \"\"\"\n    Saturated Rank-Gated Logistic Loss.\n\n    This loss applies a smooth, saturated gate based on cost ranks to the standard\n    logistic loss. It also clips the log-prob difference for stability.\n\n    - Inherits gating from Parent 0, but uses a rank-based gate instead of sigmoid(z-score).\n    - Inherits saturation (tanh) from Parent 1, but applies it to the rank gate for boundedness.\n    - New Coupling 1: A `tanh(beta * rank_gap(delta_cost))` gate smoothly focuses learning\n      on pairs with higher relative cost differences, while saturating for top-ranked pairs.\n    - New Coupling 2: Clips `delta_logp` to a max value, preventing loss from vanishing\n      for over-confident predictions and improving stability.\n    \"\"\"\n    # Read hyperparameters from the 'extra' dictionary\n    beta = extra.get('beta', 4.0)\n    clip_val = extra.get('clip_val', 5.0)\n    eps = extra.get('eps', 1e-8)\n\n    # Unpack tensors from the batch\n    cost_w = batch['cost_a']\n    cost_l = batch['cost_b']\n    logp_w = batch['log_prob_w']\n    logp_l = batch['log_prob_l']\n\n    # Sanity check: cost_w should be less than or equal to cost_l\n    assert torch.all(cost_w <= cost_l), \"Assumption failed: cost_a (winner) must be <= cost_b (loser)\"\n\n    # 1. Calculate cost and log-probability differences\n    delta_cost = cost_l - cost_w  # This will be >= 0\n    delta_logp = logp_w - logp_l  # We want this to be positive\n\n    # 2. (Inherited from Parent 1) Compute rank-based value\n    # rank_gap maps ranks to a [0, 1] range\n    if delta_cost.numel() > 1:\n        ranks = delta_cost.argsort().argsort().float()\n        rank_value = ranks / (delta_cost.numel() - 1)\n    else:\n        rank_value = torch.ones_like(delta_cost)\n\n    # 3. (New Coupling 1) Create a saturated rank-based gate\n    # The gate is 0 for the lowest rank and smoothly approaches 1 for higher ranks.\n    # tanh provides smooth saturation, preventing extreme weights.\n    gating_weight = torch.tanh(beta * rank_value)\n\n    # 4. (New Coupling 2) Clip delta_logp for stability\n    # Prevents loss from vanishing with over-confident predictions.\n    clipped_delta_logp = torch.clamp(delta_logp, max=clip_val)\n\n    # 5. Compute the core logistic loss (equivalent to -log_sigmoid)\n    # We use the clipped delta_logp here.\n    core_loss = F.softplus(-clipped_delta_logp)\n\n    # 6. Apply the gate to the core loss\n    instance_loss = gating_weight * core_loss\n\n    # Apply optional sample weights if provided\n    if 'weight' in batch and batch['weight'] is not None:\n        instance_loss = instance_loss * batch['weight']\n\n    # 7. Return the mean loss over the batch\n    return instance_loss.mean()"}, "static_ok": true, "static_reason": "", "dynamic_ok": true, "dynamic_reason": "ok", "loss_value": 0.5579563975334167, "grad_norm": 0.0}
{"generation": 7, "index": 2, "ir": {"name": "Sigmoid-Gated Log-Prob Saturated Logistic Loss", "intuition": "This loss function creates an adaptive learning signal by gating the loss based on the model's current confidence and using a saturated margin that is robust to cost outliers. The goal is to focus learning on pairs where the model is uncertain or wrong, while preventing extreme cost differences from dominating the training signal.\n\nInherited Ideas:\n- From Parent 0 (Sigmoid-Gated Rank-Weighted Logistic Loss), it inherits the idea of using a `sigmoid` gate to modulate the loss. However, instead of gating on cost, it gates on the log-probability difference, focusing on model uncertainty.\n- From Parent 1 (Adaptive Rank-Weighted Saturated Logistic Loss), it inherits the use of a saturated margin, `tanh(zscore(delta_cost))`. This makes the target log-probability difference robust to the scale of costs within a batch and stable against extreme outlier pairs, preventing them from creating an unbounded learning target.\n\nNew Coupling Ideas:\n1. **Uncertainty-Based Sigmoid Gating**: A new gating mechanism is introduced by applying a sigmoid function to the negative log-probability difference, `sigmoid(-delta_logp)`. This gate has a value close to 1 when the model is uncertain (`delta_logp` is near 0) or incorrect (`delta_logp` is negative), and it smoothly approaches 0 as the model becomes confident and correct (`delta_logp` becomes large and positive). This coupling dynamically focuses training effort on pairs the model struggles with, rather than on pairs with large but potentially easy-to-learn cost differences.\n2. **Log-Transformed Margin**: The saturated margin from Parent 1 is further transformed using a `softplus` function, `softplus(gamma * tanh(zscore(delta_cost)))`. This ensures the margin is strictly positive and non-linear. Using a log-space-like transformation (softplus is a smooth approximation of relu) on the margin creates a different learning dynamic, where the required `delta_logp` to achieve low loss grows more gently for small cost differences and more rapidly for larger ones, compared to a linear margin. This coupling preserves the stability of the tanh-zscore component while altering the target separation between log-probabilities.", "pseudocode": "1. Calculate the cost difference: delta_cost = cost(b) - cost(a).\n2. Calculate the log-probability difference: delta_logp = logp(a) - logp(b).\n3. (Inherited from Parent 1) Normalize the cost difference using z-scoring: normalized_delta_cost = zscore(delta_cost).\n4. (Inherited from Parent 1 & New Coupling 2) Compute a saturated and log-transformed margin. Apply `tanh` to the normalized cost, scale by `gamma`, and then apply `softplus`: margin = softplus(gamma * tanh(normalized_delta_cost)).\n5. Compute the core logistic loss component: core_loss = softplus(margin - delta_logp).\n6. (Inherited from Parent 0 & New Coupling 1) Compute an uncertainty-based gating weight. Apply a sigmoid to the negative log-probability difference: gating_weight = sigmoid(-delta_logp).\n7. The final loss for each pair is the product of the gating weight and the core loss: instance_loss = gating_weight * core_loss.\n8. The final loss is the average of instance_loss over the batch.", "hyperparams": {"gamma": 2.0, "eps": 1e-08}, "operators_used": ["softplus", "sigmoid", "tanh", "zscore"], "implementation_hint": {"expects": ["cost_a", "cost_b", "logp_a", "logp_b"], "returns": "scalar"}, "code": "import torch\nimport torch.nn.functional as F\n\ndef generated_loss(batch, model_output, extra):\n    \"\"\"\n    Sigmoid-Gated Log-Prob Saturated Logistic Loss.\n\n    This loss combines an uncertainty-based sigmoid gate with a saturated, log-transformed margin.\n    The goal is to focus learning on uncertain pairs while maintaining robustness to cost outliers.\n\n    - Inherits `sigmoid` gating from Parent 0, but applies it to `-delta_logp`.\n    - Inherits `tanh(zscore(delta_cost))` from Parent 1 for a stable, adaptive margin.\n    - New Coupling 1: An uncertainty gate `sigmoid(-delta_logp)` focuses training on pairs\n      where the model is not confident (delta_logp is small or negative).\n    - New Coupling 2: A log-transformed margin `softplus(gamma * tanh(zscore(delta_cost)))`\n      ensures a strictly positive margin with non-linear scaling.\n    \"\"\"\n    # Read hyperparameters from the 'extra' dictionary\n    gamma = extra.get('gamma', 2.0)\n    eps = extra.get('eps', 1e-8)\n\n    # Unpack tensors from the batch\n    cost_w = batch['cost_a']\n    cost_l = batch['cost_b']\n    logp_w = batch['log_prob_w']\n    logp_l = batch['log_prob_l']\n\n    # Sanity check: cost_w should be less than or equal to cost_l\n    assert torch.all(cost_w <= cost_l), \"Assumption failed: cost_a (winner) must be <= cost_b (loser)\"\n\n    # 1. Calculate cost and log-probability differences\n    delta_cost = cost_l - cost_w\n    delta_logp = logp_w - logp_l\n\n    # 2. (Inherited from Parent 1) Normalize delta_cost using z-scoring\n    if delta_cost.numel() > 1:\n        mean_delta_cost = delta_cost.mean()\n        std_delta_cost = delta_cost.std() + eps\n        normalized_delta_cost = (delta_cost - mean_delta_cost) / std_delta_cost\n    else:\n        normalized_delta_cost = torch.zeros_like(delta_cost)\n\n    # 3. (Inherited from Parent 1 & New Coupling 2) Compute the log-transformed saturated margin\n    # The margin is robust to cost scale (zscore), stable against outliers (tanh), and strictly positive (softplus).\n    saturated_margin_base = gamma * torch.tanh(normalized_delta_cost)\n    margin = F.softplus(saturated_margin_base)\n\n    # 4. Compute the core logistic loss component\n    core_loss = F.softplus(margin - delta_logp)\n\n    # 5. (Inherited from Parent 0 & New Coupling 1) Compute uncertainty-based gating weight\n    # This gate is high when the model is uncertain or wrong (delta_logp is small or negative).\n    with torch.no_grad():\n        # Use no_grad for the gate to ensure gradients flow only through the core loss term,\n        # preventing the model from trivially minimizing the loss by making delta_logp large.\n        gating_weight = torch.sigmoid(-delta_logp)\n\n    # 6. Combine the gate and core loss\n    instance_loss = gating_weight * core_loss\n\n    # Apply optional sample weights if provided\n    if 'weight' in batch and batch['weight'] is not None:\n        instance_loss = instance_loss * batch['weight']\n\n    # 7. Return the mean loss over the batch\n    return instance_loss.mean()"}, "static_ok": true, "static_reason": "", "dynamic_ok": true, "dynamic_reason": "ok", "loss_value": 0.5493061542510986, "grad_norm": 0.0}
{"generation": 7, "index": 3, "ir": {"name": "Saturated Sigmoid-Gated Logistic Loss with Rank-Adaptive Margin", "intuition": "This loss function creates a highly adaptive and stable learning signal by combining rank-based margin construction with a saturated, sigmoid-based gating mechanism. The goal is to focus learning on pairs where the model's preference is misaligned with the cost-based ground truth, while being robust to the scale and distribution of costs within a batch.\n\nInherited Ideas:\n- From Parent 0 (Sigmoid-Gated Rank-Weighted Logistic Loss), it inherits the idea of using a `sigmoid(zscore(delta_cost))` gate. This gate adaptively weights the loss for each pair based on how its cost difference compares to the batch's mean and standard deviation, smoothly focusing on pairs with above-average cost differences.\n- From Parent 1 (Adaptive Rank-Weighted Saturated Logistic Loss), it inherits the use of `tanh` to create a saturated, bounded signal. This prevents outlier pairs with extreme cost differences from dominating the loss calculation, promoting numerical stability.\n\nNew Coupling Ideas:\n1.  **Rank-Adaptive Margin**: A new margin is constructed by directly using the rank of the cost difference, `gamma * rank_gap(delta_cost)`. This makes the margin robust to the absolute scale and distribution of costs, as it depends only on the relative ordering of `delta_cost` within the batch. The model is thus encouraged to separate log probabilities by an amount proportional to the pair's importance rank.\n2.  **Saturated Gating**: The sigmoid gate from Parent 0 is coupled with the saturation idea from Parent 1. The final gating weight is `sigmoid(beta * tanh(zscore(delta_cost)))`. The `tanh` function first squashes the z-scored costs into a [-1, 1] range, preventing extreme outliers from creating gate weights that are too sharply 0 or 1. The `sigmoid` then maps this bounded value to a (0, 1) weight. This coupling creates a more stable and robust gate that is less sensitive to extreme cost differences while still effectively prioritizing pairs based on batch statistics.", "pseudocode": "1. Calculate the cost difference: delta_cost = cost(b) - cost(a).\n2. Calculate the log-probability difference: delta_logp = logp(a) - logp(b).\n3. (Inherited from Parent 0 & 1) Normalize the cost difference using z-scoring: normalized_delta_cost = zscore(delta_cost).\n4. (New Coupling 1 - Rank-Adaptive Margin) Compute a rank-based margin. Calculate `rank_gap(delta_cost)` and scale it by a hyperparameter `gamma`: margin = gamma * rank_gap(delta_cost).\n5. Compute the core logistic loss using the adaptive margin: core_loss = softplus(margin - delta_logp).\n6. (New Coupling 2 - Saturated Gating) Compute a stabilized gating weight. First, apply `tanh` to the normalized_delta_cost (inherited from Parent 1). Then, scale by `beta` and apply `sigmoid` (inherited from Parent 0): gating_weight = sigmoid(beta * tanh(normalized_delta_cost)).\n7. The final loss for each pair is the product of the gating weight and the core loss: instance_loss = gating_weight * core_loss.\n8. The final loss is the average of instance_loss over the batch.", "hyperparams": {"gamma": 1.0, "beta": 1.0, "eps": 1e-08}, "operators_used": ["softplus", "sigmoid", "tanh", "zscore", "rank_gap"], "implementation_hint": {"expects": ["cost_a", "cost_b", "logp_a", "logp_b"], "returns": "scalar"}, "code": "import torch\nimport torch.nn.functional as F\n\ndef generated_loss(batch, model_output, extra):\n    \"\"\"\n    Saturated Sigmoid-Gated Logistic Loss with Rank-Adaptive Margin.\n\n    This loss combines a rank-based margin with a saturated sigmoid gate for stability.\n\n    - Inherits `sigmoid(zscore(delta_cost))` gating from Parent 0 for adaptiveness.\n    - Inherits `tanh` saturation from Parent 1 for stability against outliers.\n    - New Coupling 1: A rank-adaptive margin `gamma * rank_gap(delta_cost)` that is\n      robust to cost scale and distribution.\n    - New Coupling 2: A saturated gate `sigmoid(beta * tanh(zscore(delta_cost)))` which\n      combines adaptiveness with stability, preventing extreme z-scores from causing\n      abrupt gating.\n    \"\"\"\n    # Read hyperparameters from the 'extra' dictionary\n    gamma = extra.get('gamma', 1.0)\n    beta = extra.get('beta', 1.0)\n    eps = extra.get('eps', 1e-8)\n\n    # Unpack tensors from the batch\n    cost_w = batch['cost_a']\n    cost_l = batch['cost_b']\n    logp_w = batch['log_prob_w']\n    logp_l = batch['log_prob_l']\n\n    # Sanity check: cost_w should be less than or equal to cost_l\n    assert torch.all(cost_w <= cost_l), \"Assumption failed: cost_a (winner) must be <= cost_b (loser)\"\n\n    # 1. Calculate cost and log-probability differences\n    delta_cost = cost_l - cost_w  # This will be >= 0\n    delta_logp = logp_w - logp_l  # We want this to be positive\n\n    batch_size = delta_cost.numel()\n\n    # 2. (Inherited from Parents) Normalize delta_cost using z-scoring\n    if batch_size > 1:\n        mean_delta_cost = delta_cost.mean()\n        std_delta_cost = delta_cost.std() + eps\n        normalized_delta_cost = (delta_cost - mean_delta_cost) / std_delta_cost\n    else:\n        normalized_delta_cost = torch.zeros_like(delta_cost)\n\n    # 3. (New Coupling 1) Compute a rank-adaptive margin\n    if batch_size > 1:\n        ranks = delta_cost.argsort().argsort().float()\n        rank_value = ranks / (batch_size - 1)  # Maps to [0, 1]\n    else:\n        rank_value = torch.ones_like(delta_cost)\n    margin = gamma * rank_value\n\n    # 4. Compute the core logistic loss using the adaptive margin\n    core_loss = F.softplus(margin - delta_logp)\n\n    # 5. (New Coupling 2) Compute a saturated gating weight\n    # tanh (from P1) stabilizes z-score before sigmoid (from P0) creates the gate.\n    saturated_z_score = torch.tanh(normalized_delta_cost)\n    gating_weight = torch.sigmoid(beta * saturated_z_score)\n\n    # 6. Combine gate and core loss\n    instance_loss = gating_weight * core_loss\n\n    # Apply optional sample weights if provided\n    if 'weight' in batch and batch['weight'] is not None:\n        instance_loss = instance_loss * batch['weight']\n\n    # 7. Return the mean loss over the batch\n    return instance_loss.mean()"}, "static_ok": true, "static_reason": "", "dynamic_ok": true, "dynamic_reason": "ok", "loss_value": 0.4925554096698761, "grad_norm": 0.0}
{"generation": 7, "index": 4, "ir": {"name": "Saturated Rank-Gated Logistic Loss", "intuition": "This loss function creates a highly adaptive learning signal by gating the standard logistic loss with a saturated, rank-based weight. The goal is to focus learning on pairs with a significant cost difference relative to others in the batch, while preventing extreme cost outliers from dominating the gradient signal.\n\nInherited Ideas:\n- From Parent 0 (Sigmoid-Gated Rank-Weighted Logistic Loss), it inherits the idea of using a gating mechanism to modulate the loss. Instead of a sigmoid gate on z-scored costs, this child uses a saturated rank gate.\n- From Parent 1 (Adaptive Rank-Weighted Saturated Logistic Loss), it inherits the use of `tanh` for saturation. This is applied to a normalized rank signal to create a smooth, bounded gating weight, ensuring stability.\n\nNew Coupling Ideas:\n1. **Saturated Rank Gating**: A new gating weight is created by coupling the `rank_gap` of the cost difference with the `tanh` saturation function. The rank is first normalized to be centered around zero, then passed through `tanh`. The resulting weight, `tanh(2 * rank_gap(delta_cost) - 1)`, smoothly transitions from -1 to +1. Applying `softplus` to this creates a non-negative gate that strongly emphasizes pairs in the upper half of cost ranks while aggressively down-weighting those in the lower half. This focuses learning on the most discriminative pairs in a stable, bounded manner.\n2. **Log-Probability Difference Clipping**: The log-probability difference, `delta_logp`, is clamped to a minimum value of `(-clip_val)`. This stability trick prevents the `softplus` argument from becoming excessively large if the model is extremely confident but wrong (i.e., `logp_a` is very small and `logp_b` is very large), which can lead to numerical instability and exploding gradients. It ensures the loss remains well-behaved even with pathological model outputs.", "pseudocode": "1. Calculate the cost difference: delta_cost = cost(b) - cost(a).\n2. Calculate the log-probability difference: delta_logp = logp(a) - logp(b).\n3. (Inherited from Parents 0 & 1) Compute the rank-based adaptive weight: rank_weight = rank_gap(delta_cost).\n4. (New Coupling 1) Create a saturated rank-based gating weight. Normalize the rank_weight to be in the range [-1, 1], apply a `tanh` function for smooth saturation, and then use `softplus` to make it a non-negative gate: gate = softplus(beta * tanh(2 * rank_weight - 1)). The `beta` hyperparameter controls the steepness of the gate.\n5. (New Coupling 2) For stability, clip the log-probability difference to a minimum value: clipped_delta_logp = clamp(delta_logp, min=-clip_val).\n6. Compute the core logistic loss argument. A constant margin `gamma` is used: loss_arg = gamma - clipped_delta_logp.\n7. Compute the core loss using `softplus`: core_loss = softplus(loss_arg).\n8. The final loss for each pair is the product of the gating weight and the core loss: instance_loss = gate * core_loss.\n9. The final loss is the average of instance_loss over the batch.", "hyperparams": {"gamma": 1.0, "beta": 3.0, "clip_val": 10.0}, "operators_used": ["softplus", "tanh", "clamp", "rank_gap"], "implementation_hint": {"expects": ["cost_a", "cost_b", "logp_a", "logp_b"], "returns": "scalar"}, "code": "import torch\nimport torch.nn.functional as F\n\ndef generated_loss(batch, model_output, extra):\n    \"\"\"\n    Saturated Rank-Gated Logistic Loss.\n\n    This loss combines a standard logistic loss with a gating mechanism derived from\n    the saturated rank of the cost difference within the batch.\n\n    - Inherits the concept of a modulating gate from Parent 0.\n    - Inherits the use of `tanh` for saturation from Parent 1.\n    - New Coupling 1: Creates a `softplus(beta * tanh(2 * rank_gap - 1))` gate. This smoothly\n      and strongly emphasizes pairs with higher cost-difference ranks, providing a stable,\n      bounded weighting scheme that is robust to cost distribution.\n    - New Coupling 2: Clips the `delta_logp` to prevent extremely large negative values\n      from causing numerical instability in the `softplus` calculation.\n    \"\"\"\n    # Read hyperparameters from the 'extra' dictionary\n    gamma = extra.get('gamma', 1.0)\n    beta = extra.get('beta', 3.0)\n    clip_val = extra.get('clip_val', 10.0)\n\n    # Unpack tensors from the batch\n    cost_w = batch['cost_a']\n    cost_l = batch['cost_b']\n    logp_w = batch['log_prob_w']\n    logp_l = batch['log_prob_l']\n\n    # Sanity check: cost_w should be less than or equal to cost_l\n    assert torch.all(cost_w <= cost_l), \"Assumption failed: cost_a (winner) must be <= cost_b (loser)\"\n\n    # 1. Calculate cost and log-probability differences\n    delta_cost = cost_l - cost_w  # This will be >= 0\n    delta_logp = logp_w - logp_l  # We want this to be positive\n\n    # 2. (Inherited) Compute rank-based adaptive weight from raw delta_cost\n    if delta_cost.numel() > 1:\n        # rank_gap: ranks are mapped to [0, 1]\n        ranks = delta_cost.argsort().argsort().float()\n        rank_weight = ranks / (delta_cost.numel() - 1)\n    else:\n        rank_weight = torch.ones_like(delta_cost)\n\n    # 3. (New Coupling 1) Create a saturated, rank-based gating weight\n    # Normalize rank to [-1, 1], apply tanh for smooth saturation, and scale by beta.\n    # softplus then creates a non-negative gate.\n    scaled_rank = 2 * rank_weight - 1\n    gating_arg = beta * torch.tanh(scaled_rank)\n    gate = F.softplus(gating_arg)\n\n    # 4. (New Coupling 2) Clip delta_logp for numerical stability\n    # Prevents `gamma - delta_logp` from becoming excessively large.\n    clipped_delta_logp = torch.clamp(delta_logp, min=-clip_val)\n\n    # 5. Compute the core logistic loss with the clipped logp difference\n    core_loss = F.softplus(gamma - clipped_delta_logp)\n\n    # 6. Apply the gate to the core loss\n    instance_loss = gate * core_loss\n\n    # Apply optional sample weights if provided\n    if 'weight' in batch and batch['weight'] is not None:\n        instance_loss = instance_loss * batch['weight']\n\n    # 7. Return the mean loss over the batch\n    return instance_loss.mean()"}, "static_ok": true, "static_reason": "", "dynamic_ok": true, "dynamic_reason": "ok", "loss_value": 1.2484039068222046, "grad_norm": 0.0}
{"generation": 7, "index": 5, "ir": {"name": "Rank-Modulated Saturated Logistic Loss", "intuition": "This loss function creates a highly adaptive and stable learning signal by modulating a saturated margin with a rank-based weight. The goal is to produce a loss that is robust to the scale of costs, stable against outliers, and focuses learning on pairs where the model's preference is misaligned with the ground truth, especially for pairs with a significant cost difference relative to others in the batch.\n\nInherited Ideas:\n- From Parent 0 (Sigmoid-Gated Rank-Weighted Logistic Loss), it inherits the idea of using `rank_gap(delta_cost)` to measure the relative importance of a preference pair within the batch. This allows the model to prioritize learning from pairs with larger cost differences.\n- From Parent 1 (Adaptive Rank-Weighted Saturated Logistic Loss), it inherits the idea of creating a saturated margin using `tanh(zscore(delta_cost))`. This makes the margin adaptive to the batch's cost statistics and robust to extreme outliers, preventing them from dominating the loss landscape.\n\nNew Coupling Ideas:\n1.  **Rank-Modulated Margin**: A new margin is constructed by directly multiplying the rank-based weight with the saturated, z-scored cost difference: `margin = beta * rank_gap(delta_cost) * tanh(zscore(delta_cost))`. This coupling ensures the margin is largest for pairs that have both a high rank (large relative cost difference) and a high z-score (large absolute cost difference relative to the batch mean). It effectively combines relative and absolute importance into a single, bounded term.\n2.  **Log-Probability Difference Clipping**: The log-probability difference, `delta_logp`, is clamped to a maximum value, `max_logp_diff`. This acts as a stability trick, preventing the model from becoming overconfident. If the model already strongly prefers the winning candidate (large `delta_logp`), this clipping mechanism reduces the gradient, allowing the model to focus its capacity on more uncertain or incorrectly ranked pairs. It effectively puts an upper bound on how 'correct' a pair can be, preventing a few easy pairs from dominating the training signal.", "pseudocode": "1. Calculate the cost difference: delta_cost = cost(b) - cost(a).\n2. Calculate the log-probability difference: delta_logp = logp(a) - logp(b).\n3. (Inherited from Parent 1) Normalize the cost difference using z-scoring: normalized_delta_cost = zscore(delta_cost).\n4. (Inherited from Parent 0) Compute a rank-based weight from the raw delta_cost: rank_weight = rank_gap(delta_cost).\n5. (New Coupling 1) Compute a rank-modulated, saturated margin. Multiply the rank_weight by the tanh of the normalized_delta_cost and scale by a hyperparameter `beta`: margin = beta * rank_weight * tanh(normalized_delta_cost).\n6. (New Coupling 2) Apply clipping to the log-probability difference for stability: clipped_delta_logp = clamp(delta_logp, max=max_logp_diff).\n7. Compute the core logistic loss using the adaptive margin and the clipped log-probability difference: instance_loss = softplus(margin - clipped_delta_logp).\n8. The final loss is the average of instance_loss over the batch.", "hyperparams": {"beta": 2.5, "max_logp_diff": 5.0, "eps": 1e-08}, "operators_used": ["softplus", "tanh", "zscore", "rank_gap", "clamp"], "implementation_hint": {"expects": ["cost_a", "cost_b", "logp_a", "logp_b"], "returns": "scalar"}, "code": "import torch\nimport torch.nn.functional as F\n\ndef generated_loss(batch, model_output, extra):\n    \"\"\"\n    Rank-Modulated Saturated Logistic Loss.\n\n    This loss combines a saturated, z-score normalized margin with a rank-based\n    modulation and adds a clipping mechanism for stability.\n\n    - Inherits `rank_gap` from Parent 0 to weigh by relative importance.\n    - Inherits `tanh(zscore(delta_cost))` from Parent 1 for a stable, adaptive margin.\n    - New Coupling 1: Creates a rank-modulated margin by multiplying the rank weight\n      with the saturated z-scored cost, `beta * rank_gap * tanh(zscore)`.\n    - New Coupling 2: Clips the `delta_logp` to prevent overconfidence on easy pairs,\n      improving training stability.\n    \"\"\"\n    # Read hyperparameters from the 'extra' dictionary\n    beta = extra.get('beta', 2.5)\n    max_logp_diff = extra.get('max_logp_diff', 5.0)\n    eps = extra.get('eps', 1e-8)\n\n    # Unpack tensors from the batch\n    cost_w = batch['cost_a']\n    cost_l = batch['cost_b']\n    logp_w = batch['log_prob_w']\n    logp_l = batch['log_prob_l']\n\n    # Sanity check: cost_w should be less than or equal to cost_l\n    assert torch.all(cost_w <= cost_l), \"Assumption failed: cost_a (winner) must be <= cost_b (loser)\"\n\n    # 1. Calculate cost and log-probability differences\n    delta_cost = cost_l - cost_w\n    delta_logp = logp_w - logp_l\n\n    # 2. (Inherited from Parent 1) Normalize delta_cost using z-scoring\n    if delta_cost.numel() > 1:\n        mean_delta_cost = delta_cost.mean()\n        std_delta_cost = delta_cost.std() + eps\n        normalized_delta_cost = (delta_cost - mean_delta_cost) / std_delta_cost\n    else:\n        normalized_delta_cost = torch.zeros_like(delta_cost)\n\n    # 3. (Inherited from Parent 0) Compute rank-based weight\n    if delta_cost.numel() > 1:\n        ranks = delta_cost.argsort().argsort().float()\n        rank_weight = ranks / (delta_cost.numel() - 1)\n    else:\n        rank_weight = torch.ones_like(delta_cost)\n\n    # 4. (New Coupling 1) Compute a rank-modulated, saturated margin\n    saturated_term = torch.tanh(normalized_delta_cost)\n    margin = beta * rank_weight * saturated_term\n\n    # 5. (New Coupling 2) Clip delta_logp for stability\n    clipped_delta_logp = torch.clamp(delta_logp, max=max_logp_diff)\n\n    # 6. Compute the core logistic loss\n    instance_loss = F.softplus(margin - clipped_delta_logp)\n\n    # Apply optional sample weights if provided\n    if 'weight' in batch and batch['weight'] is not None:\n        instance_loss = instance_loss * batch['weight']\n\n    # 7. Return the mean loss over the batch\n    return instance_loss.mean()"}, "static_ok": true, "static_reason": "", "dynamic_ok": true, "dynamic_reason": "ok", "loss_value": 0.6931471824645996, "grad_norm": 0.0}
{"generation": 7, "index": 6, "ir": {"name": "Rank-Modulated Saturated Logistic Loss", "intuition": "This loss function creates a highly adaptive learning signal by modulating a saturated logistic loss with a rank-based weight. The goal is to produce a loss that is robust to variations in the scale and distribution of costs within a batch, while focusing learning on pairs where the model's preference is misaligned with the cost-based ground truth.\n\nInherited Ideas:\n- From Parent 1 (Adaptive Rank-Weighted Saturated Logistic Loss), it inherits the idea of a saturated, z-score-normalized margin: `margin = gamma * tanh(zscore(delta_cost))`. This creates a margin that is robust to the absolute scale of costs (due to z-scoring) and is stabilized against extreme outliers by the `tanh` saturation.\n- From both parents, it inherits the use of `rank_gap(delta_cost)` to create a `rank_weight`. This ensures that pairs with a higher relative cost difference within the batch contribute more significantly to the gradient, focusing on the most important preference signals.\n\nNew Coupling Ideas:\n1.  **Rank-Modulated Sigmoid Loss**: Instead of using the rank as a simple linear weight on the loss, it is used to modulate the argument of a `logsigmoid` function. The core loss is `-logsigmoid(rank_weight * (delta_logp - margin))`. This couples the rank and the core preference signal in a non-linear way. For high-rank pairs (large cost gaps), the `rank_weight` is close to 1, and the loss behaves like a standard logistic loss. For low-rank pairs, the `rank_weight` scales down the argument, effectively softening the loss boundary and reducing the penalty for small preference misalignments. This provides a smoother gradient profile than simple linear weighting.\n2.  **Adaptive Temperature Scaling**: The z-scored cost difference is used to compute an adaptive temperature `T = exp(-zscore(delta_cost))`. This temperature scales the entire loss term. For pairs with a cost difference significantly above the batch mean (`zscore > 0`), the temperature `T` becomes small (`< 1`), which amplifies the loss and encourages the model to learn these important preferences more strongly. Conversely, for pairs with a below-average cost difference (`zscore < 0`), the temperature `T` becomes large (`> 1`), which dampens the loss and reduces the learning signal, preventing the model from overfitting to noisy or insignificant pairs. This mechanism provides a dynamic, data-driven focus on the most informative pairs in each batch.", "pseudocode": "1. Calculate the cost difference: delta_cost = cost(b) - cost(a).\n2. Calculate the log-probability difference: delta_logp = logp(a) - logp(b).\n3. (Inherited from both) Compute a rank-based weight from the raw delta_cost: rank_weight = rank_gap(delta_cost).\n4. (Inherited from Parent 1) Normalize the cost difference using z-scoring: normalized_delta_cost = zscore(delta_cost).\n5. (Inherited from Parent 1) Compute a saturated, z-score-normalized margin: margin = gamma * tanh(normalized_delta_cost).\n6. (New Coupling 1) Compute the rank-modulated sigmoid loss. The rank_weight modulates the difference between the log-probability gap and the margin inside a logsigmoid function: core_loss = -logsigmoid(rank_weight * (delta_logp - margin)).\n7. (New Coupling 2) Compute an adaptive temperature scale based on the normalized cost difference: temperature = exp(-normalized_delta_cost).\n8. The final loss for each pair is the product of the temperature and the core loss: instance_loss = temperature * core_loss.\n9. The final loss is the average of instance_loss over the batch.", "hyperparams": {"gamma": 1.0, "eps": 1e-08}, "operators_used": ["logsigmoid", "tanh", "zscore", "rank_gap", "exp"], "implementation_hint": {"expects": ["cost_a", "cost_b", "logp_a", "logp_b"], "returns": "scalar"}, "code": "import torch\nimport torch.nn.functional as F\n\ndef generated_loss(batch, model_output, extra):\n    \"\"\"\n    Rank-Modulated Saturated Logistic Loss.\n\n    This loss combines a saturated, z-score normalized margin with a rank-modulated\n    logsigmoid loss, which is then dynamically scaled by an adaptive temperature.\n\n    - Inherits `tanh(zscore(delta_cost))` from Parent 1 to create a stable, adaptive margin.\n    - Inherits `rank_gap` from both parents to determine a pair's relative importance.\n    - New Coupling 1: Uses `rank_weight` to modulate the argument of the logsigmoid loss,\n      creating a non-linear weighting scheme: `-logsigmoid(rank_weight * (delta_logp - margin))`.\n    - New Coupling 2: An adaptive temperature `exp(-zscore(delta_cost))` scales the final loss,\n      amplifying the signal from pairs with high cost differences and dampening it for low ones.\n    \"\"\"\n    # Read hyperparameters from the 'extra' dictionary\n    gamma = extra.get('gamma', 1.0)\n    eps = extra.get('eps', 1e-8)\n\n    # Unpack tensors from the batch\n    cost_w = batch['cost_a']\n    cost_l = batch['cost_b']\n    logp_w = batch['log_prob_w']\n    logp_l = batch['log_prob_l']\n\n    # Sanity check: cost_w should be less than or equal to cost_l\n    assert torch.all(cost_w <= cost_l), \"Assumption failed: cost_a (winner) must be <= cost_b (loser)\"\n\n    # 1. Calculate cost and log-probability differences\n    delta_cost = cost_l - cost_w  # This will be >= 0\n    delta_logp = logp_w - logp_l  # We want this to be positive\n\n    # 2. (Inherited from Parent 1) Normalize delta_cost using z-scoring\n    if delta_cost.numel() > 1:\n        mean_delta_cost = delta_cost.mean()\n        std_delta_cost = delta_cost.std() + eps\n        normalized_delta_cost = (delta_cost - mean_delta_cost) / std_delta_cost\n    else:\n        normalized_delta_cost = torch.zeros_like(delta_cost)\n\n    # 3. (Inherited from both) Compute rank-based weight from raw delta_cost\n    if delta_cost.numel() > 1:\n        ranks = delta_cost.argsort().argsort().float()\n        rank_weight = ranks / (delta_cost.numel() - 1)  # Maps to [0, 1]\n    else:\n        rank_weight = torch.ones_like(delta_cost)\n\n    # 4. (Inherited from Parent 1) Compute a saturated, z-score-normalized margin\n    margin = gamma * torch.tanh(normalized_delta_cost)\n\n    # 5. (New Coupling 1) Compute the rank-modulated sigmoid loss\n    # This is equivalent to log(1 + exp(-rank_weight * (delta_logp - margin)))\n    # but more numerically stable.\n    core_loss = -F.logsigmoid(rank_weight * (delta_logp - margin))\n\n    # 6. (New Coupling 2) Compute an adaptive temperature scale\n    # This amplifies loss for pairs with above-average cost differences.\n    temperature = torch.exp(-normalized_delta_cost)\n\n    # 7. Combine temperature and core loss\n    instance_loss = temperature * core_loss\n\n    # Apply optional sample weights if provided\n    if 'weight' in batch and batch['weight'] is not None:\n        instance_loss = instance_loss * batch['weight']\n\n    # 8. Return the mean loss over the batch\n    return instance_loss.mean()"}, "static_ok": true, "static_reason": "", "dynamic_ok": true, "dynamic_reason": "ok", "loss_value": 0.6931471824645996, "grad_norm": 0.0}
{"generation": 7, "index": 7, "ir": {"name": "Adaptive Sigmoid-Gated Logistic Loss with Rank-Modulated Margin", "intuition": "This loss function creates a highly adaptive learning signal by combining a rank-modulated margin with a sigmoid gate. The goal is to produce a loss that is robust to the scale and distribution of costs within a batch, while dynamically adjusting both the target preference margin and the overall loss weight based on a pair's relative importance.\n\nInherited Ideas:\n- From Parent 0 (Sigmoid-Gated Rank-Weighted Logistic Loss), it inherits the use of `sigmoid(zscore(delta_cost))` as a gating mechanism. This gate smoothly focuses learning on pairs with a cost difference significantly above the batch mean, while de-emphasizing pairs with small or average cost differences.\n- From Parent 1 (Adaptive Rank-Weighted Saturated Logistic Loss), it inherits the use of `rank_gap(delta_cost)` to create an adaptive weight. This rank-based weight ensures that pairs with a higher relative cost difference within the batch are treated as more important.\n\nNew Coupling Ideas:\n1.  **Rank-Modulated Saturated Margin**: A new margin is constructed by coupling the `rank_gap` with the `zscore` of the cost difference. The margin is `gamma * tanh(rank_gap(delta_cost) * zscore(delta_cost))`. This creates a margin that is not only adaptive to the batch's statistical properties (due to `zscore`) but is also scaled by the pair's rank. The `tanh` saturation prevents this combined signal from creating an unbounded margin, ensuring stability. This coupling makes the target preference gap (`delta_logp`) dependent on both the statistical and ordinal importance of the cost difference.\n2.  **Dual-Use Rank Weighting**: The `rank_gap` is used in a second role to directly weight the final gated loss. The loss is calculated as `rank_gap(delta_cost) * sigmoid_gate * core_loss`. This coupling ensures that the most important pairs (highest rank) contribute the most to the final loss value, providing a stronger, more focused gradient signal for pairs that the model should prioritize learning from.", "pseudocode": "1. Calculate the cost difference: delta_cost = cost(b) - cost(a).\n2. Calculate the log-probability difference: delta_logp = logp(a) - logp(b).\n3. (Inherited from Parents 0 & 1) Normalize the cost difference using z-scoring: normalized_delta_cost = zscore(delta_cost).\n4. (Inherited from Parents 0 & 1) Compute a rank-based weight from the raw delta_cost: rank_weight = rank_gap(delta_cost).\n5. (New Coupling 1) Compute a rank-modulated saturated margin. Multiply the rank_weight by the normalized_delta_cost, apply a `tanh` saturation, and scale by `gamma`: margin = gamma * tanh(rank_weight * normalized_delta_cost).\n6. Compute the core logistic loss using the adaptive margin: core_loss = softplus(margin - delta_logp).\n7. (Inherited from Parent 0) Compute a sigmoid-based gating weight from the normalized cost difference: gating_weight = sigmoid(normalized_delta_cost).\n8. (New Coupling 2) Compute the final instance loss by weighting the gated loss with the rank_weight: instance_loss = rank_weight * gating_weight * core_loss.\n9. The final loss is the average of instance_loss over the batch.", "hyperparams": {"gamma": 2.5, "eps": 1e-08}, "operators_used": ["softplus", "sigmoid", "tanh", "zscore", "rank_gap"], "implementation_hint": {"expects": ["cost_a", "cost_b", "logp_a", "logp_b", "weight"], "returns": "scalar"}, "code": "import torch\nimport torch.nn.functional as F\n\ndef generated_loss(batch, model_output, extra):\n    \"\"\"\n    Adaptive Sigmoid-Gated Logistic Loss with Rank-Modulated Margin.\n\n    This loss combines a rank-modulated margin with a sigmoid gate and rank-based loss weighting.\n\n    - Inherits `sigmoid(zscore(delta_cost))` gating from Parent 0 for adaptive focus.\n    - Inherits `rank_gap` from Parent 1 to determine a pair's relative importance.\n    - New Coupling 1: Creates a margin `gamma * tanh(rank_gap * zscore(delta_cost))`, making the target\n      preference gap dependent on both the statistical and ordinal importance of the cost difference.\n    - New Coupling 2: The final loss is weighted by `rank_gap`, amplifying the gradient for the\n      most important pairs in the batch.\n    \"\"\"\n    # Read hyperparameters from the 'extra' dictionary\n    gamma = extra.get('gamma', 2.5)\n    eps = extra.get('eps', 1e-8)\n\n    # Unpack tensors from the batch\n    cost_w = batch['cost_a']\n    cost_l = batch['cost_b']\n    logp_w = batch['log_prob_w']\n    logp_l = batch['log_prob_l']\n\n    # Sanity check: cost_w should be less than or equal to cost_l\n    assert torch.all(cost_w <= cost_l), \"Assumption failed: cost_a (winner) must be <= cost_b (loser)\"\n\n    # 1. Calculate cost and log-probability differences\n    delta_cost = cost_l - cost_w\n    delta_logp = logp_w - logp_l\n\n    # 2. (Inherited) Normalize delta_cost using z-scoring\n    if delta_cost.numel() > 1:\n        mean_delta_cost = delta_cost.mean()\n        std_delta_cost = delta_cost.std() + eps\n        normalized_delta_cost = (delta_cost - mean_delta_cost) / std_delta_cost\n    else:\n        normalized_delta_cost = torch.zeros_like(delta_cost)\n\n    # 3. (Inherited) Compute rank-based adaptive weight\n    if delta_cost.numel() > 1:\n        ranks = delta_cost.argsort().argsort().float()\n        rank_weight = ranks / (delta_cost.numel() - 1)  # Maps to [0, 1]\n    else:\n        rank_weight = torch.ones_like(delta_cost)\n\n    # 4. (New Coupling 1) Compute a rank-modulated saturated margin\n    # The margin combines rank, batch stats, and saturation for stability and adaptiveness.\n    margin_signal = rank_weight * normalized_delta_cost\n    margin = gamma * torch.tanh(margin_signal)\n\n    # 5. Compute the core logistic loss using the adaptive margin\n    core_loss = F.softplus(margin - delta_logp)\n\n    # 6. (Inherited from Parent 0) Compute sigmoid-based gating weight\n    gating_weight = torch.sigmoid(normalized_delta_cost)\n\n    # 7. (New Coupling 2) Combine gate, rank weight, and core loss\n    # The rank_weight is used a second time to emphasize the loss from important pairs.\n    instance_loss = rank_weight * gating_weight * core_loss\n\n    # Apply optional sample weights if provided\n    if 'weight' in batch and batch['weight'] is not None:\n        instance_loss = instance_loss * batch['weight']\n\n    # 8. Return the mean loss over the batch\n    return instance_loss.mean()"}, "static_ok": true, "static_reason": "", "dynamic_ok": true, "dynamic_reason": "ok", "loss_value": 0.1732867956161499, "grad_norm": 0.0}
{"generation": 8, "index": 0, "ir": {"name": "Rank-Gated Saturated Logistic Loss", "intuition": "This loss function creates a highly adaptive and stable learning signal by combining a saturated, z-score-normalized margin with a novel rank-based gating mechanism. The objective is to produce a loss that is robust to the scale and distribution of costs within a batch, while dynamically focusing learning on pairs with higher relative cost differences.\n\nInherited Ideas:\n- From Parent 0 (Sigmoid-Gated Rank-Weighted Logistic Loss), it inherits the idea of applying a gating mechanism to the final loss. Instead of using a sigmoid on z-scored costs, this child uses the rank directly as a smooth gate.\n- From Parent 1 (Adaptive Rank-Weighted Saturated Logistic Loss), it inherits the construction of a `saturated, z-score-normalized margin` via `gamma * tanh(zscore(delta_cost))`. This makes the margin robust to the absolute scale of costs and stable against extreme outliers in the batch.\n\nNew Coupling Ideas:\n1.  **Rank-Based Gating**: The final loss is directly weighted by `rank_gap(delta_cost)`. This acts as a smooth, monotonic gate. Pairs with the smallest cost difference in the batch are assigned a weight near zero, and the weight increases linearly with the rank of the cost difference, reaching 1 for the pair with the largest gap. This coupling gracefully de-prioritizes pairs with negligible cost differences, which might be noisy, and focuses learning on the most clearly distinguished pairs in the batch.\n2.  **Log-Probability Clipping**: The log-probability difference `delta_logp = logp(a) - logp(b)` is clipped to a minimum value of `-clip_value`. This is a stability trick to prevent the `softplus` argument from becoming excessively large if the model becomes pathologically confident in the wrong direction (`logp(b) >> logp(a)`). This prevents potential numerical overflow and gradient explosion, ensuring a more stable training process, especially early on.", "pseudocode": "1. Calculate the cost difference: delta_cost = cost(b) - cost(a).\n2. Calculate the log-probability difference: delta_logp = logp(a) - logp(b).\n3. (Inherited from Parent 1) Normalize the cost difference using z-scoring: normalized_delta_cost = zscore(delta_cost).\n4. (Inherited from Parent 1) Compute a saturated, z-score-normalized margin by applying a tanh function to the normalized cost difference and scaling by gamma: margin = gamma * tanh(normalized_delta_cost).\n5. (New Coupling 2) Apply clipping to the log-probability difference for stability: clipped_delta_logp = clamp(delta_logp, min=-clip_value).\n6. Compute the core logistic loss using the adaptive margin and the clipped log-probability difference: core_loss = softplus(margin - clipped_delta_logp).\n7. (Inherited from Parent 0 & New Coupling 1) Compute a rank-based gating weight directly from the raw delta_cost: rank_gate = rank_gap(delta_cost).\n8. The final loss for each pair is the product of the rank gate and the core loss: instance_loss = rank_gate * core_loss.\n9. The final loss is the average of instance_loss over the batch.", "hyperparams": {"gamma": 1.5, "clip_value": 10.0, "eps": 1e-08}, "operators_used": ["softplus", "tanh", "zscore", "rank_gap", "clamp"], "implementation_hint": {"expects": ["cost_a", "cost_b", "logp_a", "logp_b"], "returns": "scalar"}, "code": "import torch\nimport torch.nn.functional as F\n\ndef generated_loss(batch, model_output, extra):\n    \"\"\"\n    Rank-Gated Saturated Logistic Loss.\n\n    This loss combines a saturated, z-score-normalized margin with a rank-based gating mechanism.\n\n    - Inherits the `tanh(zscore(delta_cost))` margin from Parent 1, making it robust to cost scale and outliers.\n    - Inherits the concept of gating from Parent 0, but implements it in a new way.\n    - New Coupling 1: Uses `rank_gap(delta_cost)` as a direct weighting (gate) on the final loss,\n      smoothly focusing on pairs with larger relative cost differences.\n    - New Coupling 2: Clips `delta_logp` to prevent numerical instability from extremely incorrect model predictions,\n      avoiding potential gradient explosion.\n    \"\"\"\n    # Read hyperparameters from the 'extra' dictionary\n    gamma = extra.get('gamma', 1.5)\n    clip_value = extra.get('clip_value', 10.0)\n    eps = extra.get('eps', 1e-8)\n\n    # Unpack tensors from the batch\n    cost_w = batch['cost_a']\n    cost_l = batch['cost_b']\n    logp_w = batch['log_prob_w']\n    logp_l = batch['log_prob_l']\n\n    # Sanity check: cost_w should be less than or equal to cost_l\n    assert torch.all(cost_w <= cost_l), \"Assumption failed: cost_a (winner) must be <= cost_b (loser)\"\n\n    # 1. Calculate cost and log-probability differences\n    delta_cost = cost_l - cost_w  # This will be >= 0\n    delta_logp = logp_w - logp_l  # We want this to be positive\n\n    # 2. (Inherited from Parent 1) Normalize delta_cost using z-scoring for batch adaptiveness\n    if delta_cost.numel() > 1:\n        mean_delta_cost = delta_cost.mean()\n        std_delta_cost = delta_cost.std() + eps\n        normalized_delta_cost = (delta_cost - mean_delta_cost) / std_delta_cost\n    else:\n        normalized_delta_cost = torch.zeros_like(delta_cost)\n\n    # 3. (Inherited from Parent 1) Compute a saturated, z-score-normalized margin\n    # This is adaptive to batch stats and stable against extreme outliers.\n    margin = gamma * torch.tanh(normalized_delta_cost)\n\n    # 4. (New Coupling 2) Clip delta_logp for numerical stability\n    # Prevents the argument to softplus from becoming excessively large.\n    clipped_delta_logp = torch.clamp(delta_logp, min=-clip_value)\n\n    # 5. Compute the core logistic loss\n    core_loss = F.softplus(margin - clipped_delta_logp)\n\n    # 6. (New Coupling 1) Compute a rank-based gating weight\n    if delta_cost.numel() > 1:\n        ranks = delta_cost.argsort().argsort().float()\n        rank_gate = ranks / (delta_cost.numel() - 1) # Maps to [0, 1]\n    else:\n        rank_gate = torch.ones_like(delta_cost)\n\n    # 7. Apply the rank gate to the core loss\n    # This smoothly de-prioritizes pairs with small cost differences.\n    instance_loss = rank_gate * core_loss\n\n    # Apply optional sample weights if provided\n    if 'weight' in batch and batch['weight'] is not None:\n        instance_loss = instance_loss * batch['weight']\n\n    # 8. Return the mean loss over the batch\n    return instance_loss.mean()"}, "static_ok": true, "static_reason": "", "dynamic_ok": true, "dynamic_reason": "ok", "loss_value": 0.3465735912322998, "grad_norm": 0.0}
{"generation": 8, "index": 1, "ir": {"name": "Rank-Clipped Saturated Logistic Loss", "intuition": "This loss function creates a stable and adaptive learning signal by combining rank-based clipping with a saturated, z-score-normalized margin. The goal is to focus learning on the most informative pairs within a batch while preventing outlier costs or log-probabilities from dominating the gradient.\n\nInherited Ideas:\n- From Parent 0 (Sigmoid-Gated Rank-Weighted Logistic Loss) and Parent 1 (Adaptive Rank-Weighted Saturated Logistic Loss), it inherits the core idea of using `zscore(delta_cost)` to normalize the cost difference. This makes the loss adaptive to the batch's statistical properties and robust to the absolute scale of costs.\n- From Parent 1, it inherits the use of a saturated margin, `tanh(zscore(delta_cost))`. This prevents pairs with extreme cost differences from creating an unbounded margin, enhancing numerical stability.\n\nNew Coupling Ideas:\n1.  **Rank-Based Clipping of Log-Probabilities**: Instead of using rank to weight the loss, this design uses it to stabilize the log-probability difference (`delta_logp`). The `delta_logp` is clipped to a dynamic range defined by `[-beta * rank_gap(delta_cost), beta * rank_gap(delta_cost)]`. This coupling prevents pairs with very large or small `delta_logp` from causing instability, while allowing the clipping range to adapt based on the pair's relative importance (rank) within the batch. More important pairs (higher rank) are allowed a wider `delta_logp` range, while less important pairs are regularized more strongly towards zero.\n2.  **Adaptive Loss Weighting with ReLU**: The final loss is weighted by `relu(margin)`. Since the margin `gamma * tanh(zscore(delta_cost))` can be negative for pairs with a cost difference below the batch mean, this `relu` gate acts as a simple, effective filter. It completely zeros out the loss for pairs that are deemed 'unimportant' by the batch statistics (i.e., those with a `delta_cost` significantly below the mean), focusing the learning exclusively on pairs with an above-average cost gap. This provides a hard gating mechanism as opposed to the soft gating in Parent 0.", "pseudocode": "1. Calculate the cost difference: delta_cost = cost(b) - cost(a).\n2. Calculate the log-probability difference: delta_logp = logp(a) - logp(b).\n3. (Inherited from Parents 0 & 1) Normalize the cost difference using z-scoring: normalized_delta_cost = zscore(delta_cost).\n4. Compute the rank-gap of the cost difference: rank_weight = rank_gap(delta_cost).\n5. (Inherited from Parent 1) Compute a saturated, z-score-normalized margin: margin = gamma * tanh(normalized_delta_cost).\n6. (New Coupling 1) Create a dynamic clipping bound based on the rank: clip_bound = beta * rank_weight. Then, clip the log-probability difference: clipped_delta_logp = clamp(delta_logp, -clip_bound, clip_bound).\n7. Compute the core logistic loss using the margin and the clipped log-probability difference: core_loss = softplus(margin - clipped_delta_logp).\n8. (New Coupling 2) Compute an adaptive loss weight by applying ReLU to the margin: loss_weight = relu(margin).\n9. The final loss for each pair is the product of the adaptive weight and the core loss: instance_loss = loss_weight * core_loss.\n10. The final loss is the average of instance_loss over the batch.", "hyperparams": {"gamma": 1.5, "beta": 5.0, "eps": 1e-08}, "operators_used": ["softplus", "tanh", "zscore", "rank_gap", "clamp", "relu"], "implementation_hint": {"expects": ["cost_a", "cost_b", "logp_a", "logp_b"], "returns": "scalar"}, "code": "import torch\nimport torch.nn.functional as F\n\ndef generated_loss(batch, model_output, extra):\n    \"\"\"\n    Rank-Clipped Saturated Logistic Loss.\n\n    This loss combines a tanh-saturated margin with rank-based clipping of log-probabilities\n    and a ReLU-based loss gate for stability and adaptiveness.\n\n    - Inherits `zscore` normalization from both parents for batch adaptiveness.\n    - Inherits the `tanh` saturated margin from Parent 1 for stability against cost outliers.\n    - New Coupling 1: Clips `delta_logp` to a range `[-beta*rank_gap, beta*rank_gap]`,\n      using rank to regularize log-probability differences and prevent instability.\n    - New Coupling 2: Weights the final loss by `relu(margin)`, effectively zeroing out the loss\n      for pairs with a cost difference significantly below the batch mean.\n    \"\"\"\n    # Read hyperparameters from the 'extra' dictionary\n    gamma = extra.get('gamma', 1.5)\n    beta = extra.get('beta', 5.0)\n    eps = extra.get('eps', 1e-8)\n\n    # Unpack tensors from the batch\n    cost_w = batch['cost_a']\n    cost_l = batch['cost_b']\n    logp_w = batch['log_prob_w']\n    logp_l = batch['log_prob_l']\n\n    # Sanity check: cost_w should be less than or equal to cost_l\n    assert torch.all(cost_w <= cost_l), \"Assumption failed: cost_a (winner) must be <= cost_b (loser)\"\n\n    # 1. Calculate cost and log-probability differences\n    delta_cost = cost_l - cost_w  # This will be >= 0\n    delta_logp = logp_w - logp_l  # We want this to be positive\n\n    # 2. (Inherited) Normalize delta_cost using z-scoring\n    if delta_cost.numel() > 1:\n        mean_delta_cost = delta_cost.mean()\n        std_delta_cost = delta_cost.std() + eps\n        normalized_delta_cost = (delta_cost - mean_delta_cost) / std_delta_cost\n    else:\n        normalized_delta_cost = torch.zeros_like(delta_cost)\n\n    # 3. Compute rank-based adaptive weight from raw delta_cost\n    if delta_cost.numel() > 1:\n        ranks = delta_cost.argsort().argsort().float()\n        rank_weight = ranks / (delta_cost.numel() - 1)  # Maps to [0, 1]\n    else:\n        rank_weight = torch.ones_like(delta_cost)\n\n    # 4. (Inherited) Compute a saturated, z-score-normalized margin\n    margin = gamma * torch.tanh(normalized_delta_cost)\n\n    # 5. (New Coupling 1) Clip delta_logp based on rank\n    clip_bound = beta * rank_weight\n    clipped_delta_logp = torch.clamp(delta_logp, min=-clip_bound, max=clip_bound)\n\n    # 6. Compute the core logistic loss with the clipped logp\n    core_loss = F.softplus(margin - clipped_delta_logp)\n\n    # 7. (New Coupling 2) Compute an adaptive loss weight using ReLU on the margin\n    # This zeros out the loss for pairs with cost_delta below the batch mean.\n    loss_weight = F.relu(margin)\n\n    # 8. Combine weight and core loss\n    instance_loss = loss_weight * core_loss\n\n    # Apply optional sample weights if provided\n    if 'weight' in batch and batch['weight'] is not None:\n        instance_loss = instance_loss * batch['weight']\n\n    # 9. Return the mean loss over the batch\n    return instance_loss.mean()"}, "static_ok": true, "static_reason": "", "dynamic_ok": true, "dynamic_reason": "ok", "loss_value": 0.0, "grad_norm": 0.0}
{"generation": 8, "index": 2, "ir": {"name": "Adaptive Rank-Gated Saturated Logistic Loss", "intuition": "This loss function creates a highly adaptive learning signal by gating the loss based on the relative rank of the cost difference, and using a saturated, z-score-normalized margin. The goal is to focus learning on the top-k percentile of most important pairs in a batch, while using a stable, outlier-resistant margin for all pairs.\n\nInherited Ideas:\n- From Parent 0 (Sigmoid-Gated Rank-Weighted Logistic Loss) and Parent 1 (Adaptive Rank-Weighted Saturated Logistic Loss), it inherits the use of `rank_gap(delta_cost)` to understand the relative importance of each pair within the batch.\n- From Parent 1 (Adaptive Rank-Weighted Saturated Logistic Loss), it inherits the idea of a saturated, z-score-normalized margin: `gamma * tanh(zscore(delta_cost))`. This margin is robust to the absolute scale of costs and is stabilized against extreme outliers by the `tanh` function.\n\nNew Coupling Ideas:\n1.  **Rank-Based Gating Mechanism**: Instead of using the rank as a continuous weight, it's used to create a binary or smooth gate. The loss is multiplied by a weight derived from `rank_gap(delta_cost)`. We introduce a `gate_threshold` hyperparameter. The gate is `sigmoid(gate_sharpness * (rank_gap - gate_threshold))`. This effectively focuses the learning signal on pairs whose cost difference is in the top percentile of the batch (e.g., top 50% if threshold is 0.5), smoothly de-prioritizing less significant pairs. This is a crossover of Parent 0's gating idea and both parents' use of rank.\n2.  **Saturated Margin with Rank-Based Offset**: The core logistic loss uses the saturated z-score margin from Parent 1, but we add a small offset to the margin itself, scaled by the rank. The final margin is `gamma * tanh(zscore(delta_cost)) + beta * rank_gap(delta_cost)`. This coupling ensures that even if z-scored costs are similar, the rank still provides a small, monotonically increasing signal to the margin, helping to differentiate between pairs more finely.", "pseudocode": "1. Calculate the cost difference: delta_cost = cost(b) - cost(a).\n2. Calculate the log-probability difference: delta_logp = logp(a) - logp(b).\n3. (Inherited from Parents 0 & 1) Compute a rank-based value from the raw delta_cost: rank_value = rank_gap(delta_cost).\n4. (Inherited from Parent 1) Normalize the cost difference using z-scoring: normalized_delta_cost = zscore(delta_cost).\n5. (New Coupling 1) Compute a rank-based gating weight. Use a sigmoid function to create a smooth gate around a rank threshold: gating_weight = sigmoid(gate_sharpness * (rank_value - gate_threshold)).\n6. (New Coupling 2) Compute a saturated margin with a rank-based offset. The margin is composed of the saturated z-score term and a small linear rank term: margin = gamma * tanh(normalized_delta_cost) + beta * rank_value.\n7. Compute the core logistic loss using this composite margin: core_loss = softplus(margin - delta_logp).\n8. The final loss for each pair is the product of the gating weight and the core loss: instance_loss = gating_weight * core_loss.\n9. The final loss is the average of instance_loss over the batch.", "hyperparams": {"gamma": 1.5, "beta": 0.1, "gate_threshold": 0.5, "gate_sharpness": 10.0, "eps": 1e-08}, "operators_used": ["softplus", "tanh", "sigmoid", "zscore", "rank_gap"], "implementation_hint": {"expects": ["cost_a", "cost_b", "logp_a", "logp_b", "weight"], "returns": "scalar"}, "code": "import torch\nimport torch.nn.functional as F\n\ndef generated_loss(batch, model_output, extra):\n    \"\"\"\n    Adaptive Rank-Gated Saturated Logistic Loss.\n\n    This loss focuses learning on the top percentile of pairs by using a rank-based gate,\n    while employing a stable, saturated margin for the core loss calculation.\n\n    - Inherits `rank_gap` from both parents to determine relative pair importance.\n    - Inherits `tanh(zscore(delta_cost))` margin from Parent 1 for a stable, adaptive margin.\n    - New Coupling 1: A `sigmoid` gate based on `rank_gap` focuses learning on pairs above a\n      certain rank threshold (e.g., top 50%).\n    - New Coupling 2: A small rank-proportional offset `beta * rank_gap` is added to the\n      margin itself, ensuring rank order always influences the learning target.\n    \"\"\"\n    # Read hyperparameters from the 'extra' dictionary\n    gamma = extra.get('gamma', 1.5)\n    beta = extra.get('beta', 0.1)\n    gate_threshold = extra.get('gate_threshold', 0.5) # e.g., 0.5 focuses on top 50% of pairs\n    gate_sharpness = extra.get('gate_sharpness', 10.0)\n    eps = extra.get('eps', 1e-8)\n\n    # Unpack tensors from the batch\n    cost_w = batch['cost_a']\n    cost_l = batch['cost_b']\n    logp_w = batch['log_prob_w']\n    logp_l = batch['log_prob_l']\n\n    # Sanity check: cost_w should be less than or equal to cost_l\n    assert torch.all(cost_w <= cost_l), \"Assumption failed: cost_a (winner) must be <= cost_b (loser)\"\n\n    # 1. Calculate cost and log-probability differences\n    delta_cost = cost_l - cost_w  # This will be >= 0\n    delta_logp = logp_w - logp_l  # We want this to be positive\n\n    # 2. (Inherited from Parents 0 & 1) Compute rank-based value\n    # rank_gap(delta_cost) -> ranks normalized to [0, 1]\n    if delta_cost.numel() > 1:\n        ranks = delta_cost.argsort().argsort().float()\n        rank_value = ranks / (delta_cost.numel() - 1)\n    else:\n        rank_value = torch.ones_like(delta_cost)\n\n    # 3. (Inherited from Parent 1) Normalize delta_cost using z-scoring\n    if delta_cost.numel() > 1:\n        mean_delta_cost = delta_cost.mean()\n        std_delta_cost = delta_cost.std() + eps\n        normalized_delta_cost = (delta_cost - mean_delta_cost) / std_delta_cost\n    else:\n        normalized_delta_cost = torch.zeros_like(delta_cost)\n\n    # 4. (New Coupling 1) Compute a rank-based gating weight\n    # This smoothly selects pairs with rank > gate_threshold\n    gate_input = gate_sharpness * (rank_value - gate_threshold)\n    gating_weight = torch.sigmoid(gate_input)\n\n    # 5. (New Coupling 2) Compute a saturated margin with a rank-based offset\n    # Inherits the stable core margin from Parent 1 and adds a small rank signal.\n    saturated_zscore_margin = gamma * torch.tanh(normalized_delta_cost)\n    rank_offset = beta * rank_value\n    margin = saturated_zscore_margin + rank_offset\n\n    # 6. Compute the core logistic loss\n    core_loss = F.softplus(margin - delta_logp)\n\n    # 7. Apply the rank-based gate to the loss\n    instance_loss = gating_weight * core_loss\n\n    # Apply optional sample weights if provided\n    if 'weight' in batch and batch['weight'] is not None:\n        instance_loss = instance_loss * batch['weight']\n\n    # 8. Return the mean loss over the batch\n    return instance_loss.mean()"}, "static_ok": true, "static_reason": "", "dynamic_ok": true, "dynamic_reason": "ok", "loss_value": 0.3653518259525299, "grad_norm": 0.0}
{"generation": 8, "index": 3, "ir": {"name": "Rank-Gated Saturated Logistic Loss", "intuition": "This loss function creates a highly adaptive learning signal by combining a saturated, z-score-normalized margin with a rank-based gating mechanism. The goal is to produce a loss that is robust to the scale of costs, stable against outliers, and focuses learning primarily on pairs with a significant cost difference relative to other pairs in the same batch.\n\nInherited Ideas:\n- From Parent 0 (Sigmoid-Gated Rank-Weighted Logistic Loss), it inherits the idea of using a gating mechanism to modulate the loss. However, instead of a sigmoid gate on z-scored costs, this child uses a rank-based gate.\n- From Parent 1 (Adaptive Rank-Weighted Saturated Logistic Loss), it inherits the construction of a robust, saturated margin: `gamma * tanh(zscore(delta_cost))`. This margin adapts to the batch's cost statistics and prevents outlier pairs from creating an excessively large margin, which enhances stability.\n\nNew Coupling Ideas:\n1. **Rank-Based Gating**: The final loss is weighted by `rank_gap(delta_cost)`. This acts as a smooth, adaptive gate. Pairs with a small cost difference relative to others in the batch receive a weight near zero, effectively being ignored. As the relative cost difference increases, the weight smoothly approaches one. This coupling focuses learning on pairs that are demonstrably more important within the batch context, gracefully down-weighting pairs with minor or potentially noisy cost differences.\n2. **Log-Sigmoid Formulation**: The core loss is expressed as `-logsigmoid(delta_logp - margin)` instead of `softplus(margin - delta_logp)`. While mathematically equivalent, the log-sigmoid form directly models the log-probability of the model correctly preferring the winner, making the connection to the underlying Bradley-Terry model more explicit. This also offers a slight numerical stability advantage in certain deep learning frameworks.", "pseudocode": "1. Calculate the cost difference: delta_cost = cost(b) - cost(a).\n2. Calculate the log-probability difference: delta_logp = logp(a) - logp(b).\n3. (Inherited from Parent 1) Normalize the cost difference using z-scoring: normalized_delta_cost = zscore(delta_cost).\n4. (Inherited from Parent 1) Compute a saturated, z-score-normalized margin. Apply a tanh function to the normalized_delta_cost and scale it by a hyperparameter `gamma`: margin = gamma * tanh(normalized_delta_cost).\n5. (New Coupling 2) Compute the core loss using a log-sigmoid formulation, which encourages the log-probability difference to be greater than the margin: core_loss = -logsigmoid(delta_logp - margin).\n6. (Inherited from Parent 0 & New Coupling 1) Compute a rank-based gating weight. This weight is the normalized rank of the cost difference: gating_weight = rank_gap(delta_cost).\n7. The final loss for each pair is the product of the rank-based gate and the core loss: instance_loss = gating_weight * core_loss.\n8. The final loss is the average of instance_loss over the batch.", "hyperparams": {"gamma": 1.5, "eps": 1e-08}, "operators_used": ["logsigmoid", "tanh", "zscore", "rank_gap"], "implementation_hint": {"expects": ["cost_a", "cost_b", "logp_a", "logp_b"], "returns": "scalar"}, "code": "import torch\nimport torch.nn.functional as F\n\ndef zscore(x, eps=1e-8):\n    \"\"\"Calculates z-score for a tensor, handles single-element case.\"\"\"\n    if x.numel() <= 1:\n        return torch.zeros_like(x)\n    mean = x.mean()\n    std = x.std() + eps\n    return (x - mean) / std\n\ndef rank_gap(x):\n    \"\"\"Calculates normalized rank [0, 1] for a tensor, handles single-element case.\"\"\"\n    if x.numel() <= 1:\n        return torch.ones_like(x)\n    ranks = x.argsort().argsort().float()\n    return ranks / (x.numel() - 1)\n\ndef generated_loss(batch, model_output, extra):\n    \"\"\"\n    Rank-Gated Saturated Logistic Loss.\n\n    This loss combines a saturated margin (tanh on z-scored costs) with a rank-based gate.\n    The goal is to focus learning on pairs with high relative cost differences within the batch,\n    while using a stable, batch-adaptive margin.\n\n    - Inherits `tanh(zscore(delta_cost))` margin from Parent 1 for a stable, adaptive margin.\n    - Inherits the concept of gating from Parent 0, but implements it with `rank_gap`.\n    - New Coupling 1: Uses `rank_gap(delta_cost)` as a continuous gating mechanism to weight the loss,\n      focusing attention on pairs with larger cost differences relative to the batch.\n    - New Coupling 2: Uses the `logsigmoid` formulation for the core logistic loss, which is\n      mathematically equivalent to softplus but more explicit about the probabilistic model.\n    \"\"\"\n    # Read hyperparameters from the 'extra' dictionary\n    gamma = extra.get('gamma', 1.5)\n    eps = extra.get('eps', 1e-8)\n\n    # Unpack tensors from the batch\n    cost_w = batch['cost_a']\n    cost_l = batch['cost_b']\n    logp_w = batch['log_prob_w']\n    logp_l = batch['log_prob_l']\n\n    # Sanity check: cost_w should be less than or equal to cost_l\n    assert torch.all(cost_w <= cost_l), \"Assumption failed: cost_a (winner) must be <= cost_b (loser)\"\n\n    # 1. Calculate cost and log-probability differences\n    delta_cost = cost_l - cost_w  # This will be >= 0\n    delta_logp = logp_w - logp_l  # We want this to be positive\n\n    # 2. (Inherited from Parent 1) Normalize delta_cost using z-scoring\n    normalized_delta_cost = zscore(delta_cost, eps=eps)\n\n    # 3. (Inherited from Parent 1) Compute a saturated, z-score-normalized margin\n    # This margin is adaptive to batch stats and stable against extreme outliers.\n    margin = gamma * torch.tanh(normalized_delta_cost)\n\n    # 4. (New Coupling 2) Compute core loss using log-sigmoid formulation\n    # This is equivalent to F.softplus(margin - delta_logp)\n    core_loss = -F.logsigmoid(delta_logp - margin)\n\n    # 5. (Inherited from Parent 0 & New Coupling 1) Compute rank-based gating weight\n    # This gate smoothly emphasizes pairs with higher relative cost differences.\n    gating_weight = rank_gap(delta_cost)\n\n    # 6. Combine gate and core loss\n    instance_loss = gating_weight * core_loss\n\n    # Apply optional sample weights if provided\n    if 'weight' in batch and batch['weight'] is not None:\n        instance_loss = instance_loss * batch['weight']\n\n    # 7. Return the mean loss over the batch\n    return instance_loss.mean()"}, "static_ok": true, "static_reason": "", "dynamic_ok": false, "dynamic_reason": "forward_error: name 'zscore' is not defined", "loss_value": null, "grad_norm": null}
{"generation": 8, "index": 4, "ir": {"name": "Saturated Rank-Gated Logistic Loss", "intuition": "This loss function creates a highly adaptive and stable learning signal by gating the standard logistic loss with a saturated, rank-based weight. The goal is to focus learning on pairs with a meaningful cost difference relative to the batch, while gracefully de-prioritizing both the least significant pairs and extreme outliers.\n\nInherited Ideas:\n- From Parent 0 (Sigmoid-Gated Rank-Weighted Logistic Loss), it inherits the idea of using `rank_gap(delta_cost)` to understand the relative importance of a pair within the batch. This is a robust, non-parametric way to assess significance.\n- From Parent 1 (Adaptive Rank-Weighted Saturated Logistic Loss), it inherits the use of `tanh` to create a saturated signal. This prevents extreme values from having an unbounded influence on the loss, promoting stability.\n\nNew Coupling Ideas:\n1.  **Saturated Rank Gating**: This is the core new idea. Instead of using `rank_gap` to directly weight the loss or construct a margin, it is used as the input to a `tanh` function. The resulting `tanh(rank_gap(delta_cost))` term acts as a smooth, bounded gate on the entire loss. For pairs with low rank (small cost difference), the gate is near zero, effectively ignoring them. As the rank increases, the gate smoothly opens up and then saturates, ensuring that all pairs above a certain rank contribute strongly but without the highest-ranked pairs dominating excessively.\n2.  **Adaptive Margin from Z-Scored Costs**: The margin term, `beta * zscore(delta_cost)`, is computed directly from the z-scored cost differences. This makes the target preference gap (`delta_logp`) adaptive to the batch's statistics (mean and standard deviation). A larger `delta_cost` relative to the batch average demands a larger `delta_logp` from the model. This is different from the parents, which coupled `rank_gap` or `tanh(zscore)` into the margin. Here, the margin is purely statistical, while the gating is purely rank-based, creating a separation of concerns.", "pseudocode": "1. Calculate the cost difference: delta_cost = cost(b) - cost(a).\n2. Calculate the log-probability difference: delta_logp = logp(a) - logp(b).\n3. (Inherited from Parent 0) Compute the normalized rank of the cost difference: rank_signal = rank_gap(delta_cost).\n4. (New Coupling 1) Create a saturated gating weight by applying `tanh` to the scaled rank signal: gate_weight = tanh(alpha * rank_signal).\n5. (New Coupling 2) Create an adaptive margin from the z-scored cost difference: normalized_delta_cost = zscore(delta_cost), then margin = beta * normalized_delta_cost.\n6. Compute the core logistic loss using the adaptive margin: core_loss = softplus(margin - delta_logp).\n7. The final loss for each pair is the product of the gating weight and the core loss: instance_loss = gate_weight * core_loss.\n8. The final loss is the average of instance_loss over the batch.", "hyperparams": {"alpha": 3.0, "beta": 0.5, "eps": 1e-08}, "operators_used": ["softplus", "tanh", "zscore", "rank_gap"], "implementation_hint": {"expects": ["cost_a", "cost_b", "logp_a", "logp_b"], "returns": "scalar"}, "code": "import torch\nimport torch.nn.functional as F\n\ndef generated_loss(batch, model_output, extra):\n    \"\"\"\n    Saturated Rank-Gated Logistic Loss.\n\n    This loss uses a tanh-saturated rank signal to gate a logistic loss whose margin is\n    derived from z-scored cost differences.\n\n    - Inherits `rank_gap` from Parent 0 to create a non-parametric signal of importance.\n    - Inherits `tanh` saturation from Parent 1 for stability.\n    - New Coupling 1: A `tanh(alpha * rank_gap(delta_cost))` term acts as a smooth gate,\n      focusing on meaningfully ranked pairs while capping the influence of top-ranked ones.\n    - New Coupling 2: The margin `beta * zscore(delta_cost)` is purely statistical, making the\n      target preference gap adaptive to batch statistics, separating it from the rank-based gating.\n    \"\"\"\n    # Read hyperparameters\n    alpha = extra.get('alpha', 3.0)\n    beta = extra.get('beta', 0.5)\n    eps = extra.get('eps', 1e-8)\n\n    # Unpack tensors from the batch\n    cost_w = batch['cost_a']\n    cost_l = batch['cost_b']\n    logp_w = batch['log_prob_w']\n    logp_l = batch['log_prob_l']\n\n    # Sanity check: cost_w should be less than or equal to cost_l\n    assert torch.all(cost_w <= cost_l), \"Assumption failed: cost_a (winner) must be <= cost_b (loser)\"\n\n    # 1. Calculate cost and log-probability differences\n    delta_cost = cost_l - cost_w\n    delta_logp = logp_w - logp_l\n\n    # Handle batch size of 1 to avoid division by zero or NaN issues\n    if delta_cost.numel() > 1:\n        # (Inherited from Parent 0) Compute rank-based signal\n        ranks = delta_cost.argsort().argsort().float()\n        rank_signal = ranks / (delta_cost.numel() - 1)  # Normalized to [0, 1]\n\n        # (New Coupling 2) Compute z-scored cost for the margin\n        mean_delta_cost = delta_cost.mean()\n        std_delta_cost = delta_cost.std() + eps\n        normalized_delta_cost = (delta_cost - mean_delta_cost) / std_delta_cost\n    else:\n        rank_signal = torch.ones_like(delta_cost)\n        normalized_delta_cost = torch.zeros_like(delta_cost)\n\n    # (New Coupling 1) Create a saturated gating weight from the rank signal\n    # tanh(alpha * rank) creates a smooth gate that ignores low-rank pairs and saturates for high-rank ones.\n    gate_weight = torch.tanh(alpha * rank_signal)\n\n    # (New Coupling 2) Create an adaptive margin from z-scored costs\n    # The margin adapts to the statistical properties of the batch's cost differences.\n    margin = beta * normalized_delta_cost\n\n    # Compute the core logistic loss with the adaptive margin\n    core_loss = F.softplus(margin - delta_logp)\n\n    # Apply the saturated rank gate to the core loss\n    instance_loss = gate_weight * core_loss\n\n    # Apply optional sample weights if provided\n    if 'weight' in batch and batch['weight'] is not None:\n        instance_loss = instance_loss * batch['weight']\n\n    # Return the mean loss over the batch\n    return instance_loss.mean()"}, "static_ok": true, "static_reason": "", "dynamic_ok": true, "dynamic_reason": "ok", "loss_value": 0.5210580229759216, "grad_norm": 0.0}
{"generation": 8, "index": 5, "ir": {"name": "Rank-Gated Saturated Logistic Loss", "intuition": "This loss function creates a highly adaptive learning signal by gating the loss with the rank of the cost difference, while using a saturated margin based on z-scored costs. The goal is to robustly handle batches with diverse cost distributions and focus learning primarily on pairs with a median-to-high relative cost difference, while preventing outlier costs from dominating the learning signal.\n\nInherited Ideas:\n- From Parent 0 (Sigmoid-Gated Rank-Weighted Logistic Loss), it inherits the idea of using `zscore(delta_cost)` to normalize the cost difference, making the loss adaptive to the batch's statistical properties and robust to the absolute scale of costs.\n- From Parent 1 (Adaptive Rank-Weighted Saturated Logistic Loss), it inherits the idea of using a saturated margin, `gamma * tanh(...)`, which prevents outlier pairs with extreme cost differences from creating an unbounded margin and destabilizing training.\n\nNew Coupling Ideas:\n1.  **Saturated Z-Score Margin (Inherited Combination)**: The margin is `gamma * tanh(zscore(delta_cost))`. This combines the `zscore` from Parent 0 and `tanh` from Parent 1 to create a margin that is both adaptive to the batch's cost distribution and stable against extreme outliers.\n2.  **Rank-Based Gating**: The final loss is weighted by `rank_gap(delta_cost)`. This acts as a smooth, monotonic gate. Pairs with the smallest cost difference in the batch receive a weight near zero, and the weight increases linearly to 1 for the pair with the largest cost difference. Unlike the sigmoid gate in Parent 0 which depends on the mean, this rank-based gate is purely ordinal and robust to the distribution shape of `delta_cost`. It ensures the model focuses its capacity on learning from pairs with demonstrably larger cost gaps, as determined by their rank order.", "pseudocode": "1. Calculate the cost difference: delta_cost = cost(b) - cost(a).\n2. Calculate the log-probability difference: delta_logp = logp(a) - logp(b).\n3. (Inherited from Parent 0) Normalize the cost difference using z-scoring: normalized_delta_cost = zscore(delta_cost).\n4. (Inherited from Parent 1 & New Coupling 1) Compute a saturated, z-score-normalized margin. Apply a `tanh` function to the normalized_delta_cost and scale it by a hyperparameter `gamma`: margin = gamma * tanh(normalized_delta_cost).\n5. Compute the core logistic loss using the adaptive margin: core_loss = softplus(margin - delta_logp).\n6. (New Coupling 2) Compute a rank-based gating weight from the raw delta_cost: gating_weight = rank_gap(delta_cost).\n7. The final loss for each pair is the product of the gating weight and the core loss: instance_loss = gating_weight * core_loss.\n8. The final loss is the average of instance_loss over the batch.", "hyperparams": {"gamma": 1.5, "eps": 1e-08}, "operators_used": ["softplus", "tanh", "zscore", "rank_gap"], "implementation_hint": {"expects": ["cost_a", "cost_b", "logp_a", "logp_b"], "returns": "scalar"}, "code": "import torch\nimport torch.nn.functional as F\n\ndef generated_loss(batch, model_output, extra):\n    \"\"\"\n    Rank-Gated Saturated Logistic Loss.\n\n    This loss combines a saturated, z-score-based margin with a rank-based gating mechanism.\n    The goal is to focus learning on pairs with higher relative cost differences in a stable manner.\n\n    - Inherits `zscore` normalization from Parent 0 for batch adaptiveness.\n    - Inherits `tanh` saturation from Parent 1 for a stable margin against outliers.\n    - New Coupling 1: Combines these to create a `tanh(zscore(delta_cost))` margin.\n    - New Coupling 2: Uses `rank_gap(delta_cost)` as a gate to weight the loss, smoothly\n      focusing on pairs with higher rank-ordered cost differences within the batch.\n    \"\"\"\n    # Read hyperparameters from the 'extra' dictionary\n    gamma = extra.get('gamma', 1.5)\n    eps = extra.get('eps', 1e-8)\n\n    # Unpack tensors from the batch\n    cost_a = batch['cost_a']\n    cost_b = batch['cost_b']\n    logp_w = batch['log_prob_w']\n    logp_l = batch['log_prob_l']\n\n    # Sanity check: cost_a should be less than or equal to cost_b\n    # In DPO, a is preferred (winner), b is not (loser)\n    assert torch.all(cost_a <= cost_b), \"Assumption failed: cost_a (winner) must be <= cost_b (loser)\"\n\n    # 1. Calculate cost and log-probability differences\n    delta_cost = cost_b - cost_a  # This will be >= 0\n    delta_logp = logp_w - logp_l  # We want this to be positive\n\n    # 2. (Inherited from Parent 0) Normalize delta_cost using z-scoring\n    if delta_cost.numel() > 1:\n        mean_delta_cost = delta_cost.mean()\n        std_delta_cost = delta_cost.std() + eps\n        normalized_delta_cost = (delta_cost - mean_delta_cost) / std_delta_cost\n    else:\n        normalized_delta_cost = torch.zeros_like(delta_cost)\n\n    # 3. (Inherited from Parent 1 & New Coupling 1) Compute a saturated, z-score-normalized margin\n    # This margin is adaptive to batch stats and stable against extreme outliers.\n    margin = gamma * torch.tanh(normalized_delta_cost)\n\n    # 4. Compute the core logistic loss using the adaptive margin\n    # The loss is low when delta_logp > margin.\n    core_loss = F.softplus(margin - delta_logp)\n\n    # 5. (New Coupling 2) Compute rank-based gating weight\n    # This gate smoothly emphasizes pairs with a higher rank of cost difference.\n    if delta_cost.numel() > 1:\n        ranks = delta_cost.argsort().argsort().float()\n        gating_weight = ranks / (delta_cost.numel() - 1)  # Maps to [0, 1]\n    else:\n        gating_weight = torch.ones_like(delta_cost)\n\n    # 6. Apply the rank-based gate to the core loss\n    instance_loss = gating_weight * core_loss\n\n    # Apply optional sample weights if provided\n    if 'weight' in batch and batch['weight'] is not None:\n        instance_loss = instance_loss * batch['weight']\n\n    # 7. Return the mean loss over the batch\n    return instance_loss.mean()"}, "static_ok": true, "static_reason": "", "dynamic_ok": true, "dynamic_reason": "ok", "loss_value": 0.3465735912322998, "grad_norm": 0.0}
{"generation": 8, "index": 6, "ir": {"name": "Rank-Gated Saturated Logistic Loss", "intuition": "This loss function creates a highly adaptive learning signal that is robust to cost outliers while focusing learning on pairs with a meaningful cost difference. The loss is gated by the rank of the cost difference, ensuring that only pairs with a sufficiently high relative cost gap contribute significantly to the training signal.\n\nInherited Ideas:\n- From Parent 0 (Sigmoid-Gated Rank-Weighted Logistic Loss), it inherits the core concept of a *gating mechanism* that modulates the loss. Instead of a sigmoid gate on z-scored costs, we use a more direct rank-based gate.\n- From Parent 1 (Adaptive Rank-Weighted Saturated Logistic Loss), it inherits the construction of a *saturated, z-score-normalized margin* using `tanh(zscore(delta_cost))`. This makes the margin adaptive to the batch's cost distribution and robust to extreme outliers, preventing them from creating an unbounded learning signal.\n\nNew Coupling Ideas:\n1.  **Rank-Based Gating**: A new gating mechanism is introduced where the loss is multiplied by `rank_gap(delta_cost)`. This directly uses the relative importance of a pair (its cost difference rank) to smoothly gate its contribution. Pairs with the smallest cost differences in the batch are effectively ignored (gate is near 0), while pairs with the largest cost differences are fully considered (gate is near 1). This is a more direct way to focus on significant pairs compared to using rank as a weight or margin component.\n2.  **Log-Probability Difference Clamping**: Before being used in the loss calculation, the log-probability difference `delta_logp` is clamped to a maximum value, `max_logp_diff`. This acts as a stability trick, preventing extremely confident but potentially incorrect model predictions (very large `delta_logp`) from completely zeroing out the loss. This ensures that even for pairs where the model is already very confident, a small gradient signal can still be maintained if the margin is not met, preventing model collapse or overconfidence.", "pseudocode": "1. Calculate the cost difference: delta_cost = cost(b) - cost(a).\n2. Calculate the log-probability difference: delta_logp = logp(a) - logp(b).\n3. (New Coupling 2) Clamp the log-probability difference to a maximum value for stability: clamped_delta_logp = clamp(delta_logp, max=max_logp_diff).\n4. (Inherited from Parent 1) Compute a saturated, z-score-normalized margin. Normalize delta_cost using z-scoring, apply a tanh function, and scale by a hyperparameter `gamma`: margin = gamma * tanh(zscore(delta_cost)).\n5. Compute the core logistic loss using the clamped log-probability difference and the adaptive margin: core_loss = softplus(margin - clamped_delta_logp).\n6. (Inherited from Parent 0, modified as New Coupling 1) Compute a rank-based gating weight. This weight is the normalized rank of the cost difference: rank_gate = rank_gap(delta_cost).\n7. The final loss for each pair is the product of the rank gate and the core loss: instance_loss = rank_gate * core_loss.\n8. The final loss is the average of instance_loss over the batch.", "hyperparams": {"gamma": 1.0, "max_logp_diff": 5.0, "eps": 1e-08}, "operators_used": ["softplus", "tanh", "zscore", "rank_gap", "clamp"], "implementation_hint": {"expects": ["cost_a", "cost_b", "log_prob_w", "log_prob_l"], "returns": "scalar"}, "code": "import torch\nimport torch.nn.functional as F\n\ndef generated_loss(batch, model_output, extra):\n    \"\"\"\n    Rank-Gated Saturated Logistic Loss.\n\n    This loss combines a saturated, z-score-normalized margin for robustness with a direct\n    rank-based gating mechanism to focus learning on pairs with significant cost differences.\n\n    - Inherits the `tanh(zscore(delta_cost))` margin from Parent 1 for robustness to outliers.\n    - Inherits the concept of a gating mechanism from Parent 0.\n    - New Coupling 1: Implements the gate directly as `rank_gap(delta_cost)`, smoothly\n      zeroing out the loss for pairs with the smallest cost differences in the batch.\n    - New Coupling 2: Clamps `delta_logp` to a maximum value to prevent extremely confident\n      model predictions from prematurely eliminating the learning signal, enhancing stability.\n    \"\"\"\n    # Read hyperparameters from the 'extra' dictionary\n    gamma = extra.get('gamma', 1.0)\n    max_logp_diff = extra.get('max_logp_diff', 5.0)\n    eps = extra.get('eps', 1e-8)\n\n    # Unpack tensors from the batch\n    cost_w = batch['cost_a']\n    cost_l = batch['cost_b']\n    logp_w = batch['log_prob_w']\n    logp_l = batch['log_prob_l']\n\n    # Sanity check: cost_w should be less than or equal to cost_l\n    assert torch.all(cost_w <= cost_l), \"Assumption failed: cost_a (winner) must be <= cost_b (loser)\"\n\n    # 1. Calculate cost and log-probability differences\n    delta_cost = cost_l - cost_w  # This will be >= 0\n    delta_logp = logp_w - logp_l  # We want this to be positive\n\n    # 2. (New Coupling 2) Clamp the log-probability difference for stability\n    clamped_delta_logp = torch.clamp(delta_logp, max=max_logp_diff)\n\n    # 3. (Inherited from Parent 1) Compute a saturated, z-score-normalized margin\n    if delta_cost.numel() > 1:\n        mean_delta_cost = delta_cost.mean()\n        std_delta_cost = delta_cost.std() + eps\n        normalized_delta_cost = (delta_cost - mean_delta_cost) / std_delta_cost\n    else:\n        normalized_delta_cost = torch.zeros_like(delta_cost)\n    \n    margin = gamma * torch.tanh(normalized_delta_cost)\n\n    # 4. Compute the core logistic loss\n    core_loss = F.softplus(margin - clamped_delta_logp)\n\n    # 5. (New Coupling 1) Compute a rank-based gating weight\n    if delta_cost.numel() > 1:\n        ranks = delta_cost.argsort().argsort().float()\n        rank_gate = ranks / (delta_cost.numel() - 1)  # Maps to [0, 1]\n    else:\n        rank_gate = torch.ones_like(delta_cost)\n\n    # 6. Apply the rank gate to the core loss\n    instance_loss = rank_gate * core_loss\n\n    # Apply optional sample weights if provided\n    if 'weight' in batch and batch['weight'] is not None:\n        instance_loss = instance_loss * batch['weight']\n\n    # 7. Return the mean loss over the batch\n    return instance_loss.mean()"}, "static_ok": true, "static_reason": "", "dynamic_ok": true, "dynamic_reason": "ok", "loss_value": 0.3465735912322998, "grad_norm": 0.0}
{"generation": 8, "index": 7, "ir": {"name": "Saturated Rank-Gated Logistic Loss", "intuition": "This loss function constructs a highly adaptive learning signal by gating the standard logistic loss with a saturated, rank-based weight. The goal is to focus learning on pairs with a significant cost difference relative to others in the batch, while also introducing a stable, bounded margin that adapts to the batch's cost distribution.\n\nInherited Ideas:\n- From Parent 0 (Sigmoid-Gated Rank-Weighted Logistic Loss), it inherits the idea of using a weight (`gating_weight`) that multiplies the core loss term. This allows for selective emphasis or de-emphasis of certain pairs.\n- From Parent 1 (Adaptive Rank-Weighted Saturated Logistic Loss), it inherits the idea of using a saturated, z-score-normalized margin: `gamma * tanh(zscore(delta_cost))`. This margin is robust to the scale of costs and stable against outliers.\n\nNew Coupling Ideas:\n1.  **Saturated Rank Gating**: Instead of using `sigmoid(zscore(delta_cost))` as a gate (like Parent 0) or `rank_gap` as a linear weight (like Parent 1), this child loss couples them. It computes a gate as `tanh(alpha * rank_gap(delta_cost))`. This creates a smooth, saturating gate based on a pair's *rank* within the batch. Pairs with low rank have a gate value near zero, which smoothly increases and then saturates for high-rank pairs. This focuses learning on the most meaningfully different pairs without letting the weight grow linearly, providing more stability than a simple rank weight.\n2.  **Decoupled Margin and Gate**: The margin (`gamma * tanh(zscore(delta_cost))`) and the gate (`tanh(alpha * rank_gap(delta_cost))`) are computed from different transformations of the cost difference. The margin adapts to the *distribution* of costs (via z-score), while the gate adapts to the *relative ordering* of costs (via rank_gap). This decoupling allows the loss to simultaneously be sensitive to both the statistical properties and the ordinal structure of the cost differences in the batch.", "pseudocode": "1. Calculate the cost difference: delta_cost = cost(b) - cost(a).\n2. Calculate the log-probability difference: delta_logp = logp(a) - logp(b).\n3. (Inherited from Parent 1) Normalize delta_cost using z-scoring: normalized_delta_cost = zscore(delta_cost).\n4. (Inherited from Parent 1) Compute a saturated, z-score-normalized margin: margin = gamma * tanh(normalized_delta_cost).\n5. Compute the core logistic loss using this adaptive margin: core_loss = softplus(margin - delta_logp).\n6. (Inherited from Parent 0, modified) Compute a rank-based value for gating: rank_value = rank_gap(delta_cost).\n7. (New Coupling 1) Create a saturated rank-based gate. Apply a scaled tanh function to the rank_value: gating_weight = tanh(alpha * rank_value).\n8. (New Coupling 2) The final loss for each pair is the product of the saturated rank gate and the core loss, which uses a distribution-aware margin: instance_loss = gating_weight * core_loss.\n9. The final loss is the average of instance_loss over the batch.", "hyperparams": {"gamma": 1.0, "alpha": 3.0, "eps": 1e-08}, "operators_used": ["softplus", "tanh", "zscore", "rank_gap"], "implementation_hint": {"expects": ["cost_a", "cost_b", "logp_a", "logp_b"], "returns": "scalar"}, "code": "import torch\nimport torch.nn.functional as F\n\ndef generated_loss(batch, model_output, extra):\n    \"\"\"\n    Saturated Rank-Gated Logistic Loss.\n\n    This loss combines a saturated, z-score normalized margin with a smooth, \n    saturating gate based on the rank of the cost difference.\n\n    - Inherits the `tanh(zscore(delta_cost))` margin from Parent 1 for a stable, \n      distribution-aware margin.\n    - Inherits the concept of a multiplicative gate on the core loss from Parent 0.\n    - New Coupling 1: A new 'Saturated Rank Gate' `tanh(alpha * rank_gap(delta_cost))` \n      is used. This smoothly focuses learning on higher-ranked pairs without linear scaling.\n    - New Coupling 2: The margin and gate are decoupled, with the margin responding to cost\n      distribution (z-score) and the gate responding to cost ordering (rank_gap).\n    \"\"\"\n    # Read hyperparameters from the 'extra' dictionary\n    gamma = extra.get('gamma', 1.0)\n    alpha = extra.get('alpha', 3.0)\n    eps = extra.get('eps', 1e-8)\n\n    # Unpack tensors from the batch\n    cost_w = batch['cost_a']\n    cost_l = batch['cost_b']\n    logp_w = batch['log_prob_w']\n    logp_l = batch['log_prob_l']\n\n    # Sanity check: cost_w should be less than or equal to cost_l\n    assert torch.all(cost_w <= cost_l), \"Assumption failed: cost_a (winner) must be <= cost_b (loser)\"\n\n    # 1. Calculate cost and log-probability differences\n    delta_cost = cost_l - cost_w  # This will be >= 0\n    delta_logp = logp_w - logp_l  # We want this to be positive\n\n    # 2. (Inherited from Parent 1) Create a stable, distribution-aware margin\n    if delta_cost.numel() > 1:\n        mean_delta_cost = delta_cost.mean()\n        std_delta_cost = delta_cost.std() + eps\n        normalized_delta_cost = (delta_cost - mean_delta_cost) / std_delta_cost\n    else:\n        normalized_delta_cost = torch.zeros_like(delta_cost)\n    margin = gamma * torch.tanh(normalized_delta_cost)\n\n    # 3. Compute the core logistic loss\n    core_loss = F.softplus(margin - delta_logp)\n\n    # 4. (New Coupling 1) Create a Saturated Rank Gate\n    if delta_cost.numel() > 1:\n        # rank_gap maps ranks to [0, 1]\n        ranks = delta_cost.argsort().argsort().float()\n        rank_value = ranks / (delta_cost.numel() - 1)\n    else:\n        rank_value = torch.ones_like(delta_cost)\n    \n    # The gate smoothly saturates from 0 to tanh(alpha) as rank increases\n    gating_weight = torch.tanh(alpha * rank_value)\n\n    # 5. (New Coupling 2) Apply the gate to the core loss\n    instance_loss = gating_weight * core_loss\n\n    # Apply optional sample weights if provided\n    if 'weight' in batch and batch['weight'] is not None:\n        instance_loss = instance_loss * batch['weight']\n\n    # 6. Return the mean loss over the batch\n    return instance_loss.mean()"}, "static_ok": true, "static_reason": "", "dynamic_ok": true, "dynamic_reason": "ok", "loss_value": 0.5210580229759216, "grad_norm": 0.0}
{"generation": 9, "index": 0, "ir": {"name": "Rank-Modulated Tanh-Saturated Logistic Loss", "intuition": "This loss function creates a highly adaptive and stable learning signal by combining rank-based modulation with a saturated, z-score-normalized margin. The goal is to produce a loss that is robust to the scale and distribution of costs within a batch, while focusing learning on pairs where the model's preference is misaligned with the ground truth, and gracefully handling pairs with very small or very large cost differences.\n\nInherited Ideas:\n- From both Parent 0 and Parent 1, it inherits the core idea of using `rank_gap(delta_cost)` to understand the relative importance of a preference pair within the batch. This makes the loss adaptive to the distribution of cost differences.\n- From both Parent 0 and Parent 1, it inherits the use of `zscore(delta_cost)` to normalize the cost differences. This makes the loss robust to the absolute scale of costs by considering the batch's mean and standard deviation.\n\nNew Coupling Ideas:\n1.  **Saturated Z-Score Margin (tanh(zscore))**: This idea is inspired by Parent 1's `tanh(zscore(delta_cost))` margin. We use this coupling to create a margin that is adaptive to the batch's statistical properties (due to `zscore`) and is also stable against extreme outliers (due to `tanh` saturation). This prevents single pairs with massive cost differences from dominating the learning signal by creating an unbounded margin.\n2.  **Rank-Modulated Loss (rank_gap * loss)**: Instead of using the rank to define the margin (as in Parent 0) or as a simple weight with an offset (as in Parent 1), we use it to directly modulate the entire logistic loss term. The loss for each pair is calculated as `rank_gap(delta_cost) * softplus(margin - delta_logp)`. This coupling ensures that the contribution of each pair to the total loss is directly proportional to its relative importance (its rank). Pairs with the smallest cost difference (rank_gap  0) will have their loss smoothly scaled down to near zero, effectively ignoring them, while the most significant pairs (rank_gap  1) contribute their full loss. This provides a smooth and principled way to focus the learning.", "pseudocode": "1. Calculate the cost difference: delta_cost = cost(b) - cost(a).\n2. Calculate the log-probability difference: delta_logp = logp(a) - logp(b).\n3. (Inherited from both Parents) Normalize the cost difference using z-scoring: normalized_delta_cost = zscore(delta_cost).\n4. (Inherited from both Parents) Compute a rank-based modulation factor from the raw delta_cost: rank_modulator = rank_gap(delta_cost).\n5. (New Coupling 1) Compute a saturated, z-score-normalized margin. Apply a `tanh` function to the normalized_delta_cost and scale it by a hyperparameter `gamma`: margin = gamma * tanh(normalized_delta_cost).\n6. Compute the core logistic loss using the adaptive margin: core_loss = softplus(margin - delta_logp).\n7. (New Coupling 2) The final loss for each pair is the product of the rank modulator and the core loss: instance_loss = rank_modulator * core_loss.\n8. The final loss is the average of instance_loss over the batch.", "hyperparams": {"gamma": 1.5, "eps": 1e-08}, "operators_used": ["softplus", "tanh", "zscore", "rank_gap"], "implementation_hint": {"expects": ["cost_a", "cost_b", "log_prob_w", "log_prob_l", "weight"], "returns": "scalar"}, "code": "import torch\nimport torch.nn.functional as F\n\ndef generated_loss(batch, model_output, extra):\n    \"\"\"\n    Rank-Modulated Tanh-Saturated Logistic Loss.\n\n    This loss combines a saturated, z-score-normalized margin with a rank-based\n    modulation of the final loss value.\n\n    - Inherits `rank_gap` from both parents to determine a pair's relative importance.\n    - Inherits `zscore` from both parents for batch-adaptive cost normalization.\n    - New Coupling 1: Uses a `tanh(zscore(delta_cost))` margin, making it robust\n      to both the scale of costs and extreme outliers.\n    - New Coupling 2: The entire `softplus` loss is directly modulated by the `rank_gap`,\n      smoothly scaling the contribution of each pair from 0 to 1 based on its importance.\n    \"\"\"\n    # Read hyperparameters from the 'extra' dictionary\n    gamma = extra.get('gamma', 1.5)\n    eps = extra.get('eps', 1e-8)\n\n    # Unpack tensors from the batch\n    cost_w = batch['cost_a']\n    cost_l = batch['cost_b']\n    logp_w = batch['log_prob_w']\n    logp_l = batch['log_prob_l']\n\n    # Sanity check: cost_w should be less than or equal to cost_l\n    # This ensures delta_cost is non-negative.\n    assert torch.all(cost_w <= cost_l), \"Assumption failed: cost_a (winner) must be <= cost_b (loser)\"\n\n    # 1. Calculate cost and log-probability differences\n    delta_cost = cost_l - cost_w\n    delta_logp = logp_w - logp_l\n\n    # Handle batch size of 1 to avoid division by zero in z-score and rank_gap\n    if delta_cost.numel() > 1:\n        # 2. (Inherited) Normalize delta_cost using z-scoring\n        mean_delta_cost = delta_cost.mean()\n        std_delta_cost = delta_cost.std() + eps\n        normalized_delta_cost = (delta_cost - mean_delta_cost) / std_delta_cost\n\n        # 3. (Inherited) Compute rank-based modulation factor\n        ranks = delta_cost.argsort().argsort().float()\n        rank_modulator = ranks / (delta_cost.numel() - 1)  # Maps to [0, 1]\n    else:\n        normalized_delta_cost = torch.zeros_like(delta_cost)\n        rank_modulator = torch.ones_like(delta_cost)\n\n    # 4. (New Coupling 1) Compute a saturated, z-score-normalized margin\n    margin = gamma * torch.tanh(normalized_delta_cost)\n\n    # 5. Compute the core logistic loss using the adaptive margin\n    core_loss = F.softplus(margin - delta_logp)\n\n    # 6. (New Coupling 2) Modulate the loss by the rank factor\n    instance_loss = rank_modulator * core_loss\n\n    # Apply optional sample weights if provided\n    weights = batch.get('weight')\n    if weights is not None:\n        instance_loss = instance_loss * weights\n\n    # 7. Return the mean loss over the batch\n    return instance_loss.mean()"}, "static_ok": true, "static_reason": "", "dynamic_ok": true, "dynamic_reason": "ok", "loss_value": 0.3465735912322998, "grad_norm": 0.0}
{"generation": 9, "index": 1, "ir": {"name": "Rank-Gated Saturated Logistic Loss", "intuition": "This loss function creates a robust and adaptive learning signal by gating the loss with a pair's rank within the batch and using a saturated, z-score normalized margin. The goal is to focus learning on pairs with a meaningful cost difference while maintaining stability against outliers.\n\nInherited Ideas:\n- From Parent 0 (Sigmoid-Gated Rank-Weighted Logistic Loss), it inherits the idea of a gating mechanism that modulates the final loss. Instead of using a sigmoid on z-scored costs, this child uses the rank directly as a smooth gate.\n- From Parent 1 (Adaptive Rank-Weighted Saturated Logistic Loss), it inherits the construction of a `saturated, z-score-normalized margin` using `gamma * tanh(zscore(delta_cost))`. This makes the margin adaptive to the batch's cost distribution and robust to extreme outlier cost differences.\n\nNew Coupling Ideas:\n1.  **Rank as a Direct Gate**: The final loss is directly weighted by `rank_gap(delta_cost)`. This acts as a smooth, linear gate. Pairs with the smallest cost difference (rank 0) are assigned a weight of 0, effectively ignoring them. As the relative cost difference increases, the weight grows linearly to 1. This coupling focuses the model's capacity on learning from pairs with more significant and reliable preference signals, as determined by their rank.\n2.  **Saturated Margin with Temperature**: The saturated margin from Parent 1 is modified with a temperature parameter `tau`: `gamma * tanh(zscore(delta_cost) / tau)`. This `tau` controls the steepness of the tanh saturation. A smaller `tau` makes the margin transition more sharply from -gamma to +gamma around the mean cost difference, creating a more decisive margin for pairs near the average. A larger `tau` creates a gentler transition, making the margin more sensitive to variations further from the mean.", "pseudocode": "1. Calculate the cost difference: delta_cost = cost(b) - cost(a).\n2. Calculate the log-probability difference: delta_logp = logp(a) - logp(b).\n3. (Inherited from Parent 1) Normalize the cost difference using z-scoring: normalized_delta_cost = zscore(delta_cost).\n4. (Inherited from Parent 0 and modified) Compute a rank-based gating weight from the raw delta_cost: rank_gate = rank_gap(delta_cost).\n5. (New Coupling 2) Compute a temperature-controlled, saturated, z-score-normalized margin. Apply a scaled `tanh` function to the normalized_delta_cost: margin = gamma * tanh(normalized_delta_cost / tau).\n6. Compute the core logistic loss using the adaptive margin: core_loss = softplus(margin - delta_logp).\n7. (New Coupling 1) The final loss for each pair is the product of the rank gate and the core loss: instance_loss = rank_gate * core_loss.\n8. The final loss is the average of instance_loss over the batch.", "hyperparams": {"gamma": 1.5, "tau": 1.0, "eps": 1e-08}, "operators_used": ["softplus", "tanh", "zscore", "rank_gap"], "implementation_hint": {"expects": ["cost_a", "cost_b", "logp_a", "logp_b"], "returns": "scalar"}, "code": "import torch\nimport torch.nn.functional as F\n\ndef generated_loss(batch, model_output, extra):\n    \"\"\"\n    Rank-Gated Saturated Logistic Loss.\n\n    This loss uses a rank-based gate to modulate a logistic loss term that has a\n    saturated, z-score normalized margin. The goal is to focus learning on pairs with\n    meaningful cost differences while being robust to outliers.\n\n    - Inherits the saturated `tanh(zscore(delta_cost))` margin from Parent 1.\n    - Inherits the concept of a gate on the loss from Parent 0.\n    - New Coupling 1: Uses `rank_gap(delta_cost)` as a direct, smooth gate on the final loss,\n      effectively ignoring pairs with the smallest cost differences.\n    - New Coupling 2: Introduces a temperature `tau` to the margin calculation,\n      `tanh(zscore(delta_cost) / tau)`, to control the saturation sharpness.\n    \"\"\"\n    # Read hyperparameters from the 'extra' dictionary\n    gamma = extra.get('gamma', 1.5)\n    tau = extra.get('tau', 1.0)\n    eps = extra.get('eps', 1e-8)\n\n    # Unpack tensors from the batch\n    cost_w = batch['cost_a']\n    cost_l = batch['cost_b']\n    logp_w = batch['log_prob_w']\n    logp_l = batch['log_prob_l']\n\n    # Sanity check: cost_w should be less than or equal to cost_l\n    assert torch.all(cost_w <= cost_l), \"Assumption failed: cost_a (winner) must be <= cost_b (loser)\"\n\n    # 1. Calculate cost and log-probability differences\n    delta_cost = cost_l - cost_w  # This will be >= 0\n    delta_logp = logp_w - logp_l  # We want this to be positive\n\n    # 2. (Inherited from Parent 1) Normalize delta_cost using z-scoring\n    if delta_cost.numel() > 1:\n        mean_delta_cost = delta_cost.mean()\n        std_delta_cost = delta_cost.std() + eps\n        normalized_delta_cost = (delta_cost - mean_delta_cost) / std_delta_cost\n    else:\n        normalized_delta_cost = torch.zeros_like(delta_cost)\n\n    # 3. (Inherited from Parent 0, modified) Compute rank-based gating weight\n    if delta_cost.numel() > 1:\n        ranks = delta_cost.argsort().argsort().float()\n        # rank_gap maps ranks to [0, 1]\n        rank_gate = ranks / (delta_cost.numel() - 1)\n    else:\n        # For a single element, the gate is 1 (no relative ranking possible)\n        rank_gate = torch.ones_like(delta_cost)\n\n    # 4. (New Coupling 2) Compute a temperature-controlled, saturated, z-score-normalized margin\n    # This margin is adaptive to batch stats and stable against extreme outliers.\n    margin = gamma * torch.tanh(normalized_delta_cost / tau)\n\n    # 5. Compute the core logistic loss using the adaptive margin\n    core_loss = F.softplus(margin - delta_logp)\n\n    # 6. (New Coupling 1) Apply the rank gate to the core loss\n    # This focuses learning on pairs with higher relative cost differences.\n    instance_loss = rank_gate * core_loss\n\n    # Apply optional sample weights if provided\n    if 'weight' in batch and batch['weight'] is not None:\n        instance_loss = instance_loss * batch['weight']\n\n    # 7. Return the mean loss over the batch\n    return instance_loss.mean()"}, "static_ok": true, "static_reason": "", "dynamic_ok": true, "dynamic_reason": "ok", "loss_value": 0.3465735912322998, "grad_norm": 0.0}
{"generation": 9, "index": 2, "ir": {"name": "Saturated Sigmoid-Gated Logistic Loss", "intuition": "This loss function creates a stable and adaptive learning signal by combining a sigmoid-based gating mechanism with a saturated, z-score-normalized margin. The goal is to focus learning on pairs where the model's preference is misaligned with the cost-based ground truth, while being robust to the scale of costs and preventing outlier pairs from dominating the gradient.\n\nInherited Ideas:\n- From Parent 0 (Sigmoid-Gated Rank-Weighted Logistic Loss), it inherits the idea of using a `sigmoid(zscore(delta_cost))` gate. This acts as a smooth, adaptive filter, gracefully down-weighting pairs with cost differences below the batch mean and focusing learning on pairs that are surprisingly far apart in cost given the batch context.\n- From Parent 1 (Adaptive Rank-Weighted Saturated Logistic Loss), it inherits the concept of a saturated margin, specifically `tanh(zscore(delta_cost))`. This makes the margin adaptive to the batch's statistical properties (mean and standard deviation) and robust to the absolute scale of costs. The `tanh` function prevents extreme outlier cost differences from creating an unbounded margin, leading to greater numerical stability.\n\nNew Coupling Ideas:\n1. **Gated Margin Construction**: Instead of using the sigmoid gate to weight the final loss, it is used to directly modulate the margin itself. The margin is defined as `gamma * sigmoid(zscore(delta_cost))`. This creates a margin that smoothly scales from 0 to `gamma` based on how a pair's cost difference compares to the batch average. Pairs with small or negative z-scores get a small margin, requiring only a small log-probability difference, while pairs with large positive z-scores get a large margin, demanding a confident preference from the model.\n2. **Saturated Logistic Argument**: The `tanh` function is applied to the entire argument of the logistic loss, `tanh(margin - delta_logp)`. This novel stability trick ensures that the value inside the `softplus` function is bounded between -1 and 1. It prevents extremely large positive or negative values of `margin - delta_logp` (which could occur if `delta_logp` is an outlier) from causing numerical instability or generating excessively large gradients, effectively clipping the gradient magnitude in a smooth, differentiable way.", "pseudocode": "1. Calculate the cost difference: delta_cost = cost(b) - cost(a).\n2. Calculate the log-probability difference: delta_logp = logp(a) - logp(b).\n3. (Inherited from Parents 0 & 1) Normalize the cost difference using z-scoring: normalized_delta_cost = zscore(delta_cost).\n4. (New Coupling 1) Compute a gated margin. Apply a sigmoid function to the normalized cost difference and scale by a hyperparameter `gamma`: margin = gamma * sigmoid(normalized_delta_cost).\n5. (New Coupling 2) Compute a saturated logistic argument. Calculate the difference between the margin and the log-probability difference, then apply the `tanh` function to bound the result: logistic_argument = tanh(margin - delta_logp).\n6. Compute the core loss using the saturated argument: instance_loss = softplus(logistic_argument).\n7. The final loss is the average of instance_loss over the batch.", "hyperparams": {"gamma": 2.0, "eps": 1e-08}, "operators_used": ["softplus", "sigmoid", "tanh", "zscore"], "implementation_hint": {"expects": ["cost_a", "cost_b", "logp_a", "logp_b"], "returns": "scalar"}, "code": "import torch\nimport torch.nn.functional as F\n\ndef generated_loss(batch, model_output, extra):\n    \"\"\"\n    Saturated Sigmoid-Gated Logistic Loss.\n\n    This loss combines a sigmoid gate on z-scored costs to create an adaptive margin,\n    and uses a tanh function to saturate the logistic loss argument for stability.\n\n    - Inherits `sigmoid(zscore(delta_cost))` gating from Parent 0.\n    - Inherits `tanh` saturation and `zscore` normalization from Parent 1.\n    - New Coupling 1: The margin is `gamma * sigmoid(zscore(delta_cost))`, directly\n      coupling the gate with margin creation for a smooth, adaptive target.\n    - New Coupling 2: The entire logistic argument `(margin - delta_logp)` is passed\n      through `tanh` before `softplus`, bounding it to [-1, 1] for stability.\n    \"\"\"\n    # Read hyperparameters from the 'extra' dictionary\n    gamma = extra.get('gamma', 2.0)\n    eps = extra.get('eps', 1e-8)\n\n    # Unpack tensors from the batch\n    cost_w = batch['cost_a']\n    cost_l = batch['cost_b']\n    logp_w = batch['log_prob_w']\n    logp_l = batch['log_prob_l']\n\n    # Sanity check: cost_w should be less than or equal to cost_l\n    assert torch.all(cost_w <= cost_l), \"Assumption failed: cost_a (winner) must be <= cost_b (loser)\"\n\n    # 1. Calculate cost and log-probability differences\n    delta_cost = cost_l - cost_w  # This will be >= 0\n    delta_logp = logp_w - logp_l  # We want this to be positive\n\n    # 2. (Inherited) Normalize delta_cost using z-scoring\n    if delta_cost.numel() > 1:\n        mean_delta_cost = delta_cost.mean()\n        std_delta_cost = delta_cost.std() + eps\n        normalized_delta_cost = (delta_cost - mean_delta_cost) / std_delta_cost\n    else:\n        normalized_delta_cost = torch.zeros_like(delta_cost)\n\n    # 3. (New Coupling 1) Compute a gated margin using sigmoid\n    # This margin smoothly scales from 0 to gamma based on the z-scored cost difference.\n    margin = gamma * torch.sigmoid(normalized_delta_cost)\n\n    # 4. (New Coupling 2) Compute a saturated logistic argument using tanh\n    # This bounds the input to softplus, preventing extreme gradients and instability.\n    logistic_argument = torch.tanh(margin - delta_logp)\n\n    # 5. Compute the final instance loss\n    instance_loss = F.softplus(logistic_argument)\n\n    # Apply optional sample weights if provided\n    if 'weight' in batch and batch['weight'] is not None:\n        instance_loss = instance_loss * batch['weight']\n\n    # 6. Return the mean loss over the batch\n    return instance_loss.mean()"}, "static_ok": true, "static_reason": "", "dynamic_ok": true, "dynamic_reason": "ok", "loss_value": 1.1447601318359375, "grad_norm": 0.0}
{"generation": 9, "index": 3, "ir": {"name": "Saturated Rank-Gated Logistic Loss", "intuition": "This loss function creates a robust and adaptive learning signal by gating the standard logistic loss with a saturated rank-based weight. The goal is to focus learning on pairs with significant cost differences relative to the batch, while ensuring stability by saturating the margin for extreme outliers and preventing the loss from vanishing completely for the least significant pairs.\n\nInherited Ideas:\n- From Parent 0 (Sigmoid-Gated Rank-Weighted Logistic Loss), it inherits the idea of using a gating mechanism on the core loss. Instead of a sigmoid gate on z-scored costs, this child uses the rank itself as the primary gate, directly tying the loss magnitude to the pair's relative importance.\n- From Parent 1 (Adaptive Rank-Weighted Saturated Logistic Loss), it inherits the concept of a saturated, z-score-normalized margin: `gamma * tanh(zscore(delta_cost))`. This makes the target preference gap (`margin`) adaptive to the batch's cost distribution while being robust to extreme outliers.\n\nNew Coupling Ideas:\n1.  **Rank-Gated Loss**: The core logistic loss, `softplus(margin - delta_logp)`, is directly multiplied by `rank_gap(delta_cost)`. This acts as a 'rank gate', where the loss signal is linearly scaled by the rank of the cost difference. This is a simpler, more direct coupling than the sigmoid gate in Parent 0, focusing learning proportionally on more important pairs.\n2.  **Gated Offset for Stability**: A small constant `offset` is added to the rank gate (`rank_weight + offset`). The entire logistic loss is then multiplied by this gated offset: `(rank_weight + offset) * core_loss`. This ensures that even pairs with the smallest cost difference (and thus a rank weight near zero) still contribute a small, non-zero gradient, preventing the model from completely ignoring them and improving numerical stability.", "pseudocode": "1. Calculate the cost difference: delta_cost = cost(b) - cost(a).\n2. Calculate the log-probability difference: delta_logp = logp(a) - logp(b).\n3. (Inherited from Parent 1) Compute a saturated, z-score-normalized margin. Normalize delta_cost using z-scoring, apply a tanh function, and scale by a hyperparameter `gamma`: margin = gamma * tanh(zscore(delta_cost)).\n4. (Inherited from Parent 0, modified) Compute a rank-based gating weight from the raw delta_cost: rank_weight = rank_gap(delta_cost).\n5. Compute the core logistic loss using the adaptive margin: core_loss = softplus(margin - delta_logp).\n6. (New Coupling 1 & 2) Create a stabilized rank gate by adding a small `offset` to the rank weight: stabilized_gate = rank_weight + offset.\n7. Apply the stabilized rank gate to the core loss: instance_loss = stabilized_gate * core_loss.\n8. The final loss is the average of instance_loss over the batch.", "hyperparams": {"gamma": 1.5, "offset": 0.05, "eps": 1e-08}, "operators_used": ["softplus", "tanh", "zscore", "rank_gap"], "implementation_hint": {"expects": ["cost_a", "cost_b", "logp_a", "logp_b", "weight"], "returns": "scalar"}, "code": "import torch\nimport torch.nn.functional as F\n\ndef generated_loss(batch, model_output, extra):\n    \"\"\"\n    Saturated Rank-Gated Logistic Loss.\n\n    This loss combines a saturated, z-score normalized margin with a rank-based\n    gating mechanism, stabilized by a small offset.\n\n    - Inherits a saturated z-score margin `tanh(zscore(delta_cost))` from Parent 1 for a stable, adaptive target.\n    - Inherits the idea of gating from Parent 0, but implements it directly with `rank_gap`.\n    - New Coupling 1: The core loss is gated by the rank weight, `rank_weight * core_loss`.\n    - New Coupling 2: A small `offset` is added to the rank gate, `(rank_weight + offset)`, to ensure\n      all pairs contribute a non-zero gradient, improving stability.\n    \"\"\"\n    # Read hyperparameters from the 'extra' dictionary\n    gamma = extra.get('gamma', 1.5)\n    offset = extra.get('offset', 0.05)\n    eps = extra.get('eps', 1e-8)\n\n    # Unpack tensors from the batch\n    cost_w = batch['cost_a']\n    cost_l = batch['cost_b']\n    logp_w = batch['log_prob_w']\n    logp_l = batch['log_prob_l']\n\n    # Sanity check: cost_w should be less than or equal to cost_l\n    assert torch.all(cost_w <= cost_l), \"Assumption failed: cost_a (winner) must be <= cost_b (loser)\"\n\n    # 1. Calculate cost and log-probability differences\n    delta_cost = cost_l - cost_w  # This will be >= 0\n    delta_logp = logp_w - logp_l  # We want this to be positive\n\n    # Handle batch size of 1 to prevent NaN from z-score and rank_gap\n    if delta_cost.numel() > 1:\n        # (Inherited from Parent 1) Normalize delta_cost using z-scoring\n        mean_delta_cost = delta_cost.mean()\n        std_delta_cost = delta_cost.std() + eps\n        normalized_delta_cost = (delta_cost - mean_delta_cost) / std_delta_cost\n        \n        # (Inherited from Parent 0, modified) Compute rank-based gating weight\n        ranks = delta_cost.argsort().argsort().float()\n        rank_weight = ranks / (delta_cost.numel() - 1)  # Maps to [0, 1]\n    else:\n        normalized_delta_cost = torch.zeros_like(delta_cost)\n        rank_weight = torch.ones_like(delta_cost)\n\n    # 2. (Inherited from Parent 1) Compute a saturated, z-score-normalized margin\n    # This margin is adaptive to batch stats and stable against extreme outliers.\n    margin = gamma * torch.tanh(normalized_delta_cost)\n\n    # 3. Compute the core logistic loss using the adaptive margin\n    core_loss = F.softplus(margin - delta_logp)\n\n    # 4. (New Coupling 1 & 2) Create a stabilized rank gate and apply it to the loss\n    # The offset ensures a minimal gradient even for pairs with rank_weight = 0.\n    stabilized_gate = rank_weight + offset\n    instance_loss = stabilized_gate * core_loss\n\n    # Apply optional sample weights if provided\n    if 'weight' in batch and batch['weight'] is not None:\n        instance_loss = instance_loss * batch['weight']\n\n    # 5. Return the mean loss over the batch\n    return instance_loss.mean()"}, "static_ok": true, "static_reason": "", "dynamic_ok": true, "dynamic_reason": "ok", "loss_value": 0.3812309503555298, "grad_norm": 0.0}
{"generation": 9, "index": 4, "ir": {"name": "Saturated Sigmoid-Gated Logistic Loss with Rank-Adaptive Margin", "intuition": "This loss function creates a highly adaptive and stable learning signal by combining several normalization and gating techniques. The primary goal is to focus learning on misaligned preferences (where `logp(a) - logp(b)` is low) while being robust to the scale and distribution of costs within a batch and preventing outlier pairs from dominating.\n\nInherited Ideas:\n- From Parent 0 (Sigmoid-Gated Rank-Weighted Logistic Loss), it inherits the idea of a `sigmoid(zscore(delta_cost))` gate. This acts as a smooth, adaptive filter, gracefully de-emphasizing pairs with cost differences that are below the batch average, thereby focusing on the most statistically significant preference signals.\n- From Parent 1 (Adaptive Rank-Weighted Saturated Logistic Loss), it inherits the idea of a saturated margin, specifically `tanh(zscore(delta_cost))`. This makes the target preference margin robust to the absolute scale of costs and prevents extreme outlier pairs from creating an unbounded, potentially destabilizing margin.\n\nNew Coupling Ideas:\n1. **Rank-Adaptive Margin Scaling**: A new margin is constructed by coupling the saturated z-score cost with a rank-based scaling factor. The margin is `gamma * rank_gap(delta_cost) * tanh(zscore(delta_cost))`. This unique coupling ensures that the margin is not only bounded and normalized (from `tanh(zscore)`) but also directly proportional to the relative importance of the pair within the batch (from `rank_gap`). A pair with a high cost difference relative to its batch peers will have both a high rank-gap and a high tanh value, resulting in a strong, well-defined margin. Conversely, a pair with a low relative cost difference will have its margin gracefully shrink towards zero.\n2. **Log-Sigmoid Core Loss**: Instead of the standard `softplus` logistic loss, this child uses a `logsigmoid` formulation: `logsigmoid(delta_logp - margin)`. This is mathematically equivalent to `-softplus(margin - delta_logp)` but can offer better numerical stability, especially when the argument `margin - delta_logp` becomes very large. This change ensures the core loss calculation is robust while maintaining the same underlying logistic objective.", "pseudocode": "1. Calculate the cost difference: delta_cost = cost(b) - cost(a).\n2. Calculate the log-probability difference: delta_logp = logp(a) - logp(b).\n3. (Inherited from Parents 0 & 1) Normalize the cost difference using z-scoring: normalized_delta_cost = zscore(delta_cost).\n4. Compute the rank-based adaptive weight from the raw delta_cost: rank_weight = rank_gap(delta_cost).\n5. (Inherited from Parent 1) Compute a saturated, normalized cost term: saturated_cost = tanh(normalized_delta_cost).\n6. (New Coupling 1) Compute a rank-adaptive margin by coupling the rank weight and the saturated cost: margin = gamma * rank_weight * saturated_cost.\n7. (New Coupling 2) Compute the core loss using a numerically stable log-sigmoid formulation: core_loss = -logsigmoid(delta_logp - margin).\n8. (Inherited from Parent 0) Compute a sigmoid-based gating weight on the normalized cost: gating_weight = sigmoid(normalized_delta_cost).\n9. The final loss for each pair is the product of the gating weight and the core loss: instance_loss = gating_weight * core_loss.\n10. The final loss is the average of instance_loss over the batch.", "hyperparams": {"gamma": 2.0, "eps": 1e-08}, "operators_used": ["logsigmoid", "sigmoid", "tanh", "zscore", "rank_gap"], "implementation_hint": {"expects": ["cost_a", "cost_b", "logp_a", "logp_b", "weight"], "returns": "scalar"}, "code": "import torch\nimport torch.nn.functional as F\n\ndef generated_loss(batch, model_output, extra):\n    \"\"\"\n    Saturated Sigmoid-Gated Logistic Loss with Rank-Adaptive Margin.\n\n    This loss combines multiple normalization and gating techniques for a robust\n    and adaptive preference signal.\n\n    - Inherits `sigmoid(zscore(delta_cost))` gating from Parent 0 to focus on pairs\n      with above-average cost differences.\n    - Inherits `tanh(zscore(delta_cost))` from Parent 1 to create a bounded,\n      normalized representation of the cost difference, preventing outlier dominance.\n    - New Coupling 1: Creates a rank-adaptive margin by coupling the rank-gap with the\n      saturated z-score: `margin = gamma * rank_gap * tanh(zscore(delta_cost))`. This\n      makes the margin sensitive to both batch statistics and relative rank.\n    - New Coupling 2: Uses a numerically stable `logsigmoid` formulation for the core\n      logistic loss, replacing the common `softplus`.\n    \"\"\"\n    # Read hyperparameters from the 'extra' dictionary\n    gamma = extra.get('gamma', 2.0)\n    eps = extra.get('eps', 1e-8)\n\n    # Unpack tensors from the batch\n    cost_w = batch['cost_a']\n    cost_l = batch['cost_b']\n    logp_w = batch['log_prob_w']\n    logp_l = batch['log_prob_l']\n\n    # Sanity check: cost_w should be less than or equal to cost_l\n    assert torch.all(cost_w <= cost_l), \"Assumption failed: cost_a (winner) must be <= cost_b (loser)\"\n\n    # 1. Calculate cost and log-probability differences\n    delta_cost = cost_l - cost_w  # This will be >= 0\n    delta_logp = logp_w - logp_l  # We want this to be positive\n\n    # 2. (Inherited from Parents 0 & 1) Normalize delta_cost using z-scoring\n    if delta_cost.numel() > 1:\n        mean_delta_cost = delta_cost.mean()\n        std_delta_cost = delta_cost.std() + eps\n        normalized_delta_cost = (delta_cost - mean_delta_cost) / std_delta_cost\n    else:\n        normalized_delta_cost = torch.zeros_like(delta_cost)\n\n    # 3. Compute rank-based adaptive weight from raw delta_cost\n    if delta_cost.numel() > 1:\n        ranks = delta_cost.argsort().argsort().float()\n        rank_weight = ranks / (delta_cost.numel() - 1)  # Maps to [0, 1]\n    else:\n        rank_weight = torch.ones_like(delta_cost)\n\n    # 4. (Inherited from Parent 1) Compute a saturated, normalized cost term\n    saturated_cost = torch.tanh(normalized_delta_cost)\n\n    # 5. (New Coupling 1) Compute a rank-adaptive margin\n    # The margin is scaled by both the rank and the saturated z-score of the cost difference.\n    margin = gamma * rank_weight * saturated_cost\n\n    # 6. (New Coupling 2) Compute the core loss using a stable log-sigmoid formulation\n    # This is equivalent to softplus(margin - delta_logp).\n    core_loss = -F.logsigmoid(delta_logp - margin)\n\n    # 7. (Inherited from Parent 0) Compute sigmoid-based gating weight\n    # This gate smoothly emphasizes pairs with above-average cost differences.\n    gating_weight = torch.sigmoid(normalized_delta_cost)\n\n    # 8. Combine gate and core loss\n    instance_loss = gating_weight * core_loss\n\n    # Apply optional sample weights if provided\n    if 'weight' in batch and batch['weight'] is not None:\n        instance_loss = instance_loss * batch['weight']\n\n    # 9. Return the mean loss over the batch\n    return instance_loss.mean()"}, "static_ok": true, "static_reason": "", "dynamic_ok": true, "dynamic_reason": "ok", "loss_value": 0.3465735912322998, "grad_norm": 0.0}
{"generation": 9, "index": 5, "ir": {"name": "Sigmoid-Gated Saturated Logistic Loss", "intuition": "This loss function creates a highly adaptive and stable learning signal by combining a saturated margin with a sigmoid-based gating mechanism, both driven by batch-normalized cost differences. The goal is to produce a loss that is robust to the scale of costs, stable against outlier pairs, and focuses learning on pairs with a meaningful cost difference relative to the batch context.\n\nInherited Ideas:\n- From Parent 0 (Sigmoid-Gated Rank-Weighted Logistic Loss), it inherits the idea of using a `sigmoid(zscore(delta_cost))` gate. This mechanism adaptively weights the loss, smoothly de-prioritizing pairs with cost differences below the batch mean and focusing on those with above-average differences.\n- From Parent 1 (Adaptive Rank-Weighted Saturated Logistic Loss), it inherits the idea of a saturated margin, `tanh(zscore(delta_cost))`. This creates a margin that is robust to the absolute scale of costs and prevents outlier pairs with extreme cost differences from creating an unbounded margin, leading to greater numerical stability.\n\nNew Coupling Ideas:\n1.  **Dual-Use Normalization**: The `zscore(delta_cost)` term is used for two distinct purposes simultaneously: it drives both the saturated margin (`tanh`) and the adaptive gate (`sigmoid`). This creates a tight coupling where the mechanism that sets the learning target (the margin) is directly linked to the mechanism that determines the importance of that target (the gate). This ensures consistency: pairs with a high, saturated margin will also receive a high gating weight.\n2.  **Log-Probability Difference Clamping**: A stability trick is introduced by clamping the log-probability difference, `delta_logp`, to a minimum value of `-beta`. This prevents the `softplus` argument from becoming excessively large and positive if the model is extremely confident but wrong (i.e., `logp(a)` is much smaller than `logp(b)`). This clamping bounds the maximum loss for any single mispredicted sample, preventing a single outlier from dominating the batch gradient and improving overall training stability.", "pseudocode": "1. Calculate the cost difference: delta_cost = cost(b) - cost(a).\n2. Calculate the log-probability difference: delta_logp = logp(a) - logp(b).\n3. (New Coupling 2) Clamp the log-probability difference to a minimum value for stability: clamped_delta_logp = clamp(delta_logp, min=-beta).\n4. (Shared Inheritance) Normalize the cost difference using z-scoring: normalized_delta_cost = zscore(delta_cost).\n5. (Inherited from Parent 1) Compute a saturated, z-score-normalized margin. Apply a `tanh` function to the normalized_delta_cost and scale it by `gamma`: margin = gamma * tanh(normalized_delta_cost).\n6. Compute the core logistic loss using the adaptive margin and the clamped log-probability difference: core_loss = softplus(margin - clamped_delta_logp).\n7. (Inherited from Parent 0 & New Coupling 1) Compute a sigmoid-based gating weight using the same normalized_delta_cost: gating_weight = sigmoid(normalized_delta_cost).\n8. The final loss for each pair is the product of the gating weight and the core loss: instance_loss = gating_weight * core_loss.\n9. The final loss is the average of instance_loss over the batch.", "hyperparams": {"gamma": 1.0, "beta": 10.0, "eps": 1e-08}, "operators_used": ["softplus", "sigmoid", "tanh", "zscore", "clamp"], "implementation_hint": {"expects": ["cost_a", "cost_b", "logp_a", "logp_b"], "returns": "scalar"}, "code": "import torch\nimport torch.nn.functional as F\n\ndef generated_loss(batch, model_output, extra):\n    \"\"\"\n    Sigmoid-Gated Saturated Logistic Loss.\n\n    This loss combines a saturated margin with a sigmoid gate, both driven by z-scored costs.\n    The goal is a stable loss that focuses on pairs with meaningful cost differences.\n\n    - Inherits `sigmoid(zscore(delta_cost))` gating from Parent 0 to adaptively weight the loss.\n    - Inherits `tanh(zscore(delta_cost))` margin from Parent 1 for a stable, bounded target.\n    - New Coupling 1: The same `zscore(delta_cost)` term drives both the margin and the gate,\n      creating a consistent signal for both the target and its importance.\n    - New Coupling 2: Clamps `delta_logp` to prevent extremely large loss values from\n      individual mispredictions, enhancing training stability.\n    \"\"\"\n    # Read hyperparameters from the 'extra' dictionary\n    gamma = extra.get('gamma', 1.0)\n    beta = extra.get('beta', 10.0)\n    eps = extra.get('eps', 1e-8)\n\n    # Unpack tensors from the batch\n    cost_w = batch['cost_a']\n    cost_l = batch['cost_b']\n    logp_w = batch['log_prob_w']\n    logp_l = batch['log_prob_l']\n\n    # Sanity check: cost_w should be less than or equal to cost_l\n    assert torch.all(cost_w <= cost_l), \"Assumption failed: cost_a (winner) must be <= cost_b (loser)\"\n\n    # 1. Calculate cost and log-probability differences\n    delta_cost = cost_l - cost_w  # This will be >= 0\n    delta_logp = logp_w - logp_l  # We want this to be positive\n\n    # 2. (New Coupling 2) Clamp delta_logp for stability\n    # Prevents loss from exploding if model is confidently wrong (large negative delta_logp).\n    clamped_delta_logp = torch.clamp(delta_logp, min=-beta)\n\n    # 3. (Shared Inheritance) Normalize delta_cost using z-scoring\n    if delta_cost.numel() > 1:\n        mean_delta_cost = delta_cost.mean()\n        std_delta_cost = delta_cost.std() + eps\n        normalized_delta_cost = (delta_cost - mean_delta_cost) / std_delta_cost\n    else:\n        normalized_delta_cost = torch.zeros_like(delta_cost)\n\n    # 4. (Inherited from Parent 1) Compute a saturated, z-score-normalized margin\n    margin = gamma * torch.tanh(normalized_delta_cost)\n\n    # 5. Compute the core logistic loss using the adaptive margin and clamped delta_logp\n    core_loss = F.softplus(margin - clamped_delta_logp)\n\n    # 6. (Inherited from Parent 0 & New Coupling 1) Compute sigmoid-based gating weight\n    # This gate smoothly emphasizes pairs with above-average cost differences.\n    gating_weight = torch.sigmoid(normalized_delta_cost)\n\n    # 7. Combine gate and core loss\n    instance_loss = gating_weight * core_loss\n\n    # Apply optional sample weights if provided\n    if 'weight' in batch and batch['weight'] is not None:\n        instance_loss = instance_loss * batch['weight']\n\n    # 8. Return the mean loss over the batch\n    return instance_loss.mean()"}, "static_ok": true, "static_reason": "", "dynamic_ok": true, "dynamic_reason": "ok", "loss_value": 0.3465735912322998, "grad_norm": 0.0}
{"generation": 9, "index": 6, "ir": {"name": "Saturated Rank-Gated Logistic Loss", "intuition": "This loss function creates a stable and adaptive learning signal by gating the standard logistic loss with a saturated, rank-based weight. The goal is to focus learning on pairs with a meaningful cost difference relative to the rest of the batch, while preventing extreme cost outliers from dominating the training signal and providing a stable learning target.\n\nInherited Ideas:\n- From Parent 0 (Sigmoid-Gated Rank-Weighted Logistic Loss), it inherits the idea of using a gating mechanism to modulate the final loss. Instead of gating with sigmoid(z-score), it uses a new rank-based gate.\n- From Parent 1 (Adaptive Rank-Weighted Saturated Logistic Loss), it inherits the idea of using a saturated (`tanh`) margin based on z-scored cost differences. This ensures the learning target (`margin`) is robust to the scale of costs and stable against outliers.\n\nNew Coupling Ideas:\n1. **Saturated Rank-Based Gating**: A new gating weight is constructed by coupling `rank_gap` with `tanh`. The gate is `tanh(beta * rank_gap(delta_cost))`. This creates a smooth gate that is 0 for the pair with the smallest cost difference and smoothly saturates towards 1 for pairs with higher relative cost differences. Unlike a linear rank weight, this `tanh` saturation prevents the highest-ranked pairs from having disproportionately high influence, providing a more balanced focus on all pairs with a non-minimal cost gap.\n2. **Log-Probability Difference Clipping**: The log-probability difference, `delta_logp`, is clamped to a maximum value `max_logp_diff`. This is a stability trick that prevents the model from developing overly confident and potentially unstable preferences. By capping the reward `delta_logp` can achieve for a single pair, it encourages the model to distribute its probability mass more evenly and avoids excessively large gradients that can arise from extreme `delta_logp` values, especially early in training.", "pseudocode": "1. Calculate the cost difference: delta_cost = cost(b) - cost(a).\n2. Calculate the log-probability difference: delta_logp = logp(a) - logp(b).\n3. (New Coupling 2 - Stability Trick) Clip the log-probability difference to a maximum value: clipped_delta_logp = clamp(delta_logp, max=max_logp_diff).\n4. (Inherited from Parent 1) Normalize the cost difference using z-scoring: normalized_delta_cost = zscore(delta_cost).\n5. (Inherited from Parent 1) Compute a saturated, z-score-normalized margin: margin = gamma * tanh(normalized_delta_cost).\n6. Compute the core logistic loss using the saturated margin and clipped log-probability difference: core_loss = softplus(margin - clipped_delta_logp).\n7. (Inherited from Parent 0 & New Coupling 1) Compute a saturated, rank-based gating weight. First, compute the rank gap: rank_weight = rank_gap(delta_cost). Then, apply a scaled tanh function: gating_weight = tanh(beta * rank_weight).\n8. The final loss for each pair is the product of the gating weight and the core loss: instance_loss = gating_weight * core_loss.\n9. The final loss is the average of instance_loss over the batch.", "hyperparams": {"gamma": 1.0, "beta": 3.0, "max_logp_diff": 5.0, "eps": 1e-08}, "operators_used": ["softplus", "tanh", "zscore", "rank_gap", "clamp"], "implementation_hint": {"expects": ["cost_a", "cost_b", "logp_a", "logp_b"], "returns": "scalar"}, "code": "import torch\nimport torch.nn.functional as F\n\ndef generated_loss(batch, model_output, extra):\n    \"\"\"\n    Saturated Rank-Gated Logistic Loss.\n\n    This loss uses a saturated rank-based gate to modulate a logistic loss term\n    that has a saturated margin and a clipped log-probability difference.\n\n    - Inherits `tanh(zscore(delta_cost))` margin from Parent 1 for a stable learning target.\n    - Inherits the concept of a final loss gate from Parent 0.\n    - New Coupling 1: A `tanh(beta * rank_gap(delta_cost))` gate smoothly focuses learning\n      on pairs with meaningful cost differences, while saturating to prevent high-rank\n      pairs from dominating.\n    - New Coupling 2: Clips `delta_logp` as a stability trick to prevent overly confident\n      model predictions and extreme gradients.\n    \"\"\"\n    # Read hyperparameters from the 'extra' dictionary\n    gamma = extra.get('gamma', 1.0)\n    beta = extra.get('beta', 3.0)\n    max_logp_diff = extra.get('max_logp_diff', 5.0)\n    eps = extra.get('eps', 1e-8)\n\n    # Unpack tensors from the batch\n    cost_w = batch['cost_a']\n    cost_l = batch['cost_b']\n    logp_w = batch['log_prob_w']\n    logp_l = batch['log_prob_l']\n\n    # Sanity check: cost_w should be less than or equal to cost_l\n    assert torch.all(cost_w <= cost_l), \"Assumption failed: cost_a (winner) must be <= cost_b (loser)\"\n\n    # 1. Calculate cost and log-probability differences\n    delta_cost = cost_l - cost_w\n    delta_logp = logp_w - logp_l\n\n    # 2. (New Coupling 2) Clip delta_logp for stability\n    clipped_delta_logp = torch.clamp(delta_logp, max=max_logp_diff)\n\n    # 3. (Inherited from Parent 1) Normalize delta_cost using z-scoring\n    if delta_cost.numel() > 1:\n        mean_delta_cost = delta_cost.mean()\n        std_delta_cost = delta_cost.std() + eps\n        normalized_delta_cost = (delta_cost - mean_delta_cost) / std_delta_cost\n    else:\n        normalized_delta_cost = torch.zeros_like(delta_cost)\n\n    # 4. (Inherited from Parent 1) Compute a saturated, z-score-normalized margin\n    margin = gamma * torch.tanh(normalized_delta_cost)\n\n    # 5. Compute the core logistic loss\n    core_loss = F.softplus(margin - clipped_delta_logp)\n\n    # 6. (Inherited from Parent 0 & New Coupling 1) Compute saturated rank-based gate\n    if delta_cost.numel() > 1:\n        ranks = delta_cost.argsort().argsort().float()\n        rank_gap_val = ranks / (delta_cost.numel() - 1) # Maps to [0, 1]\n    else:\n        rank_gap_val = torch.ones_like(delta_cost)\n    \n    gating_weight = torch.tanh(beta * rank_gap_val)\n\n    # 7. Apply the gate to the core loss\n    instance_loss = gating_weight * core_loss\n\n    # Apply optional sample weights if provided\n    if 'weight' in batch and batch['weight'] is not None:\n        instance_loss = instance_loss * batch['weight']\n\n    # 8. Return the mean loss over the batch\n    return instance_loss.mean()"}, "static_ok": true, "static_reason": "", "dynamic_ok": true, "dynamic_reason": "ok", "loss_value": 0.5210580229759216, "grad_norm": 0.0}
{"generation": 9, "index": 7, "ir": {"name": "Adaptive Sigmoid-Gated Logistic Loss with Rank-Saturated Margin", "intuition": "This loss function creates a highly adaptive and stable learning signal by blending rank-based and z-score-based normalization for both the margin and the final loss weighting. The objective is to intelligently focus learning on misaligned preferences while being robust to the scale and distribution of costs within a batch.\n\nInherited Ideas:\n- From Parent 0 (Sigmoid-Gated Rank-Weighted Logistic Loss), it inherits the idea of using a sigmoid-based gate, `sigmoid(zscore(delta_cost))`, to weight the final loss. This smoothly emphasizes pairs with a cost difference that is statistically significant (above average) within the batch context.\n- From Parent 1 (Adaptive Rank-Weighted Saturated Logistic Loss), it inherits the use of `tanh(zscore(delta_cost))` to create a saturated, batch-normalized margin. This prevents outlier pairs from creating an unbounded margin, leading to greater numerical stability.\n\nNew Coupling Ideas:\n1. **Rank-Saturated Margin**: A new margin is constructed by coupling the rank of the cost difference with the saturated z-score. The margin is `gamma * rank_gap(delta_cost) * tanh(zscore(delta_cost))`. This creates a dynamic margin that is large for pairs that are both high-rank (relatively important) and have a large z-score (statistically significant cost gap). It gracefully reduces the margin for pairs that might have a high rank but a small z-score (e.g., in a batch with low cost variance), or vice-versa.\n2. **Clamped Logistic Loss Core**: The core loss is computed as `softplus(clamp(margin - delta_logp, min=min_val))`. This stability trick prevents the argument to `softplus` from becoming excessively large and positive, which can happen if the model's `delta_logp` is a large negative number (very wrong prediction). By clamping the value, we prevent potential gradient explosion from extremely mispredicted pairs, improving training stability without affecting the gradient for most pairs.", "pseudocode": "1. Calculate the cost difference: delta_cost = cost(b) - cost(a).\n2. Calculate the log-probability difference: delta_logp = logp(a) - logp(b).\n3. (Inherited) Normalize the cost difference using z-scoring: normalized_delta_cost = zscore(delta_cost).\n4. (Inherited) Compute a rank-based weight from the raw delta_cost: rank_weight = rank_gap(delta_cost).\n5. (New Coupling 1) Compute a Rank-Saturated Margin. Multiply the rank weight by the tanh-saturated normalized cost, scaled by `gamma`: margin = gamma * rank_weight * tanh(normalized_delta_cost).\n6. (New Coupling 2) Compute a clamped logistic loss core. Calculate `margin - delta_logp`, clamp its value to have a minimum of `min_val`, and then apply the softplus function: core_loss = softplus(clamp(margin - delta_logp, min=min_val)).\n7. (Inherited) Compute a sigmoid-based gating weight from the normalized cost: gating_weight = sigmoid(normalized_delta_cost).\n8. The final loss for each pair is the product of the gating weight and the clamped core loss: instance_loss = gating_weight * core_loss.\n9. The final loss is the average of instance_loss over the batch.", "hyperparams": {"gamma": 2.0, "min_val": -5.0, "eps": 1e-08}, "operators_used": ["softplus", "sigmoid", "tanh", "zscore", "rank_gap", "clamp"], "implementation_hint": {"expects": ["cost_a", "cost_b", "logp_a", "logp_b", "weight"], "returns": "scalar"}, "code": "import torch\nimport torch.nn.functional as F\n\ndef generated_loss(batch, model_output, extra):\n    \"\"\"\n    Adaptive Sigmoid-Gated Logistic Loss with Rank-Saturated Margin.\n\n    This loss blends rank and z-score normalizations for both the margin and the loss weight.\n\n    - Inherits `sigmoid(zscore(delta_cost))` gating from Parent 0 to focus on statistically significant pairs.\n    - Inherits `tanh(zscore(delta_cost))` saturation from Parent 1 for a stable, outlier-resistant margin component.\n    - New Coupling 1: Creates a margin `gamma * rank_gap * tanh(zscore(delta_cost))`, combining relative importance (rank) with statistical significance (z-score).\n    - New Coupling 2: Uses `clamp` on the argument to `softplus` to prevent gradient explosion from extremely mispredicted pairs, enhancing stability.\n    \"\"\"\n    # Read hyperparameters from the 'extra' dictionary\n    gamma = extra.get('gamma', 2.0)\n    min_val = extra.get('min_val', -5.0)\n    eps = extra.get('eps', 1e-8)\n\n    # Unpack tensors from the batch\n    cost_w = batch['cost_a']\n    cost_l = batch['cost_b']\n    logp_w = batch['log_prob_w']\n    logp_l = batch['log_prob_l']\n\n    # Sanity check: cost_w should be less than or equal to cost_l\n    assert torch.all(cost_w <= cost_l), \"Assumption failed: cost_a (winner) must be <= cost_b (loser)\"\n\n    # 1. Calculate cost and log-probability differences\n    delta_cost = cost_l - cost_w  # This will be >= 0\n    delta_logp = logp_w - logp_l  # We want this to be positive\n\n    # 2. (Inherited) Normalize delta_cost using z-scoring\n    if delta_cost.numel() > 1:\n        mean_delta_cost = delta_cost.mean()\n        std_delta_cost = delta_cost.std() + eps\n        normalized_delta_cost = (delta_cost - mean_delta_cost) / std_delta_cost\n    else:\n        normalized_delta_cost = torch.zeros_like(delta_cost)\n\n    # 3. (Inherited) Compute rank-based adaptive weight from raw delta_cost\n    if delta_cost.numel() > 1:\n        ranks = delta_cost.argsort().argsort().float()\n        rank_weight = ranks / (delta_cost.numel() - 1)  # Maps to [0, 1]\n    else:\n        rank_weight = torch.ones_like(delta_cost)\n\n    # 4. (New Coupling 1) Compute a Rank-Saturated Margin\n    # This margin is large for pairs that are both high-rank and have a large z-score.\n    saturated_z_score = torch.tanh(normalized_delta_cost)\n    margin = gamma * rank_weight * saturated_z_score\n\n    # 5. (New Coupling 2) Compute a clamped logistic loss core\n    # Clamping prevents gradient explosion from very wrong predictions.\n    softplus_arg = margin - delta_logp\n    clamped_softplus_arg = torch.clamp(softplus_arg, min=min_val)\n    core_loss = F.softplus(clamped_softplus_arg)\n\n    # 6. (Inherited) Compute sigmoid-based gating weight\n    # This gate smoothly emphasizes pairs with above-average cost differences.\n    gating_weight = torch.sigmoid(normalized_delta_cost)\n\n    # 7. Combine gate and core loss\n    instance_loss = gating_weight * core_loss\n\n    # Apply optional sample weights if provided\n    if 'weight' in batch and batch['weight'] is not None:\n        instance_loss = instance_loss * batch['weight']\n\n    # 8. Return the mean loss over the batch\n    return instance_loss.mean()"}, "static_ok": true, "static_reason": "", "dynamic_ok": true, "dynamic_reason": "ok", "loss_value": 0.3465735912322998, "grad_norm": 0.0}
