{"generation": 0, "index": 0, "ir": {"name": "Adaptive Margin Rank Loss", "intuition": "This loss function creates a dynamic margin based on the normalized cost difference between two solutions. The intuition is that if one solution is significantly better than another (large cost gap), the model should be much more confident in preferring it (requiring a larger log probability difference). The loss is structured as a hinge-like or softplus-like penalty. It only penalizes the model if its preference (indicated by log probability difference) does not align with the cost-based preference by a sufficient, cost-dependent margin. The use of tanh and softplus ensures the loss is smooth and numerically stable, preventing explosions from extreme cost or log probability differences.", "pseudocode": "def adaptive_margin_rank_loss(cost_a, cost_b, logp_a, logp_b, beta, tau):\n    # 1. Calculate log probability difference\n    logp_diff = logp_a - logp_b\n\n    # 2. Calculate normalized cost difference and determine the better solution\n    # rank_gap ensures the sign reflects which solution is better (a is better if > 0)\n    cost_gap = rank_gap(cost_b, cost_a) # (cost_b - cost_a)\n\n    # 3. Create an adaptive margin using tanh for stable scaling\n    # The margin is proportional to the cost gap but bounded between 0 and beta.\n    # tanh is used to gracefully handle very large cost differences.\n    adaptive_margin = beta * tanh(cost_gap / tau)\n\n    # 4. Formulate the loss using softplus for a smooth hinge-like effect\n    # The loss penalizes the model if logp_diff is not greater than the adaptive_margin.\n    # The term inside softplus is `margin - logp_diff`.\n    # If logp_diff > margin, this term is negative, and softplus -> 0 (low loss).\n    # If logp_diff < margin, this term is positive, and softplus > 0 (high loss).\n    loss = softplus(adaptive_margin - logp_diff)\n\n    return loss", "hyperparams": {"beta": {"description": "Maximum margin factor. Controls the upper bound of the desired log probability difference.", "default_value": 5.0}, "tau": {"description": "Temperature for scaling the cost gap. A smaller tau makes the margin more sensitive to small cost differences. Controls the steepness of the tanh function.", "default_value": 1.0}}, "operators_used": ["rank_gap", "tanh", "softplus"], "implementation_hint": {"expects": ["{'cost_a': \"Tensor of costs for solution 'a' in each pair. Shape: [batch_size]\", 'cost_b': \"Tensor of costs for solution 'b' in each pair. Shape: [batch_size]\", 'logp_a': \"Tensor of log probabilities for solution 'a' in each pair. Shape: [batch_size]\", 'logp_b': \"Tensor of log probabilities for solution 'b' in each pair. Shape: [batch_size]\"}"], "returns": "A scalar loss value, typically the mean of the loss over the batch."}}, "static_ok": false, "static_reason": "implementation_hint.returns must be 'scalar'."}
{"generation": 0, "index": 1, "ir": {"name": "Adaptive Margin Sigmoid Loss (AMSL)", "intuition": "This loss function creates an adaptive margin based on the normalized cost difference between two solutions. The core idea is that when the cost difference is large, the model should have a stronger preference (a larger log probability difference) for the better solution. The loss uses `tanh` to squash the normalized cost difference, creating a bounded, adaptive margin `m`. This margin is then used in a sigmoid-based loss structure `logsigmoid(logit_diff - m)`. This design ensures that: 1) The 'target' for the logit difference scales with the 'obviousness' of the preference (the cost gap). 2) Using `tanh` and `logsigmoid` provides excellent numerical stability, preventing explosions from extreme cost or logit differences. The loss encourages the logit difference to exceed this adaptive margin, penalizing the model more for mis-ranking obvious pairs.", "pseudocode": "def amsl_loss(cost_a, cost_b, logp_a, logp_b, alpha, beta):\n  # Assume cost_a < cost_b, so a is the preferred solution\n  logit_diff = logp_a - logp_b\n\n  # 1. Calculate the raw cost gap\n  cost_gap = cost_b - cost_a\n\n  # 2. Normalize the cost gap. Here, rank_gap is a robust choice.\n  # It computes (cost_b - cost_a) / (max_cost - min_cost) across a batch.\n  # We use clamp to avoid division by zero in case all costs are the same.\n  normalized_gap = rank_gap(cost_a, cost_b)\n\n  # 3. Create a bounded, adaptive margin using tanh.\n  # The margin will be in [0, alpha].\n  # A larger normalized_gap leads to a larger margin.\n  adaptive_margin = alpha * tanh(beta * normalized_gap)\n\n  # 4. Compute the final loss using a sigmoid structure.\n  # We want logit_diff to be greater than adaptive_margin.\n  # The loss is -log(sigmoid(logit_diff - adaptive_margin)).\n  # This is equivalent to softplus(-(logit_diff - adaptive_margin)).\n  loss = -logsigmoid(logit_diff - adaptive_margin)\n\n  return loss", "hyperparams": {"alpha": {"description": "Maximum margin value. Controls the upper bound of the target logit difference. A larger alpha encourages a larger separation for clearly distinct pairs.", "default": 2.0}, "beta": {"description": "Scaling factor for the normalized cost gap. Controls how quickly the margin saturates to alpha as the cost gap increases. A larger beta means the margin grows faster for smaller gaps.", "default": 5.0}}, "operators_used": ["rank_gap", "tanh", "logsigmoid"], "implementation_hint": {"expects": ["{'preferred_logp': 'Log probability of the preferred solution in each pair (e.g., logp(a) where cost(a) < cost(b)). A 1D tensor of shape (batch_size,).', 'dispreferred_logp': 'Log probability of the dispreferred solution in each pair (e.g., logp(b)). A 1D tensor of shape (batch_size,).', 'preferred_cost': 'Cost of the preferred solution in each pair (e.g., cost(a)). A 1D tensor of shape (batch_size,).', 'dispreferred_cost': 'Cost of the dispreferred solution in each pair (e.g., cost(b)). A 1D tensor of shape (batch_size,).', 'all_costs_in_batch': 'A 1D tensor containing all costs from all pairs in the current batch, used by rank_gap for normalization. Shape: (2 * batch_size,).'}"], "returns": "A scalar loss value, typically the mean of the loss over the batch."}}, "static_ok": false, "static_reason": "implementation_hint.returns must be 'scalar'."}
{"generation": 0, "index": 2, "ir": {"name": "AdaptiveMarginHingeLoss", "intuition": "This loss function, named 'Adaptive Margin Hinge Loss', is designed to address the challenges of preference learning in combinatorial optimization, particularly focusing on numerical stability and sensitivity to cost differences. The core idea is to create an adaptive margin that is proportional to the normalized difference in costs between two solutions. Instead of a fixed margin as in traditional hinge losses, this margin `m(\u0394cost)` grows as the cost difference `\u0394cost = cost(b) - cost(a)` increases, but its influence is capped by a `tanh` function to prevent extreme values from dominating the gradient. The loss is activated only when the model's preference `logit_diff = logp(a) - logp(b)` does not sufficiently reflect the cost-based preference. Specifically, the loss is `softplus(m(\u0394cost) - logit_diff)`. This structure ensures that: (1) When the model already prefers the better solution `a` (i.e., `logit_diff` is large and positive), the loss approaches zero. (2) When the model incorrectly prefers the worse solution `b` or is uncertain, the loss becomes positive, pushing `logp(a)` up and `logp(b)` down. (3) The `tanh` and `softplus` functions guarantee that the loss and its gradients are always bounded, preventing numerical instability from extremely large cost or logit differences. The `rank_gap` operator is used to normalize the cost difference, making the loss less sensitive to the absolute scale of costs and more focused on their relative ranking within a batch.", "pseudocode": "def adaptive_margin_hinge_loss(cost_a, cost_b, logp_a, logp_b, batch_costs, alpha, beta):\n    # Ensure cost_a is the better (smaller) cost\n    if cost_a > cost_b:\n        cost_a, cost_b = cost_b, cost_a\n        logp_a, logp_b = logp_b, logp_a\n\n    # Calculate log probability difference, representing model's preference for 'a' over 'b'\n    logit_diff = logp_a - logp_b\n\n    # Calculate the normalized cost difference using rank_gap against the entire batch's costs\n    # This makes the margin adaptive to the distribution of costs in the current batch\n    normalized_cost_gap = rank_gap(cost_a, cost_b, context=batch_costs)\n\n    # Create an adaptive margin 'm' that is a function of the normalized cost gap.\n    # tanh ensures the margin is bounded between 0 and alpha, providing stability.\n    # alpha controls the maximum margin size, beta controls the steepness.\n    adaptive_margin = alpha * tanh(beta * normalized_cost_gap)\n\n    # Use a softplus-based hinge loss. The loss is positive only if logit_diff < adaptive_margin.\n    # This pushes the model to create a logp difference of at least 'adaptive_margin' in favor of the better solution.\n    # softplus(x) = log(1 + exp(x)), which is a smooth approximation of relu(x)\n    loss = softplus(adaptive_margin - logit_diff)\n\n    return loss", "hyperparams": {"alpha": 2.0, "beta": 5.0}, "operators_used": ["rank_gap", "tanh", "softplus"], "implementation_hint": {"expects": ["{'pair_cost_a': \"Scalar cost for solution 'a' in a pair.\", 'pair_cost_b': \"Scalar cost for solution 'b' in a pair.\", 'pair_logp_a': \"Scalar log probability for solution 'a'.\", 'pair_logp_b': \"Scalar log probability for solution 'b'.\", 'batch_all_costs': 'A 1D tensor containing all costs (from all pairs) in the current batch. Used as context for rank_gap.'}"], "returns": "A scalar loss value for the pair."}}, "static_ok": false, "static_reason": "implementation_hint.returns must be 'scalar'."}
{"generation": 0, "index": 3, "ir": {"name": "SigmoidAttenuatedRankLoss", "intuition": "This loss function is designed to be robust and adaptive for combinatorial optimization preference learning. It uses the rank gap of costs, normalized by the batch's cost range, to create a stable and scale-invariant target preference margin. This margin is then compared against the model's logit difference. The core idea is to use a sigmoid function to attenuate the loss signal when the model is already very confident about the correct preference (large positive logit difference) or when the cost difference is negligible. This prevents the model from becoming overconfident on easy pairs and allows it to focus on correcting misranked pairs or learning from pairs with significant cost differences. The softplus function ensures the final loss is non-negative and smooth, while a clamping mechanism on the cost rank gap prevents extreme values from dominating the training signal, ensuring overall numerical stability.", "pseudocode": "def SigmoidAttenuatedRankLoss(costs_a, costs_b, logits_a, logits_b, beta, margin, tau):\n    # 1. Calculate cost difference using a stable rank-based metric.\n    # rank_gap normalizes the difference by the range of costs in the batch.\n    cost_delta = rank_gap(costs_a, costs_b)\n\n    # 2. Introduce a margin and clamp to prevent extreme values.\n    # This creates a target preference strength based on cost.\n    target = clamp(cost_delta + margin, min_val=-1.0, max_val=1.0)\n\n    # 3. Calculate the model's predicted preference strength.\n    logit_diff = logits_a - logits_b\n\n    # 4. Calculate the core loss term: the discrepancy between target and prediction.\n    # The term is scaled by `beta`.\n    # The sign is negative because if costs_a < costs_b, we want logits_a > logits_b.\n    # So we want to maximize `logit_diff` when `target` is positive.\n    # The loss pushes `(logit_diff - target)` towards zero.\n    # We use `(target - logit_diff)` to align with standard loss minimization where a positive value indicates error.\n    discrepancy = beta * (target - logit_diff)\n\n    # 5. Attenuate the loss using a sigmoid function.\n    # When the model's prediction already aligns well with the target (logit_diff >> target), the discrepancy is large and negative.\n    # sigmoid(large_positive_value) -> 1, so the loss is not attenuated.\n    # This is the 'punishment' for misranking.\n    # When logit_diff is much larger than target, discrepancy is large and negative.\n    # sigmoid(-discrepancy) will be close to 1. We want to reduce the loss here.\n    # Let's re-evaluate the core logic. We want to penalize `logit_diff < target`.\n    # Let `x = logit_diff - target`. If `x` is negative (mis-ranked), we want high loss.\n    # A simple logistic loss form is `log(1 + exp(-beta * x))`.\n    # This is equivalent to `softplus(-beta * x)`.\n    # Let's use this simpler, more standard form.\n\n    # --- Revised Pseudocode ---\n    # 1. Calculate normalized and clamped cost-based target preference.\n    cost_delta = rank_gap(costs_a, costs_b) # Should be positive if cost_a < cost_b\n    target_margin = clamp(cost_delta + margin, min_val=-1.0, max_val=1.0)\n\n    # 2. Calculate logit difference.\n    logit_diff = logits_a - logits_b\n\n    # 3. Formulate the argument for the loss function.\n    # We want to encourage `logit_diff` to be greater than `target_margin`.\n    # This is a margin-based ranking formulation.\n    # The loss should be high if `logit_diff - target_margin` is small or negative.\n    # The argument to a logistic-style loss is `-(logit_diff - target_margin)`.\n    loss_argument = target_margin - logit_diff\n\n    # 4. Apply a scaled and temperature-controlled softplus.\n    # softplus(x) = log(1 + exp(x)). It's a smooth version of ReLU.\n    # The `tau` parameter controls the sharpness of the loss curve.\n    # Small tau -> steeper loss (like a hinge loss).\n    # Large tau -> softer loss.\n    loss = softplus(beta * loss_argument / tau)\n\n    return loss.mean()", "hyperparams": {"beta": 10.0, "margin": 0.01, "tau": 1.0}, "operators_used": ["rank_gap", "clamp", "softplus"], "implementation_hint": {"expects": ["{'pair_cost_a': 'Tensor, shape (batch_size,)', 'pair_cost_b': 'Tensor, shape (batch_size,)', 'pair_logit_a': 'Tensor, shape (batch_size,)', 'pair_logit_b': 'Tensor, shape (batch_size,)'}"], "returns": "Scalar tensor representing the mean loss over the batch."}}, "static_ok": false, "static_reason": "implementation_hint.returns must be 'scalar'."}
{"generation": 1, "index": 0, "ir": {"name": "AdaptiveMarginHingeLoss", "intuition": "This loss function, inspired by the hinge loss, introduces an adaptive margin that is proportional to the normalized cost difference between two solutions. The core idea is that when the cost difference is large, the model should be more confident in its preference (i.e., have a larger log probability difference). The loss penalizes the model only when its log probability difference does not respect the cost-based preference by a sufficient margin. We use `tanh` to clamp the log probability difference and a normalized cost difference to prevent extreme values, ensuring numerical stability. The `softplus` function ensures the loss is always non-negative and smooth.", "pseudocode": "1. Calculate the cost difference: \u0394_cost = cost_b - cost_a.\n2. Normalize the cost difference to a stable range, e.g., [-1, 1], using a scaling factor or `tanh`. Let's call it normalized_\u0394_cost. This will serve as an adaptive margin.\n3. Calculate the log probability difference: logit_diff = logp(a) - logp(b).\n4. Clamp the logit_diff to a symmetric range (e.g., [-k, k]) using `tanh` to prevent extreme values from dominating the loss. Let's call it bounded_logit_diff.\n5. The core of the loss is `margin - logit_diff`. Here, the margin is adaptive: `margin = alpha * normalized_\u0394_cost`.\n6. The loss is computed as `softplus(alpha * normalized_\u0394_cost - beta * bounded_logit_diff)`. This penalizes the model if `beta * bounded_logit_diff` is not greater than the adaptive margin `alpha * normalized_\u0394_cost`.\n7. When cost(a) < cost(b), \u0394_cost > 0, the loss becomes `softplus(positive_margin - bounded_logit_diff)`, pushing `logit_diff` to be positive. When cost(a) > cost(b), \u0394_cost < 0, the loss becomes `softplus(-positive_margin - bounded_logit_diff)`, pushing `logit_diff` to be negative.", "hyperparams": {"alpha": 2.0, "beta": 1.0, "cost_scale": 0.1, "logit_clamp_scale": 10.0}, "operators_used": ["tanh", "softplus"], "implementation_hint": {"expects": ["{'pair_cost_a': \"Tensor of costs for solution 'a' in each pair.\", 'pair_cost_b': \"Tensor of costs for solution 'b' in each pair.\", 'pair_logp_a': \"Tensor of log probabilities for solution 'a' in each pair.\", 'pair_logp_b': \"Tensor of log probabilities for solution 'b' in each pair.\"}"], "returns": "A scalar loss value, typically the mean of the losses for all pairs in a batch."}}, "static_ok": false, "static_reason": "implementation_hint.returns must be 'scalar'."}
{"generation": 1, "index": 1, "ir": {"name": "Adaptive Margin Sigmoid Loss (AMSL)", "intuition": "This loss function dynamically adjusts the margin based on the magnitude of the cost difference between two solutions. When the cost difference is large, the model is penalized more heavily for getting the preference wrong, creating a stronger learning signal. When the difference is small (i.e., the solutions are of similar quality), the margin is smaller, making the model less sensitive to minor, potentially noisy, cost variations. The `tanh` function is used to create a bounded, non-linear scaling of the cost difference, which prevents extreme cost gaps from causing the loss to explode. The final `logsigmoid` ensures the loss is smooth, bounded, and numerically stable, similar to DPO, but with a cost-aware, adaptive margin.", "pseudocode": "def amsl_loss(cost_a, cost_b, logp_a, logp_b, beta, margin_scale):\n    # Calculate the difference in log probabilities (logits)\n    logit_diff = logp_a - logp_b\n\n    # Calculate the normalized cost difference and clamp it to avoid extreme values\n    cost_diff = clamp(cost_b - cost_a, min=-5.0, max=5.0) # Clamp for stability\n\n    # Create an adaptive margin using tanh, scaled by the cost difference\n    # tanh makes the margin bounded between [-margin_scale, margin_scale]\n    adaptive_margin = margin_scale * tanh(cost_diff)\n\n    # The core argument for the loss function\n    # If cost_a < cost_b, cost_diff is positive, adaptive_margin is positive.\n    # We want logit_diff to be positive, so we penalize (logit_diff - adaptive_margin) being negative.\n    # The loss pushes logit_diff towards the adaptive_margin.\n    argument = beta * (logit_diff - adaptive_margin)\n\n    # Use logsigmoid for a stable, bounded loss.\n    # The negative sign makes it a loss to be minimized.\n    loss = -logsigmoid(argument)\n\n    return loss", "hyperparams": {"beta": {"value": 0.5, "description": "Temperature parameter that scales the logit difference, controlling the sharpness of the sigmoid. Similar to DPO's beta."}, "margin_scale": {"value": 1.0, "description": "Maximum value for the adaptive margin. Controls how much the cost difference can influence the target logit gap."}}, "operators_used": ["logsigmoid", "tanh", "clamp"], "implementation_hint": {"expects": ["{'pair_cost_a': \"Tensor of costs for solution 'a' in each pair.\", 'pair_cost_b': \"Tensor of costs for solution 'b' in each pair.\", 'pair_logp_a': \"Tensor of log probabilities for solution 'a' in each pair.\", 'pair_logp_b': \"Tensor of log probabilities for solution 'b' in each pair.\"}"], "returns": "A scalar loss value, typically the mean of the loss over the batch of pairs."}}, "static_ok": false, "static_reason": "implementation_hint.returns must be 'scalar'."}
{"generation": 1, "index": 2, "ir": {"name": "Sigmoid-Scaled Rank-Gap Hinge Loss", "intuition": "The core idea is to create a hinge-like loss that is activated only when the model's preference contradicts the ground truth cost. The magnitude of this loss is dynamically scaled by the 'rank gap' between the costs of the two solutions. This scaling is passed through a sigmoid function to ensure that very large cost differences don't cause the loss to explode, but still contribute more significantly than small differences. This makes the loss signal proportional to the severity of the mis-ranking, while maintaining numerical stability. The loss is zero when the model correctly prefers the better solution, encouraging it to maintain correct preferences without penalty.", "pseudocode": "1. Calculate the raw cost difference: delta_cost = cost_b - cost_a.\n2. Calculate the rank gap, which is a normalized and bounded representation of the cost difference. Here we use a sigmoid function for stable scaling: rank_gap_scaled = sigmoid(delta_cost / temp).\n3. Calculate the difference in model's log probabilities: logp_diff = logp_a - logp_b.\n4. The loss is a hinge-like structure, activated only when the better solution 'a' (cost_a < cost_b) has a lower log probability than 'b'. The penalty is the product of the scaled rank gap and the negative log probability difference.\n5. loss = relu(-logp_diff) * rank_gap_scaled", "hyperparams": {"temp": {"value": 10.0, "description": "Temperature for scaling the cost difference before the sigmoid. A smaller value makes the scaling steeper, a larger value makes it softer."}}, "operators_used": ["sigmoid", "relu"], "implementation_hint": {"expects": ["{'cost_a': 'Scalar cost of solution a (lower is better).', 'cost_b': 'Scalar cost of solution b.', 'logp_a': 'Log probability of solution a from the model.', 'logp_b': 'Log probability of solution b from the model.'}"], "returns": "A single scalar loss value, non-negative."}}, "static_ok": false, "static_reason": "implementation_hint.returns must be 'scalar'."}
{"generation": 1, "index": 3, "ir": {"name": "AdaptiveMarginHingeLoss", "intuition": "This loss function, named Adaptive Margin Hinge Loss, is designed for stable preference learning in combinatorial optimization. Its core idea is to combine a classic hinge loss structure with an adaptive margin that is sensitive to the magnitude of the cost difference between two solutions. The standard hinge loss `max(0, margin - (logp_a - logp_b))` penalizes the model if the preferred solution 'a' is not favored by at least a fixed `margin`. Our innovation is to make this margin dynamic. When the cost difference `cost(b) - cost(a)` is large, it signifies a strong preference, so the margin should also be large, demanding a more confident prediction from the model. Conversely, when the cost difference is small, the solutions are nearly equivalent, so the margin should shrink, preventing the model from over-optimizing on negligible differences. We use `tanh` to scale the cost difference into a bounded, non-linear margin, preventing extreme cost gaps from creating excessively large penalties. The `softplus` function is used as a smooth approximation of the `relu` (hinge) operator, ensuring the final loss is smooth and everywhere differentiable, which is beneficial for gradient-based optimization. This design enforces preference consistency while maintaining numerical stability and adapting intelligently to the signal strength of the preference data.", "pseudocode": "1. Calculate the cost difference: delta_cost = cost_b - cost_a. This is positive if 'a' is the preferred solution.\n2. Calculate the log probability difference: logp_diff = logp_a - logp_b.\n3. Compute an adaptive margin based on the cost difference. Use the tanh function to squash the scaled cost difference into a bounded range, e.g., [0, beta]. This prevents the margin from exploding. adaptive_margin = beta * tanh(clamp(delta_cost * alpha, min=-10, max=10)). The clamp adds an extra layer of stability.\n4. Formulate the core loss term, which is a smooth hinge loss. It penalizes the model if the log probability difference does not exceed the adaptive margin. The target is for logp_diff to be greater than adaptive_margin. So the term to minimize is adaptive_margin - logp_diff.\n5. Apply the softplus function to the term from the previous step to create a smooth, non-negative loss: loss = softplus(adaptive_margin - logp_diff). This is a smooth version of max(0, adaptive_margin - logp_diff).", "hyperparams": {"alpha": 0.1, "beta": 5.0}, "operators_used": ["tanh", "softplus", "clamp"], "implementation_hint": {"expects": ["{'cost_a': \"Tensor of costs for solution 'a' in a batch of pairs.\", 'cost_b': \"Tensor of costs for solution 'b' in a batch of pairs.\", 'logp_a': \"Tensor of log probabilities for solution 'a'.\", 'logp_b': \"Tensor of log probabilities for solution 'b'.\"}"], "returns": "A scalar loss value, typically the mean of the loss over the batch."}}, "static_ok": false, "static_reason": "implementation_hint.returns must be 'scalar'."}
{"generation": 1, "index": 4, "ir": {"name": "SigmoidAttenuatedRankGapLoss", "intuition": "This loss function, termed 'Sigmoid Attenuated Rank Gap Loss' (SARG), is designed to address the challenge of learning from preference pairs with highly variable cost differences. The core idea is to use the rank gap, a robust measure of cost difference, as the primary driver for the preference signal. To ensure numerical stability and prevent disproportionate influence from pairs with extreme cost gaps, the rank gap is non-linearly scaled using a sigmoid function. This squashes the gap into a bounded range (e.g., [0, 1]), effectively attenuating the signal from outliers. This scaled cost difference then acts as a dynamic margin or weight for a standard logistic-style preference loss. The loss is then formulated such that when the model's preference (logit difference) aligns with the ground truth cost preference, the loss is low. When they misalign, the loss is high, and the magnitude of this loss is proportional to the sigmoid-scaled cost difference, providing a stable yet informative gradient for learning.", "pseudocode": "def sarg_loss(cost_a, cost_b, logit_a, logit_b, beta, margin):\n  # Determine the preferred (w) and dispreferred (l) items based on cost\n  if cost_a < cost_b:\n    cost_w, cost_l = cost_a, cost_b\n    logit_w, logit_l = logit_a, logit_b\n  else:\n    cost_w, cost_l = cost_b, cost_a\n    logit_w, logit_l = logit_b, logit_a\n\n  # Calculate the rank gap for the cost difference\n  # rank_gap is assumed to be a pre-computed, normalized value for the pair\n  # For a single pair, it's just a placeholder for a scaled difference.\n  # In a batch context, rank_gap(cost_w, cost_l) would be (rank(cost_l) - rank(cost_w)) / N\n  # Here, we use a simplified proxy: a sigmoid-scaled absolute difference for stability\n  cost_delta = abs(cost_w - cost_l)\n  \n  # Attenuate the cost delta using a sigmoid to create a stable weight between 0 and 1\n  # This prevents extreme cost differences from dominating the loss.\n  # 'beta' controls the steepness of this attenuation.\n  attenuation_factor = sigmoid(beta * cost_delta)\n\n  # Calculate the logit difference, which represents the model's preference strength\n  logit_diff = logit_w - logit_l\n\n  # The core loss: a softplus (log(1+exp(-x))) applied to the logit difference.\n  # The argument is shifted by a margin and scaled by the attenuated cost gap.\n  # This means a larger, more confident cost difference demands a stronger logit separation.\n  loss = softplus(-attenuation_factor * (logit_diff - margin))\n\n  return loss", "hyperparams": {"beta": 1.0, "margin": 0.0}, "operators_used": ["sigmoid", "softplus", "rank_gap"], "implementation_hint": {"expects": ["{'cost_w': 'Tensor of costs for the preferred trajectories in a batch (lower is better).', 'cost_l': 'Tensor of costs for the dispreferred trajectories in a batch.', 'logits_w': 'Tensor of model logits for the preferred trajectories.', 'logits_l': 'Tensor of model logits for the dispreferred trajectories.', 'rank_gap_w_l': 'Tensor of pre-computed, normalized rank gaps between preferred and dispreferred trajectories. This is the primary signal.'}"], "returns": "A scalar loss value, typically the mean of the losses for all pairs in the batch."}}, "static_ok": false, "static_reason": "implementation_hint.returns must be 'scalar'."}
{"generation": 1, "index": 5, "ir": {"name": "AdaptiveMarginHingeLoss", "intuition": "This loss function, named 'Adaptive Margin Hinge Loss', is designed for stable preference learning in combinatorial optimization. Its core idea is to combine a classic hinge loss structure with an adaptive margin that is sensitive to the magnitude of the cost difference between two solutions. The hinge loss (ReLU) penalizes the model only when its preference contradicts the ground truth cost ordering. The 'adaptive margin' is a non-linear function of the cost difference, scaled by a hyperparameter. Specifically, we use a sigmoid function on the normalized cost difference. This ensures that when the cost difference is very large, the required margin for the model's log probability difference saturates, preventing the loss from growing infinitely and causing gradient explosion. Conversely, for small cost differences, the margin is small, allowing the model to focus on clear-cut cases without being overly penalized for ambiguity. This approach provides a balance between enforcing correct preference ranking and maintaining numerical stability.", "pseudocode": "def AdaptiveMarginHingeLoss(cost_a, cost_b, logp_a, logp_b, beta, tau):\n  # Determine which is the winning (w) and losing (l) solution based on cost\n  if cost_a < cost_b:\n    cost_w, cost_l = cost_a, cost_b\n    logp_w, logp_l = logp_a, logp_b\n  else:\n    cost_w, cost_l = cost_b, cost_a\n    logp_w, logp_l = logp_b, logp_a\n\n  # Calculate the difference in log probabilities. A positive value is desired.\n  log_prob_diff = logp_w - logp_l\n\n  # Calculate the cost difference. Always non-negative.\n  cost_diff = cost_l - cost_w\n\n  # Create an adaptive margin that is a function of the cost difference.\n  # The sigmoid function squashes the cost difference into a (0, 1) range.\n  # This prevents the margin from becoming excessively large for huge cost differences.\n  # 'tau' is a temperature/scale parameter that controls the steepness of the sigmoid.\n  adaptive_margin = sigmoid(cost_diff / tau)\n\n  # The loss is a hinge loss (ReLU) with the adaptive margin.\n  # Loss is incurred only if log_prob_diff < adaptive_margin, i.e., the model's preference for the winner is not strong enough.\n  # 'beta' is a scaling factor for the overall loss magnitude.\n  loss = beta * relu(adaptive_margin - log_prob_diff)\n\n  return loss", "hyperparams": {"beta": 1.0, "tau": 1.0}, "operators_used": ["relu", "sigmoid"], "implementation_hint": {"expects": ["cost_w: a tensor of costs for the winning solutions in a batch", "cost_l: a tensor of costs for the losing solutions in a batch", "logp_w: a tensor of log probabilities for the winning solutions", "logp_l: a tensor of log probabilities for the losing solutions"], "returns": "scalar: the mean loss over the batch"}}, "static_ok": false, "static_reason": "implementation_hint.returns must be 'scalar'."}
{"generation": 1, "index": 6, "ir": {"name": "AdaptiveMarginHingeLoss", "intuition": "This loss function, inspired by the hinge loss, introduces an adaptive margin that depends on the normalized cost difference between two solutions. The core idea is that when the cost difference is large, the model should be more confident in its preference (i.e., the log probability difference should be larger). We use `tanh` to map the cost difference to a bounded range [-1, 1], which acts as a dynamic margin. The `softplus` function is then used to create a smooth, one-sided penalty, similar to a hinge loss, which only penalizes the model when its preference (`logp_diff`) doesn't align with the cost-based preference (`-tanh(...)`). The `tanh` and `softplus` combination ensures the loss is smooth, differentiable, and numerically stable, preventing explosions from extreme cost or logit differences.", "pseudocode": "def adaptive_margin_hinge_loss(cost_a, cost_b, logp_a, logp_b, beta, margin_scale):\n    # Normalize cost difference to a stable range [-1, 1]\n    # The sign of cost_diff indicates which solution is better\n    cost_diff = cost_a - cost_b\n    \n    # Use tanh to create a bounded, scaled target for the log probability difference.\n    # If cost_a < cost_b, target is negative, encouraging logp_a > logp_b.\n    # The magnitude of the target is proportional to the magnitude of the cost difference.\n    target_preference_direction = -tanh(cost_diff * beta)\n    \n    # The log probability difference reflects the model's current preference.\n    logp_diff = logp_a - logp_b\n    \n    # We want logp_diff to align with target_preference_direction.\n    # The loss is incurred when the model's preference (logp_diff) is 'worse' than the target.\n    # We scale the target to control the desired margin magnitude.\n    # The expression inside softplus is: margin_scale * target - logp_diff\n    # This is equivalent to: logp_diff - margin_scale * target_preference_direction\n    # The loss is high if logp_diff is much smaller than the desired positive margin, or much larger than the desired negative margin.\n    loss = softplus(margin_scale * target_preference_direction - logp_diff)\n    \n    return loss", "hyperparams": {"beta": {"default": 0.1, "description": "Scaling factor for the cost difference before applying tanh. A larger beta makes the margin more sensitive to small cost differences."}, "margin_scale": {"default": 1.0, "description": "Scales the target margin produced by tanh. Controls the overall magnitude of the desired log probability difference."}}, "operators_used": ["tanh", "softplus"], "implementation_hint": {"expects": ["{'cost_a': \"Tensor of costs for solution 'a' in each pair.\", 'cost_b': \"Tensor of costs for solution 'b' in each pair.\", 'logp_a': \"Tensor of log probabilities for solution 'a' in each pair.\", 'logp_b': \"Tensor of log probabilities for solution 'b' in each pair.\"}"], "returns": "A scalar loss value, typically the mean of the loss over the batch of pairs."}}, "static_ok": false, "static_reason": "implementation_hint.returns must be 'scalar'."}
{"generation": 1, "index": 7, "ir": {"name": "SigmoidAttenuatedRankLoss", "intuition": "The core idea is to create a hinge-like loss that is activated when the model's preference contradicts the ground-truth cost preference. The magnitude of the penalty is scaled by how 'wrong' the model is (the log-probability difference), but this scaling is attenuated by a sigmoid function of the cost difference. This attenuation serves two purposes: 1) For very large cost differences, the loss 'saturates', preventing extreme gradients and focusing the model on learning the correct preference order rather than perfectly matching the cost ratio. 2) For very small cost differences, the scaling factor approaches 0.5, providing a gentle, non-zero gradient that prevents the model from becoming indifferent to minor but real improvements. This design avoids the potential instability of directly using cost differences as a scaling factor, especially when they are very large or close to zero.", "pseudocode": "def SigmoidAttenuatedRankLoss(cost_a, cost_b, logp_a, logp_b, beta, tau):\n    # Let's assume cost_a is the better solution (preferred)\n    # The loss should be computed for the pair (preferred, unpreferred)\n    # Ensure cost_w (winner) < cost_l (loser)\n    if cost_a < cost_b:\n        cost_w, cost_l = cost_a, cost_b\n        logp_w, logp_l = logp_a, logp_b\n    else:\n        cost_w, cost_l = cost_b, cost_a\n        logp_w, logp_l = logp_b, logp_a\n\n    # Calculate the difference in log probabilities. We want logp_w > logp_l.\n    log_prob_diff = logp_l - logp_w\n\n    # Calculate the rank gap of costs. This is non-negative.\n    cost_gap = rank_gap(cost_l, cost_w) # Equivalent to cost_l - cost_w\n\n    # Create a stable, bounded scaling factor based on the cost gap.\n    # sigmoid(-x) for x > 0 is in (0, 0.5). We scale it to be in (0, 1).\n    # tau controls the sensitivity to the cost gap.\n    cost_attenuation_factor = 2.0 * sigmoid(-cost_gap / tau)\n\n    # The core loss is a softplus (smooth relu) on the log_prob_diff.\n    # This penalizes cases where logp_l > logp_w.\n    # beta controls the overall steepness of the loss.\n    base_loss = softplus(beta * log_prob_diff)\n\n    # Modulate the base loss with the cost-derived attenuation factor.\n    # When cost_gap is large, factor -> 0, loss -> 0 (easy case, less to learn).\n    # When cost_gap is small, factor -> 1, loss is fully applied.\n    # Wait, the logic should be reversed. Large cost gap is an IMPORTANT mistake.\n    # Let's redefine the factor.\n\n    # Corrected Pseudocode:\n    if cost_a < cost_b:\n        cost_w, cost_l = cost_a, cost_b\n        logp_w, logp_l = logp_a, logp_b\n    else:\n        cost_w, cost_l = cost_b, cost_a\n        logp_w, logp_l = logp_b, logp_a\n\n    log_prob_diff = logp_l - logp_w\n    cost_gap = rank_gap(cost_l, cost_w)\n\n    # Sigmoid of a positive value is in (0.5, 1). This factor is close to 1 for large gaps.\n    # This correctly weighs mistakes on large-gap pairs more heavily.\n    cost_importance_weight = sigmoid(cost_gap / tau)\n\n    # The loss is the softplus of the log-probability difference, weighted by cost importance.\n    # The beta parameter scales the logits before the softplus, similar to DPO.\n    loss = cost_importance_weight * softplus(beta * log_prob_diff)\n\n    return loss", "hyperparams": {"beta": 0.5, "tau": 10.0}, "operators_used": ["rank_gap", "sigmoid", "softplus"], "implementation_hint": {"expects": ["pair_cost_a", "pair_cost_b", "pair_logp_a", "pair_logp_b"], "returns": "scalar_loss_per_pair"}}, "static_ok": false, "static_reason": "implementation_hint.returns must be 'scalar'."}
{"generation": 2, "index": 0, "ir": {"name": "SigmoidAttenuatedRankGapLoss", "intuition": "This loss function aims to create a dynamic margin based on the normalized cost difference. It uses the `rank_gap` of costs, normalized to a [-1, 1] range via `tanh`, to define how strongly the model should prefer one solution over another. The core idea is that a larger cost difference should create a stronger push for the model to align its probability distribution with the ground-truth preference. The loss is calculated as a softplus on the difference between the logit difference and this dynamic margin. A sigmoid function is applied to the logit difference to attenuate its effect when it becomes very large, preventing extreme gradients and ensuring the loss remains bounded, which enhances numerical stability. This design avoids the vanishing gradient issue of standard logistic loss for well-separated pairs while still providing a smooth, stable, and informative learning signal across a wide range of scenarios.", "pseudocode": "def SigmoidAttenuatedRankGapLoss(cost_a, cost_b, logp_a, logp_b, alpha, beta):\n    # 1. Calculate the log probability difference.\n    logit_diff = logp_a - logp_b\n\n    # 2. Calculate the normalized rank gap of costs.\n    # rank_gap ensures the sign is correct (positive if a is worse than b).\n    # tanh scales it to a [-1, 1] range, acting as a soft normalization.\n    # The result is positive if cost(a) > cost(b), which is the 'wrong' direction for the preferred solution 'a'.\n    normalized_cost_gap = tanh(rank_gap(cost_a, cost_b))\n\n    # 3. Create the core loss term.\n    # The target is for `logit_diff` to be positive if `cost_a < cost_b` (i.e., `normalized_cost_gap` is negative).\n    # We want to penalize `beta * normalized_cost_gap - logit_diff`.\n    # For numerical stability, we attenuate the logit_diff's contribution with a sigmoid.\n    # This prevents extreme logit differences from dominating the loss and causing instability.\n    # The term inside softplus is `-(logit_diff - beta * normalized_cost_gap)` but with attenuation.\n    loss_term = alpha * sigmoid(logit_diff) - beta * normalized_cost_gap\n\n    # 4. Apply softplus to get a non-negative, smooth loss.\n    # softplus(x) = log(1 + exp(x)). It's a smooth version of ReLU.\n    # We want to penalize when `loss_term` is positive.\n    # If cost(a) < cost(b), normalized_cost_gap is negative. We want logit_diff to be positive.\n    # The loss should be `softplus(-logit_diff + target_margin)` where target_margin depends on cost gap.\n    # Let's re-formulate to be clearer:\n    # We want to minimize softplus(target - actual).\n    # target = beta * (-normalized_cost_gap)  (positive if a is better)\n    # actual = alpha * sigmoid(logit_diff)\n    # loss = softplus(actual - target) is not quite right.\n    # Let's use the DPO form: softplus(-y * (logit_diff - offset))\n    # where y is the preference label (+1 if a is preferred, -1 if b is preferred)\n    # y = -sign(normalized_cost_gap)\n    # offset = beta * abs(normalized_cost_gap)\n    # This is getting complicated. Let's stick to the simpler, more direct formulation.\n    #\n    # Let's re-think the core term. We want to penalize `logit_diff` being small or negative when `cost_a < cost_b`.\n    # Let `pref_indicator = -1` if `cost_a < cost_b`, `+1` if `cost_b < cost_a`.\n    # This is `sign(rank_gap(cost_a, cost_b))`.\n    # DPO-style loss: softplus(-pref_indicator * logit_diff).\n    # Our version scales this by the magnitude of the cost difference.\n    # Let `target_logit_diff = beta * -normalized_cost_gap`.\n    # We want `logit_diff` to be close to or greater than `target_logit_diff`.\n    # So, we penalize `target_logit_diff - logit_diff`.\n    # To add stability, we use the attenuated logit diff.\n    # Revised Pseudocode:\n\n    logit_diff = logp_a - logp_b\n    cost_delta = rank_gap(cost_a, cost_b) # Positive if cost_a > cost_b\n\n    # The 'target' for the logit difference, scaled by the cost gap.\n    # If cost_a < cost_b, cost_delta is negative, target is positive, encouraging logp_a > logp_b.\n    target_margin = -beta * tanh(cost_delta)\n\n    # The 'actual' logit difference, attenuated by a sigmoid to prevent extreme values.\n    # alpha scales the impact of the logit difference.\n    actual_attenuated_diff = alpha * tanh(logit_diff / alpha) # Use tanh for symmetry and stability\n\n    # The core of the loss: we want the actual difference to be greater than the target margin.\n    # We are penalized if `actual_attenuated_diff < target_margin`.\n    # So, we calculate loss on `target_margin - actual_attenuated_diff`.\n    loss = softplus(target_margin - actual_attenuated_diff)\n\n    return loss", "hyperparams": {"alpha": 10.0, "beta": 5.0}, "operators_used": ["rank_gap", "tanh", "softplus"], "implementation_hint": {"expects": ["{'pair_cost_a': \"Tensor of costs for solution 'a' in each pair.\", 'pair_cost_b': \"Tensor of costs for solution 'b' in each pair.\", 'pair_logp_a': \"Tensor of log probabilities for solution 'a' in each pair.\", 'pair_logp_b': \"Tensor of log probabilities for solution 'b' in each pair.\"}"], "returns": "A scalar loss value, typically the mean of the loss over the batch of pairs."}}, "static_ok": false, "static_reason": "implementation_hint.returns must be 'scalar'."}
{"generation": 2, "index": 1, "ir": {"name": "Adaptive Margin Ranking Loss with Temperature Scaling", "intuition": "This loss function, named AMR-TS, is a variant of the classic ranking loss. Its core idea is to make the required 'margin' between the log probabilities of two solutions (a, b) adaptive. This margin is not a fixed hyperparameter but is dynamically scaled by the normalized difference in their costs. If solution 'a' is significantly better than 'b' (large cost difference), the model is pushed much harder to assign a higher log probability to 'a'. Conversely, if their costs are very close, the loss becomes lenient, requiring only a small or zero margin. To ensure numerical stability and control the sensitivity to cost differences, the cost gap is first normalized using a z-score over the batch and then passed through a tanh function to squash it into a bounded range [-1, 1]. A temperature parameter 'tau' fine-tunes how sharply the cost difference translates into a margin, preventing extreme values. The final loss is calculated using softplus, which is a smooth and non-negative approximation of the relu function, ensuring the loss is always non-negative and has smooth gradients.", "pseudocode": "def AMR_TS_loss(cost_a, cost_b, logp_a, logp_b, batch_costs, tau, beta):\n  # 1. Calculate the raw cost difference\n  delta_cost = cost_a - cost_b\n\n  # 2. Normalize the cost difference over the batch for stability and scale-invariance\n  # rank_gap provides a robust measure of difference. zscore normalizes it.\n  normalized_gap = zscore(rank_gap(cost_a, cost_b, batch_costs))\n\n  # 3. Create an adaptive margin by squashing the normalized gap\n  # tanh maps the unbounded z-score to a bounded [-1, 1] range\n  adaptive_margin = tanh(normalized_gap / tau)\n\n  # 4. Calculate the log probability difference\n  delta_logp = logp_a - logp_b\n\n  # 5. Combine into a ranking formulation\n  # The loss is softplus(beta * (adaptive_margin - delta_logp))\n  # If cost_a < cost_b, adaptive_margin is negative. The loss encourages delta_logp to be positive.\n  # If cost_a > cost_b, adaptive_margin is positive. The loss encourages delta_logp to be negative.\n  # The term is structured as (margin - (y_pred - y_ref)), which is equivalent to (y_ref - y_pred + margin).\n  # Here, our 'target' for delta_logp is the adaptive_margin. So we compute softplus(beta * (adaptive_margin - delta_logp)).\n  loss = softplus(beta * (adaptive_margin - delta_logp))\n\n  return loss", "hyperparams": {"tau": {"description": "Temperature for scaling the normalized cost gap. Lower tau makes the margin more sensitive to cost differences. Recommended: 0.1 to 1.0.", "default": 0.5}, "beta": {"description": "Scaling factor for the final loss value, controlling the magnitude of gradients. Recommended: 1.0 to 10.0.", "default": 5.0}}, "operators_used": ["rank_gap", "zscore", "tanh", "softplus"], "implementation_hint": {"expects": ["{'pair_cost_a': \"Scalar cost for solution 'a' in a pair.\", 'pair_cost_b': \"Scalar cost for solution 'b' in a pair.\", 'pair_logp_a': \"Scalar log probability for solution 'a'.\", 'pair_logp_b': \"Scalar log probability for solution 'b'.\", 'batch_all_costs': 'A 1D tensor containing all costs from the current batch, used for normalization context.'}"], "returns": "A single scalar loss value for the pair."}}, "static_ok": false, "static_reason": "implementation_hint.returns must be 'scalar'."}
{"generation": 2, "index": 2, "ir": {"name": "AdaptiveMarginHingeLoss", "intuition": "This loss function, named 'Adaptive Margin Hinge Loss', is inspired by the classic hinge loss used in SVMs but with a crucial adaptation for preference learning. The core idea is to establish a 'margin' that the preferred solution's log-probability should exceed the less preferred one's. Unlike a fixed margin, this margin is adaptive: it scales with the normalized difference in costs (the 'rank gap'). A larger cost difference demands a larger log-probability separation, making the learning signal proportional to the magnitude of preference. We use `tanh` to squash the cost difference, preventing extreme cost gaps from creating excessively large, unstable margins. The `softplus` function is used as a smooth, non-negative hinge, which only penalizes the model when the log-probability difference `logp_a - logp_b` fails to meet the adaptive margin `m`. This design ensures the loss is zero for correctly and confidently ranked pairs, focusing the training effort on ambiguous or misranked cases, while maintaining numerical stability.", "pseudocode": "def AdaptiveMarginHingeLoss(cost_a, cost_b, logp_a, logp_b, beta, tau):\n    # Ensure a is the better solution\n    if cost_a > cost_b:\n        cost_a, cost_b = cost_b, cost_a\n        logp_a, logp_b = logp_b, logp_a\n\n    # Calculate the difference in costs (cost_b > cost_a)\n    delta_cost = cost_b - cost_a\n\n    # Create a normalized, bounded adaptive margin using tanh.\n    # tanh input is scaled by tau to control saturation speed.\n    # The result is scaled by beta to control the maximum margin size.\n    adaptive_margin = beta * tanh(delta_cost / tau)\n\n    # Calculate the log-probability difference for the preferred pair (a, b)\n    log_prob_diff = logp_a - logp_b\n\n    # The hinge loss: penalize if log_prob_diff is not greater than the margin.\n    # softplus(x) = log(1 + exp(x)). This is a smooth version of max(0, x).\n    # We want to minimize softplus(margin - log_prob_diff).\n    # This is equivalent to max(0, margin - log_prob_diff) but smooth.\n    loss = softplus(adaptive_margin - log_prob_diff)\n\n    return loss", "hyperparams": {"beta": {"value": 2.0, "description": "Controls the maximum size of the adaptive margin. A larger beta demands a larger log-probability gap for pairs with significant cost differences."}, "tau": {"value": 1.0, "description": "A temperature/scaling parameter for the cost difference. It controls the sensitivity of the margin to the cost gap. A smaller tau makes the margin grow faster for small cost differences."}}, "operators_used": ["tanh", "softplus"], "implementation_hint": {"expects": ["{'cost_w': 'Scalar cost of the winning/preferred solution.', 'cost_l': 'Scalar cost of the losing/less-preferred solution.', 'logp_w': 'Log-probability of the winning solution from the model.', 'logp_l': 'Log-probability of the losing solution from the model.'}"], "returns": "scalar_loss"}}, "static_ok": false, "static_reason": "implementation_hint.returns must be 'scalar'."}
{"generation": 2, "index": 3, "ir": {"name": "AdaptiveMarginHingeLoss", "intuition": "This loss function, named Adaptive Margin Hinge Loss (AMHL), combines the principles of hinge loss with an adaptive margin mechanism. The core idea is to create a 'satisfaction boundary' for the model's preference. The margin is not fixed; instead, it's a function of the normalized cost difference between the two solutions. When the cost difference is large, the model is pushed more strongly to have a correspondingly large log-probability difference. When the cost difference is small, the model is allowed to have a smaller, or even zero, preference, preventing it from over-optimizing on noisy or insignificant cost variations. The use of `tanh` on the logit difference and `softplus` for the final loss ensures smoothness and numerical stability, preventing gradient explosion from extreme logit or cost differences.", "pseudocode": "def amhl_loss(cost_a, cost_b, logp_a, logp_b, beta, tau):\n    # 1. Calculate cost difference and normalize it to a [0, 1] range.\n    # rank_gap provides a stable, scale-invariant measure of relative quality.\n    cost_diff_normalized = rank_gap(cost_a, cost_b)\n\n    # 2. Define an adaptive margin that grows with the normalized cost difference.\n    # The margin is 0 when costs are equal and approaches beta as cost_diff_normalized -> 1.\n    adaptive_margin = beta * cost_diff_normalized\n\n    # 3. Calculate the log probability difference (logit difference).\n    logit_diff = logp_a - logp_b\n\n    # 4. Stabilize the logit difference using tanh. This bounds it to [-1, 1],\n    # preventing extreme values from causing numerical instability in the loss.\n    # The temperature 'tau' controls the steepness of the tanh function.\n    stable_logit_diff = tanh(logit_diff / tau)\n\n    # 5. Formulate the core hinge-like term.\n    # We want stable_logit_diff to be greater than the adaptive_margin.\n    # The loss is incurred only when this condition is violated.\n    # The structure is `max(0, margin - value)`.\n    hinge_term = adaptive_margin - stable_logit_diff\n\n    # 6. Apply softplus to the hinge term to make it a smooth, non-negative loss.\n    # softplus(x) = log(1 + exp(x)). This is a smooth approximation of ReLU(x).\n    loss = softplus(hinge_term)\n\n    return loss", "hyperparams": {"beta": {"value": 2.0, "description": "Scales the maximum margin. A higher beta enforces a stronger preference for solutions with significant cost improvements."}, "tau": {"value": 1.0, "description": "Temperature for the tanh function. A smaller tau makes the tanh transition steeper, approximating a sign function. A larger tau makes it smoother, dampening the effect of the logit difference."}}, "operators_used": ["rank_gap", "tanh", "softplus"], "implementation_hint": {"expects": ["{'cost_a': \"Tensor of costs for solution 'a' in each pair.\", 'cost_b': \"Tensor of costs for solution 'b' in each pair (where cost_a < cost_b).\", 'logp_a': \"Tensor of log probabilities for solution 'a' in each pair.\", 'logp_b': \"Tensor of log probabilities for solution 'b' in each pair.\"}"], "returns": "A scalar loss value, typically the mean of the loss over the batch."}}, "static_ok": false, "static_reason": "implementation_hint.returns must be 'scalar'."}
{"generation": 2, "index": 4, "ir": {"name": "AdaptiveMarginHingeLoss", "intuition": "This loss function, termed Adaptive Margin Hinge Loss (AMHL), is designed for robust preference learning in combinatorial optimization. Its core idea is to combine a hinge-loss structure with an adaptive margin that is sensitive to the magnitude of the cost difference. The loss penalizes the model only when its preference (indicated by log probability difference) contradicts the ground-truth cost preference, and the penalty's severity is scaled by how 'wrong' the model is. Specifically, it uses a softplus-based hinge mechanism (relu(margin - x)) for smoothness and stability. The 'margin' is not fixed; it is dynamically calculated based on the rank-normalized cost difference. This adaptive margin ensures that pairs with a large, clear cost difference enforce a stronger preference signal, while pairs with a small, ambiguous cost difference provide a gentler learning signal, preventing the model from over-optimizing on noisy or insignificant preferences. The use of tanh on the log probability difference and a clamped, normalized cost gap ensures that all inputs to the core loss calculation are bounded, preventing numerical instability from extreme input values.", "pseudocode": "def amhl_loss(cost_a, cost_b, logp_a, logp_b, beta, tau, margin_scale):\n    # 1. Determine the winning (w) and losing (l) solutions based on cost.\n    # We assume cost_a is the better solution (winner) without loss of generality.\n    cost_w, cost_l = cost_a, cost_b\n    logp_w, logp_l = logp_a, logp_b\n\n    # 2. Calculate the rank-normalized cost gap. This represents the 'strength' of the preference.\n    # The gap is clamped to [0, 1] for stability.\n    cost_gap = rank_gap(cost_l, cost_w) # Should be non-negative\n    normalized_gap = clamp(cost_gap, 0.0, 1.0)\n\n    # 3. Define an adaptive margin based on the normalized cost gap.\n    # A larger cost difference leads to a larger required margin.\n    adaptive_margin = margin_scale * normalized_gap\n\n    # 4. Calculate the log probability difference and scale it with a temperature `beta`.\n    # Use tanh to bound the difference between [-1, 1], preventing explosion.\n    logit_diff = logp_w - logp_l\n    bounded_logit_diff = tanh(beta * logit_diff)\n\n    # 5. Compute the core loss using a smooth hinge-loss formulation (softplus).\n    # The loss is incurred only if the bounded logit difference is less than the adaptive margin.\n    # `softplus(x) = log(1 + exp(x))` is a smooth approximation of `relu(x)`.\n    # We use `softplus(margin - x)` which is a smooth `relu(margin - x)`.\n    # The `tau` parameter controls the sharpness of the softplus function.\n    loss_value = softplus(tau * (adaptive_margin - bounded_logit_diff)) / tau\n\n    return loss_value", "hyperparams": {"beta": {"value": 1.0, "description": "Temperature parameter to scale the log probability difference before applying tanh. Controls the sensitivity to logit differences."}, "tau": {"value": 5.0, "description": "Sharpness parameter for the softplus function. Higher values make softplus behave more like relu, creating a harder hinge."}, "margin_scale": {"value": 0.5, "description": "Scaling factor for the adaptive margin. Controls the maximum target separation in the bounded logit space."}}, "operators_used": ["rank_gap", "clamp", "tanh", "softplus"], "implementation_hint": {"expects": ["{'name': 'cost_a', 'description': \"Scalar cost of solution 'a' (the preferred one, lower is better).\"}", "{'name': 'cost_b', 'description': \"Scalar cost of solution 'b' (the non-preferred one).\"}", "{'name': 'logp_a', 'description': \"Log probability of the preferred solution 'a' from the model.\"}", "{'name': 'logp_b', 'description': \"Log probability of the non-preferred solution 'b' from the model.\"}"], "returns": "A scalar, non-negative loss value."}}, "static_ok": false, "static_reason": "implementation_hint.returns must be 'scalar'."}
{"generation": 2, "index": 5, "ir": {"name": "AdaptiveMarginHingeLoss", "intuition": "This loss function, named Adaptive Margin Hinge Loss (AM-Hinge), is inspired by the classic hinge loss but introduces two key innovations for preference learning in combinatorial optimization. First, it uses an adaptive margin that is proportional to the normalized cost difference between two solutions. A larger cost difference implies a stronger preference, which should demand a larger log probability gap. Second, it uses `tanh` to squash the log probability difference, preventing the loss from exploding when the model is overly confident or wrong, thus ensuring numerical stability. The loss is activated only when the model's preference (`logp(a) - logp(b)`) is not sufficiently aligned with the ground truth preference (`cost(a) < cost(b)`) as defined by the adaptive margin. This encourages the model to learn the correct preference ranking, with a sense of proportionality, while remaining robust to extreme values.", "pseudocode": "def am_hinge_loss(cost_a, cost_b, logp_a, logp_b, alpha, beta, tau):\n    # Assume cost_a < cost_b (winner a, loser b)\n    # 1. Calculate the normalized cost difference. rank_gap ensures it's always non-negative.\n    cost_diff = rank_gap(cost_a, cost_b)\n    normalized_cost_diff = normalize(cost_diff)\n\n    # 2. Define an adaptive margin based on the normalized cost difference.\n    # A larger cost gap demands a larger log probability gap.\n    adaptive_margin = alpha * normalized_cost_diff\n\n    # 3. Calculate the log probability difference.\n    logp_diff = logp_a - logp_b\n\n    # 4. Use tanh to squash the log probability difference for stability.\n    # The scaling factor 'beta' controls the sensitivity of the squashed difference.\n    squashed_logp_diff = tau * tanh(beta * logp_diff)\n\n    # 5. Calculate the hinge-like loss.\n    # The loss is positive only if the squashed logp_diff is less than the required adaptive_margin.\n    loss = relu(adaptive_margin - squashed_logp_diff)\n\n    return loss", "hyperparams": {"alpha": {"value": 2.0, "description": "Margin scaling factor. Controls how much the normalized cost difference translates into a required log probability margin."}, "beta": {"value": 0.5, "description": "Log probability difference scaling factor before tanh. Controls the sensitivity in the central region of logp_diff."}, "tau": {"value": 1.0, "description": "Maximum value for the squashed log probability difference, as tanh output is scaled by tau. Can be seen as a cap on the effective logp_diff."}}, "operators_used": ["rank_gap", "normalize", "tanh", "relu"], "implementation_hint": {"expects": ["{'cost_w': 'Tensor of costs for the winning solutions in a batch.', 'cost_l': 'Tensor of costs for the losing solutions in a batch.', 'logp_w': 'Tensor of log probabilities for the winning solutions.', 'logp_l': 'Tensor of log probabilities for the losing solutions.'}"], "returns": "A scalar loss value, typically the mean of the batch."}}, "static_ok": false, "static_reason": "implementation_hint.returns must be 'scalar'."}
{"generation": 2, "index": 6, "ir": {"name": "AdaptiveMarginHingeLoss", "intuition": "This loss function, named Adaptive Margin Hinge Loss (AMHL), is inspired by the hinge loss used in SVMs but with a crucial modification for preference learning. The core idea is to create a 'margin' that is not fixed but dynamically adapts to the magnitude of the cost difference between two solutions. When the cost difference is large, the model is penalized more heavily for preferring the wrong solution, creating a strong learning signal. When the cost difference is small (i.e., the solutions are of similar quality), the margin shrinks, making the loss more tolerant to minor preference mismatches and preventing the model from overfitting to noisy or insignificant preferences. The use of `tanh` to scale the logit difference and `softplus` (a smooth version of ReLU) ensures the loss is smooth, always non-negative, and numerically stable, avoiding issues with extreme input values.", "pseudocode": "def amhl_loss(cost_a, cost_b, logp_a, logp_b, alpha, beta, tau):\n    # Ensure cost_w is the better solution (winner)\n    if cost_a < cost_b:\n        cost_w, cost_l = cost_a, cost_b\n        logp_w, logp_l = logp_a, logp_b\n    else:\n        cost_w, cost_l = cost_b, cost_a\n        logp_w, logp_l = logp_b, logp_a\n\n    # 1. Calculate the normalized cost difference (0 to 1 range)\n    # rank_gap is a safe way to handle cost differences, e.g., rank_gap(x, y) = tanh(x - y)\n    cost_delta = rank_gap(cost_l, cost_w) # a value in [0, 1) representing the normalized gap\n\n    # 2. Create an adaptive margin based on the cost difference\n    # The margin is larger for larger cost differences\n    adaptive_margin = alpha * cost_delta\n\n    # 3. Calculate the scaled log probability difference\n    # tanh helps to bound the logit difference, preventing extreme values from dominating the loss\n    logit_diff = logp_w - logp_l\n    scaled_logit_diff = beta * tanh(logit_diff / beta)\n\n    # 4. Compute the core hinge-like loss\n    # We want scaled_logit_diff to be greater than adaptive_margin\n    # The loss is softplus(margin - value), which is a smooth version of max(0, margin - value)\n    loss = softplus(adaptive_margin - scaled_logit_diff, beta=tau)\n\n    return loss", "hyperparams": {"alpha": {"value": 5.0, "description": "Scales the cost difference to determine the size of the adaptive margin. A larger alpha creates a stronger push for solutions with significant cost gaps."}, "beta": {"value": 1.0, "description": "Controls the saturation of the logit difference via tanh. A smaller beta makes the tanh saturate faster, reducing sensitivity to very large logit differences."}, "tau": {"value": 1.0, "description": "The 'temperature' or 'smoothness' parameter of the softplus function. A larger tau makes the softplus function approximate the hinge loss (ReLU) more closely."}}, "operators_used": ["rank_gap", "tanh", "softplus"], "implementation_hint": {"expects": ["{'cost_w': 'Tensor of costs for the winning solutions in a batch (lower cost).', 'cost_l': 'Tensor of costs for the losing solutions in a batch (higher cost).', 'logp_w': 'Tensor of model log-probabilities for the winning solutions.', 'logp_l': 'Tensor of model log-probabilities for the losing solutions.'}"], "returns": "A scalar loss value, typically the mean of the loss computed for each pair in the batch."}}, "static_ok": false, "static_reason": "implementation_hint.returns must be 'scalar'."}
{"generation": 2, "index": 7, "ir": {"name": "AdaptiveMarginHingeLoss", "intuition": "This loss function, named Adaptive Margin Hinge Loss (AMH-Loss), is designed to dynamically adjust the learning signal based on the magnitude of the cost difference between two solutions. It combines the robustness of a hinge-like loss with an adaptive margin that is a function of the cost difference. The core idea is that when the cost difference is large, the model should be more confident in its preference (i.e., the log probability difference should be larger). We use `tanh` to map the cost difference to a bounded range [-1, 1], creating a normalized 'preference strength'. This strength is then scaled by a hyperparameter `alpha` to form an adaptive margin. The loss is then a `softplus` (a smooth version of ReLU) applied to this margin minus the log probability difference. This structure ensures: 1) No penalty if the model's preference `logp(a) - logp(b)` correctly exceeds the adaptive margin. 2) A smoothly increasing penalty when the model's preference is incorrect or insufficient. 3) Bounded gradients and numerical stability due to the use of `tanh` and `softplus`, preventing issues with extreme cost or logit differences.", "pseudocode": "def amh_loss(cost_a, cost_b, logp_a, logp_b, alpha, beta):\n  # Calculate the difference in log probabilities\n  log_prob_diff = logp_a - logp_b\n\n  # Calculate the normalized and scaled cost difference\n  # rank_gap ensures the sign is correct (positive if a is better)\n  # tanh squashes the difference to [-1, 1] for stability\n  # beta scales the cost difference before the tanh, controlling sensitivity\n  cost_delta_normalized = tanh(beta * rank_gap(cost_b, cost_a))\n\n  # The adaptive margin is proportional to the normalized cost difference\n  # A larger cost improvement demands a larger log probability gap\n  adaptive_margin = alpha * cost_delta_normalized\n\n  # Hinge-like loss structure using softplus for smoothness\n  # Loss is incurred if log_prob_diff is not greater than the adaptive_margin\n  # The structure is softplus(margin - value), which is a smooth max(0, margin - value)\n  loss = softplus(adaptive_margin - log_prob_diff)\n\n  return loss", "hyperparams": {"alpha": {"default": 5.0, "description": "Scales the adaptive margin. A higher alpha demands a larger log probability difference for a given cost improvement, enforcing a stronger preference signal."}, "beta": {"default": 0.1, "description": "Scales the cost difference before the tanh function. It controls the sensitivity of the margin to the cost difference. A smaller beta makes the margin respond more linearly to small cost differences, while a larger beta pushes the margin towards its maximum value (+/-alpha) more quickly."}}, "operators_used": ["rank_gap", "tanh", "softplus"], "implementation_hint": {"expects": ["{'cost_a': \"Tensor of costs for solution 'a' in each pair.\", 'cost_b': \"Tensor of costs for solution 'b' in each pair.\", 'logp_a': \"Tensor of log probabilities for solution 'a' in each pair.\", 'logp_b': \"Tensor of log probabilities for solution 'b' in each pair.\"}"], "returns": "A scalar loss value, typically the mean of the loss over the batch of pairs."}}, "static_ok": false, "static_reason": "implementation_hint.returns must be 'scalar'."}
