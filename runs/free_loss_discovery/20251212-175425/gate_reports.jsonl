{"generation": 0, "index": 0, "ir": {"name": "Sigmoid-Scaled Hinge Loss with Rank-Gap Modulation", "intuition": "This loss function combines a hinge-like structure with two key modulations. First, it uses the rank-gap of cost differences to create a dynamic, instance-aware margin, pushing harder on pairs with a significant but not extreme cost difference. Second, it scales the loss by a sigmoid of the log-probability difference, which focuses the gradient on 'difficult' pairs where the model is uncertain or wrong, while saturating for pairs where the model is already confident. This prevents extreme cost or probability differences from dominating the training signal, ensuring numerical stability and balanced learning.", "pseudocode": "1. For each pair (w, l) where cost(w) < cost(l), calculate the log-probability difference: logp_diff = logp(w) - logp(l).\n2. Calculate the cost difference: cost_diff = cost(l) - cost(w).\n3. Normalize the cost differences across the batch using a rank-gap transformation to get a stable, scaled margin 'm'.\n4. Compute a hinge-like term: hinge = relu(m - logp_diff). This is the core penalty for incorrect preference.\n5. Compute a scaling factor based on the log-probability difference: scale = sigmoid(-beta * logp_diff). This factor is high when the model prefers the loser (logp_diff < 0) and low when it correctly prefers the winner.\n6. The final loss for the pair is the product of the hinge term and the scaling factor: loss = hinge * scale.\n7. The total loss is the weighted mean of individual pair losses over the batch.", "hyperparams": {"beta": 1.0, "gamma": 0.5, "epsilon": 1e-08}, "operators_used": ["log", "relu", "sigmoid", "rank_gap"], "implementation_hint": {"expects": ["cost_a", "cost_b", "logp_a", "logp_b"], "returns": "scalar"}, "code": "import torch\nimport torch.nn.functional as F\n\ndef rank_gap(x, gamma=0.5, epsilon=1e-8):\n    \"\"\"A whitelisted operator for stable cost normalization.\"\"\"\n    # Sort the values to get ranks\n    sorted_x, _ = torch.sort(x)\n    # Create a rank mapping\n    ranks = torch.empty_like(x)\n    ranks[torch.argsort(x)] = torch.arange(len(x), device=x.device, dtype=x.dtype)\n    # Normalize ranks to [0, 1]\n    normalized_ranks = ranks / (len(x) - 1 + epsilon)\n    # Apply a non-linear transform (e.g., logit-like)\n    # We use log to approximate a logit-like scaling for ranks near 0 and 1\n    transformed_ranks = torch.log(normalized_ranks + epsilon) - torch.log(1.0 - normalized_ranks + epsilon)\n    # Scale by gamma\n    return gamma * transformed_ranks\n\ndef generated_loss(batch, model_output, hyperparams):\n    \"\"\"\n    Computes the Sigmoid-Scaled Hinge Loss with Rank-Gap Modulation.\n\n    Args:\n        batch (dict): A dictionary containing tensors from the dataloader.\n                      Expected keys: 'cost_a', 'cost_b', 'weight' (optional).\n        model_output (dict): A dictionary containing model outputs.\n                             Expected keys: 'log_prob_w', 'log_prob_l'.\n        hyperparams (dict): A dictionary of hyperparameters for the loss.\n                            Expected keys: 'beta', 'gamma', 'epsilon'.\n\n    Returns:\n        torch.Tensor: A scalar tensor representing the final batch loss.\n    \"\"\"\n    # Unpack hyperparameters\n    beta = hyperparams.get('beta', 1.0)\n    gamma = hyperparams.get('gamma', 0.5)\n    epsilon = hyperparams.get('epsilon', 1e-8)\n\n    # Unpack data\n    # Assuming cost_w corresponds to log_prob_w, and cost_l to log_prob_l\n    cost_w = batch['cost_a'] # Winner costs\n    cost_l = batch['cost_b'] # Loser costs\n    logp_w = model_output['log_prob_w']\n    logp_l = model_output['log_prob_l']\n    weights = batch.get('weight', None)\n\n    # 1. Calculate log-probability difference\n    # logp_diff > 0 means the model correctly prefers the winner\n    logp_diff = logp_w - logp_l\n\n    # 2. Calculate positive cost difference\n    cost_diff = cost_l - cost_w\n\n    # 3. Normalize cost differences using rank_gap to create a dynamic margin 'm'\n    # This makes the margin robust to the scale of costs in the batch.\n    # We add epsilon to handle cases where all cost_diffs are identical.\n    margin = rank_gap(cost_diff, gamma=gamma, epsilon=epsilon)\n    # Ensure margin is non-negative, as rank_gap can be negative for small values\n    margin = F.relu(margin)\n\n    # 4. Compute the core hinge-like term\n    # This penalizes cases where logp_diff is smaller than the target margin.\n    hinge_term = F.relu(margin - logp_diff)\n\n    # 5. Compute the adaptive scaling factor\n    # This factor is close to 1 when the model is wrong (logp_diff << 0)\n    # and close to 0 when the model is very confident and correct (logp_diff >> 0).\n    # It focuses gradients on uncertain or incorrect predictions.\n    scaling_factor = torch.sigmoid(-beta * logp_diff)\n\n    # 6. Combine terms to get the loss for each pair\n    instance_loss = hinge_term * scaling_factor\n\n    # 7. Apply weights if provided and compute the final mean loss\n    if weights is not None:\n        loss = (instance_loss * weights).sum() / (weights.sum() + epsilon)\n    else:\n        loss = instance_loss.mean()\n\n    return loss"}, "static_ok": true, "static_reason": "", "dynamic_ok": false, "dynamic_reason": "forward_error: 'log_prob_w'", "loss_value": null, "grad_norm": null}
{"generation": 0, "index": 1, "ir": {"name": "Adaptive Sigmoid Margin Loss", "intuition": "This loss function compares the model's preference (difference in log probabilities) against the ground truth preference (difference in costs). It uses a sigmoid function to create a smooth, bounded loss. The key idea is an adaptive margin: the 'target' log probability difference that the model should achieve is not fixed, but scales with the normalized cost difference. If two solutions have very different costs, the model is pushed harder to separate them. If their costs are similar, the model is penalized less for being uncertain. This adaptation is achieved by scaling the log-probability difference by a factor derived from the tanh of the cost difference, which keeps the scaling bounded and stable.", "pseudocode": "1. For each pair (a, b) where cost(a) < cost(b), calculate the cost difference `delta_cost = cost(b) - cost(a)` and the log probability difference `delta_logp = logp(a) - logp(b)`.\n2. Normalize the cost difference using a tanh function to get a bounded 'cost signal' between 0 and 1: `cost_signal = tanh(delta_cost / temp_cost)`.\n3. Create an adaptive margin `margin_target` that is proportional to this cost signal: `margin_target = alpha * cost_signal`.\n4. The loss is then the negative log-sigmoid of the difference between the model's preference and the adaptive margin: `loss = -logsigmoid(delta_logp - margin_target)`. This pushes `delta_logp` to be greater than the margin.\n5. The final loss is the weighted mean of these individual losses over the batch.", "hyperparams": {"alpha": 2.0, "temp_cost": 1.0}, "operators_used": ["logsigmoid", "tanh"], "implementation_hint": {"expects": ["cost_a", "cost_b", "logp_a", "logp_b"], "returns": "scalar"}, "code": "import torch\nimport torch.nn.functional as F\n\ndef generated_loss(batch, model_output, extra):\n    \"\"\"\n    Computes the Adaptive Sigmoid Margin Loss.\n\n    The loss encourages the log probability of the better solution (winner) to be\n    higher than the log probability of the worse solution (loser).\n    The target margin between them is adaptive, scaling with the normalized\n    difference in their costs.\n\n    Args:\n        batch (dict): A dictionary containing tensors from the dataloader.\n                      Expected keys:\n                      'cost_a', 'cost_b': Costs of the two solutions in a pair. Shape: [N].\n                      'log_prob_w', 'log_prob_l': Model's log probabilities for the\n                                                winner and loser solutions. Shape: [N].\n                      'weight' (optional): Per-sample loss weights. Shape: [N].\n        model_output: Not used in this loss implementation.\n        extra (dict): A dictionary for hyperparameters. Expected keys:\n                      'alpha' (float): A scaling factor for the adaptive margin.\n                      'temp_cost' (float): A temperature for normalizing the cost difference.\n\n    Returns:\n        torch.Tensor: A scalar tensor representing the mean loss over the batch.\n    \"\"\"\n    # For preference learning, we assume the dataset provides pairs where one is strictly better.\n    # Let's define the winner (w) and loser (l) based on costs.\n    # cost_w < cost_l\n    cost_w = torch.min(batch['cost_a'], batch['cost_b'])\n    cost_l = torch.max(batch['cost_a'], batch['cost_b'])\n\n    # The 'log_prob_w' and 'log_prob_l' are assumed to be provided directly by the dataset/wrapper,\n    # corresponding to the winner and loser solutions respectively.\n    log_prob_w = batch['log_prob_w']\n    log_prob_l = batch['log_prob_l']\n\n    # Retrieve hyperparameters\n    alpha = extra.get('alpha', 2.0)\n    temp_cost = extra.get('temp_cost', 1.0)\n\n    # 1. Calculate cost and log probability differences\n    # delta_cost is guaranteed to be >= 0\n    delta_cost = cost_l - cost_w\n    delta_logp = log_prob_w - log_prob_l\n\n    # 2. Normalize the cost difference into a stable, bounded 'cost signal' [0, 1)\n    # Using tanh ensures numerical stability for extreme cost differences.\n    # temp_cost controls the sensitivity of the signal to the cost difference.\n    cost_signal = torch.tanh(delta_cost / temp_cost)\n\n    # 3. Create an adaptive margin based on the cost signal.\n    # The target separation in log-probs is proportional to how different the costs are.\n    # alpha scales the overall magnitude of the margin.\n    adaptive_margin = alpha * cost_signal\n\n    # 4. Compute the core loss using negative log-sigmoid.\n    # This is equivalent to softplus(-(delta_logp - adaptive_margin)).\n    # The loss is low if delta_logp > adaptive_margin (model agrees and exceeds margin).\n    # The loss is high if delta_logp < adaptive_margin (model disagrees or doesn't meet margin).\n    # The use of logsigmoid ensures the loss is bounded and smooth.\n    loss = -F.logsigmoid(delta_logp - adaptive_margin)\n\n    # 5. Apply optional weights and compute the mean.\n    weights = batch.get('weight', None)\n    if weights is not None:\n        loss = loss * weights\n        return loss.sum() / weights.sum().clamp(min=1e-8)\n    else:\n        return loss.mean()"}, "static_ok": true, "static_reason": "", "dynamic_ok": true, "dynamic_reason": "ok", "loss_value": 1.7204114198684692, "grad_norm": 0.0}
{"generation": 0, "index": 2, "ir": {"name": "Z-Scored Rank-Gap Preference Loss", "intuition": "This loss function conceptualizes the learning problem as aligning two different ranking systems: one based on true costs and another on model-assigned log-probabilities. It computes a 'rank gap' for both systems, which measures how much better one solution is than another. These gaps are then z-score normalized across the batch to make them comparable and robust to scale. The final loss is a softplus-regularized difference between these normalized gaps, which penalizes the model when its perceived preference gap (`logp(w) - logp(l)`) is smaller than the normalized cost gap, effectively pushing the model to prefer the better solution more strongly.", "pseudocode": "1. For a batch of (winner, loser) pairs, calculate the cost difference `delta_cost = cost(loser) - cost(winner)` and the log-probability difference `delta_logp = logp(winner) - logp(loser)`.\n2. Apply the `rank_gap` operator to `delta_cost` and `delta_logp` to get `cost_gap` and `logp_gap`. This non-linearly transforms the differences, making them more robust.\n3. Within the batch, compute the mean and standard deviation of `cost_gap` and `logp_gap`.\n4. Use these statistics to z-score normalize both `cost_gap` and `logp_gap`, yielding `z_cost_gap` and `z_logp_gap`.\n5. The core loss term is `softplus(margin - (z_cost_gap + z_logp_gap))`. This penalizes pairs where the combined normalized gaps do not exceed a certain margin, pushing the model to increase its preference for the winner, especially when the cost difference is large.\n6. Average this term across the batch to get the final scalar loss.", "hyperparams": {"margin": 1.0, "epsilon": 1e-08, "rank_gap_scale": 1.0}, "operators_used": ["rank_gap", "zscore", "softplus"], "implementation_hint": {"expects": ["cost_w", "cost_l", "log_prob_w", "log_prob_l"], "returns": "scalar"}, "code": "import torch\nimport torch.nn.functional as F\n\ndef rank_gap(x, scale=1.0):\n    \"\"\"A non-linear, bounded transformation of a difference.\"\"\"\n    return torch.tanh(x * scale)\n\ndef zscore(x, epsilon=1e-8):\n    \"\"\"Calculates z-score for a tensor across its elements.\"\"\"\n    if x.numel() <= 1:\n        return torch.zeros_like(x)\n    mean = x.mean()\n    std = x.std()\n    return (x - mean) / (std + epsilon)\n\ndef generated_loss(batch, model_output, extra):\n    \"\"\"\n    Implements the Z-Scored Rank-Gap Preference Loss.\n\n    Args:\n        batch (dict): A dictionary containing tensors from the data loader.\n                      Expected keys: 'cost_w', 'cost_l', 'log_prob_w', 'log_prob_l'.\n                      'cost_w' and 'cost_l' are costs of winner and loser solutions.\n                      'log_prob_w' and 'log_prob_l' are model-assigned log probabilities.\n        model_output: Not used in this loss formulation, as log-probs are in the batch.\n        extra (dict): A dictionary for hyperparameters. Expected keys: 'margin', 'epsilon', 'rank_gap_scale'.\n\n    Returns:\n        torch.Tensor: A scalar tensor representing the mean loss over the batch.\n    \"\"\"\n    cost_w = batch['cost_w']\n    cost_l = batch['cost_l']\n    logp_w = batch['log_prob_w']\n    logp_l = batch['log_prob_l']\n    \n    # Retrieve hyperparameters\n    margin = extra.get('margin', 1.0)\n    epsilon = extra.get('epsilon', 1e-8)\n    rank_gap_scale = extra.get('rank_gap_scale', 1.0)\n    \n    # 1. Calculate raw differences (deltas)\n    # delta_cost is positive, representing how much better the winner is\n    delta_cost = cost_l - cost_w\n    # delta_logp is the model's preference margin for the winner\n    delta_logp = logp_w - logp_l\n    \n    # 2. Apply a non-linear, bounded rank-gap transformation\n    # This makes the measure robust to extreme cost/logit differences\n    cost_gap = rank_gap(delta_cost, scale=rank_gap_scale)\n    logp_gap = rank_gap(delta_logp, scale=rank_gap_scale)\n    \n    # 3. Normalize the gaps using z-scoring across the batch\n    # This makes the gaps from different domains (cost vs. logp) comparable\n    z_cost_gap = zscore(cost_gap, epsilon=epsilon)\n    z_logp_gap = zscore(logp_gap, epsilon=epsilon)\n    \n    # 4. Calculate the core loss term\n    # The loss is incurred when the model's preference (z_logp_gap) plus the\n    # problem's difficulty (z_cost_gap) is less than a margin.\n    # We want z_cost_gap + z_logp_gap to be large and positive.\n    # The loss pushes the model to increase logp_w or decrease logp_l.\n    loss_per_item = F.softplus(margin - (z_cost_gap + z_logp_gap))\n    \n    # 5. Apply sample weights if provided\n    weights = batch.get('weight')\n    if weights is not None:\n        loss_per_item = loss_per_item * weights\n        \n    # 6. Return the mean loss\n    return loss_per_item.mean()"}, "static_ok": true, "static_reason": "", "dynamic_ok": false, "dynamic_reason": "forward_error: 'cost_w'", "loss_value": null, "grad_norm": null}
{"generation": 0, "index": 3, "ir": {"name": "Rank-Gap Attenuated Sigmoid Loss", "intuition": "This loss function uses a sigmoid-like structure to penalize incorrect preference rankings, similar to standard logistic loss. However, its behavior is modulated by the normalized difference in solution costs (the 'rank gap'). When the cost difference is large, the loss applies strong pressure to correct the model's preference. When the cost difference is small (i.e., the solutions are of similar quality), the loss signal is attenuated, preventing the model from overfitting to minor, potentially noisy differences in cost. A margin hyperparameter ensures a 'zone of indifference' where small log-probability differences are not penalized if the cost difference is also small. The use of `tanh` and `softplus` ensures numerical stability and bounded gradients for extreme inputs.", "pseudocode": "1. Calculate the difference in model log-probabilities: `logp_diff = logp(winner) - logp(loser)`. 2. Calculate the difference in costs: `cost_diff = cost(loser) - cost(winner)`. 3. Normalize the cost difference across the batch to get a stable 'rank gap' signal. 4. Transform the rank gap into a scaling factor `alpha` between 0 and 1 using a scaled `tanh` function. A large cost difference results in `alpha` close to 1; a small difference results in `alpha` close to 0. 5. Compute the core loss using `softplus` on `-(alpha * logp_diff - margin)`. This is a margin-based logistic loss where the log-probability difference is scaled by the cost-derived `alpha`. 6. The final loss is the mean of this value over the batch, optionally weighted.", "hyperparams": {"margin": 0.0, "tanh_scale": 1.0, "eps": 1e-08}, "operators_used": ["softplus", "tanh", "zscore", "clamp"], "implementation_hint": {"expects": ["cost_w", "cost_l", "log_prob_w", "log_prob_l"], "returns": "scalar"}, "code": "import torch\nimport torch.nn.functional as F\n\n# Helper for z-score normalization\ndef zscore(x, eps=1e-8):\n    # This helper is not strictly required by the function signature\n    # but is used conceptually in the implementation.\n    if x.numel() <= 1:\n        return torch.zeros_like(x)\n    return (x - x.mean()) / (x.std() + eps)\n\n\ndef generated_loss(batch, model_output, extra):\n    \"\"\"\n    Calculates the Rank-Gap Attenuated Sigmoid Loss.\n\n    Args:\n        batch (dict): A dictionary containing tensors from the data loader.\n                      Expected keys: 'cost_w', 'cost_l', 'log_prob_w', 'log_prob_l'.\n                      'cost_w' and 'cost_l' are costs of winner and loser solutions.\n                      'log_prob_w' and 'log_prob_l' are model's log probabilities.\n                      An optional 'weight' tensor can be used for weighted loss.\n        model_output: Not used in this loss formulation, but part of the signature.\n        extra (dict): A dictionary for hyperparameters. Expected keys: 'margin', 'tanh_scale', 'eps'.\n\n    Returns:\n        torch.Tensor: A scalar tensor representing the mean loss.\n    \"\"\"\n    # Read hyperparameters from the 'extra' dictionary\n    margin = extra.get('margin', 0.0)\n    tanh_scale = extra.get('tanh_scale', 1.0)\n    eps = extra.get('eps', 1e-8)\n\n    # Unpack data from the batch dictionary\n    cost_w = batch['cost_w']\n    cost_l = batch['cost_l']\n    logp_w = batch['log_prob_w']\n    logp_l = batch['log_prob_l']\n    weights = batch.get('weight', None)\n\n    # Step 1: Calculate log-probability difference\n    # logp_diff > 0 means the model correctly prefers the winner\n    logp_diff = logp_w - logp_l\n\n    # Step 2: Calculate cost difference (always non-negative by definition)\n    # cost_l should be >= cost_w\n    cost_diff = cost_l - cost_w\n\n    # Step 3: Normalize the cost difference to get a stable rank gap signal\n    # Using a simplified z-score logic directly\n    if cost_diff.numel() > 1:\n        cost_diff_std = cost_diff.std()\n        normalized_cost_diff = cost_diff / (cost_diff_std + eps)\n    else:\n        normalized_cost_diff = torch.zeros_like(cost_diff)\n    \n    # Clamp to prevent extreme values from having too much influence after normalization\n    normalized_cost_diff = torch.clamp(normalized_cost_diff, min=0.0, max=5.0)\n\n    # Step 4: Transform the rank gap into a scaling factor `alpha` using tanh\n    # alpha is in [0, 1]. It's ~0 for small cost diffs, and ~1 for large ones.\n    alpha = torch.tanh(tanh_scale * normalized_cost_diff)\n\n    # Step 5: Compute the core loss using softplus for numerical stability.\n    # This is equivalent to log(1 + exp(-(alpha * logp_diff - margin)))\n    # The loss is high if logp_diff is negative (wrong preference)\n    # The penalty is scaled by `alpha`, which depends on how different the costs are.\n    # The margin creates a zone of indifference.\n    loss_per_item = F.softplus(-(alpha * logp_diff - margin))\n\n    # Step 6: Apply optional sample weights and compute the mean\n    if weights is not None:\n        loss = (loss_per_item * weights).sum() / weights.sum().clamp(min=eps)\n    else:\n        loss = loss_per_item.mean()\n\n    return loss"}, "static_ok": true, "static_reason": "", "dynamic_ok": false, "dynamic_reason": "forward_error: 'cost_w'", "loss_value": null, "grad_norm": null}
{"generation": 0, "index": 4, "ir": {"name": "Sigmoid-Gated Adaptive Margin Loss", "intuition": "This loss function adapts its learning signal based on the magnitude of the cost difference between two solutions. It uses a sigmoid function of the normalized cost difference to create a 'gate' or 'weight'. When the cost difference is small, the gate is close to 0.5, applying a moderate learning signal. When the cost difference is large, the gate approaches 1, applying a stronger, more confident signal. This gate scales a softplus-based margin loss, which penalizes the model when its predicted preference (`logp(a) > logp(b)`) mismatches the ground truth (`cost(a) < cost(b)`). The use of `tanh` on the log-probability difference ensures the loss remains bounded even with extreme logit differences, preventing numerical instability.", "pseudocode": "1. Calculate the cost difference `delta_cost = cost(b) - cost(a)`.\n2. Normalize the cost difference, for example using a z-score over the batch. Let's call this `norm_delta_cost`.\n3. Compute a gating factor `gate = sigmoid(beta * norm_delta_cost)` which will be close to 1 for large cost differences and 0.5 for small ones.\n4. Calculate the log-probability difference `delta_logp = logp(a) - logp(b)`.\n5. To ensure stability, pass the log-probability difference through a `tanh` function: `stable_delta_logp = tanh(delta_logp / tau)`.\n6. The core loss is a softplus function, which acts like a smooth hinge loss: `base_loss = softplus(-stable_delta_logp)`.\n7. The final loss is the base loss scaled by the adaptive gate: `loss = gate * base_loss`.\n8. Average the loss over the batch.", "hyperparams": {"beta": 1.0, "tau": 5.0}, "operators_used": ["sigmoid", "softplus", "tanh", "zscore"], "implementation_hint": {"expects": ["cost_a", "cost_b", "logp_a", "logp_b"], "returns": "scalar"}, "code": "import torch\nimport torch.nn.functional as F\n\ndef zscore(x):\n    \"\"\"Numerically stable z-score normalization.\"\"\"\n    if x.numel() <= 1:\n        return torch.zeros_like(x)\n    mean = x.mean()\n    std = x.std()\n    # Add a small epsilon to prevent division by zero for constant inputs\n    return (x - mean) / (std + 1e-8)\n\ndef generated_loss(batch, model_output, extra):\n    \"\"\"\n    Computes the Sigmoid-Gated Adaptive Margin Loss.\n\n    The loss is designed to be numerically stable and adaptive to the magnitude\n    of the cost difference between preferred and non-preferred solutions.\n    \"\"\"\n    # Unpack costs. Assuming cost_a is the better (winning) solution.\n    # The problem statement implies cost(a) < cost(b), so 'a' is the winner.\n    cost_w = batch['cost_a']\n    cost_l = batch['cost_b']\n\n    # Unpack log probabilities from the model output\n    log_prob_w = model_output['log_prob_w']\n    log_prob_l = model_output['log_prob_l']\n    \n    # Optional weights per sample\n    weights = batch.get('weight', None)\n\n    # Retrieve hyperparameters\n    beta = extra.get('beta', 1.0)\n    tau = extra.get('tau', 5.0)\n\n    # 1. Calculate the cost difference. Positive since cost_l > cost_w.\n    delta_cost = cost_l - cost_w\n\n    # 2. Normalize the cost difference for stable gating.\n    # zscore is a whitelisted custom operator, implemented above.\n    norm_delta_cost = zscore(delta_cost)\n\n    # 3. Compute the adaptive gate using sigmoid.\n    # The gate value approaches 1 for large positive norm_delta_cost, and 0.5 for ~0.\n    gate = torch.sigmoid(beta * norm_delta_cost)\n\n    # 4. Calculate the log-probability difference.\n    # We want log_prob_w to be greater than log_prob_l.\n    delta_logp = log_prob_w - log_prob_l\n\n    # 5. Stabilize the log-probability difference using tanh.\n    # This bounds the input to the softplus, preventing Inf/NaN.\n    # The division by tau controls the saturation of the tanh.\n    stable_delta_logp = torch.tanh(delta_logp / tau)\n\n    # 6. Calculate the base loss using softplus.\n    # softplus(-x) is a smooth approximation of relu(-x).\n    # It penalizes cases where log_prob_w is not sufficiently larger than log_prob_l.\n    base_loss = F.softplus(-stable_delta_logp)\n\n    # 7. Apply the adaptive gate to the base loss.\n    instance_loss = gate * base_loss\n\n    # Apply optional weights\n    if weights is not None:\n        instance_loss = instance_loss * weights\n\n    # 8. Return the mean loss over the batch.\n    return instance_loss.mean()"}, "static_ok": true, "static_reason": "", "dynamic_ok": false, "dynamic_reason": "forward_error: 'log_prob_w'", "loss_value": null, "grad_norm": null}
{"generation": 0, "index": 5, "ir": {"name": "Sigmoid-Gated Rank Gap Loss", "intuition": "This loss function uses the difference in solution costs to dynamically scale the learning signal. It combines a rank-based preference term (similar to DPO) with a sigmoid-gated magnitude term derived from the normalized cost difference. When the cost difference is small, the loss behaves like a standard preference loss. When the cost difference is large, the sigmoid gate 'opens up', amplifying the loss to more strongly enforce the preference. This allows the model to learn more aggressively from clear-cut examples while being less sensitive to noisy preferences where costs are nearly identical. A softplus function ensures the final loss is non-negative and well-behaved.", "pseudocode": "1. For each pair of solutions (winner, loser), calculate the log-probability difference: logp_diff = logp(winner) - logp(loser).\n2. Calculate the cost difference: cost_diff = cost(loser) - cost(winner). This is always non-negative.\n3. Normalize the cost differences across the batch using z-score to make them scale-invariant.\n4. Compute a 'magnitude gate' by passing the normalized cost difference through a sigmoid function. This gate value is close to 0.5 for small cost differences and approaches 1.0 for large differences.\n5. Compute the core preference loss term by multiplying the log-probability difference by the magnitude gate. This scales the preference signal by how 'obvious' the win is.\n6. Apply a softplus function to the result, with a temperature scaling parameter, to create a smooth, non-negative loss. This ensures that when logp(winner) > logp(loser), the loss approaches zero, and when logp(winner) < logp(loser), the loss increases.\n7. Take the weighted mean of this value across the batch.", "hyperparams": {"tau": 1.0, "gate_temp": 1.0, "eps": 1e-08}, "operators_used": ["zscore", "sigmoid", "softplus"], "implementation_hint": {"expects": ["cost_a", "cost_b", "log_prob_w", "log_prob_l"], "returns": "scalar"}, "code": "import torch\nimport torch.nn.functional as F\n\ndef zscore(x, eps=1e-8):\n    \"\"\"Custom z-score normalization to be whitelisted.\"\"\"\n    if x.numel() <= 1:\n        return torch.zeros_like(x)\n    mean = x.mean()\n    std = x.std()\n    return (x - mean) / (std + eps)\n\ndef generated_loss(batch, model_output, extra):\n    \"\"\"\n    Computes the Sigmoid-Gated Rank Gap Loss.\n\n    Args:\n        batch (dict): A dictionary containing tensors from the dataloader.\n                      Expected keys:\n                      - 'cost_a': Costs of the winning solutions (lower is better), shape [N].\n                      - 'cost_b': Costs of the losing solutions, shape [N].\n                      - 'log_prob_w': Log probabilities of winning solutions, shape [N].\n                      - 'log_prob_l': Log probabilities of losing solutions, shape [N].\n                      - 'weight' (optional): Per-sample weights, shape [N].\n        model_output (dict): Output from the model (not used in this loss structure).\n        extra (dict): Dictionary for hyperparameters. Expected keys:\n                      - 'tau': Temperature for the final softplus, controls sharpness.\n                      - 'gate_temp': Temperature for the sigmoid gate.\n                      - 'eps': Epsilon for stable normalization.\n\n    Returns:\n        torch.Tensor: A scalar loss value.\n    \"\"\"\n    # 1. Unpack inputs\n    # Note: The problem statement uses (a, b) where cost(a) < cost(b).\n    # The code uses (w, l) for winner/loser for clarity.\n    cost_w = batch['cost_a']\n    cost_l = batch['cost_b']\n    logp_w = batch['log_prob_w']\n    logp_l = batch['log_prob_l']\n    weights = batch.get('weight', None)\n\n    # 2. Get hyperparameters\n    tau = extra.get('tau', 1.0)\n    gate_temp = extra.get('gate_temp', 1.0)\n    eps = extra.get('eps', 1e-8)\n\n    # 3. Calculate log probability and cost differences\n    logp_diff = logp_w - logp_l  # We want this to be positive\n    cost_diff = cost_l - cost_w  # By definition, this is >= 0\n\n    # 4. Normalize cost differences to get a stable signal for the gate\n    # zscore is a whitelisted custom operator\n    normalized_cost_diff = zscore(cost_diff, eps=eps)\n\n    # 5. Compute the sigmoid gate based on normalized cost difference\n    # This gate value approaches 1 for large cost gaps and is ~0.5 for average gaps.\n    magnitude_gate = torch.sigmoid(gate_temp * normalized_cost_diff)\n\n    # 6. Compute the core preference term\n    # The loss signal is -logp_diff. We want to penalize when logp_diff is negative.\n    # We scale this signal by the magnitude_gate.\n    # The negative sign makes it a quantity to be minimized (loss).\n    gated_rank_term = -logp_diff * magnitude_gate\n\n    # 7. Apply softplus for a smooth, non-negative, and robust loss\n    # The term inside softplus is positive when we need to apply a penalty\n    # and negative when the model's preference is correct.\n    loss_per_sample = F.softplus(gated_rank_term / tau) * tau\n\n    # 8. Apply weights and compute the mean loss\n    if weights is not None:\n        loss = (loss_per_sample * weights).sum() / weights.sum().clamp(min=eps)\n    else:\n        loss = loss_per_sample.mean()\n\n    return loss"}, "static_ok": true, "static_reason": "", "dynamic_ok": false, "dynamic_reason": "forward_error: name 'zscore' is not defined", "loss_value": null, "grad_norm": null}
{"generation": 0, "index": 6, "ir": {"name": "Adaptive Margin Ranking Loss with Cost Normalization", "intuition": "This loss function uses a dynamic margin that adapts to the magnitude of the cost difference between two solutions. For pairs with a large cost gap, the model is pushed more strongly to get the preference right. For pairs with a small, almost negligible cost gap, the margin is smaller, relaxing the pressure. This prevents the model from obsessing over trivial differences while focusing its capacity on learning meaningful preferences. Both the cost differences and log-probability differences are normalized to ensure numerical stability and consistent gradient behavior across different problem scales and model states.", "pseudocode": "1. For a batch of solution pairs (a, b), calculate the cost difference ΔC = cost(b) - cost(a) and the log-probability difference ΔL = logp(a) - logp(b). Note that ΔC is positive if 'a' is the better solution.\n2. Normalize the cost differences across the batch using a robust method like z-score normalization to get ΔC_norm. This makes the loss scale-invariant.\n3. Create an adaptive margin 'm' that is a function of the normalized cost difference. We use a sigmoid function: m = α * sigmoid(β * ΔC_norm), where α and β are hyperparameters controlling the margin's scale and steepness.\n4. The core of the loss is a margin-based hinge loss: max(0, m - ΔL). This penalizes the model only if the log-probability difference ΔL is smaller than the required margin 'm'.\n5. Apply a softplus function to the hinge loss result, i.e., softplus(m - ΔL), to create a smooth, non-zero gradient everywhere, which can improve optimization stability compared to a hard relu/hinge.\n6. Average the loss over the batch, applying optional sample weights if provided.", "hyperparams": {"alpha": 2.0, "beta": 1.0, "epsilon": 1e-08}, "operators_used": ["softplus", "sigmoid", "zscore"], "implementation_hint": {"expects": ["cost_a", "cost_b", "logp_a", "logp_b"], "returns": "scalar"}, "code": "import torch\nimport torch.nn.functional as F\n\ndef zscore(x, epsilon=1e-8):\n    \"\"\"Custom z-score normalization to be whitelisted.\"\"\"\n    mean = x.mean()\n    std = x.std()\n    return (x - mean) / (std + epsilon)\n\ndef generated_loss(batch, model_output, extra):\n    \"\"\"\n    Calculates the Adaptive Margin Ranking Loss.\n\n    The better solution 'a' is assumed to be `log_prob_w` (winner) and\n    the worse solution 'b' is assumed to be `log_prob_l` (loser).\n    So, cost_a < cost_b.\n    \"\"\"\n    # Read hyperparameters with defaults\n    hyperparams = extra.get('hyperparams', {})\n    alpha = hyperparams.get('alpha', 2.0)\n    beta = hyperparams.get('beta', 1.0)\n    epsilon = hyperparams.get('epsilon', 1e-8)\n\n    # Unpack batch data\n    cost_a = batch['cost_a']\n    cost_b = batch['cost_b']\n    logp_a = batch['log_prob_w']\n    logp_b = batch['log_prob_l']\n    weights = batch.get('weight', None)\n\n    # Ensure cost_a is better (lower) than cost_b\n    # This is a strong assumption based on 'w' and 'l' naming\n    cost_diff = cost_b - cost_a\n\n    # Step 1: Normalize the cost differences to be scale-invariant\n    # This is a whitelisted custom operator\n    if cost_diff.numel() > 1:\n        normalized_cost_diff = zscore(cost_diff, epsilon=epsilon)\n    else:\n        # Handle batch size of 1 where zscore is trivial\n        normalized_cost_diff = torch.zeros_like(cost_diff)\n\n    # Step 2: Create an adaptive margin based on the normalized cost difference\n    # The margin is larger for pairs with a more significant cost gap.\n    # sigmoid ensures the margin is bounded and smooth.\n    margin = alpha * torch.sigmoid(beta * normalized_cost_diff)\n\n    # Step 3: Calculate the log-probability difference\n    # We want logp(a) > logp(b), so logp_diff should be positive.\n    logp_diff = logp_a - logp_b\n\n    # Step 4: Calculate the core loss using a smooth hinge-like objective\n    # The loss is softplus(margin - logp_diff).\n    # This penalizes cases where logp_diff < margin.\n    # softplus(x) = log(1 + exp(x)), which is a smooth version of relu(x).\n    loss_per_item = F.softplus(margin - logp_diff)\n\n    # Step 5: Apply weights and compute the final mean loss\n    if weights is not None:\n        loss = (loss_per_item * weights).sum() / weights.sum().clamp(min=epsilon)\n    else:\n        loss = loss_per_item.mean()\n\n    return loss"}, "static_ok": true, "static_reason": "", "dynamic_ok": false, "dynamic_reason": "forward_error: name 'zscore' is not defined", "loss_value": null, "grad_norm": null}
{"generation": 0, "index": 7, "ir": {"name": "Sigmoid-Scaled Adaptive Margin Loss", "intuition": "This loss function uses an adaptive margin that grows with the relative cost difference between two solutions. The intuition is that if one solution is vastly better than another, the model's log-probability gap should be proportionally larger. The cost difference is scaled by a sigmoid function to keep it bounded and prevent extreme values from dominating the loss. The final loss is a softplus, which is a smooth, non-negative approximation of a hinge loss, ensuring that the model is penalized when its preference is incorrect but the penalty saturates gracefully.", "pseudocode": "1. Calculate the cost difference: delta_cost = cost(b) - cost(a). Since a is better, this is positive.\n2. Calculate the log-probability difference: delta_logp = logp(a) - logp(b).\n3. Normalize the cost difference using a sigmoid function, scaled by a temperature hyperparameter 'tau'. This creates a bounded 'cost signal' between 0 and 1.\n4. Create an adaptive margin by scaling this cost signal with a hyperparameter 'alpha'. The margin is now: margin = alpha * sigmoid(delta_cost / tau).\n5. The core of the loss is to enforce that delta_logp should be greater than the margin. We formulate this as: loss = softplus(margin - delta_logp).\n6. If sample weights are provided, multiply the loss for each pair by its weight.\n7. Return the mean of the losses over the batch.", "hyperparams": {"alpha": 1.0, "tau": 1.0}, "operators_used": ["sigmoid", "softplus"], "implementation_hint": {"expects": ["cost_a", "cost_b", "logp_a", "logp_b"], "returns": "scalar"}, "code": "import torch\nimport torch.nn.functional as F\n\ndef generated_loss(batch, model_output, extra):\n    \"\"\"\n    Calculates the Sigmoid-Scaled Adaptive Margin Loss.\n\n    Args:\n        batch (dict): A dictionary containing tensors from the dataloader.\n                      Expected keys:\n                      - 'cost_a': Cost of the better solution (winner), shape [N].\n                      - 'cost_b': Cost of the worse solution (loser), shape [N].\n                      - 'weight': Optional sample weights, shape [N].\n        model_output (dict): A dictionary containing model outputs.\n                             Expected keys:\n                             - 'log_prob_w': Log probability of the winner, shape [N].\n                             - 'log_prob_l': Log probability of the loser, shape [N].\n        extra (dict): A dictionary for hyperparameters.\n                      Expected keys:\n                      - 'alpha': Scaling factor for the margin.\n                      - 'tau': Temperature for scaling the cost difference.\n\n    Returns:\n        torch.Tensor: A scalar tensor representing the mean loss.\n    \"\"\"\n    # Retrieve hyperparameters\n    alpha = extra.get('alpha', 1.0)\n    tau = extra.get('tau', 1.0)\n\n    # Unpack inputs, assuming 'a' is the winner and 'b' is the loser\n    cost_w = batch['cost_a']\n    cost_l = batch['cost_b']\n    logp_w = model_output['log_prob_w']\n    logp_l = model_output['log_prob_l']\n\n    # Ensure costs are floats to avoid integer division issues\n    delta_cost = (cost_l - cost_w).float()\n    delta_logp = logp_w - logp_l\n\n    # 1. Create the adaptive margin\n    # The margin is based on the sigmoid of the normalized cost difference.\n    # sigmoid makes the margin bounded between 0 and alpha.\n    # tau controls how quickly the margin saturates with increasing cost difference.\n    # A small tau makes it sensitive to small cost differences.\n    margin = alpha * torch.sigmoid(delta_cost / tau)\n\n    # 2. Calculate the core loss using softplus\n    # softplus(margin - delta_logp) is a smooth hinge-like loss.\n    # It penalizes cases where delta_logp < margin.\n    # If delta_logp > margin, the loss is close to zero (reward).\n    # If delta_logp < margin, the loss is positive (penalty).\n    loss_per_item = F.softplus(margin - delta_logp)\n\n    # 3. Apply optional sample weights\n    weights = batch.get('weight', None)\n    if weights is not None:\n        loss_per_item = loss_per_item * weights\n\n    # 4. Return the mean loss over the batch\n    return loss_per_item.mean()"}, "static_ok": true, "static_reason": "", "dynamic_ok": false, "dynamic_reason": "forward_error: 'log_prob_w'", "loss_value": null, "grad_norm": null}
{"generation": 1, "index": 0, "ir": {"name": "Softplus Adaptive Margin Loss", "intuition": "This loss function compares the model's preference (difference in log probabilities) against the ground truth preference (difference in costs). It uses a softplus function to create a smooth, non-negative loss. The key idea is an adaptive margin: the 'target' log probability difference that the model should achieve is not fixed, but scales with the normalized cost difference. If two solutions have very different costs, the model is pushed harder to separate them. If their costs are similar, the model is penalized less for being uncertain. This adaptation is achieved by scaling the log-probability difference by a factor derived from the tanh of the cost difference, which keeps the scaling bounded and stable. Using softplus instead of negative logsigmoid is a common alternative for hinge-like losses.", "pseudocode": "1. For each pair (a, b) where cost(a) < cost(b), calculate the cost difference `delta_cost = cost(b) - cost(a)` and the log probability difference `delta_logp = logp(a) - logp(b)`.\n2. Normalize the cost difference using a tanh function to get a bounded 'cost signal' between 0 and 1: `cost_signal = tanh(delta_cost / temp_cost)`.\n3. Create an adaptive margin `margin_target` that is proportional to this cost signal: `margin_target = alpha * cost_signal`.\n4. The loss is then the softplus of the difference between the adaptive margin and the model's preference: `loss = softplus(margin_target - delta_logp)`. This pushes `delta_logp` to be greater than the margin.\n5. The final loss is the weighted mean of these individual losses over the batch.", "hyperparams": {"alpha": 2.0, "temp_cost": 1.0}, "operators_used": ["softplus", "tanh"], "implementation_hint": {"expects": ["cost_a", "cost_b", "log_prob_w", "log_prob_l", "weight"], "returns": "scalar"}, "code": "import torch\nimport torch.nn.functional as F\n\ndef generated_loss(batch, model_output, extra):\n    \"\"\"\n    Computes the Softplus Adaptive Margin Loss.\n\n    The loss encourages the log probability of the better solution (winner) to be\n    higher than the log probability of the worse solution (loser).\n    The target margin between them is adaptive, scaling with the normalized\n    difference in their costs. This is a conservative mutation of the parent\n    loss, replacing `-logsigmoid(x)` with `softplus(-x)` which are mathematically equivalent.\n\n    Args:\n        batch (dict): A dictionary containing tensors from the dataloader.\n                      Expected keys:\n                      'cost_a', 'cost_b': Costs of the two solutions in a pair. Shape: [N].\n                      'log_prob_w', 'log_prob_l': Model's log probabilities for the\n                                                winner and loser solutions. Shape: [N].\n                      'weight' (optional): Per-sample loss weights. Shape: [N].\n        model_output: Not used in this loss implementation.\n        extra (dict): A dictionary for hyperparameters. Expected keys:\n                      'alpha' (float): A scaling factor for the adaptive margin.\n                      'temp_cost' (float): A temperature for normalizing the cost difference.\n\n    Returns:\n        torch.Tensor: A scalar tensor representing the mean loss over the batch.\n    \"\"\"\n    # For preference learning, we assume the dataset provides pairs where one is strictly better.\n    # Let's define the winner (w) and loser (l) based on costs.\n    # cost_w < cost_l\n    cost_w = torch.min(batch['cost_a'], batch['cost_b'])\n    cost_l = torch.max(batch['cost_a'], batch['cost_b'])\n\n    # The 'log_prob_w' and 'log_prob_l' are assumed to be provided directly by the dataset/wrapper,\n    # corresponding to the winner and loser solutions respectively.\n    log_prob_w = batch['log_prob_w']\n    log_prob_l = batch['log_prob_l']\n\n    # Retrieve hyperparameters\n    alpha = extra.get('alpha', 2.0)\n    temp_cost = extra.get('temp_cost', 1.0)\n\n    # 1. Calculate cost and log probability differences\n    # delta_cost is guaranteed to be >= 0\n    delta_cost = cost_l - cost_w\n    delta_logp = log_prob_w - log_prob_l\n\n    # 2. Normalize the cost difference into a stable, bounded 'cost signal' [0, 1)\n    # Using tanh ensures numerical stability for extreme cost differences.\n    # temp_cost controls the sensitivity of the signal to the cost difference.\n    cost_signal = torch.tanh(delta_cost / temp_cost)\n\n    # 3. Create an adaptive margin based on the cost signal.\n    # The target separation in log-probs is proportional to how different the costs are.\n    # alpha scales the overall magnitude of the margin.\n    adaptive_margin = alpha * cost_signal\n\n    # 4. Compute the core loss using softplus.\n    # This is equivalent to -logsigmoid(delta_logp - adaptive_margin).\n    # The loss is low if delta_logp > adaptive_margin (model agrees and exceeds margin).\n    # The loss is high if delta_logp < adaptive_margin (model disagrees or doesn't meet margin).\n    loss = F.softplus(adaptive_margin - delta_logp)\n\n    # 5. Apply optional weights and compute the mean.\n    weights = batch.get('weight', None)\n    if weights is not None:\n        loss = loss * weights\n        # Use clamp for numerical stability if weights sum to zero\n        return loss.sum() / weights.sum().clamp(min=1e-8)\n    else:\n        return loss.mean()"}, "static_ok": true, "static_reason": "", "dynamic_ok": true, "dynamic_reason": "ok", "loss_value": 1.7204114198684692, "grad_norm": 0.0}
{"generation": 1, "index": 1, "ir": {"name": "Adaptive Sigmoid Margin Loss with Softplus", "intuition": "This loss function compares the model's preference (difference in log probabilities) against a target margin that adapts to the cost difference. It uses a sigmoid function to create a smooth, bounded loss. The adaptive margin scales with the cost difference, pushing the model harder to separate solutions with large cost gaps. This version replaces `tanh` with `softplus` for normalizing the cost difference. This creates a cost signal that is unbounded but grows sub-linearly, providing a strong signal for large cost differences while still being smooth and non-negative.", "pseudocode": "1. For each pair (a, b) where cost(a) < cost(b), calculate the cost difference `delta_cost = cost(b) - cost(a)` and the log probability difference `delta_logp = logp(a) - logp(b)`.\n2. Normalize the cost difference using a softplus function to get a smooth, non-negative 'cost signal': `cost_signal = softplus(delta_cost / temp_cost)`.\n3. Create an adaptive margin `margin_target` that is proportional to this cost signal: `margin_target = alpha * cost_signal`.\n4. The loss is then the negative log-sigmoid of the difference between the model's preference and the adaptive margin: `loss = -logsigmoid(delta_logp - margin_target)`. This pushes `delta_logp` to be greater than the margin.\n5. The final loss is the weighted mean of these individual losses over the batch.", "hyperparams": {"alpha": 1.0, "temp_cost": 1.0}, "operators_used": ["logsigmoid", "softplus"], "implementation_hint": {"expects": ["cost_a", "cost_b", "log_prob_w", "log_prob_l", "weight"], "returns": "scalar"}, "code": "import torch\nimport torch.nn.functional as F\n\ndef generated_loss(batch, model_output, extra):\n    \"\"\"\n    Computes the Adaptive Sigmoid Margin Loss with a Softplus-based cost signal.\n\n    The loss encourages the log probability of the better solution (winner) to be\n    higher than the log probability of the worse solution (loser).\n    The target margin between them is adaptive, scaling with the normalized\n    difference in their costs.\n\n    Args:\n        batch (dict): A dictionary containing tensors from the dataloader.\n                      Expected keys:\n                      'cost_a', 'cost_b': Costs of the two solutions in a pair. Shape: [N].\n                      'log_prob_w', 'log_prob_l': Model's log probabilities for the\n                                                winner and loser solutions. Shape: [N].\n                      'weight' (optional): Per-sample loss weights. Shape: [N].\n        model_output: Not used in this loss implementation.\n        extra (dict): A dictionary for hyperparameters. Expected keys:\n                      'alpha' (float): A scaling factor for the adaptive margin.\n                      'temp_cost' (float): A temperature for normalizing the cost difference.\n\n    Returns:\n        torch.Tensor: A scalar tensor representing the mean loss over the batch.\n    \"\"\"\n    # For preference learning, we assume the dataset provides pairs where one is strictly better.\n    # Let's define the winner (w) and loser (l) based on costs.\n    # cost_w < cost_l\n    cost_w = torch.min(batch['cost_a'], batch['cost_b'])\n    cost_l = torch.max(batch['cost_a'], batch['cost_b'])\n\n    # The 'log_prob_w' and 'log_prob_l' are assumed to be provided directly by the dataset/wrapper,\n    # corresponding to the winner and loser solutions respectively.\n    log_prob_w = batch['log_prob_w']\n    log_prob_l = batch['log_prob_l']\n\n    # Retrieve hyperparameters\n    alpha = extra.get('alpha', 1.0)\n    temp_cost = extra.get('temp_cost', 1.0)\n\n    # 1. Calculate cost and log probability differences\n    # delta_cost is guaranteed to be >= 0\n    delta_cost = cost_l - cost_w\n    delta_logp = log_prob_w - log_prob_l\n\n    # 2. Normalize the cost difference into a smooth, non-negative 'cost signal'\n    # Using softplus instead of tanh creates an unbounded but sub-linearly growing signal.\n    # temp_cost controls the sensitivity of the signal to the cost difference.\n    cost_signal = F.softplus(delta_cost / temp_cost)\n\n    # 3. Create an adaptive margin based on the cost signal.\n    # The target separation in log-probs is proportional to how different the costs are.\n    # alpha scales the overall magnitude of the margin.\n    adaptive_margin = alpha * cost_signal\n\n    # 4. Compute the core loss using negative log-sigmoid.\n    # This is equivalent to softplus(-(delta_logp - adaptive_margin)).\n    # The loss is low if delta_logp > adaptive_margin (model agrees and exceeds margin).\n    # The loss is high if delta_logp < adaptive_margin (model disagrees or doesn't meet margin).\n    loss = -F.logsigmoid(delta_logp - adaptive_margin)\n\n    # 5. Apply optional weights and compute the mean.\n    weights = batch.get('weight', None)\n    if weights is not None:\n        loss = loss * weights\n        # Use a small epsilon to prevent division by zero if all weights are zero.\n        return loss.sum() / weights.sum().clamp(min=1e-8)\n    else:\n        return loss.mean()"}, "static_ok": true, "static_reason": "", "dynamic_ok": true, "dynamic_reason": "ok", "loss_value": 1.5514447689056396, "grad_norm": 0.0}
{"generation": 1, "index": 2, "ir": {"name": "Sigmoid Log-Cost Margin Loss", "intuition": "This loss function compares the model's preference (difference in log probabilities) against the ground truth preference (difference in costs). It uses a sigmoid loss structure. The key idea is to define the target margin directly from the logarithm of the cost difference. Using `log(1 + delta_cost)` ensures that the margin is zero when costs are equal, grows as the cost difference increases, and is less sensitive to very large cost differences due to the compressive nature of the logarithm. This provides a natural, parameter-free way to scale the margin based on the magnitude of the cost gap, pushing the model harder to separate pairs with larger cost differences.", "pseudocode": "1. For each pair (a, b) where cost(a) < cost(b), calculate the cost difference `delta_cost = cost(b) - cost(a)` and the log probability difference `delta_logp = logp(a) - logp(b)`.\n2. Define a margin target based on the logarithm of the cost difference: `margin_target = log(1 + delta_cost / temp)`.\n3. The loss is then the negative log-sigmoid of the difference between the model's preference and the log-cost margin: `loss = -logsigmoid(delta_logp - margin_target)`. This pushes `delta_logp` to be greater than the margin.\n4. The final loss is the weighted mean of these individual losses over the batch.", "hyperparams": {"temp": 1.0}, "operators_used": ["logsigmoid", "log"], "implementation_hint": {"expects": ["cost_a", "cost_b", "log_prob_w", "log_prob_l", "weight"], "returns": "scalar"}, "code": "import torch\nimport torch.nn.functional as F\n\ndef generated_loss(batch, model_output, extra):\n    \"\"\"\n    Computes the Sigmoid Log-Cost Margin Loss.\n\n    The loss encourages the log probability of the better solution (winner) to be\n    higher than the log probability of the worse solution (loser).\n    The target margin between them is adaptive, scaling with the logarithm of the\n    cost difference.\n\n    Args:\n        batch (dict): A dictionary containing tensors from the dataloader.\n                      Expected keys:\n                      'cost_a', 'cost_b': Costs of the two solutions in a pair. Shape: [N].\n                      'log_prob_w', 'log_prob_l': Model's log probabilities for the\n                                                winner and loser solutions. Shape: [N].\n                      'weight' (optional): Per-sample loss weights. Shape: [N].\n        model_output: Not used in this loss implementation.\n        extra (dict): A dictionary for hyperparameters. Expected keys:\n                      'temp' (float): A temperature for scaling the cost difference before the log.\n\n    Returns:\n        torch.Tensor: A scalar tensor representing the mean loss over the batch.\n    \"\"\"\n    # For preference learning, we assume the dataset provides pairs where one is strictly better.\n    # cost_w < cost_l\n    cost_w = torch.min(batch['cost_a'], batch['cost_b'])\n    cost_l = torch.max(batch['cost_a'], batch['cost_b'])\n\n    # The 'log_prob_w' and 'log_prob_l' are assumed to be provided directly by the dataset/wrapper,\n    # corresponding to the winner and loser solutions respectively.\n    log_prob_w = batch['log_prob_w']\n    log_prob_l = batch['log_prob_l']\n\n    # Retrieve hyperparameters\n    temp = extra.get('temp', 1.0)\n\n    # 1. Calculate cost and log probability differences\n    # delta_cost is guaranteed to be >= 0\n    delta_cost = cost_l - cost_w\n    delta_logp = log_prob_w - log_prob_l\n\n    # 2. Create an adaptive margin based on the log of the cost difference.\n    # Using log(1 + x) ensures the margin is 0 when delta_cost is 0 and grows monotonically.\n    # The log naturally dampens the effect of very large cost differences.\n    # temp scales the cost difference before the logarithm.\n    log_cost_margin = torch.log(1 + delta_cost / temp)\n\n    # 3. Compute the core loss using negative log-sigmoid.\n    # This is equivalent to softplus(-(delta_logp - log_cost_margin)).\n    # The loss is low if delta_logp > log_cost_margin (model agrees and exceeds margin).\n    # The loss is high if delta_logp < log_cost_margin (model disagrees or doesn't meet margin).\n    loss = -F.logsigmoid(delta_logp - log_cost_margin)\n\n    # 4. Apply optional weights and compute the mean.\n    weights = batch.get('weight', None)\n    if weights is not None:\n        loss = loss * weights\n        # Use a small epsilon to prevent division by zero if all weights are zero.\n        return loss.sum() / weights.sum().clamp(min=1e-8)\n    else:\n        return loss.mean()"}, "static_ok": true, "static_reason": "", "dynamic_ok": true, "dynamic_reason": "ok", "loss_value": 1.0986123085021973, "grad_norm": 0.0}
{"generation": 1, "index": 3, "ir": {"name": "Adaptive Sigmoid Margin Loss with Z-Score", "intuition": "This loss function compares the model's preference (difference in log probabilities) against the ground truth preference (difference in costs). It uses a sigmoid function to create a smooth, bounded loss. The key idea is an adaptive margin: the 'target' log probability difference that the model should achieve scales with the cost difference. This version uses z-score normalization on the cost differences within the batch, making the margin's sensitivity robust to the overall scale of costs in a given batch. If two solutions have a cost difference that is large *relative to other pairs in the batch*, the model is pushed harder to separate them. This is a conservative change from the parent, which used a fixed temperature scaling.", "pseudocode": "1. For each pair (a, b) where cost(a) < cost(b), calculate the cost difference `delta_cost = cost(b) - cost(a)` and the log probability difference `delta_logp = logp(a) - logp(b)`.\n2. Normalize the cost differences across the batch using z-scoring: `z_delta_cost = zscore(delta_cost)`.\n3. Create an adaptive margin `margin_target` that is proportional to this normalized cost signal: `margin_target = alpha * z_delta_cost`.\n4. The loss is then the negative log-sigmoid of the difference between the model's preference and the adaptive margin: `loss = -logsigmoid(delta_logp - margin_target)`. This pushes `delta_logp` to be greater than the margin.\n5. The final loss is the weighted mean of these individual losses over the batch.", "hyperparams": {"alpha": 0.5}, "operators_used": ["logsigmoid", "zscore"], "implementation_hint": {"expects": ["cost_a", "cost_b", "log_prob_w", "log_prob_l", "weight"], "returns": "scalar"}, "code": "import torch\nimport torch.nn.functional as F\n\ndef zscore(x, eps=1e-8):\n    \"\"\"Computes z-score normalization for a tensor.\"\"\"\n    if x.numel() <= 1:\n        return x - x.mean()\n    return (x - x.mean()) / (x.std() + eps)\n\ndef generated_loss(batch, model_output, extra):\n    \"\"\"\n    Computes the Adaptive Sigmoid Margin Loss with Z-Score normalization.\n\n    The loss encourages the log probability of the better solution (winner) to be\n    higher than the log probability of the worse solution (loser).\n    The target margin between them is adaptive, scaling with the z-scored\n    difference in their costs within the batch.\n\n    Args:\n        batch (dict): A dictionary containing tensors from the dataloader.\n                      Expected keys:\n                      'cost_a', 'cost_b': Costs of the two solutions in a pair. Shape: [N].\n                      'log_prob_w', 'log_prob_l': Model's log probabilities for the\n                                                winner and loser solutions. Shape: [N].\n                      'weight' (optional): Per-sample loss weights. Shape: [N].\n        model_output: Not used in this loss implementation.\n        extra (dict): A dictionary for hyperparameters. Expected keys:\n                      'alpha' (float): A scaling factor for the adaptive margin.\n\n    Returns:\n        torch.Tensor: A scalar tensor representing the mean loss over the batch.\n    \"\"\"\n    # Define the winner (w) and loser (l) based on costs.\n    # cost_w < cost_l\n    cost_w = torch.min(batch['cost_a'], batch['cost_b'])\n    cost_l = torch.max(batch['cost_a'], batch['cost_b'])\n\n    # The 'log_prob_w' and 'log_prob_l' are assumed to be provided directly by the dataset/wrapper,\n    # corresponding to the winner and loser solutions respectively.\n    log_prob_w = batch['log_prob_w']\n    log_prob_l = batch['log_prob_l']\n\n    # Retrieve hyperparameters\n    alpha = extra.get('alpha', 0.5)\n\n    # 1. Calculate cost and log probability differences\n    delta_cost = cost_l - cost_w\n    delta_logp = log_prob_w - log_prob_l\n\n    # 2. Normalize the cost difference using z-scoring across the batch.\n    # This makes the margin adaptive to the batch's specific cost distribution.\n    z_delta_cost = zscore(delta_cost)\n\n    # 3. Create an adaptive margin based on the z-scored cost signal.\n    # The target separation is proportional to the relative cost difference in the batch.\n    adaptive_margin = alpha * z_delta_cost\n\n    # 4. Compute the core loss using negative log-sigmoid.\n    # This is equivalent to softplus(-(delta_logp - adaptive_margin)).\n    # The loss is low if delta_logp > adaptive_margin.\n    # The loss is high if delta_logp < adaptive_margin.\n    loss = -F.logsigmoid(delta_logp - adaptive_margin)\n\n    # 5. Apply optional weights and compute the mean.\n    weights = batch.get('weight', None)\n    if weights is not None:\n        loss = loss * weights\n        # Use a small epsilon to prevent division by zero for zero-weight batches.\n        return loss.sum() / weights.sum().clamp(min=1e-8)\n    else:\n        return loss.mean()"}, "static_ok": true, "static_reason": "", "dynamic_ok": false, "dynamic_reason": "forward_error: name 'zscore' is not defined", "loss_value": null, "grad_norm": null}
{"generation": 1, "index": 4, "ir": {"name": "Softplus Adaptive Margin Loss", "intuition": "This loss function compares the model's preference (difference in log probabilities) against the ground truth preference (difference in costs). It uses a softplus function to create a smooth, non-negative loss. The key idea is an adaptive margin: the 'target' log probability difference that the model should achieve is not fixed, but scales with the normalized cost difference. If two solutions have very different costs, the model is pushed harder to separate them. If their costs are similar, the model is penalized less for being uncertain. This adaptation is achieved by scaling the log-probability difference by a factor derived from the tanh of the cost difference, which keeps the scaling bounded and stable. The softplus function provides a smooth penalty when the model's preference falls short of this adaptive margin.", "pseudocode": "1. For each pair (a, b) where cost(a) < cost(b), calculate the cost difference `delta_cost = cost(b) - cost(a)` and the log probability difference `delta_logp = logp(a) - logp(b)`.\n2. Normalize the cost difference using a tanh function to get a bounded 'cost signal' between 0 and 1: `cost_signal = tanh(delta_cost / temp_cost)`.\n3. Create an adaptive margin `margin_target` that is proportional to this cost signal: `margin_target = alpha * cost_signal`.\n4. The loss is then the softplus of the negative difference between the model's preference and the adaptive margin: `loss = softplus(margin_target - delta_logp)`. This pushes `delta_logp` to be greater than the margin.\n5. The final loss is the weighted mean of these individual losses over the batch.", "hyperparams": {"alpha": 2.0, "temp_cost": 1.0}, "operators_used": ["softplus", "tanh"], "implementation_hint": {"expects": ["cost_a", "cost_b", "log_prob_w", "log_prob_l", "weight"], "returns": "scalar"}, "code": "import torch\nimport torch.nn.functional as F\n\ndef generated_loss(batch, model_output, extra):\n    \"\"\"\n    Computes the Softplus Adaptive Margin Loss.\n\n    The loss encourages the log probability of the better solution (winner) to be\n    higher than the log probability of the worse solution (loser).\n    The target margin between them is adaptive, scaling with the normalized\n    difference in their costs.\n\n    Args:\n        batch (dict): A dictionary containing tensors from the dataloader.\n                      Expected keys:\n                      'cost_a', 'cost_b': Costs of the two solutions in a pair. Shape: [N].\n                      'log_prob_w', 'log_prob_l': Model's log probabilities for the\n                                                winner and loser solutions. Shape: [N].\n                      'weight' (optional): Per-sample loss weights. Shape: [N].\n        model_output: Not used in this loss implementation.\n        extra (dict): A dictionary for hyperparameters. Expected keys:\n                      'alpha' (float): A scaling factor for the adaptive margin.\n                      'temp_cost' (float): A temperature for normalizing the cost difference.\n\n    Returns:\n        torch.Tensor: A scalar tensor representing the mean loss over the batch.\n    \"\"\"\n    # For preference learning, we assume the dataset provides pairs where one is strictly better.\n    # Let's define the winner (w) and loser (l) based on costs.\n    # cost_w < cost_l\n    cost_w = torch.min(batch['cost_a'], batch['cost_b'])\n    cost_l = torch.max(batch['cost_a'], batch['cost_b'])\n\n    # The 'log_prob_w' and 'log_prob_l' are assumed to be provided directly by the dataset/wrapper,\n    # corresponding to the winner and loser solutions respectively.\n    log_prob_w = batch['log_prob_w']\n    log_prob_l = batch['log_prob_l']\n\n    # Retrieve hyperparameters\n    alpha = extra.get('alpha', 2.0)\n    temp_cost = extra.get('temp_cost', 1.0)\n\n    # 1. Calculate cost and log probability differences\n    # delta_cost is guaranteed to be >= 0\n    delta_cost = cost_l - cost_w\n    delta_logp = log_prob_w - log_prob_l\n\n    # 2. Normalize the cost difference into a stable, bounded 'cost signal' [0, 1)\n    # Using tanh ensures numerical stability for extreme cost differences.\n    # temp_cost controls the sensitivity of the signal to the cost difference.\n    cost_signal = torch.tanh(delta_cost / temp_cost)\n\n    # 3. Create an adaptive margin based on the cost signal.\n    # The target separation in log-probs is proportional to how different the costs are.\n    # alpha scales the overall magnitude of the margin.\n    adaptive_margin = alpha * cost_signal\n\n    # 4. Compute the core loss using softplus.\n    # This is mathematically equivalent to -logsigmoid(x) but can sometimes be\n    # more numerically stable or offer different gradient characteristics.\n    # The loss is low if delta_logp > adaptive_margin (model agrees and exceeds margin).\n    # The loss is high if delta_logp < adaptive_margin (model disagrees or doesn't meet margin).\n    loss = F.softplus(adaptive_margin - delta_logp)\n\n    # 5. Apply optional weights and compute the mean.\n    weights = batch.get('weight', None)\n    if weights is not None:\n        loss = loss * weights\n        # Use a small epsilon to prevent division by zero if all weights are zero.\n        return loss.sum() / weights.sum().clamp(min=1e-8)\n    else:\n        return loss.mean()"}, "static_ok": true, "static_reason": "", "dynamic_ok": true, "dynamic_reason": "ok", "loss_value": 1.7204114198684692, "grad_norm": 0.0}
{"generation": 1, "index": 5, "ir": {"name": "Adaptive Sigmoid Margin Loss with Z-Score", "intuition": "This loss function compares the model's preference (difference in log probabilities) against the ground truth preference (difference in costs). It uses a sigmoid function for a smooth, bounded loss. The core idea is an adaptive margin: the 'target' log probability difference scales with the cost difference. Instead of using a simple tanh normalization on raw cost differences, this version first standardizes the cost differences across the batch (z-score normalization). This makes the margin less sensitive to the absolute scale of costs in a given batch and more dependent on the relative cost differences within it. A softplus function is then used on the z-scored differences to ensure the margin is always non-negative before scaling.", "pseudocode": "1. For each pair (a, b) where cost(a) < cost(b), calculate the cost difference `delta_cost = cost(b) - cost(a)` and the log probability difference `delta_logp = logp(a) - logp(b)`.\n2. Standardize the cost differences across the batch to get `z_delta_cost` (z-score normalization).\n3. Apply a softplus function to the standardized differences to ensure they are non-negative: `cost_signal = softplus(z_delta_cost)`.\n4. Create an adaptive margin `margin_target` that is proportional to this cost signal: `margin_target = alpha * cost_signal`.\n5. The loss is then the negative log-sigmoid of the difference between the model's preference and the adaptive margin: `loss = -logsigmoid(delta_logp - margin_target)`. This pushes `delta_logp` to be greater than the margin.\n6. The final loss is the weighted mean of these individual losses over the batch.", "hyperparams": {"alpha": 1.0, "eps": 1e-06}, "operators_used": ["logsigmoid", "softplus", "zscore"], "implementation_hint": {"expects": ["cost_a", "cost_b", "log_prob_w", "log_prob_l", "weight"], "returns": "scalar"}, "code": "import torch\nimport torch.nn.functional as F\n\ndef zscore(x, eps=1e-6):\n    \"\"\"Calculates the z-score of a tensor along the first dimension.\"\"\"\n    if x.numel() <= 1:\n        return torch.zeros_like(x)\n    mean = x.mean()\n    std = x.std()\n    return (x - mean) / (std + eps)\n\ndef generated_loss(batch, model_output, extra):\n    \"\"\"\n    Computes the Adaptive Sigmoid Margin Loss with Z-Score Normalization.\n\n    The loss encourages the log probability of the better solution (winner) to be\n    higher than the log probability of the worse solution (loser).\n    The target margin between them is adaptive, scaling with the z-score normalized\n    difference in their costs.\n\n    Args:\n        batch (dict): A dictionary containing tensors from the dataloader.\n                      Expected keys:\n                      'cost_a', 'cost_b': Costs of the two solutions in a pair. Shape: [N].\n                      'log_prob_w', 'log_prob_l': Model's log probabilities for the\n                                                winner and loser solutions. Shape: [N].\n                      'weight' (optional): Per-sample loss weights. Shape: [N].\n        model_output: Not used in this loss implementation.\n        extra (dict): A dictionary for hyperparameters. Expected keys:\n                      'alpha' (float): A scaling factor for the adaptive margin.\n                      'eps' (float): A small epsilon for numerical stability in z-score.\n\n    Returns:\n        torch.Tensor: A scalar tensor representing the mean loss over the batch.\n    \"\"\"\n    cost_w = torch.min(batch['cost_a'], batch['cost_b'])\n    cost_l = torch.max(batch['cost_a'], batch['cost_b'])\n\n    log_prob_w = batch['log_prob_w']\n    log_prob_l = batch['log_prob_l']\n\n    alpha = extra.get('alpha', 1.0)\n    eps = extra.get('eps', 1e-6)\n\n    delta_cost = cost_l - cost_w\n    delta_logp = log_prob_w - log_prob_l\n\n    # 2. Standardize the cost differences across the batch (z-score).\n    z_delta_cost = zscore(delta_cost, eps=eps)\n\n    # 3. Apply softplus to ensure the signal is non-negative and smooth.\n    cost_signal = F.softplus(z_delta_cost)\n\n    # 4. Create an adaptive margin based on the cost signal.\n    # The target separation in log-probs is proportional to how different the costs are\n    # relative to other pairs in the batch.\n    adaptive_margin = alpha * cost_signal\n\n    # 5. Compute the core loss using negative log-sigmoid.\n    # This pushes delta_logp to be greater than the adaptive_margin.\n    loss = -F.logsigmoid(delta_logp - adaptive_margin)\n\n    # 6. Apply optional weights and compute the mean.\n    weights = batch.get('weight', None)\n    if weights is not None:\n        loss = loss * weights\n        # Use clamp to avoid division by zero if all weights are zero\n        return loss.sum() / weights.sum().clamp(min=1e-8)\n    else:\n        return loss.mean()"}, "static_ok": true, "static_reason": "", "dynamic_ok": false, "dynamic_reason": "forward_error: name 'zscore' is not defined", "loss_value": null, "grad_norm": null}
{"generation": 1, "index": 6, "ir": {"name": "Sigmoid Margin Loss with Softplus Cost Scaling", "intuition": "This loss function compares the model's preference (difference in log probabilities) against a target margin. It uses a sigmoid function to create a smooth, bounded loss. The key idea is an adaptive margin: the 'target' log probability difference scales with the cost difference. Instead of using tanh for normalization, this variant uses a softplus function on the cost difference. Softplus provides a smooth, non-negative, and unbounded scaling factor, meaning that as the cost difference grows very large, the target margin also continues to grow, pushing the model to create an even larger separation in log probabilities for pairs that are very far apart in quality. This contrasts with tanh, which saturates.", "pseudocode": "1. For each pair (a, b) where cost(a) < cost(b), calculate the cost difference `delta_cost = cost(b) - cost(a)` and the log probability difference `delta_logp = logp(a) - logp(b)`.\n2. Transform the cost difference using a softplus function to get a smooth, non-negative 'cost signal': `cost_signal = softplus(delta_cost / temp_cost)`.\n3. Create an adaptive margin `margin_target` that is proportional to this cost signal: `margin_target = alpha * cost_signal`.\n4. The loss is then the negative log-sigmoid of the difference between the model's preference and the adaptive margin: `loss = -logsigmoid(delta_logp - margin_target)`. This pushes `delta_logp` to be greater than the margin.\n5. The final loss is the weighted mean of these individual losses over the batch.", "hyperparams": {"alpha": 1.0, "temp_cost": 1.0}, "operators_used": ["logsigmoid", "softplus"], "implementation_hint": {"expects": ["cost_a", "cost_b", "log_prob_w", "log_prob_l", "weight"], "returns": "scalar"}, "code": "import torch\nimport torch.nn.functional as F\n\ndef generated_loss(batch, model_output, extra):\n    \"\"\"\n    Computes the Sigmoid Margin Loss with Softplus Cost Scaling.\n\n    The loss encourages the log probability of the better solution (winner) to be\n    higher than the log probability of the worse solution (loser).\n    The target margin between them is adaptive, scaling with the softplus-transformed\n    difference in their costs.\n\n    Args:\n        batch (dict): A dictionary containing tensors from the dataloader.\n                      Expected keys:\n                      'cost_a', 'cost_b': Costs of the two solutions in a pair. Shape: [N].\n                      'log_prob_w', 'log_prob_l': Model's log probabilities for the\n                                                winner and loser solutions. Shape: [N].\n                      'weight' (optional): Per-sample loss weights. Shape: [N].\n        model_output: Not used in this loss implementation.\n        extra (dict): A dictionary for hyperparameters. Expected keys:\n                      'alpha' (float): A scaling factor for the adaptive margin.\n                      'temp_cost' (float): A temperature for scaling the cost difference.\n\n    Returns:\n        torch.Tensor: A scalar tensor representing the mean loss over the batch.\n    \"\"\"\n    # For preference learning, we assume the dataset provides pairs where one is strictly better.\n    # Let's define the winner (w) and loser (l) based on costs.\n    # cost_w < cost_l\n    cost_w = torch.min(batch['cost_a'], batch['cost_b'])\n    cost_l = torch.max(batch['cost_a'], batch['cost_b'])\n\n    # The 'log_prob_w' and 'log_prob_l' are assumed to be provided directly by the dataset/wrapper,\n    # corresponding to the winner and loser solutions respectively.\n    log_prob_w = batch['log_prob_w']\n    log_prob_l = batch['log_prob_l']\n\n    # Retrieve hyperparameters\n    alpha = extra.get('alpha', 1.0)\n    temp_cost = extra.get('temp_cost', 1.0)\n\n    # 1. Calculate cost and log probability differences\n    # delta_cost is guaranteed to be >= 0\n    delta_cost = cost_l - cost_w\n    delta_logp = log_prob_w - log_prob_l\n\n    # 2. Transform the cost difference using softplus to get a smooth, non-negative signal.\n    # Unlike tanh, softplus does not saturate, allowing the margin to grow with large cost differences.\n    # temp_cost controls the sensitivity of the signal to the cost difference.\n    cost_signal = F.softplus(delta_cost / temp_cost)\n\n    # 3. Create an adaptive margin based on the cost signal.\n    # The target separation in log-probs is proportional to how different the costs are.\n    # alpha scales the overall magnitude of the margin.\n    adaptive_margin = alpha * cost_signal\n\n    # 4. Compute the core loss using negative log-sigmoid.\n    # This is equivalent to softplus(-(delta_logp - adaptive_margin)).\n    # The loss is low if delta_logp > adaptive_margin (model agrees and exceeds margin).\n    # The loss is high if delta_logp < adaptive_margin (model disagrees or doesn't meet margin).\n    loss = -F.logsigmoid(delta_logp - adaptive_margin)\n\n    # 5. Apply optional weights and compute the mean.\n    weights = batch.get('weight', None)\n    if weights is not None:\n        loss = loss * weights\n        return loss.sum() / weights.sum().clamp(min=1e-8)\n    else:\n        return loss.mean()"}, "static_ok": true, "static_reason": "", "dynamic_ok": true, "dynamic_reason": "ok", "loss_value": 1.5514447689056396, "grad_norm": 0.0}
{"generation": 1, "index": 7, "ir": {"name": "Softplus Adaptive Margin Loss", "intuition": "This loss function compares the model's preference (difference in log probabilities) against a target margin that adapts to the ground truth cost difference. The core idea is to use `softplus` to create a smooth, non-negative loss. The target margin for the log probability difference scales with the `softplus` of the cost difference. This means that as the cost gap between two solutions widens, the model is pushed much harder to separate them, but the scaling is smooth and avoids the saturation issues of `tanh`. The `softplus` function on the cost difference ensures the margin is always non-negative and grows approximately linearly for large cost differences.", "pseudocode": "1. For each pair (a, b) where cost(a) < cost(b), calculate the cost difference `delta_cost = cost(b) - cost(a)` and the log probability difference `delta_logp = logp(a) - logp(b)`.\n2. Transform the cost difference using a softplus function to create a smooth, non-negative 'cost signal': `cost_signal = softplus(delta_cost / temp_cost)`.\n3. Create an adaptive margin `margin_target` that is proportional to this cost signal: `margin_target = alpha * cost_signal`.\n4. The loss is then the softplus of the margin minus the model's preference: `loss = softplus(margin_target - delta_logp)`. This is a smooth approximation of `max(0, margin_target - delta_logp)` and penalizes cases where `delta_logp` is less than the target margin.\n5. The final loss is the weighted mean of these individual losses over the batch.", "hyperparams": {"alpha": 1.0, "temp_cost": 1.0}, "operators_used": ["softplus"], "implementation_hint": {"expects": ["cost_a", "cost_b", "log_prob_w", "log_prob_l", "weight"], "returns": "scalar"}, "code": "import torch\nimport torch.nn.functional as F\n\ndef generated_loss(batch, model_output, extra):\n    \"\"\"\n    Computes the Softplus Adaptive Margin Loss.\n\n    The loss encourages the log probability of the better solution (winner) to be\n    higher than the log probability of the worse solution (loser) by an adaptive margin.\n    The target margin scales with the softplus-transformed difference in their costs.\n\n    Args:\n        batch (dict): A dictionary containing tensors from the dataloader.\n                      Expected keys:\n                      'cost_a', 'cost_b': Costs of the two solutions in a pair. Shape: [N].\n                      'log_prob_w', 'log_prob_l': Model's log probabilities for the\n                                                winner and loser solutions. Shape: [N].\n                      'weight' (optional): Per-sample loss weights. Shape: [N].\n        model_output: Not used in this loss implementation.\n        extra (dict): A dictionary for hyperparameters. Expected keys:\n                      'alpha' (float): A scaling factor for the adaptive margin.\n                      'temp_cost' (float): A temperature for normalizing the cost difference.\n\n    Returns:\n        torch.Tensor: A scalar tensor representing the mean loss over the batch.\n    \"\"\"\n    # For preference learning, we assume the dataset provides pairs where one is strictly better.\n    # cost_w < cost_l\n    cost_w = torch.min(batch['cost_a'], batch['cost_b'])\n    cost_l = torch.max(batch['cost_a'], batch['cost_b'])\n\n    # The 'log_prob_w' and 'log_prob_l' are assumed to be provided directly, corresponding\n    # to the winner and loser solutions respectively.\n    log_prob_w = batch['log_prob_w']\n    log_prob_l = batch['log_prob_l']\n\n    # Retrieve hyperparameters\n    alpha = extra.get('alpha', 1.0)\n    temp_cost = extra.get('temp_cost', 1.0)\n\n    # 1. Calculate cost and log probability differences\n    # delta_cost is guaranteed to be >= 0\n    delta_cost = cost_l - cost_w\n    delta_logp = log_prob_w - log_prob_l\n\n    # 2. Transform the cost difference into a smooth, non-negative 'cost signal'.\n    # Using softplus instead of tanh provides a non-saturating signal that grows\n    # approximately linearly for large cost differences.\n    cost_signal = F.softplus(delta_cost / temp_cost)\n\n    # 3. Create an adaptive margin based on the cost signal.\n    # The target separation in log-probs is proportional to how different the costs are.\n    # alpha scales the overall magnitude of the margin.\n    adaptive_margin = alpha * cost_signal\n\n    # 4. Compute the core loss using softplus.\n    # This is a smooth hinge loss: softplus(margin - prediction).\n    # The loss is low if delta_logp > adaptive_margin.\n    # The loss is high if delta_logp < adaptive_margin.\n    loss = F.softplus(adaptive_margin - delta_logp)\n\n    # 5. Apply optional weights and compute the mean.\n    weights = batch.get('weight', None)\n    if weights is not None:\n        loss = loss * weights\n        # Use a small epsilon to prevent division by zero if all weights are zero.\n        return loss.sum() / weights.sum().clamp(min=1e-8)\n    else:\n        return loss.mean()"}, "static_ok": true, "static_reason": "", "dynamic_ok": true, "dynamic_reason": "ok", "loss_value": 1.5514447689056396, "grad_norm": 0.0}
{"generation": 2, "index": 0, "ir": {"name": "Rank-Gap Scaled Log-Sigmoid Loss", "intuition": "This loss function combines the log-sigmoid preference modeling framework with an adaptive margin that is sensitive to both the absolute cost difference and the relative rank of that difference within the batch.\n\nInherited ideas:\n- From **Parent 0 & 1**: The core concept of an adaptive margin, where the target separation `delta_logp` scales with the cost difference `delta_cost`. The loss is incurred when the model's preference `delta_logp` falls short of this margin.\n- From **Parent 1**: The use of `tanh` to create a bounded signal from the cost difference, ensuring that extremely large cost gaps do not create unbounded target margins, which improves numerical stability.\n\nNew coupling ideas:\n1.  **Rank-Gap Scaling**: The primary innovation is scaling the margin by the `rank_gap` of the cost differences within the batch. The `delta_cost` for each pair is ranked, and this rank (normalized to [0, 1]) is used as a scaling factor. This makes the loss pay more attention to correctly ordering pairs with a larger cost difference *relative to other pairs in the same batch*. A pair with a `delta_cost` of 5.0 might be the most significant in one batch (rank_gap -> 1.0) but average in another (rank_gap -> 0.5), and the loss adapts accordingly.\n2.  **Log-Sigmoid Formulation**: Instead of the `softplus(margin - x)` hinge-loss formulation from the parents, this child uses the more common preference loss form `-logsigmoid(x - margin)`. This directly maximizes the log-probability of the model's preference `delta_logp` exceeding the adaptive margin.", "pseudocode": "1. For each pair (winner, loser) in the batch, calculate the cost difference `delta_cost = cost_loser - cost_winner` and the log probability difference `delta_logp = logp_winner - logp_loser`.\n2. Create a bounded 'cost signal' using `tanh`: `cost_signal = tanh(delta_cost / temp_cost)`.\n3. Calculate the `rank_gap` of the `delta_cost` values across the batch. This normalizes each cost difference based on its rank, yielding a value in [0, 1].\n4. Combine the absolute cost signal and the relative rank signal to form the adaptive margin. The `rank_gap` acts as a dynamic scaler: `margin = alpha * cost_signal * rank_gap`.\n5. Compute the loss using a log-sigmoid formulation, which penalizes cases where `delta_logp` is less than the adaptive margin: `loss = -logsigmoid(delta_logp - margin)`.\n6. The final loss is the weighted mean of these individual losses over the batch.", "hyperparams": {"alpha": 1.5, "temp_cost": 1.0}, "operators_used": ["logsigmoid", "tanh", "rank_gap"], "implementation_hint": {"expects": ["cost_a", "cost_b", "log_prob_w", "log_prob_l", "weight"], "returns": "scalar"}, "code": "import torch\nimport torch.nn.functional as F\n\ndef rank_gap(x, eps=1e-9):\n    \"\"\"Computes the rank of each element in x and normalizes it to [0, 1].\"\"\"\n    # Add small random noise to break ties\n    x_perturbed = x + torch.rand_like(x, dtype=torch.float) * eps\n    # Get ranks (1-based)\n    ranks = x_perturbed.argsort().argsort().float() + 1.0\n    # Normalize to [0, 1] range\n    # If batch size is 1, rank is 1, so ranks-1 is 0. Avoid division by zero.\n    n = x.size(0)\n    if n > 1:\n        return (ranks - 1.0) / (n - 1.0)\n    else:\n        # For a single element, its relative importance is maximal\n        return torch.ones_like(x)\n\ndef generated_loss(batch, model_output, extra):\n    \"\"\"\n    Computes the Rank-Gap Scaled Log-Sigmoid Loss.\n\n    This loss encourages the model's log-probability difference to exceed an\n    adaptive margin. The margin is a product of two components: (1) a tanh-transformed\n    absolute cost difference, and (2) the rank-based relative importance of that\n    cost difference within the batch.\n\n    Args:\n        batch (dict): A dictionary containing tensors from the dataloader.\n                      Expected keys:\n                      'cost_a', 'cost_b': Costs of the two solutions. Shape: [N].\n                      'log_prob_w', 'log_prob_l': Model's log probabilities for the\n                                                winner and loser. Shape: [N].\n                      'weight' (optional): Per-sample loss weights. Shape: [N].\n        model_output: Not used.\n        extra (dict): A dictionary for hyperparameters. Expected keys:\n                      'alpha' (float): Base scaling factor for the margin.\n                      'temp_cost' (float): Temperature for the cost difference.\n\n    Returns:\n        torch.Tensor: A scalar tensor representing the mean loss.\n    \"\"\"\n    # Identify winner (w) and loser (l) based on costs\n    cost_w = torch.min(batch['cost_a'], batch['cost_b'])\n    cost_l = torch.max(batch['cost_a'], batch['cost_b'])\n    log_prob_w = batch['log_prob_w']\n    log_prob_l = batch['log_prob_l']\n\n    alpha = extra.get('alpha', 1.5)\n    temp_cost = extra.get('temp_cost', 1.0)\n\n    # 1. Calculate cost and log probability differences\n    delta_cost = cost_l - cost_w\n    delta_logp = log_prob_w - log_prob_l\n\n    # 2. Inherited idea: Create a bounded cost signal using tanh\n    cost_signal = torch.tanh(delta_cost / temp_cost)\n\n    # 3. New coupling idea: Calculate rank-based importance of the cost difference\n    # This makes the margin sensitive to the batch distribution of cost gaps\n    batch_rank_gap = rank_gap(delta_cost)\n\n    # 4. New coupling idea: Combine absolute and relative signals for the margin\n    # The margin is scaled by both how large the cost gap is (cost_signal)\n    # and how significant that gap is within the current batch (batch_rank_gap).\n    adaptive_margin = alpha * cost_signal * batch_rank_gap\n\n    # 5. Inherited idea (re-formulated): Use a log-sigmoid loss formulation\n    # This is equivalent to softplus(-(delta_logp - adaptive_margin))\n    # It penalizes when delta_logp is smaller than the adaptive_margin.\n    loss = -F.logsigmoid(delta_logp - adaptive_margin)\n\n    # 6. Apply optional weights and compute the mean\n    weights = batch.get('weight', None)\n    if weights is not None:\n        loss = loss * weights\n        return loss.sum() / weights.sum().clamp(min=1e-8)\n    else:\n        return loss.mean()"}, "static_ok": true, "static_reason": "", "dynamic_ok": false, "dynamic_reason": "forward_error: name 'rank_gap' is not defined", "loss_value": null, "grad_norm": null}
{"generation": 2, "index": 1, "ir": {"name": "Rank-Gap Stabilized Log-Sigmoid Loss", "intuition": "This loss function combines the core logistic loss structure with an adaptive margin that is sensitive to the relative ranking of cost differences within a batch. It inherits the fundamental log-sigmoid loss structure (`-logsigmoid(delta_logp - margin)`) which is a common and effective preference loss formulation. From the parents, it inherits the concept of an adaptive margin that scales with the cost difference (`delta_cost`). The first parent uses an unbounded `softplus` for this scaling, while the second uses a bounded `tanh`. This child loss takes inspiration from both by using `tanh` for a bounded base margin, ensuring stability against extreme cost differences.\n\nThe first new coupling idea is to further modulate this margin using the `rank_gap` of the cost differences. The `rank_gap` operator computes the normalized rank of each `delta_cost` within the batch. This makes the margin sensitive not just to the absolute cost difference, but also to its relative importance in the current batch. A pair with a large `delta_cost` that is also the largest in the batch will receive the strongest margin signal. A pair with a large absolute `delta_cost` but which is only average for the current batch will receive a more moderate signal. This helps the model prioritize learning from the most discriminative examples in each batch.\n\nThe second new coupling idea is a stability trick. The final loss term, `softplus(-zscored_delta_logp)`, is added as a regularizer. It z-scores the log-probability differences within the batch and applies a softplus penalty. This encourages the distribution of `delta_logp` values to remain well-behaved (not too large or small, centered around zero before the margin is applied), which can prevent gradient explosion or vanishing and improve overall training stability.", "pseudocode": "1. For each pair (winner, loser), calculate the cost difference `delta_cost = cost_loser - cost_winner` and the log probability difference `delta_logp = logp_winner - logp_loser`.\n2. Inherited Idea (Parent 2): Create a base adaptive margin using a bounded function of the cost difference. `base_margin = alpha * tanh(delta_cost / temp_cost)`. This ensures the margin is stable and doesn't grow infinitely.\n3. New Coupling 1: Modulate the margin based on the relative importance of the cost difference within the batch. Calculate the normalized rank of each `delta_cost` using `rank_gap(delta_cost)`. The final adaptive margin is the sum of the base margin and this rank-based signal: `adaptive_margin = base_margin + rank_weight * rank_gap(delta_cost)`.\n4. Inherited Idea (Core Preference Loss): Compute the primary preference loss using a log-sigmoid formulation, which is equivalent to `softplus(-(delta_logp - adaptive_margin))`. This penalizes the model when `delta_logp` is less than the `adaptive_margin`. `preference_loss = softplus(adaptive_margin - delta_logp)`.\n5. New Coupling 2: Introduce a stability term. Z-score the `delta_logp` values across the batch to normalize them. Apply a softplus penalty to the negative z-scored values: `stability_loss = stability_weight * softplus(-zscore(delta_logp))`. This encourages the distribution of `delta_logp` to stay centered and have a reasonable variance, preventing extreme values.\n6. The final loss is the sum of the preference loss and the stability loss, averaged over the batch.", "hyperparams": {"alpha": 1.0, "temp_cost": 1.0, "rank_weight": 0.5, "stability_weight": 0.1}, "operators_used": ["softplus", "tanh", "zscore", "rank_gap"], "implementation_hint": {"expects": ["cost_a", "cost_b", "logp_w", "logp_l"], "returns": "scalar"}, "code": "import torch\nimport torch.nn.functional as F\n\ndef _rank_gap(x):\n    \"\"\"Computes the rank gap of a 1D tensor.\"\"\"\n    if x.numel() <= 1:\n        return torch.zeros_like(x)\n    ranks = torch.empty_like(x, dtype=torch.float)\n    ranks[x.argsort()] = torch.arange(x.numel(), device=x.device, dtype=torch.float)\n    ranks = (ranks / (x.numel() - 1)) * 2 - 1\n    return ranks\n\ndef _zscore(x):\n    \"\"\"Computes the z-score of a 1D tensor.\"\"\"\n    if x.numel() <= 1:\n        return torch.zeros_like(x)\n    mean = x.mean()\n    std = x.std()\n    # Use a small epsilon for numerical stability if std is zero\n    return (x - mean) / (std + 1e-8)\n\ndef generated_loss(batch, model_output, extra):\n    \"\"\"\n    Computes the Rank-Gap Stabilized Log-Sigmoid Loss.\n\n    This loss combines an adaptive margin based on both the absolute cost difference\n    (via tanh) and its relative rank within the batch. It also includes a\n    stabilization term on the z-scored log-probability differences.\n    \"\"\"\n    cost_w = torch.min(batch['cost_a'], batch['cost_b'])\n    cost_l = torch.max(batch['cost_a'], batch['cost_b'])\n    log_prob_w = batch['log_prob_w']\n    log_prob_l = batch['log_prob_l']\n\n    alpha = extra.get('alpha', 1.0)\n    temp_cost = extra.get('temp_cost', 1.0)\n    rank_weight = extra.get('rank_weight', 0.5)\n    stability_weight = extra.get('stability_weight', 0.1)\n\n    # 1. Calculate base differences\n    delta_cost = cost_l - cost_w\n    delta_logp = log_prob_w - log_prob_l\n\n    # 2. Inherit bounded margin from Parent 2\n    base_margin = alpha * torch.tanh(delta_cost / temp_cost)\n\n    # 3. New Coupling 1: Add rank-based margin modulation\n    # The rank_gap operator provides a signal from -1 to 1 based on relative rank\n    cost_rank_signal = _rank_gap(delta_cost)\n    adaptive_margin = base_margin + rank_weight * cost_rank_signal\n    \n    # 4. Inherit core preference loss structure (log-sigmoid / softplus)\n    preference_loss = F.softplus(adaptive_margin - delta_logp)\n\n    # 5. New Coupling 2: Add stability term on z-scored log-prob differences\n    zscored_delta_logp = _zscore(delta_logp)\n    stability_loss = stability_weight * F.softplus(-zscored_delta_logp)\n\n    # 6. Combine losses\n    loss = preference_loss + stability_loss\n\n    weights = batch.get('weight', None)\n    if weights is not None:\n        loss = loss * weights\n        return loss.sum() / weights.sum().clamp(min=1e-8)\n    else:\n        return loss.mean()"}, "static_ok": true, "static_reason": "", "dynamic_ok": false, "dynamic_reason": "forward_error: name '_rank_gap' is not defined", "loss_value": null, "grad_norm": null}
{"generation": 2, "index": 2, "ir": {"name": "Rank-Gap Scaled Bradley-Terry Loss", "intuition": "This loss function combines the adaptive margin concept from its parents with a classic probabilistic framework (Bradley-Terry) and introduces a new stability mechanism based on cost ranking.\n\nInherited Ideas:\n- From both parents, it inherits the core structure of an adaptive margin loss, where the target separation between log probabilities (`delta_logp`) is not fixed but scales with the difference in costs (`delta_cost`).\n- From Parent 1 (the one using `tanh`), it inherits the idea of using a saturating function (`tanh`) to transform `delta_cost` into a bounded signal. This prevents extremely large cost differences from creating pathologically large loss values.\n\nNew Coupling Ideas:\n1.  **Bradley-Terry Formulation:** Instead of using a hinge-like loss (`softplus(margin - delta_logp)`), this child loss uses the Bradley-Terry model's negative log-likelihood form: `-logsigmoid(delta_logp - margin)`. This frames the problem as maximizing the probability that the winner is preferred over the loser by at least a certain margin, offering a more probabilistic interpretation.\n2.  **Rank-Gap Normalization:** A new coupling mechanism is introduced to stabilize training across batches with different cost distributions. The cost difference `delta_cost` is normalized by the `rank_gap` of costs within the batch before being passed to the `tanh` function. `rank_gap` is the difference between the 95th and 5th percentile of costs. This makes the margin's sensitivity (`temp_cost`) robust to the scale and variance of costs in a given batch, preventing the `tanh` function from saturating too quickly or not activating enough.", "pseudocode": "1. For each pair (winner, loser) in the batch, calculate the cost difference `delta_cost = cost_loser - cost_winner` and the log probability difference `delta_logp = logp_winner - logp_loser`.\n2. Compute the rank-gap of all costs (both winner and loser) in the batch. This is the difference between the 95th and 5th percentile of costs. Add a small epsilon for stability.\n3. Normalize the cost difference by this rank-gap: `normalized_delta_cost = delta_cost / rank_gap`.\n4. Create a bounded cost signal using `tanh`: `cost_signal = tanh(normalized_delta_cost / temp_cost)`.\n5. Define the adaptive margin as a scaled version of this signal: `margin = alpha * cost_signal`.\n6. The loss for each pair is calculated using a Bradley-Terry-style formula with the margin: `loss = -logsigmoid(delta_logp - margin)`.\n7. The final loss is the weighted mean of these individual losses over the batch.", "hyperparams": {"alpha": 1.5, "temp_cost": 0.5}, "operators_used": ["logsigmoid", "tanh", "rank_gap"], "implementation_hint": {"expects": ["cost_a", "cost_b", "logp_a", "logp_b"], "returns": "scalar"}, "code": "import torch\nimport torch.nn.functional as F\n\ndef rank_gap(x, q_low=0.05, q_high=0.95, eps=1e-6):\n    \"\"\"Calculates the difference between high and low quantiles of a tensor.\"\"\"\n    if x.numel() < 2:\n        return torch.tensor(1.0, device=x.device)\n    q_low_val = torch.quantile(x, q_low)\n    q_high_val = torch.quantile(x, q_high)\n    return (q_high_val - q_low_val).clamp(min=eps)\n\ndef generated_loss(batch, model_output, extra):\n    \"\"\"\n    Computes the Rank-Gap Scaled Bradley-Terry Loss.\n\n    This loss encourages the log probability of the winner to be higher than the loser's\n    by an adaptive margin. The margin is derived from the tanh of the cost difference,\n    which is dynamically normalized by the batch-wise cost rank-gap for stability.\n    The core loss uses a logsigmoid form, reflecting the Bradley-Terry model.\n\n    Args:\n        batch (dict): A dictionary containing tensors from the dataloader.\n                      Expected keys:\n                      'cost_a', 'cost_b': Costs of the two solutions. Shape: [N].\n                      'log_prob_w', 'log_prob_l': Log probabilities for winner/loser. Shape: [N].\n                      'weight' (optional): Per-sample loss weights. Shape: [N].\n        model_output: Not used in this loss implementation.\n        extra (dict): A dictionary for hyperparameters. Expected keys:\n                      'alpha' (float): Scaling factor for the adaptive margin.\n                      'temp_cost' (float): Temperature for the tanh function after normalization.\n\n    Returns:\n        torch.Tensor: A scalar tensor representing the mean loss over the batch.\n    \"\"\"\n    # Identify winner (w) and loser (l) costs\n    cost_w = torch.min(batch['cost_a'], batch['cost_b'])\n    cost_l = torch.max(batch['cost_a'], batch['cost_b'])\n\n    log_prob_w = batch['log_prob_w']\n    log_prob_l = batch['log_prob_l']\n\n    # Retrieve hyperparameters\n    alpha = extra.get('alpha', 1.5)\n    temp_cost = extra.get('temp_cost', 0.5)\n\n    # 1. Calculate cost and log probability differences\n    delta_cost = cost_l - cost_w\n    delta_logp = log_prob_w - log_prob_l\n\n    # 2. Compute the rank-gap for normalization (new coupling idea)\n    all_costs = torch.cat([cost_w, cost_l])\n    cost_range = rank_gap(all_costs)\n\n    # 3. Normalize delta_cost by the computed range\n    normalized_delta_cost = delta_cost / cost_range\n\n    # 4. Create a bounded cost signal using tanh (inherited from Parent 1)\n    cost_signal = torch.tanh(normalized_delta_cost / temp_cost)\n\n    # 5. Define the adaptive margin (inherited from both parents)\n    adaptive_margin = alpha * cost_signal\n\n    # 6. Compute the loss using a logsigmoid formulation (new coupling idea)\n    # This is -log(P(w > l)), where the preference score difference is (delta_logp - margin)\n    loss = -F.logsigmoid(delta_logp - adaptive_margin)\n\n    # 7. Apply optional weights and compute the mean\n    weights = batch.get('weight', None)\n    if weights is not None:\n        loss = loss * weights\n        return loss.sum() / weights.sum().clamp(min=1e-8)\n    else:\n        return loss.mean()"}, "static_ok": true, "static_reason": "", "dynamic_ok": false, "dynamic_reason": "forward_error: name 'rank_gap' is not defined", "loss_value": null, "grad_norm": null}
{"generation": 2, "index": 3, "ir": {"name": "Rank-Gap Scaled Bradley-Terry Loss", "intuition": "This loss function combines the adaptive margin concept from its parents with the probabilistic foundation of the Bradley-Terry model. \n\nInherited ideas:\n- From both parents, it inherits the core structure of an adaptive margin loss, where the target separation between log probabilities (`delta_logp`) is not fixed but depends on the cost difference (`delta_cost`).\n- From Parent 1, it inherits the use of `tanh` to create a bounded and stable 'cost signal' from the cost difference, ensuring that extreme cost gaps do not cause numerical instability.\n\nNew coupling ideas:\n1.  **Probabilistic Framing**: Instead of a hinge-like `softplus` loss, this child uses a `logsigmoid` formulation, which is equivalent to the negative log-likelihood of the Bradley-Terry preference model. The loss is `logsigmoid(adaptive_margin - delta_logp)`, which encourages `delta_logp > adaptive_margin` in a probabilistic manner.\n2.  **Rank-Gap Normalization**: The cost difference `delta_cost` is normalized using a novel `rank_gap` operator before being fed into the `tanh` function. This operator computes `(x - x_min) / (x_max - x_min + epsilon)`, effectively scaling the cost differences within the current batch to the [0, 1] range. This makes the loss less sensitive to the absolute scale of costs and more focused on their relative ranking within the batch, improving stability and adaptability across different problem domains.", "pseudocode": "1. For each pair (a, b) where cost(a) < cost(b), calculate the cost difference `delta_cost = cost(b) - cost(a)` and the log probability difference `delta_logp = logp(a) - logp(b)`.\n2. Normalize the batch of `delta_cost` values using a rank-gap operator. This scales all cost differences in the batch to the approximate range [0, 1] based on the minimum and maximum `delta_cost` in the batch.\n3. Convert the normalized rank-gap into a bounded signal using `tanh`. This ensures the signal is stable and smooth.\n4. Create an adaptive margin by scaling this signal with a hyperparameter `alpha`: `adaptive_margin = alpha * tanh(normalized_rank_gap)`.\n5. Compute the loss using a logsigmoid function, which corresponds to the negative log-likelihood of a Bradley-Terry model: `loss = -logsigmoid(delta_logp - adaptive_margin)`. This penalizes cases where the model's preference `delta_logp` is less than the target margin.\n6. The final loss is the weighted mean of these individual losses over the batch.", "hyperparams": {"alpha": 2.0, "epsilon": 1e-08}, "operators_used": ["logsigmoid", "tanh", "rank_gap"], "implementation_hint": {"expects": ["cost_a", "cost_b", "log_prob_w", "log_prob_l", "weight"], "returns": "scalar"}, "code": "import torch\nimport torch.nn.functional as F\n\ndef rank_gap(x, epsilon=1e-8):\n    \"\"\"Normalizes x to the range [0, 1] based on batch min and max.\"\"\"\n    min_val = torch.min(x)\n    max_val = torch.max(x)\n    # Add epsilon to the denominator for stability if all values are the same.\n    return (x - min_val) / (max_val - min_val + epsilon)\n\ndef generated_loss(batch, model_output, extra):\n    \"\"\"\n    Computes the Rank-Gap Scaled Bradley-Terry Loss.\n\n    This loss encourages the log probability of the winner to be higher than the loser's\n    by an adaptive margin. The margin is derived from the batch-normalized rank-gap of\n    cost differences, making it robust to the scale of costs. The core loss uses a\n    logsigmoid formulation, grounding it in a probabilistic preference model.\n\n    Args:\n        batch (dict): A dictionary containing tensors from the dataloader.\n                      Expected keys:\n                      'cost_a', 'cost_b': Costs of the two solutions. Shape: [N].\n                      'log_prob_w', 'log_prob_l': Model's log probabilities for the\n                                                winner and loser solutions. Shape: [N].\n                      'weight' (optional): Per-sample loss weights. Shape: [N].\n        model_output: Not used in this loss implementation.\n        extra (dict): A dictionary for hyperparameters. Expected keys:\n                      'alpha' (float): A scaling factor for the adaptive margin.\n                      'epsilon' (float): A small value for numerical stability in rank_gap.\n\n    Returns:\n        torch.Tensor: A scalar tensor representing the mean loss over the batch.\n    \"\"\"\n    # Assume cost_w < cost_l\n    cost_w = torch.min(batch['cost_a'], batch['cost_b'])\n    cost_l = torch.max(batch['cost_a'], batch['cost_b'])\n\n    log_prob_w = batch['log_prob_w']\n    log_prob_l = batch['log_prob_l']\n\n    alpha = extra.get('alpha', 2.0)\n    epsilon = extra.get('epsilon', 1e-8)\n\n    # 1. Calculate cost and log probability differences\n    delta_cost = cost_l - cost_w\n    delta_logp = log_prob_w - log_prob_l\n\n    # 2. Normalize cost difference using the batch-aware rank_gap operator.\n    # This makes the loss less sensitive to the absolute scale of costs.\n    normalized_cost_gap = rank_gap(delta_cost, epsilon=epsilon)\n\n    # 3. Create a bounded signal from the normalized gap.\n    # Inherited from Parent 1.\n    cost_signal = torch.tanh(normalized_cost_gap)\n\n    # 4. Create an adaptive margin based on the cost signal.\n    # Inherited from both parents.\n    adaptive_margin = alpha * cost_signal\n\n    # 5. Compute the core loss using logsigmoid for a probabilistic interpretation.\n    # This is equivalent to softplus(margin - delta_logp), but framed as NLL.\n    # The goal is to make (delta_logp - adaptive_margin) large and positive.\n    loss = -F.logsigmoid(delta_logp - adaptive_margin)\n\n    # 6. Apply optional weights and compute the mean.\n    weights = batch.get('weight', None)\n    if weights is not None:\n        loss = loss * weights\n        return loss.sum() / weights.sum().clamp(min=1e-8)\n    else:\n        return loss.mean()"}, "static_ok": true, "static_reason": "", "dynamic_ok": false, "dynamic_reason": "forward_error: name 'rank_gap' is not defined", "loss_value": null, "grad_norm": null}
{"generation": 2, "index": 4, "ir": {"name": "Rank-Normalized Softplus Margin Loss", "intuition": "This loss function combines an adaptive margin with a rank-based normalization of the cost signal. The core loss structure is inherited from both parents, using `softplus(margin - delta_logp)` to penalize the model when its log probability difference (`delta_logp`) falls short of a target margin.\n\nInherited ideas:\n- From both parents, it inherits the fundamental structure of an adaptive margin loss: `loss = softplus(margin - delta_logp)`, where the margin is a function of the cost difference `delta_cost`.\n- From Parent 1 (the `tanh` version), it inherits the idea of creating a bounded signal from the cost difference to prevent the margin from growing uncontrollably. However, it replaces the `tanh` function.\n\nNew coupling ideas:\n1.  **Rank-Gap Normalization**: Instead of using `tanh` or `softplus` on the raw `delta_cost`, we first compute the `rank_gap` of the costs within the batch. This transforms the cost difference into a percentile-like score (from 0 to 1), making the margin target invariant to the absolute scale of costs and robust to outliers. This `rank_gap` provides a bounded, non-parametric signal of preference strength.\n2.  **Adaptive Temperature Scaling**: The `delta_logp` is scaled by an adaptive temperature `exp(beta * (1 - rank_gap))`. When the rank gap is small (costs are similar), the temperature is high, effectively softening the log-probability difference and reducing the penalty for small errors. Conversely, when the rank gap is large (costs are very different), the temperature approaches 1 (`exp(0)`), demanding a more precise log-probability separation. This focuses the model's learning on clear-cut cases while being lenient on ambiguous ones.", "pseudocode": "1. For each pair (winner, loser) in the batch, calculate the cost difference `delta_cost = cost_loser - cost_winner` and the log probability difference `delta_logp = logp_winner - logp_loser`.\n2. Calculate the rank-gap signal for all cost differences in the batch. This non-parametrically normalizes `delta_cost` into a `cost_signal` between 0 and 1, representing its relative magnitude within the batch.\n3. Compute the target margin: `margin = alpha * cost_signal`.\n4. Introduce a new adaptive temperature based on the cost signal: `temp = exp(beta * (1.0 - cost_signal))`. This temperature is high for small cost signals and approaches 1 for large ones.\n5. Scale the model's log probability difference by this temperature: `scaled_delta_logp = delta_logp / temp`.\n6. Calculate the per-sample loss using the softplus function: `loss = softplus(margin - scaled_delta_logp)`. This penalizes the model when its temperature-scaled preference is less than the rank-based margin.\n7. Compute the final loss as the weighted mean over the batch.", "hyperparams": {"alpha": 1.5, "beta": 1.0}, "operators_used": ["softplus", "exp", "rank_gap"], "implementation_hint": {"expects": ["cost_a", "cost_b", "logp_a", "logp_b", "weight"], "returns": "scalar"}, "code": "import torch\nimport torch.nn.functional as F\n\ndef rank_gap(x):\n    \"\"\"Non-parametrically normalizes a tensor to [0, 1] based on rank.\n\n    Args:\n        x (torch.Tensor): A 1D tensor of values.\n\n    Returns:\n        torch.Tensor: The rank-normalized tensor.\n    \"\"\"\n    if x.numel() <= 1:\n        return torch.zeros_like(x)\n    \n    # Get the rank of each element\n    # Note: Sorting and then finding original indices is a common way to implement rank.\n    # torch.argsort(torch.argsort(x)) gives the rank.\n    ranks = torch.argsort(torch.argsort(x)).float()\n    \n    # Normalize ranks to be in [0, 1]\n    # Subtract 1 from numel to make the max rank map to 1.\n    # Add a small epsilon to avoid division by zero for a single-element tensor.\n    n = x.numel() - 1\n    return ranks / max(n, 1e-8)\n\ndef generated_loss(batch, model_output, extra):\n    \"\"\"\n    Computes the Rank-Normalized Softplus Margin Loss.\n\n    This loss combines an adaptive margin based on the rank-normalized cost difference\n    with an adaptive temperature scaling on the log-probability difference.\n\n    Args:\n        batch (dict): A dictionary containing tensors from the dataloader.\n                      Expected keys:\n                      'cost_a', 'cost_b': Costs of the two solutions. Shape: [N].\n                      'log_prob_w', 'log_prob_l': Log probabilities for winner/loser. Shape: [N].\n                      'weight' (optional): Per-sample loss weights. Shape: [N].\n        model_output: Not used.\n        extra (dict): Dictionary for hyperparameters. Expected keys:\n                      'alpha' (float): Scaling factor for the margin.\n                      'beta' (float): Controls the sensitivity of the adaptive temperature.\n\n    Returns:\n        torch.Tensor: A scalar tensor representing the mean loss over the batch.\n    \"\"\"\n    cost_w = torch.min(batch['cost_a'], batch['cost_b'])\n    cost_l = torch.max(batch['cost_a'], batch['cost_b'])\n    log_prob_w = batch['log_prob_w']\n    log_prob_l = batch['log_prob_l']\n\n    alpha = extra.get('alpha', 1.5)\n    beta = extra.get('beta', 1.0)\n\n    # 1. Calculate cost and log probability differences\n    delta_cost = cost_l - cost_w\n    delta_logp = log_prob_w - log_prob_l\n    \n    # 2. (New Idea 1) Create a bounded, rank-based signal from the cost difference.\n    # This is robust to outliers and cost scaling.\n    with torch.no_grad(): # Ranking should not be part of the autograd graph\n      cost_signal = rank_gap(delta_cost)\n\n    # 3. Compute the target margin based on the rank signal.\n    # Inherited from both parents: `margin = alpha * cost_signal`\n    margin = alpha * cost_signal\n\n    # 4. (New Idea 2) Create an adaptive temperature.\n    # When cost_signal is small (ambiguous pair), temp is high (e^beta), softening the loss.\n    # When cost_signal is large (clear pair), temp is 1 (e^0), enforcing the margin strictly.\n    temperature = torch.exp(beta * (1.0 - cost_signal))\n\n    # 5. Scale the model's preference by the temperature.\n    scaled_delta_logp = delta_logp / temperature\n    \n    # 6. Compute the core loss.\n    # Inherited from both parents: `softplus(margin - prediction)`\n    loss = F.softplus(margin - scaled_delta_logp)\n\n    # 7. Apply optional weights and compute the mean.\n    weights = batch.get('weight', None)\n    if weights is not None:\n        loss = loss * weights\n        return loss.sum() / weights.sum().clamp(min=1e-8)\n    else:\n        return loss.mean()"}, "static_ok": true, "static_reason": "", "dynamic_ok": false, "dynamic_reason": "forward_error: name 'rank_gap' is not defined", "loss_value": null, "grad_norm": null}
{"generation": 2, "index": 5, "ir": {"name": "Rank-Gap Scaled Softplus Loss", "intuition": "This loss function combines an adaptive margin with a novel scaling mechanism to focus the learning signal. It inherits the core structure of using a softplus-based smooth hinge loss from both parents, where the loss is calculated as `softplus(margin - delta_logp)`. \n\nFrom Parent 1, it inherits the use of a saturating `tanh` function to create a bounded 'cost signal' from the cost difference. This ensures that extreme cost differences do not create an unbounded margin, promoting stability.\n\nFrom Parent 0 (and 1), it inherits the general concept of an adaptive margin, where the target separation between log probabilities (`margin`) scales with the difference in ground-truth costs.\n\nAs a new coupling idea, this child loss introduces a dynamic scaling factor based on the relative ranking of cost differences within the batch. It calculates a 'rank gap' for each pair, which is a value between 0 and 1 indicating how its cost difference compares to others in the same batch. This rank gap is then used to modulate the `alpha` hyperparameter, effectively creating a per-sample `alpha`. Pairs with a large cost difference relative to the batch (high rank gap) receive a stronger learning signal (higher effective alpha), while pairs with a small cost difference (low rank gap) receive a gentler push. This focuses the model's capacity on learning the most obvious preferences first and prevents it from being overly penalized for uncertainty on near-indistinguishable pairs. The scaling is smoothed with a `softplus` function to prevent abrupt changes in the learning signal.", "pseudocode": "1. For each pair (a, b) where cost(a) < cost(b), calculate the cost difference `delta_cost = cost(b) - cost(a)` and the log probability difference `delta_logp = logp(a) - logp(b)`.\n2. Inherited from Parent 1: Normalize the cost difference using a `tanh` function to get a bounded 'cost signal' between 0 and 1: `cost_signal = tanh(delta_cost / temp_cost)`.\n3. Inherited from both Parents: Create a base adaptive margin `base_margin` that is proportional to this cost signal: `base_margin = alpha * cost_signal`.\n4. New Coupling 1: Calculate the 'rank gap' of `delta_cost` within the batch. This maps each `delta_cost` to a value in [0, 1] based on its percentile rank.\n5. New Coupling 2: Use the rank gap to create a per-sample adaptive scaling factor. The factor is `softplus(rank_gap_scaler * (rank_gap - 0.5))`. This smoothly increases the scaling for pairs with above-median cost differences and decreases it for those below.\n6. The final margin is the base margin multiplied by this adaptive scaling factor: `final_margin = base_margin * adaptive_scaler`.\n7. Inherited from both Parents: The loss is the `softplus` of the final margin minus the model's preference: `loss = softplus(final_margin - delta_logp)`.\n8. The final loss is the weighted mean of these individual losses over the batch.", "hyperparams": {"alpha": 1.5, "temp_cost": 1.0, "rank_gap_scaler": 2.0}, "operators_used": ["softplus", "tanh", "rank_gap"], "implementation_hint": {"expects": ["cost_a", "cost_b", "log_prob_w", "log_prob_l", "weight"], "returns": "scalar"}, "code": "import torch\nimport torch.nn.functional as F\n\ndef _rank_gap(x):\n    \"\"\"Computes the rank of each element in x and normalizes it to [0, 1].\"\"\"\n    if x.numel() <= 1:\n        return torch.zeros_like(x)\n    # stable_argsort twice gives the rank\n    ranks = torch.argsort(torch.argsort(x).float()).float()\n    # Normalize to [0, 1]\n    normalized_ranks = ranks / (x.numel() - 1)\n    return normalized_ranks\n\ndef generated_loss(batch, model_output, extra):\n    \"\"\"\n    Computes the Rank-Gap Scaled Softplus Loss.\n\n    This loss encourages the log probability of the winner to be higher than the loser\n    by an adaptive margin. The margin is based on the tanh-transformed cost difference\n    (from Parent 1) and is further scaled by a dynamic factor derived from the\n    rank of the cost difference within the batch (new idea).\n    The core loss uses a softplus function (from both parents).\n\n    Args:\n        batch (dict): A dictionary containing tensors from the dataloader.\n                      Expected keys:\n                      'cost_a', 'cost_b': Costs of the two solutions. Shape: [N].\n                      'log_prob_w', 'log_prob_l': Model's log probabilities for the\n                                                winner and loser solutions. Shape: [N].\n                      'weight' (optional): Per-sample loss weights. Shape: [N].\n        model_output: Not used in this loss implementation.\n        extra (dict): A dictionary for hyperparameters. Expected keys:\n                      'alpha' (float): Base scaling factor for the margin.\n                      'temp_cost' (float): Temperature for normalizing cost difference.\n                      'rank_gap_scaler' (float): Controls the steepness of the rank-based scaling.\n\n    Returns:\n        torch.Tensor: A scalar tensor representing the mean loss over the batch.\n    \"\"\"\n    cost_w = torch.min(batch['cost_a'], batch['cost_b'])\n    cost_l = torch.max(batch['cost_a'], batch['cost_b'])\n    log_prob_w = batch['log_prob_w']\n    log_prob_l = batch['log_prob_l']\n\n    alpha = extra.get('alpha', 1.5)\n    temp_cost = extra.get('temp_cost', 1.0)\n    rank_gap_scaler = extra.get('rank_gap_scaler', 2.0)\n\n    delta_cost = cost_l - cost_w\n    delta_logp = log_prob_w - log_prob_l\n\n    # Idea from Parent 1: Use tanh for a stable, bounded cost signal.\n    # temp_cost controls the sensitivity of the signal.\n    cost_signal = torch.tanh(delta_cost / temp_cost)\n\n    # Idea from both Parents: Create a base adaptive margin.\n    base_margin = alpha * cost_signal\n\n    # New Coupling Idea 1: Compute rank gap to understand relative cost differences.\n    with torch.no_grad():\n        # Detach to prevent gradients from flowing through the ranking process.\n        cost_diff_rank_gap = _rank_gap(delta_cost.detach())\n\n    # New Coupling Idea 2: Create a smooth, dynamic scaler from the rank gap.\n    # This increases the effective margin for pairs with a relatively large cost gap\n    # and decreases it for pairs with a small gap, focusing learning.\n    # softplus(scaler * (rank - 0.5)) centers the scaling around the median rank.\n    adaptive_scaler = F.softplus(rank_gap_scaler * (cost_diff_rank_gap - 0.5))\n\n    # Combine base margin with the adaptive scaler.\n    final_margin = base_margin * adaptive_scaler\n\n    # Idea from both Parents: Use softplus for a smooth hinge loss.\n    # The loss is low if delta_logp > final_margin.\n    loss = F.softplus(final_margin - delta_logp)\n\n    weights = batch.get('weight', None)\n    if weights is not None:\n        loss = loss * weights\n        return loss.sum() / weights.sum().clamp(min=1e-8)\n    else:\n        return loss.mean()"}, "static_ok": true, "static_reason": "", "dynamic_ok": false, "dynamic_reason": "forward_error: name '_rank_gap' is not defined", "loss_value": null, "grad_norm": null}
{"generation": 2, "index": 6, "ir": {"name": "Z-Scored Adaptive Margin LogSigmoid Loss", "intuition": "This loss function combines the adaptive margin concept from both parents with a new batch-level normalization scheme for robustness. \n\nInherited ideas:\n- From both parents, it inherits the core structure of an adaptive margin loss, where the target separation between the winner's and loser's log probabilities (`delta_logp`) is determined by the magnitude of their cost difference (`delta_cost`).\n- From Parent 0, it inherits the use of `softplus` to transform the cost difference into a non-saturating, unbounded signal. This ensures that very large cost differences create a correspondingly large margin target, pushing the model harder to distinguish them.\n\nNew coupling ideas:\n1.  **Z-Score Normalization:** Instead of simply scaling the cost difference by a temperature, we apply a batch-level z-score normalization to the `delta_cost` values. This makes the margin less sensitive to the absolute scale of costs in a given batch and more dependent on the relative cost differences within that batch. This can improve stability and reduce hyperparameter tuning, as the effective scale of `delta_cost` is standardized before being used.\n2.  **LogSigmoid Formulation:** Instead of using a `softplus(margin - delta_logp)` formulation, we switch to a `logsigmoid` formulation: `-logsigmoid(delta_logp - margin)`. This is a common and numerically stable way to express a preference loss, directly encouraging `delta_logp` to be greater than the `margin`. It frames the problem as maximizing the log-likelihood of the preference.", "pseudocode": "1. For each pair (a, b) where cost(a) < cost(b), calculate the cost difference `delta_cost = cost(b) - cost(a)` and the log probability difference `delta_logp = logp(a) - logp(b)`.\n2. Across the entire batch, compute the z-score of the `delta_cost` vector. This normalizes the cost differences to have a mean of 0 and a standard deviation of 1, making the subsequent margin calculation robust to the scale of costs in the batch.\n3. Transform the z-scored cost differences using a softplus function to create a smooth, non-negative, and unbounded 'cost signal'.\n4. Create an adaptive margin `margin` by scaling this cost signal with a hyperparameter `alpha`.\n5. The loss is then calculated as the negative log-sigmoid of the difference between the model's preference and the adaptive margin: `loss = -logsigmoid(delta_logp - margin)`. This penalizes cases where `delta_logp` is less than the target margin.\n6. The final loss is the weighted mean of these individual losses over the batch.", "hyperparams": {"alpha": 1.0}, "operators_used": ["logsigmoid", "softplus", "zscore"], "implementation_hint": {"expects": ["cost_a", "cost_b", "log_prob_w", "log_prob_l", "weight"], "returns": "scalar"}, "code": "import torch\nimport torch.nn.functional as F\n\ndef zscore(x, epsilon=1e-8):\n    \"\"\"Numerically stable z-score normalization.\"\"\"\n    # Only normalize if there's more than one element and variance is non-trivial\n    if x.numel() <= 1:\n        return x - x.mean()\n    std, mean = torch.std_mean(x, unbiased=False)\n    return (x - mean) / (std + epsilon)\n\ndef generated_loss(batch, model_output, extra):\n    \"\"\"\n    Computes the Z-Scored Adaptive Margin LogSigmoid Loss.\n\n    This loss encourages the log probability of the winner to be higher than the loser's\n    by an adaptive margin. The margin is derived from the z-scored and softplus-transformed\n    cost difference, making it robust to the scale of costs within a batch. The loss is\n    formulated using the numerically stable logsigmoid function.\n\n    Args:\n        batch (dict): A dictionary containing tensors from the dataloader.\n                      Expected keys:\n                      'cost_a', 'cost_b': Costs of the two solutions. Shape: [N].\n                      'log_prob_w', 'log_prob_l': Model's log probabilities for the\n                                                winner and loser. Shape: [N].\n                      'weight' (optional): Per-sample loss weights. Shape: [N].\n        model_output: Not used in this loss implementation.\n        extra (dict): A dictionary for hyperparameters. Expected key:\n                      'alpha' (float): A scaling factor for the adaptive margin.\n\n    Returns:\n        torch.Tensor: A scalar tensor representing the mean loss over the batch.\n    \"\"\"\n    # Assume cost_w < cost_l is established by the data loader\n    cost_w = torch.min(batch['cost_a'], batch['cost_b'])\n    cost_l = torch.max(batch['cost_a'], batch['cost_b'])\n\n    log_prob_w = batch['log_prob_w']\n    log_prob_l = batch['log_prob_l']\n\n    # Retrieve hyperparameter\n    alpha = extra.get('alpha', 1.0)\n\n    # 1. Calculate cost and log probability differences\n    delta_cost = cost_l - cost_w\n    delta_logp = log_prob_w - log_prob_l\n\n    # 2. Apply z-score normalization to delta_cost across the batch\n    # This makes the margin calculation robust to the scale of costs\n    delta_cost_normalized = zscore(delta_cost)\n\n    # 3. Transform the normalized cost difference into an unbounded, non-negative signal\n    # This is inherited from Parent 0's use of softplus for a non-saturating signal.\n    cost_signal = F.softplus(delta_cost_normalized)\n\n    # 4. Create an adaptive margin based on the cost signal\n    adaptive_margin = alpha * cost_signal\n\n    # 5. Compute the loss using a logsigmoid formulation\n    # This encourages delta_logp > adaptive_margin\n    loss = -F.logsigmoid(delta_logp - adaptive_margin)\n\n    # 6. Apply optional weights and compute the mean\n    weights = batch.get('weight', None)\n    if weights is not None:\n        loss = loss * weights\n        return loss.sum() / weights.sum().clamp(min=1e-8)\n    else:\n        return loss.mean()"}, "static_ok": true, "static_reason": "", "dynamic_ok": false, "dynamic_reason": "forward_error: name 'zscore' is not defined", "loss_value": null, "grad_norm": null}
{"generation": 2, "index": 7, "ir": {"name": "Adaptive Sigmoid-Weighted Loss", "intuition": "This loss function combines an adaptive margin with a dynamic weighting scheme. The core loss is based on the Bradley-Terry model, using `logsigmoid` of the log-probability difference, which is a standard preference loss. \n\nInherited ideas:\n- From Parent 0/1: The concept of an adaptive margin, `margin = alpha * cost_signal`, is inherited. The model is pushed harder to separate pairs with larger cost differences.\n- From Parent 1: The use of `tanh` to create a bounded `cost_signal` from the cost difference (`delta_cost`) is inherited. This ensures the margin remains stable and doesn't grow infinitely.\n\nNew coupling ideas:\n1.  **Dynamic Loss Weighting:** Instead of a simple hinge-like loss `softplus(margin - delta_logp)`, this child loss introduces a dynamic weight that modulates a standard `logsigmoid` loss. This weight, `loss_weight = 1.0 + sigmoid(margin - delta_logp)`, increases the penalty when the model's preference (`delta_logp`) falls short of the target margin. When the model's preference is much lower than the margin, the sigmoid approaches 1, effectively doubling the loss's importance. When the preference meets or exceeds the margin, the sigmoid approaches 0, and the weight approaches 1, applying a standard penalty.\n2.  **Coupling Margin and Loss Form:** The adaptive margin is not used to set a hard threshold but to dynamically scale the importance of the base `logsigmoid` loss. This creates a smoother penalty landscape, where the model is always encouraged to increase `delta_logp`, but the gradient strength is amplified for pairs where the model's confidence is particularly misaligned with the ground-truth cost difference.", "pseudocode": "1. For each pair (winner, loser), calculate the cost difference `delta_cost = cost(loser) - cost(winner)` and the log probability difference `delta_logp = logp(winner) - logp(loser)`.\n2. Create a bounded 'cost signal' from the cost difference using `tanh`: `cost_signal = tanh(delta_cost / temp_cost)`.\n3. Calculate an adaptive margin target based on this signal: `margin = alpha * cost_signal`.\n4. Calculate the base preference loss using `logsigmoid`: `base_loss = -logsigmoid(delta_logp)`.\n5. Calculate a dynamic weight based on the difference between the target margin and the model's current preference: `loss_weight = 1.0 + sigmoid(margin - delta_logp)`. This weight is close to 2.0 when the model fails to meet the margin and close to 1.0 when it succeeds.\n6. The final loss for the pair is the base loss multiplied by the dynamic weight: `loss = loss_weight * base_loss`.\n7. The final loss is the weighted mean of these individual losses over the batch.", "hyperparams": {"alpha": 2.0, "temp_cost": 1.0}, "operators_used": ["logsigmoid", "sigmoid", "tanh"], "implementation_hint": {"expects": ["cost_a", "cost_b", "logp_w", "logp_l", "weight"], "returns": "scalar"}, "code": "import torch\nimport torch.nn.functional as F\n\ndef generated_loss(batch, model_output, extra):\n    \"\"\"\n    Computes the Adaptive Sigmoid-Weighted Loss.\n\n    This loss uses a standard logsigmoid preference loss, but dynamically weights\n    each sample based on how well the model's log probability difference meets an\n    adaptive margin. The margin is derived from the ground-truth cost difference.\n\n    Args:\n        batch (dict): A dictionary containing tensors from the dataloader.\n                      Expected keys:\n                      'cost_a', 'cost_b': Costs of the two solutions in a pair. Shape: [N].\n                      'log_prob_w', 'log_prob_l': Model's log probabilities for the\n                                                winner and loser solutions. Shape: [N].\n                      'weight' (optional): Per-sample loss weights. Shape: [N].\n        model_output: Not used in this loss implementation.\n        extra (dict): A dictionary for hyperparameters. Expected keys:\n                      'alpha' (float): A scaling factor for the adaptive margin.\n                      'temp_cost' (float): A temperature for normalizing the cost difference.\n\n    Returns:\n        torch.Tensor: A scalar tensor representing the mean loss over the batch.\n    \"\"\"\n    # Assume cost_w < cost_l for winner (w) and loser (l).\n    cost_w = torch.min(batch['cost_a'], batch['cost_b'])\n    cost_l = torch.max(batch['cost_a'], batch['cost_b'])\n\n    log_prob_w = batch['log_prob_w']\n    log_prob_l = batch['log_prob_l']\n\n    # Retrieve hyperparameters\n    alpha = extra.get('alpha', 2.0)\n    temp_cost = extra.get('temp_cost', 1.0)\n\n    # 1. Calculate cost and log probability differences\n    delta_cost = cost_l - cost_w\n    delta_logp = log_prob_w - log_prob_l\n\n    # 2. Inherited from Parent 1: Create a bounded 'cost signal' using tanh.\n    # This signal is in [0, 1) and represents the normalized cost gap.\n    cost_signal = torch.tanh(delta_cost / temp_cost)\n\n    # 3. Inherited from Parent 0/1: Calculate an adaptive margin target.\n    # This is the desired log probability separation for a given cost gap.\n    adaptive_margin = alpha * cost_signal\n\n    # 4. Calculate the base preference loss using logsigmoid (Bradley-Terry model).\n    # This loss is always non-negative and encourages delta_logp to be positive.\n    base_loss = -F.logsigmoid(delta_logp)\n\n    # 5. New Coupling Idea: Create a dynamic weight based on the margin error.\n    # The weight is ~2 when delta_logp << margin, and ~1 when delta_logp >= margin.\n    # This amplifies the loss for samples where the model's confidence is much lower\n    # than what the cost difference suggests it should be.\n    margin_error = adaptive_margin - delta_logp\n    loss_weight = 1.0 + torch.sigmoid(margin_error)\n\n    # 6. Apply the dynamic weight to the base loss.\n    loss = loss_weight * base_loss\n\n    # 7. Apply optional batch weights and compute the mean.\n    weights = batch.get('weight', None)\n    if weights is not None:\n        loss = loss * weights\n        # Use a small epsilon to prevent division by zero if all weights are zero.\n        return loss.sum() / weights.sum().clamp(min=1e-8)\n    else:\n        return loss.mean()"}, "static_ok": true, "static_reason": "", "dynamic_ok": true, "dynamic_reason": "ok", "loss_value": 1.2622262239456177, "grad_norm": 0.0}
{"generation": 3, "index": 0, "ir": {"name": "Adaptive Margin Loss with Rank-Gap Z-Scoring", "intuition": "This loss function synergizes the adaptive margin concept from its parents with a novel normalization scheme. \n\nInherited from Parent 0 and 1: The core idea is an adaptive margin, where the target separation between the winner's and loser's log-probabilities (`delta_logp`) scales with the ground-truth cost difference (`delta_cost`). This ensures the model is pushed harder to distinguish between solutions with a large quality gap. The loss is computed using `softplus(margin - delta_logp)`, providing a smooth, non-negative penalty when the model's preference does not meet the target margin.\n\nInherited from Parent 1: The cost difference is transformed using a bounded function, `tanh`, to create a stable `cost_signal` that saturates for very large cost differences, preventing the margin from growing uncontrollably.\n\nNew Coupling Idea 1 (Rank-Gap Z-Scoring): Instead of using the raw `delta_logp`, we normalize it within the batch using a z-score. This normalization stabilizes training by making the loss invariant to the absolute scale and shift of the model's log-probabilities, focusing instead on the relative ranking of preferences within the batch. This prevents gradients from becoming too large or small due to model output drift.\n\nNew Coupling Idea 2 (Rank-Gap Clipping): To further enhance stability, especially in the presence of outlier preferences, the z-scored `delta_logp` is clamped to a reasonable range (e.g., [-3, 3]). This prevents a single mis-ranked pair with an extreme `delta_logp` from dominating the batch gradient.", "pseudocode": "1. For each pair (winner, loser) in the batch, calculate the cost difference `delta_cost = cost_loser - cost_winner` and the log probability difference `delta_logp = logp_winner - logp_loser`.\n2. Compute a bounded 'cost signal' from the cost difference: `cost_signal = tanh(delta_cost / temp_cost)`.\n3. Calculate the adaptive target margin for each pair: `margin = alpha * cost_signal`.\n4. Normalize the `delta_logp` values across the entire batch by computing their z-score: `delta_logp_z = zscore(delta_logp)`.\n5. For stability, clip the z-scored values to a predefined range: `delta_logp_z_clipped = clamp(delta_logp_z, min_clip, max_clip)`.\n6. Compute the per-sample loss using the softplus function on the difference between the target margin and the normalized, clipped model preference: `loss = softplus(margin - delta_logp_z_clipped)`.\n7. Calculate the final loss as the weighted mean of the per-sample losses over the batch.", "hyperparams": {"alpha": 2.0, "temp_cost": 1.0, "clip_range": 3.0}, "operators_used": ["softplus", "tanh", "zscore", "clamp"], "implementation_hint": {"expects": ["cost_a", "cost_b", "log_prob_w", "log_prob_l", "weight"], "returns": "scalar"}, "code": "import torch\nimport torch.nn.functional as F\n\ndef zscore(x, epsilon=1e-8):\n    \"\"\"Computes the z-score of a tensor, a custom whitelisted operator.\"\"\"\n    # Ensure there's more than one element to avoid NaN std dev\n    if x.numel() <= 1:\n        return torch.zeros_like(x)\n    mean = x.mean()\n    std = x.std()\n    return (x - mean) / (std + epsilon)\n\ndef generated_loss(batch, model_output, extra):\n    \"\"\"\n    Computes an adaptive margin loss where the model's log-probability difference\n    is z-scored and clipped for stability.\n\n    Args:\n        batch (dict): A dictionary containing tensors from the dataloader.\n                      Expected keys:\n                      'cost_a', 'cost_b': Costs of the two solutions in a pair. Shape: [N].\n                      'log_prob_w', 'log_prob_l': Model's log probabilities for the\n                                                winner and loser solutions. Shape: [N].\n                      'weight' (optional): Per-sample loss weights. Shape: [N].\n        model_output: Not used in this loss implementation.\n        extra (dict): A dictionary for hyperparameters. Expected keys:\n                      'alpha' (float): A scaling factor for the adaptive margin.\n                      'temp_cost' (float): A temperature for normalizing the cost difference.\n                      'clip_range' (float): The symmetric range for clipping z-scores.\n\n    Returns:\n        torch.Tensor: A scalar tensor representing the mean loss over the batch.\n    \"\"\"\n    # Assume cost_w < cost_l is the preference\n    cost_w = torch.min(batch['cost_a'], batch['cost_b'])\n    cost_l = torch.max(batch['cost_a'], batch['cost_b'])\n    log_prob_w = batch['log_prob_w']\n    log_prob_l = batch['log_prob_l']\n\n    # Retrieve hyperparameters\n    alpha = extra.get('alpha', 2.0)\n    temp_cost = extra.get('temp_cost', 1.0)\n    clip_range = extra.get('clip_range', 3.0)\n\n    # 1. Calculate cost and log probability differences\n    delta_cost = cost_l - cost_w\n    delta_logp = log_prob_w - log_prob_l\n\n    # 2. Inherit from Parent 1: Use tanh to create a bounded cost signal.\n    # This creates a stable, bounded signal from the cost difference.\n    cost_signal = torch.tanh(delta_cost / temp_cost)\n\n    # 3. Inherit from both parents: Create an adaptive margin based on the cost signal.\n    adaptive_margin = alpha * cost_signal\n\n    # 4. New Coupling Idea 1: Normalize delta_logp across the batch using z-scoring.\n    # This stabilizes training by focusing on relative preference ranks within the batch,\n    # making the loss invariant to the scale and shift of log-probabilities.\n    delta_logp_z = zscore(delta_logp)\n\n    # 5. New Coupling Idea 2: Clip the z-scored values for robustness against outliers.\n    # This prevents extreme preference scores from a single pair from dominating the gradient.\n    delta_logp_z_clipped = torch.clamp(delta_logp_z, -clip_range, clip_range)\n\n    # 6. Inherit from both parents: Compute the core loss using softplus.\n    # The loss penalizes cases where the normalized model preference falls short of the margin.\n    loss = F.softplus(adaptive_margin - delta_logp_z_clipped)\n\n    # 7. Apply optional weights and compute the mean.\n    weights = batch.get('weight', None)\n    if weights is not None:\n        loss = loss * weights\n        return loss.sum() / weights.sum().clamp(min=1e-8)\n    else:\n        return loss.mean()"}, "static_ok": true, "static_reason": "", "dynamic_ok": false, "dynamic_reason": "forward_error: name 'zscore' is not defined", "loss_value": null, "grad_norm": null}
{"generation": 3, "index": 1, "ir": {"name": "Z-Scored LogSigmoid Adaptive Margin Loss", "intuition": "This loss function combines the adaptive margin concept from its parents with a new normalization and stability mechanism. It inherits the core structure of `loss = softplus(margin - delta_logp)` where the margin is proportional to a cost signal. From one parent, it inherits the use of `tanh` to create a bounded, saturating cost signal, which prevents extremely large cost differences from dominating the loss. From the other, it implicitly inherits the idea of a non-saturating signal, but re-imagines it. The key new coupling idea is to normalize the model's log-probability difference (`delta_logp`) using a batch-wise Z-score. This stabilizes training by centering the model's preference predictions around zero with a standard deviation of one for each batch. This prevents the model's output scale from drifting and makes the fixed `alpha` hyperparameter more effective across different training stages. An additional coupling is the use of `logsigmoid` instead of `softplus` for the final loss calculation. While mathematically related, `logsigmoid(x)` is equivalent to `softplus(-x)`, which frames the problem as maximizing the log-likelihood of the model's preference (`delta_logp`) exceeding the target margin, a common formulation in preference learning.", "pseudocode": "1. For each pair (winner, loser), calculate the cost difference `delta_cost = cost(loser) - cost(winner)` and the log probability difference `delta_logp = logp(winner) - logp(loser)`.\n2. (Inherited from Parent 1) Create a bounded 'cost signal' from the cost difference using the `tanh` function: `cost_signal = tanh(delta_cost / temp_cost)`.\n3. (Inherited from both) Define an adaptive target margin proportional to this signal: `margin_target = alpha * cost_signal`.\n4. (New Coupling 1 - Normalization) Normalize the log probability differences across the entire batch using a Z-score transformation: `delta_logp_zscored = zscore(delta_logp)`. This makes the loss less sensitive to the absolute scale of the model's log probabilities.\n5. (New Coupling 2 - Loss Formulation) Compute the final loss using `logsigmoid`. The loss for each sample is `-logsigmoid(delta_logp_zscored - margin_target)`. This encourages the z-scored log probability difference to be greater than the target margin.\n6. The final loss is the weighted mean of these individual losses over the batch.", "hyperparams": {"alpha": 1.5, "temp_cost": 1.0}, "operators_used": ["logsigmoid", "tanh", "zscore"], "implementation_hint": {"expects": ["cost_a", "cost_b", "log_prob_w", "log_prob_l", "weight"], "returns": "scalar"}, "code": "import torch\nimport torch.nn.functional as F\n\ndef zscore(x, eps=1e-8):\n    \"\"\"Batch-wise z-score normalization.\"\"\"\n    if x.numel() <= 1:\n        return x - x.mean()\n    mean = x.mean()\n    std = x.std().clamp(min=eps)\n    return (x - mean) / std\n\ndef generated_loss(batch, model_output, extra):\n    \"\"\"\n    Computes the Z-Scored LogSigmoid Adaptive Margin Loss.\n\n    This loss encourages the model's preference (logp_w - logp_l) to exceed an\n    adaptive margin. The margin is derived from the tanh-transformed cost difference,\n    making it bounded. The model's log-probability difference is normalized using a\n    batch-wise z-score for stability. The final loss is computed using logsigmoid.\n\n    Args:\n        batch (dict): A dictionary containing tensors from the dataloader.\n                      Expected keys:\n                      'cost_a', 'cost_b': Costs of the two solutions. Shape: [N].\n                      'log_prob_w', 'log_prob_l': Model's log probabilities for the\n                                                winner and loser. Shape: [N].\n                      'weight' (optional): Per-sample loss weights. Shape: [N].\n        model_output: Not used in this loss implementation.\n        extra (dict): A dictionary for hyperparameters. Expected keys:\n                      'alpha' (float): Scaling factor for the adaptive margin.\n                      'temp_cost' (float): Temperature for the cost difference normalization.\n\n    Returns:\n        torch.Tensor: A scalar tensor representing the mean loss over the batch.\n    \"\"\"\n    # For preference learning, we assume winner (w) has lower cost than loser (l).\n    cost_w = torch.min(batch['cost_a'], batch['cost_b'])\n    cost_l = torch.max(batch['cost_a'], batch['cost_b'])\n\n    log_prob_w = batch['log_prob_w']\n    log_prob_l = batch['log_prob_l']\n\n    # Retrieve hyperparameters\n    alpha = extra.get('alpha', 1.5)\n    temp_cost = extra.get('temp_cost', 1.0)\n\n    # 1. Calculate cost and log probability differences\n    delta_cost = cost_l - cost_w\n    delta_logp = log_prob_w - log_prob_l\n\n    # 2. (Inherited) Create a bounded 'cost signal' using tanh.\n    cost_signal = torch.tanh(delta_cost / temp_cost)\n\n    # 3. (Inherited) Define an adaptive target margin.\n    margin_target = alpha * cost_signal\n\n    # 4. (New Coupling) Normalize the model's preference signal using z-score.\n    # This stabilizes training by decoupling the loss from the scale of log-probabilities.\n    delta_logp_zscored = zscore(delta_logp)\n\n    # 5. (New Coupling) Compute the loss using logsigmoid.\n    # loss = -logsigmoid(x) is equivalent to softplus(-x).\n    # We want delta_logp_zscored to be greater than margin_target.\n    # So, the argument to logsigmoid should be (delta_logp_zscored - margin_target).\n    loss = -F.logsigmoid(delta_logp_zscored - margin_target)\n\n    # 6. Apply optional weights and compute the mean.\n    weights = batch.get('weight', None)\n    if weights is not None:\n        loss = loss * weights\n        return loss.sum() / weights.sum().clamp(min=1e-8)\n    else:\n        return loss.mean()"}, "static_ok": true, "static_reason": "", "dynamic_ok": false, "dynamic_reason": "forward_error: name 'zscore' is not defined", "loss_value": null, "grad_norm": null}
{"generation": 3, "index": 2, "ir": {"name": "Rank-Gap Scaled Softplus Margin Loss", "intuition": "This loss function combines the adaptive margin concept from its parents with a novel rank-based scaling mechanism for enhanced stability and focus on relative cost differences. \n\nInherited ideas:\n- From both parents, it inherits the core structure of an adaptive margin loss: `loss = softplus(margin - delta_logp)`, which smoothly penalizes the model when its log probability difference (`delta_logp`) falls short of a target margin.\n- From both parents, it also inherits the idea of making this margin dependent on the ground-truth cost difference (`delta_cost`), so that pairs with larger cost gaps are pushed to have larger log probability gaps.\n\nNew coupling ideas:\n1. **Rank-Gap Scaling**: Instead of using the raw `delta_cost`, which can have a very wide and unpredictable distribution, the loss uses a `rank_gap` normalization. The `delta_cost` values across the batch are converted to their fractional ranks (0 to 1). This maps the cost differences to a stable, bounded range, making the loss less sensitive to outliers and the absolute scale of costs. This rank-based signal is then scaled by a hyperparameter `beta` to control the margin's magnitude.\n2. **Log-Probability Z-Score Normalization**: To prevent the model's output `delta_logp` from drifting to extreme values or having a large batch-wise offset, it is normalized using a z-score transformation within the batch. This stabilizes the learning dynamics by ensuring the log probability differences are centered around zero with a standard deviation of one before being compared to the margin. This focuses the loss on the relative log-probability ordering within the batch, rather than their absolute values.", "pseudocode": "1. For each pair (winner, loser) in the batch, calculate the cost difference `delta_cost = cost_loser - cost_winner` and the log probability difference `delta_logp = logp_winner - logp_loser`.\n2. Apply a z-score normalization to `delta_logp` across the batch to get `delta_logp_norm`. This centers the distribution and scales it to unit variance.\n3. Convert the `delta_cost` values for the entire batch into fractional ranks from 0 to 1 using the `rank_gap` operator. This creates a bounded and robust `cost_signal`.\n4. Calculate the adaptive margin by scaling this `cost_signal` with a hyperparameter `beta`: `margin = beta * cost_signal`.\n5. Compute the per-sample loss using the softplus function on the difference between the margin and the normalized log probability difference: `loss = softplus(margin - delta_logp_norm)`.\n6. Calculate the final loss as the weighted average of the per-sample losses over the batch.", "hyperparams": {"beta": 2.5}, "operators_used": ["softplus", "zscore", "rank_gap"], "implementation_hint": {"expects": ["cost_a", "cost_b", "logp_a", "logp_b"], "returns": "scalar"}, "code": "import torch\nimport torch.nn.functional as F\n\ndef rank_gap(x):\n    \"\"\"Converts a tensor to its fractional ranks. Ties are handled by averaging ranks.\"\"\"\n    if x.numel() == 0:\n        return x\n    ranks = torch.empty_like(x, dtype=torch.float)\n    ranks[x.argsort()] = torch.arange(len(x), device=x.device, dtype=torch.float)\n    # Normalize to [0, 1] for a batch of size > 1\n    if len(x) > 1:\n        ranks = ranks / (len(x) - 1)\n    else:\n        ranks = torch.zeros_like(x) # Single element is rank 0\n    return ranks\n\ndef zscore(x):\n    \"\"\"Computes z-score for a tensor, with a safe guard for zero standard deviation.\"\"\"\n    if x.numel() <= 1:\n        return torch.zeros_like(x)\n    mean = x.mean()\n    std = x.std()\n    # Add a small epsilon to prevent division by zero for constant inputs\n    return (x - mean) / (std + 1e-8)\n\ndef generated_loss(batch, model_output, extra):\n    \"\"\"\n    Computes the Rank-Gap Scaled Softplus Margin Loss.\n\n    This loss encourages the model's log probability difference to exceed an adaptive margin.\n    The margin is derived from the rank-normalized cost difference, making it robust to cost outliers.\n    The model's log probability difference is z-score normalized for training stability.\n\n    Args:\n        batch (dict): A dictionary containing tensors from the dataloader.\n                      Expected keys:\n                      'cost_a', 'cost_b': Costs of the two solutions. Shape: [N].\n                      'log_prob_w', 'log_prob_l': Log probabilities for winner and loser. Shape: [N].\n                      'weight' (optional): Per-sample loss weights. Shape: [N].\n        model_output: Not used.\n        extra (dict): A dictionary for hyperparameters. Expected keys:\n                      'beta' (float): Scales the rank-based margin.\n\n    Returns:\n        torch.Tensor: A scalar tensor representing the mean loss over the batch.\n    \"\"\"\n    cost_w = torch.min(batch['cost_a'], batch['cost_b'])\n    cost_l = torch.max(batch['cost_a'], batch['cost_b'])\n    log_prob_w = batch['log_prob_w']\n    log_prob_l = batch['log_prob_l']\n\n    beta = extra.get('beta', 2.5)\n\n    # 1. Calculate cost and log probability differences\n    delta_cost = cost_l - cost_w\n    delta_logp = log_prob_w - log_prob_l\n\n    # 2. (New Coupling) Normalize delta_logp using z-score for stability\n    delta_logp_norm = zscore(delta_logp)\n\n    # 3. (New Coupling) Create a bounded cost signal using rank-gap normalization\n    # This makes the margin robust to the scale and distribution of costs.\n    cost_signal = rank_gap(delta_cost)\n\n    # 4. (Inherited) Calculate an adaptive margin based on the cost signal\n    # Beta controls the magnitude of the target separation.\n    margin = beta * cost_signal\n\n    # 5. (Inherited) Compute the core loss using softplus\n    # This penalizes cases where the normalized logp difference is less than the margin.\n    loss = F.softplus(margin - delta_logp_norm)\n\n    # 6. Apply optional weights and compute the mean\n    weights = batch.get('weight', None)\n    if weights is not None:\n        loss = loss * weights\n        return loss.sum() / weights.sum().clamp(min=1e-8)\n    else:\n        return loss.mean()"}, "static_ok": true, "static_reason": "", "dynamic_ok": false, "dynamic_reason": "forward_error: name 'zscore' is not defined", "loss_value": null, "grad_norm": null}
{"generation": 3, "index": 3, "ir": {"name": "Rank-Gap Scaled LogSigmoid Loss", "intuition": "This loss function combines the log-sigmoid structure for preference learning with an adaptive margin that is sensitive to both the magnitude and the rank of the cost difference. \n\nInherited ideas:\n- From both parents, it inherits the concept of an **adaptive margin** that scales with the cost difference (`delta_cost`). The model is pushed harder to separate pairs with a larger cost gap.\n- It also inherits the general structure of comparing the model's preference (`delta_logp`) against this margin.\n\nNew coupling ideas:\n1.  **LogSigmoid Core:** Instead of the `softplus(margin - x)` structure from the parents, this loss uses a `logsigmoid` formulation, specifically `logsigmoid(delta_logp - margin)`. This is a common and stable way to frame preference loss, directly encouraging `delta_logp` to be larger than `margin`.\n2.  **Rank-Gap Scaling:** The margin is scaled not only by the magnitude of the cost difference (`delta_cost`) but also by its relative rank within the batch. The `rank_gap` operator computes a normalized rank (0 to 1) for each `delta_cost`. This new term, `beta * rank_gap`, is added to the margin. This coupling ensures that pairs with an unusually large cost difference *for the current batch* receive an extra push, making the loss function dynamically adapt to the batch's difficulty distribution. This helps prevent the loss from being dominated by a few outlier pairs with globally huge cost differences, while still focusing on the most significant pairs within any given batch.", "pseudocode": "1. For each pair (winner, loser), calculate the cost difference `delta_cost = cost_loser - cost_winner` and the log probability difference `delta_logp = logp_winner - logp_loser`.\n2. Normalize the batch of `delta_cost` values to get a rank-based signal `cost_rank_signal` between 0 and 1 using the `rank_gap` operator. This indicates the relative importance of each pair's cost gap within the batch.\n3. Calculate a magnitude-based cost signal by applying a `tanh` function to the scaled cost difference: `cost_magnitude_signal = tanh(delta_cost / temp_cost)`. This is inherited from one of the parents.\n4. Combine the magnitude and rank signals to form the final adaptive margin: `margin = alpha * cost_magnitude_signal + beta * cost_rank_signal`.\n5. Compute the per-sample loss using a logsigmoid formulation, which penalizes cases where the model's preference is less than the adaptive margin: `loss = -logsigmoid(delta_logp - margin)`.\n6. The final loss is the weighted mean of these individual losses over the batch.", "hyperparams": {"alpha": 1.0, "beta": 0.5, "temp_cost": 1.0}, "operators_used": ["logsigmoid", "tanh", "rank_gap"], "implementation_hint": {"expects": ["cost_a", "cost_b", "log_prob_w", "log_prob_l", "weight"], "returns": "scalar"}, "code": "import torch\nimport torch.nn.functional as F\n\ndef rank_gap(x):\n    \"\"\"Computes the rank of each element in x and normalizes it to [0, 1].\"\"\"\n    if x.numel() <= 1:\n        return torch.zeros_like(x)\n    ranks = x.argsort().argsort().float()\n    # Normalize ranks to be in the [0, 1] range\n    normalized_ranks = ranks / (x.numel() - 1)\n    return normalized_ranks\n\ndef generated_loss(batch, model_output, extra):\n    \"\"\"\n    Computes the Rank-Gap Scaled LogSigmoid Loss.\n\n    This loss encourages logp(winner) > logp(loser) by an adaptive margin.\n    The margin is a combination of the tanh-scaled cost difference (magnitude)\n    and the normalized rank of the cost difference within the batch (relative importance).\n\n    Args:\n        batch (dict): A dictionary containing tensors from the dataloader.\n                      Expected keys:\n                      'cost_a', 'cost_b': Costs of the two solutions in a pair. Shape: [N].\n                      'log_prob_w', 'log_prob_l': Model's log probabilities for the\n                                                winner and loser solutions. Shape: [N].\n                      'weight' (optional): Per-sample loss weights. Shape: [N].\n        model_output: Not used in this loss implementation.\n        extra (dict): A dictionary for hyperparameters. Expected keys:\n                      'alpha' (float): Scaling factor for the magnitude-based margin.\n                      'beta' (float): Scaling factor for the rank-based margin.\n                      'temp_cost' (float): Temperature for cost difference normalization.\n\n    Returns:\n        torch.Tensor: A scalar tensor representing the mean loss over the batch.\n    \"\"\"\n    # For preference learning, we assume cost_w < cost_l\n    cost_w = torch.min(batch['cost_a'], batch['cost_b'])\n    cost_l = torch.max(batch['cost_a'], batch['cost_b'])\n\n    log_prob_w = batch['log_prob_w']\n    log_prob_l = batch['log_prob_l']\n\n    # Retrieve hyperparameters\n    alpha = extra.get('alpha', 1.0)\n    beta = extra.get('beta', 0.5)\n    temp_cost = extra.get('temp_cost', 1.0)\n\n    # 1. Calculate cost and log probability differences\n    # delta_cost is guaranteed to be >= 0\n    delta_cost = cost_l - cost_w\n    delta_logp = log_prob_w - log_prob_l\n\n    # 2. (New Coupling) Calculate the rank-based signal for the margin\n    cost_rank_signal = rank_gap(delta_cost)\n\n    # 3. (Inherited) Calculate the magnitude-based signal using tanh for stability\n    cost_magnitude_signal = torch.tanh(delta_cost / temp_cost)\n\n    # 4. Combine signals to form the final adaptive margin\n    # The margin is now sensitive to both absolute magnitude and relative rank\n    adaptive_margin = alpha * cost_magnitude_signal + beta * cost_rank_signal\n\n    # 5. (New Coupling) Compute the core loss using a stable logsigmoid formulation\n    # This encourages delta_logp > adaptive_margin\n    loss = -F.logsigmoid(delta_logp - adaptive_margin)\n\n    # 6. Apply optional weights and compute the mean\n    weights = batch.get('weight', None)\n    if weights is not None:\n        loss = loss * weights\n        return loss.sum() / weights.sum().clamp(min=1e-8)\n    else:\n        return loss.mean()"}, "static_ok": true, "static_reason": "", "dynamic_ok": false, "dynamic_reason": "forward_error: name 'rank_gap' is not defined", "loss_value": null, "grad_norm": null}
{"generation": 3, "index": 4, "ir": {"name": "Normalized Softplus-Tanh Margin Loss", "intuition": "This loss function synthesizes two approaches for creating an adaptive margin based on cost differences. It inherits the core structure of a softplus hinge loss, `softplus(margin - delta_logp)`, from both parents, which penalizes the model when its log probability difference (`delta_logp`) falls short of a target margin.\n\nFrom Parent 1, it inherits the use of `softplus` to transform the cost difference, creating a non-saturating signal that grows with the cost gap. From Parent 2, it inherits the use of `tanh` for the same purpose, which provides a bounded, stable signal. This child loss combines them: `margin = alpha * (softplus_signal + tanh_signal)`.\n\nTwo new coupling ideas are introduced for stability and control. First, the log probability difference (`delta_logp`) is dynamically normalized using z-scoring across the batch. This prevents extreme log probability values from dominating the loss, making training more stable and less sensitive to the output scale of the model. Second, the final per-sample loss is clamped to a maximum value `max_loss`. This acts as a gradient clipping mechanism, preventing large individual errors (e.g., from noisy data) from causing disruptive gradient updates.", "pseudocode": "1. For each pair (winner w, loser l), calculate the cost difference `delta_cost = cost_l - cost_w` and the log probability difference `delta_logp = logp_w - logp_l`.\n2. (New Coupling) Normalize the log probability differences across the batch using z-score to get `normalized_delta_logp`. This stabilizes the loss against the scale of model outputs.\n3. (Inherited from Parent 1) Calculate a non-saturating cost signal: `softplus_signal = softplus(delta_cost / temp_cost)`.\n4. (Inherited from Parent 2) Calculate a bounded, stable cost signal: `tanh_signal = tanh(delta_cost / temp_cost)`.\n5. Combine the inherited signals to form a hybrid adaptive margin: `margin = alpha * (softplus_signal + tanh_signal)`. This margin benefits from both linear growth and bounded stability.\n6. Calculate the core loss using the softplus hinge formulation: `loss = softplus(margin - normalized_delta_logp)`.\n7. (New Coupling) Clamp the per-sample loss to a maximum value `max_loss` to prevent outlier samples from generating excessively large gradients.\n8. The final loss is the weighted mean of these clamped losses over the batch.", "hyperparams": {"alpha": 0.5, "temp_cost": 1.0, "max_loss": 10.0}, "operators_used": ["softplus", "tanh", "zscore", "clamp"], "implementation_hint": {"expects": ["cost_a", "cost_b", "log_prob_w", "log_prob_l", "weight"], "returns": "scalar"}, "code": "import torch\nimport torch.nn.functional as F\n\ndef zscore(x, eps=1e-8):\n    \"\"\"Calculates z-score for a tensor across its first dimension.\"\"\"\n    if x.numel() <= 1:\n        return x\n    mean = x.mean()\n    std = x.std()\n    return (x - mean) / (std + eps)\n\ndef generated_loss(batch, model_output, extra):\n    \"\"\"\n    Computes the Normalized Softplus-Tanh Margin Loss.\n\n    This loss combines cost signals from both softplus and tanh to create a hybrid\n    adaptive margin. It also introduces z-score normalization on the log-probability\n    difference and clamping on the final loss for enhanced stability.\n\n    Args:\n        batch (dict): A dictionary containing tensors from the dataloader.\n                      Expected keys:\n                      'cost_a', 'cost_b': Costs of the two solutions in a pair. Shape: [N].\n                      'log_prob_w', 'log_prob_l': Model's log probabilities for the\n                                                winner and loser solutions. Shape: [N].\n                      'weight' (optional): Per-sample loss weights. Shape: [N].\n        model_output: Not used in this loss implementation.\n        extra (dict): A dictionary for hyperparameters. Expected keys:\n                      'alpha' (float): Scaling factor for the adaptive margin.\n                      'temp_cost' (float): Temperature for normalizing the cost difference.\n                      'max_loss' (float): Upper bound for clamping the per-sample loss.\n\n    Returns:\n        torch.Tensor: A scalar tensor representing the mean loss over the batch.\n    \"\"\"\n    # Assume cost_w < cost_l\n    cost_w = torch.min(batch['cost_a'], batch['cost_b'])\n    cost_l = torch.max(batch['cost_a'], batch['cost_b'])\n    log_prob_w = batch['log_prob_w']\n    log_prob_l = batch['log_prob_l']\n\n    # Retrieve hyperparameters\n    alpha = extra.get('alpha', 0.5)\n    temp_cost = extra.get('temp_cost', 1.0)\n    max_loss = extra.get('max_loss', 10.0)\n\n    # 1. Calculate cost and log probability differences\n    delta_cost = cost_l - cost_w\n    delta_logp = log_prob_w - log_prob_l\n\n    # 2. (New Coupling) Normalize delta_logp for stability\n    normalized_delta_logp = zscore(delta_logp)\n\n    # 3. (Inherited) Calculate a non-saturating cost signal from softplus\n    softplus_signal = F.softplus(delta_cost / temp_cost)\n\n    # 4. (Inherited) Calculate a bounded cost signal from tanh\n    tanh_signal = torch.tanh(delta_cost / temp_cost)\n\n    # 5. Combine signals to form a hybrid adaptive margin\n    # Alpha is scaled down since we are summing two signals\n    adaptive_margin = alpha * (softplus_signal + tanh_signal)\n\n    # 6. Compute the core loss using the softplus hinge formulation\n    loss_per_sample = F.softplus(adaptive_margin - normalized_delta_logp)\n\n    # 7. (New Coupling) Clamp the loss to prevent gradient explosion from outliers\n    clamped_loss = torch.clamp(loss_per_sample, max=max_loss)\n\n    # 8. Apply optional weights and compute the mean\n    weights = batch.get('weight', None)\n    if weights is not None:\n        loss = clamped_loss * weights\n        return loss.sum() / weights.sum().clamp(min=1e-8)\n    else:\n        return clamped_loss.mean()"}, "static_ok": true, "static_reason": "", "dynamic_ok": false, "dynamic_reason": "forward_error: name 'zscore' is not defined", "loss_value": null, "grad_norm": null}
{"generation": 3, "index": 5, "ir": {"name": "Rank-Gap Scaled Softplus Loss", "intuition": "This loss function combines the adaptive margin concept from its parents with a novel scaling mechanism based on cost ranks. The core idea is to create a dynamic margin that adapts to the magnitude of the cost difference, similar to the parents. This is inherited from both parents, which use a transformed cost difference (`delta_cost`) to set a target for the log probability difference (`delta_logp`). The `softplus(margin - delta_logp)` structure, which provides a smooth hinge-like penalty, is also inherited from both parents.\n\nTo introduce a new coupling, we modulate the loss based on the relative ranking of cost differences within the batch. We use a `rank_gap` operator on `delta_cost`, which normalizes the cost difference into a percentile-like value from 0 to 1. This new `cost_rank_signal` is then used to scale the entire loss term. This has two effects: 1) It emphasizes pairs with a larger cost difference relative to others in the same batch, pushing the model to learn more from clear-cut examples. 2) It de-emphasizes pairs with small cost differences, preventing the model from over-penalizing minor preference violations or noise, thus improving stability. This rank-based scaling is a new coupling idea not present in the parents. For numerical stability, we use `tanh` to create a bounded margin, an idea inherited from the second parent, preventing the margin from growing uncontrollably.", "pseudocode": "1. For each pair (winner, loser), calculate the cost difference `delta_cost = cost(loser) - cost(winner)` and the log probability difference `delta_logp = logp(winner) - logp(loser)`. This is a standard step inherited from both parents.\n2. Create an adaptive margin using a bounded transformation of the cost difference: `margin = alpha * tanh(delta_cost / temp_cost)`. This is inherited from the second parent, using `tanh` for stability.\n3. Compute the base preference loss using a smooth hinge function: `base_loss = softplus(margin - delta_logp)`. This structure is inherited from both parents.\n4. Introduce a new coupling: Calculate a `cost_rank_signal` by applying a `rank_gap` normalization to the `delta_cost` across the batch. This signal will range from 0 to 1, representing the percentile rank of each pair's cost difference.\n5. Apply a small offset `epsilon` to the rank signal to ensure even the lowest-ranked pairs contribute a minimal amount to the gradient: `rank_signal_stable = cost_rank_signal + epsilon`.\n6. Modulate the base loss by this stable rank signal: `final_loss = rank_signal_stable * base_loss`.\n7. The final loss is the weighted mean of these modulated individual losses over the batch.", "hyperparams": {"alpha": 1.5, "temp_cost": 1.0, "epsilon": 0.05}, "operators_used": ["softplus", "tanh", "rank_gap"], "implementation_hint": {"expects": ["cost_a", "cost_b", "log_prob_w", "log_prob_l", "weight"], "returns": "scalar"}, "code": "import torch\nimport torch.nn.functional as F\n\ndef rank_gap(x):\n    \"\"\"Normalizes a tensor to [0, 1] based on its rank.\"\"\"\n    if x.numel() <= 1:\n        return torch.ones_like(x)\n    # Compute ranks (average for ties)\n    sorted_indices = torch.argsort(x)\n    ranks = torch.empty_like(x)\n    ranks[sorted_indices] = torch.arange(len(x), device=x.device, dtype=x.dtype)\n    # Normalize ranks to [0, 1]\n    # Subtract 1 from denominator for N-1 scaling\n    normalized_ranks = ranks / (len(x) - 1).clamp(min=1.0)\n    return normalized_ranks\n\ndef generated_loss(batch, model_output, extra):\n    \"\"\"\n    Computes the Rank-Gap Scaled Softplus Loss.\n\n    Inherits the softplus adaptive margin structure from parents.\n    Uses tanh for a bounded margin (from parent 1).\n    Introduces a new coupling by scaling the loss of each pair by the\n    rank of its cost difference within the batch.\n\n    Args:\n        batch (dict): A dictionary containing tensors from the dataloader.\n                      Expected keys:\n                      'cost_a', 'cost_b': Costs of the two solutions. Shape: [N].\n                      'log_prob_w', 'log_prob_l': Log probabilities for winner/loser. Shape: [N].\n                      'weight' (optional): Per-sample loss weights. Shape: [N].\n        model_output: Not used.\n        extra (dict): Hyperparameters. Expected keys:\n                      'alpha' (float): Scaling factor for the margin.\n                      'temp_cost' (float): Temperature for cost difference scaling.\n                      'epsilon' (float): Small offset for the rank signal to ensure non-zero gradients.\n\n    Returns:\n        torch.Tensor: A scalar tensor representing the mean loss.\n    \"\"\"\n    # Identify winner (w) and loser (l) based on costs\n    cost_w = torch.min(batch['cost_a'], batch['cost_b'])\n    cost_l = torch.max(batch['cost_a'], batch['cost_b'])\n\n    log_prob_w = batch['log_prob_w']\n    log_prob_l = batch['log_prob_l']\n\n    # Retrieve hyperparameters\n    alpha = extra.get('alpha', 1.5)\n    temp_cost = extra.get('temp_cost', 1.0)\n    epsilon = extra.get('epsilon', 0.05)\n\n    # 1. Calculate cost and log probability differences (Inherited)\n    delta_cost = cost_l - cost_w\n    delta_logp = log_prob_w - log_prob_l\n\n    # 2. Create a bounded adaptive margin using tanh (Inherited from Parent 1)\n    margin = alpha * torch.tanh(delta_cost / temp_cost.clamp(min=1e-8))\n\n    # 3. Compute the base preference loss using softplus (Inherited from both parents)\n    base_loss = F.softplus(margin - delta_logp)\n\n    # 4. New Coupling: Calculate a rank-based signal from the cost difference\n    # This normalizes the importance of each pair based on its relative cost gap\n    # within the batch.\n    with torch.no_grad(): # Ranking should not contribute to gradients\n        cost_rank_signal = rank_gap(delta_cost)\n    \n    # 5. New Stability Trick: Add a small epsilon to the rank signal\n    # This ensures that even pairs with the smallest cost difference in the batch\n    # contribute to the loss, preventing zero gradients.\n    stable_rank_signal = cost_rank_signal + epsilon\n\n    # 6. Modulate the base loss by the stable rank signal\n    loss = stable_rank_signal * base_loss\n\n    # 7. Apply optional weights and compute the mean\n    weights = batch.get('weight', None)\n    if weights is not None:\n        loss = loss * weights\n        return loss.sum() / weights.sum().clamp(min=1e-8)\n    else:\n        return loss.mean()"}, "static_ok": true, "static_reason": "", "dynamic_ok": false, "dynamic_reason": "forward_error: 'float' object has no attribute 'clamp'", "loss_value": null, "grad_norm": null}
{"generation": 3, "index": 6, "ir": {"name": "Rank-Gap Scaled Hinge Loss", "intuition": "This loss function synergizes the adaptive margin concept from its parents with a novel rank-based scaling mechanism for enhanced stability and focus on difficult examples. \n\nInherited ideas:\n- From both parents, it inherits the core structure of an adaptive margin loss: `loss = hinge_like_function(margin - delta_logp)`, where `delta_logp` is the model's preference `logp(winner) - logp(loser)`. This penalizes the model when its preference for the better solution is smaller than a target margin.\n- From Parent 0, it inherits the use of `softplus` to define the loss term `softplus(margin - delta_logp)`. This provides a smooth, non-saturating penalty.\n\nNew coupling ideas:\n1.  **Bounded Margin via `tanh`**: The margin is calculated using `tanh` on the cost difference, similar to Parent 1. This ensures the target separation required from the model is bounded, preventing extremely large cost differences from creating excessively large loss values and gradients, which enhances numerical stability.\n2.  **Rank-Gap Scaling**: A new coupling idea is introduced where the per-sample loss is dynamically scaled by a factor derived from the 'rank gap' of the model's own predictions. We calculate `rank_gap(delta_logp)`, which maps the batch-wise rank of `delta_logp` to a value in [0, 1]. This value is then transformed so that samples where the model is most 'wrong' (i.e., `delta_logp` is low or negative, having a low rank) receive the highest weight. This focuses the training gradient on the most challenging preference pairs in each batch, acting as a form of self-adjusting hard-example mining.", "pseudocode": "1. For each pair (winner, loser), calculate the cost difference `delta_cost = cost(loser) - cost(winner)` and the log probability difference `delta_logp = logp(winner) - logp(loser)`.\n2. Inherit from Parent 1: Calculate a bounded, adaptive margin by applying `tanh` to the scaled cost difference: `margin = alpha * tanh(delta_cost / temp_cost)`. This prevents the margin from growing uncontrollably.\n3. Inherit from Parent 0: Calculate the base per-sample loss using a smooth hinge formulation: `base_loss = softplus(margin - delta_logp)`.\n4. New Coupling 1 (Rank-Gap Scaling): Compute a dynamic, per-sample weight based on the model's own predictions. First, calculate the rank gap of `delta_logp` across the batch: `rank = rank_gap(delta_logp)`. This gives a value in [0, 1] where higher `delta_logp` gets a higher rank.\n5. New Coupling 2 (Weight Transformation): Transform this rank into a focusing weight: `focus_weight = (1.0 - rank)`. This assigns higher weights to samples where the model is less confident or incorrect (low `delta_logp` -> low rank -> high weight).\n6. Apply the focusing weight to the base loss: `final_loss = focus_weight * base_loss`.\n7. Compute the weighted mean of `final_loss` over the batch.", "hyperparams": {"alpha": 1.5, "temp_cost": 1.0}, "operators_used": ["softplus", "tanh", "rank_gap"], "implementation_hint": {"expects": ["cost_a", "cost_b", "log_prob_w", "log_prob_l", "weight"], "returns": "scalar"}, "code": "import torch\nimport torch.nn.functional as F\n\ndef _rank_gap(x):\n    \"\"\"Computes the rank of each element in x and normalizes it to [0, 1].\"\"\"\n    if x.numel() == 0:\n        return x\n    # stable_sort=True is not available in all PyTorch versions, but is good practice.\n    # For this implementation, we proceed without it to maintain broader compatibility.\n    ranks = x.argsort().argsort().to(x.dtype)\n    # Normalize ranks to [0, 1]\n    # Add a small epsilon to the denominator to avoid division by zero if batch size is 1.\n    return ranks / (x.numel() - 1).clamp(min=1.0)\n\ndef generated_loss(batch, model_output, extra):\n    \"\"\"\n    Computes the Rank-Gap Scaled Hinge Loss.\n\n    This loss uses a tanh-based adaptive margin (from Parent 1) within a\n    softplus hinge loss structure (from Parent 0). It introduces a new coupling\n    where each sample's loss is weighted by the inverse rank of the model's\n    predicted log-probability difference, effectively focusing on harder examples.\n\n    Args:\n        batch (dict): A dictionary containing tensors from the dataloader.\n                      Expected keys:\n                      'cost_a', 'cost_b': Costs of the two solutions in a pair. Shape: [N].\n                      'log_prob_w', 'log_prob_l': Model's log probabilities for the\n                                                winner and loser solutions. Shape: [N].\n                      'weight' (optional): Per-sample loss weights. Shape: [N].\n        model_output: Not used in this loss implementation.\n        extra (dict): A dictionary for hyperparameters. Expected keys:\n                      'alpha' (float): Scaling factor for the adaptive margin.\n                      'temp_cost' (float): Temperature for normalizing the cost difference.\n\n    Returns:\n        torch.Tensor: A scalar tensor representing the mean loss over the batch.\n    \"\"\"\n    # For preference learning, cost_w < cost_l\n    cost_w = torch.min(batch['cost_a'], batch['cost_b'])\n    cost_l = torch.max(batch['cost_a'], batch['cost_b'])\n\n    log_prob_w = batch['log_prob_w']\n    log_prob_l = batch['log_prob_l']\n\n    alpha = extra.get('alpha', 1.5)\n    temp_cost = extra.get('temp_cost', 1.0).clamp(min=1e-8)\n\n    # 1. Calculate cost and log probability differences\n    delta_cost = cost_l - cost_w\n    delta_logp = log_prob_w - log_prob_l\n\n    # 2. (Inherited from Parent 1) Calculate a bounded adaptive margin using tanh.\n    # This prevents extreme gradients from very large cost differences.\n    cost_signal = torch.tanh(delta_cost / temp_cost)\n    adaptive_margin = alpha * cost_signal\n\n    # 3. (Inherited from Parent 0) Calculate the base loss using a softplus hinge.\n    base_loss = F.softplus(adaptive_margin - delta_logp)\n\n    # 4. (New Coupling) Compute a dynamic weight based on the rank of model's prediction.\n    # Detach delta_logp to ensure gradients don't flow through the ranking process itself.\n    with torch.no_grad():\n        # rank_gap maps the batch-wise rank of delta_logp to [0, 1].\n        # A low delta_logp (model is wrong) gets a low rank.\n        logp_rank = _rank_gap(delta_logp)\n        # We want to up-weight the items the model gets wrong, so we invert the rank.\n        # Low rank (wrong) -> high weight. High rank (correct) -> low weight.\n        focus_weight = (1.0 - logp_rank)\n\n    # 5. Apply the dynamic focus weight to the loss.\n    weighted_loss = focus_weight * base_loss\n\n    # 6. Apply optional external weights and compute the mean.\n    # The final weights are a combination of the external weights and our dynamic focus weights.\n    external_weights = batch.get('weight', None)\n    if external_weights is not None:\n        weighted_loss = weighted_loss * external_weights\n        return weighted_loss.sum() / external_weights.sum().clamp(min=1e-8)\n    else:\n        return weighted_loss.mean()"}, "static_ok": true, "static_reason": "", "dynamic_ok": false, "dynamic_reason": "forward_error: 'float' object has no attribute 'clamp'", "loss_value": null, "grad_norm": null}
{"generation": 3, "index": 7, "ir": {"name": "Rank-Normalized Bradley-Terry Loss", "intuition": "This loss function combines ideas from both parents while introducing a rank-based normalization scheme for stability and robustness. It inherits the core Bradley-Terry-like structure from the first parent, where the loss is based on `logsigmoid` of the difference between model preference and a target margin. It also inherits the concept of an adaptive margin from both parents, where the target preference gap scales with the ground truth cost difference. The first new coupling idea is to normalize the cost difference using a `rank_gap` function instead of `softplus` or `tanh`. This `rank_gap` normalization maps cost differences to a stable [-1, 1] range based on their rank within the batch, making the loss less sensitive to the absolute scale of costs and outliers. The second new coupling idea is a dynamic beta schedule. Instead of a fixed `beta`, its value is annealed from a low starting value to a higher target value over training. This allows the model to learn coarse preferences early on (low beta, weak penalty) and fine-tune its discrimination for smaller cost differences later in training (high beta, strong penalty), which can improve training stability.", "pseudocode": "1. For each pair (winner, loser), calculate the log probability difference `delta_logp = logp_w - logp_l` and the cost difference `delta_cost = cost_l - cost_w`.\n2. Inherited Idea (Parent 1 & 2): An adaptive margin will be used. The target preference gap should depend on the cost difference.\n3. New Coupling Idea 1 (Rank Normalization): Normalize the `delta_cost` values across the batch using a `rank_gap` function. This maps the cost differences to a robust [-1, 1] range based on their relative ranks, creating a `cost_signal` that is insensitive to outliers.\n4. Inherited Idea (Parent 1): Formulate the loss using a `logsigmoid` function, which is a common basis for binary preference tasks like Bradley-Terry models. The core term will be `delta_logp - margin`.\n5. New Coupling Idea 2 (Beta Annealing): Introduce a temperature parameter `beta` that scales the entire loss term inside the `logsigmoid`. This `beta` is not fixed but is annealed during training (e.g., linearly) from `beta_start` to `beta_end`. This allows the model to focus on easy examples first and harder ones later.\n6. Combine these ideas: The loss for a single pair is `-logsigmoid(beta * (delta_logp - alpha * cost_signal))`. The `alpha` hyperparameter scales the rank-normalized margin.\n7. The final loss is the weighted mean of these individual losses over the batch.", "hyperparams": {"alpha": 1.0, "beta_start": 0.1, "beta_end": 1.0}, "operators_used": ["logsigmoid", "rank_gap"], "implementation_hint": {"expects": ["cost_a", "cost_b", "logp_w", "logp_l", "weight"], "returns": "scalar"}, "code": "import torch\nimport torch.nn.functional as F\n\ndef rank_gap(x):\n    \"\"\"Normalizes a tensor based on its rank to the range [-1, 1].\"\"\"\n    if x.numel() <= 1:\n        return torch.zeros_like(x)\n    \n    # argsort twice to get the rank of each element\n    ranks = x.argsort().argsort().float()\n    # Normalize ranks to [0, 1]\n    normalized_ranks = ranks / (x.numel() - 1)\n    # Scale to [-1, 1]\n    return 2.0 * normalized_ranks - 1.0\n\ndef generated_loss(batch, model_output, extra):\n    \"\"\"\n    Computes the Rank-Normalized Bradley-Terry Loss.\n\n    This loss encourages the log probability of the winner to be higher than that\n    of the loser by a margin. The margin is adaptively set based on the\n    rank-normalized cost difference within the batch. A beta annealing schedule\n    is used to control the strength of the preference signal during training.\n\n    Args:\n        batch (dict): A dictionary containing tensors from the dataloader.\n                      Expected keys:\n                      'cost_a', 'cost_b': Costs of the two solutions. Shape: [N].\n                      'log_prob_w', 'log_prob_l': Model's log probabilities for\n                                                winner and loser. Shape: [N].\n                      'weight' (optional): Per-sample loss weights. Shape: [N].\n        model_output: Not used in this loss implementation.\n        extra (dict): A dictionary for hyperparameters and training state.\n                      Expected keys:\n                      'alpha' (float): Scales the rank-normalized margin.\n                      'beta_start' (float): Initial value for the beta schedule.\n                      'beta_end' (float): Final value for the beta schedule.\n                      'current_step' (int): Current training step.\n                      'total_steps' (int): Total training steps for annealing.\n\n    Returns:\n        torch.Tensor: A scalar tensor representing the mean loss over the batch.\n    \"\"\"\n    # Determine winner (w) and loser (l) from costs\n    cost_w = torch.min(batch['cost_a'], batch['cost_b'])\n    cost_l = torch.max(batch['cost_a'], batch['cost_b'])\n    log_prob_w = batch['log_prob_w']\n    log_prob_l = batch['log_prob_l']\n\n    # Retrieve hyperparameters\n    alpha = extra.get('alpha', 1.0)\n    beta_start = extra.get('beta_start', 0.1)\n    beta_end = extra.get('beta_end', 1.0)\n    current_step = extra.get('current_step', 0)\n    total_steps = extra.get('total_steps', 10000) # Default total steps if not provided\n\n    # 1. Calculate cost and log probability differences\n    delta_cost = cost_l - cost_w\n    delta_logp = log_prob_w - log_prob_l\n    \n    # 2. New Coupling Idea 1: Rank-based normalization for the cost signal\n    # This makes the margin robust to the scale of costs and outliers.\n    with torch.no_grad(): # Ranks should not contribute to gradients\n        cost_signal = rank_gap(delta_cost)\n    \n    # Inherited Idea: The target margin scales with the cost signal.\n    adaptive_margin = alpha * cost_signal\n    \n    # 3. New Coupling Idea 2: Beta annealing schedule\n    # Linearly anneal beta from beta_start to beta_end.\n    progress = min(1.0, current_step / total_steps)\n    beta = beta_start + (beta_end - beta_start) * progress\n\n    # 4. Inherited Idea: Bradley-Terry / logsigmoid formulation\n    # The loss is the negative log-likelihood of preferring the winner.\n    # The beta term controls the confidence of the preference.\n    loss = -F.logsigmoid(beta * (delta_logp - adaptive_margin))\n\n    # 5. Apply optional weights and compute the mean.\n    weights = batch.get('weight', None)\n    if weights is not None:\n        loss = loss * weights\n        return loss.sum() / weights.sum().clamp(min=1e-8)\n    else:\n        return loss.mean()"}, "static_ok": true, "static_reason": "", "dynamic_ok": false, "dynamic_reason": "forward_error: name 'rank_gap' is not defined", "loss_value": null, "grad_norm": null}
{"generation": 4, "index": 0, "ir": {"name": "Rank-Gap Scaled Hinge Loss", "intuition": "This loss function combines the adaptive margin concept from both parents with a new rank-based normalization scheme for stability and dynamic scaling. \n\nInherited from Parent 0 and 1:\n- The core loss structure `softplus(margin - delta_logp)` is inherited from both parents. This creates a smooth hinge loss that penalizes the model when its preference `delta_logp` is smaller than a target margin.\n- The idea of an adaptive margin, where the target separation between winner and loser log-probabilities scales with the cost difference, is also inherited from both parents.\n\nNew Coupling Ideas:\n1.  **Rank-Gap Normalization**: Instead of using a static `temp_cost` to normalize the cost difference `delta_cost`, we introduce a dynamic normalization factor. This factor is calculated as the gap between the `rank_p` percentile and `(100 - rank_p)` percentile of the `delta_cost` distribution within the current batch. This makes the margin's sensitivity robust to variations in the scale of costs across different batches and datasets. If the batch contains mostly similar pairs, the normalization factor will be small, making the margin sensitive to small cost differences. If the batch contains a wide range of cost differences, the factor will be larger, preventing the margin from exploding for outliers.\n2.  **Sigmoid Cost Signal**: The normalized cost difference is passed through a `sigmoid` function, inheriting the bounded nature of Parent 1's `tanh` signal but providing a slightly different gradient profile. This ensures the margin target is stable and bounded between 0 and `alpha`.", "pseudocode": "1. For each pair (winner, loser), calculate the cost difference `delta_cost = cost_loser - cost_winner` and the log probability difference `delta_logp = logp_winner - logp_loser`.\n2. In the current batch, calculate the `rank_p` percentile and the `(100 - rank_p)` percentile of all `delta_cost` values.\n3. Compute the `rank_gap` as the difference between these two percentile values. Add a small epsilon for stability to prevent division by zero.\n4. Normalize the `delta_cost` for each pair using this batch-specific `rank_gap`: `normalized_delta_cost = delta_cost / rank_gap`.\n5. Transform the normalized cost difference into a bounded 'cost signal' using a sigmoid function: `cost_signal = sigmoid(normalized_delta_cost)`.\n6. Create the adaptive margin by scaling the cost signal: `margin_target = alpha * cost_signal`.\n7. The final loss for each pair is the softplus of the margin minus the model's preference: `loss = softplus(margin_target - delta_logp)`.\n8. Compute the weighted mean of these individual losses over the batch.", "hyperparams": {"alpha": 2.0, "rank_p": 90.0}, "operators_used": ["softplus", "sigmoid", "rank_gap"], "implementation_hint": {"expects": ["cost_a", "cost_b", "log_prob_w", "log_prob_l", "weight"], "returns": "scalar"}, "code": "import torch\nimport torch.nn.functional as F\n\ndef generated_loss(batch, model_output, extra):\n    \"\"\"\n    Computes the Rank-Gap Scaled Hinge Loss.\n\n    This loss encourages the log probability of the winner to be higher than the loser's\n    by a margin that adapts to the cost difference. The cost difference is dynamically\n    normalized within each batch using a percentile-based rank gap, making the loss\n    robust to varying cost scales.\n\n    Args:\n        batch (dict): A dictionary containing tensors from the dataloader.\n                      Expected keys:\n                      'cost_a', 'cost_b': Costs of the two solutions. Shape: [N].\n                      'log_prob_w', 'log_prob_l': Model's log probabilities for the\n                                                winner and loser. Shape: [N].\n                      'weight' (optional): Per-sample loss weights. Shape: [N].\n        model_output: Not used in this loss implementation.\n        extra (dict): A dictionary for hyperparameters. Expected keys:\n                      'alpha' (float): The maximum margin scaling factor.\n                      'rank_p' (float): The percentile (e.g., 90.0) used to calculate the rank gap for normalization.\n\n    Returns:\n        torch.Tensor: A scalar tensor representing the mean loss over the batch.\n    \"\"\"\n    # Assume cost_w < cost_l for winner (w) and loser (l).\n    cost_w = torch.min(batch['cost_a'], batch['cost_b'])\n    cost_l = torch.max(batch['cost_a'], batch['cost_b'])\n\n    log_prob_w = batch['log_prob_w']\n    log_prob_l = batch['log_prob_l']\n\n    # Retrieve hyperparameters\n    alpha = extra.get('alpha', 2.0)\n    rank_p = extra.get('rank_p', 90.0)\n    epsilon = 1e-6\n\n    # 1. Calculate cost and log probability differences\n    delta_cost = cost_l - cost_w\n    delta_logp = log_prob_w - log_prob_l\n\n    # Detach delta_cost for percentile calculation to prevent gradients from flowing through the normalization factor.\n    delta_cost_detached = delta_cost.detach()\n\n    # 2 & 3. Compute rank-gap for dynamic, batch-wise normalization.\n    # This makes the loss robust to the overall scale of costs in a batch.\n    if delta_cost_detached.numel() > 1:\n        # Convert rank_p to a quantile q in [0, 1]\n        q_high = rank_p / 100.0\n        q_low = 1.0 - q_high\n        \n        # Use torch.quantile, which is differentiable if needed, but we detach.\n        high_percentile = torch.quantile(delta_cost_detached, q_high, interpolation='linear')\n        low_percentile = torch.quantile(delta_cost_detached, q_low, interpolation='linear')\n        \n        rank_gap = high_percentile - low_percentile\n    else:\n        # Handle single-element batch case\n        rank_gap = torch.tensor(1.0, device=delta_cost.device)\n\n    # Add epsilon for numerical stability, especially if all delta_costs are the same.\n    stable_rank_gap = rank_gap.clamp(min=epsilon)\n\n    # 4. Normalize delta_cost\n    normalized_delta_cost = delta_cost / stable_rank_gap\n\n    # 5. Transform into a bounded cost signal using sigmoid\n    cost_signal = torch.sigmoid(normalized_delta_cost)\n\n    # 6. Create the adaptive margin\n    adaptive_margin = alpha * cost_signal\n\n    # 7. Compute the core loss using softplus (smooth hinge loss)\n    loss = F.softplus(adaptive_margin - delta_logp)\n\n    # 8. Apply optional weights and compute the mean.\n    weights = batch.get('weight', None)\n    if weights is not None:\n        loss = loss * weights\n        return loss.sum() / weights.sum().clamp(min=epsilon)\n    else:\n        return loss.mean()"}, "static_ok": true, "static_reason": "", "dynamic_ok": true, "dynamic_reason": "ok", "loss_value": 2.1269280910491943, "grad_norm": 0.0}
{"generation": 4, "index": 1, "ir": {"name": "Rank-Gap Scaled LogSigmoid Loss", "intuition": "This loss function combines the core idea of an adaptive margin from both parents with a new rank-based scaling mechanism for improved stability and focus on relative preference ordering. \n\nInherited Ideas:\n- From both parents, it inherits the fundamental structure of an adaptive margin, where the target separation between log probabilities (`delta_logp`) scales with the ground-truth cost difference (`delta_cost`).\n- From Parent 1 (tanh version), it inherits the use of `tanh` to create a bounded and stable 'cost signal' from `delta_cost`, preventing extreme cost differences from causing exploding margins and gradients.\n\nNew Coupling Ideas:\n1.  **LogSigmoid Core**: Instead of the `softplus(margin - delta_logp)` structure, this loss uses the Bradley-Terry-like form `-logsigmoid(scaled_delta_logp - margin)`. This frames the problem as maximizing the log-likelihood of the preference, which is a common and effective approach.\n2.  **Rank-Gap Scaling**: The model's preference signal, `delta_logp`, is normalized using a `rank_gap` function before being used in the loss. This new coupling stabilizes training by transforming the raw `delta_logp` values, which can have arbitrary scale, into a well-behaved range based on their batch-wise ranks. It focuses the loss on the relative ordering of preferences within the batch rather than their absolute magnitudes, making the training process less sensitive to outliers in model logits.", "pseudocode": "1. For each pair (a, b) where cost(a) < cost(b), calculate the cost difference `delta_cost = cost(b) - cost(a)` and the log probability difference `delta_logp = logp(a) - logp(b)`.\n2. (Inherited from Parent 1) Create a bounded, adaptive margin from the cost difference. First, compute a cost signal using `tanh`: `cost_signal = tanh(delta_cost / temp_cost)`. Then, scale it: `margin = alpha * cost_signal`.\n3. (New Coupling) Normalize the model's log probability differences across the batch using the `rank_gap` operator. This transforms `delta_logp` into a stable, rank-based representation, `scaled_delta_logp`.\n4. (New Coupling) Compute the loss using a logsigmoid formulation, which is common in preference learning. The loss for each pair is `-logsigmoid(scaled_delta_logp - margin)`. This penalizes cases where the rank-normalized preference is smaller than the target margin.\n5. The final loss is the weighted mean of these individual losses over the batch.", "hyperparams": {"alpha": 1.5, "temp_cost": 1.0}, "operators_used": ["logsigmoid", "tanh", "rank_gap"], "implementation_hint": {"expects": ["cost_a", "cost_b", "logp_w", "logp_l", "weight"], "returns": "scalar"}, "code": "import torch\nimport torch.nn.functional as F\n\ndef rank_gap(x):\n    \"\"\"Transforms a 1D tensor to a stable range based on its rank.\n    The output is in [-1, 1]. The gap between consecutive sorted elements is constant.\n    \"\"\"\n    if x.numel() <= 1:\n        return torch.zeros_like(x)\n    \n    # Get the sorted indices\n    _, sorted_indices = torch.sort(x)\n    \n    # Create a tensor to store the ranks\n    ranks = torch.empty_like(x, dtype=x.dtype)\n    \n    # Assign ranks based on sorted position. The `scatter_` operation\n    # performs `ranks[sorted_indices] = torch.arange(...)`.\n    ranks.scatter_(0, sorted_indices, torch.arange(x.numel(), device=x.device, dtype=x.dtype))\n    \n    # Normalize ranks to be in [-1, 1]\n    # For N elements, ranks are 0, 1, ..., N-1\n    # We map them to -1, -1 + 2/(N-1), ..., 1\n    # If N=1, denominator is 0, handled by the initial check.\n    n_minus_1 = x.numel() - 1\n    return (2.0 * ranks / n_minus_1) - 1.0 if n_minus_1 > 0 else torch.zeros_like(x)\n\ndef generated_loss(batch, model_output, extra):\n    \"\"\"\n    Computes the Rank-Gap Scaled LogSigmoid Loss.\n\n    This loss encourages the model's preference to exceed an adaptive margin.\n    The margin is derived from the tanh of the cost difference (from Parent 1).\n    The model's log-probability difference is stabilized using a novel rank_gap\n    normalization before being compared against the margin within a logsigmoid function.\n\n    Args:\n        batch (dict): A dictionary containing tensors from the dataloader.\n                      Expected keys:\n                      'cost_a', 'cost_b': Costs of the two solutions. Shape: [N].\n                      'log_prob_w', 'log_prob_l': Log probabilities for winner/loser. Shape: [N].\n                      'weight' (optional): Per-sample loss weights. Shape: [N].\n        model_output: Not used in this loss implementation.\n        extra (dict): A dictionary for hyperparameters. Expected keys:\n                      'alpha' (float): Scaling factor for the adaptive margin.\n                      'temp_cost' (float): Temperature for normalizing the cost difference.\n\n    Returns:\n        torch.Tensor: A scalar tensor representing the mean loss over the batch.\n    \"\"\"\n    # Assume cost_w < cost_l is established by the data loader.\n    cost_w = torch.min(batch['cost_a'], batch['cost_b'])\n    cost_l = torch.max(batch['cost_a'], batch['cost_b'])\n    log_prob_w = batch['log_prob_w']\n    log_prob_l = batch['log_prob_l']\n\n    # Retrieve hyperparameters\n    alpha = extra.get('alpha', 1.5)\n    temp_cost = extra.get('temp_cost', 1.0)\n\n    # 1. Calculate cost and log probability differences\n    delta_cost = cost_l - cost_w\n    delta_logp = log_prob_w - log_prob_l\n\n    # 2. (Inherited from Parent 1) Create a bounded, adaptive margin from the cost difference.\n    # tanh ensures the signal is bounded in [0, 1), providing stability.\n    cost_signal = torch.tanh(delta_cost / temp_cost.clamp(min=1e-8))\n    adaptive_margin = alpha * cost_signal\n\n    # 3. (New Coupling) Normalize the model's preference signal using rank_gap.\n    # This stabilizes the loss by focusing on relative preference strength within the batch.\n    scaled_delta_logp = rank_gap(delta_logp)\n\n    # 4. (New Coupling) Compute the loss using a logsigmoid formulation.\n    # The loss is -log P(w > l), where P is modeled by a sigmoid.\n    # This encourages scaled_delta_logp to be greater than the adaptive_margin.\n    loss = -F.logsigmoid(scaled_delta_logp - adaptive_margin)\n\n    # 5. Apply optional weights and compute the mean.\n    weights = batch.get('weight', None)\n    if weights is not None:\n        loss = loss * weights\n        return loss.sum() / weights.sum().clamp(min=1e-8)\n    else:\n        return loss.mean()"}, "static_ok": true, "static_reason": "", "dynamic_ok": false, "dynamic_reason": "forward_error: 'float' object has no attribute 'clamp'", "loss_value": null, "grad_norm": null}
{"generation": 4, "index": 2, "ir": {"name": "Rank-Gap Stabilized Softplus Loss", "intuition": "This loss function synthesizes the adaptive margin concept from its parents while introducing a new stabilization and normalization mechanism. \n\nInherited ideas:\n- From both parents, it inherits the core structure of `softplus(margin - delta_logp)`, which creates a smooth, non-negative penalty when the model's preference (`delta_logp`) falls short of a target margin.\n- From Parent 0, it inherits the use of `softplus` on the cost difference (`delta_cost`) to create an unbounded, non-saturating signal. This ensures that larger cost differences translate into proportionally larger margins, pushing the model harder to distinguish clearly superior solutions.\n\nNew coupling ideas:\n1. **Rank-Gap Normalization:** Instead of normalizing the cost difference directly with a temperature parameter, we use the `rank_gap` operator on the batch of cost differences. This normalizes the `delta_cost` values based on their relative ranks within the current batch. This makes the loss less sensitive to the absolute scale of costs and more robust to outliers, focusing on the relative ordering of preference strengths.\n2. **Log-Probability Z-Score:** To stabilize training and prevent extreme log-probability values from dominating the loss, we apply a z-score normalization to the log-probability difference (`delta_logp`) across the batch. This centers the model's preference signal around zero with a standard deviation of one, ensuring the gradient contributions from this term are well-behaved and of a consistent magnitude from batch to batch.", "pseudocode": "1. For each pair (winner, loser), calculate the cost difference `delta_cost = cost_loser - cost_winner` and the log probability difference `delta_logp = logp_winner - logp_loser`.\n2. Apply z-score normalization to the `delta_logp` values across the entire batch to get a stabilized preference signal, `delta_logp_stable`.\n3. Normalize the `delta_cost` values across the batch using the `rank_gap` operator to get `delta_cost_ranked`. This converts absolute cost differences into a robust, rank-based signal.\n4. Calculate an adaptive margin using the `softplus` of the ranked cost difference, scaled by a hyperparameter `alpha`: `margin = alpha * softplus(delta_cost_ranked)`.\n5. The per-sample loss is the softplus of the difference between the margin and the stabilized model preference: `loss = softplus(margin - delta_logp_stable)`.\n6. The final loss is the weighted mean of these individual losses over the batch.", "hyperparams": {"alpha": 1.5, "eps": 1e-08}, "operators_used": ["softplus", "zscore", "rank_gap"], "implementation_hint": {"expects": ["cost_a", "cost_b", "log_prob_w", "log_prob_l", "weight"], "returns": "scalar"}, "code": "import torch\nimport torch.nn.functional as F\n\n# Helper functions for the custom operators\ndef zscore(x, eps=1e-8):\n    \"\"\"Calculates z-score for a tensor.\"\"\"\n    if x.numel() <= 1:\n        return x # Cannot compute mean/std for a single element\n    mean = torch.mean(x)\n    std = torch.std(x)\n    return (x - mean) / (std + eps)\n\ndef rank_gap(x):\n    \"\"\"Calculates the rank gap for a tensor.\"\"\"\n    if x.numel() <= 1:\n        return x # Cannot compute rank for a single element\n    # Create a tensor of ranks [0, 1, ..., N-1]\n    ranks = torch.arange(x.shape[0], device=x.device, dtype=x.dtype)\n    # Get the indices that would sort the original tensor\n    sorted_indices = torch.argsort(x)\n    # Create a new tensor to store the ranks\n    ranked_x = torch.zeros_like(x)\n    # Assign ranks to the new tensor based on the sorted order\n    ranked_x[sorted_indices] = ranks\n    # Normalize ranks to be in the range [0, 1]\n    return ranked_x / (x.shape[0] - 1).clamp(min=1.0)\n\ndef generated_loss(batch, model_output, extra):\n    \"\"\"\n    Computes the Rank-Gap Stabilized Softplus Loss.\n\n    This loss uses a softplus-based adaptive margin (from Parent 0) where the margin\n    is determined by the rank-gap normalized cost difference. It also stabilizes the\n    model's log-probability difference using z-score normalization before comparison.\n\n    Args:\n        batch (dict): A dictionary containing tensors from the dataloader.\n                      Expected keys:\n                      'cost_a', 'cost_b': Costs of the two solutions in a pair. Shape: [N].\n                      'log_prob_w', 'log_prob_l': Model's log probabilities for the\n                                                winner and loser. Shape: [N].\n                      'weight' (optional): Per-sample loss weights. Shape: [N].\n        model_output: Not used in this loss implementation.\n        extra (dict): A dictionary for hyperparameters. Expected keys:\n                      'alpha' (float): Scaling factor for the adaptive margin.\n                      'eps' (float): Small epsilon for numerical stability in z-score.\n\n    Returns:\n        torch.Tensor: A scalar tensor representing the mean loss over the batch.\n    \"\"\"\n    # Identify winner (w) and loser (l) costs and log probabilities\n    cost_w = torch.min(batch['cost_a'], batch['cost_b'])\n    cost_l = torch.max(batch['cost_a'], batch['cost_b'])\n    log_prob_w = batch['log_prob_w']\n    log_prob_l = batch['log_prob_l']\n\n    # Retrieve hyperparameters\n    alpha = extra.get('alpha', 1.5)\n    eps = extra.get('eps', 1e-8)\n\n    # 1. Calculate raw differences\n    delta_cost = cost_l - cost_w\n    delta_logp = log_prob_w - log_prob_l\n\n    # 2. Apply z-score normalization to delta_logp for stability (New Coupling Idea)\n    # This stabilizes the model's output signal across the batch.\n    delta_logp_stable = zscore(delta_logp, eps=eps)\n\n    # 3. Normalize delta_cost using rank_gap (New Coupling Idea)\n    # This makes the cost signal robust to scale and outliers.\n    delta_cost_ranked = rank_gap(delta_cost)\n\n    # 4. Calculate an adaptive margin using softplus on the ranked cost difference (Inherited Idea)\n    # This creates an unbounded, non-saturating margin based on relative cost difference.\n    margin = alpha * F.softplus(delta_cost_ranked)\n\n    # 5. Compute the core loss using the softplus hinge structure (Inherited Idea)\n    loss_per_sample = F.softplus(margin - delta_logp_stable)\n\n    # 6. Apply optional weights and compute the mean.\n    weights = batch.get('weight', None)\n    if weights is not None:\n        loss = loss_per_sample * weights\n        return loss.sum() / weights.sum().clamp(min=1e-8)\n    else:\n        return loss_per_sample.mean()"}, "static_ok": true, "static_reason": "", "dynamic_ok": false, "dynamic_reason": "forward_error: name 'zscore' is not defined", "loss_value": null, "grad_norm": null}
{"generation": 4, "index": 3, "ir": {"name": "Rank-Gap Stabilized Logistic Loss", "intuition": "This loss function combines the core logistic loss structure with an adaptive margin that is sensitive to the relative ranking of cost differences within a batch. \n\nInherited ideas:\n- From 'Softplus Adaptive Margin Loss' (both parents), it inherits the fundamental structure of an adaptive margin loss: `loss = loss_fn(margin - delta_logp)`, where the margin depends on the cost difference `delta_cost`. It also inherits the use of a temperature parameter `temp_cost` to scale the cost difference. \n- It uses `logsigmoid` as its core loss component, which is a common and stable choice for binary preference tasks, similar in spirit to the `softplus` used by the parents.\n\nNew coupling ideas:\n1. **Rank-Gap Normalization:** Instead of directly using the raw or `tanh`-transformed cost difference, the margin is based on the `rank_gap` of the cost differences within the batch. The `rank_gap` operator normalizes values based on their rank, mapping them to the range [-1, 1]. This makes the loss less sensitive to the absolute magnitude of cost differences and more focused on their relative ordering, providing robustness to outliers and varying cost scales across different problems or batches. The resulting `cost_signal` is then scaled by `sigmoid` to be in [0, 1].\n2. **Log-Prob Z-Scoring:** To improve numerical stability and prevent extreme log-probability values from dominating the loss, the log-probability difference `delta_logp` is z-score normalized across the batch. This centers the distribution of log-prob differences around zero with a standard deviation of one, ensuring that the gradient magnitudes are well-behaved and the learning process is more stable, especially during early training phases.", "pseudocode": "1. For each pair (a, b) where cost(a) < cost(b), calculate the cost difference `delta_cost = cost(b) - cost(a)` and the log probability difference `delta_logp = logp(a) - logp(b)` for the entire batch.\n2. Apply z-score normalization to the batch of `delta_logp` values to get `delta_logp_norm`. This stabilizes the log-probability inputs to the loss function.\n3. Compute a 'cost signal' based on the relative ranking of cost differences. First, apply the `rank_gap` operator to `delta_cost / temp_cost` across the batch. Then, pass this rank-based signal through a `sigmoid` function to map it to a range of [0, 1].\n4. Create the adaptive margin by scaling this cost signal: `margin = alpha * cost_signal`.\n5. The loss for each pair is computed using a logistic loss formulation: `loss = -logsigmoid(delta_logp_norm - margin)`. This penalizes cases where the normalized log-prob difference is less than the rank-based margin.\n6. The final loss is the weighted mean of these individual losses over the batch.", "hyperparams": {"alpha": 1.5, "temp_cost": 1.0}, "operators_used": ["logsigmoid", "sigmoid", "zscore", "rank_gap"], "implementation_hint": {"expects": ["cost_a", "cost_b", "log_prob_w", "log_prob_l", "weight"], "returns": "scalar"}, "code": "import torch\nimport torch.nn.functional as F\n\ndef _rank_gap(x):\n    \"\"\"Computes the rank-gap normalization of a 1D tensor.\"\"\"\n    if x.numel() <= 1:\n        return torch.zeros_like(x)\n    sorted_indices = torch.argsort(x)\n    ranks = torch.empty_like(x)\n    ranks[sorted_indices] = torch.arange(x.numel(), device=x.device, dtype=x.dtype)\n    # Normalize ranks to [-1, 1]\n    return 2 * (ranks / (x.numel() - 1)) - 1\n\ndef _zscore(x):\n    \"\"\"Computes the z-score normalization of a 1D tensor.\"\"\"\n    if x.numel() <= 1:\n        return torch.zeros_like(x)\n    mean = x.mean()\n    std = x.std()\n    # Add a small epsilon for stability if standard deviation is zero\n    return (x - mean) / (std + 1e-8)\n\ndef generated_loss(batch, model_output, extra):\n    \"\"\"\n    Computes the Rank-Gap Stabilized Logistic Loss.\n\n    This loss uses a logistic (logsigmoid) framework. The target margin is adaptive,\n    determined by the rank-gap of cost differences within the batch. This makes the\n    loss robust to cost outliers. Additionally, the log-probability difference is\n    z-score normalized for improved numerical stability.\n\n    Args:\n        batch (dict): A dictionary containing tensors from the dataloader.\n                      Expected keys:\n                      'cost_a', 'cost_b': Costs of the two solutions. Shape: [N].\n                      'log_prob_w', 'log_prob_l': Model's log probabilities for the\n                                                winner and loser. Shape: [N].\n                      'weight' (optional): Per-sample loss weights. Shape: [N].\n        model_output: Not used in this loss implementation.\n        extra (dict): A dictionary for hyperparameters. Expected keys:\n                      'alpha' (float): Scaling factor for the adaptive margin.\n                      'temp_cost' (float): Temperature for scaling cost differences.\n\n    Returns:\n        torch.Tensor: A scalar tensor representing the mean loss over the batch.\n    \"\"\"\n    cost_w = torch.min(batch['cost_a'], batch['cost_b'])\n    cost_l = torch.max(batch['cost_a'], batch['cost_b'])\n    log_prob_w = batch['log_prob_w']\n    log_prob_l = batch['log_prob_l']\n\n    alpha = extra.get('alpha', 1.5)\n    temp_cost = extra.get('temp_cost', 1.0)\n\n    # 1. Calculate cost and log probability differences\n    delta_cost = cost_l - cost_w\n    delta_logp = log_prob_w - log_prob_l\n\n    # 2. (New Coupling) Apply z-score normalization to delta_logp for stability\n    delta_logp_norm = _zscore(delta_logp)\n\n    # 3. (New Coupling) Compute a rank-based cost signal\n    # Scale costs by temperature before ranking\n    scaled_delta_cost = delta_cost / temp_cost\n    # rank_gap maps values to [-1, 1] based on their sorted order\n    ranked_signal = _rank_gap(scaled_delta_cost)\n    # sigmoid maps the rank signal to [0, 1] to serve as a margin weight\n    cost_signal = torch.sigmoid(ranked_signal)\n\n    # 4. (Inherited) Create an adaptive margin from the cost signal\n    adaptive_margin = alpha * cost_signal\n\n    # 5. (Inherited) Compute the core loss using a logistic formulation\n    # We want delta_logp_norm > adaptive_margin, so the argument to logsigmoid\n    # should be positive. loss = -logsigmoid(model_score - target_score)\n    loss = -F.logsigmoid(delta_logp_norm - adaptive_margin)\n\n    # 6. Apply optional weights and compute the mean\n    weights = batch.get('weight', None)\n    if weights is not None:\n        loss = loss * weights\n        return loss.sum() / weights.sum().clamp(min=1e-8)\n    else:\n        return loss.mean()"}, "static_ok": true, "static_reason": "", "dynamic_ok": false, "dynamic_reason": "forward_error: name '_zscore' is not defined", "loss_value": null, "grad_norm": null}
{"generation": 4, "index": 4, "ir": {"name": "Adaptive Sigmoid Margin Loss", "intuition": "This loss function synthesizes ideas from its parents to create a stable and adaptive preference learning objective. \n\nFrom its first parent, it inherits the core loss structure: `softplus(margin - delta_logp)`. This provides a smooth, non-negative penalty when the model's preference (`delta_logp`) falls short of a target margin.\n\nFrom its second parent, it inherits the idea of using a bounded function, `tanh`, to create a normalized 'cost signal' from the cost difference (`delta_cost`). This ensures that the margin remains stable and doesn't grow infinitely, even with extreme cost differences.\n\nA new coupling idea is introduced: instead of using the `tanh`-based cost signal to define a *linear* margin (e.g., `alpha * signal`), it's used to define a *dynamic beta* for a sigmoid-like loss. The loss is formulated as `softplus(beta * (-delta_logp))`. The `beta` term, `1 + alpha * tanh(delta_cost / temp_cost)`, dynamically adjusts the steepness of the loss curve. When costs are very similar (`delta_cost` is near zero), `beta` is close to 1, resulting in a standard Bradley-Terry-like loss (`-logsigmoid(delta_logp)`). When costs are very different, `beta` increases, making the loss function steeper and pushing the model much harder to separate the log probabilities. This creates a margin implicitly, where the required `delta_logp` to achieve low loss increases as `delta_cost` grows, but does so in a stable, bounded manner.", "pseudocode": "1. For each pair (winner, loser), calculate the cost difference `delta_cost = cost_loser - cost_winner` and the log probability difference `delta_logp = logp_winner - logp_loser`.\n2. Inherit from Parent 2: Create a bounded 'cost signal' between 0 and 1 by applying `tanh` to the scaled cost difference: `cost_signal = tanh(delta_cost / temp_cost)`.\n3. New Coupling Idea: Use this cost signal to compute a dynamic scaling factor `beta`. `beta = 1 + alpha * cost_signal`. This `beta` will range from 1 (for identical costs) up to `1 + alpha`.\n4. Inherit from Parent 1: Use the `softplus` function to formulate the core loss. Combine it with the dynamic beta to create the final loss term: `loss = softplus(beta * (-delta_logp))`. This is equivalent to `-logsigmoid(beta * delta_logp)` but uses the parent operator.\n5. The final loss is the weighted mean of these individual losses over the batch.", "hyperparams": {"alpha": 3.0, "temp_cost": 1.0}, "operators_used": ["softplus", "tanh"], "implementation_hint": {"expects": ["cost_a", "cost_b", "logp_w", "logp_l", "weight"], "returns": "scalar"}, "code": "import torch\nimport torch.nn.functional as F\n\ndef generated_loss(batch, model_output, extra):\n    \"\"\"\n    Computes the Adaptive Sigmoid Margin Loss.\n\n    This loss encourages the log probability of the winner to be higher than the loser's.\n    It uses a dynamic scaling factor 'beta', derived from the tanh of the cost difference,\n    to adjust the steepness of the loss landscape. When costs are very different,\n    the loss becomes steeper, implicitly creating a larger margin requirement.\n\n    Args:\n        batch (dict): A dictionary containing tensors from the dataloader.\n                      Expected keys:\n                      'cost_a', 'cost_b': Costs of the two solutions. Shape: [N].\n                      'log_prob_w', 'log_prob_l': Model's log probabilities for the\n                                                winner and loser. Shape: [N].\n                      'weight' (optional): Per-sample loss weights. Shape: [N].\n        model_output: Not used in this loss implementation.\n        extra (dict): A dictionary for hyperparameters. Expected keys:\n                      'alpha' (float): Controls the maximum steepness of the loss.\n                      'temp_cost' (float): Temperature for normalizing the cost difference.\n\n    Returns:\n        torch.Tensor: A scalar tensor representing the mean loss over the batch.\n    \"\"\"\n    # Determine winner (w) and loser (l) based on costs.\n    # cost_w < cost_l\n    cost_w = torch.min(batch['cost_a'], batch['cost_b'])\n    cost_l = torch.max(batch['cost_a'], batch['cost_b'])\n\n    # Log probabilities are assumed to be provided for the winner and loser directly.\n    log_prob_w = batch['log_prob_w']\n    log_prob_l = batch['log_prob_l']\n\n    # Retrieve hyperparameters\n    alpha = extra.get('alpha', 3.0)\n    temp_cost = extra.get('temp_cost', 1.0)\n\n    # 1. Calculate cost and log probability differences\n    delta_cost = cost_l - cost_w\n    delta_logp = log_prob_w - log_prob_l\n\n    # 2. Create a bounded 'cost signal' from the cost difference (Inherited from Parent 2).\n    # This ensures the signal is stable, ranging from [0, 1).\n    cost_signal = torch.tanh(delta_cost / temp_cost)\n\n    # 3. New Coupling: Use the cost signal to compute a dynamic scaling factor 'beta'.\n    # beta ranges from 1 to (1 + alpha), adjusting the loss steepness.\n    # When delta_cost is 0, beta=1 (standard sigmoid loss).\n    # When delta_cost is large, beta -> 1+alpha (steeper loss, larger implicit margin).\n    beta = 1.0 + alpha * cost_signal\n    \n    # Detach beta to prevent gradients from flowing through the cost signal into model parameters,\n    # which could happen if costs were model-dependent. This is a stability trick.\n    beta = beta.detach()\n\n    # 4. Compute the core loss using softplus (Inherited from Parent 1) and the dynamic beta.\n    # This is equivalent to -logsigmoid(beta * delta_logp) but uses the specified operator.\n    # It penalizes cases where delta_logp is not sufficiently positive, with the penalty\n    # scaled by how different the ground truth costs are.\n    loss = F.softplus(-beta * delta_logp)\n\n    # 5. Apply optional weights and compute the mean.\n    weights = batch.get('weight', None)\n    if weights is not None:\n        loss = loss * weights\n        return loss.sum() / weights.sum().clamp(min=1e-8)\n    else:\n        return loss.mean()"}, "static_ok": true, "static_reason": "", "dynamic_ok": true, "dynamic_reason": "ok", "loss_value": 0.6931471824645996, "grad_norm": 0.0}
{"generation": 4, "index": 5, "ir": {"name": "Z-Scored Adaptive Sigmoid Loss", "intuition": "This loss function combines the adaptive margin concept from both parents with a new normalization scheme for improved stability and robustness across different cost distributions. \n\nIt inherits the core structure of `softplus(margin - delta_logp)` from both parents, which provides a smooth, non-negative penalty. From the second parent, it inherits the use of a bounded function (`sigmoid`) to transform the cost difference into a stable signal between 0 and 1, preventing extreme cost gaps from creating excessively large loss values. \n\nAs a new coupling idea, it introduces a batch-level z-score normalization on the cost differences (`delta_cost`) *before* applying the sigmoid transformation. This makes the adaptive margin less sensitive to the absolute scale of costs in a given batch and more dependent on the relative ranking of cost differences within that batch. A `delta_cost` that is one standard deviation above the batch mean will consistently map to a similar point on the sigmoid curve, regardless of whether the costs are small or large. A second new idea is a `margin_offset`, which ensures that even when the cost difference is zero or negative (due to normalization), there is still a small, positive minimum margin, enforcing a preference for the winning solution.", "pseudocode": "1. For each pair (winner, loser), calculate the cost difference `delta_cost = cost_loser - cost_winner` and the log probability difference `delta_logp = logp_winner - logp_loser`.\n2. Apply batch-level z-score normalization to the cost differences: `delta_cost_norm = zscore(delta_cost)`. This centers the distribution of cost differences around 0 with a standard deviation of 1.\n3. Transform the normalized cost difference into a bounded 'cost signal' between 0 and 1 using the sigmoid function: `cost_signal = sigmoid(delta_cost_norm)`. This signal reflects the relative strength of the preference within the batch.\n4. Create an adaptive margin by scaling this signal and adding a small offset: `margin = alpha * cost_signal + margin_offset`. The offset ensures a minimal preference is always enforced.\n5. The loss for each pair is calculated using a smooth hinge-loss formulation: `loss = softplus(margin - delta_logp)`. This penalizes the model when its preference `delta_logp` is less than the target margin.\n6. The final loss is the weighted mean of these individual losses over the batch.", "hyperparams": {"alpha": 2.0, "margin_offset": 0.1, "zscore_eps": 1e-06}, "operators_used": ["softplus", "sigmoid", "zscore"], "implementation_hint": {"expects": ["cost_a", "cost_b", "logp_w", "logp_l", "weight"], "returns": "scalar"}, "code": "import torch\nimport torch.nn.functional as F\n\ndef zscore(x, eps=1e-6):\n    \"\"\"Batch-level z-score normalization.\"\"\"\n    # Only normalize if there's more than one element to avoid std=0\n    if x.numel() <= 1:\n        return x - x.mean()\n    mean = x.mean()\n    std = x.std().clamp(min=eps)\n    return (x - mean) / std\n\ndef generated_loss(batch, model_output, extra):\n    \"\"\"\n    Computes the Z-Scored Adaptive Sigmoid Loss.\n\n    This loss encourages the log probability of the winner to be higher than the loser's\n    by an adaptive margin. The margin is derived from the z-scored cost difference,\n    making it robust to the scale of costs in a batch.\n\n    Args:\n        batch (dict): A dictionary containing tensors from the dataloader.\n                      Expected keys:\n                      'cost_a', 'cost_b': Costs of the two solutions. Shape: [N].\n                      'log_prob_w', 'log_prob_l': Log probabilities for winner/loser. Shape: [N].\n                      'weight' (optional): Per-sample loss weights. Shape: [N].\n        model_output: Not used in this loss implementation.\n        extra (dict): A dictionary for hyperparameters. Expected keys:\n                      'alpha' (float): Scaling factor for the adaptive margin.\n                      'margin_offset' (float): A small positive offset for the margin.\n                      'zscore_eps' (float): Epsilon for z-score std to prevent division by zero.\n\n    Returns:\n        torch.Tensor: A scalar tensor representing the mean loss over the batch.\n    \"\"\"\n    # Identify winner (w) and loser (l) based on costs\n    cost_w = torch.min(batch['cost_a'], batch['cost_b'])\n    cost_l = torch.max(batch['cost_a'], batch['cost_b'])\n\n    log_prob_w = batch['log_prob_w']\n    log_prob_l = batch['log_prob_l']\n\n    # Retrieve hyperparameters\n    alpha = extra.get('alpha', 2.0)\n    margin_offset = extra.get('margin_offset', 0.1)\n    zscore_eps = extra.get('zscore_eps', 1e-6)\n\n    # 1. Calculate cost and log probability differences\n    delta_cost = cost_l - cost_w\n    delta_logp = log_prob_w - log_prob_l\n\n    # 2. Apply batch-level z-score normalization to cost differences\n    # This makes the margin robust to the absolute scale of costs.\n    delta_cost_norm = zscore(delta_cost, eps=zscore_eps)\n\n    # 3. Transform normalized cost difference into a bounded signal [0, 1]\n    # Inherits the idea of a bounded signal from the tanh-based parent.\n    cost_signal = torch.sigmoid(delta_cost_norm)\n\n    # 4. Create an adaptive margin with a minimum offset\n    # The offset ensures a small positive margin even for pairs with average cost difference.\n    adaptive_margin = alpha * cost_signal + margin_offset\n\n    # 5. Compute the core loss using softplus\n    # This structure is inherited from both parents.\n    loss = F.softplus(adaptive_margin - delta_logp)\n\n    # 6. Apply optional weights and compute the mean.\n    weights = batch.get('weight', None)\n    if weights is not None:\n        loss = loss * weights\n        # Use a small epsilon to prevent division by zero if all weights are zero.\n        return loss.sum() / weights.sum().clamp(min=1e-8)\n    else:\n        return loss.mean()"}, "static_ok": true, "static_reason": "", "dynamic_ok": false, "dynamic_reason": "forward_error: name 'zscore' is not defined", "loss_value": null, "grad_norm": null}
{"generation": 4, "index": 6, "ir": {"name": "Rank-Gap Scaled LogSigmoid Loss", "intuition": "This loss combines a non-saturating adaptive margin from Parent 0 with a stable, bounded log-sigmoid core structure, similar in spirit to the softplus hinge loss in both parents. \n\nInherited ideas:\n- From Parent 0: We inherit the use of `softplus` on the cost difference (`softplus(delta_cost)`) to create a non-saturating, unbounded margin. This ensures that as the true cost gap grows, the target preference gap for the model also continues to grow, pushing the model to better distinguish very different solutions.\n- From both parents (implicitly, as `softplus(x)` is related to `-logsigmoid(-x)`): We use a sigmoid-based loss structure (`logsigmoid`) which is common in binary classification and preference learning for its probabilistic interpretation and smooth gradients.\n\nNew coupling ideas:\n1.  **Log-Sigmoid Core:** Instead of the `softplus(margin - delta_logp)` formulation, we use `-logsigmoid(delta_logp - margin)`. This is a common preference loss form (like DPO) that frames the problem as correctly classifying the preference. It is numerically stable and has a nice probabilistic interpretation.\n2.  **Rank-Gap Normalization:** To prevent extreme values in `delta_logp` from causing instability, especially early in training, we normalize `delta_logp` using a batch-wise `rank_gap` function. This function transforms the raw log-probability differences into a uniform distribution based on their ranks within the batch. This stabilizes the loss by bounding its inputs and makes it less sensitive to the absolute scale of log-probabilities, focusing instead on their relative ordering.", "pseudocode": "1. For each pair (a, b) where cost(a) < cost(b), calculate the cost difference `delta_cost = cost(b) - cost(a)` and the log probability difference `delta_logp = logp(a) - logp(b)`.\n2. Inherit from Parent 0: Create a non-saturating adaptive margin using the softplus of the cost difference: `margin = alpha * softplus(delta_cost / temp_cost)`.\n3. New idea 1: Normalize the model's log probability difference using a batch-wise rank-gap transformation. This maps `delta_logp` values to a stable range (e.g., [-1, 1]) based on their rank within the batch: `normalized_delta_logp = rank_gap(delta_logp)`.\n4. New idea 2: Combine the margin and normalized preference using a log-sigmoid loss core. The loss is `-logsigmoid(normalized_delta_logp - margin)`. This penalizes cases where the normalized preference is smaller than the target margin.\n5. The final loss is the weighted mean of these individual losses over the batch.", "hyperparams": {"alpha": 1.0, "temp_cost": 1.0}, "operators_used": ["logsigmoid", "softplus", "rank_gap"], "implementation_hint": {"expects": ["cost_a", "cost_b", "logp_w", "logp_l", "weight"], "returns": "scalar"}, "code": "import torch\nimport torch.nn.functional as F\n\ndef _rank_gap(x):\n    \"\"\"Transforms a 1D tensor to a uniform distribution based on rank.\n\n    The output is scaled to be in the range [-1, 1]. This is a stable\n    normalization technique that is robust to outliers.\n    \"\"\"\n    if x.numel() <= 1:\n        return torch.zeros_like(x)\n    # Get the rank of each element\n    indices = x.argsort()\n    ranks = torch.empty_like(x)\n    ranks[indices] = torch.arange(len(x), device=x.device, dtype=x.dtype)\n\n    # Normalize ranks to be in [-1, 1]\n    # Subtracting 0.5 centers the distribution around 0 before scaling.\n    # N - 1 is used for the denominator to ensure the max value is 1 and min is -1.\n    n = len(x)\n    normalized_ranks = (ranks / (n - 1) - 0.5) * 2.0\n    return normalized_ranks\n\ndef generated_loss(batch, model_output, extra):\n    \"\"\"\n    Computes the Rank-Gap Scaled LogSigmoid Loss.\n\n    This loss encourages the log probability of the winner to be higher than the loser\n    by a margin that adapts to the cost difference. The margin is non-saturating,\n    scaling with softplus(delta_cost). The model's log-prob difference is stabilized\n    using a batch-wise rank-gap normalization before being compared to the margin.\n    The core loss is a stable logsigmoid formulation.\n\n    Args:\n        batch (dict): A dictionary containing tensors from the dataloader.\n                      Expected keys:\n                      'cost_a', 'cost_b': Costs of the two solutions. Shape: [N].\n                      'log_prob_w', 'log_prob_l': Log probabilities for winner/loser. Shape: [N].\n                      'weight' (optional): Per-sample loss weights. Shape: [N].\n        model_output: Not used.\n        extra (dict): Dictionary for hyperparameters. Expected keys:\n                      'alpha' (float): Scaling factor for the adaptive margin.\n                      'temp_cost' (float): Temperature for the cost difference.\n\n    Returns:\n        torch.Tensor: A scalar tensor representing the mean loss over the batch.\n    \"\"\"\n    # Assume cost_w < cost_l\n    cost_w = torch.min(batch['cost_a'], batch['cost_b'])\n    cost_l = torch.max(batch['cost_a'], batch['cost_b'])\n\n    log_prob_w = batch['log_prob_w']\n    log_prob_l = batch['log_prob_l']\n\n    alpha = extra.get('alpha', 1.0)\n    temp_cost = extra.get('temp_cost', 1.0)\n\n    # 1. Calculate cost and log probability differences\n    delta_cost = cost_l - cost_w\n    delta_logp = log_prob_w - log_prob_l\n\n    # 2. (Inherited from Parent 0) Create a non-saturating adaptive margin.\n    # This margin grows as the cost difference grows, pushing the model harder\n    # for pairs with large cost gaps.\n    cost_signal = F.softplus(delta_cost / temp_cost)\n    adaptive_margin = alpha * cost_signal\n\n    # 3. (New Idea) Normalize the model's preference signal using rank-gap.\n    # This stabilizes the loss by making it robust to the absolute scale or outliers\n    # in log-probabilities, focusing on the relative ordering within the batch.\n    normalized_delta_logp = _rank_gap(delta_logp)\n\n    # 4. (New Idea) Compute the core loss using a stable logsigmoid formulation.\n    # The loss is -log P(winner is preferred over loser), where the logit is\n    # (normalized_delta_logp - adaptive_margin).\n    # We want this logit to be large and positive.\n    loss = -F.logsigmoid(normalized_delta_logp - adaptive_margin)\n\n    # 5. Apply optional weights and compute the mean.\n    weights = batch.get('weight', None)\n    if weights is not None:\n        loss = loss * weights\n        return loss.sum() / weights.sum().clamp(min=1e-8)\n    else:\n        return loss.mean()"}, "static_ok": true, "static_reason": "", "dynamic_ok": false, "dynamic_reason": "forward_error: name '_rank_gap' is not defined", "loss_value": null, "grad_norm": null}
{"generation": 4, "index": 7, "ir": {"name": "Z-Scored Softplus-Tanh Hybrid Margin Loss", "intuition": "This loss function synthesizes ideas from its parents to create a robust, adaptive margin-based preference loss. \n\nInherited ideas:\n- From Parent 0, it inherits the use of `softplus` on the cost difference (`delta_cost`) to create an unbounded, smoothly increasing signal. This ensures that very large cost differences translate to a strong learning signal.\n- From Parent 1, it inherits the use of `tanh` on the cost difference. This provides a bounded, stable signal that prevents extremely large cost differences from creating outlier loss values.\n- From both parents, it inherits the fundamental structure of `softplus(margin - delta_logp)`, which acts as a smooth hinge loss, penalizing the model when its log probability difference (`delta_logp`) falls short of the target margin.\n\nNew coupling ideas:\n1. **Hybrid Margin:** The core innovation is to combine the `softplus` (unbounded) and `tanh` (bounded) cost signals into a single hybrid margin. This is done by adding them together. The `softplus` component dominates for large cost differences, ensuring the model is pushed harder, while the `tanh` component provides a stable base margin, especially for small to medium cost differences.\n2. **Z-Score Normalization:** To improve stability and make the hyperparameters (`alpha`, `temp_cost`) less sensitive to the specific scale of costs in a batch, the `delta_cost` is z-scored (standardized) before being used. This centers the cost differences around zero with a standard deviation of one for the batch, making the behavior of `softplus` and `tanh` more consistent across different datasets and training phases.", "pseudocode": "1. For each pair (winner, loser), calculate the cost difference `delta_cost = cost_loser - cost_winner` and the log probability difference `delta_logp = logp_winner - logp_loser`.\n2. **(New Coupling)** Apply z-score normalization to the batch of `delta_cost` values to get `normalized_delta_cost`. This stabilizes the loss by making it invariant to the scale and shift of costs in the batch.\n3. **(Inherited from Parent 0)** Calculate an unbounded cost signal using `softplus`: `unbounded_signal = softplus(normalized_delta_cost / temp_cost)`.\n4. **(Inherited from Parent 1)** Calculate a bounded cost signal using `tanh`: `bounded_signal = tanh(normalized_delta_cost / temp_cost)`.\n5. **(New Coupling)** Create a hybrid margin by combining both signals: `hybrid_margin = alpha * (unbounded_signal + bounded_signal)`.\n6. **(Inherited from both)** Compute the final loss using a smooth hinge formulation: `loss = softplus(hybrid_margin - delta_logp)`.\n7. The final loss is the weighted mean of these individual losses over the batch.", "hyperparams": {"alpha": 0.5, "temp_cost": 1.0}, "operators_used": ["softplus", "tanh", "zscore"], "implementation_hint": {"expects": ["cost_a", "cost_b", "log_prob_w", "log_prob_l", "weight"], "returns": "scalar"}, "code": "import torch\nimport torch.nn.functional as F\n\ndef _zscore(x, epsilon=1e-8):\n    \"\"\"Helper function for z-score normalization.\"\"\"\n    # Only normalize if there is more than one element to avoid std=0\n    if x.numel() <= 1:\n        return x\n    mean = x.mean()\n    std = x.std()\n    return (x - mean) / (std + epsilon)\n\ndef generated_loss(batch, model_output, extra):\n    \"\"\"\n    Computes the Z-Scored Softplus-Tanh Hybrid Margin Loss.\n\n    This loss encourages the log probability of the winner to be higher than the loser's\n    by a hybrid margin. The margin is constructed by combining a softplus-based (unbounded)\n    and a tanh-based (bounded) signal, both derived from the z-scored cost difference.\n\n    Args:\n        batch (dict): A dictionary containing tensors from the dataloader.\n                      Expected keys:\n                      'cost_a', 'cost_b': Costs of the two solutions in a pair. Shape: [N].\n                      'log_prob_w', 'log_prob_l': Model's log probabilities for the\n                                                winner and loser solutions. Shape: [N].\n                      'weight' (optional): Per-sample loss weights. Shape: [N].\n        model_output: Not used in this loss implementation.\n        extra (dict): A dictionary for hyperparameters. Expected keys:\n                      'alpha' (float): A scaling factor for the hybrid margin.\n                      'temp_cost' (float): A temperature for the cost difference signals.\n\n    Returns:\n        torch.Tensor: A scalar tensor representing the mean loss over the batch.\n    \"\"\"\n    # Define winner (w) and loser (l) based on costs.\n    # cost_w < cost_l\n    cost_w = torch.min(batch['cost_a'], batch['cost_b'])\n    cost_l = torch.max(batch['cost_a'], batch['cost_b'])\n\n    log_prob_w = batch['log_prob_w']\n    log_prob_l = batch['log_prob_l']\n\n    # Retrieve hyperparameters\n    alpha = extra.get('alpha', 0.5)\n    temp_cost = extra.get('temp_cost', 1.0)\n\n    # 1. Calculate cost and log probability differences\n    delta_cost = cost_l - cost_w\n    delta_logp = log_prob_w - log_prob_l\n\n    # 2. (New Coupling) Apply z-score normalization for stability\n    normalized_delta_cost = _zscore(delta_cost)\n\n    # Scale by temperature\n    scaled_cost_diff = normalized_delta_cost / temp_cost\n\n    # 3. (Inherited from Parent 0) Calculate unbounded signal\n    unbounded_signal = F.softplus(scaled_cost_diff)\n\n    # 4. (Inherited from Parent 1) Calculate bounded signal\n    bounded_signal = torch.tanh(scaled_cost_diff)\n\n    # 5. (New Coupling) Create a hybrid margin by combining both signals\n    hybrid_margin = alpha * (unbounded_signal + bounded_signal)\n\n    # 6. (Inherited from both) Compute the core loss using a smooth hinge formulation\n    loss = F.softplus(hybrid_margin - delta_logp)\n\n    # 7. Apply optional weights and compute the mean.\n    weights = batch.get('weight', None)\n    if weights is not None:\n        loss = loss * weights\n        return loss.sum() / weights.sum().clamp(min=1e-8)\n    else:\n        return loss.mean()"}, "static_ok": true, "static_reason": "", "dynamic_ok": false, "dynamic_reason": "forward_error: name '_zscore' is not defined", "loss_value": null, "grad_norm": null}
{"generation": 5, "index": 0, "ir": {"name": "Rank-Gap Stabilized Hinge Loss", "intuition": "This loss function synergizes the adaptive margin concept from its parents with a novel rank-based stabilization mechanism. \n\nFrom its parents, it inherits the core structure of a softplus hinge loss: `softplus(margin - delta_logp)`. This penalizes the model when its predicted log-probability difference (`delta_logp`) falls short of a target margin. It also inherits the idea of making this margin adaptive to the ground-truth cost difference, pushing the model harder to distinguish pairs with larger cost gaps.\n\nAs a new coupling idea, this loss introduces a rank-based normalization for the cost signal. Instead of using the raw cost differences, which can have an unbounded scale and be sensitive to outliers, it uses the `rank_gap` of the cost differences. This operator transforms the cost differences into a stable, zero-centered distribution based on their relative ordering within the batch. This makes the adaptive margin more robust to the specific scale of costs in a given batch. \n\nA second new idea is a stability trick that clamps the `delta_logp` before it enters the final loss calculation. This prevents extremely confident (but potentially wrong) predictions from generating huge negative values inside the `softplus`, which can lead to vanishing gradients and unstable training. The final loss is computed using `logsigmoid` for its probabilistic interpretation and excellent numerical stability, applied to the difference between the clamped log-probability gap and the rank-gap-derived margin.", "pseudocode": "1. For each pair (winner, loser), calculate the cost difference `delta_cost = cost_loser - cost_winner` and the log probability difference `delta_logp = logp_winner - logp_loser`.\n2. Inherited Idea (Parents): Use a hinge-loss structure where an adaptive margin is compared against the model's predicted `delta_logp`.\n3. New Coupling Idea 1: Instead of using `delta_cost` directly, compute the `rank_gap` of the `delta_cost` values across the entire batch. This normalizes the cost signal based on its relative rank, making it robust to outliers and scale.\n4. Inherited Idea (Parents): Create an adaptive margin proportional to this rank-gap normalized cost signal: `margin = alpha * rank_gap(delta_cost)`.\n5. New Coupling Idea 2: For stability, clamp the `delta_logp` to a reasonable range (e.g., [-10, 10]) before using it in the loss. This prevents extreme predictions from causing vanishing gradients.\n6. Compute the final loss for each sample using a stable `logsigmoid` formulation: `loss = -logsigmoid(clamped_delta_logp - margin)`. This is equivalent to `softplus(margin - clamped_delta_logp)` but is often more numerically stable.\n7. The final loss is the weighted mean of these individual losses over the batch.", "hyperparams": {"alpha": 0.5, "clamp_min": -10.0, "clamp_max": 10.0}, "operators_used": ["logsigmoid", "rank_gap", "clamp"], "implementation_hint": {"expects": ["cost_a", "cost_b", "logp_w", "logp_l", "weight"], "returns": "scalar"}, "code": "import torch\nimport torch.nn.functional as F\n\ndef rank_gap(x: torch.Tensor) -> torch.Tensor:\n    \"\"\"Normalizes a tensor by its rank, scaled to [-0.5, 0.5].\"\"\"\n    if x.numel() <= 1:\n        return torch.zeros_like(x)\n    ranks = torch.empty_like(x, dtype=torch.float)\n    ranks[x.argsort()] = torch.arange(len(x), device=x.device, dtype=torch.float)\n    ranks = (ranks / (len(x) - 1)) - 0.5\n    return ranks\n\ndef generated_loss(batch, model_output, extra):\n    \"\"\"\n    Computes the Rank-Gap Stabilized Hinge Loss.\n\n    This loss encourages the log probability of the winner to be higher than the loser's\n    by a margin that adapts to the rank of the cost difference within the batch.\n    It uses clamping for stability and a logsigmoid formulation.\n\n    Args:\n        batch (dict): A dictionary containing tensors from the dataloader.\n                      Expected keys:\n                      'cost_a', 'cost_b': Costs of the two solutions. Shape: [N].\n                      'log_prob_w', 'log_prob_l': Model's log probabilities for the\n                                                winner and loser. Shape: [N].\n                      'weight' (optional): Per-sample loss weights. Shape: [N].\n        model_output: Not used in this loss implementation.\n        extra (dict): A dictionary for hyperparameters. Expected keys:\n                      'alpha' (float): Scales the rank-based margin.\n                      'clamp_min' (float): Minimum value for clamping logp difference.\n                      'clamp_max' (float): Maximum value for clamping logp difference.\n\n    Returns:\n        torch.Tensor: A scalar tensor representing the mean loss over the batch.\n    \"\"\"\n    # Assume winner (w) has lower cost than loser (l)\n    cost_w = torch.min(batch['cost_a'], batch['cost_b'])\n    cost_l = torch.max(batch['cost_a'], batch['cost_b'])\n    log_prob_w = batch['log_prob_w']\n    log_prob_l = batch['log_prob_l']\n\n    # Retrieve hyperparameters\n    alpha = extra.get('alpha', 0.5)\n    clamp_min = extra.get('clamp_min', -10.0)\n    clamp_max = extra.get('clamp_max', 10.0)\n\n    # 1. Calculate cost and log probability differences\n    delta_cost = cost_l - cost_w\n    delta_logp = log_prob_w - log_prob_l\n\n    # 2. (New Coupling) Create a robust, rank-based cost signal\n    # This normalizes the cost differences based on their batch-wise ordering,\n    # making the margin robust to the scale of costs.\n    with torch.no_grad():\n        cost_signal = rank_gap(delta_cost)\n\n    # 3. (Inherited) Create an adaptive margin from the cost signal\n    adaptive_margin = alpha * cost_signal\n\n    # 4. (New Coupling) Clamp delta_logp for numerical stability\n    # This prevents extreme model predictions from causing vanishing gradients.\n    clamped_delta_logp = torch.clamp(delta_logp, min=clamp_min, max=clamp_max)\n\n    # 5. (Inherited) Compute the core loss using a stable logsigmoid formulation.\n    # This is equivalent to softplus(margin - x) but can be more stable.\n    # The loss is low if clamped_delta_logp > adaptive_margin.\n    loss = -F.logsigmoid(clamped_delta_logp - adaptive_margin)\n\n    # 6. Apply optional weights and compute the mean.\n    weights = batch.get('weight', None)\n    if weights is not None:\n        loss = loss * weights\n        return loss.sum() / weights.sum().clamp(min=1e-8)\n    else:\n        return loss.mean()"}, "static_ok": true, "static_reason": "", "dynamic_ok": false, "dynamic_reason": "forward_error: name 'rank_gap' is not defined", "loss_value": null, "grad_norm": null}
{"generation": 5, "index": 1, "ir": {"name": "Adaptive Margin Loss with Z-Score Normalization and Rank Gap", "intuition": "This loss function synthesizes ideas from its parents to create a robust, adaptive preference loss. \n\nInherited Ideas:\n- From both parents, it inherits the core structure of an adaptive margin loss: `loss = softplus(margin - delta_logp)`. This smooth hinge loss penalizes the model when its log-probability difference (`delta_logp`) for a preferred solution (`w`) over a less-preferred one (`l`) falls short of a target margin.\n- From one parent, it inherits the use of `tanh` to create a bounded 'cost signal' from the cost difference (`delta_cost`). This prevents extremely large cost differences from creating excessively large, unstable loss values.\n\nNew Coupling Ideas:\n1.  **Z-Score Normalization of Costs**: Before calculating the cost signal, the cost differences (`delta_cost`) for the entire batch are standardized using Z-score normalization. This makes the `temp_cost` hyperparameter less sensitive to the absolute scale of costs in the dataset. The normalized costs are then clamped to prevent outliers from dominating the `tanh` input.\n2.  **Rank Gap Modulation**: The final loss is multiplied by a 'rank gap' term, `rank_gap = 1.0 - exp(-beta * |rank(cost_l) - rank(cost_w)|)`. This term, inspired by ranking losses, dynamically increases the loss weight for pairs that are far apart in the batch's cost-ranking. This encourages the model to prioritize learning the relative ordering of significantly different solutions over those that are already close in cost, adding a new layer of supervision based on the global ranking within the batch.", "pseudocode": "1. For each pair (w, l) in the batch, where cost(w) < cost(l), calculate the cost difference `delta_cost = cost(l) - cost(w)` and the log probability difference `delta_logp = logp(w) - logp(l)`.\n2. Standardize the `delta_cost` vector for the entire batch using Z-score normalization to get `delta_cost_norm`. This makes the process robust to the scale of costs.\n3. Clamp the normalized cost differences to a reasonable range (e.g., [-3, 3]) to mitigate the effect of extreme outliers.\n4. Create a bounded 'cost signal' by applying a `tanh` function to the clamped, normalized cost differences: `cost_signal = tanh(delta_cost_norm / temp_cost)`.\n5. Calculate the adaptive margin based on this signal: `margin = alpha * cost_signal`.\n6. Compute the base per-sample loss using a softplus function: `base_loss = softplus(margin - delta_logp)`.\n7. Calculate a 'rank gap' weight for each pair. First, find the rank of `cost_w` and `cost_l` within the sorted costs of the entire batch. Then compute `rank_gap_weight = 1.0 - exp(-beta * abs(rank(cost_l) - rank(cost_w)))`.\n8. Modulate the base loss with this rank gap weight: `final_loss = base_loss * rank_gap_weight`.\n9. Return the weighted mean of `final_loss` over the batch.", "hyperparams": {"alpha": 1.5, "temp_cost": 1.0, "beta": 0.1, "z_clamp": 3.0}, "operators_used": ["softplus", "tanh", "zscore", "clamp", "exp", "rank_gap"], "implementation_hint": {"expects": ["cost_a", "cost_b", "logp_a", "logp_b"], "returns": "scalar"}, "code": "import torch\nimport torch.nn.functional as F\n\ndef generated_loss(batch, model_output, extra):\n    \"\"\"\n    Computes the Adaptive Margin Loss with Z-Score Normalization and Rank Gap.\n\n    Inherits the softplus(margin - delta_logp) structure and the use of tanh for a bounded cost signal.\n    Introduces two new ideas:\n    1. Z-score normalization of cost differences across the batch for scale invariance.\n    2. A 'rank gap' modulator that up-weights the loss for pairs that are far apart in the batch's cost ranking.\n    \"\"\"\n    # For preference learning, we assume the dataset provides pairs where one is strictly better.\n    cost_w = torch.min(batch['cost_a'], batch['cost_b'])\n    cost_l = torch.max(batch['cost_a'], batch['cost_b'])\n    log_prob_w = batch['log_prob_w']\n    log_prob_l = batch['log_prob_l']\n\n    # Retrieve hyperparameters\n    alpha = extra.get('alpha', 1.5)\n    temp_cost = extra.get('temp_cost', 1.0)\n    beta = extra.get('beta', 0.1)\n    z_clamp = extra.get('z_clamp', 3.0)\n\n    # 1. Calculate cost and log probability differences\n    delta_cost = cost_l - cost_w\n    delta_logp = log_prob_w - log_prob_l\n\n    # Early exit for a batch with no cost difference to avoid division by zero in z-score\n    if torch.all(delta_cost == 0):\n        return torch.tensor(0.0, device=delta_cost.device)\n\n    # 2. New Coupling: Z-score normalization of cost differences for batch-wise scale invariance\n    delta_cost_mean = delta_cost.mean()\n    delta_cost_std = delta_cost.std().clamp(min=1e-8) # Avoid division by zero\n    delta_cost_norm = (delta_cost - delta_cost_mean) / delta_cost_std\n\n    # 3. New Coupling: Clamp to prevent extreme outliers from creating huge signals\n    delta_cost_clamped = torch.clamp(delta_cost_norm, -z_clamp, z_clamp)\n\n    # 4. Inherited: Create a bounded cost signal using tanh\n    cost_signal = torch.tanh(delta_cost_clamped / temp_cost)\n\n    # 5. Inherited: Calculate the adaptive margin\n    adaptive_margin = alpha * cost_signal\n\n    # 6. Inherited: Compute the base per-sample loss using softplus\n    base_loss = F.softplus(adaptive_margin - delta_logp)\n\n    # 7. New Coupling: Calculate rank gap modulation\n    # This requires ranking all costs in the batch\n    all_costs = torch.cat([cost_w, cost_l], dim=0)\n    sorted_costs, _ = torch.sort(all_costs)\n    \n    # Find the rank of each cost_w and cost_l in the sorted list\n    # Note: `torch.searchsorted` is an efficient way to find ranks\n    rank_w = torch.searchsorted(sorted_costs, cost_w)\n    rank_l = torch.searchsorted(sorted_costs, cost_l)\n    \n    rank_diff = (rank_l - rank_w).float()\n    \n    # The rank_gap_weight approaches 1 as rank_diff increases, and is 0 if ranks are the same.\n    rank_gap_weight = 1.0 - torch.exp(-beta * rank_diff)\n\n    # 8. Modulate the base loss with the rank gap weight\n    final_loss = base_loss * rank_gap_weight.detach() # Detach to treat it as a pure weighting factor\n\n    # 9. Apply optional weights and compute the mean.\n    weights = batch.get('weight', None)\n    if weights is not None:\n        final_loss = final_loss * weights\n        return final_loss.sum() / weights.sum().clamp(min=1e-8)\n    else:\n        return final_loss.mean()"}, "static_ok": true, "static_reason": "", "dynamic_ok": true, "dynamic_reason": "ok", "loss_value": 0.5532031655311584, "grad_norm": 0.0}
{"generation": 5, "index": 2, "ir": {"name": "Rank-Gap Scaled Hinge Loss", "intuition": "This loss function synthesizes ideas from its parents to create a robust and adaptive preference learning objective. \n\nInherited ideas:\n- From Parent 0, it inherits the use of `softplus(delta_cost)` to create a non-saturating, unbounded margin target. This ensures that as the true cost gap between two solutions becomes very large, the model is pushed proportionally harder to separate them, avoiding the gradient saturation of bounded functions like `tanh`.\n- From both parents, it inherits the core structure of a hinge-like loss, specifically `softplus(margin - delta_logp)`, which penalizes the model when its predicted log-probability difference (`delta_logp`) falls short of the target margin.\n\nNew coupling ideas:\n1. **Rank-Gap Normalization of Costs**: Before calculating the margin, the raw cost differences (`delta_cost`) are normalized using a `rank_gap` transformation. This replaces absolute cost values with a score based on their relative ranking within the batch, making the loss less sensitive to outliers or shifts in the cost distribution. This is a stability trick that focuses the loss on relative preference ordering rather than absolute magnitudes.\n2. **Dynamic Log-Probability Scaling**: The model's log-probability difference (`delta_logp`) is dynamically scaled by `sigmoid(delta_logp)`. This scaling factor gently dampens the gradient for very large, confident predictions (where `sigmoid(delta_logp)` approaches 1) and for very wrong predictions (where it approaches 0). This helps stabilize training by preventing excessively large gradients from highly confident but correct predictions, while also moderating the pull from extremely incorrect ones, focusing learning on the uncertain middle ground.", "pseudocode": "1. For each pair (winner, loser), calculate the raw cost difference `delta_cost = cost_loser - cost_winner` and the log probability difference `delta_logp = logp_winner - logp_loser`.\n2. Apply a rank-gap normalization to the `delta_cost` values across the entire batch to obtain `norm_delta_cost`. This makes the signal robust to cost scale and outliers.\n3. Calculate an adaptive margin target using the inherited `softplus` idea on the normalized cost difference: `margin = alpha * softplus(norm_delta_cost / temp_cost)`.\n4. Introduce a new dynamic scaling factor for the model's preference based on its own confidence: `logp_scale = sigmoid(delta_logp)`.\n5. Scale the model's log-probability difference by this factor: `scaled_delta_logp = logp_scale * delta_logp`.\n6. Compute the final loss using the inherited softplus hinge structure, but with the scaled log-probability difference: `loss = softplus(margin - scaled_delta_logp)`.\n7. Return the weighted mean of the loss across the batch.", "hyperparams": {"alpha": 1.5, "temp_cost": 1.0}, "operators_used": ["softplus", "sigmoid", "rank_gap"], "implementation_hint": {"expects": ["cost_a", "cost_b", "logp_w", "logp_l", "weight"], "returns": "scalar"}, "code": "import torch\nimport torch.nn.functional as F\n\ndef rank_gap(x):\n    \"\"\"Numerically stable rank-based normalization.\n    Transforms values to be in the range [0, 1] based on their sorted order.\n    \"\"\"\n    if x.numel() <= 1:\n        return torch.ones_like(x) * 0.5\n    \n    # Create indices for sorting\n    _, sorted_indices = torch.sort(x)\n    \n    # Create a tensor of ranks [0, 1, 2, ..., N-1]\n    ranks = torch.arange(x.numel(), device=x.device, dtype=x.dtype)\n    \n    # Create an empty tensor to store the results\n    ranked_x = torch.zeros_like(x)\n    \n    # Use the sorted indices to place the ranks in the original order\n    ranked_x[sorted_indices] = ranks\n    \n    # Normalize ranks to be in [0, 1]\n    # Add a small epsilon to the denominator to avoid division by zero if there's only one element\n    normalized_ranks = ranked_x / (x.numel() - 1.0).clamp(min=1e-8)\n    return normalized_ranks\n\ndef generated_loss(batch, model_output, extra):\n    \"\"\"\n    Computes the Rank-Gap Scaled Hinge Loss.\n\n    This loss encourages the log probability of the winner to be higher than the loser's\n    by a margin that adapts to the rank-normalized cost difference. It also introduces\n    a dynamic scaling on the model's log-probability difference to stabilize gradients.\n\n    Args:\n        batch (dict): A dictionary containing tensors from the dataloader.\n                      Expected keys:\n                      'cost_a', 'cost_b': Costs of the two solutions. Shape: [N].\n                      'log_prob_w', 'log_prob_l': Model's log probabilities for the\n                                                winner and loser. Shape: [N].\n                      'weight' (optional): Per-sample loss weights. Shape: [N].\n        model_output: Not used in this loss implementation.\n        extra (dict): A dictionary for hyperparameters. Expected keys:\n                      'alpha' (float): Scaling factor for the adaptive margin.\n                      'temp_cost' (float): Temperature for the cost signal.\n\n    Returns:\n        torch.Tensor: A scalar tensor representing the mean loss over the batch.\n    \"\"\"\n    # Identify winner (w) and loser (l) based on costs\n    cost_w = torch.min(batch['cost_a'], batch['cost_b'])\n    cost_l = torch.max(batch['cost_a'], batch['cost_b'])\n    log_prob_w = batch['log_prob_w']\n    log_prob_l = batch['log_prob_l']\n\n    # Retrieve hyperparameters\n    alpha = extra.get('alpha', 1.5)\n    temp_cost = extra.get('temp_cost', 1.0)\n\n    # 1. Calculate raw differences\n    delta_cost = cost_l - cost_w\n    delta_logp = log_prob_w - log_prob_l\n\n    # 2. (New Coupling) Normalize cost difference using rank_gap for stability\n    norm_delta_cost = rank_gap(delta_cost)\n\n    # 3. (Inherited from Parent 0) Calculate adaptive margin with softplus\n    # This provides a non-saturating target based on the relative cost gap.\n    cost_signal = F.softplus(norm_delta_cost / temp_cost)\n    margin = alpha * cost_signal\n\n    # 4. (New Coupling) Create a dynamic scaling factor from model's own confidence\n    # This sigmoid term gently dampens gradients for extreme delta_logp values.\n    logp_scale = torch.sigmoid(delta_logp)\n\n    # 5. Apply the dynamic scaling to the model's preference score\n    scaled_delta_logp = logp_scale * delta_logp\n\n    # 6. (Inherited from both) Compute the core hinge loss using softplus\n    loss = F.softplus(margin - scaled_delta_logp)\n\n    # 7. Apply optional weights and compute the mean\n    weights = batch.get('weight', None)\n    if weights is not None:\n        loss = loss * weights\n        return loss.sum() / weights.sum().clamp(min=1e-8)\n    else:\n        return loss.mean()"}, "static_ok": true, "static_reason": "", "dynamic_ok": false, "dynamic_reason": "forward_error: name 'rank_gap' is not defined", "loss_value": null, "grad_norm": null}
{"generation": 5, "index": 3, "ir": {"name": "Rank-Gap Scaled Bradley-Terry Loss", "intuition": "This loss function combines the core Bradley-Terry model with an adaptive margin mechanism inspired by both parents, and introduces a new rank-based scaling for stability and focus. \n\nInherited ideas:\n- From both parents, it inherits the concept of an **adaptive margin**, where the target separation between log probabilities (`delta_logp`) is not fixed but depends on the cost difference (`delta_cost`).\n- From Parent 0, it inherits the use of `softplus` on the cost difference to create a non-saturating, unbounded signal. This ensures that very large cost differences result in a very strong push for the model to agree.\n\nNew coupling ideas:\n1.  **Bradley-Terry Core:** Instead of a hinge-like loss (`softplus(margin - delta_logp)`), this child uses the `logsigmoid` formulation common in Bradley-Terry models: `loss = -logsigmoid(delta_logp - margin)`. This frames the problem as maximizing the log-likelihood of the observed preference, shifted by the adaptive margin.\n2.  **Rank-Gap Scaling:** The loss for each pair is dynamically scaled by a factor derived from the rank of its cost difference within the batch. The `rank_gap` operator computes `1 - rank(delta_cost) / N`, which gives a weight close to 1.0 for pairs with the largest cost differences and a weight close to 0.0 for pairs with the smallest. This coupling focuses the training gradient on the most unambiguous and informative pairs in each batch, while down-weighting pairs with very similar costs, which might be noisy or less important to get right.", "pseudocode": "1. For each pair (winner, loser) in the batch, calculate the cost difference `delta_cost = cost_loser - cost_winner` and the log probability difference `delta_logp = logp_winner - logp_loser`.\n2. Create an unbounded adaptive margin by scaling the softplus of the cost difference: `margin = alpha * softplus(delta_cost / temp_cost)`. This is inherited from Parent 0.\n3. Calculate the rank-based scaling factor for each pair. First, compute the rank of each `delta_cost` within the batch. Then, normalize these ranks to a [0, 1] range and invert them to get a 'rank gap' weight: `rank_weight = rank_gap(delta_cost)`. This is a new coupling idea.\n4. Compute the core per-sample loss using a Bradley-Terry-like formulation, incorporating the adaptive margin: `per_sample_loss = -logsigmoid(delta_logp - margin)`. This penalizes cases where `delta_logp` is smaller than the target `margin`.\n5. Apply the rank-based scaling to the loss: `scaled_loss = rank_weight * per_sample_loss`.\n6. The final loss is the weighted mean of these scaled losses over the batch.", "hyperparams": {"alpha": 0.5, "temp_cost": 1.0}, "operators_used": ["logsigmoid", "softplus", "rank_gap"], "implementation_hint": {"expects": ["cost_a", "cost_b", "log_prob_w", "log_prob_l", "weight"], "returns": "scalar"}, "code": "import torch\nimport torch.nn.functional as F\n\ndef rank_gap(x):\n    \"\"\"Computes a weight based on the rank of each element in x.\"\"\"\n    ranks = x.argsort().argsort().float()\n    # Normalize ranks to [0, 1] and invert to get a 'gap' score\n    # Largest x gets highest weight (close to 1), smallest x gets lowest weight (close to 0)\n    # Add a small epsilon to avoid division by zero if batch size is 1\n    return ranks / (x.numel() - 1 + 1e-8)\n\ndef generated_loss(batch, model_output, extra):\n    \"\"\"\n    Computes the Rank-Gap Scaled Bradley-Terry Loss.\n\n    This loss uses a Bradley-Terry-like objective (-logsigmoid) where the model's\n    log probability difference is encouraged to exceed an adaptive margin.\n    The margin is calculated using a non-saturating softplus function on the cost difference.\n    A novel rank-gap scaling mechanism is introduced to prioritize training on pairs with\n    the largest cost differences within the batch.\n\n    Args:\n        batch (dict): A dictionary containing tensors from the dataloader.\n                      Expected keys:\n                      'cost_a', 'cost_b': Costs of the two solutions in a pair. Shape: [N].\n                      'log_prob_w', 'log_prob_l': Model's log probabilities for the\n                                                winner and loser solutions. Shape: [N].\n                      'weight' (optional): Per-sample loss weights. Shape: [N].\n        model_output: Not used in this loss implementation.\n        extra (dict): A dictionary for hyperparameters. Expected keys:\n                      'alpha' (float): A scaling factor for the adaptive margin.\n                      'temp_cost' (float): A temperature for normalizing the cost difference.\n\n    Returns:\n        torch.Tensor: A scalar tensor representing the mean loss over the batch.\n    \"\"\"\n    # Determine winner (w) and loser (l) based on costs. cost_w < cost_l\n    cost_w = torch.min(batch['cost_a'], batch['cost_b'])\n    cost_l = torch.max(batch['cost_a'], batch['cost_b'])\n\n    # Log probabilities are assumed to be provided for winner and loser directly.\n    log_prob_w = batch['log_prob_w']\n    log_prob_l = batch['log_prob_l']\n\n    # Retrieve hyperparameters\n    alpha = extra.get('alpha', 0.5)\n    temp_cost = extra.get('temp_cost', 1.0)\n\n    # 1. Calculate cost and log probability differences\n    delta_cost = cost_l - cost_w\n    delta_logp = log_prob_w - log_prob_l\n\n    # 2. Create an unbounded adaptive margin (Inherited from Parent 0)\n    # The softplus function provides a non-saturating signal that grows with cost difference.\n    adaptive_margin = alpha * F.softplus(delta_cost / temp_cost)\n\n    # 3. Calculate rank-based scaling factor (New Coupling Idea)\n    # This gives higher weight to pairs with a larger cost difference in the batch.\n    # It helps focus the model on learning from the most informative examples.\n    with torch.no_grad(): # Ranks should not contribute to the gradient\n        rank_weight = rank_gap(delta_cost)\n\n    # 4. Compute the core loss using a Bradley-Terry-like formulation\n    # The loss is low when delta_logp is much larger than the adaptive_margin.\n    loss_arg = delta_logp - adaptive_margin\n    per_sample_loss = -F.logsigmoid(loss_arg)\n\n    # 5. Apply the rank-based scaling to the loss\n    scaled_loss = rank_weight * per_sample_loss\n\n    # 6. Apply optional dataset weights and compute the mean.\n    weights = batch.get('weight', None)\n    if weights is not None:\n        final_loss = scaled_loss * weights\n        return final_loss.sum() / weights.sum().clamp(min=1e-8)\n    else:\n        return scaled_loss.mean()"}, "static_ok": true, "static_reason": "", "dynamic_ok": false, "dynamic_reason": "forward_error: name 'rank_gap' is not defined", "loss_value": null, "grad_norm": null}
{"generation": 5, "index": 4, "ir": {"name": "Adaptive Margin LogSigmoid Loss with Cost Z-Scoring", "intuition": "This loss function combines the core structure of an adaptive margin loss with robust normalization and a classic sigmoid-based preference formulation. \n\nInherited ideas:\n- From Parent 0 and 1, it inherits the concept of an **adaptive margin**, where the target separation between the winner's and loser's log-probabilities (`delta_logp`) scales with the difference in their costs (`delta_cost`). This ensures that the model is pushed harder to distinguish between pairs with a large quality gap.\n- From Parent 0, it inherits the use of `softplus` to create a **non-saturating cost signal**. This allows the margin to grow approximately linearly for large cost differences, preventing the learning signal from vanishing for easy examples.\n\nNew coupling ideas:\n1. **Z-Score Normalization**: Instead of simple temperature scaling, the `delta_cost` for the entire batch is normalized using z-scoring (`(x - mean) / std`). This makes the margin's scale invariant to the distribution of cost differences in a batch, improving stability and reducing sensitivity to the `temp_cost` hyperparameter. The normalized costs are then clamped to prevent extreme outliers from dominating the loss.\n2. **LogSigmoid Formulation**: The final loss is computed as `-logsigmoid(delta_logp - adaptive_margin)`. This is a common and probabilistically interpretable formulation for preference learning, equivalent to `softplus(adaptive_margin - delta_logp)` but framed differently. It directly maximizes the log-probability of the model's preference `delta_logp` being greater than the target `adaptive_margin`.", "pseudocode": "1. For each pair (winner, loser), calculate the cost difference `delta_cost = cost_loser - cost_winner` and the log probability difference `delta_logp = logp_winner - logp_loser`.\n2. Across the entire batch, compute the mean and standard deviation of `delta_cost`.\n3. Normalize the `delta_cost` values using z-scoring: `norm_delta_cost = (delta_cost - mean) / (std + epsilon)`.\n4. Clamp the `norm_delta_cost` to a reasonable range (e.g., [-3, 3]) to mitigate the effect of extreme outliers.\n5. Create a non-saturating cost signal by applying a softplus function to the clamped, normalized cost difference: `cost_signal = softplus(norm_delta_cost / temp_cost)`.\n6. Calculate the adaptive margin by scaling the cost signal: `adaptive_margin = alpha * cost_signal`.\n7. The loss for each pair is computed using a logsigmoid formulation: `loss = -logsigmoid(delta_logp - adaptive_margin)`.\n8. The final loss is the weighted mean of these individual losses over the batch.", "hyperparams": {"alpha": 1.0, "temp_cost": 1.0, "clamp_range": 3.0}, "operators_used": ["softplus", "logsigmoid", "zscore", "clamp"], "implementation_hint": {"expects": ["cost_a", "cost_b", "logp_chosen", "logp_rejected", "weight"], "returns": "scalar"}, "code": "import torch\nimport torch.nn.functional as F\n\ndef generated_loss(batch, model_output, extra):\n    \"\"\"\n    Computes an adaptive margin loss using z-score normalization for costs and a logsigmoid formulation.\n\n    Inherits the adaptive margin concept from parents, where the target log-probability gap\n    scales with the cost difference. It uses a non-saturating softplus function (like Parent 0)\n    to create the cost signal. A new coupling idea is to normalize the batch-wise cost\n    differences using z-scoring before applying the softplus, making the margin robust to\n    variations in cost scales. The final loss uses the standard logsigmoid preference loss structure.\n\n    Args:\n        batch (dict): A dictionary containing tensors from the dataloader.\n                      Expected keys:\n                      'cost_a', 'cost_b': Costs of the two solutions in a pair. Shape: [N].\n                      'log_prob_w', 'log_prob_l': Model's log probabilities for the\n                                                winner and loser solutions. Shape: [N].\n                      'weight' (optional): Per-sample loss weights. Shape: [N].\n        model_output: Not used in this loss implementation.\n        extra (dict): A dictionary for hyperparameters. Expected keys:\n                      'alpha' (float): Scaling factor for the adaptive margin.\n                      'temp_cost' (float): Temperature for scaling the z-scored cost difference.\n                      'clamp_range' (float): The range (+/-) to clamp z-scored costs.\n\n    Returns:\n        torch.Tensor: A scalar tensor representing the mean loss over the batch.\n    \"\"\"\n    # Assume cost_w < cost_l\n    cost_w = torch.min(batch['cost_a'], batch['cost_b'])\n    cost_l = torch.max(batch['cost_a'], batch['cost_b'])\n    log_prob_w = batch['log_prob_w']\n    log_prob_l = batch['log_prob_l']\n\n    # Retrieve hyperparameters\n    alpha = extra.get('alpha', 1.0)\n    temp_cost = extra.get('temp_cost', 1.0)\n    clamp_range = extra.get('clamp_range', 3.0)\n    epsilon = 1e-8\n\n    # 1. Calculate cost and log probability differences\n    delta_cost = cost_l - cost_w\n    delta_logp = log_prob_w - log_prob_l\n\n    # 2. (New Idea) Z-Score normalize delta_cost across the batch for stability\n    if delta_cost.numel() > 1:\n        cost_mean = delta_cost.mean()\n        cost_std = delta_cost.std()\n        # The 'zscore' operator is implemented here\n        norm_delta_cost = (delta_cost - cost_mean) / (cost_std + epsilon)\n    else:\n        # Handle batch size of 1 to avoid std=0 -> NaN\n        norm_delta_cost = torch.zeros_like(delta_cost)\n\n    # 3. (New Idea) Clamp to prevent outliers from creating extreme margins\n    clamped_norm_cost = torch.clamp(norm_delta_cost, -clamp_range, clamp_range)\n\n    # 4. (Inherited from Parent 0) Create a non-saturating cost signal with softplus\n    cost_signal = F.softplus(clamped_norm_cost / temp_cost)\n\n    # 5. (Inherited from Parents) Create an adaptive margin\n    adaptive_margin = alpha * cost_signal\n\n    # 6. (New Idea) Compute loss using logsigmoid formulation\n    # This is equivalent to softplus(adaptive_margin - delta_logp)\n    loss = -F.logsigmoid(delta_logp - adaptive_margin)\n\n    # 7. Apply optional weights and compute the mean.\n    weights = batch.get('weight', None)\n    if weights is not None:\n        loss = loss * weights\n        return loss.sum() / weights.sum().clamp(min=epsilon)\n    else:\n        return loss.mean()"}, "static_ok": true, "static_reason": "", "dynamic_ok": true, "dynamic_reason": "ok", "loss_value": 1.0986123085021973, "grad_norm": 0.0}
{"generation": 5, "index": 5, "ir": {"name": "Softplus Z-Scored Adaptive Margin Loss", "intuition": "This loss function combines the adaptive margin concept from its parents with a novel normalization scheme. The core loss structure, `softplus(margin - delta_logp)`, is inherited from both parents, creating a smooth penalty when the model's preference (`delta_logp`) falls short of a target margin.\n\nFrom one parent, it inherits the use of `softplus` to transform the cost difference into an unbounded, non-negative signal. This ensures that larger cost differences create proportionally larger target margins without saturation.\n\nAs a new coupling idea, this child introduces batch-level z-score normalization for both the cost difference (`delta_cost`) and the log-probability difference (`delta_logp`). This makes the loss less sensitive to the absolute scale of costs and log probabilities, which can vary significantly across different models or problem domains. By operating on standardized values, the loss becomes more robust and the hyperparameter `alpha` becomes more interpretable as a desired correlation coefficient between the standardized signals. A small epsilon is added during normalization for stability, preventing division by zero in batches with no variance.", "pseudocode": "1. For each pair (winner, loser), calculate the cost difference `delta_cost = cost_loser - cost_winner` and the log probability difference `delta_logp = logp_winner - logp_loser`.\n2. Apply z-score normalization across the entire batch to both `delta_cost` and `delta_logp` to get `z_cost` and `z_logp`. This centers the values around 0 with a standard deviation of 1, making them scale-invariant. Add a small epsilon to the standard deviation for numerical stability.\n3. Inherit the use of `softplus` from one parent to transform the normalized cost difference into a non-negative, unbounded signal: `cost_signal = softplus(z_cost)`.\n4. Create an adaptive margin target by scaling this signal: `margin_target = alpha * cost_signal`.\n5. The core loss is calculated using the `softplus` function, inherited from both parents: `loss = softplus(margin_target - z_logp)`. This penalizes the model when its normalized preference `z_logp` is less than the target margin derived from the normalized cost difference.\n6. The final loss is the weighted mean of these individual losses over the batch.", "hyperparams": {"alpha": 1.0, "epsilon": 1e-06}, "operators_used": ["softplus", "zscore"], "implementation_hint": {"expects": ["cost_a", "cost_b", "log_prob_w", "log_prob_l", "weight"], "returns": "scalar"}, "code": "import torch\nimport torch.nn.functional as F\n\ndef zscore(x, epsilon=1e-6):\n    \"\"\"Batch-wise z-score normalization with a stability epsilon.\"\"\"\n    # Ensure there's more than one element to calculate std dev\n    if x.numel() <= 1:\n        return x - x.mean()\n    \n    mean = x.mean()\n    std = x.std()\n    return (x - mean) / (std + epsilon)\n\ndef generated_loss(batch, model_output, extra):\n    \"\"\"\n    Computes the Softplus Z-Scored Adaptive Margin Loss.\n\n    This loss encourages the model's preference to align with the ground truth\n    cost difference. It inherits the softplus-based margin from its parents but\n    introduces a new coupling: z-score normalization of both the cost and\n    log-probability differences at the batch level for improved stability and\n    scale invariance.\n\n    Args:\n        batch (dict): A dictionary containing tensors from the dataloader.\n                      Expected keys:\n                      'cost_a', 'cost_b': Costs of the two solutions. Shape: [N].\n                      'log_prob_w', 'log_prob_l': Model's log probabilities for the\n                                                winner and loser. Shape: [N].\n                      'weight' (optional): Per-sample loss weights. Shape: [N].\n        model_output: Not used in this loss implementation.\n        extra (dict): A dictionary for hyperparameters. Expected keys:\n                      'alpha' (float): A scaling factor for the adaptive margin.\n                      'epsilon' (float): Small value for numerical stability in z-scoring.\n\n    Returns:\n        torch.Tensor: A scalar tensor representing the mean loss over the batch.\n    \"\"\"\n    # Retrieve hyperparameters\n    alpha = extra.get('alpha', 1.0)\n    epsilon = extra.get('epsilon', 1e-6)\n\n    # 1. Calculate cost and log probability differences\n    cost_w = torch.min(batch['cost_a'], batch['cost_b'])\n    cost_l = torch.max(batch['cost_a'], batch['cost_b'])\n    delta_cost = cost_l - cost_w\n\n    log_prob_w = batch['log_prob_w']\n    log_prob_l = batch['log_prob_l']\n    delta_logp = log_prob_w - log_prob_l\n\n    # 2. (New Coupling Idea) Apply z-score normalization across the batch.\n    # This makes the loss robust to the absolute scale of costs and log-probs.\n    z_cost = zscore(delta_cost, epsilon=epsilon)\n    z_logp = zscore(delta_logp, epsilon=epsilon)\n\n    # 3. (Inherited Idea) Transform the normalized cost difference into a non-negative signal.\n    # Using softplus, inherited from one parent, creates an unbounded signal that\n    # grows with the relative cost difference within the batch.\n    cost_signal = F.softplus(z_cost)\n\n    # 4. Create an adaptive margin based on the normalized cost signal.\n    adaptive_margin = alpha * cost_signal\n\n    # 5. (Inherited Idea) Compute the core loss using softplus.\n    # This smooth hinge loss penalizes instances where the normalized model preference\n    # z_logp is less than the target margin.\n    loss = F.softplus(adaptive_margin - z_logp)\n\n    # 6. Apply optional weights and compute the mean.\n    weights = batch.get('weight', None)\n    if weights is not None:\n        loss = loss * weights\n        return loss.sum() / weights.sum().clamp(min=1e-8)\n    else:\n        return loss.mean()"}, "static_ok": true, "static_reason": "", "dynamic_ok": false, "dynamic_reason": "forward_error: name 'zscore' is not defined", "loss_value": null, "grad_norm": null}
{"generation": 5, "index": 6, "ir": {"name": "Adaptive Margin Loss with Z-Score Normalization", "intuition": "This loss function synergizes two adaptive margin concepts from its parents and introduces a batch-level normalization for enhanced stability and adaptivity.\n\nInherited ideas:\n1.  **From Parent 0 (Softplus Margin):** It inherits the use of `softplus` to create an unbounded, non-negative target margin from the cost difference (`delta_cost`). This ensures that pairs with larger cost gaps receive a proportionally larger target separation, pushing the model harder where the preference is clearest.\n2.  **From Parent 1 (Tanh Margin):** It inherits the use of `tanh` to create a bounded, saturating signal from the cost difference. This is used not for the margin itself, but to dynamically scale the loss, effectively down-weighting the penalty for pairs with extremely large cost differences where the preference is likely already obvious.\n\nNew coupling ideas:\n1.  **Z-Score Normalization:** Instead of relying solely on a fixed temperature `temp_cost`, this loss first normalizes the `delta_cost` across the batch using z-scoring (`(x - mean) / std`). This makes the margin calculation robust to variations in the scale and distribution of cost differences between batches, leading to more stable training. A temperature is still used after z-scoring to control the sensitivity.\n2.  **Dynamic Loss Scaling:** The final loss term is scaled by `(1 - tanh(normalized_delta_cost))`. This acts as a dynamic weight, reducing the loss contribution from pairs with very large cost differences (where `tanh` approaches 1) and focusing the training on more ambiguous pairs with smaller cost gaps.", "pseudocode": "1. For each pair (winner, loser), calculate the cost difference `delta_cost = cost_loser - cost_winner` and the log probability difference `delta_logp = logp_winner - logp_loser`.\n2. Across the entire batch, compute the mean and standard deviation of `delta_cost`.\n3. Normalize `delta_cost` for each pair using z-scoring: `normalized_delta_cost = (delta_cost - mean) / (std + epsilon)`.\n4. Inherit from Parent 0: Calculate an unbounded adaptive margin using the `softplus` function on the normalized cost difference: `margin = alpha * softplus(normalized_delta_cost / temp_cost)`.\n5. Inherit from Parent 1: Calculate a bounded cost signal using the `tanh` function: `cost_signal = tanh(normalized_delta_cost / temp_cost)`.\n6. Compute the core preference violation loss as `softplus(margin - delta_logp)`.\n7. Introduce a new coupling: Dynamically scale the loss using the tanh-based signal. The scaling factor `(1 - cost_signal)` reduces the loss for pairs with very large cost differences, focusing the model on harder examples. The final per-sample loss is `loss = (1 - cost_signal) * softplus(margin - delta_logp)`.\n8. Compute the weighted mean of the per-sample losses over the batch.", "hyperparams": {"alpha": 1.0, "temp_cost": 2.0, "z_score_eps": 1e-06}, "operators_used": ["softplus", "tanh", "zscore"], "implementation_hint": {"expects": ["cost_a", "cost_b", "logp_w", "logp_l", "weight"], "returns": "scalar"}, "code": "import torch\nimport torch.nn.functional as F\n\ndef generated_loss(batch, model_output, extra):\n    \"\"\"\n    Computes an Adaptive Margin Loss with Z-Score Normalization and Dynamic Scaling.\n\n    This loss encourages the log probability of the winner to be higher than the loser's\n    by an adaptive margin. The margin is derived from the z-score normalized cost\n    difference, making it robust to varying cost scales across batches. The final loss\n    is dynamically scaled to de-emphasize pairs with extremely large cost differences.\n\n    Args:\n        batch (dict): A dictionary containing tensors from the dataloader.\n                      Expected keys:\n                      'cost_a', 'cost_b': Costs of the two solutions. Shape: [N].\n                      'log_prob_w', 'log_prob_l': Model's log probabilities for the\n                                                winner and loser. Shape: [N].\n                      'weight' (optional): Per-sample loss weights. Shape: [N].\n        model_output: Not used in this loss implementation.\n        extra (dict): A dictionary for hyperparameters. Expected keys:\n                      'alpha' (float): Scaling factor for the adaptive margin.\n                      'temp_cost' (float): Temperature for scaling normalized costs.\n                      'z_score_eps' (float): Epsilon for stable z-score normalization.\n\n    Returns:\n        torch.Tensor: A scalar tensor representing the mean loss over the batch.\n    \"\"\"\n    # Identify winner (w) and loser (l) costs and log probabilities\n    cost_w = torch.min(batch['cost_a'], batch['cost_b'])\n    cost_l = torch.max(batch['cost_a'], batch['cost_b'])\n    log_prob_w = batch['log_prob_w']\n    log_prob_l = batch['log_prob_l']\n\n    # Retrieve hyperparameters\n    alpha = extra.get('alpha', 1.0)\n    temp_cost = extra.get('temp_cost', 2.0)\n    z_score_eps = extra.get('z_score_eps', 1e-6)\n\n    # 1. Calculate cost and log probability differences\n    delta_cost = cost_l - cost_w\n    delta_logp = log_prob_w - log_prob_l\n\n    # 2. & 3. New Coupling: Z-score normalization of delta_cost across the batch\n    # This makes the loss invariant to the absolute scale of costs in a batch.\n    if delta_cost.numel() > 1:\n        cost_mean = delta_cost.mean()\n        cost_std = delta_cost.std()\n        normalized_delta_cost = (delta_cost - cost_mean) / (cost_std + z_score_eps)\n    else:\n        # Handle batch size of 1 to avoid NaN from std=0\n        normalized_delta_cost = torch.zeros_like(delta_cost)\n\n    # Scale by temperature after normalization\n    scaled_norm_cost = normalized_delta_cost / temp_cost\n\n    # 4. Inherit from Parent 0: Unbounded margin via softplus\n    # The margin is now based on the relative cost difference within the batch.\n    adaptive_margin = alpha * F.softplus(scaled_norm_cost)\n\n    # 5. Inherit from Parent 1: Bounded signal via tanh\n    # This will be used for dynamic loss scaling.\n    cost_signal_tanh = torch.tanh(scaled_norm_cost)\n\n    # 6. Compute the core preference violation loss (smooth hinge loss)\n    core_loss = F.softplus(adaptive_margin - delta_logp)\n\n    # 7. New Coupling: Dynamic loss scaling\n    # The factor (1 - tanh) is close to 1 for small cost differences and\n    # approaches 0 for very large cost differences, focusing training on harder examples.\n    # Clamp is used for stability, though tanh is bounded.\n    loss_scale_factor = 1.0 - cost_signal_tanh.clamp(min=0.0, max=1.0)\n    loss = loss_scale_factor * core_loss\n\n    # 8. Apply optional weights and compute the mean.\n    weights = batch.get('weight', None)\n    if weights is not None:\n        loss = loss * weights\n        # Use a small epsilon to prevent division by zero if all weights are zero.\n        return loss.sum() / weights.sum().clamp(min=1e-8)\n    else:\n        return loss.mean()"}, "static_ok": true, "static_reason": "", "dynamic_ok": true, "dynamic_reason": "ok", "loss_value": 1.0986123085021973, "grad_norm": 0.0}
{"generation": 5, "index": 7, "ir": {"name": "Rank-Gap Scaled LogSigmoid Loss", "intuition": "This loss function combines the core Bradley-Terry model structure with an adaptive margin that is sensitive to both the magnitude and the rank of cost differences within a batch. It inherits the `softplus(margin - delta_logp)` structure from both parents, which is a smooth hinge loss that penalizes the model when its preference `delta_logp` falls short of a target margin. It also inherits the idea of an adaptive margin based on cost differences.\n\nFrom Parent 0, it inherits the use of a non-saturating function (`softplus`) on the cost difference to create a cost signal, ensuring that very large cost gaps continue to provide a stronger learning signal. From Parent 1, it implicitly shares the goal of creating a bounded but informative signal.\n\nTwo new coupling ideas are introduced for stability and batch-aware adaptation:\n1. **Rank-Gap Normalization**: Instead of using the raw cost difference, which can have high variance across batches, we use the `rank_gap` of the costs. This operator maps the cost difference to a value between 0 and 1 based on its rank percentile within the batch. This makes the margin less sensitive to the absolute scale of costs in a given batch and more focused on the relative ordering, improving stability.\n2. **LogSigmoid Formulation**: The core loss is expressed as `-logsigmoid(delta_logp - margin)`. This is mathematically equivalent to `softplus(margin - delta_logp)` but often provides better numerical stability and gradient flow, especially when the argument is very large or very small. It frames the problem as maximizing the log-likelihood of the observed preference, modulated by the adaptive margin.", "pseudocode": "1. For each pair (winner, loser) in the batch, calculate the log probability difference `delta_logp = logp(winner) - logp(loser)` and the cost difference `delta_cost = cost(loser) - cost(winner)`.\n2. Normalize the cost differences across the entire batch using the `rank_gap` operator. This computes the percentile rank of each `delta_cost`, yielding a stable `cost_signal` between 0 and 1.\n3. Inherited Idea (Parent 0): Apply a non-saturating transformation, `softplus`, to this normalized `cost_signal`. This gently amplifies the signal from pairs with a larger relative cost gap within the batch.\n4. Define an adaptive margin as `margin = alpha * softplus(cost_signal)`. This margin is now sensitive to the rank-ordering of cost differences.\n5. New Coupling Idea: Calculate the loss using a numerically stable `logsigmoid` formulation: `loss = -logsigmoid(delta_logp - margin)`. This penalizes cases where `delta_logp` is less than the adaptive margin.\n6. The final loss is the weighted mean of these individual losses over the batch.", "hyperparams": {"alpha": 2.0}, "operators_used": ["logsigmoid", "softplus", "rank_gap"], "implementation_hint": {"expects": ["cost_a", "cost_b", "log_prob_w", "log_prob_l", "weight"], "returns": "scalar"}, "code": "import torch\nimport torch.nn.functional as F\n\ndef rank_gap(x):\n    \"\"\"Computes the rank-based gap normalization for a tensor x.\"\"\"\n    if x.numel() <= 1:\n        return torch.zeros_like(x)\n    \n    # Sort the tensor and find the ranks\n    sorted_indices = torch.argsort(x)\n    ranks = torch.empty_like(sorted_indices, dtype=torch.float)\n    ranks[sorted_indices] = torch.arange(x.numel(), device=x.device, dtype=x.float)\n    \n    # Normalize ranks to [0, 1]\n    normalized_ranks = ranks / (x.numel() - 1).clamp(min=1.0)\n    return normalized_ranks\n\ndef generated_loss(batch, model_output, extra):\n    \"\"\"\n    Computes the Rank-Gap Scaled LogSigmoid Loss.\n\n    This loss encourages the log probability of the winner to be higher than the loser's\n    by an adaptive margin. The margin is determined by the rank-gap of the cost\n    differences within the batch, making it robust to cost scaling.\n\n    Args:\n        batch (dict): A dictionary containing tensors from the dataloader.\n                      Expected keys:\n                      'cost_a', 'cost_b': Costs of the two solutions. Shape: [N].\n                      'log_prob_w', 'log_prob_l': Model's log probabilities for the\n                                                winner and loser. Shape: [N].\n                      'weight' (optional): Per-sample loss weights. Shape: [N].\n        model_output: Not used in this implementation.\n        extra (dict): A dictionary for hyperparameters. Expected keys:\n                      'alpha' (float): A scaling factor for the adaptive margin.\n\n    Returns:\n        torch.Tensor: A scalar tensor representing the mean loss over the batch.\n    \"\"\"\n    # Identify winner (w) and loser (l) based on costs.\n    cost_w = torch.min(batch['cost_a'], batch['cost_b'])\n    cost_l = torch.max(batch['cost_a'], batch['cost_b'])\n\n    log_prob_w = batch['log_prob_w']\n    log_prob_l = batch['log_prob_l']\n\n    alpha = extra.get('alpha', 2.0)\n\n    # 1. Calculate cost and log probability differences\n    delta_cost = cost_l - cost_w\n    delta_logp = log_prob_w - log_prob_l\n\n    # 2. (New Coupling Idea) Normalize cost differences using rank_gap for stability.\n    # This makes the signal robust to the absolute scale of costs in a batch.\n    cost_signal_ranked = rank_gap(delta_cost)\n\n    # 3. (Inherited from Parent 0) Apply softplus for non-saturating amplification.\n    # This gives more weight to pairs with a higher relative rank-gap.\n    amplified_signal = F.softplus(cost_signal_ranked)\n\n    # 4. Define the adaptive margin.\n    adaptive_margin = alpha * amplified_signal\n\n    # 5. (New Coupling Idea) Compute loss using stable logsigmoid formulation.\n    # This is equivalent to softplus(adaptive_margin - delta_logp) but can be\n    # more numerically stable. It encourages delta_logp > adaptive_margin.\n    loss = -F.logsigmoid(delta_logp - adaptive_margin)\n\n    # 6. Apply optional weights and compute the mean.\n    weights = batch.get('weight', None)\n    if weights is not None:\n        loss = loss * weights\n        return loss.sum() / weights.sum().clamp(min=1e-8)\n    else:\n        return loss.mean()"}, "static_ok": true, "static_reason": "", "dynamic_ok": false, "dynamic_reason": "forward_error: name 'rank_gap' is not defined", "loss_value": null, "grad_norm": null}
{"generation": 6, "index": 0, "ir": {"name": "Softplus Margin with Z-Score Normalization and Log-Sigmoid Preference", "intuition": "This loss function creates a robust, adaptive preference loss by combining features from its parents with a new stability-focused coupling.\n\nInherited Ideas:\n- From Parent 0, it inherits the use of Z-score normalization for the batch-wise cost differences (`delta_cost`). This makes the margin calculation robust to the absolute scale of costs in a given batch, improving generalization across different problems.\n- From Parent 1, it inherits the use of `softplus` to transform the (now normalized) cost difference into a non-saturating, non-negative margin. Unlike `tanh`, this allows the target margin to grow for pairs with very large cost differences, pushing the model harder to distinguish them.\n\nNew Coupling Ideas:\n1.  **Log-Sigmoid Preference Term**: Instead of using a raw log-probability difference (`delta_logp`), the loss is formulated using `logsigmoid(delta_logp)`. This is a common technique from Bradley-Terry models and DPO-style losses. It reframes the objective as maximizing the log-likelihood of the preferred choice. This term is inherently bounded between (-inf, 0), which improves numerical stability and prevents the model from being overly penalized for extremely confident (but correct) predictions where `delta_logp` might become very large.\n2.  **Dynamic Temperature Scaling**: The temperature parameter `temp_cost`, which scales the normalized cost difference before the `softplus` function, is made dynamic. It is set to the standard deviation of the batch's `delta_logp` values. This couples the margin's sensitivity to the model's current output distribution. If the model's log-probability differences are very spread out (high variance), the temperature increases, softening the margin and preventing excessively large loss values. Conversely, if the model is uncertain (low variance in `delta_logp`), the temperature decreases, creating a sharper, more demanding margin.", "pseudocode": "1. For each pair (w, l) in the batch, where cost(w) < cost(l), calculate the cost difference `delta_cost = cost(l) - cost(w)` and the log probability difference `delta_logp = logp(w) - logp(l)`.\n2. Inherited from Parent 0: Standardize the `delta_cost` vector for the entire batch using Z-score normalization to get `delta_cost_norm`. This makes the margin robust to the scale of costs.\n3. New Coupling: Calculate a dynamic temperature `temp_cost` as the standard deviation of the `delta_logp` values across the batch. Clamp it to a minimum value for stability.\n4. Inherited from Parent 1: Create a non-saturating, adaptive margin by applying a `softplus` function to the scaled, normalized cost difference: `margin = alpha * softplus(delta_cost_norm / temp_cost)`.\n5. New Coupling: Calculate the model's preference term using `logsigmoid(delta_logp)`. This bounds the term and improves stability.\n6. The final loss for each sample is `- (preference_term - margin)`. This encourages the log-sigmoid of the preference to be high, while penalizing it by an amount proportional to the adaptive margin. A larger margin acts as a regularizer, pushing the model to be more confident in its preference.\n7. Return the weighted mean of the loss over the batch.", "hyperparams": {"alpha": 0.1, "min_temp": 0.1}, "operators_used": ["zscore", "softplus", "logsigmoid", "clamp"], "implementation_hint": {"expects": ["cost_a", "cost_b", "logp_a", "logp_b"], "returns": "scalar"}, "code": "import torch\nimport torch.nn.functional as F\n\ndef generated_loss(batch, model_output, extra):\n    \"\"\"\n    Computes the Softplus Margin with Z-Score Normalization and Log-Sigmoid Preference.\n\n    Inherited Ideas:\n    - Z-score normalization of cost differences for scale invariance (from Parent 0).\n    - Softplus transformation to create a non-saturating margin (from Parent 1).\n\n    New Coupling Ideas:\n    1. Log-sigmoid on the log-probability difference for a bounded, stable preference term.\n    2. Dynamic temperature `temp_cost` based on the standard deviation of `delta_logp`,\n       coupling the margin's steepness to the model's current output variance.\n    \"\"\"\n    # For preference learning, we assume the dataset provides pairs where one is strictly better.\n    cost_w = torch.min(batch['cost_a'], batch['cost_b'])\n    cost_l = torch.max(batch['cost_a'], batch['cost_b'])\n    log_prob_w = batch['log_prob_w']\n    log_prob_l = batch['log_prob_l']\n\n    # Retrieve hyperparameters\n    alpha = extra.get('alpha', 0.1)\n    min_temp = extra.get('min_temp', 0.1)\n\n    # 1. Calculate cost and log probability differences\n    delta_cost = cost_l - cost_w\n    delta_logp = log_prob_w - log_prob_l\n\n    # Early exit for a batch with no cost difference to avoid division by zero in z-score\n    if torch.all(delta_cost < 1e-8):\n        # If no cost difference, the margin should be zero.\n        # The loss is just the negative log-likelihood of the preferences.\n        loss = -F.logsigmoid(delta_logp)\n    else:\n        # 2. Inherited from Parent 0: Z-score normalization of cost differences\n        delta_cost_mean = delta_cost.mean()\n        delta_cost_std = delta_cost.std().clamp(min=1e-8) # Avoid division by zero\n        delta_cost_norm = (delta_cost - delta_cost_mean) / delta_cost_std\n\n        # 3. New Coupling: Dynamic temperature based on logp standard deviation\n        with torch.no_grad():\n            temp_cost = delta_logp.std().clamp(min=min_temp)\n\n        # 4. Inherited from Parent 1: Create a non-saturating margin using softplus\n        margin = alpha * F.softplus(delta_cost_norm / temp_cost)\n\n        # 5. New Coupling: Calculate preference term using logsigmoid for stability\n        preference_term = F.logsigmoid(delta_logp)\n\n        # 6. Combine into final loss. We want to maximize `preference_term - margin`,\n        # so we minimize its negative.\n        loss = -(preference_term - margin)\n\n    # 7. Apply optional weights and compute the mean.\n    weights = batch.get('weight', None)\n    if weights is not None:\n        loss = loss * weights\n        return loss.sum() / weights.sum().clamp(min=1e-8)\n    else:\n        return loss.mean()"}, "static_ok": true, "static_reason": "", "dynamic_ok": true, "dynamic_reason": "ok", "loss_value": 0.7624619007110596, "grad_norm": 0.0}
{"generation": 6, "index": 1, "ir": {"name": "Rank-Modulated Bradley-Terry Loss with Z-Score Cost Signal", "intuition": "This loss function creates a hybrid between a Bradley-Terry style logistic loss and an adaptive margin loss, incorporating ideas from both parents while introducing a new coupling mechanism.\n\nInherited Ideas:\n- From Parent 1 (Softplus Adaptive Margin Loss), it inherits the core `softplus` structure for the loss calculation, specifically `softplus(margin - delta_logp)`. This provides a smooth and non-negative hinge-like loss.\n- From Parent 0 (Adaptive Margin Loss with Z-Score), it inherits the idea of using Z-score normalization on the batch-wise cost differences (`delta_cost`) to create a scale-invariant cost signal. This makes the loss less sensitive to the absolute magnitude of costs in a given batch.\n\nNew Coupling Ideas:\n1.  **Exponential Cost Signal**: Instead of using the normalized cost signal directly as a linear margin (e.g., `alpha * signal`), the child loss uses it as an exponent in a Bradley-Terry-like formulation: `loss = logsigmoid(delta_logp - exp(alpha * zscore(delta_cost)))`. This can be rewritten as `softplus(exp(alpha * zscore(delta_cost)) - delta_logp)`, connecting it to the inherited `softplus` structure. This coupling creates a margin that grows exponentially with the normalized cost difference, pushing the model much harder to separate pairs with significantly different costs, while having a gentler effect on pairs with similar costs.\n2.  **Rank Gap Modulation**: The child also inherits the rank gap modulation from Parent 0, but applies it in a novel way. The term `rank_gap = 1.0 - exp(-beta * rank_diff)` is used to modulate the strength of the exponential cost signal itself (`alpha * rank_gap_weight * zscore(delta_cost)`). This means that the exponential growth of the margin is amplified for pairs that are far apart in the batch's cost ranking. It focuses the strong exponential penalty on pairs that are not just different in cost value, but also significantly separated in their relative ordering within the batch.", "pseudocode": "1. For each pair (w, l) in the batch, where cost(w) < cost(l), calculate the cost difference `delta_cost = cost(l) - cost(w)` and the log probability difference `delta_logp = logp(w) - logp(l)`.\n2. Standardize the `delta_cost` vector for the entire batch using Z-score normalization to get `delta_cost_norm`. Clamp this to a reasonable range (e.g., [-3, 3]) to prevent outliers from creating extreme values.\n3. Calculate a 'rank gap' weight for each pair. First, find the rank of `cost_w` and `cost_l` within the sorted costs of the entire batch. Then compute `rank_gap_weight = 1.0 - exp(-beta * abs(rank(cost_l) - rank(cost(w))))`.\n4. Create a rank-modulated cost signal by multiplying the normalized cost difference by the rank gap weight: `modulated_signal = alpha * rank_gap_weight * delta_cost_norm`.\n5. Compute an exponential margin from this signal: `exp_margin = exp(modulated_signal)`. This margin grows non-linearly, aggressively penalizing large, rank-separated cost differences.\n6. Calculate the per-sample loss using a softplus function, which is equivalent to a logistic loss: `loss = softplus(exp_margin - delta_logp)`.\n7. Return the weighted mean of the loss over the batch.", "hyperparams": {"alpha": 1.0, "beta": 0.05, "z_clamp": 3.0}, "operators_used": ["softplus", "exp", "zscore", "clamp", "rank_gap"], "implementation_hint": {"expects": ["cost_a", "cost_b", "logp_w", "logp_l"], "returns": "scalar"}, "code": "import torch\nimport torch.nn.functional as F\n\ndef generated_loss(batch, model_output, extra):\n    \"\"\"\n    Computes the Rank-Modulated Bradley-Terry Loss with Z-Score Cost Signal.\n\n    Inherits:\n    - The softplus(margin - delta_logp) structure (Parent 1).\n    - Z-score normalization of cost differences for a scale-invariant signal (Parent 0).\n    \n    Introduces:\n    1. An exponential margin `exp(alpha * ...)` creating a Bradley-Terry style loss where the preference strength grows exponentially with the cost difference.\n    2. Coupling the rank gap weight (from Parent 0) directly into the exponent to modulate the strength of this exponential margin.\n    \"\"\"\n    # For preference learning, we assume the dataset provides pairs where one is strictly better.\n    cost_w = torch.min(batch['cost_a'], batch['cost_b'])\n    cost_l = torch.max(batch['cost_a'], batch['cost_b'])\n    log_prob_w = batch['log_prob_w']\n    log_prob_l = batch['log_prob_l']\n\n    # Retrieve hyperparameters\n    alpha = extra.get('alpha', 1.0)\n    beta = extra.get('beta', 0.05)\n    z_clamp = extra.get('z_clamp', 3.0)\n\n    # 1. Calculate cost and log probability differences\n    delta_cost = cost_l - cost_w\n    delta_logp = log_prob_w - log_prob_l\n\n    # Early exit for a batch with no cost difference to avoid division by zero\n    if torch.all(delta_cost < 1e-6):\n        return torch.tensor(0.0, device=delta_cost.device)\n\n    # 2. Inherited: Z-score normalization of cost differences for batch-wise scale invariance\n    delta_cost_mean = delta_cost.mean()\n    delta_cost_std = delta_cost.std().clamp(min=1e-8)\n    delta_cost_norm = (delta_cost - delta_cost_mean) / delta_cost_std\n    delta_cost_clamped = torch.clamp(delta_cost_norm, -z_clamp, z_clamp)\n\n    # 3. New Coupling Part 1: Calculate rank gap modulation weight\n    all_costs = torch.cat([cost_w, cost_l], dim=0)\n    # Use a stable sort for deterministic ranks\n    sorted_costs, _ = torch.sort(all_costs, stable=True)\n    rank_w = torch.searchsorted(sorted_costs, cost_w)\n    rank_l = torch.searchsorted(sorted_costs, cost_l)\n    rank_diff = (rank_l - rank_w).float()\n    # rank_gap_weight approaches 1 as rank_diff increases\n    rank_gap_weight = 1.0 - torch.exp(-beta * rank_diff)\n\n    # 4. New Coupling Part 2: Modulate the normalized signal with the rank gap weight\n    # This directly couples the rank information into the margin calculation\n    modulated_signal = alpha * rank_gap_weight.detach() * delta_cost_clamped\n\n    # 5. New Coupling Part 3: Compute an exponential margin\n    # This creates a Bradley-Terry-like term where preference strength grows exponentially\n    exp_margin = torch.exp(modulated_signal)\n\n    # 6. Inherited: Compute the core loss using softplus\n    # This is equivalent to -log_sigmoid(delta_logp - log(exp_margin)) but more stable\n    # The final form is softplus(margin - delta_logp)\n    per_sample_loss = F.softplus(exp_margin - delta_logp)\n\n    # 7. Apply optional weights and compute the mean.\n    weights = batch.get('weight', None)\n    if weights is not None:\n        per_sample_loss = per_sample_loss * weights\n        return per_sample_loss.sum() / weights.sum().clamp(min=1e-8)\n    else:\n        return per_sample_loss.mean()"}, "static_ok": true, "static_reason": "", "dynamic_ok": true, "dynamic_reason": "ok", "loss_value": 1.31326162815094, "grad_norm": 0.0}
{"generation": 6, "index": 2, "ir": {"name": "Softplus-Sigmoid Adaptive Margin Loss with Z-Score Normalization", "intuition": "This loss function creates a robust, adaptive preference loss by merging concepts from its parents and introducing a new coupling mechanism.\n\nInherited Ideas:\n- From Parent 0, it inherits the use of Z-score normalization for the batch-wise cost differences (`delta_cost`). This makes the margin calculation invariant to the absolute scale of costs within a batch, improving stability and reducing hyperparameter sensitivity.\n- From Parent 1, it inherits the use of `softplus` to create a non-saturating, unbounded 'cost signal' from the (now normalized) cost differences. This ensures that larger cost gaps translate into proportionally larger target margins without a ceiling effect.\n- From both parents, it adopts the fundamental adaptive margin structure: `loss = softplus(margin - delta_logp)`.\n\nNew Coupling Ideas:\n1.  **Sigmoid-Modulated Margin**: A new coupling mechanism is introduced where the `alpha` hyperparameter, which scales the margin, is itself modulated by a sigmoid function of the log-probability difference (`delta_logp`). The margin is now calculated as `margin = (alpha * sigmoid(delta_logp)) * cost_signal`. This creates a dynamic scaling effect: when the model is already confident and correct (`delta_logp` is large and positive), the sigmoid term approaches 1, applying the full margin pressure. Conversely, when the model is very wrong (`delta_logp` is large and negative), the sigmoid term approaches 0, reducing the margin and thus the loss gradient. This acts as a stability trick, preventing excessively large gradients from incorrect but confident predictions and focusing learning on pairs where the model is uncertain or slightly wrong.\n2.  **Clamped Z-Score**: To further enhance stability, the Z-scored cost differences are clamped to a reasonable range (e.g., [-3, 3]) before being passed to the `softplus` function. This prevents extreme cost outliers within a batch from creating disproportionately large margin targets and loss values.", "pseudocode": "1. For each pair (w, l) in the batch where cost(w) < cost(l), calculate the cost difference `delta_cost = cost(l) - cost(w)` and log probability difference `delta_logp = logp(w) - logp(l)`.\n2. Inherited Idea (Parent 0): Standardize the `delta_cost` vector for the entire batch using Z-score normalization to get `delta_cost_norm`.\n3. New Coupling Idea: Clamp the `delta_cost_norm` vector to a fixed range (e.g., [-3, 3]) to mitigate the effect of extreme outliers.\n4. Inherited Idea (Parent 1): Transform the clamped, normalized cost differences into a non-saturating signal using `softplus`: `cost_signal = softplus(clamped_delta_cost_norm / temp_cost)`.\n5. New Coupling Idea: Create a dynamic margin scalar by modulating the `alpha` hyperparameter with a sigmoid function of the model's current preference: `dynamic_alpha = alpha * sigmoid(delta_logp)`.\n6. Combine the cost signal and the dynamic scalar to form the final adaptive margin: `margin = dynamic_alpha * cost_signal`.\n7. Compute the per-sample loss using the core structure from both parents: `loss = softplus(margin - delta_logp)`.\n8. Return the weighted mean of the loss over the batch.", "hyperparams": {"alpha": 1.0, "temp_cost": 1.0, "z_clamp": 3.0}, "operators_used": ["softplus", "sigmoid", "zscore", "clamp"], "implementation_hint": {"expects": ["cost_a", "cost_b", "logp_a", "logp_b"], "returns": "scalar"}, "code": "import torch\nimport torch.nn.functional as F\n\ndef generated_loss(batch, model_output, extra):\n    \"\"\"\n    Computes the Softplus-Sigmoid Adaptive Margin Loss with Z-Score Normalization.\n\n    Inherits Z-score normalization from Parent 0 and the use of softplus for the cost signal from Parent 1.\n    Introduces two new couplings:\n    1. A sigmoid modulation of the margin based on the current log-probability difference (delta_logp),\n       which dampens the loss for very incorrect predictions, improving stability.\n    2. Clamping of the Z-scored costs before the softplus transformation to handle outliers.\n    \"\"\"\n    # For preference learning, we assume the dataset provides pairs where one is strictly better.\n    cost_w = torch.min(batch['cost_a'], batch['cost_b'])\n    cost_l = torch.max(batch['cost_a'], batch['cost_b'])\n    log_prob_w = batch['log_prob_w']\n    log_prob_l = batch['log_prob_l']\n\n    # Retrieve hyperparameters\n    alpha = extra.get('alpha', 1.0)\n    temp_cost = extra.get('temp_cost', 1.0)\n    z_clamp = extra.get('z_clamp', 3.0)\n\n    # 1. Calculate cost and log probability differences\n    delta_cost = cost_l - cost_w\n    delta_logp = log_prob_w - log_prob_l\n\n    # Early exit if there's no cost variation to prevent division by zero in z-score\n    if delta_cost.std() < 1e-8:\n        # If no cost difference, the ideal margin is 0. The loss is softplus(-delta_logp).\n        # This is equivalent to standard logistic loss.\n        loss = F.softplus(-delta_logp)\n    else:\n        # 2. Inherited (Parent 0): Z-score normalization of cost differences\n        delta_cost_mean = delta_cost.mean()\n        delta_cost_std = delta_cost.std()\n        delta_cost_norm = (delta_cost - delta_cost_mean) / delta_cost_std\n\n        # 3. New Coupling: Clamp the normalized costs to prevent outliers from creating extreme margins\n        delta_cost_clamped = torch.clamp(delta_cost_norm, -z_clamp, z_clamp)\n\n        # 4. Inherited (Parent 1): Create a non-saturating cost signal using softplus\n        cost_signal = F.softplus(delta_cost_clamped / temp_cost)\n\n        # 5. New Coupling: Modulate the margin scale with a sigmoid of the model's prediction\n        # This reduces the margin (and gradient) for very wrong predictions (large negative delta_logp)\n        dynamic_alpha = alpha * torch.sigmoid(delta_logp.detach()) # Detach to treat as a dynamic weight\n\n        # 6. Calculate the final adaptive margin\n        adaptive_margin = dynamic_alpha * cost_signal\n\n        # 7. Compute the core loss using softplus (from both parents)\n        loss = F.softplus(adaptive_margin - delta_logp)\n\n    # Apply optional weights and compute the mean.\n    weights = batch.get('weight', None)\n    if weights is not None:\n        loss = loss * weights\n        return loss.sum() / weights.sum().clamp(min=1e-8)\n    else:\n        return loss.mean()"}, "static_ok": true, "static_reason": "", "dynamic_ok": true, "dynamic_reason": "ok", "loss_value": 0.6931471824645996, "grad_norm": 0.0}
{"generation": 6, "index": 3, "ir": {"name": "Softplus-Sigmoid Adaptive Margin Loss", "intuition": "This child loss function creates a robust, adaptive preference loss by merging concepts from its parents and introducing a new coupling for stability and control.\n\nInherited Ideas:\n- From Parent 1 (`Softplus Adaptive Margin Loss`), it inherits the use of `softplus` to transform the cost difference (`delta_cost`) into a non-saturating, non-negative signal. This ensures that as the cost gap between two solutions widens, the target margin for the model's preference also increases in a smooth, unbounded way.\n- From Parent 0 (`Adaptive Margin Loss with Z-Score...`), it inherits the core loss structure of `softplus(margin - delta_logp)`. This smooth hinge loss penalizes the model when its log-probability difference (`delta_logp`) falls short of the target margin.\n\nNew Coupling Ideas:\n1.  **Log-Sigmoid Transformation of Log-Probabilities**: Instead of directly using the raw `delta_logp`, the loss first applies a `logsigmoid` transformation: `log_preference_ratio = logsigmoid(delta_logp)`. This maps the unbounded `delta_logp` (from -inf to +inf) to a bounded range (from -inf to 0), effectively representing the log-probability of preferring the winning solution. This transformation stabilizes the loss calculation by preventing extremely large `delta_logp` values from causing instability or vanishing gradients, acting as a soft clamp on the model's output within the loss function itself.\n2.  **Margin as a Target Log-Preference Ratio**: The adaptive margin, derived from the `softplus` of the cost difference, is now interpreted as a target for this new `log_preference_ratio` rather than for the raw `delta_logp`. The final loss becomes `softplus(margin - logsigmoid(delta_logp))`. This re-frames the learning objective: the model is pushed to make its log-preference ratio for the correct pair at least as high as the margin dictated by the cost difference.", "pseudocode": "1. For each pair (w, l) in the batch, where cost(w) < cost(l), calculate the cost difference `delta_cost = cost(l) - cost(w)` and the log probability difference `delta_logp = logp(w) - logp(l)`.\n2. (Inherited from Parent 1) Transform the cost difference using a `softplus` function to create a smooth, non-negative, and non-saturating 'cost signal': `cost_signal = softplus(delta_cost / temp_cost)`.\n3. Calculate the adaptive margin, which is proportional to this cost signal: `margin = alpha * cost_signal`.\n4. (New Coupling) Transform the model's log-probability difference using the `logsigmoid` function to get a bounded log-preference ratio: `log_preference_ratio = logsigmoid(delta_logp)`.\n5. (Inherited from Parent 0 & New Coupling) Compute the loss as the softplus of the difference between the target margin and the model's log-preference ratio: `loss = softplus(margin - log_preference_ratio)`.\n6. Return the weighted mean of the loss over the batch.", "hyperparams": {"alpha": 0.5, "temp_cost": 1.0}, "operators_used": ["softplus", "logsigmoid"], "implementation_hint": {"expects": ["cost_a", "cost_b", "logp_a", "logp_b"], "returns": "scalar"}, "code": "import torch\nimport torch.nn.functional as F\n\ndef generated_loss(batch, model_output, extra):\n    \"\"\"\n    Computes the Softplus-Sigmoid Adaptive Margin Loss.\n\n    This loss function combines ideas from its parents to create a stable and adaptive loss.\n    - Inherits the use of `softplus` on the cost difference to create a non-saturating margin (from Parent 1).\n    - Inherits the `softplus(margin - prediction)` core loss structure (from Parent 0).\n    - Introduces a new coupling: it uses `logsigmoid` to transform the model's log-probability difference,\n      stabilizing the loss by mapping the unbounded `delta_logp` to the range (-inf, 0].\n    \"\"\"\n    # For preference learning, we assume the dataset provides pairs where one is strictly better.\n    # cost_w < cost_l\n    cost_w = torch.min(batch['cost_a'], batch['cost_b'])\n    cost_l = torch.max(batch['cost_a'], batch['cost_b'])\n\n    # The 'log_prob_w' and 'log_prob_l' are assumed to be provided directly,\n    # corresponding to the winner and loser solutions respectively.\n    log_prob_w = batch['log_prob_w']\n    log_prob_l = batch['log_prob_l']\n\n    # Retrieve hyperparameters\n    alpha = extra.get('alpha', 0.5)\n    temp_cost = extra.get('temp_cost', 1.0)\n\n    # 1. Calculate cost and log probability differences\n    # delta_cost is guaranteed to be >= 0\n    delta_cost = cost_l - cost_w\n    delta_logp = log_prob_w - log_prob_l\n\n    # 2. Inherited Idea (Parent 1): Transform the cost difference into a smooth, non-negative 'cost signal'.\n    # Using softplus provides a non-saturating signal that grows for large cost differences.\n    cost_signal = F.softplus(delta_cost / temp_cost)\n\n    # 3. Create an adaptive margin based on the cost signal.\n    # The target separation is proportional to how different the costs are.\n    adaptive_margin = alpha * cost_signal\n\n    # 4. New Coupling Idea: Transform the model's log-prob difference using `logsigmoid`.\n    # This maps the unbounded delta_logp to a bounded range (-inf, 0], which stabilizes the loss\n    # by preventing extreme values from causing numerical issues.\n    log_preference_ratio = F.logsigmoid(delta_logp)\n\n    # 5. Inherited Idea (Parent 0) & New Coupling: Compute the core loss using softplus.\n    # The loss is low if log_preference_ratio > adaptive_margin.\n    # Since log_preference_ratio is always <= 0, the margin here is effectively pushing the model's\n    # preference for the winner to be very high (close to 1 in probability space).\n    per_sample_loss = F.softplus(adaptive_margin - log_preference_ratio)\n\n    # 6. Apply optional weights and compute the mean.\n    weights = batch.get('weight', None)\n    if weights is not None:\n        per_sample_loss = per_sample_loss * weights\n        # Use a small epsilon to prevent division by zero if all weights are zero.\n        return per_sample_loss.sum() / weights.sum().clamp(min=1e-8)\n    else:\n        return per_sample_loss.mean()"}, "static_ok": true, "static_reason": "", "dynamic_ok": true, "dynamic_reason": "ok", "loss_value": 1.5803322792053223, "grad_norm": 0.0}
{"generation": 6, "index": 4, "ir": {"name": "Sigmoid-Gated Adaptive Margin Loss with Rank Gap", "intuition": "This loss function creates a robust, adaptive preference loss by combining ideas from its parents and introducing a new gating mechanism.\n\nInherited Ideas:\n- From both parents, it inherits the core structure of an adaptive margin loss: `loss = softplus(margin - delta_logp)`. This penalizes the model when its log-probability difference for a preferred solution falls short of a target margin.\n- From Parent 0 (`Adaptive Margin Loss with Z-Score Normalization and Rank Gap`), it inherits the idea of modulating the final loss by a `rank_gap` term. This up-weights the loss for pairs that are far apart in the batch's cost ranking, encouraging the model to prioritize learning more significant preference distinctions.\n- From Parent 1 (`Softplus Adaptive Margin Loss`), it inherits the use of `softplus(delta_cost)` to create a smooth, non-saturating signal from the cost difference. This ensures the margin grows as the cost gap widens, without the upper bound imposed by a `tanh` function.\n\nNew Coupling Ideas:\n1.  **Sigmoid Gating of the Margin**: The adaptive margin, derived from `softplus(delta_cost)`, is multiplied by a sigmoid-transformed version of the log-probability difference (`delta_logp`). The margin is calculated as `margin = alpha * softplus(delta_cost) * sigmoid(-gamma * delta_logp)`. This creates a dynamic margin that is largest when the model is most 'confused' (i.e., `delta_logp` is close to zero or negative). When the model is already correctly preferring the winner by a large margin (`delta_logp` is large and positive), the sigmoid gate approaches zero, effectively reducing the target margin and preventing the loss from punishing the model for not achieving an unnecessarily large separation. This focuses the learning on difficult or incorrectly ranked pairs.\n2.  **Clamping the Cost Signal**: The `softplus(delta_cost)` signal is clamped to a maximum value (`margin_clamp`). This prevents extremely large cost differences (outliers) from creating an unbounded, numerically unstable margin target, adding robustness to the loss calculation.", "pseudocode": "1. For each pair (w, l) in the batch, where cost(w) < cost(l), calculate the cost difference `delta_cost = cost(l) - cost(w)` and the log probability difference `delta_logp = logp(w) - logp(l)`.\n2. Inherited from Parent 1: Create a smooth, non-saturating cost signal using `softplus`: `cost_signal = softplus(delta_cost / temp_cost)`.\n3. New Coupling Idea: Clamp the cost signal to a maximum value to prevent outliers from creating excessively large margins: `clamped_cost_signal = clamp(cost_signal, max=margin_clamp)`.\n4. New Coupling Idea: Create a sigmoid gate that is sensitive to the model's current preference. The gate value is high when the model is wrong or uncertain (`delta_logp` is small/negative) and low when the model is confident and correct (`delta_logp` is large/positive): `gate = sigmoid(-gamma * delta_logp)`.\n5. Calculate the dynamic, gated margin by combining the clamped cost signal and the sigmoid gate: `adaptive_margin = alpha * clamped_cost_signal * gate`.\n6. Compute the per-sample loss using the core structure from both parents: `base_loss = softplus(adaptive_margin - delta_logp)`.\n7. Inherited from Parent 0: Calculate a rank gap weight. First, find the rank of `cost_w` and `cost_l` within the sorted costs of the entire batch. Then compute `rank_gap_weight = 1.0 - exp(-beta * abs(rank(cost_l) - rank(cost_w)))`.\n8. Modulate the base loss with the rank gap weight: `final_loss = base_loss * rank_gap_weight`.\n9. Return the weighted mean of `final_loss` over the batch.", "hyperparams": {"alpha": 2.0, "temp_cost": 1.0, "gamma": 0.5, "beta": 0.05, "margin_clamp": 10.0}, "operators_used": ["softplus", "sigmoid", "clamp", "exp", "rank_gap"], "implementation_hint": {"expects": ["cost_a", "cost_b", "logp_w", "logp_l"], "returns": "scalar"}, "code": "import torch\nimport torch.nn.functional as F\n\ndef generated_loss(batch, model_output, extra):\n    \"\"\"\n    Computes the Sigmoid-Gated Adaptive Margin Loss with Rank Gap.\n\n    Inherits:\n    - The softplus(margin - delta_logp) structure from both parents.\n    - The use of softplus(delta_cost) for a non-saturating cost signal from Parent 1.\n    - The use of a rank_gap modulator from Parent 0.\n\n    Introduces:\n    1. A sigmoid gate on the margin, which reduces the target margin when the model is already confident,\n       focusing learning on difficult pairs.\n    2. Clamping of the cost signal to prevent outliers from creating extreme margins.\n    \"\"\"\n    # For preference learning, we assume the dataset provides pairs where one is strictly better.\n    cost_w = torch.min(batch['cost_a'], batch['cost_b'])\n    cost_l = torch.max(batch['cost_a'], batch['cost_b'])\n    log_prob_w = batch['log_prob_w']\n    log_prob_l = batch['log_prob_l']\n\n    # Retrieve hyperparameters\n    alpha = extra.get('alpha', 2.0)\n    temp_cost = extra.get('temp_cost', 1.0)\n    gamma = extra.get('gamma', 0.5)\n    beta = extra.get('beta', 0.05)\n    margin_clamp = extra.get('margin_clamp', 10.0)\n\n    # 1. Calculate cost and log probability differences\n    delta_cost = cost_l - cost_w\n    delta_logp = log_prob_w - log_prob_l\n\n    # 2. Inherited (Parent 1): Create a non-saturating cost signal with softplus\n    cost_signal = F.softplus(delta_cost / temp_cost)\n\n    # 3. New Coupling: Clamp the cost signal for stability against outliers\n    clamped_cost_signal = torch.clamp(cost_signal, max=margin_clamp)\n\n    # 4. New Coupling: Create a sigmoid gate based on model's current preference (delta_logp)\n    # Gate is ~1 when delta_logp is negative (wrong preference) or small (uncertain).\n    # Gate is ~0 when delta_logp is large and positive (correct preference).\n    gate = torch.sigmoid(-gamma * delta_logp)\n\n    # 5. Calculate the dynamic, gated adaptive margin\n    adaptive_margin = alpha * clamped_cost_signal * gate.detach() # Detach gate to not penalize high delta_logp\n\n    # 6. Inherited (Both Parents): Compute the base per-sample loss\n    base_loss = F.softplus(adaptive_margin - delta_logp)\n\n    # 7. Inherited (Parent 0): Calculate rank gap modulation\n    # This requires ranking all costs in the batch\n    all_costs = torch.cat([cost_w, cost_l], dim=0)\n    sorted_costs, _ = torch.sort(all_costs)\n\n    # Find the rank of each cost_w and cost_l in the sorted list\n    rank_w = torch.searchsorted(sorted_costs, cost_w)\n    rank_l = torch.searchsorted(sorted_costs, cost_l)\n    rank_diff = (rank_l - rank_w).float()\n\n    # rank_gap_weight approaches 1 as rank_diff increases\n    rank_gap_weight = 1.0 - torch.exp(-beta * rank_diff)\n\n    # 8. Modulate the base loss with the rank gap weight\n    final_loss = base_loss * rank_gap_weight.detach() # Detach to treat as a pure weighting factor\n\n    # 9. Apply optional weights and compute the mean.\n    weights = batch.get('weight', None)\n    if weights is not None:\n        final_loss = final_loss * weights\n        return final_loss.sum() / weights.sum().clamp(min=1e-8)\n    else:\n        return final_loss.mean()"}, "static_ok": true, "static_reason": "", "dynamic_ok": true, "dynamic_reason": "ok", "loss_value": 0.8543357253074646, "grad_norm": 0.0}
{"generation": 6, "index": 5, "ir": {"name": "Softplus-LogSigmoid Preference Loss with Rank-Gap Normalization", "intuition": "This loss function creates a robust preference learning objective by combining the strengths of its parents and introducing novel coupling mechanisms for stability and adaptivity.\n\nInherited Ideas:\n- From Parent 1 (Softplus Adaptive Margin Loss), it inherits the use of `softplus` to transform the raw cost difference (`delta_cost`) into a smooth, non-saturating, and unbounded adaptive margin. This ensures that as the cost gap between two solutions widens, the learning signal continues to grow, pushing the model to create a correspondingly larger separation in log-probabilities.\n- From Parent 0 (Adaptive Margin Loss with Z-Score...), it inherits the use of a `rank_gap` modulator. This term weights the loss based on the relative ranking of the solutions within the batch, forcing the model to prioritize learning the order of significantly different solutions over those that are already close in cost.\n\nNew Coupling Ideas:\n1. **LogSigmoid Core Loss**: Instead of the common `softplus(margin - delta_logp)` structure, this loss uses a `logsigmoid` formulation: `loss = -logsigmoid(delta_logp - margin)`. This is mathematically equivalent to `softplus(margin - delta_logp)` but is often more numerically stable, especially when the argument to the function becomes very large or small. It frames the problem as maximizing the log-likelihood of the preferred solution's log-probability being greater than the loser's by at least the target margin.\n2. **Rank-Gap Normalization of Margin**: The adaptive margin, calculated from the `softplus` of the cost difference, is normalized by the batch's average rank gap. The margin becomes `margin = alpha * cost_signal / (mean(rank_gap) + epsilon)`. This coupling makes the `alpha` hyperparameter less sensitive to the specific cost distribution of a batch. In batches where solutions are far apart in rank (high mean rank gap), the margin is scaled down, preventing excessively large loss values. Conversely, in batches where solutions are clustered together (low mean rank gap), the margin is scaled up to provide a stronger learning signal.", "pseudocode": "1. For each pair (w, l) in the batch, where cost(w) < cost(l), calculate the cost difference `delta_cost = cost(l) - cost(w)` and the log probability difference `delta_logp = logp(w) - logp(l)`.\n2. Inherited from Parent 1: Transform the `delta_cost` into a smooth, non-negative, and non-saturating signal using `cost_signal = softplus(delta_cost / temp_cost)`.\n3. Inherited from Parent 0: Calculate a 'rank gap' weight for each pair. First, find the rank of `cost_w` and `cost_l` within the sorted costs of the entire batch. Then compute `rank_gap = abs(rank(cost_l) - rank(cost_w))`.\n4. New Coupling (Rank-Gap Normalization): Calculate the mean rank gap across the batch. Normalize the `cost_signal` by this mean rank gap to create the final adaptive margin: `margin = alpha * cost_signal / (mean(rank_gap) + epsilon)`.\n5. New Coupling (LogSigmoid Core Loss): Compute the base per-sample loss using the `logsigmoid` function, which is a numerically stable equivalent to `softplus(margin - x)`: `base_loss = -logsigmoid(delta_logp - margin)`.\n6. Modulate the base loss by the individual `rank_gap` for each pair. This focuses the model on pairs that are far apart in the batch's cost ranking: `final_loss = base_loss * rank_gap`.\n7. Return the weighted mean of `final_loss` over the batch.", "hyperparams": {"alpha": 1.0, "temp_cost": 1.0, "epsilon": 1e-06}, "operators_used": ["softplus", "logsigmoid", "rank_gap"], "implementation_hint": {"expects": ["cost_a", "cost_b", "logp_a", "logp_b"], "returns": "scalar"}, "code": "import torch\nimport torch.nn.functional as F\n\ndef generated_loss(batch, model_output, extra):\n    \"\"\"\n    Computes the Softplus-LogSigmoid Preference Loss with Rank-Gap Normalization.\n\n    Inherits:\n    - `softplus` on `delta_cost` to create a non-saturating margin (from Parent 1).\n    - `rank_gap` modulation to focus on significantly different pairs (from Parent 0).\n\n    Introduces:\n    1. A `logsigmoid` core loss for numerical stability: `-logsigmoid(delta_logp - margin)`.\n    2. Normalization of the adaptive margin by the batch's average rank gap, making `alpha` more robust.\n    \"\"\"\n    # For preference learning, we assume the dataset provides pairs where one is strictly better.\n    cost_w = torch.min(batch['cost_a'], batch['cost_b'])\n    cost_l = torch.max(batch['cost_a'], batch['cost_b'])\n    log_prob_w = batch['log_prob_w']\n    log_prob_l = batch['log_prob_l']\n\n    # Retrieve hyperparameters\n    alpha = extra.get('alpha', 1.0)\n    temp_cost = extra.get('temp_cost', 1.0)\n    epsilon = extra.get('epsilon', 1e-6)\n\n    # 1. Calculate cost and log probability differences\n    delta_cost = cost_l - cost_w\n    delta_logp = log_prob_w - log_prob_l\n\n    # 2. Inherited: Create a non-saturating cost signal using softplus\n    cost_signal = F.softplus(delta_cost / temp_cost)\n\n    # 3. Inherited: Calculate rank gap\n    # This requires ranking all costs in the batch\n    all_costs = torch.cat([cost_w, cost_l], dim=0)\n    sorted_costs, _ = torch.sort(all_costs)\n    \n    # Find the rank of each cost_w and cost_l in the sorted list\n    rank_w = torch.searchsorted(sorted_costs, cost_w)\n    rank_l = torch.searchsorted(sorted_costs, cost_l)\n    \n    rank_gap = (rank_l - rank_w).float().clamp(min=0)\n\n    # 4. New Coupling: Normalize margin by the mean rank gap of the batch\n    mean_rank_gap = rank_gap.mean().detach()\n    adaptive_margin = alpha * cost_signal / (mean_rank_gap + epsilon)\n\n    # 5. New Coupling: Use numerically stable logsigmoid loss formulation\n    # -logsigmoid(x) is equivalent to softplus(-x)\n    # We want softplus(margin - delta_logp), which is softplus(-(delta_logp - margin))\n    # So, we compute -logsigmoid(delta_logp - margin)\n    base_loss = -F.logsigmoid(delta_logp - adaptive_margin)\n\n    # 6. Modulate the base loss with the rank gap weight\n    # Using the raw rank_gap as a linear weight.\n    final_loss = base_loss * rank_gap.detach() # Detach to treat as a pure weighting factor\n\n    # 7. Apply optional weights and compute the mean.\n    weights = batch.get('weight', None)\n    if weights is not None:\n        final_loss = final_loss * weights\n        return final_loss.sum() / weights.sum().clamp(min=1e-8)\n    else:\n        return final_loss.mean()"}, "static_ok": true, "static_reason": "", "dynamic_ok": false, "dynamic_reason": "loss 11.7605 outside soft range [-5.0, 5.0]", "loss_value": 11.760456085205078, "grad_norm": 0.0}
{"generation": 6, "index": 6, "ir": {"name": "Sigmoid-Gated Adaptive Margin Loss with Rank-Gap Annealing", "intuition": "This loss function creates a robust, adaptive preference learning objective by combining ideas from its parents and introducing a novel annealing mechanism.\n\nInherited Ideas:\n- From Parent 1 (Softplus Adaptive Margin Loss), it inherits the core structure of `softplus(margin - delta_logp)`, which provides a smooth hinge loss penalizing the model when its log-probability difference (`delta_logp`) is smaller than a target margin.\n- From Parent 0 (Adaptive Margin Loss with Z-Score Normalization and Rank Gap), it inherits the concept of using a 'rank gap' to modulate the loss. This emphasizes learning the relative ordering of pairs that are far apart in the batch's cost ranking.\n\nNew Coupling Ideas:\n1.  **Sigmoid Gating for Margin**: Instead of using `tanh` or `softplus` directly on the cost difference, this loss uses a `sigmoid` function to create a gate. The adaptive margin is calculated as `alpha * sigmoid((delta_cost - cost_midpoint) / temp_cost) * delta_logp.abs().detach()`. This gates the margin based on whether the cost difference is significant relative to a `cost_midpoint`. The margin also scales with the magnitude of the model's current log-probability difference, focusing the gradient on pairs where the model is already confident but potentially in the wrong direction.\n2.  **Rank-Gap Annealing**: The rank-gap weight, `1 - exp(-beta * rank_diff)`, is used to anneal the loss itself. The final loss is `(1 - rank_gap_weight) * base_loss + rank_gap_weight * logsigmoid(-delta_logp)`. When the rank gap is small (pairs are close in cost), the loss focuses on the adaptive margin (`base_loss`). As the rank gap grows, the loss smoothly transitions towards a simpler Bradley-Terry objective (`logsigmoid(-delta_logp)`), which just enforces the correct preference direction without a complex margin. This annealing stabilizes training by using a simpler objective for easy-to-distinguish pairs.", "pseudocode": "1. For each pair (w, l) where cost(w) < cost(l), calculate the cost difference `delta_cost = cost(l) - cost(w)` and log probability difference `delta_logp = logp(w) - logp(l)`.\n2. Calculate a 'rank gap' weight. First, find the rank of `cost_w` and `cost_l` within the sorted costs of the entire batch. Then compute `rank_gap_weight = 1.0 - exp(-beta * abs(rank(cost_l) - rank(cost_w)))`.\n3. Create a sigmoid-based gate from the cost difference: `gate = sigmoid((delta_cost - cost_midpoint) / temp_cost)`.\n4. Calculate an adaptive margin by scaling the gate with `alpha` and the detached magnitude of the current `delta_logp`: `adaptive_margin = alpha * gate * abs(delta_logp)`.\n5. Compute the primary margin-based loss component using softplus: `base_loss = softplus(adaptive_margin - delta_logp)`.\n6. Compute a secondary, simpler loss component based on the Bradley-Terry model: `bt_loss = -logsigmoid(delta_logp)`.\n7. Combine the two loss components using the rank-gap weight as an annealing factor: `final_loss = (1 - rank_gap_weight) * base_loss + rank_gap_weight * bt_loss`.\n8. Return the weighted mean of `final_loss` over the batch.", "hyperparams": {"alpha": 0.5, "beta": 0.05, "temp_cost": 1.0, "cost_midpoint": 0.0}, "operators_used": ["softplus", "sigmoid", "exp", "logsigmoid", "rank_gap"], "implementation_hint": {"expects": ["cost_a", "cost_b", "log_prob_w", "log_prob_l", "weight"], "returns": "scalar"}, "code": "import torch\nimport torch.nn.functional as F\n\ndef generated_loss(batch, model_output, extra):\n    \"\"\"\n    Computes the Sigmoid-Gated Adaptive Margin Loss with Rank-Gap Annealing.\n\n    Inherits:\n    - The softplus(margin - delta_logp) structure (Parent 1).\n    - The use of a rank-gap signal to modulate the loss (Parent 0).\n\n    New Couplings:\n    1. Sigmoid Gating: The margin is gated by a sigmoid function of the cost difference, creating a soft threshold.\n       The margin also scales with the magnitude of the model's current logp difference.\n    2. Rank-Gap Annealing: The rank-gap weight smoothly interpolates between a margin-based loss for\n       similarly-ranked pairs and a simpler Bradley-Terry loss for pairs that are far apart in rank.\n    \"\"\"\n    cost_w = torch.min(batch['cost_a'], batch['cost_b'])\n    cost_l = torch.max(batch['cost_a'], batch['cost_b'])\n    log_prob_w = batch['log_prob_w']\n    log_prob_l = batch['log_prob_l']\n\n    # Retrieve hyperparameters\n    alpha = extra.get('alpha', 0.5)\n    beta = extra.get('beta', 0.05)\n    temp_cost = extra.get('temp_cost', 1.0)\n    cost_midpoint = extra.get('cost_midpoint', 0.0)\n\n    # 1. Calculate base differences\n    delta_cost = cost_l - cost_w\n    delta_logp = log_prob_w - log_prob_l\n\n    # 2. Inherited Idea: Calculate rank gap modulation weight\n    all_costs = torch.cat([cost_w, cost_l], dim=0)\n    # Use in-place sort for efficiency if memory is a concern, but this is safer\n    sorted_costs, _ = torch.sort(all_costs)\n    rank_w = torch.searchsorted(sorted_costs, cost_w)\n    rank_l = torch.searchsorted(sorted_costs, cost_l)\n    rank_diff = (rank_l - rank_w).float()\n    # rank_gap_weight is close to 0 for small rank_diff, approaches 1 for large rank_diff\n    rank_gap_weight = 1.0 - torch.exp(-beta * rank_diff)\n\n    # 3. New Coupling: Sigmoid Gating for Margin\n    # The gate activates when delta_cost is significantly larger than the midpoint\n    gate = torch.sigmoid((delta_cost - cost_midpoint) / temp_cost.clamp(min=1e-8))\n\n    # 4. New Coupling: Adaptive margin scales with gate and current model belief\n    # Detach delta_logp to ensure the gradient through the margin only depends on the gate\n    adaptive_margin = alpha * gate * delta_logp.abs().detach()\n\n    # 5. Inherited Idea: Primary margin-based loss component\n    base_loss = F.softplus(adaptive_margin - delta_logp)\n\n    # 6. Simpler Bradley-Terry loss component for annealing\n    # Equivalent to softplus(-delta_logp)\n    bt_loss = -F.logsigmoid(delta_logp)\n\n    # 7. New Coupling: Rank-Gap Annealing\n    # Interpolate between the two losses. For pairs with a small rank gap, we use the complex\n    # margin loss. For pairs with a large rank gap, we use the simpler BT loss.\n    # Detach the weight so it only acts as a blending factor, not a source of gradient.\n    final_loss = torch.lerp(base_loss, bt_loss, rank_gap_weight.detach())\n\n    # 8. Apply optional weights and compute the mean.\n    weights = batch.get('weight', None)\n    if weights is not None:\n        final_loss = final_loss * weights\n        return final_loss.sum() / weights.sum().clamp(min=1e-8)\n    else:\n        return final_loss.mean()"}, "static_ok": true, "static_reason": "", "dynamic_ok": false, "dynamic_reason": "forward_error: 'float' object has no attribute 'clamp'", "loss_value": null, "grad_norm": null}
{"generation": 6, "index": 7, "ir": {"name": "Rank-Modulated Bradley-Terry Loss", "intuition": "This loss function creates a hybrid between a Bradley-Terry style logistic loss and an adaptive margin loss, modulated by the relative ranking of solutions within a batch.\n\nInherited Ideas:\n- From `Softplus Adaptive Margin Loss` (Parent 1), it inherits the idea of using the cost difference (`delta_cost`) to create a non-negative, non-saturating signal that scales the loss. Instead of using it to define a margin, we use it to directly scale the log-probability difference.\n- From `Adaptive Margin Loss with Z-Score Normalization and Rank Gap` (Parent 0), it inherits the concept of using the rank difference between the winner and loser solutions as a modulating factor. This `rank_gap` term emphasizes learning from pairs that are far apart in the cost hierarchy of the batch.\n\nNew Coupling Ideas:\n1. **Bradley-Terry with Cost Scaling**: The core loss is a logistic loss, `logsigmoid(x)`, similar to the Bradley-Terry model. However, the input `x` is not just the log-probability difference (`delta_logp`), but a cost-scaled version: `delta_logp * softplus(delta_cost / temp_cost)`. This couples the cost difference directly into the logistic loss, making the loss gradient steeper for pairs with a larger cost gap. It encourages the model to not just get the preference right, but to be much more confident (larger `delta_logp`) when the cost difference is significant.\n2. **Log-Sum-Exp Stabilization on Ranks**: Instead of a simple linear rank difference, the rank gap is calculated using a Log-Sum-Exp (LSE) formulation: `rank_gap = log(1 + exp(beta * (rank_l - rank_w))) / beta`. This is a smooth, differentiable, and numerically stable approximation of `max(0, rank_l - rank_w)`. It provides a non-negative weight that is zero for same-rank pairs and grows smoothly as the ranks diverge, preventing negative weights and ensuring stability.", "pseudocode": "1. For each pair (w, l) where cost(w) < cost(l), calculate the cost difference `delta_cost = cost(l) - cost(w)` and the log probability difference `delta_logp = logp(w) - logp(l)`.\n2. Create a non-saturating, non-negative 'cost signal' from the cost difference: `cost_signal = softplus(delta_cost / temp_cost)`. This is inherited from Parent 1's margin calculation.\n3. Scale the log probability difference by this cost signal: `scaled_delta_logp = delta_logp * cost_signal`.\n4. Calculate a 'rank gap' weight for each pair, inherited from Parent 0. First, find the rank of `cost_w` and `cost_l` within the sorted costs of the entire batch. Then, compute the rank gap using a numerically stable Log-Sum-Exp function: `rank_gap = log(1 + exp(beta * (rank(l) - rank(w)))) / beta`.\n5. Compute the per-sample loss by multiplying the rank gap with a logistic loss applied to the scaled log-probability difference: `loss = rank_gap * logsigmoid(-scaled_delta_logp)`. Note the negation inside `logsigmoid` to ensure the loss decreases as `scaled_delta_logp` increases.\n6. Return the negative of the weighted mean of these loss values. The final negation converts the log-likelihood maximization into a loss minimization problem.", "hyperparams": {"temp_cost": 1.0, "beta": 0.1}, "operators_used": ["softplus", "logsigmoid", "rank_gap", "log", "exp"], "implementation_hint": {"expects": ["cost_a", "cost_b", "logp_a", "logp_b", "weight"], "returns": "scalar"}, "code": "import torch\nimport torch.nn.functional as F\n\ndef generated_loss(batch, model_output, extra):\n    \"\"\"\n    Computes the Rank-Modulated Bradley-Terry Loss.\n\n    Inherits:\n    - The use of a softplus-transformed cost difference to create a non-saturating signal (from Parent 1).\n    - The use of rank difference to modulate the loss importance (from Parent 0).\n\n    New Couplings:\n    1. A Bradley-Terry style logistic loss where the log-probability difference is directly scaled by the cost signal.\n    2. A Log-Sum-Exp formulation for the rank gap for improved numerical stability and smoothness.\n    \"\"\"\n    # Ensure cost_w < cost_l for preference pairs\n    cost_w = torch.min(batch['cost_a'], batch['cost_b'])\n    cost_l = torch.max(batch['cost_a'], batch['cost_b'])\n    log_prob_w = batch['log_prob_w']\n    log_prob_l = batch['log_prob_l']\n\n    # Retrieve hyperparameters\n    temp_cost = extra.get('temp_cost', 1.0)\n    beta = extra.get('beta', 0.1)\n\n    # 1. Calculate cost and log probability differences\n    delta_cost = cost_l - cost_w\n    delta_logp = log_prob_w - log_prob_l\n\n    # 2. Inherited Idea: Create a non-saturating cost signal using softplus\n    cost_signal = F.softplus(delta_cost / temp_cost)\n\n    # 3. New Coupling: Scale the log-prob difference by the cost signal\n    scaled_delta_logp = delta_logp * cost_signal.detach() # Detach to treat it as a target scale\n\n    # 4. Inherited Idea & New Coupling: Calculate rank gap modulation with Log-Sum-Exp\n    # This requires ranking all costs in the batch\n    all_costs = torch.cat([cost_w, cost_l], dim=0)\n    sorted_costs, _ = torch.sort(all_costs)\n    \n    # Find the rank of each cost_w and cost_l in the sorted list\n    rank_w = torch.searchsorted(sorted_costs, cost_w, right=True)\n    rank_l = torch.searchsorted(sorted_costs, cost_l, right=True)\n    \n    rank_diff = (rank_l - rank_w).float()\n\n    # The LSE formulation is a smooth, stable approximation of max(0, rank_diff)\n    # It ensures the weight is always non-negative and grows with rank difference.\n    # Clamp beta to avoid instability.\n    beta_safe = max(beta, 1e-6)\n    rank_gap = F.softplus(beta_safe * rank_diff) / beta_safe\n    \n    # 5. Compute the core loss: a rank-weighted logistic loss (Bradley-Terry style)\n    # We want to maximize log P(w > l), which is logsigmoid(scaled_delta_logp).\n    # For loss minimization, we minimize the negative log-likelihood.\n    log_likelihood = F.logsigmoid(scaled_delta_logp)\n    loss = -rank_gap.detach() * log_likelihood # Detach rank_gap to act as a pure weight\n\n    # 6. Apply optional weights and compute the mean.\n    weights = batch.get('weight', None)\n    if weights is not None:\n        loss = loss * weights\n        return loss.sum() / weights.sum().clamp(min=1e-8)\n    else:\n        return loss.mean()"}, "static_ok": true, "static_reason": "", "dynamic_ok": false, "dynamic_reason": "loss 12.3651 outside soft range [-5.0, 5.0]", "loss_value": 12.365057945251465, "grad_norm": 0.0}
{"generation": 7, "index": 0, "ir": {"name": "Sigmoid-Gated Adaptive Loss with Rank-Gap Normalization", "intuition": "This loss function creates a robust, adaptive preference signal by combining ideas from its parents and introducing novel coupling mechanisms.\n\nInherited Ideas:\n- From Parent 1 (Softplus Adaptive Margin Loss), it inherits the core structure of a softplus-based hinge loss: `softplus(margin - delta_logp)`. This provides a smooth penalty when the model's log-probability difference doesn't meet a target margin.\n- From Parent 0 (Adaptive Margin Loss with Z-Score Normalization and Rank Gap), it inherits the concept of using a rank-based metric to modulate the loss. Instead of using it as a final weight, we integrate it directly into the margin calculation.\n\nNew Coupling Ideas:\n1.  **Sigmoid Gating Mechanism**: The loss introduces a sigmoid gate, `sigmoid(beta * (delta_logp - offset))`, which dynamically re-weights the primary loss term. When the model's preference `delta_logp` is already very high (i.e., the pair is 'easy'), the sigmoid gate approaches 1, effectively turning off the loss for that sample. This allows the model to focus its capacity on more difficult or ambiguous pairs where its preference signal is weak or incorrect, preventing overconfidence on easy examples.\n2.  **Rank-Gap Normalized Cost Signal**: Instead of using raw cost differences or Z-scored costs to create the adaptive margin, we introduce a new normalization scheme. The cost difference `delta_cost` is divided by the rank difference `rank_gap` between the pair within the batch. This `rank_gap` acts as a measure of 'perceptual distance' in the current context. Dividing by it normalizes the cost difference, making the resulting `margin` less sensitive to the absolute scale of costs and more attuned to the relative ordering of solutions in the batch. A `clamp` is used on the normalized signal to ensure stability against pairs with identical ranks.", "pseudocode": "1. For each pair (w, l) where cost(w) < cost(l), calculate the cost difference `delta_cost = cost(l) - cost(w)` and log probability difference `delta_logp = logp(w) - logp(l)`.\n2. Calculate the rank of each `cost_w` and `cost_l` within the sorted costs of the entire batch. Compute the rank difference `rank_gap = rank(l) - rank(w)`.\n3. Create a 'rank-gap normalized' cost signal. Divide `delta_cost` by the `rank_gap`, adding a small epsilon to the denominator for stability. Clamp the result to prevent extreme values, especially when the rank gap is small. This is the new coupling idea.\n4. Inherit the idea of an adaptive margin. Calculate the margin as `margin = alpha * rank_gap_normalized_cost_signal`.\n5. Inherit the core loss structure. Compute the base loss using `base_loss = softplus(margin - delta_logp)`.\n6. Introduce a new sigmoid gating mechanism. Calculate a gate value `gate = sigmoid(beta * (delta_logp - offset))`. This gate value approaches 1 for 'easy' pairs where `delta_logp` is already high, and approaches 0 for 'hard' pairs.\n7. Apply the gate to the loss: `final_loss = (1.0 - gate) * base_loss`. This focuses the training on pairs that the model finds difficult.\n8. Return the weighted mean of `final_loss` over the batch.", "hyperparams": {"alpha": 1.0, "beta": 0.5, "offset": 2.0, "clamp_max": 5.0}, "operators_used": ["softplus", "sigmoid", "clamp", "rank_gap"], "implementation_hint": {"expects": ["cost_a", "cost_b", "log_prob_w", "log_prob_l"], "returns": "scalar"}, "code": "import torch\nimport torch.nn.functional as F\n\ndef generated_loss(batch, model_output, extra):\n    \"\"\"\n    Computes the Sigmoid-Gated Adaptive Loss with Rank-Gap Normalization.\n\n    Inherits:\n    - The softplus(margin - delta_logp) structure from the Softplus Adaptive Margin Loss.\n    - The use of a rank-based metric from the Adaptive Margin Loss with Rank Gap.\n\n    Introduces:\n    1. Rank-Gap Normalization: The cost difference is normalized by the difference in ranks\n       within the batch to create a scale-invariant, context-aware margin.\n    2. Sigmoid Gating: A sigmoid gate diminishes the loss for 'easy' pairs where the model\n       already exhibits a strong preference, focusing training on more difficult examples.\n    \"\"\"\n    cost_w = torch.min(batch['cost_a'], batch['cost_b'])\n    cost_l = torch.max(batch['cost_a'], batch['cost_b'])\n    log_prob_w = batch['log_prob_w']\n    log_prob_l = batch['log_prob_l']\n\n    alpha = extra.get('alpha', 1.0)\n    beta = extra.get('beta', 0.5)\n    offset = extra.get('offset', 2.0)\n    clamp_max = extra.get('clamp_max', 5.0)\n\n    delta_cost = cost_l - cost_w\n    delta_logp = log_prob_w - log_prob_l\n\n    # New Coupling 1: Rank-Gap Normalized Cost Signal\n    # Get ranks of all costs in the batch to find the rank gap\n    all_costs = torch.cat([cost_w, cost_l], dim=0)\n    sorted_costs, _ = torch.sort(all_costs)\n    rank_w = torch.searchsorted(sorted_costs, cost_w).float()\n    rank_l = torch.searchsorted(sorted_costs, cost_l).float()\n    rank_gap = rank_l - rank_w\n\n    # Normalize cost difference by rank gap. Add epsilon for stability.\n    # This makes the signal relative to the batch's cost distribution.\n    rank_gap_stable = rank_gap.clamp(min=1.0) # Prevent division by zero if ranks are the same\n    normalized_cost_signal = delta_cost / rank_gap_stable\n    \n    # Clamp the signal to prevent extreme margin values\n    clamped_signal = torch.clamp(normalized_cost_signal, max=clamp_max)\n\n    # Inherited: Calculate adaptive margin from the new signal\n    adaptive_margin = alpha * clamped_signal\n\n    # Inherited: Compute the core loss using softplus\n    base_loss = F.softplus(adaptive_margin - delta_logp)\n\n    # New Coupling 2: Sigmoid Gating Mechanism\n    # The gate approaches 1 for easy examples (high delta_logp), reducing their loss contribution.\n    # The offset determines the threshold for what is considered 'easy'.\n    gate = torch.sigmoid(beta * (delta_logp.detach() - offset))\n\n    # Apply the gate to focus on hard examples\n    final_loss = (1.0 - gate) * base_loss\n\n    # Apply optional weights and compute the mean\n    weights = batch.get('weight', None)\n    if weights is not None:\n        final_loss = final_loss * weights\n        return final_loss.sum() / weights.sum().clamp(min=1e-8)\n    else:\n        return final_loss.mean()"}, "static_ok": true, "static_reason": "", "dynamic_ok": true, "dynamic_reason": "ok", "loss_value": 0.5299336910247803, "grad_norm": 0.0}
{"generation": 7, "index": 1, "ir": {"name": "Sigmoid-Gated Adaptive Margin Loss with Rank-Gap Normalization", "intuition": "This loss function creates a robust, adaptive preference signal by combining a smooth, non-saturating margin from one parent with a batch-aware weighting scheme from the other, coupled with a new gating mechanism.\n\nInherited Ideas:\n- From 'Softplus Adaptive Margin Loss' (Parent 1), it inherits the core idea of using `softplus(delta_cost)` to create a smooth, unbounded, and non-negative adaptive margin. This ensures that larger cost differences result in a proportionally larger target separation for the model's log probabilities, without the saturation issues of functions like `tanh`.\n- From 'Adaptive Margin Loss with Z-Score Normalization and Rank Gap' (Parent 0), it inherits the concept of using the relative rank of costs within a batch to modulate the loss. Specifically, it uses the rank difference to create a dynamic weighting factor, focusing the model's attention on pairs that are far apart in the batch's cost distribution.\n\nNew Coupling Ideas:\n1.  **Rank-Gap Normalization of Margin**: Instead of just using `rank_gap` as a final loss weight, it is used to directly normalize the adaptive margin itself. The margin is divided by `1 + beta * rank_gap_normalized`, where `rank_gap_normalized` is the rank difference scaled by the batch size. This couples the margin's scale directly to the pair's relative importance in the batch. For pairs with a small rank gap, the margin is larger (less division), pushing the model to learn fine-grained distinctions. For pairs with a large rank gap, the margin is smaller, as the `softplus(delta_cost)` term is already large and a huge margin is not needed, preventing potential gradient explosion.\n2.  **Sigmoid Gating on Log-Probability Difference**: The model's log-probability difference (`delta_logp`) is passed through a sigmoid gate: `delta_logp * sigmoid(delta_logp)`. This serves two purposes: it down-weights the contribution of pairs where the model is already very confident (large positive `delta_logp`), preventing it from wasting capacity on already-learned examples. It also heavily penalizes pairs where the model is confidently wrong (large negative `delta_logp`), as the sigmoid approaches zero, amplifying the negative term.", "pseudocode": "1. For each pair (w, l) where cost(w) < cost(l), calculate the cost difference `delta_cost = cost(l) - cost(w)` and the log probability difference `delta_logp = logp(w) - logp(l)`.\n2. Inherit from Parent 1: Calculate a smooth, non-saturating adaptive margin base using `softplus`: `margin_base = alpha * softplus(delta_cost / temp_cost)`.\n3. Inherit from Parent 0: Determine the cost ranks for all `cost_w` and `cost_l` in the batch. Calculate the absolute rank difference `rank_diff = abs(rank(cost_l) - rank(cost_w))`.\n4. New Coupling (Rank-Gap Normalization): Normalize the rank difference by the batch size. Use this to create a denominator that modulates the margin: `margin_normalizer = 1.0 + beta * (rank_diff / batch_size)`. The modulated margin is `margin = margin_base / margin_normalizer`.\n5. New Coupling (Sigmoid Gating): Apply a sigmoid gate to the model's log-probability difference to modulate its contribution: `gated_delta_logp = delta_logp * sigmoid(delta_logp)`.\n6. Compute the core per-sample loss using the main softplus structure: `loss = softplus(margin - gated_delta_logp)`.\n7. Return the weighted mean of the per-sample losses over the batch.", "hyperparams": {"alpha": 1.0, "temp_cost": 1.0, "beta": 0.5}, "operators_used": ["softplus", "sigmoid", "rank_gap", "log"], "implementation_hint": {"expects": ["cost_w", "cost_l", "log_prob_w", "log_prob_l"], "returns": "scalar"}, "code": "import torch\nimport torch.nn.functional as F\n\ndef generated_loss(batch, model_output, extra):\n    \"\"\"\n    Computes the Sigmoid-Gated Adaptive Margin Loss with Rank-Gap Normalization.\n\n    Inherits:\n    - The use of `softplus(delta_cost)` for a non-saturating margin (from Parent 1).\n    - The use of rank differences for batch-aware modulation (from Parent 0).\n\n    Introduces:\n    - A new coupling where the rank gap normalizes the margin itself, not the final loss.\n    - A sigmoid gate on the model's log-probability difference to focus on uncertain or incorrect predictions.\n    \"\"\"\n    # For preference learning, we assume cost_w < cost_l.\n    cost_w = torch.min(batch['cost_a'], batch['cost_b'])\n    cost_l = torch.max(batch['cost_a'], batch['cost_b'])\n    log_prob_w = batch['log_prob_w']\n    log_prob_l = batch['log_prob_l']\n\n    # Retrieve hyperparameters\n    alpha = extra.get('alpha', 1.0)\n    temp_cost = extra.get('temp_cost', 1.0)\n    beta = extra.get('beta', 0.5)\n\n    # 1. Calculate cost and log probability differences\n    delta_cost = cost_l - cost_w\n    delta_logp = log_prob_w - log_prob_l\n    batch_size = delta_cost.shape[0]\n\n    # 2. Inherited from Parent 1: Calculate a smooth, non-saturating margin base.\n    margin_base = alpha * F.softplus(delta_cost / temp_cost)\n\n    # 3. Inherited from Parent 0: Calculate rank differences.\n    # Combine all costs to establish a batch-wide ranking.\n    all_costs = torch.cat([cost_w, cost_l], dim=0)\n    sorted_costs, _ = torch.sort(all_costs)\n    \n    # Find ranks efficiently using searchsorted.\n    rank_w = torch.searchsorted(sorted_costs, cost_w)\n    rank_l = torch.searchsorted(sorted_costs, cost_l)\n    rank_diff = (rank_l - rank_w).float()\n\n    # 4. New Coupling: Rank-Gap Normalization of the margin.\n    # Normalize rank diff by batch size to keep the term scale-invariant to N.\n    # For pairs far apart in rank, the denominator increases, reducing the margin.\n    # This prevents extremely large margins for pairs that are already easy to distinguish.\n    # Adding 1.0 ensures the denominator is always >= 1.\n    margin_normalizer = 1.0 + beta * (rank_diff / (2 * batch_size))\n    adaptive_margin = margin_base / margin_normalizer.detach() # Detach to treat as a pure weighting factor\n\n    # 5. New Coupling: Sigmoid Gating on delta_logp.\n    # This penalizes confident incorrect predictions more and reduces focus on already-correct ones.\n    gated_delta_logp = delta_logp * torch.sigmoid(delta_logp)\n\n    # 6. Compute the core loss structure.\n    loss = F.softplus(adaptive_margin - gated_delta_logp)\n\n    # 7. Apply optional weights and compute the mean.\n    weights = batch.get('weight', None)\n    if weights is not None:\n        loss = loss * weights\n        return loss.sum() / weights.sum().clamp(min=1e-8)\n    else:\n        return loss.mean()"}, "static_ok": true, "static_reason": "", "dynamic_ok": true, "dynamic_reason": "ok", "loss_value": 1.3505098819732666, "grad_norm": 0.0}
{"generation": 7, "index": 2, "ir": {"name": "Sigmoid-Gated Adaptive Margin Loss with Rank-Gap Annealing", "intuition": "This loss function creates an adaptive preference margin by blending ideas from its parents and introducing novel coupling mechanisms for stability and improved learning dynamics.\n\nInherited Ideas:\n- From both parents, it inherits the core structure of a margin-based preference loss: `loss = softplus(margin - delta_logp)`. This smooth hinge loss penalizes the model when the log-probability difference between the winning and losing solutions falls short of a target margin.\n- From Parent 1 (`Adaptive Margin Loss with Z-Score Normalization and Rank Gap`), it inherits the idea of using the batch-wise rank gap between solutions (`rank_diff`) as an important signal. This recognizes that pairs with a larger difference in quality should be treated differently.\n- From Parent 2 (`Softplus Adaptive Margin Loss`), it inherits the use of the raw cost difference (`delta_cost`) to directly influence the margin, ensuring that larger cost gaps demand a larger log-probability separation.\n\nNew Coupling Ideas:\n1.  **Sigmoid Gating Mechanism**: The margin is a product of two components: a base margin derived from the cost difference and a sigmoid gate controlled by the rank difference. The margin is `alpha * (delta_cost / temp_cost) * sigmoid(beta * (rank_diff - rank_shift))`. This gating mechanism ensures that pairs with a very small rank difference (i.e., they are close neighbors in the sorted batch) have their target margin significantly down-weighted. This prevents the model from being aggressively penalized for small preference errors on nearly-indistinguishable pairs, focusing its capacity on learning more significant preference orderings first.\n2.  **Dynamic Rank-Gap Annealing**: The `rank_shift` hyperparameter in the sigmoid gate is not fixed but is scheduled to increase over training steps. It starts at a low value (e.g., 1.0), meaning only pairs that are immediately adjacent in rank get down-weighted. As training progresses, `rank_shift` increases, gradually raising the bar and requiring the model to distinguish between pairs that are further apart in rank. This annealing schedule acts as a curriculum, starting with easy distinctions and moving to harder ones, which can stabilize early training and lead to better final performance.", "pseudocode": "1. For each pair (w, l) where cost(w) < cost(l), calculate the cost difference `delta_cost = cost(l) - cost(w)` and the log probability difference `delta_logp = logp(w) - logp(l)`.\n2. Collect all costs from the batch and determine the rank of each `cost_w` and `cost_l` within the sorted list of all costs.\n3. Calculate the rank difference for each pair: `rank_diff = rank(cost_l) - rank(cost_w)`.\n4. Determine the current `rank_shift` value based on the current training step using a linear annealing schedule from a start to an end value.\n5. Compute a sigmoid gating factor: `gate = sigmoid(beta * (rank_diff - rank_shift))`. This gate approaches 1 for pairs with a large rank difference and approaches 0 for pairs with a rank difference smaller than the current `rank_shift`.\n6. Calculate the adaptive margin by multiplying the cost-based signal with the sigmoid gate: `margin = alpha * (delta_cost / temp_cost) * gate`. This effectively suppresses the target margin for pairs that are too close in rank for the current stage of training.\n7. Compute the per-sample loss using the softplus function: `loss = softplus(margin - delta_logp)`.\n8. Return the weighted mean of the loss over the batch.", "hyperparams": {"alpha": 1.0, "temp_cost": 10.0, "beta": 1.0, "rank_shift_start": 1.0, "rank_shift_end": 5.0, "anneal_steps": 10000}, "operators_used": ["softplus", "sigmoid", "rank_gap"], "implementation_hint": {"expects": ["cost_w", "cost_l", "logp_w", "logp_l"], "returns": "scalar"}, "code": "import torch\nimport torch.nn.functional as F\n\ndef generated_loss(batch, model_output, extra):\n    \"\"\"\n    Computes the Sigmoid-Gated Adaptive Margin Loss with Rank-Gap Annealing.\n\n    Inherits:\n    - The core `softplus(margin - delta_logp)` structure from both parents.\n    - The use of rank differences from Parent 1.\n    - The use of raw cost differences for margin scaling from Parent 2.\n\n    New Couplings:\n    1. Sigmoid Gating: The margin is gated by a sigmoid function of the rank difference,\n       suppressing the margin for pairs that are close in rank.\n    2. Rank-Gap Annealing: The threshold for this gating (`rank_shift`) is annealed over\n       training, creating a curriculum that starts with easy distinctions and moves to harder ones.\n    \"\"\"\n    cost_w = torch.min(batch['cost_a'], batch['cost_b'])\n    cost_l = torch.max(batch['cost_a'], batch['cost_b'])\n    log_prob_w = batch['log_prob_w']\n    log_prob_l = batch['log_prob_l']\n\n    # Retrieve hyperparameters\n    alpha = extra.get('alpha', 1.0)\n    temp_cost = extra.get('temp_cost', 10.0)\n    beta = extra.get('beta', 1.0)\n    rank_shift_start = extra.get('rank_shift_start', 1.0)\n    rank_shift_end = extra.get('rank_shift_end', 5.0)\n    anneal_steps = extra.get('anneal_steps', 10000)\n\n    # Calculate cost and log probability differences\n    delta_cost = cost_l - cost_w\n    delta_logp = log_prob_w - log_prob_l\n\n    # Calculate rank difference (Inherited from Parent 1)\n    all_costs = torch.cat([cost_w, cost_l], dim=0)\n    # Use a stable sort for deterministic ranks\n    sorted_costs, _ = torch.sort(all_costs, stable=True)\n    rank_w = torch.searchsorted(sorted_costs, cost_w)\n    rank_l = torch.searchsorted(sorted_costs, cost_l)\n    rank_diff = (rank_l - rank_w).float()\n\n    # New Coupling 1: Dynamic Rank-Gap Annealing\n    # Get the current training step, default to 0 if not available\n    current_step = extra.get('current_step', 0)\n    progress = min(1.0, current_step / anneal_steps)\n    current_rank_shift = rank_shift_start + (rank_shift_end - rank_shift_start) * progress\n\n    # New Coupling 2: Sigmoid Gating Mechanism\n    # The gate value approaches 0 for pairs with rank_diff << current_rank_shift,\n    # and 1 for pairs with rank_diff >> current_rank_shift.\n    gate = torch.sigmoid(beta * (rank_diff - current_rank_shift))\n\n    # Calculate the adaptive margin\n    # The base margin is proportional to delta_cost (Inherited from Parent 2)\n    base_margin = alpha * (delta_cost / temp_cost)\n    # The final margin is gated by the rank difference\n    adaptive_margin = base_margin * gate.detach() # Detach gate to act as a pure weight\n\n    # Compute the core loss using softplus (Inherited from both)\n    loss = F.softplus(adaptive_margin - delta_logp)\n\n    # Apply optional weights and compute the mean\n    weights = batch.get('weight', None)\n    if weights is not None:\n        loss = loss * weights\n        return loss.sum() / weights.sum().clamp(min=1e-8)\n    else:\n        return loss.mean()"}, "static_ok": true, "static_reason": "", "dynamic_ok": true, "dynamic_reason": "ok", "loss_value": 0.7443966269493103, "grad_norm": 0.0}
{"generation": 7, "index": 3, "ir": {"name": "Rank-Weighted Bradley-Terry Loss with Softplus Cost Scaling", "intuition": "This loss function blends the probabilistic foundation of Bradley-Terry models with adaptive, rank-aware weighting, inheriting key ideas from both parents while introducing new couplings for stability and expressiveness.\n\nInherited Ideas:\n- From the 'Softplus Adaptive Margin Loss' (Parent 1), it inherits the use of `softplus(delta_cost / temp_cost)` to create a smooth, non-saturating, and non-negative signal from the cost difference. This ensures that larger cost gaps translate into stronger learning signals without being overly sensitive to extreme outliers.\n- From the 'Adaptive Margin Loss with Z-Score Normalization and Rank Gap' (Parent 0), it inherits the concept of using a rank-based weighting scheme. This `rank_gap` term up-weights the loss for pairs that are far apart in the batch's cost ranking, focusing the model's attention on learning the most significant relative orderings.\n\nNew Coupling Ideas:\n1. **Bradley-Terry Core with LogSigmoid**: Instead of a margin-based loss like `softplus(margin - delta_logp)`, the core of this loss is based on the Bradley-Terry model: `-logsigmoid(delta_logp)`. This frames the problem as maximizing the log-likelihood of the observed preference, which is a common and stable objective for preference learning.\n2. **Coupling Cost Signal and Rank Gap as a Composite Weight**: The `softplus` cost signal and the `rank_gap` are not used to form a margin. Instead, they are multiplied together (`cost_signal * rank_gap`) to form a composite, adaptive weight for the core `-logsigmoid` loss. This new coupling means the loss for a given pair is amplified based on both its absolute cost difference (via `softplus`) and its relative importance within the batch's ranking (via `rank_gap`). This provides a richer, more nuanced supervision signal than either component alone.", "pseudocode": "1. For each pair (w, l) in the batch, where cost(w) < cost(l), calculate the cost difference `delta_cost = cost(l) - cost(w)` and the log probability difference `delta_logp = logp(w) - logp(l)`.\n2. Compute the base preference loss for each pair using the Bradley-Terry model: `base_loss = -logsigmoid(delta_logp)`.\n3. Inherited from Parent 1: Calculate a smooth, non-negative cost signal by applying `softplus` to the scaled cost difference: `cost_signal = softplus(delta_cost / temp_cost)`.\n4. Inherited from Parent 0: Calculate a 'rank gap' weight. First, find the rank of `cost_w` and `cost_l` within the sorted costs of the entire batch. Then compute `rank_gap = 1.0 - exp(-beta * abs(rank(cost_l) - rank(cost_w)))`.\n5. New Coupling: Create a composite adaptive weight by multiplying the cost signal and the rank gap: `adaptive_weight = cost_signal * rank_gap`.\n6. Modulate the base loss with this composite weight: `final_loss = base_loss * adaptive_weight`.\n7. Return the weighted mean of `final_loss` over the batch.", "hyperparams": {"temp_cost": 1.0, "beta": 0.1}, "operators_used": ["logsigmoid", "softplus", "exp", "rank_gap"], "implementation_hint": {"expects": ["cost_a", "cost_b", "logp_a", "logp_b"], "returns": "scalar"}, "code": "import torch\nimport torch.nn.functional as F\n\ndef generated_loss(batch, model_output, extra):\n    \"\"\"\n    Computes a Rank-Weighted Bradley-Terry Loss with Softplus Cost Scaling.\n\n    Inherits:\n    - The use of `softplus(delta_cost)` for a smooth, non-saturating cost signal (from Parent 1).\n    - The use of a `rank_gap` term to up-weight pairs that are far apart in the batch's cost ranking (from Parent 0).\n\n    New Couplings:\n    1. Uses a Bradley-Terry `-logsigmoid(delta_logp)` core instead of a margin-based loss.\n    2. Couples the cost signal and rank gap by multiplying them to form a single adaptive weight, which then modulates the core loss.\n    \"\"\"\n    cost_w = torch.min(batch['cost_a'], batch['cost_b'])\n    cost_l = torch.max(batch['cost_a'], batch['cost_b'])\n    log_prob_w = batch['log_prob_w']\n    log_prob_l = batch['log_prob_l']\n\n    # Retrieve hyperparameters\n    temp_cost = extra.get('temp_cost', 1.0)\n    beta = extra.get('beta', 0.1)\n\n    # 1. Calculate cost and log probability differences\n    delta_cost = cost_l - cost_w\n    delta_logp = log_prob_w - log_prob_l\n\n    # 2. New Coupling: Use Bradley-Terry model as the core loss\n    # This is equivalent to binary cross-entropy on the preference prediction.\n    base_loss = -F.logsigmoid(delta_logp)\n\n    # 3. Inherited Idea (Parent 1): Create a smooth, non-saturating cost signal\n    cost_signal = F.softplus(delta_cost / temp_cost)\n\n    # 4. Inherited Idea (Parent 0): Calculate rank gap modulation\n    all_costs = torch.cat([cost_w, cost_l], dim=0)\n    # Use torch.sort for differentiability, although ranks themselves are discrete.\n    sorted_costs, _ = torch.sort(all_costs)\n    \n    # Find the rank of each cost_w and cost_l in the sorted list\n    rank_w = torch.searchsorted(sorted_costs, cost_w)\n    rank_l = torch.searchsorted(sorted_costs, cost_l)\n    \n    rank_diff = (rank_l - rank_w).float()\n    \n    # The rank_gap approaches 1 as rank_diff increases, and is 0 if ranks are the same.\n    rank_gap = 1.0 - torch.exp(-beta * rank_diff)\n\n    # 5. New Coupling: Combine cost signal and rank gap into a composite weight\n    # Detach the weight to prevent gradients from flowing through the weighting scheme\n    # back to the model parameters, treating it as a pure loss-shaping mechanism.\n    adaptive_weight = (cost_signal * rank_gap).detach()\n\n    # 6. Modulate the base loss with the composite adaptive weight\n    final_loss = base_loss * adaptive_weight\n\n    # 7. Apply optional weights and compute the mean.\n    weights = batch.get('weight', None)\n    if weights is not None:\n        final_loss = final_loss * weights\n        return final_loss.sum() / weights.sum().clamp(min=1e-8)\n    else:\n        return final_loss.mean()"}, "static_ok": true, "static_reason": "", "dynamic_ok": true, "dynamic_reason": "ok", "loss_value": 0.7265004515647888, "grad_norm": 0.0}
{"generation": 7, "index": 4, "ir": {"name": "Rank-Modulated Log-Ratio Loss", "intuition": "This loss function combines the core preference structure of a log-ratio loss with adaptive, rank-based modulation.\n\nInherited Ideas:\n- From Parent 1 (Adaptive Margin Loss with Z-Score Normalization and Rank Gap), it inherits the idea of using a rank-based weighting scheme. The loss for pairs that are far apart in the batch's cost ranking is amplified, focusing the model's attention on learning coarse-grained preferences first.\n- From Parent 2 (Softplus Adaptive Margin Loss), it inherits the use of `softplus` to transform the cost difference (`delta_cost`) into a smooth, non-negative, and non-saturating signal. This ensures that larger cost differences consistently translate to a stronger learning signal.\n\nThe overall loss structure is a log-ratio loss, `log(1 + exp(margin - delta_logp))`, which is equivalent to `softplus(margin - delta_logp)`. This is a common and stable formulation for preference learning.\n\nNew Coupling Ideas:\n1.  **Dynamic Margin Scaling**: The margin is calculated as `margin = softplus(delta_cost / temp_cost)`. This makes the target separation in log-probabilities smoothly and non-linearly increase with the cost difference, avoiding the saturation of `tanh` (from Parent 1) while still being non-negative.\n2.  **Rank-Modulated Log-Probabilities**: Instead of modulating the final loss value, the rank gap weight is applied directly to the log-probability difference (`delta_logp`). The new term becomes `rank_gap * delta_logp`. This reframes the problem: for pairs that are far apart in rank, the model's perceived log-probability difference is amplified, effectively giving the model 'credit' for getting the easy examples right and encouraging it to focus its capacity on harder, closer-ranked pairs where the margin is not yet met. This subtle change shifts the modulation from being a simple loss weight to an integral part of the preference comparison itself.", "pseudocode": "1. For each pair (w, l) in the batch, where cost(w) < cost(l), calculate the cost difference `delta_cost = cost(l) - cost(w)` and the log probability difference `delta_logp = logp(w) - logp(l)`.\n2. Inherited from Parent 2: Create a smooth, non-saturating 'cost signal' by applying a softplus function to the scaled cost difference: `cost_signal = softplus(delta_cost / temp_cost)`.\n3. New Coupling Idea 1: Define the target margin directly from this cost signal: `margin = alpha * cost_signal`.\n4. Inherited from Parent 1: Calculate a 'rank gap' weight for each pair. First, find the rank of `cost_w` and `cost_l` within the sorted costs of the entire batch. Then compute `rank_gap_weight = 1.0 - exp(-beta * abs(rank(cost_l) - rank(cost_w)))`.\n5. New Coupling Idea 2: Modulate the model's log-probability difference directly with the rank gap weight: `modulated_delta_logp = rank_gap_weight * delta_logp`.\n6. Compute the per-sample loss using a log-ratio (or softplus) formulation: `loss = softplus(margin - modulated_delta_logp)`.\n7. Return the weighted mean of the loss over the batch.", "hyperparams": {"alpha": 1.0, "temp_cost": 1.0, "beta": 0.05}, "operators_used": ["softplus", "exp", "log"], "implementation_hint": {"expects": ["cost_a", "cost_b", "log_prob_w", "log_prob_l", "weight"], "returns": "scalar"}, "code": "import torch\nimport torch.nn.functional as F\n\ndef generated_loss(batch, model_output, extra):\n    \"\"\"\n    Computes the Rank-Modulated Log-Ratio Loss.\n\n    Inherits rank-based weighting from Parent 1 and the use of softplus on cost differences from Parent 2.\n    Introduces two new couplings:\n    1. A dynamic margin based directly on the non-saturating softplus of the cost difference.\n    2. A modulation of the log-probability difference by the rank gap, changing the preference term itself.\n    \"\"\"\n    # For preference learning, we assume the dataset provides pairs where one is strictly better.\n    cost_w = torch.min(batch['cost_a'], batch['cost_b'])\n    cost_l = torch.max(batch['cost_a'], batch['cost_b'])\n    log_prob_w = batch['log_prob_w']\n    log_prob_l = batch['log_prob_l']\n\n    # Retrieve hyperparameters\n    alpha = extra.get('alpha', 1.0)\n    temp_cost = extra.get('temp_cost', 1.0)\n    beta = extra.get('beta', 0.05)\n\n    # 1. Calculate cost and log probability differences\n    # delta_cost is guaranteed to be >= 0\n    delta_cost = cost_l - cost_w\n    delta_logp = log_prob_w - log_prob_l\n\n    # 2. Inherited from Parent 2: Create a non-saturating cost signal\n    cost_signal = F.softplus(delta_cost / temp_cost)\n\n    # 3. New Coupling Idea 1: Define the target margin from the cost signal\n    margin = alpha * cost_signal\n\n    # 4. Inherited from Parent 1: Calculate rank gap modulation weight\n    # This requires ranking all costs in the batch\n    all_costs = torch.cat([cost_w, cost_l], dim=0)\n    # Use unique_consecutive to handle ties in ranking gracefully\n    sorted_costs = torch.unique_consecutive(all_costs.sort().values)\n\n    # Find the rank of each cost_w and cost_l in the sorted unique list\n    rank_w = torch.searchsorted(sorted_costs, cost_w, right=True) - 1\n    rank_l = torch.searchsorted(sorted_costs, cost_l, right=True) - 1\n\n    rank_diff = (rank_l - rank_w).float().clamp(min=0)\n\n    # The rank_gap_weight approaches 1 as rank_diff increases, and is 0 if ranks are the same.\n    rank_gap_weight = 1.0 - torch.exp(-beta * rank_diff)\n\n    # 5. New Coupling Idea 2: Modulate delta_logp directly\n    # This amplifies the model's perceived logp difference for widely separated pairs.\n    modulated_delta_logp = rank_gap_weight.detach() * delta_logp\n\n    # 6. Compute the core loss using softplus (equivalent to log(1 + exp(...)))\n    # The loss is low if modulated_delta_logp > margin.\n    loss_per_sample = F.softplus(margin - modulated_delta_logp)\n\n    # 7. Apply optional weights and compute the mean.\n    weights = batch.get('weight', None)\n    if weights is not None:\n        loss_per_sample = loss_per_sample * weights\n        return loss_per_sample.sum() / weights.sum().clamp(min=1e-8)\n    else:\n        return loss_per_sample.mean()"}, "static_ok": true, "static_reason": "", "dynamic_ok": true, "dynamic_reason": "ok", "loss_value": 1.5514447689056396, "grad_norm": 0.0}
{"generation": 7, "index": 5, "ir": {"name": "Rank-Aware Log-Sigmoid Preference Loss", "intuition": "This loss function creates a robust, rank-aware preference signal by combining a classic log-sigmoid structure with adaptive, batch-normalized cost information.\n\nInherited Ideas:\n- From 'Softplus Adaptive Margin Loss' (Parent 1), it inherits the idea of using the raw cost difference `delta_cost` to create a signal that scales the loss. Instead of forming a margin inside a `softplus`, this cost signal now directly scales the log-sigmoid loss term, making pairs with larger cost differences contribute more to the gradient.\n- From 'Adaptive Margin Loss with Z-Score Normalization' (Parent 0), it inherits the technique of using Z-score normalization on the batch-wise `delta_cost`. This makes the scaling factor invariant to the absolute scale of costs in a given batch, improving stability and reducing hyperparameter sensitivity.\n\nNew Coupling Ideas:\n1.  **Log-Sigmoid Core with Adaptive Scaling**: The core of the loss is `logsigmoid(delta_logp)`. However, instead of a simple margin, this term is multiplied by an adaptive weight derived from the cost difference. The final loss is `-weight * logsigmoid(delta_logp)`. This structure is inspired by Bradley-Terry models but is enhanced with dynamic, cost-aware weighting.\n2.  **Softplus-Gated Rank Gap**: The loss incorporates a `rank_gap` term, similar to Parent 0, which emphasizes pairs that are far apart in the batch's cost ranking. The novel coupling is that this rank gap is passed through a `softplus` function. This creates a smooth, non-negative, and non-saturating gate that starts at `log(2)` for items with the same rank and grows approximately linearly as the rank difference increases. This provides a more robust and continuously increasing emphasis on widely separated pairs compared to the `1 - exp(-x)` formulation.", "pseudocode": "1. For each pair (w, l) in the batch where cost(w) < cost(l), calculate the cost difference `delta_cost = cost(l) - cost(w)` and the log probability difference `delta_logp = logp(w) - logp(l)`.\n2. Standardize the `delta_cost` vector for the entire batch using Z-score normalization to get `delta_cost_norm`. This is inherited from Parent 0 for scale invariance.\n3. Create a non-negative, unbounded cost signal by applying `softplus` to the normalized cost differences: `cost_signal = softplus(delta_cost_norm)`. This is inspired by the use of `softplus` in Parent 1.\n4. Calculate a 'rank gap' signal. First, find the rank of `cost_w` and `cost_l` within all sorted costs in the batch. The raw rank difference is `rank_diff = rank(cost_l) - rank(cost_w)`.\n5. Create a smooth, non-saturating rank gate by applying `softplus` to the scaled rank difference: `rank_gate = softplus(beta * rank_diff)`. This is a new coupling.\n6. Combine the cost signal and the rank gate to form a final adaptive weight for each pair: `adaptive_weight = cost_signal * rank_gate`.\n7. Compute the core preference loss for each pair using the log-sigmoid function: `base_loss = -logsigmoid(delta_logp)`.\n8. Modulate the base loss with the adaptive weight: `final_loss = adaptive_weight * base_loss`.\n9. Return the weighted mean of `final_loss` over the batch.", "hyperparams": {"beta": 0.05}, "operators_used": ["logsigmoid", "softplus", "zscore", "rank_gap"], "implementation_hint": {"expects": ["cost_a", "cost_b", "logp_a", "logp_b"], "returns": "scalar"}, "code": "import torch\nimport torch.nn.functional as F\n\ndef generated_loss(batch, model_output, extra):\n    \"\"\"\n    Computes the Rank-Aware Log-Sigmoid Preference Loss.\n\n    Inherits:\n    - The use of Z-score normalization on cost differences for scale-invariance (from Parent 0).\n    - The use of a softplus-transformed cost difference to create an adaptive, non-negative signal (from Parent 1).\n\n    Introduces:\n    - A core log-sigmoid loss, `-w * logsigmoid(delta_logp)`, where `w` is an adaptive weight.\n    - A softplus-gated rank gap, `softplus(beta * rank_diff)`, for a smooth, non-saturating emphasis on widely separated pairs.\n    \"\"\"\n    # For preference learning, we assume cost_w < cost_l.\n    cost_w = torch.min(batch['cost_a'], batch['cost_b'])\n    cost_l = torch.max(batch['cost_a'], batch['cost_b'])\n    log_prob_w = batch['log_prob_w']\n    log_prob_l = batch['log_prob_l']\n\n    # Retrieve hyperparameters\n    beta = extra.get('beta', 0.05)\n\n    # 1. Calculate cost and log probability differences\n    delta_cost = cost_l - cost_w\n    delta_logp = log_prob_w - log_prob_l\n\n    # Early exit for a batch with no cost difference to avoid division by zero in z-score\n    if torch.all(delta_cost < 1e-8):\n        return torch.tensor(0.0, device=delta_cost.device)\n\n    # 2. Inherited: Z-score normalization of cost differences for batch-wise scale invariance\n    delta_cost_mean = delta_cost.mean()\n    delta_cost_std = delta_cost.std().clamp(min=1e-8) # Avoid division by zero\n    delta_cost_norm = (delta_cost - delta_cost_mean) / delta_cost_std\n\n    # 3. Inherited: Create a non-negative, unbounded cost signal using softplus\n    cost_signal = F.softplus(delta_cost_norm)\n\n    # 4. New Coupling: Calculate rank gap\n    # This requires ranking all costs in the batch\n    all_costs = torch.cat([cost_w, cost_l], dim=0)\n    sorted_costs, _ = torch.sort(all_costs)\n    \n    # Find the rank of each cost_w and cost_l in the sorted list\n    rank_w = torch.searchsorted(sorted_costs, cost_w)\n    rank_l = torch.searchsorted(sorted_costs, cost_l)\n    rank_diff = (rank_l - rank_w).float()\n\n    # 5. New Coupling: Create a smooth, non-saturating rank gate using softplus\n    rank_gate = F.softplus(beta * rank_diff)\n    \n    # 6. Combine signals into a final adaptive weight\n    # Detach to ensure weights do not contribute to gradients w.r.t. cost prediction models, if any.\n    adaptive_weight = (cost_signal * rank_gate).detach()\n\n    # 7. New Coupling: Compute the core preference loss using log-sigmoid\n    # This term is maximized when delta_logp is large and positive.\n    base_loss = -F.logsigmoid(delta_logp)\n\n    # 8. Modulate the base loss with the adaptive weight\n    final_loss = adaptive_weight * base_loss\n\n    # 9. Apply optional weights and compute the mean.\n    weights = batch.get('weight', None)\n    if weights is not None:\n        final_loss = final_loss * weights\n        return final_loss.sum() / weights.sum().clamp(min=1e-8)\n    else:\n        return final_loss.mean()"}, "static_ok": true, "static_reason": "", "dynamic_ok": true, "dynamic_reason": "ok", "loss_value": 0.5626588463783264, "grad_norm": 0.0}
{"generation": 7, "index": 6, "ir": {"name": "Rank-Aware LogSigmoid Loss with Softplus Margin", "intuition": "This loss function combines the core structures of its parents to create a robust, rank-aware preference loss. The design aims to be less sensitive to the absolute scale of costs while still aggressively penalizing misordered pairs that are far apart in cost.\n\nInherited Ideas:\n- From Parent 1 ('Softplus Adaptive Margin Loss'), it inherits the use of `softplus(delta_cost / temp_cost)` to create a smooth, non-saturating adaptive margin. This ensures that as the cost gap between two solutions widens, the target log-probability separation also grows, pushing the model harder to distinguish them.\n- From both parents, it inherits the fundamental structure of a margin-based loss, where the model's log-probability difference `delta_logp` is compared against a target `margin` derived from the cost difference `delta_cost`.\n\nNew Coupling Ideas:\n1.  **LogSigmoid Loss Formulation**: Instead of the common `softplus(margin - delta_logp)`, this child uses a `logsigmoid` formulation: `loss = -logsigmoid(delta_logp - margin)`. This is mathematically equivalent to `softplus(margin - delta_logp)` but can offer slightly better numerical stability. It frames the problem as maximizing the log-likelihood of the model's preference `delta_logp` being greater than the target `margin`.\n2.  **Rank Gap Modulation**: Borrowing the concept from Parent 0, the loss is modulated by a rank-gap weight. This weight, `rank_gap = 1.0 - exp(-beta * |rank(cost_l) - rank(cost_w)|)`, amplifies the loss for pairs that are far apart in the batch's cost ranking. This new coupling directs the model's attention towards correcting significant ranking errors over fine-tuning pairs that are already close in cost, adding a global, batch-aware perspective to the local pairwise comparison.", "pseudocode": "1. For each pair (w, l) in the batch where cost(w) < cost(l), calculate the cost difference `delta_cost = cost(l) - cost(w)` and the log probability difference `delta_logp = logp(w) - logp(l)`.\n2. Inherited from Parent 1: Transform the cost difference using a softplus function to create a smooth, non-saturating 'cost signal': `cost_signal = softplus(delta_cost / temp_cost)`.\n3. Calculate the adaptive margin based on this signal: `margin = alpha * cost_signal`.\n4. New Coupling 1: Compute the base per-sample loss using a numerically stable logsigmoid formulation, which is equivalent to `softplus(margin - delta_logp)`: `base_loss = -logsigmoid(delta_logp - margin)`.\n5. New Coupling 2: Calculate a 'rank gap' weight for each pair. First, find the rank of `cost_w` and `cost_l` within all sorted costs of the batch. Then compute `rank_gap_weight = 1.0 - exp(-beta * abs(rank(cost_l) - rank(cost_w)))`.\n6. Modulate the base loss with the rank gap weight: `final_loss = base_loss * rank_gap_weight`.\n7. Return the weighted mean of `final_loss` over the batch.", "hyperparams": {"alpha": 1.0, "temp_cost": 1.0, "beta": 0.05}, "operators_used": ["logsigmoid", "softplus", "exp", "rank_gap"], "implementation_hint": {"expects": ["cost_a", "cost_b", "log_prob_w", "log_prob_l", "weight"], "returns": "scalar"}, "code": "import torch\nimport torch.nn.functional as F\n\ndef generated_loss(batch, model_output, extra):\n    \"\"\"\n    Computes the Rank-Aware LogSigmoid Loss with Softplus Margin.\n\n    Inherits the softplus-based adaptive margin from Parent 1.\n    Introduces two new ideas:\n    1. A logsigmoid loss formulation, `loss = -logsigmoid(delta_logp - margin)`, which is a stable alternative to `softplus(margin - delta_logp)`.\n    2. A 'rank gap' modulator from Parent 0 that up-weights the loss for pairs that are far apart in the batch's cost ranking.\n    \"\"\"\n    # For preference learning, we assume the dataset provides pairs where one is strictly better.\n    cost_w = torch.min(batch['cost_a'], batch['cost_b'])\n    cost_l = torch.max(batch['cost_a'], batch['cost_b'])\n    log_prob_w = batch['log_prob_w']\n    log_prob_l = batch['log_prob_l']\n\n    # Retrieve hyperparameters\n    alpha = extra.get('alpha', 1.0)\n    temp_cost = extra.get('temp_cost', 1.0)\n    beta = extra.get('beta', 0.05)\n\n    # 1. Calculate cost and log probability differences\n    # delta_cost is guaranteed to be >= 0\n    delta_cost = cost_l - cost_w\n    delta_logp = log_prob_w - log_prob_l\n\n    # 2. Inherited Idea (Parent 1): Create a non-saturating cost signal using softplus.\n    cost_signal = F.softplus(delta_cost / temp_cost)\n\n    # 3. Calculate the adaptive margin.\n    adaptive_margin = alpha * cost_signal\n\n    # 4. New Coupling 1: Compute the base loss using the logsigmoid formulation.\n    # This is equivalent to softplus(adaptive_margin - delta_logp) but can be more stable.\n    base_loss = -F.logsigmoid(delta_logp - adaptive_margin)\n\n    # 5. New Coupling 2: Calculate rank gap modulation.\n    # This requires ranking all costs in the batch.\n    all_costs = torch.cat([cost_w, cost_l], dim=0)\n    # Using searchsorted on the sorted unique values is a robust way to get ranks.\n    sorted_unique_costs = torch.unique(all_costs, sorted=True)\n    \n    # Find the rank of each cost_w and cost_l in the sorted list.\n    rank_w = torch.searchsorted(sorted_unique_costs, cost_w)\n    rank_l = torch.searchsorted(sorted_unique_costs, cost_l)\n    \n    # rank_diff is guaranteed to be >= 0 because cost_l >= cost_w\n    rank_diff = (rank_l - rank_w).float()\n    \n    # The rank_gap_weight approaches 1 as rank_diff increases, and is 0 if ranks are the same.\n    rank_gap_weight = 1.0 - torch.exp(-beta * rank_diff)\n\n    # 6. Modulate the base loss with the rank gap weight.\n    # Detach the weight so it doesn't contribute to gradients w.r.t. costs of other samples.\n    final_loss = base_loss * rank_gap_weight.detach()\n\n    # 7. Apply optional weights and compute the mean.\n    weights = batch.get('weight', None)\n    if weights is not None:\n        final_loss = final_loss * weights\n        # Use a small epsilon to prevent division by zero if all weights are zero.\n        return final_loss.sum() / weights.sum().clamp(min=1e-8)\n    else:\n        return final_loss.mean()"}, "static_ok": true, "static_reason": "", "dynamic_ok": true, "dynamic_reason": "ok", "loss_value": 0.07566481083631516, "grad_norm": 0.0}
{"generation": 7, "index": 7, "ir": {"name": "Rank-Modulated Bradley-Terry Loss", "intuition": "This loss function creates a hybrid between a Bradley-Terry style preference loss and an adaptive margin loss, modulated by the relative ranking of solutions within a batch.\n\nInherited Ideas:\n- From 'Softplus Adaptive Margin Loss' (Parent 1), it inherits the use of `softplus` on the cost difference (`delta_cost`) to create a smooth, non-saturating signal that defines an adaptive margin. This ensures that pairs with larger cost differences are pushed apart more strongly.\n- From 'Adaptive Margin Loss with Z-Score Normalization and Rank Gap' (Parent 2), it inherits the idea of using the rank difference between the winner and loser costs within a batch to modulate the loss. This 'rank gap' focuses the model's attention on learning the relative ordering of solutions that are significantly far apart in the batch's cost landscape.\n\nNew Coupling Ideas:\n1.  **Bradley-Terry Core with Adaptive Margin**: Instead of the common `softplus(margin - delta_logp)` structure, this loss adopts a Bradley-Terry-like form: `logsigmoid(delta_logp)`. The adaptive margin, derived from the `softplus` of the cost difference, is not used to set a target for `delta_logp`, but rather to scale the entire logsigmoid loss. This means the loss for a mis-ordered pair (`delta_logp < 0`) is amplified proportionally to how different their costs are, but it does not penalize pairs that already exceed a certain margin.\n2.  **Combined Modulation**: The final loss is modulated by the product of the adaptive margin (from costs) and the rank gap weight (from ranks). The expression `(1 + adaptive_margin) * rank_gap_weight` serves as a dynamic, per-sample loss weight. The `1 + ...` term ensures that even pairs with zero cost difference still contribute to the loss (governed by the rank gap), preventing the loss from vanishing for identical-cost pairs.", "pseudocode": "1. For each pair (w, l) in the batch, where cost(w) < cost(l), calculate the cost difference `delta_cost = cost(l) - cost(w)` and the log probability difference `delta_logp = logp(w) - logp(l)`.\n2. Compute the base preference loss using a Bradley-Terry style formulation: `base_loss = -logsigmoid(delta_logp)`. This penalizes cases where `logp(w)` is not sufficiently larger than `logp(l)`.\n3. Inherited Idea 1: Calculate a smooth, non-saturating 'cost signal' by applying softplus to the scaled cost difference: `cost_signal = softplus(delta_cost / temp_cost)`.\n4. Inherited Idea 2: Calculate a 'rank gap' weight. Find the rank of `cost_w` and `cost_l` within the sorted costs of the entire batch. Compute `rank_gap_weight = 1.0 - exp(-beta * abs(rank(l) - rank(w)))`.\n5. New Coupling: Create a combined dynamic weight for each sample. This weight is the product of the rank gap and a term derived from the cost signal: `dynamic_weight = (1.0 + alpha * cost_signal) * rank_gap_weight`.\n6. Modulate the base loss with this dynamic weight: `final_loss = base_loss * dynamic_weight`.\n7. Return the weighted mean of `final_loss` over the batch.", "hyperparams": {"alpha": 1.0, "temp_cost": 1.0, "beta": 0.1}, "operators_used": ["logsigmoid", "softplus", "exp", "rank_gap"], "implementation_hint": {"expects": ["cost_w", "cost_l", "logp_w", "logp_l"], "returns": "scalar"}, "code": "import torch\nimport torch.nn.functional as F\n\ndef generated_loss(batch, model_output, extra):\n    \"\"\"\n    Computes the Rank-Modulated Bradley-Terry Loss.\n\n    This loss combines a Bradley-Terry style preference loss (`-logsigmoid(delta_logp)`)\n    with a dynamic, per-sample weighting scheme.\n\n    Inherited Ideas:\n    - Inherits `softplus(delta_cost)` to create a smooth, non-saturating adaptive margin signal (from Parent 1).\n    - Inherits the use of a 'rank gap' to modulate the loss based on the relative ranking of costs within the batch (from Parent 2).\n\n    New Coupling Ideas:\n    - Uses the adaptive margin to scale a Bradley-Terry loss instead of setting a target in a hinge loss.\n    - Combines the cost-based margin and rank-based gap into a single multiplicative dynamic weight.\n    \"\"\"\n    # For preference learning, we assume the dataset provides pairs where one is strictly better.\n    cost_w = torch.min(batch['cost_a'], batch['cost_b'])\n    cost_l = torch.max(batch['cost_a'], batch['cost_b'])\n    log_prob_w = batch['log_prob_w']\n    log_prob_l = batch['log_prob_l']\n\n    # Retrieve hyperparameters\n    alpha = extra.get('alpha', 1.0)\n    temp_cost = extra.get('temp_cost', 1.0)\n    beta = extra.get('beta', 0.1)\n\n    # 1. Calculate cost and log probability differences\n    delta_cost = cost_l - cost_w\n    delta_logp = log_prob_w - log_prob_l\n\n    # 2. Compute the base loss using a Bradley-Terry style formulation.\n    # The negative sign makes it a loss to be minimized.\n    base_loss = -F.logsigmoid(delta_logp)\n\n    # --- Create Dynamic Weight ---\n\n    # 3. Inherited Idea 1: Calculate adaptive margin signal from cost difference using softplus.\n    cost_signal = F.softplus(delta_cost / temp_cost)\n\n    # 4. Inherited Idea 2: Calculate rank gap modulation.\n    # This requires ranking all costs in the batch.\n    all_costs = torch.cat([cost_w, cost_l], dim=0)\n    sorted_costs, _ = torch.sort(all_costs)\n    \n    # Find the rank of each cost_w and cost_l in the sorted list\n    rank_w = torch.searchsorted(sorted_costs, cost_w)\n    rank_l = torch.searchsorted(sorted_costs, cost_l)\n    \n    rank_diff = (rank_l - rank_w).float()\n    \n    # The rank_gap_weight approaches 1 as rank_diff increases, and is 0 if ranks are the same.\n    rank_gap_weight = 1.0 - torch.exp(-beta * rank_diff)\n\n    # 5. New Coupling: Combine cost signal and rank gap into a single dynamic weight.\n    # The `1.0 + ...` ensures a baseline weight, preventing the weight from being zero if costs are identical.\n    # The weight is detached to ensure it only acts as a scaling factor, not a source of gradients itself.\n    dynamic_weight = ((1.0 + alpha * cost_signal) * rank_gap_weight).detach()\n\n    # 6. Modulate the base loss with the dynamic weight.\n    final_loss = base_loss * dynamic_weight\n\n    # 7. Apply optional batch weights and compute the mean.\n    weights = batch.get('weight', None)\n    if weights is not None:\n        final_loss = final_loss * weights\n        return final_loss.sum() / weights.sum().clamp(min=1e-8)\n    else:\n        return final_loss.mean()"}, "static_ok": true, "static_reason": "", "dynamic_ok": true, "dynamic_reason": "ok", "loss_value": 1.2797036170959473, "grad_norm": 0.0}
{"generation": 8, "index": 0, "ir": {"name": "Softplus-Gated Log-Sigmoid Loss with Rank-Gap Margin", "intuition": "This loss function combines the classic log-sigmoid structure with a dynamic, rank-aware margin and a smooth gating mechanism to create a stable and adaptive preference signal.\n\nInherited Ideas:\n- From 'Rank-Aware Log-Sigmoid Preference Loss' (Parent 1), it inherits the core loss structure of `-logsigmoid(...)`, which is a standard and well-behaved loss for binary preference tasks. It also inherits the idea of using the rank difference between pairs to modulate the loss, focusing more on pairs that are widely separated in the batch's cost distribution.\n- From 'Sigmoid-Gated Adaptive Margin Loss with Rank-Gap Normalization' (Parent 0), it inherits the concept of creating an adaptive margin based on the cost difference (`delta_cost`). This margin is then used to shift the log-sigmoid function, setting a target separation for the model's log probabilities.\n\nNew Coupling Ideas:\n1.  **Rank-Gap as Margin**: Instead of using `softplus(delta_cost)` as the margin, this loss uses the rank difference directly. The margin is defined as `beta * rank_gap_normalized`. This couples the target log-probability separation directly to the relative importance of the pair within the batch, rather than their absolute cost difference. This makes the loss more robust to cost scaling and focuses on relative ordering.\n2.  **Softplus Gating on Loss Term**: The entire loss term, `margin - delta_logp`, is passed through a `softplus` function before being fed into the `-logsigmoid`. The loss becomes `-logsigmoid(softplus(margin - delta_logp))`. This acts as a smooth, non-negative gate. When the model is correct (`delta_logp > margin`), the input to `softplus` is negative and close to zero, resulting in a small loss. When the model is incorrect (`delta_logp < margin`), the `softplus` acts like a linear function, producing a large positive input to `-logsigmoid` and thus a large loss. This gating prevents the loss from becoming negative and stabilizes training by ensuring the argument to `logsigmoid` is always non-negative.", "pseudocode": "1. For each pair (w, l) where cost(w) < cost(l), calculate the log probability difference `delta_logp = logp(w) - logp(l)`.\n2. Inherit from Parent 1: Determine the cost ranks for all `cost_w` and `cost_l` in the batch. Calculate the rank difference `rank_diff = rank(cost_l) - rank(cost_w)`.\n3. New Coupling (Rank-Gap as Margin): Normalize the rank difference by the batch size. Create an adaptive margin directly from this normalized rank gap: `margin = beta * (rank_diff / batch_size)`.\n4. New Coupling (Softplus Gating): Calculate the difference between the target margin and the model's output: `diff = margin - delta_logp`. Apply a softplus gate to this difference to ensure it's non-negative and smooth: `gated_diff = softplus(diff)`.\n5. Inherit from Parent 1: Compute the core per-sample loss using the `-logsigmoid` structure, but with the gated difference as its argument: `loss = -logsigmoid(gated_diff)`.\n6. Return the weighted mean of the per-sample losses over the batch.", "hyperparams": {"beta": 1.0}, "operators_used": ["logsigmoid", "softplus", "rank_gap"], "implementation_hint": {"expects": ["cost_a", "cost_b", "logp_w", "logp_l"], "returns": "scalar"}, "code": "import torch\nimport torch.nn.functional as F\n\ndef generated_loss(batch, model_output, extra):\n    \"\"\"\n    Computes the Softplus-Gated Log-Sigmoid Loss with Rank-Gap Margin.\n\n    Inherits:\n    - The core `-logsigmoid` loss structure (from Parent 1).\n    - The use of rank differences to create a batch-aware signal (from both Parents).\n\n    Introduces:\n    - A new coupling where the normalized rank gap directly forms the margin.\n    - A `softplus` gate on the `margin - delta_logp` term to ensure stability and a non-negative input to the `logsigmoid`.\n    \"\"\"\n    # For preference learning, we assume cost_w < cost_l.\n    cost_w = torch.min(batch['cost_a'], batch['cost_b'])\n    cost_l = torch.max(batch['cost_a'], batch['cost_b'])\n    log_prob_w = batch['log_prob_w']\n    log_prob_l = batch['log_prob_l']\n\n    # Retrieve hyperparameters\n    beta = extra.get('beta', 1.0)\n\n    # 1. Calculate log probability difference\n    delta_logp = log_prob_w - log_prob_l\n    batch_size = delta_logp.shape[0]\n\n    # 2. Inherited: Calculate rank differences\n    # Combine all costs to establish a batch-wide ranking.\n    all_costs = torch.cat([cost_w, cost_l], dim=0)\n    sorted_costs, _ = torch.sort(all_costs)\n    \n    # Find ranks efficiently using searchsorted.\n    rank_w = torch.searchsorted(sorted_costs, cost_w)\n    rank_l = torch.searchsorted(sorted_costs, cost_l)\n    rank_diff = (rank_l - rank_w).float()\n\n    # 3. New Coupling: Rank-Gap as Margin\n    # The margin is proportional to how far apart the items are in the batch's cost ranking.\n    # Normalizing by 2*batch_size (total items) makes it scale-invariant.\n    # Detach to treat the margin as a fixed target for the current step.\n    adaptive_margin = (beta * rank_diff / (2 * batch_size)).detach()\n\n    # 4. New Coupling: Softplus Gating\n    # The term `margin - delta_logp` represents how much the model misses the target separation.\n    # Applying softplus ensures the input to logsigmoid is non-negative, enhancing stability.\n    # It smoothly transitions from near-zero (when model is correct) to linear (when incorrect).\n    gated_diff = F.softplus(adaptive_margin - delta_logp)\n\n    # 5. Inherited: Compute the core loss with the gated difference\n    # This is equivalent to log(1 + exp(-gated_diff)), which is always well-behaved.\n    loss = -F.logsigmoid(gated_diff)\n\n    # 6. Apply optional weights and compute the mean.\n    weights = batch.get('weight', None)\n    if weights is not None:\n        loss = loss * weights\n        # Use clamp to prevent division by zero for an all-zero weight tensor\n        return loss.sum() / weights.sum().clamp(min=1e-8)\n    else:\n        return loss.mean()"}, "static_ok": true, "static_reason": "", "dynamic_ok": true, "dynamic_reason": "ok", "loss_value": 0.32029983401298523, "grad_norm": 0.0}
{"generation": 8, "index": 1, "ir": {"name": "Rank-Normalized Bradley-Terry Loss with Confidence Gating", "intuition": "This loss function reframes the preference learning problem by directly weighting the classic Bradley-Terry loss (`-logsigmoid(delta_logp)`) with a composite, batch-aware signal. This signal is designed to be robust and focus the model on meaningful pairs.\n\nInherited Ideas:\n- From 'Rank-Aware Log-Sigmoid Preference Loss' (Parent 1), it inherits the core structure of a Bradley-Terry-like loss, `-weight * logsigmoid(delta_logp)`. This provides a probabilistic interpretation and a strong, non-saturating gradient when the model is wrong.\n- From 'Sigmoid-Gated Adaptive Margin Loss with Rank-Gap Normalization' (Parent 0), it inherits the idea of using the rank gap to directly normalize a component of the loss. Instead of normalizing the margin, it now normalizes the `delta_cost` signal itself, making the cost-based weighting adaptive to the pair's relative importance within the batch.\n\nNew Coupling Ideas:\n1.  **Rank-Gap Normalization of Cost Signal**: The `delta_cost` is first passed through a `softplus` to ensure it is non-negative and smooth. This raw signal is then divided by `1 + beta * rank_gap_normalized`. This couples the cost information with rank information directly. For pairs with a large rank gap (easy to distinguish), the denominator grows, down-weighting their contribution and allowing the model to focus on finer-grained, harder examples with small rank gaps.\n2.  **Tanh Confidence Gating**: The final loss is gated by `(1 - tanh(delta_logp))`. This gate has two effects: for pairs the model is already confident about (large positive `delta_logp`), `tanh` approaches 1, and the gate `(1 - tanh)` approaches 0, reducing the loss and preventing the model from over-optimizing on easy pairs. For pairs the model is confidently wrong about (large negative `delta_logp`), `tanh` approaches -1, and the gate approaches 2, amplifying the loss signal and forcing the model to correct its most significant errors.", "pseudocode": "1. For each pair (w, l) where cost(w) < cost(l), calculate the cost difference `delta_cost = cost(l) - cost(w)` and the log probability difference `delta_logp = logp(w) - logp(l)`.\n2. Inherit from Parent 1: Calculate the core loss term using the log-sigmoid function, `base_loss = -logsigmoid(delta_logp)`.\n3. Inherit from Parent 0: Determine the cost ranks for all `cost_w` and `cost_l` in the batch. Calculate the normalized rank difference `rank_gap_norm = rank_diff / (2 * batch_size)`.\n4. New Coupling (Rank-Gap Normalization of Cost Signal): Create a smooth, non-negative cost signal using `softplus(delta_cost / temp_cost)`. Normalize this signal by the rank gap: `adaptive_weight = softplus(...) / (1.0 + beta * rank_gap_norm)`. This weight is large for pairs with small rank gaps and small for pairs with large rank gaps.\n5. New Coupling (Tanh Confidence Gating): Create a confidence gate that down-weights easy examples and up-weights confidently wrong examples: `confidence_gate = 1.0 - tanh(delta_logp)`.\n6. Combine the components: Modulate the base loss with both the adaptive weight and the confidence gate: `loss = confidence_gate * adaptive_weight * base_loss`.\n7. Return the weighted mean of the per-sample losses over the batch.", "hyperparams": {"beta": 1.0, "temp_cost": 1.0}, "operators_used": ["logsigmoid", "softplus", "tanh", "rank_gap"], "implementation_hint": {"expects": ["cost_a", "cost_b", "logp_w", "logp_l"], "returns": "scalar"}, "code": "import torch\nimport torch.nn.functional as F\n\ndef generated_loss(batch, model_output, extra):\n    \"\"\"\n    Computes the Rank-Normalized Bradley-Terry Loss with Confidence Gating.\n\n    Inherits:\n    - The core `-weight * logsigmoid(delta_logp)` structure (from Parent 1).\n    - The use of rank gap for normalization (from Parent 0).\n\n    Introduces:\n    - A new coupling where rank gap normalizes the softplus-transformed cost difference, creating a batch-aware adaptive weight.\n    - A `(1 - tanh(delta_logp))` gate to modulate the loss based on model confidence, focusing on hard or incorrect examples.\n    \"\"\"\n    # For preference learning, we assume cost_w < cost_l.\n    cost_w = torch.min(batch['cost_a'], batch['cost_b'])\n    cost_l = torch.max(batch['cost_a'], batch['cost_b'])\n    log_prob_w = batch['log_prob_w']\n    log_prob_l = batch['log_prob_l']\n\n    # Retrieve hyperparameters\n    beta = extra.get('beta', 1.0)\n    temp_cost = extra.get('temp_cost', 1.0)\n\n    # 1. Calculate cost and log probability differences\n    delta_cost = cost_l - cost_w\n    delta_logp = log_prob_w - log_prob_l\n    batch_size = delta_cost.shape[0]\n\n    # 2. Inherited from Parent 1: Core Bradley-Terry / log-sigmoid loss term\n    base_loss = -F.logsigmoid(delta_logp)\n\n    # 3. Inherited from Parent 0: Calculate rank differences\n    all_costs = torch.cat([cost_w, cost_l], dim=0)\n    sorted_costs, _ = torch.sort(all_costs)\n    rank_w = torch.searchsorted(sorted_costs, cost_w)\n    rank_l = torch.searchsorted(sorted_costs, cost_l)\n    rank_diff = (rank_l - rank_w).float()\n    # Normalize by total number of items to make it independent of batch size\n    rank_gap_norm = rank_diff / (2 * batch_size)\n\n    # 4. New Coupling: Rank-Gap Normalization of Cost Signal\n    # Create a smooth, non-negative signal from the cost difference\n    cost_signal = F.softplus(delta_cost / temp_cost)\n    # Normalize the signal using the rank gap. Pairs far apart in rank (large rank_gap_norm)\n    # will have their weight reduced, focusing the loss on harder, fine-grained pairs.\n    # Adding 1.0 prevents division by zero and ensures the divisor is >= 1.\n    adaptive_weight = cost_signal / (1.0 + beta * rank_gap_norm)\n    # Detach the weight so it doesn't propagate gradients through the cost/rank calculations.\n    adaptive_weight = adaptive_weight.detach()\n\n    # 5. New Coupling: Tanh Confidence Gating\n    # When delta_logp is large & positive (confident & correct), tanh -> 1, gate -> 0. Loss is minimized.\n    # When delta_logp is large & negative (confident & incorrect), tanh -> -1, gate -> 2. Loss is amplified.\n    # When delta_logp is near 0 (uncertain), tanh -> 0, gate -> 1. Loss is unchanged.\n    confidence_gate = 1.0 - torch.tanh(delta_logp)\n\n    # 6. Combine all components for the final loss\n    loss = confidence_gate * adaptive_weight * base_loss\n\n    # 7. Apply optional weights and compute the mean\n    weights = batch.get('weight', None)\n    if weights is not None:\n        loss = loss * weights\n        # Use clamp to prevent division by zero for an all-zero weight tensor\n        return loss.sum() / weights.sum().clamp(min=1e-8)\n    else:\n        return loss.mean()"}, "static_ok": true, "static_reason": "", "dynamic_ok": true, "dynamic_reason": "ok", "loss_value": 0.6068558096885681, "grad_norm": 0.0}
{"generation": 8, "index": 2, "ir": {"name": "Rank-Gated Log-Sigmoid Loss with Softplus Margin", "intuition": "This loss function combines the classic Bradley-Terry structure of a log-sigmoid loss with an adaptive, non-saturating margin, gated by a smooth, rank-aware weighting factor. The goal is to create a loss that is sensitive to both the magnitude of cost differences and the relative ranking of pairs within a batch.\n\nInherited Ideas:\n- From 'Rank-Aware Log-Sigmoid Preference Loss' (Parent 1), it inherits the core loss structure `-logsigmoid(delta_logp - margin)`. This is a robust and well-understood preference loss formulation.\n- From 'Sigmoid-Gated Adaptive Margin Loss with Rank-Gap Normalization' (Parent 0), it inherits the idea of using `softplus(delta_cost)` to create a smooth, non-saturating, and adaptive margin that grows with the cost difference. This ensures that pairs with larger cost differences are expected to have a larger log-probability separation.\n\nNew Coupling Ideas:\n1. **Rank-Gap Softplus Gating**: Instead of using the rank gap to normalize the margin (as in Parent 0) or as part of a final loss weight (as in Parent 1), it is used to create a smooth gate that multiplies the entire loss term. The rank difference is passed through `softplus(beta * rank_diff)`, creating a non-negative, non-saturating weight. This smoothly emphasizes pairs that are far apart in the batch's cost distribution, focusing the model's attention on learning the most significant preferences first, while still applying a non-zero loss to all pairs.\n2. **Margin Clamping for Stability**: While the softplus margin is non-saturating, it can grow very large for pairs with extreme cost differences, potentially leading to large gradients. A new stability trick is introduced: `clamp(margin, max=max_margin)`. This caps the target log-probability separation at a reasonable maximum, preventing outlier pairs from dominating the batch gradient and improving training stability without sacrificing the adaptive nature of the margin for most pairs.", "pseudocode": "1. For each pair (w, l) where cost(w) < cost(l), calculate `delta_cost = cost(l) - cost(w)` and `delta_logp = logp(w) - logp(l)`.\n2. Inherit from Parent 0: Calculate a smooth, non-saturating adaptive margin using `softplus`: `margin_base = alpha * softplus(delta_cost / temp_cost)`.\n3. New Coupling (Stability): Clamp the margin to a maximum value to prevent gradient explosion from outlier pairs: `margin = clamp(margin_base, max=max_margin)`.\n4. Determine the cost ranks for all `cost_w` and `cost_l` in the batch. Calculate the rank difference `rank_diff = rank(cost_l) - rank(cost_w)`.\n5. New Coupling (Rank Gating): Create a smooth, non-saturating gate by applying `softplus` to the scaled rank difference: `rank_gate = softplus(beta * rank_diff)`. This gate will weight the final loss.\n6. Inherit from Parent 1: Compute the core preference loss using the log-sigmoid function, incorporating the adaptive margin: `per_sample_loss = -logsigmoid(delta_logp - margin)`.\n7. Apply the rank gate to modulate the loss: `gated_loss = rank_gate * per_sample_loss`.\n8. Return the weighted mean of the `gated_loss` over the batch.", "hyperparams": {"alpha": 1.0, "temp_cost": 1.0, "beta": 0.1, "max_margin": 10.0}, "operators_used": ["logsigmoid", "softplus", "clamp", "rank_gap"], "implementation_hint": {"expects": ["cost_a", "cost_b", "logp_w", "logp_l"], "returns": "scalar"}, "code": "import torch\nimport torch.nn.functional as F\n\ndef generated_loss(batch, model_output, extra):\n    \"\"\"\n    Computes the Rank-Gated Log-Sigmoid Loss with Softplus Margin.\n\n    Inherits:\n    - The use of `softplus(delta_cost)` for a non-saturating margin (from Parent 0).\n    - The core `-logsigmoid(delta_logp - margin)` structure (from Parent 1).\n\n    Introduces:\n    - A softplus-gated rank gap, `softplus(beta * rank_diff)`, to smoothly weight the loss of each pair.\n    - A stability trick by clamping the adaptive margin to a maximum value (`max_margin`).\n    \"\"\"\n    # For preference learning, we assume cost_w < cost_l.\n    cost_w = torch.min(batch['cost_a'], batch['cost_b'])\n    cost_l = torch.max(batch['cost_a'], batch['cost_b'])\n    log_prob_w = batch['log_prob_w']\n    log_prob_l = batch['log_prob_l']\n\n    # Retrieve hyperparameters\n    alpha = extra.get('alpha', 1.0)\n    temp_cost = extra.get('temp_cost', 1.0)\n    beta = extra.get('beta', 0.1)\n    max_margin = extra.get('max_margin', 10.0)\n\n    # 1. Calculate cost and log probability differences\n    delta_cost = cost_l - cost_w\n    delta_logp = log_prob_w - log_prob_l\n\n    # 2. Inherited from Parent 0: Calculate a smooth, non-saturating adaptive margin.\n    # The margin increases as the difference in cost increases.\n    margin_base = alpha * F.softplus(delta_cost / temp_cost)\n\n    # 3. New Coupling (Stability): Clamp the margin to prevent instability from extreme cost differences.\n    adaptive_margin = torch.clamp(margin_base, max=max_margin)\n\n    # 4. Calculate rank differences.\n    # Combine all costs to establish a batch-wide ranking.\n    all_costs = torch.cat([cost_w, cost_l], dim=0)\n    sorted_costs, _ = torch.sort(all_costs)\n    \n    # Find ranks efficiently using searchsorted.\n    rank_w = torch.searchsorted(sorted_costs, cost_w)\n    rank_l = torch.searchsorted(sorted_costs, cost_l)\n    rank_diff = (rank_l - rank_w).float()\n\n    # 5. New Coupling (Rank Gating): Create a smooth, non-saturating gate from rank difference.\n    # This gives more weight to pairs that are far apart in the cost ranking.\n    rank_gate = F.softplus(beta * rank_diff)\n    \n    # 6. Inherited from Parent 1: Compute the core preference loss using log-sigmoid.\n    # The model is encouraged to make `delta_logp > adaptive_margin`.\n    per_sample_loss = -F.logsigmoid(delta_logp - adaptive_margin.detach())\n\n    # 7. Apply the rank gate to the loss.\n    # Detach the gate so it acts as a pure weight, not a gradient path for cost models.\n    gated_loss = rank_gate.detach() * per_sample_loss\n\n    # 8. Apply optional weights and compute the mean.\n    weights = batch.get('weight', None)\n    if weights is not None:\n        gated_loss = gated_loss * weights\n        # Use clamp to avoid division by zero if all weights are zero.\n        return gated_loss.sum() / weights.sum().clamp(min=1e-8)\n    else:\n        return gated_loss.mean()"}, "static_ok": true, "static_reason": "", "dynamic_ok": true, "dynamic_reason": "ok", "loss_value": 2.7676234245300293, "grad_norm": 0.0}
{"generation": 8, "index": 3, "ir": {"name": "Rank-Gated Log-Sigmoid Loss with Z-Score Margin", "intuition": "This loss function creates a preference signal that is both robust to the scale of costs and sensitive to the relative ranking of examples within a batch. It combines the classic Bradley-Terry log-sigmoid structure with a dynamically scaled margin that is further modulated by a rank-based gate.\n\nInherited Ideas:\n- From 'Rank-Aware Log-Sigmoid Preference Loss' (Parent 1), it inherits the core loss structure of `-logsigmoid(delta_logp - margin)`. This provides a probabilistic interpretation where the model is encouraged to make the log-probability of the winning candidate greater than the loser's by at least a certain margin.\n- From 'Sigmoid-Gated Adaptive Margin Loss with Rank-Gap Normalization' (Parent 0), it inherits the use of `rank_gap` to modulate the loss, emphasizing pairs that are far apart in the batch's cost distribution. It also inherits the general principle of creating an adaptive margin based on cost differences.\n\nNew Coupling Ideas:\n1.  **Z-Score Normalized Margin**: The margin `M` is derived from the Z-score of the cost differences (`delta_cost`). The margin is calculated as `alpha * softplus(zscore(delta_cost))`. This makes the margin's scale invariant to the absolute magnitude and variance of costs in a batch, leading to more stable training and less sensitivity to the `alpha` hyperparameter. The `softplus` ensures the margin is always non-negative and smooth.\n2.  **Tanh-Gated Rank Weighting**: The final loss is multiplied by a weight derived from the rank difference. This weight is computed as `1.0 + beta * tanh(rank_diff / batch_size)`. The `tanh` function creates a smooth, bounded (between 0 and 2) weight. For pairs with a small rank difference, the weight is close to 1.0. As the rank difference grows, the weight smoothly increases, saturating at `1.0 + beta`. This provides a stable mechanism to up-weight the importance of distinguishing clearly separated pairs without risking the exploding gradients that an unbounded weight (like `softplus` or linear scaling) might cause.", "pseudocode": "1. For each pair (w, l) where cost(w) < cost(l), calculate the cost difference `delta_cost = cost(l) - cost(w)` and the log probability difference `delta_logp = logp(w) - logp(l)`.\n2. New Coupling (Z-Score Margin): Standardize the `delta_cost` vector for the batch using Z-score normalization. Calculate the adaptive margin `M = alpha * softplus(zscore(delta_cost))`. This margin is robust to cost scale.\n3. Inherit from Parent 1: Compute the core loss using a log-sigmoid structure with the adaptive margin: `base_loss = -logsigmoid(delta_logp - M)`.\n4. Inherit from Parent 0: Determine the cost ranks for all `cost_w` and `cost_l` in the batch. Calculate the rank difference `rank_diff = rank(cost_l) - rank(cost_w)`.\n5. New Coupling (Tanh-Gated Rank Weighting): Create a smooth, bounded weight from the rank difference: `rank_weight = 1.0 + beta * tanh(rank_diff / batch_size)`. Dividing by batch size provides scale invariance.\n6. Modulate the base loss with the rank weight: `final_loss = rank_weight * base_loss`.\n7. Return the weighted mean of the per-sample losses over the batch.", "hyperparams": {"alpha": 0.5, "beta": 1.0}, "operators_used": ["logsigmoid", "softplus", "zscore", "rank_gap", "tanh"], "implementation_hint": {"expects": ["cost_a", "cost_b", "logp_a", "logp_b"], "returns": "scalar"}, "code": "import torch\nimport torch.nn.functional as F\n\ndef generated_loss(batch, model_output, extra):\n    \"\"\"\n    Computes the Rank-Gated Log-Sigmoid Loss with Z-Score Margin.\n\n    Inherits:\n    - The core `-logsigmoid(delta_logp - margin)` structure (from Parent 1).\n    - The use of rank differences to modulate the loss signal (from Parent 0).\n\n    Introduces:\n    - A Z-score normalized margin `alpha * softplus(zscore(delta_cost))` for cost scale invariance.\n    - A tanh-gated rank weight `1 + beta * tanh(rank_diff / N)` for smooth, bounded emphasis on separated pairs.\n    \"\"\"\n    # For preference learning, we assume cost_w < cost_l.\n    cost_w = torch.min(batch['cost_a'], batch['cost_b'])\n    cost_l = torch.max(batch['cost_a'], batch['cost_b'])\n    log_prob_w = batch['log_prob_w']\n    log_prob_l = batch['log_prob_l']\n\n    # Retrieve hyperparameters\n    alpha = extra.get('alpha', 0.5)\n    beta = extra.get('beta', 1.0)\n\n    # 1. Calculate cost and log probability differences\n    delta_cost = cost_l - cost_w\n    delta_logp = log_prob_w - log_prob_l\n    batch_size = delta_cost.shape[0]\n\n    # Handle batches with no cost variance to prevent NaN from z-score\n    if delta_cost.std() < 1e-8:\n        margin = torch.zeros_like(delta_cost)\n    else:\n        # 2. New Coupling (Z-Score Margin): Standardize delta_cost and create margin\n        delta_cost_mean = delta_cost.mean()\n        delta_cost_std = delta_cost.std().clamp(min=1e-8)\n        delta_cost_zscore = (delta_cost - delta_cost_mean) / delta_cost_std\n        margin = alpha * F.softplus(delta_cost_zscore)\n\n    # 3. Inherit from Parent 1: Compute the core log-sigmoid loss with the margin\n    # Detach margin so it acts as a target, not part of the backprop path for cost models\n    base_loss = -F.logsigmoid(delta_logp - margin.detach())\n\n    # 4. Inherit from Parent 0: Calculate rank differences\n    all_costs = torch.cat([cost_w, cost_l], dim=0)\n    sorted_costs, _ = torch.sort(all_costs)\n    rank_w = torch.searchsorted(sorted_costs, cost_w)\n    rank_l = torch.searchsorted(sorted_costs, cost_l)\n    rank_diff = (rank_l - rank_w).float()\n\n    # 5. New Coupling (Tanh-Gated Rank Weighting): Create a smooth, bounded weight\n    # The weight ranges from 1.0 to (1.0 + beta), providing stable emphasis.\n    rank_weight = 1.0 + beta * torch.tanh(rank_diff / (2 * batch_size))\n\n    # 6. Modulate the loss with the rank weight\n    # Detach weight to ensure it only scales the loss magnitude\n    final_loss = rank_weight.detach() * base_loss\n\n    # 7. Apply optional weights and compute the mean\n    weights = batch.get('weight', None)\n    if weights is not None:\n        final_loss = final_loss * weights\n        return final_loss.sum() / weights.sum().clamp(min=1e-8)\n    else:\n        return final_loss.mean()"}, "static_ok": true, "static_reason": "", "dynamic_ok": true, "dynamic_reason": "ok", "loss_value": 1.0134624242782593, "grad_norm": 0.0}
{"generation": 8, "index": 4, "ir": {"name": "Rank-Gated Log-Sigmoid Loss with Tanh Margin", "intuition": "This loss function creates a preference signal that is both sensitive to the magnitude of cost differences and robust to the batch-wise distribution of those costs. It combines a classic log-sigmoid structure with a bounded, adaptive margin and a novel rank-based gating mechanism.\n\nInherited Ideas:\n- From 'Rank-Aware Log-Sigmoid Preference Loss' (Parent 1), it inherits the core loss structure of a weighted log-sigmoid: `weight * -logsigmoid(delta_logp - margin)`. This provides a probabilistic interpretation and a strong gradient for misclassified pairs.\n- From 'Sigmoid-Gated Adaptive Margin Loss with Rank-Gap Normalization' (Parent 0), it inherits the use of `rank_gap` to create a batch-aware signal that emphasizes pairs with a large separation in the cost distribution. This helps the model prioritize learning from more distinct pairs within a batch.\n\nNew Coupling Ideas:\n1.  **Tanh Adaptive Margin**: A new margin is created using `tanh(alpha * delta_cost)`. This margin is adaptive to the cost difference but is bounded between 0 and 1. This prevents pairs with extremely large cost differences from creating an unbounded margin, which could lead to gradient explosion and dominate the training. The `tanh` function provides a smooth saturation, balancing sensitivity to small cost differences with stability for large ones.\n2.  **Rank-Gated Loss**: The entire per-sample loss is gated by a rank-based term. The rank difference between the worse and better item is normalized by the batch size and then passed through a `softplus` function: `softplus(beta * normalized_rank_diff)`. This acts as a smooth, non-negative gate that starts near `log(2)` for items with similar ranks and grows as the rank separation increases. This directly couples the batch-wise importance of a pair (its rank gap) to the magnitude of its loss contribution, effectively focusing the training on pairs that are most informative from a batch perspective.", "pseudocode": "1. For each pair (w, l) where cost(w) < cost(l), calculate the cost difference `delta_cost = cost(l) - cost(w)` and the log probability difference `delta_logp = logp(w) - logp(l)`.\n2. New Coupling (Tanh Margin): Calculate a bounded, adaptive margin using the hyperbolic tangent function: `margin = tanh(alpha * delta_cost)`.\n3. Inherit from Parent 1 (Core Structure): Calculate the base loss using a log-sigmoid function, incorporating the new margin: `base_loss = -logsigmoid(delta_logp - margin)`.\n4. Inherit from Parent 0 (Rank Signal): Determine the cost ranks for all `cost_w` and `cost_l` in the batch. Calculate the rank difference `rank_diff = rank(cost_l) - rank(cost_w)`.\n5. New Coupling (Rank-Gated Loss): Normalize the rank difference by the batch size. Apply a `softplus` function to this normalized difference to create a smooth, non-negative gate: `rank_gate = softplus(beta * rank_diff / batch_size)`.\n6. Combine the base loss and the rank gate: `final_loss = rank_gate * base_loss`.\n7. Return the weighted mean of the per-sample losses over the batch.", "hyperparams": {"alpha": 0.5, "beta": 5.0}, "operators_used": ["logsigmoid", "tanh", "softplus", "rank_gap"], "implementation_hint": {"expects": ["cost_a", "cost_b", "log_prob_w", "log_prob_l", "weight"], "returns": "scalar"}, "code": "import torch\nimport torch.nn.functional as F\n\ndef generated_loss(batch, model_output, extra):\n    \"\"\"\n    Computes the Rank-Gated Log-Sigmoid Loss with Tanh Margin.\n\n    Inherits:\n    - The core log-sigmoid loss structure (from Parent 1).\n    - The use of rank differences for batch-aware importance weighting (from Parent 0).\n\n    Introduces:\n    - A new tanh-based adaptive margin that is bounded, preventing gradient explosion from large cost differences.\n    - A new coupling where a softplus-transformed rank gap directly gates the final loss value for each pair.\n    \"\"\"\n    # Ensure cost_w < cost_l for preference learning\n    cost_w = torch.min(batch['cost_a'], batch['cost_b'])\n    cost_l = torch.max(batch['cost_a'], batch['cost_b'])\n    log_prob_w = batch['log_prob_w']\n    log_prob_l = batch['log_prob_l']\n\n    # Retrieve hyperparameters\n    alpha = extra.get('alpha', 0.5)\n    beta = extra.get('beta', 5.0)\n\n    # 1. Calculate cost and log probability differences\n    delta_cost = cost_l - cost_w\n    delta_logp = log_prob_w - log_prob_l\n    batch_size = delta_cost.shape[0]\n\n    # 2. New Coupling: Tanh Adaptive Margin\n    # tanh creates a margin that is sensitive to delta_cost but bounded in [0, 1].\n    # This prevents extremely large cost differences from creating unstable, unbounded margins.\n    margin = torch.tanh(alpha * delta_cost)\n\n    # 3. Inherited from Parent 1: Core log-sigmoid loss structure\n    # The goal is to make (delta_logp - margin) as large and positive as possible.\n    base_loss = -F.logsigmoid(delta_logp - margin.detach())\n\n    # 4. Inherited from Parent 0: Calculate rank differences\n    # Combine all costs to establish a batch-wide ranking.\n    all_costs = torch.cat([cost_w, cost_l], dim=0)\n    sorted_costs, _ = torch.sort(all_costs)\n    \n    # Find ranks efficiently using searchsorted.\n    rank_w = torch.searchsorted(sorted_costs, cost_w)\n    rank_l = torch.searchsorted(sorted_costs, cost_l)\n    rank_diff = (rank_l - rank_w).float()\n\n    # 5. New Coupling: Rank-Gated Loss\n    # Normalize rank_diff by batch size to make it scale-invariant.\n    # softplus creates a smooth, non-negative gate that increases with rank separation.\n    # This emphasizes pairs that are well-separated in the batch's cost distribution.\n    normalized_rank_diff = rank_diff / (2 * batch_size)\n    rank_gate = F.softplus(beta * normalized_rank_diff)\n    \n    # 6. Combine the base loss and the rank gate\n    # The gate acts as a dynamic weight, focusing on important pairs.\n    final_loss = rank_gate.detach() * base_loss\n\n    # 7. Apply optional weights and compute the mean.\n    weights = batch.get('weight', None)\n    if weights is not None:\n        final_loss = final_loss * weights\n        # Use clamp to avoid division by zero for an all-zero weight tensor\n        return final_loss.sum() / weights.sum().clamp(min=1e-8)\n    else:\n        return final_loss.mean()"}, "static_ok": true, "static_reason": "", "dynamic_ok": true, "dynamic_reason": "ok", "loss_value": 2.4516618251800537, "grad_norm": 0.0}
{"generation": 8, "index": 5, "ir": {"name": "Rank-Gated Log-Sigmoid Loss with Tanh Margin", "intuition": "This loss function synergizes a classic log-sigmoid structure with a dynamic, rank-aware margin. It aims to be robust by normalizing cost differences while focusing learning on pairs where the model is uncertain or incorrect.\n\nInherited Ideas:\n- From 'Rank-Aware Log-Sigmoid Preference Loss' (Parent 1), it inherits the core loss structure of `-logsigmoid(delta_logp - margin)`. This provides a strong probabilistic foundation, encouraging the model to maximize the log-probability difference, but now with an explicit margin term.\n- From 'Sigmoid-Gated Adaptive Margin Loss with Rank-Gap Normalization' (Parent 0), it inherits the idea of using the rank difference (`rank_gap`) within the batch to directly modulate a key component of the loss. Instead of modulating the margin, here it gates the entire loss value.\n\nNew Coupling Ideas:\n1.  **Tanh-based Adaptive Margin**: The margin is calculated as `alpha * tanh(beta * delta_cost_norm)`. This couples a bounded activation function (`tanh`) with a batch-normalized cost difference (`zscore`). The `tanh` function creates a saturating margin, preventing extremely large cost differences from creating an unstable, unbounded margin. This is a stable alternative to the unbounded `softplus` margin seen in the parents.\n2.  **Rank-Gap Gating of the Loss**: The final per-sample loss is multiplied by a rank-based gate: `loss * (1 + rank_gap / batch_size)`. This directly amplifies the loss for pairs that are far apart in the batch's cost distribution, forcing the model to prioritize learning these more significant preferences. The scaling by `batch_size` ensures the gate's magnitude is consistent across different batch sizes.", "pseudocode": "1. For each pair (w, l), calculate the cost difference `delta_cost = cost(l) - cost(w)` and the log probability difference `delta_logp = logp(w) - logp(l)`.\n2. Normalize the `delta_cost` vector across the batch using Z-score to get `delta_cost_norm`. This makes the margin scale-invariant.\n3. New Coupling (Tanh Margin): Compute a bounded, adaptive margin using the hyperbolic tangent function: `margin = alpha * tanh(beta * delta_cost_norm)`.\n4. Inherited from Parent 1: Calculate the core preference loss using a margin-based log-sigmoid formulation: `base_loss = -logsigmoid(delta_logp - margin)`.\n5. Inherited from Parent 0: Determine the cost ranks for all `cost_w` and `cost_l` in the batch. Calculate the rank difference `rank_gap = rank(cost_l) - rank(cost_w)`.\n6. New Coupling (Rank-Gap Gating): Create a gate that amplifies the loss for pairs with a large rank gap: `rank_gate = 1.0 + (rank_gap / batch_size)`.\n7. Apply the gate to the base loss: `final_loss = base_loss * rank_gate`.\n8. Return the weighted mean of the `final_loss` over the batch.", "hyperparams": {"alpha": 1.0, "beta": 0.5}, "operators_used": ["logsigmoid", "tanh", "zscore", "rank_gap"], "implementation_hint": {"expects": ["cost_a", "cost_b", "logp_a", "logp_b", "weight"], "returns": "scalar"}, "code": "import torch\nimport torch.nn.functional as F\n\ndef generated_loss(batch, model_output, extra):\n    \"\"\"\n    Computes the Rank-Gated Log-Sigmoid Loss with Tanh Margin.\n\n    Inherits:\n    - The core `-logsigmoid(delta_logp - margin)` structure (from Parent 1).\n    - The use of rank differences for batch-aware modulation (from Parent 0).\n\n    Introduces:\n    - A bounded adaptive margin using `tanh` on z-scored cost differences.\n    - A direct gating mechanism where the final loss is amplified by the normalized rank gap.\n    \"\"\"\n    # For preference learning, we assume cost_w < cost_l.\n    cost_w = torch.min(batch['cost_a'], batch['cost_b'])\n    cost_l = torch.max(batch['cost_a'], batch['cost_b'])\n    log_prob_w = batch['log_prob_w']\n    log_prob_l = batch['log_prob_l']\n\n    # Retrieve hyperparameters\n    alpha = extra.get('alpha', 1.0)\n    beta = extra.get('beta', 0.5)\n\n    # 1. Calculate cost and log probability differences\n    delta_cost = cost_l - cost_w\n    delta_logp = log_prob_w - log_prob_l\n    batch_size = delta_cost.shape[0]\n\n    # Early exit for a batch with no cost difference to avoid division by zero in z-score\n    if torch.all(delta_cost < 1e-8):\n        return torch.tensor(0.0, device=delta_cost.device)\n\n    # 2. Normalize delta_cost for scale-invariant margin\n    delta_cost_mean = delta_cost.mean()\n    delta_cost_std = delta_cost.std().clamp(min=1e-8)\n    delta_cost_norm = (delta_cost - delta_cost_mean) / delta_cost_std\n\n    # 3. New Coupling: Compute a bounded, adaptive margin using tanh\n    # This prevents the margin from becoming excessively large for outliers.\n    margin = alpha * torch.tanh(beta * delta_cost_norm)\n\n    # 4. Inherited from Parent 1: Calculate core log-sigmoid loss with margin\n    base_loss = -F.logsigmoid(delta_logp - margin.detach())\n\n    # 5. Inherited from Parent 0: Calculate rank differences\n    all_costs = torch.cat([cost_w, cost_l], dim=0)\n    sorted_costs, _ = torch.sort(all_costs)\n    rank_w = torch.searchsorted(sorted_costs, cost_w)\n    rank_l = torch.searchsorted(sorted_costs, cost_l)\n    rank_gap = (rank_l - rank_w).float()\n\n    # 6. New Coupling: Create a gate to amplify loss for pairs with large rank gaps\n    # Dividing by 2*N (total items) makes it a relative rank gap.\n    # Adding 1.0 ensures the gate is always >= 1.\n    rank_gate = 1.0 + (rank_gap / (2 * batch_size))\n\n    # 7. Apply the rank gate to the loss\n    final_loss = base_loss * rank_gate.detach()\n\n    # 8. Apply optional weights and compute the mean.\n    weights = batch.get('weight', None)\n    if weights is not None:\n        final_loss = final_loss * weights\n        return final_loss.sum() / weights.sum().clamp(min=1e-8)\n    else:\n        return final_loss.mean()"}, "static_ok": true, "static_reason": "", "dynamic_ok": true, "dynamic_reason": "ok", "loss_value": 1.0397206544876099, "grad_norm": 0.0}
{"generation": 8, "index": 6, "ir": {"name": "Rank-Normalized Bradley-Terry Loss with Gated Margin", "intuition": "This loss function reframes the preference learning problem by adaptively weighting the classic Bradley-Terry (log-sigmoid) loss. It combines a rank-normalized cost signal with a smoothly gated margin to create a stable and expressive objective.\n\nInherited Ideas:\n- From 'Rank-Aware Log-Sigmoid Preference Loss' (Parent 1), it inherits the core structure of a weighted log-sigmoid loss: `-weight * logsigmoid(delta_logp)`. This provides a probabilistic interpretation and a strong gradient signal.\n- From 'Sigmoid-Gated Adaptive Margin Loss with Rank-Gap Normalization' (Parent 0), it inherits the idea of using a rank-based normalization scheme. Instead of normalizing a margin, we normalize the cost difference itself, making the loss scale-invariant with respect to the batch's cost distribution.\n\nNew Coupling Ideas:\n1. **Rank-Gap Normalization of Cost Signal**: The `delta_cost` is normalized by the batch's rank gap (`rank_l - rank_w`). This novel coupling, `delta_cost / (1 + rank_gap)`, makes the cost signal relative. For pairs with a large rank gap (easy to distinguish), the cost signal is down-weighted, preventing them from dominating the loss. For pairs with a small rank gap (hard to distinguish), the cost signal is amplified, focusing the model on fine-grained differences. This is more direct than Parent 0's margin normalization.\n2. **Tanh-Gated Margin**: A margin is introduced to the log-probability difference: `delta_logp - margin`. This margin is dynamically calculated using a `tanh` gate on the normalized cost signal: `gamma * tanh(normalized_cost_signal)`. The `tanh` function creates a bounded, saturating margin. This prevents extremely large cost differences from creating an unbounded margin that could cause gradient explosion, ensuring numerical stability while still providing a strong separation target for most pairs.", "pseudocode": "1. For each pair (w, l) where cost(w) < cost(l), calculate the cost difference `delta_cost = cost(l) - cost(w)` and the log probability difference `delta_logp = logp(w) - logp(l)`.\n2. Inherit from Parent 0: Determine the cost ranks for all `cost_w` and `cost_l` in the batch. Calculate the rank difference `rank_gap = rank(cost_l) - rank(cost_w)`.\n3. New Coupling (Rank-Gap Normalization of Cost): Create a rank-normalized cost signal by dividing the cost difference by the rank gap: `normalized_cost_signal = delta_cost / (1.0 + rank_gap)`. Adding 1.0 to the denominator prevents division by zero.\n4. New Coupling (Tanh-Gated Margin): Compute a bounded, adaptive margin using the `tanh` function on the normalized cost signal: `margin = gamma * tanh(normalized_cost_signal)`. `gamma` is a hyperparameter controlling the maximum margin size.\n5. Inherit from Parent 1: Use the core structure of a weighted log-sigmoid loss. The loss is computed as `loss = -logsigmoid(delta_logp - margin)`.\n6. Create an adaptive weight for each loss term using `softplus` on the normalized cost signal: `adaptive_weight = softplus(normalized_cost_signal)`.\n7. Combine the weight and the core loss: `final_loss = adaptive_weight * loss`.\n8. Return the weighted mean of `final_loss` over the batch.", "hyperparams": {"gamma": 0.5}, "operators_used": ["logsigmoid", "softplus", "tanh", "rank_gap"], "implementation_hint": {"expects": ["cost_a", "cost_b", "log_prob_w", "log_prob_l", "weight"], "returns": "scalar"}, "code": "import torch\nimport torch.nn.functional as F\n\ndef generated_loss(batch, model_output, extra):\n    \"\"\"\n    Computes the Rank-Normalized Bradley-Terry Loss with Gated Margin.\n\n    Inherits:\n    - The core weighted log-sigmoid loss structure from Parent 1.\n    - The use of rank differences for batch-aware normalization from Parent 0.\n\n    Introduces:\n    - A new coupling where the cost difference is directly normalized by the rank gap.\n    - A tanh-gated margin applied to the log-probability difference for stable separation.\n    \"\"\"\n    # For preference learning, we assume cost_w < cost_l.\n    cost_w = torch.min(batch['cost_a'], batch['cost_b'])\n    cost_l = torch.max(batch['cost_a'], batch['cost_b'])\n    log_prob_w = batch['log_prob_w']\n    log_prob_l = batch['log_prob_l']\n\n    # Retrieve hyperparameters\n    gamma = extra.get('gamma', 0.5)\n\n    # 1. Calculate cost and log probability differences\n    delta_cost = cost_l - cost_w\n    delta_logp = log_prob_w - log_prob_l\n\n    # 2. Inherited from Parent 0: Calculate rank differences\n    all_costs = torch.cat([cost_w, cost_l], dim=0)\n    # Use searchsorted for efficient, differentiable ranking\n    sorted_costs, _ = torch.sort(all_costs)\n    rank_w = torch.searchsorted(sorted_costs, cost_w)\n    rank_l = torch.searchsorted(sorted_costs, cost_l)\n    rank_gap = (rank_l - rank_w).float().clamp(min=0) # Ensure non-negative\n\n    # 3. New Coupling: Rank-Gap Normalization of Cost Signal\n    # Add 1.0 to the denominator to prevent division by zero and handle same-rank pairs.\n    # Detach to treat normalization as a fixed target for the current batch.\n    normalized_cost_signal = (delta_cost / (1.0 + rank_gap)).detach()\n\n    # 4. New Coupling: Tanh-Gated Margin\n    # tanh creates a bounded margin, preventing instability from very large cost differences.\n    margin = gamma * torch.tanh(normalized_cost_signal)\n\n    # 5. Inherited from Parent 1: Core log-sigmoid loss, now with a margin\n    # We want to encourage delta_logp > margin.\n    base_loss = -F.logsigmoid(delta_logp - margin)\n\n    # 6. Create an adaptive weight for each loss term\n    # softplus ensures the weight is non-negative and grows with the normalized cost difference.\n    adaptive_weight = F.softplus(normalized_cost_signal)\n\n    # 7. Combine the weight and the core loss\n    final_loss = adaptive_weight * base_loss\n\n    # 8. Apply optional weights and compute the mean\n    weights = batch.get('weight', None)\n    if weights is not None:\n        final_loss = final_loss * weights\n        # Use clamp to prevent division by zero if all weights are zero\n        return final_loss.sum() / weights.sum().clamp(min=1e-8)\n    else:\n        return final_loss.mean()"}, "static_ok": true, "static_reason": "", "dynamic_ok": true, "dynamic_reason": "ok", "loss_value": 0.5118374824523926, "grad_norm": 0.0}
{"generation": 8, "index": 7, "ir": {"name": "Rank-Normalized Margin Loss with Tanh Gate", "intuition": "This loss function creates a stable and adaptive preference signal by combining a classic log-sigmoid structure with a dynamically scaled margin. It focuses learning on difficult pairs by modulating the loss based on the model's current confidence.\n\nInherited Ideas:\n- From 'Rank-Aware Log-Sigmoid Preference Loss' (Parent 1), it inherits the core loss structure of `-logsigmoid(scaled_term)`. This provides a probabilistic interpretation and a strong gradient signal, especially when the model is wrong.\n- From 'Sigmoid-Gated Adaptive Margin Loss' (Parent 0), it inherits the idea of creating an adaptive margin that is normalized by the rank gap. This ensures the margin's scale is directly coupled to the pair's relative importance within the batch, preventing oversized margins for pairs that are already easy to distinguish based on their large cost differences.\n\nNew Coupling Ideas:\n1. **Rank-Normalized Margin within Log-Sigmoid**: Instead of using the rank-normalized margin in a `softplus` structure as in Parent 0, it is coupled directly with the log-probability difference inside the `logsigmoid` function. The core term becomes `-logsigmoid(delta_logp - margin)`. This frames the problem as ensuring the log-probability difference `delta_logp` exceeds the `margin`, which itself is dynamically scaled by the rank gap. This provides a clear, margin-based objective within a probabilistic loss framework.\n2. **Tanh Confidence Gating**: The final loss is multiplied by a gate: `1.0 - tanh(clamp(delta_logp, min=0))`. This gate has two effects: for pairs the model already correctly prefers (positive `delta_logp`), the `tanh` approaches 1, and the gate `(1 - tanh)` approaches 0, reducing the loss contribution and preventing the model from wasting capacity on already-learned examples. For pairs the model incorrectly prefers (negative `delta_logp`), the `clamp` makes the term 0, `tanh(0)` is 0, and the gate is 1, applying the full loss. This creates a one-sided focus mechanism on misclassified or uncertain pairs, improving training efficiency and stability.", "pseudocode": "1. For each pair (w, l) where cost(w) < cost(l), calculate the cost difference `delta_cost = cost(l) - cost(w)` and the log probability difference `delta_logp = logp(w) - logp(l)`.\n2. Inherit from Parent 0: Determine the cost ranks for `cost_w` and `cost_l` in the batch. Calculate the rank difference `rank_diff = rank(cost_l) - rank(cost_w)`.\n3. Inherit from Parent 0: Create a base margin using `softplus(delta_cost)`. Normalize this margin using the rank difference to create the `adaptive_margin = (alpha * softplus(delta_cost)) / (1.0 + beta * rank_diff)`.\n4. New Coupling (Margin in Log-Sigmoid): Combine the model's log-probability difference with the adaptive margin to form the core loss argument: `argument = delta_logp - adaptive_margin`.\n5. Inherit from Parent 1: Calculate the base preference loss using the `logsigmoid` function: `base_loss = -logsigmoid(argument)`.\n6. New Coupling (Tanh Gating): Create a confidence gate that down-weights already correct predictions. Calculate `gate = 1.0 - tanh(relu(delta_logp))`. This gate is close to 1 for incorrect/uncertain pairs and close to 0 for confident, correct pairs.\n7. Compute the final per-sample loss by applying the gate: `final_loss = gate * base_loss`.\n8. Return the weighted mean of the per-sample losses over the batch.", "hyperparams": {"alpha": 1.0, "beta": 0.1}, "operators_used": ["logsigmoid", "softplus", "tanh", "relu", "rank_gap"], "implementation_hint": {"expects": ["cost_a", "cost_b", "logp_a", "logp_b"], "returns": "scalar"}, "code": "import torch\nimport torch.nn.functional as F\n\ndef generated_loss(batch, model_output, extra):\n    \"\"\"\n    Computes the Rank-Normalized Margin Loss with Tanh Gate.\n\n    Inherits:\n    - The core `-logsigmoid()` structure for a probabilistic loss (from Parent 1).\n    - The use of a rank-gap-normalized margin to make the target separation batch-aware (from Parent 0).\n\n    Introduces:\n    - A new coupling where the rank-normalized margin is used *inside* the logsigmoid, as `-logsigmoid(delta_logp - margin)`.\n    - A Tanh-based confidence gate, `1 - tanh(relu(delta_logp))`, to focus loss on misclassified or uncertain pairs.\n    \"\"\"\n    # For preference learning, we assume cost_w < cost_l.\n    cost_w = torch.min(batch['cost_a'], batch['cost_b'])\n    cost_l = torch.max(batch['cost_a'], batch['cost_b'])\n    log_prob_w = batch['log_prob_w']\n    log_prob_l = batch['log_prob_l']\n\n    # Retrieve hyperparameters\n    alpha = extra.get('alpha', 1.0)\n    beta = extra.get('beta', 0.1)\n\n    # 1. Calculate cost and log probability differences\n    delta_cost = cost_l - cost_w\n    delta_logp = log_prob_w - log_prob_l\n    batch_size = delta_cost.shape[0]\n\n    # 2. Inherited from Parent 0: Calculate rank differences.\n    all_costs = torch.cat([cost_w, cost_l], dim=0)\n    sorted_costs, _ = torch.sort(all_costs)\n    rank_w = torch.searchsorted(sorted_costs, cost_w)\n    rank_l = torch.searchsorted(sorted_costs, cost_l)\n    rank_diff = (rank_l - rank_w).float()\n\n    # 3. Inherited from Parent 0: Create a rank-normalized adaptive margin.\n    # The margin is larger for bigger cost differences but is tempered by the rank gap.\n    margin_base = alpha * F.softplus(delta_cost)\n    # Add 1.0 to the denominator to prevent division by zero and ensure it's always >= 1.\n    margin_normalizer = 1.0 + beta * rank_diff\n    adaptive_margin = margin_base / margin_normalizer.detach()\n\n    # 4. New Coupling: Place the margin inside the logsigmoid term.\n    argument = delta_logp - adaptive_margin\n\n    # 5. Inherited from Parent 1: Use the core logsigmoid loss structure.\n    base_loss = -F.logsigmoid(argument)\n\n    # 6. New Coupling: Tanh Confidence Gating.\n    # For delta_logp > 0 (correct preference), relu(delta_logp) > 0, tanh(...) approaches 1, gate approaches 0.\n    # For delta_logp <= 0 (incorrect preference), relu(delta_logp) = 0, tanh(0) = 0, gate is 1.\n    # This focuses the loss on samples the model gets wrong.\n    confidence_gate = 1.0 - torch.tanh(F.relu(delta_logp.detach()))\n\n    # 7. Compute the final loss.\n    final_loss = confidence_gate * base_loss\n\n    # 8. Apply optional weights and compute the mean.\n    weights = batch.get('weight', None)\n    if weights is not None:\n        final_loss = final_loss * weights\n        # Use clamp to prevent division by zero for an all-zero weight tensor.\n        return final_loss.sum() / weights.sum().clamp(min=1e-8)\n    else:\n        return final_loss.mean()"}, "static_ok": true, "static_reason": "", "dynamic_ok": true, "dynamic_reason": "ok", "loss_value": 0.9772549271583557, "grad_norm": 0.0}
{"generation": 9, "index": 0, "ir": {"name": "Tanh-Gated Log-Sigmoid Loss with Rank-Aware Margin", "intuition": "This loss function combines the classic log-sigmoid structure for preference learning with a dynamic, rank-aware margin, and introduces a novel gating mechanism to stabilize training and focus learning on difficult examples.\n\nInherited Ideas:\n- From 'Rank-Aware Log-Sigmoid Preference Loss' (Parent 1), it inherits the core loss structure of `-logsigmoid(X)`, where `X` is a function of the log-probability difference. This provides a probabilistic interpretation rooted in the Bradley-Terry model.\n- From 'Sigmoid-Gated Adaptive Margin Loss with Rank-Gap Normalization' (Parent 0), it inherits the idea of using `softplus(delta_cost)` to create a smooth, non-saturating, and non-negative adaptive margin that scales with the difference in cost between the preferred and dispreferred candidates.\n\nNew Coupling Ideas:\n1.  **Rank-Aware Margin in Log-Sigmoid**: Instead of using cost information to weight the final loss (as in Parent 1), the `softplus(delta_cost)` term is integrated *inside* the `logsigmoid` as a margin. The core term becomes `-logsigmoid(delta_logp - margin)`. This directly enforces a separation in log-probabilities that is proportional to the cost difference, making the optimization target more explicit.\n2.  **Tanh Gating on Log-Probability Difference**: The model's log-probability difference `delta_logp` is passed through a `tanh` gate: `delta_logp * tanh(gamma * delta_logp)`. This serves two key purposes for stability and focus: (a) For pairs where the model is already very confident and correct (large positive `delta_logp`), the `tanh` term saturates to 1, preventing the loss from being driven by excessively large log-probability differences and avoiding potential gradient explosion. (b) For pairs where the model is confidently wrong (large negative `delta_logp`), the gate also saturates, ensuring a strong but bounded penalty, unlike an unbounded linear term.", "pseudocode": "1. For each pair (w, l) where cost(w) < cost(l), calculate the cost difference `delta_cost = cost(l) - cost(w)` and the log probability difference `delta_logp = logp(w) - logp(l)`.\n2. Inherit from Parent 0: Calculate a smooth, non-saturating adaptive margin using `softplus`: `margin = alpha * softplus(delta_cost / temp_cost)`. Detach this margin to treat it as a fixed target for each pair.\n3. New Coupling (Tanh Gating): Apply a scaled `tanh` gate to the model's log-probability difference to create a bounded, stabilized signal: `gated_delta_logp = delta_logp * tanh(gamma * delta_logp)`.\n4. New Coupling (Rank-Aware Margin in Log-Sigmoid): Combine the gated log-probability difference and the adaptive margin inside a `logsigmoid` function. This is inherited from Parent 1's use of a `logsigmoid` core.\n5. Compute the final per-sample loss: `loss = -logsigmoid(gated_delta_logp - margin)`.\n6. Return the weighted mean of the per-sample losses over the batch.", "hyperparams": {"alpha": 1.0, "temp_cost": 1.0, "gamma": 1.0}, "operators_used": ["logsigmoid", "softplus", "tanh", "log"], "implementation_hint": {"expects": ["cost_a", "cost_b", "logp_a", "logp_b"], "returns": "scalar"}, "code": "import torch\nimport torch.nn.functional as F\n\ndef generated_loss(batch, model_output, extra):\n    \"\"\"\n    Computes the Tanh-Gated Log-Sigmoid Loss with Rank-Aware Margin.\n\n    Inherits:\n    - The core `-logsigmoid(X)` structure from Parent 1.\n    - The use of `softplus(delta_cost)` for an adaptive margin from Parent 0.\n\n    Introduces:\n    - A new coupling where the adaptive margin is placed *inside* the logsigmoid, creating a direct separation target.\n    - A `tanh` gate on the log-probability difference to stabilize gradients and prevent blowup from overly confident predictions.\n    \"\"\"\n    # For preference learning, we assume cost_w < cost_l.\n    cost_w = torch.min(batch['cost_a'], batch['cost_b'])\n    cost_l = torch.max(batch['cost_a'], batch['cost_b'])\n    log_prob_w = batch['log_prob_w']\n    log_prob_l = batch['log_prob_l']\n\n    # Retrieve hyperparameters\n    alpha = extra.get('alpha', 1.0)\n    temp_cost = extra.get('temp_cost', 1.0)\n    gamma = extra.get('gamma', 1.0)\n\n    # 1. Calculate cost and log probability differences\n    delta_cost = cost_l - cost_w\n    delta_logp = log_prob_w - log_prob_l\n\n    # 2. Inherited from Parent 0: Calculate a smooth, non-saturating adaptive margin.\n    # We detach the margin as it serves as a target, not something to be optimized through.\n    margin = alpha * F.softplus(delta_cost / temp_cost).detach()\n\n    # 3. New Coupling: Tanh Gating on delta_logp.\n    # This bounds the influence of delta_logp, preventing gradient explosion from pairs where the model is already very confident.\n    # The gate is `delta_logp * tanh(gamma * delta_logp)`.\n    gated_delta_logp = delta_logp * torch.tanh(gamma * delta_logp)\n\n    # 4. New Coupling & Inheritance from Parent 1: Combine into a logsigmoid loss.\n    # The margin is now inside the logsigmoid, creating a direct separation target.\n    # The loss is minimized when `gated_delta_logp > margin`.\n    loss = -F.logsigmoid(gated_delta_logp - margin)\n\n    # 5. Apply optional weights and compute the mean.\n    weights = batch.get('weight', None)\n    if weights is not None:\n        loss = loss * weights\n        # Use clamp to avoid division by zero for an all-zero weight tensor\n        return loss.sum() / weights.sum().clamp(min=1e-8)\n    else:\n        return loss.mean()"}, "static_ok": true, "static_reason": "", "dynamic_ok": true, "dynamic_reason": "ok", "loss_value": 1.5514447689056396, "grad_norm": 0.0}
{"generation": 9, "index": 1, "ir": {"name": "Rank-Gated Log-Sigmoid Loss with Tanh-Normalized Margin", "intuition": "This loss function blends the stability of the classic log-sigmoid framework with a dynamic, bounded margin, creating a robust preference signal that adapts to both the cost and rank distribution within a batch.\n\nInherited Ideas:\n- From 'Rank-Aware Log-Sigmoid Preference Loss' (Parent 1), it inherits the core structure of a weighted log-sigmoid loss: `-weight * logsigmoid(delta_logp)`. This provides a strong probabilistic foundation and avoids the potential for vanishing gradients when the model is already correct, unlike margin-based losses.\n- From 'Sigmoid-Gated Adaptive Margin Loss with Rank-Gap Normalization' (Parent 0), it inherits the use of a rank-gap signal to modulate the loss. This focuses the model's attention on pairs that are far apart in the batch's cost distribution, treating them as more significant learning signals.\n\nNew Coupling Ideas:\n1.  **Tanh-Normalized Adaptive Margin**: Instead of directly weighting the loss with cost differences, this child loss introduces a dynamic margin inside the `logsigmoid` term: `logsigmoid(delta_logp - margin)`. The margin itself is a novel construction: `margin = alpha * tanh(beta * softplus(zscore(delta_cost)))`. This creates a margin that is adaptive to the batch's cost differences (via `zscore` and `softplus`) but is smoothly bounded between 0 and `alpha` by the `tanh` function. This prevents excessively large margins from causing numerical instability or overly aggressive updates for pairs with huge cost differences.\n2.  **Softplus Rank Gating**: The final loss is gated by a rank-based weight, `rank_gate * -logsigmoid(...)`. The gate is calculated as `softplus(gamma * rank_diff)`. This smoothly and non-saturatingly increases the weight for pairs with larger rank separation, providing a more robust and continuous emphasis compared to linear or exponential weighting schemes.", "pseudocode": "1. For each pair (w, l) where cost(w) < cost(l), calculate the cost difference `delta_cost = cost(l) - cost(w)` and the log probability difference `delta_logp = logp(w) - logp(l)`.\n2. Inherit from Parent 1 and Parent 0: Standardize the `delta_cost` vector for the entire batch using Z-score normalization to get `delta_cost_norm` for scale invariance.\n3. New Coupling (Tanh-Normalized Margin): Create an adaptive, bounded margin. First, apply `softplus` to the normalized cost difference. Then, scale this by `beta` and pass it through a `tanh` function. Finally, scale by `alpha` to control the maximum margin: `margin = alpha * tanh(beta * softplus(delta_cost_norm))`.\n4. Inherit from Parent 0: Calculate the rank difference `rank_diff = rank(cost_l) - rank(cost_w)` based on the positions of `cost_w` and `cost_l` within all sorted costs in the batch.\n5. New Coupling (Softplus Rank Gating): Create a smooth, non-negative, and non-saturating weight from the rank difference: `rank_gate = softplus(gamma * rank_diff)`.\n6. Inherit from Parent 1: Compute the core preference loss using the log-sigmoid function, but now incorporating the adaptive margin: `base_loss = -logsigmoid(delta_logp - margin)`.\n7. Modulate the base loss with the rank gate: `final_loss = rank_gate * base_loss`.\n8. Return the weighted mean of `final_loss` over the batch.", "hyperparams": {"alpha": 2.0, "beta": 0.5, "gamma": 0.1}, "operators_used": ["logsigmoid", "softplus", "tanh", "zscore", "rank_gap"], "implementation_hint": {"expects": ["cost_a", "cost_b", "log_prob_w", "log_prob_l", "weight"], "returns": "scalar"}, "code": "import torch\nimport torch.nn.functional as F\n\ndef generated_loss(batch, model_output, extra):\n    \"\"\"\n    Computes the Rank-Gated Log-Sigmoid Loss with Tanh-Normalized Margin.\n\n    Inherits:\n    - The core weighted log-sigmoid structure from Parent 1.\n    - The use of rank differences for batch-aware modulation from Parent 0.\n\n    Introduces:\n    - A tanh-normalized adaptive margin inside the logsigmoid, providing a bounded target separation.\n    - A softplus-gated rank difference as a final loss weight for smooth, non-saturating emphasis.\n    \"\"\"\n    # For preference learning, we assume cost_w < cost_l.\n    cost_w = torch.min(batch['cost_a'], batch['cost_b'])\n    cost_l = torch.max(batch['cost_a'], batch['cost_b'])\n    log_prob_w = batch['log_prob_w']\n    log_prob_l = batch['log_prob_l']\n\n    # Retrieve hyperparameters\n    alpha = extra.get('alpha', 2.0)\n    beta = extra.get('beta', 0.5)\n    gamma = extra.get('gamma', 0.1)\n\n    # 1. Calculate cost and log probability differences\n    delta_cost = cost_l - cost_w\n    delta_logp = log_prob_w - log_prob_l\n\n    # Handle batches with no cost variance to prevent division by zero in z-score\n    if delta_cost.std() < 1e-8:\n        delta_cost_norm = torch.zeros_like(delta_cost)\n    else:\n        # 2. Inherited: Z-score normalization of cost differences for batch-wise scale invariance\n        delta_cost_mean = delta_cost.mean()\n        delta_cost_std = delta_cost.std().clamp(min=1e-8)\n        delta_cost_norm = (delta_cost - delta_cost_mean) / delta_cost_std\n\n    # 3. New Coupling: Tanh-Normalized Adaptive Margin\n    # softplus ensures the input to tanh is non-negative, so the margin is always positive.\n    # tanh ensures the margin is smoothly bounded between 0 and alpha.\n    margin = alpha * torch.tanh(beta * F.softplus(delta_cost_norm))\n\n    # 4. Inherited: Calculate rank difference\n    all_costs = torch.cat([cost_w, cost_l], dim=0)\n    sorted_costs, _ = torch.sort(all_costs)\n    rank_w = torch.searchsorted(sorted_costs, cost_w)\n    rank_l = torch.searchsorted(sorted_costs, cost_l)\n    rank_diff = (rank_l - rank_w).float()\n\n    # 5. New Coupling: Softplus Rank Gating\n    # Creates a smooth, non-saturating weight based on rank separation.\n    rank_gate = F.softplus(gamma * rank_diff)\n\n    # 6. Inherited: Compute core log-sigmoid loss with the new margin\n    base_loss = -F.logsigmoid(delta_logp - margin.detach())\n\n    # 7. Modulate the base loss with the rank gate\n    final_loss = rank_gate.detach() * base_loss\n\n    # 8. Apply optional weights and compute the mean.\n    weights = batch.get('weight', None)\n    if weights is not None:\n        final_loss = final_loss * weights\n        return final_loss.sum() / weights.sum().clamp(min=1e-8)\n    else:\n        return final_loss.mean()"}, "static_ok": true, "static_reason": "", "dynamic_ok": true, "dynamic_reason": "ok", "loss_value": 1.928462266921997, "grad_norm": 0.0}
{"generation": 9, "index": 2, "ir": {"name": "Rank-Gated Log-Sigmoid Loss with Z-Score Margin", "intuition": "This loss function creates a stable, rank-aware preference signal by combining a classic log-sigmoid structure with a dynamically scaled margin. It focuses learning on pairs that are either closely ranked or where the model is confidently wrong.\n\nInherited Ideas:\n- From 'Rank-Aware Log-Sigmoid Preference Loss' (Parent 1), it inherits the core loss structure of `-logsigmoid(delta_logp - margin)`. This provides a probabilistic interpretation and a strong gradient signal, especially when the model's preference is incorrect.\n- From 'Sigmoid-Gated Adaptive Margin Loss with Rank-Gap Normalization' (Parent 0), it inherits the idea of using the rank difference (`rank_gap`) between pairs to modulate the loss, focusing attention on pairs with specific relationships within the batch's cost distribution.\n\nNew Coupling Ideas:\n1.  **Z-Scored Cost Difference as Margin**: Instead of creating a complex margin from softplus-transformed costs, this loss uses the batch-normalized (Z-scored) cost difference directly as the margin: `margin = alpha * zscore(cost_l - cost_w)`. This couples the margin directly to the relative cost difference within the batch, making it robust to the absolute scale of costs and adaptive to the current batch's statistics. `alpha` controls the strength of this margin.\n2.  **Inverse Rank Gating**: The loss is weighted by an inverse rank gate: `weight = 1.0 / (1.0 + beta * rank_gap)`. This is a novel coupling where pairs with a *small* rank difference (i.e., similar costs) are given higher weight. This forces the model to learn fine-grained distinctions between closely competing candidates, which are often the hardest and most important examples. The denominator prevents division by zero and smooths the weighting.", "pseudocode": "1. For each pair (w, l) where cost(w) < cost(l), calculate the cost difference `delta_cost = cost(l) - cost(w)` and the log probability difference `delta_logp = logp(w) - logp(l)`.\n2. New Coupling (Z-Score Margin): Standardize the `delta_cost` vector for the batch using Z-score normalization. This normalized value, scaled by `alpha`, becomes the adaptive margin: `margin = alpha * zscore(delta_cost)`.\n3. Inherit from Parent 0 (Rank Modulation): Determine the cost ranks for all `cost_w` and `cost_l` in the batch. Calculate the rank difference `rank_gap = rank(cost_l) - rank(cost_w)`.\n4. New Coupling (Inverse Rank Gating): Create a weight that emphasizes pairs with small rank gaps. Calculate `weight = 1.0 / (1.0 + beta * rank_gap)`. This focuses the loss on fine-grained distinctions.\n5. Inherit from Parent 1 (Log-Sigmoid Core): Compute the core per-sample loss using the log-sigmoid function with the adaptive margin: `loss = -logsigmoid(delta_logp - margin)`.\n6. Combine the loss with the rank-based weight: `final_loss = weight * loss`.\n7. Return the mean of the final losses over the batch, applying optional external weights if provided.", "hyperparams": {"alpha": 0.5, "beta": 0.1}, "operators_used": ["logsigmoid", "zscore", "rank_gap"], "implementation_hint": {"expects": ["cost_a", "cost_b", "logp_a", "logp_b"], "returns": "scalar"}, "code": "import torch\nimport torch.nn.functional as F\n\ndef generated_loss(batch, model_output, extra):\n    \"\"\"\n    Computes the Rank-Gated Log-Sigmoid Loss with Z-Score Margin.\n\n    Inherits:\n    - The core `-logsigmoid(delta_logp - margin)` structure from Parent 1.\n    - The use of rank differences for batch-aware modulation from Parent 0.\n\n    Introduces:\n    - A new margin coupling where the margin is the Z-scored cost difference.\n    - An inverse rank gate `1 / (1 + beta * rank_gap)` to up-weight pairs with similar costs.\n    \"\"\"\n    # For preference learning, we assume cost_w < cost_l.\n    cost_w = torch.min(batch['cost_a'], batch['cost_b'])\n    cost_l = torch.max(batch['cost_a'], batch['cost_b'])\n    log_prob_w = batch['log_prob_w']\n    log_prob_l = batch['log_prob_l']\n\n    # Retrieve hyperparameters\n    alpha = extra.get('alpha', 0.5)\n    beta = extra.get('beta', 0.1)\n\n    # 1. Calculate cost and log probability differences\n    delta_cost = cost_l - cost_w\n    delta_logp = log_prob_w - log_prob_l\n    batch_size = delta_cost.shape[0]\n\n    # Handle batches with no cost variance to prevent NaN from z-scoring\n    if delta_cost.std() < 1e-8:\n        margin = torch.zeros_like(delta_cost)\n    else:\n        # 2. New Coupling: Z-Scored Cost Difference as Margin\n        delta_cost_mean = delta_cost.mean()\n        delta_cost_std = delta_cost.std()\n        delta_cost_norm = (delta_cost - delta_cost_mean) / delta_cost_std.clamp(min=1e-8)\n        margin = alpha * delta_cost_norm\n\n    # 3. Inherited: Calculate rank gap\n    all_costs = torch.cat([cost_w, cost_l], dim=0)\n    # Use torch.sort for compatibility and clarity\n    sorted_costs_unique = torch.unique_consecutive(torch.sort(all_costs).values)\n    rank_w = torch.searchsorted(sorted_costs_unique, cost_w, right=True) - 1\n    rank_l = torch.searchsorted(sorted_costs_unique, cost_l, right=True) - 1\n    rank_gap = (rank_l - rank_w).float().clamp(min=0)\n\n    # 4. New Coupling: Inverse Rank Gating\n    # This weight is higher for pairs with a small rank gap, focusing on fine-grained examples.\n    rank_weight = 1.0 / (1.0 + beta * rank_gap)\n\n    # 5. Inherited: Compute the core log-sigmoid loss\n    # Detach margin and weight so they act as targets/weights, not variables to be optimized through\n    # in the context of a model that might predict costs.\n    loss = -F.logsigmoid(delta_logp - margin.detach())\n\n    # 6. Apply the rank-based weight\n    final_loss = rank_weight.detach() * loss\n\n    # 7. Apply optional batch weights and compute the mean.\n    weights = batch.get('weight', None)\n    if weights is not None:\n        final_loss = final_loss * weights\n        return final_loss.sum() / weights.sum().clamp(min=1e-8)\n    else:\n        return final_loss.mean()"}, "static_ok": true, "static_reason": "", "dynamic_ok": true, "dynamic_reason": "ok", "loss_value": 0.6301338076591492, "grad_norm": 0.0}
{"generation": 9, "index": 3, "ir": {"name": "Rank-Gated Log-Sigmoid Loss with Tanh-Scaled Margin", "intuition": "This loss function creates a stable and adaptive preference signal by combining a classic log-sigmoid structure with a dynamically scaled margin. It focuses on difficult examples by using rank information to gate the loss.\n\nInherited Ideas:\n- From 'Rank-Aware Log-Sigmoid Preference Loss' (Parent 1), it inherits the core loss structure of `-logsigmoid(delta_logp - margin)`. This is a classic preference loss formulation that encourages the log-probability difference to be greater than a margin.\n- From 'Sigmoid-Gated Adaptive Margin Loss with Rank-Gap Normalization' (Parent 0), it inherits the use of `rank_gap` to modulate the loss. Instead of normalizing a margin or being part of a weight, here the rank gap is used as a direct gate on the final loss value, focusing the model on pairs with a small rank difference (i.e., those that are harder to distinguish).\n\nNew Coupling Ideas:\n1.  **Tanh-Scaled Adaptive Margin**: The margin is not static or simply proportional to the cost difference. It's calculated as `alpha * tanh(beta * delta_cost)`. This couples the margin to the cost difference in a bounded way. For small `delta_cost`, the margin is approximately linear (`alpha * beta * delta_cost`), but for very large `delta_cost`, the margin saturates at `alpha`. This prevents extremely large cost differences from creating excessively large margins, which could lead to gradient explosion and numerical instability, while still providing an adaptive signal.\n2.  **Inverse Rank-Gap Gating**: The final loss is multiplied by `1 / (1 + gamma * rank_gap)`. This is a new form of gating. It heavily down-weights the loss for pairs that are far apart in the batch's cost ranking (large `rank_gap`), effectively telling the model to ignore pairs that are already easy to distinguish. Conversely, it preserves the full loss signal for pairs with a small rank difference, focusing training on fine-grained distinctions and difficult cases. The `+1` in the denominator ensures stability and that the gate is always between 0 and 1.", "pseudocode": "1. For each pair (w, l) where cost(w) < cost(l), calculate the cost difference `delta_cost = cost(l) - cost(w)` and the log probability difference `delta_logp = logp(w) - logp(l)`.\n2. New Coupling (Tanh-Scaled Margin): Calculate a bounded, adaptive margin using `margin = alpha * tanh(beta * delta_cost)`. This prevents the margin from growing uncontrollably with large cost differences.\n3. Inherit from Parent 1: Compute the base preference loss using a log-sigmoid structure with the adaptive margin: `base_loss = -logsigmoid(delta_logp - margin)`.\n4. Inherit from Parent 0: Determine the cost ranks for all `cost_w` and `cost_l` in the batch. Calculate the normalized rank difference `rank_gap = abs(rank(cost_l) - rank(cost_w)) / (2 * batch_size)`.\n5. New Coupling (Inverse Rank-Gap Gating): Create a gating factor that down-weights easy-to-distinguish pairs: `rank_gate = 1.0 / (1.0 + gamma * rank_gap)`.\n6. Apply the gate to the base loss: `final_loss = rank_gate * base_loss`.\n7. Return the weighted mean of the final loss over the batch.", "hyperparams": {"alpha": 1.0, "beta": 0.1, "gamma": 5.0}, "operators_used": ["logsigmoid", "tanh", "rank_gap"], "implementation_hint": {"expects": ["cost_w", "cost_l", "logp_w", "logp_l"], "returns": "scalar"}, "code": "import torch\nimport torch.nn.functional as F\n\ndef generated_loss(batch, model_output, extra):\n    \"\"\"\n    Computes the Rank-Gated Log-Sigmoid Loss with Tanh-Scaled Margin.\n\n    Inherits:\n    - The core `-logsigmoid(delta_logp - margin)` structure from Parent 1.\n    - The use of rank_gap for modulation, inspired by Parent 0.\n\n    Introduces:\n    - A tanh-scaled adaptive margin `alpha * tanh(beta * delta_cost)` for stability.\n    - An inverse rank-gap gate `1 / (1 + gamma * rank_gap)` to focus on hard-to-distinguish pairs.\n    \"\"\"\n    # For preference learning, we assume cost_w < cost_l.\n    cost_w = torch.min(batch['cost_a'], batch['cost_b'])\n    cost_l = torch.max(batch['cost_a'], batch['cost_b'])\n    log_prob_w = batch['log_prob_w']\n    log_prob_l = batch['log_prob_l']\n\n    # Retrieve hyperparameters\n    alpha = extra.get('alpha', 1.0)\n    beta = extra.get('beta', 0.1)\n    gamma = extra.get('gamma', 5.0)\n\n    # 1. Calculate cost and log probability differences\n    delta_cost = cost_l - cost_w\n    delta_logp = log_prob_w - log_prob_l\n    batch_size = delta_cost.shape[0]\n\n    # 2. New Coupling: Tanh-Scaled Adaptive Margin\n    # This creates a margin that grows with delta_cost but is bounded by alpha,\n    # preventing numerical instability from very large cost differences.\n    adaptive_margin = alpha * torch.tanh(beta * delta_cost)\n\n    # 3. Inherited from Parent 1: Core log-sigmoid loss structure\n    # The model is encouraged to make delta_logp > adaptive_margin.\n    base_loss = -F.logsigmoid(delta_logp - adaptive_margin.detach())\n\n    # 4. Inherited from Parent 0: Calculate rank gap\n    # Combine all costs to establish a batch-wide ranking.\n    all_costs = torch.cat([cost_w, cost_l], dim=0)\n    # Sorting is needed to establish ranks\n    sorted_indices = torch.argsort(all_costs)\n    ranks = torch.empty_like(sorted_indices)\n    ranks[sorted_indices] = torch.arange(len(all_costs), device=all_costs.device)\n    rank_w = ranks[:batch_size]\n    rank_l = ranks[batch_size:]\n    \n    # Calculate normalized rank difference\n    rank_diff = (rank_l - rank_w).float()\n    # Normalize by the max possible rank difference (2*N-1) for scale invariance\n    # We use 2*N as a simple, stable approximation.\n    normalized_rank_gap = rank_diff / (2 * batch_size)\n    \n    # 5. New Coupling: Inverse Rank-Gap Gating\n    # This gate is close to 1 for small rank gaps (hard pairs) and approaches 0 for large gaps (easy pairs).\n    # This focuses training on fine-grained distinctions.\n    rank_gate = 1.0 / (1.0 + gamma * normalized_rank_gap.detach())\n\n    # 6. Apply the gate to the loss\n    final_loss = rank_gate * base_loss\n\n    # 7. Apply optional weights and compute the mean\n    weights = batch.get('weight', None)\n    if weights is not None:\n        final_loss = final_loss * weights\n        # Use clamp to avoid division by zero if all weights are zero\n        return final_loss.sum() / weights.sum().clamp(min=1e-8)\n    else:\n        return final_loss.mean()"}, "static_ok": true, "static_reason": "", "dynamic_ok": true, "dynamic_reason": "ok", "loss_value": 0.22065287828445435, "grad_norm": 0.0}
{"generation": 9, "index": 4, "ir": {"name": "Adaptive Log-Sigmoid Loss with Tanh-Gated Rank Margin", "intuition": "This loss function synergizes the classic log-sigmoid preference structure with a dynamic, rank-aware margin, stabilized by batch normalization and a smooth gating mechanism.\n\nInherited Ideas:\n- From 'Rank-Aware Log-Sigmoid Preference Loss' (Parent 1), it inherits the core loss structure of `-logsigmoid(delta_logp - margin)`. This provides a probabilistic interpretation (Bradley-Terry) while allowing for a margin, which is a common and effective technique for robust preference learning.\n- From 'Sigmoid-Gated Adaptive Margin Loss with Rank-Gap Normalization' (Parent 0), it inherits the idea of using the rank difference between costs within a batch (`rank_gap`) to create a dynamic, batch-aware signal. This focuses the model on distinguishing between items that are far apart in the batch's cost distribution.\n\nNew Coupling Ideas:\n1.  **Rank-Gap Margin with Tanh Gating**: Instead of using the cost difference (`delta_cost`) to define the margin, this loss uses the normalized rank difference. The margin is `alpha * tanh(beta * rank_gap_norm)`. The `rank_gap` is normalized by the batch size to make it scale-invariant. The `tanh` function is then applied to create a smooth, bounded margin between `[-alpha, alpha]`. This couples the margin directly to the relative importance of a pair within the batch, preventing extreme margin values that could arise from outlier cost differences, thus improving stability.\n2.  **Z-Score Normalization of Log-Probability Difference**: The log-probability difference (`delta_logp`) is standardized using Z-score normalization across the batch before being used in the loss. This stabilizes training by preventing extreme `delta_logp` values (from either a very confident or very uncertain model) from producing exploding gradients. It recenters the model's output distribution for the batch, making the fixed `tanh`-gated margin more effective and reducing sensitivity to the overall scale of the model's logits.", "pseudocode": "1. For each pair (w, l) where cost(w) < cost(l), calculate the log probability difference `delta_logp = logp(w) - logp(l)`.\n2. Inherit from Parent 0: Determine the cost ranks for all `cost_w` and `cost_l` in the batch. Calculate the rank difference `rank_diff = rank(cost_l) - rank(cost_w)`.\n3. New Coupling (Rank-Gap Margin with Tanh Gating): Normalize the rank difference by the batch size. Apply a scaled `tanh` function to this normalized rank difference to create a smooth, bounded, and adaptive margin: `margin = alpha * tanh(beta * rank_diff / batch_size)`.\n4. New Coupling (Z-Score Normalization): Standardize the `delta_logp` vector for the entire batch using Z-score normalization to get `delta_logp_norm`. This stabilizes the loss calculation against outliers in model predictions.\n5. Inherit from Parent 1: Compute the core loss using the log-sigmoid structure, incorporating the normalized log-probability difference and the new adaptive margin: `loss = -logsigmoid(delta_logp_norm - margin)`.\n6. Return the weighted mean of the per-sample losses over the batch.", "hyperparams": {"alpha": 1.0, "beta": 5.0}, "operators_used": ["logsigmoid", "tanh", "zscore", "rank_gap"], "implementation_hint": {"expects": ["cost_a", "cost_b", "log_prob_w", "log_prob_l", "weight"], "returns": "scalar"}, "code": "import torch\nimport torch.nn.functional as F\n\ndef generated_loss(batch, model_output, extra):\n    \"\"\"\n    Computes the Adaptive Log-Sigmoid Loss with Tanh-Gated Rank Margin.\n\n    Inherits:\n    - The core `-logsigmoid(delta_logp - margin)` structure (from Parent 1).\n    - The use of rank differences for a batch-aware signal (from Parent 0).\n\n    Introduces:\n    - A new margin coupling based on `tanh(rank_gap)`, making it smooth, bounded, and derived from relative batch importance rather than absolute cost.\n    - Z-score normalization of `delta_logp` to stabilize training against extreme model outputs.\n    \"\"\"\n    # For preference learning, cost_w < cost_l is assumed.\n    cost_w = torch.min(batch['cost_a'], batch['cost_b'])\n    cost_l = torch.max(batch['cost_a'], batch['cost_b'])\n    log_prob_w = batch['log_prob_w']\n    log_prob_l = batch['log_prob_l']\n\n    # Retrieve hyperparameters\n    alpha = extra.get('alpha', 1.0) # Controls the max margin size\n    beta = extra.get('beta', 5.0)   # Controls the steepness of the tanh curve\n\n    # 1. Calculate log probability difference\n    delta_logp = log_prob_w - log_prob_l\n    batch_size = delta_logp.shape[0]\n\n    # 2. Inherited from Parent 0: Calculate rank differences\n    all_costs = torch.cat([cost_w, cost_l], dim=0)\n    sorted_costs, _ = torch.sort(all_costs)\n    rank_w = torch.searchsorted(sorted_costs, cost_w)\n    rank_l = torch.searchsorted(sorted_costs, cost_l)\n    rank_diff = (rank_l - rank_w).float()\n\n    # 3. New Coupling: Tanh-gated rank-gap margin\n    # Normalizing by 2*batch_size (total elements) makes it scale-invariant.\n    # The tanh creates a smooth, bounded margin from [-alpha, alpha].\n    # The margin is detached as it's a target, not something to be optimized through.\n    normalized_rank_diff = rank_diff / (2 * batch_size)\n    margin = alpha * torch.tanh(beta * normalized_rank_diff)\n    margin = margin.detach()\n\n    # 4. New Coupling: Z-score normalization of delta_logp for stability\n    if batch_size > 1:\n        delta_logp_mean = delta_logp.mean()\n        delta_logp_std = delta_logp.std().clamp(min=1e-8)\n        delta_logp_norm = (delta_logp - delta_logp_mean) / delta_logp_std\n    else:\n        # Cannot compute z-score for a single element, so we just center it.\n        delta_logp_norm = delta_logp - delta_logp.mean()\n\n    # 5. Inherited from Parent 1: Compute the core log-sigmoid loss\n    # The goal is to make `delta_logp_norm` > `margin`.\n    loss = -F.logsigmoid(delta_logp_norm - margin)\n\n    # 6. Apply optional weights and compute the mean.\n    weights = batch.get('weight', None)\n    if weights is not None:\n        loss = loss * weights\n        # Use clamp to prevent division by zero if all weights are zero.\n        return loss.sum() / weights.sum().clamp(min=1e-8)\n    else:\n        return loss.mean()"}, "static_ok": true, "static_reason": "", "dynamic_ok": true, "dynamic_reason": "ok", "loss_value": 1.303493618965149, "grad_norm": 0.0}
{"generation": 9, "index": 5, "ir": {"name": "Rank-Gated Log-Sigmoid Loss with Tanh Margin", "intuition": "This loss function combines the classic log-sigmoid structure with a dynamic, rank-aware margin, creating a preference signal that is both stable and sensitive to the relative importance of pairs within a batch.\n\nInherited Ideas:\n- From 'Rank-Aware Log-Sigmoid Preference Loss' (Parent 1), it inherits the core loss structure of `-logsigmoid(delta_logp - margin)`. This provides a strong probabilistic foundation, encouraging the model to maximize the log-likelihood of the preferred choice.\n- From 'Sigmoid-Gated Adaptive Margin Loss with Rank-Gap Normalization' (Parent 0), it inherits the use of a sigmoid gate on the log-probability difference, `delta_logp * sigmoid(delta_logp)`. This serves as a stability mechanism, down-weighting the loss contribution from pairs where the model is already very confident and correct, allowing it to focus on more difficult or incorrectly classified examples.\n\nNew Coupling Ideas:\n1.  **Tanh-Scaled Rank Gap Margin**: A new adaptive margin is created by coupling the rank difference with a `tanh` function: `margin = alpha * tanh(beta * rank_gap_normalized)`. The rank gap is first normalized by the batch size to ensure scale invariance. The `tanh` function then smoothly maps this normalized rank gap to a bounded margin between 0 and `alpha`. This ensures that pairs with a larger rank difference (more important to get right) have a larger margin, but it prevents the margin from growing uncontrollably, which enhances numerical stability.\n2.  **Adaptive Loss Weighting with Z-Scored Cost**: The final per-sample loss is weighted by an adaptive factor derived from the cost difference. The `delta_cost` is first standardized using a Z-score across the batch. This normalized score is then passed through a `softplus` function, `softplus(delta_cost_zscore)`, creating a non-negative, unbounded weight. This couples the magnitude of the loss to the magnitude of the cost difference in a scale-invariant manner, ensuring that pairs with larger real-world cost differences contribute more significantly to the training signal.", "pseudocode": "1. For each pair (w, l) where cost(w) < cost(l), calculate the cost difference `delta_cost = cost(l) - cost(w)` and the log probability difference `delta_logp = logp(w) - logp(l)`.\n2. Inherit from Parent 0: Apply a sigmoid gate to the log-probability difference: `gated_delta_logp = delta_logp * sigmoid(delta_logp)`.\n3. New Coupling (Tanh Margin): Determine the cost ranks for all items in the batch. Calculate the normalized rank difference `rank_gap_norm = (rank(l) - rank(w)) / (2 * batch_size)`. Create a bounded, adaptive margin using `margin = alpha * tanh(beta * rank_gap_norm)`.\n4. New Coupling (Adaptive Weighting): Standardize the `delta_cost` vector for the batch using Z-score to get `delta_cost_zscore`. Create an adaptive weight using `weight_adaptive = softplus(delta_cost_zscore)`.\n5. Inherit from Parent 1: Compute the core loss using a log-sigmoid structure, incorporating the new margin and gated log-probabilities: `base_loss = -logsigmoid(gated_delta_logp - margin)`.\n6. Combine the base loss with the adaptive weight: `per_sample_loss = weight_adaptive * base_loss`.\n7. Return the mean of the per-sample losses over the batch, applying optional external weights if provided.", "hyperparams": {"alpha": 2.0, "beta": 5.0}, "operators_used": ["logsigmoid", "sigmoid", "tanh", "softplus", "zscore", "rank_gap"], "implementation_hint": {"expects": ["cost_w", "cost_l", "logp_w", "logp_l", "weight"], "returns": "scalar"}, "code": "import torch\nimport torch.nn.functional as F\n\ndef generated_loss(batch, model_output, extra):\n    \"\"\"\n    Computes the Rank-Gated Log-Sigmoid Loss with Tanh Margin.\n\n    Inherits:\n    - The core `-logsigmoid(delta_logp - margin)` structure (from Parent 1).\n    - A sigmoid gate on `delta_logp` for stability and focus (from Parent 0).\n\n    Introduces:\n    - A new tanh-based adaptive margin from the normalized rank gap.\n    - An adaptive loss weight based on the z-scored cost difference.\n    \"\"\"\n    # For preference learning, we assume cost_w < cost_l.\n    cost_w = torch.min(batch['cost_a'], batch['cost_b'])\n    cost_l = torch.max(batch['cost_a'], batch['cost_b'])\n    log_prob_w = batch['log_prob_w']\n    log_prob_l = batch['log_prob_l']\n\n    # Retrieve hyperparameters\n    alpha = extra.get('alpha', 2.0) # Controls the maximum margin\n    beta = extra.get('beta', 5.0)   # Controls the steepness of the tanh margin\n\n    # 1. Calculate base differences\n    delta_cost = cost_l - cost_w\n    delta_logp = log_prob_w - log_prob_l\n    batch_size = delta_cost.shape[0]\n\n    # 2. Inherited from Parent 0: Sigmoid gating on delta_logp\n    # This down-weights pairs where the model is already very confident and correct.\n    gated_delta_logp = delta_logp * torch.sigmoid(delta_logp)\n\n    # 3. New Coupling: Tanh-based adaptive margin from rank gap\n    # Combine all costs to establish a batch-wide ranking.\n    all_costs = torch.cat([cost_w, cost_l], dim=0)\n    sorted_costs, _ = torch.sort(all_costs)\n    \n    # Find ranks efficiently using searchsorted.\n    rank_w = torch.searchsorted(sorted_costs, cost_w)\n    rank_l = torch.searchsorted(sorted_costs, cost_l)\n    rank_diff = (rank_l - rank_w).float()\n    \n    # Normalize rank diff by total number of items to make it batch-size invariant.\n    rank_gap_norm = rank_diff / (2.0 * batch_size)\n    \n    # Tanh creates a smooth, bounded margin from the rank gap.\n    margin = alpha * torch.tanh(beta * rank_gap_norm)\n\n    # 4. New Coupling: Adaptive loss weighting from z-scored cost difference\n    if torch.all(delta_cost < 1e-8):\n        # If all costs are the same, z-score is undefined and weight is neutral.\n        delta_cost_zscore = torch.zeros_like(delta_cost)\n    else:\n        delta_cost_mean = delta_cost.mean()\n        delta_cost_std = delta_cost.std().clamp(min=1e-8)\n        delta_cost_zscore = (delta_cost - delta_cost_mean) / delta_cost_std\n    \n    # Softplus creates a non-negative, unbounded weight.\n    adaptive_weight = F.softplus(delta_cost_zscore)\n\n    # 5. Inherited from Parent 1: Core log-sigmoid loss structure\n    # We want to maximize `gated_delta_logp - margin`, so we minimize its negative log-sigmoid.\n    base_loss = -F.logsigmoid(gated_delta_logp - margin.detach())\n\n    # 6. Modulate the base loss with the adaptive weight\n    per_sample_loss = adaptive_weight.detach() * base_loss\n\n    # 7. Apply optional external weights and compute the mean.\n    weights = batch.get('weight', None)\n    if weights is not None:\n        per_sample_loss = per_sample_loss * weights\n        return per_sample_loss.sum() / weights.sum().clamp(min=1e-8)\n    else:\n        return per_sample_loss.mean()"}, "static_ok": true, "static_reason": "", "dynamic_ok": true, "dynamic_reason": "ok", "loss_value": 1.4579558372497559, "grad_norm": 0.0}
{"generation": 9, "index": 6, "ir": {"name": "Rank-Normalized Bradley-Terry Loss", "intuition": "This loss function reframes the classic Bradley-Terry model (log-sigmoid loss) by introducing an adaptive, rank-aware margin. The goal is to create a loss that is both statistically grounded and sensitive to the relative importance of pairs within a batch.\n\nInherited Ideas:\n- From 'Rank-Aware Log-Sigmoid Preference Loss' (Parent 1), it inherits the core `logsigmoid` structure, which is a standard and well-behaved loss for pairwise preferences. This provides a probabilistic interpretation of the model's output.\n- From 'Sigmoid-Gated Adaptive Margin Loss with Rank-Gap Normalization' (Parent 0), it inherits the idea of using `softplus(delta_cost)` to create a smooth, non-negative, and non-saturating adaptive margin. This ensures that pairs with larger cost differences are expected to have a larger log-probability difference.\n\nNew Coupling Ideas:\n1. **Rank-Normalized Margin**: Instead of just using the rank gap to weight the final loss, it is used to directly normalize the adaptive margin itself. The margin `softplus(delta_cost)` is divided by `log(1 + rank_gap)`. This couples the margin's scale directly to the pair's relative importance. For pairs with a small rank gap, the denominator is small, preserving a larger margin and pushing the model to learn fine-grained distinctions. For pairs with a large rank gap, the denominator grows, reducing the margin. This prevents the margin from becoming excessively large for pairs that are already easy to distinguish based on their large cost difference, which helps stabilize gradients.\n2. **Log-Sigmoid with Adaptive Margin**: The core loss structure is `logsigmoid(delta_logp - margin)`. This is a novel coupling that directly incorporates the rank-normalized adaptive margin into the log-sigmoid function. This is different from Parent 1, which used cost/rank as an external weight, and Parent 0, which used a hinge-like `softplus(margin - delta_logp)` structure. This formulation maintains the probabilistic interpretation of the log-sigmoid loss while making the target log-probability difference dependent on both the absolute cost difference and the relative rank difference.", "pseudocode": "1. For each pair (w, l) where cost(w) < cost(l), calculate the cost difference `delta_cost = cost(l) - cost(w)` and the log probability difference `delta_logp = logp(w) - logp(l)`.\n2. Inherit from Parent 0: Calculate a smooth, non-saturating base margin using `margin_base = softplus(delta_cost / temp_cost)`.\n3. Determine the cost ranks for all `cost_w` and `cost_l` in the batch. Calculate the rank difference `rank_gap = rank(cost_l) - rank(cost_w)`.\n4. New Coupling (Rank-Normalized Margin): Create a rank-based normalizer using the natural logarithm: `rank_normalizer = log(1.0 + beta * rank_gap)`. Add a small epsilon for stability. The final adaptive margin is `margin = margin_base / rank_normalizer`.\n5. New Coupling (Log-Sigmoid with Adaptive Margin): Compute the core per-sample loss by incorporating the adaptive margin directly into the log-sigmoid function, which is inherited from Parent 1. The loss is `-logsigmoid(delta_logp - margin)`.\n6. Return the weighted mean of the per-sample losses over the batch.", "hyperparams": {"temp_cost": 1.0, "beta": 0.1}, "operators_used": ["logsigmoid", "softplus", "log", "rank_gap"], "implementation_hint": {"expects": ["cost_a", "cost_b", "log_prob_w", "log_prob_l", "weight"], "returns": "scalar"}, "code": "import torch\nimport torch.nn.functional as F\n\ndef generated_loss(batch, model_output, extra):\n    \"\"\"\n    Computes the Rank-Normalized Bradley-Terry Loss.\n\n    Inherits:\n    - The core `logsigmoid` structure for pairwise preference (from Parent 1).\n    - The use of `softplus(delta_cost)` for a smooth, adaptive margin (from Parent 0).\n\n    Introduces:\n    - A novel coupling where the rank gap normalizes the margin via `log(1 + beta * rank_gap)`.\n    - A direct integration of this adaptive margin into the log-sigmoid function: `-logsigmoid(delta_logp - margin)`.\n    \"\"\"\n    # For preference learning, cost_w < cost_l.\n    cost_w = torch.min(batch['cost_a'], batch['cost_b'])\n    cost_l = torch.max(batch['cost_a'], batch['cost_b'])\n    log_prob_w = batch['log_prob_w']\n    log_prob_l = batch['log_prob_l']\n\n    # Retrieve hyperparameters\n    temp_cost = extra.get('temp_cost', 1.0)\n    beta = extra.get('beta', 0.1)\n\n    # 1. Calculate cost and log probability differences\n    delta_cost = cost_l - cost_w\n    delta_logp = log_prob_w - log_prob_l\n    batch_size = delta_cost.shape[0]\n\n    # 2. Inherited from Parent 0: Calculate a smooth, non-saturating base margin.\n    # Using a temperature parameter to control the margin's sensitivity to cost differences.\n    margin_base = F.softplus(delta_cost / temp_cost)\n\n    # 3. Calculate rank differences (rank_gap).\n    # Combine all costs to establish a batch-wide ranking.\n    all_costs = torch.cat([cost_w, cost_l], dim=0)\n    sorted_costs, _ = torch.sort(all_costs)\n    \n    # Find ranks efficiently using searchsorted.\n    rank_w = torch.searchsorted(sorted_costs, cost_w)\n    rank_l = torch.searchsorted(sorted_costs, cost_l)\n    rank_gap = (rank_l - rank_w).float()\n\n    # 4. New Coupling: Rank-Normalized Margin.\n    # The log function provides a smooth, non-linear normalization.\n    # For small rank_gap, log(1+x) ~ x, providing gentle scaling.\n    # For large rank_gap, the log growth slows down, preventing over-suppression of the margin.\n    # The 1.0 ensures the argument to log is always >= 1.\n    # A small epsilon prevents division by zero if rank_gap is 0.\n    rank_normalizer = torch.log(1.0 + beta * rank_gap).clamp(min=1e-8)\n    adaptive_margin = (margin_base / rank_normalizer).detach() # Detach to treat margin as a target\n\n    # 5. New Coupling: Log-Sigmoid with Adaptive Margin.\n    # This encourages `delta_logp` to be greater than the `adaptive_margin`.\n    # The structure is `log(1 + exp(-(delta_logp - margin)))`, which is minimized as `delta_logp` increases.\n    loss = -F.logsigmoid(delta_logp - adaptive_margin)\n\n    # 6. Apply optional weights and compute the mean.\n    weights = batch.get('weight', None)\n    if weights is not None:\n        loss = loss * weights\n        # Use clamp to avoid division by zero if all weights are zero.\n        return loss.sum() / weights.sum().clamp(min=1e-8)\n    else:\n        return loss.mean()"}, "static_ok": true, "static_reason": "", "dynamic_ok": true, "dynamic_reason": "ok", "loss_value": 1.5999393463134766, "grad_norm": 0.0}
{"generation": 9, "index": 7, "ir": {"name": "Adaptive Log-Sigmoid Loss with Tanh-Gated Rank Margin", "intuition": "This loss function creates a stable, rank-aware preference signal by combining a classic log-sigmoid structure with a novel, bounded margin that is dynamically modulated by both cost differences and batch-wise rank information.\n\nInherited Ideas:\n- From 'Rank-Aware Log-Sigmoid Preference Loss' (Parent 1), it inherits the core loss structure of `-logsigmoid(delta_logp - margin)`. This is a robust, probabilistic formulation that encourages the log-probability difference to exceed a certain margin.\n- From 'Sigmoid-Gated Adaptive Margin Loss with Rank-Gap Normalization' (Parent 0), it inherits the use of a `softplus`-transformed cost difference (`softplus(delta_cost)`) as a key component of the margin. This ensures the margin is always non-negative and grows smoothly as the cost gap between a pair increases.\n\nNew Coupling Ideas:\n1.  **Tanh-Gated Rank Margin**: The margin is constructed by coupling the cost-based signal with a rank-based signal through a `tanh` gate. The base margin is `alpha * softplus(delta_cost / temp_cost)`. This base margin is then multiplied by `tanh(beta * rank_gap)`. This gating mechanism has a desirable property: for pairs with a small rank difference, the margin is small (as `tanh(x) ≈ x` for small x), forcing the model to learn fine-grained distinctions. For pairs with a large rank difference, the `tanh` function saturates at 1, preventing the rank gap from creating an excessively large, numerically unstable margin. This creates a bounded, rank-aware margin that focuses learning without risking gradient explosion.\n2.  **Sigmoid-Weighted Loss**: The final per-sample loss is multiplied by `sigmoid(delta_cost)`. This acts as a confidence weight. For pairs with a very small cost difference (close to zero), the sigmoid weight is close to 0.5, giving them moderate importance. For pairs with a large cost difference, the weight approaches 1.0, emphasizing these 'obvious' pairs and ensuring the model learns the broad preference landscape correctly. This smoothly de-emphasizes pairs with negligible cost differences, which might otherwise introduce noise into the training process.", "pseudocode": "1. For each pair (w, l) where cost(w) < cost(l), calculate the cost difference `delta_cost = cost(l) - cost(w)` and the log probability difference `delta_logp = logp(w) - logp(l)`.\n2. Inherit from Parent 0: Calculate a smooth, non-negative base margin signal using `softplus`: `margin_base = alpha * softplus(delta_cost / temp_cost)`.\n3. Determine the cost ranks for all `cost_w` and `cost_l` in the batch. Calculate the normalized rank difference `rank_gap = (rank(cost_l) - rank(cost_w)) / (2 * batch_size)`.\n4. New Coupling (Tanh-Gated Rank Margin): Create a bounded rank-based gate using `tanh(beta * rank_gap)`. Combine this with the base margin to form the final adaptive margin: `adaptive_margin = margin_base * tanh(beta * rank_gap)`.\n5. Inherit from Parent 1: Compute the core preference loss using a log-sigmoid structure with the adaptive margin: `base_loss = -logsigmoid(delta_logp - adaptive_margin)`.\n6. New Coupling (Sigmoid-Weighted Loss): Calculate a confidence weight for each pair based on the cost difference: `confidence_weight = sigmoid(delta_cost)`. This weight de-emphasizes pairs with very small cost differences.\n7. Modulate the base loss with the confidence weight: `final_loss = confidence_weight * base_loss`.\n8. Return the weighted mean of the per-sample losses over the batch.", "hyperparams": {"alpha": 1.0, "temp_cost": 1.0, "beta": 5.0}, "operators_used": ["logsigmoid", "softplus", "tanh", "sigmoid", "rank_gap"], "implementation_hint": {"expects": ["cost_a", "cost_b", "log_prob_w", "log_prob_l"], "returns": "scalar"}, "code": "import torch\nimport torch.nn.functional as F\n\ndef generated_loss(batch, model_output, extra):\n    \"\"\"\n    Computes the Adaptive Log-Sigmoid Loss with Tanh-Gated Rank Margin.\n\n    Inherits:\n    - The core `-logsigmoid(delta_logp - margin)` structure (from Parent 1).\n    - The use of `softplus(delta_cost)` to create a smooth, non-negative margin component (from Parent 0).\n\n    Introduces:\n    - A Tanh-gated rank margin, `softplus(delta_cost) * tanh(rank_gap)`, which creates a bounded but responsive margin.\n    - A sigmoid weight, `sigmoid(delta_cost)`, applied to the final loss to de-emphasize pairs with negligible cost differences.\n    \"\"\"\n    # For preference learning, we assume cost_w < cost_l.\n    cost_w = torch.min(batch['cost_a'], batch['cost_b'])\n    cost_l = torch.max(batch['cost_a'], batch['cost_b'])\n    log_prob_w = batch['log_prob_w']\n    log_prob_l = batch['log_prob_l']\n\n    # Retrieve hyperparameters\n    alpha = extra.get('alpha', 1.0)\n    temp_cost = extra.get('temp_cost', 1.0)\n    beta = extra.get('beta', 5.0)\n\n    # 1. Calculate cost and log probability differences\n    delta_cost = cost_l - cost_w\n    delta_logp = log_prob_w - log_prob_l\n    batch_size = delta_cost.shape[0]\n\n    # 2. Inherited from Parent 0: Calculate a smooth, non-negative base margin signal.\n    margin_base = alpha * F.softplus(delta_cost / temp_cost)\n\n    # 3. Calculate rank differences.\n    all_costs = torch.cat([cost_w, cost_l], dim=0)\n    # Use torch.sort for compatibility and clarity\n    sorted_costs, _ = torch.sort(all_costs)\n    rank_w = torch.searchsorted(sorted_costs, cost_w)\n    rank_l = torch.searchsorted(sorted_costs, cost_l)\n    rank_diff = (rank_l - rank_w).float()\n\n    # 4. New Coupling: Tanh-Gated Rank Margin.\n    # tanh provides a smooth gate that saturates, preventing the margin from exploding for large rank gaps.\n    # The rank difference is used directly, as tanh naturally bounds its influence.\n    rank_gate = torch.tanh(beta * rank_diff)\n    adaptive_margin = margin_base * rank_gate\n\n    # 5. Inherited from Parent 1: Compute the core loss using the log-sigmoid structure.\n    # The loss encourages delta_logp > adaptive_margin.\n    # Detach margin to treat it as a target, not part of the model's computation graph wrt cost prediction.\n    base_loss = -F.logsigmoid(delta_logp - adaptive_margin.detach())\n\n    # 6. New Coupling: Sigmoid-Weighted Loss.\n    # This weight smoothly de-emphasizes pairs with near-zero cost differences,\n    # which can be noisy, while emphasizing clear preferences.\n    confidence_weight = torch.sigmoid(delta_cost).detach()\n\n    # 7. Modulate the base loss.\n    loss = confidence_weight * base_loss\n\n    # 8. Apply optional weights and compute the mean.\n    weights = batch.get('weight', None)\n    if weights is not None:\n        loss = loss * weights\n        # Use clamp to prevent division by zero for an all-zero weight tensor\n        return loss.sum() / weights.sum().clamp(min=1e-8)\n    else:\n        return loss.mean()"}, "static_ok": true, "static_reason": "", "dynamic_ok": true, "dynamic_reason": "ok", "loss_value": 1.1341971158981323, "grad_norm": 0.0}
