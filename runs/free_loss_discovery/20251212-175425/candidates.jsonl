{"generation": 0, "index": 1, "ir": {"name": "Adaptive Sigmoid Margin Loss", "intuition": "This loss function compares the model's preference (difference in log probabilities) against the ground truth preference (difference in costs). It uses a sigmoid function to create a smooth, bounded loss. The key idea is an adaptive margin: the 'target' log probability difference that the model should achieve is not fixed, but scales with the normalized cost difference. If two solutions have very different costs, the model is pushed harder to separate them. If their costs are similar, the model is penalized less for being uncertain. This adaptation is achieved by scaling the log-probability difference by a factor derived from the tanh of the cost difference, which keeps the scaling bounded and stable.", "pseudocode": "1. For each pair (a, b) where cost(a) < cost(b), calculate the cost difference `delta_cost = cost(b) - cost(a)` and the log probability difference `delta_logp = logp(a) - logp(b)`.\n2. Normalize the cost difference using a tanh function to get a bounded 'cost signal' between 0 and 1: `cost_signal = tanh(delta_cost / temp_cost)`.\n3. Create an adaptive margin `margin_target` that is proportional to this cost signal: `margin_target = alpha * cost_signal`.\n4. The loss is then the negative log-sigmoid of the difference between the model's preference and the adaptive margin: `loss = -logsigmoid(delta_logp - margin_target)`. This pushes `delta_logp` to be greater than the margin.\n5. The final loss is the weighted mean of these individual losses over the batch.", "hyperparams": {"alpha": 2.0, "temp_cost": 1.0}, "operators_used": ["logsigmoid", "tanh"], "implementation_hint": {"expects": ["cost_a", "cost_b", "logp_a", "logp_b"], "returns": "scalar"}, "code": "import torch\nimport torch.nn.functional as F\n\ndef generated_loss(batch, model_output, extra):\n    \"\"\"\n    Computes the Adaptive Sigmoid Margin Loss.\n\n    The loss encourages the log probability of the better solution (winner) to be\n    higher than the log probability of the worse solution (loser).\n    The target margin between them is adaptive, scaling with the normalized\n    difference in their costs.\n\n    Args:\n        batch (dict): A dictionary containing tensors from the dataloader.\n                      Expected keys:\n                      'cost_a', 'cost_b': Costs of the two solutions in a pair. Shape: [N].\n                      'log_prob_w', 'log_prob_l': Model's log probabilities for the\n                                                winner and loser solutions. Shape: [N].\n                      'weight' (optional): Per-sample loss weights. Shape: [N].\n        model_output: Not used in this loss implementation.\n        extra (dict): A dictionary for hyperparameters. Expected keys:\n                      'alpha' (float): A scaling factor for the adaptive margin.\n                      'temp_cost' (float): A temperature for normalizing the cost difference.\n\n    Returns:\n        torch.Tensor: A scalar tensor representing the mean loss over the batch.\n    \"\"\"\n    # For preference learning, we assume the dataset provides pairs where one is strictly better.\n    # Let's define the winner (w) and loser (l) based on costs.\n    # cost_w < cost_l\n    cost_w = torch.min(batch['cost_a'], batch['cost_b'])\n    cost_l = torch.max(batch['cost_a'], batch['cost_b'])\n\n    # The 'log_prob_w' and 'log_prob_l' are assumed to be provided directly by the dataset/wrapper,\n    # corresponding to the winner and loser solutions respectively.\n    log_prob_w = batch['log_prob_w']\n    log_prob_l = batch['log_prob_l']\n\n    # Retrieve hyperparameters\n    alpha = extra.get('alpha', 2.0)\n    temp_cost = extra.get('temp_cost', 1.0)\n\n    # 1. Calculate cost and log probability differences\n    # delta_cost is guaranteed to be >= 0\n    delta_cost = cost_l - cost_w\n    delta_logp = log_prob_w - log_prob_l\n\n    # 2. Normalize the cost difference into a stable, bounded 'cost signal' [0, 1)\n    # Using tanh ensures numerical stability for extreme cost differences.\n    # temp_cost controls the sensitivity of the signal to the cost difference.\n    cost_signal = torch.tanh(delta_cost / temp_cost)\n\n    # 3. Create an adaptive margin based on the cost signal.\n    # The target separation in log-probs is proportional to how different the costs are.\n    # alpha scales the overall magnitude of the margin.\n    adaptive_margin = alpha * cost_signal\n\n    # 4. Compute the core loss using negative log-sigmoid.\n    # This is equivalent to softplus(-(delta_logp - adaptive_margin)).\n    # The loss is low if delta_logp > adaptive_margin (model agrees and exceeds margin).\n    # The loss is high if delta_logp < adaptive_margin (model disagrees or doesn't meet margin).\n    # The use of logsigmoid ensures the loss is bounded and smooth.\n    loss = -F.logsigmoid(delta_logp - adaptive_margin)\n\n    # 5. Apply optional weights and compute the mean.\n    weights = batch.get('weight', None)\n    if weights is not None:\n        loss = loss * weights\n        return loss.sum() / weights.sum().clamp(min=1e-8)\n    else:\n        return loss.mean()"}, "fitness": {"hf_like_score": 33.71875762939453, "validation_objective": 33.71875762939453, "generalization_penalty": 0.0, "generalization_objectives": {"100": 33.52845573425293}, "train_score_mean": 39.54556152248382, "train_loss_mean": 1.3330533207654953, "pair_count": 129023909, "config": {"hf": {"problem": "tsp", "hf_steps": 1000, "train_problem_size": 100, "valid_problem_sizes": [100], "train_batch_size": 64, "pomo_size": 64, "learning_rate": 0.0003, "weight_decay": 1e-06, "alpha": 0.05, "device": "cuda", "seed": 1234, "num_validation_episodes": 128, "validation_batch_size": 64, "generalization_penalty_weight": 1.0, "pool_version": "v0"}, "free_loss": {"f1_steps": 1000, "f2_steps": 100, "f3_enabled": false}}, "loss_ir": {"name": "Adaptive Sigmoid Margin Loss", "intuition": "This loss function compares the model's preference (difference in log probabilities) against the ground truth preference (difference in costs). It uses a sigmoid function to create a smooth, bounded loss. The key idea is an adaptive margin: the 'target' log probability difference that the model should achieve is not fixed, but scales with the normalized cost difference. If two solutions have very different costs, the model is pushed harder to separate them. If their costs are similar, the model is penalized less for being uncertain. This adaptation is achieved by scaling the log-probability difference by a factor derived from the tanh of the cost difference, which keeps the scaling bounded and stable.", "hyperparams": {"alpha": 2.0, "temp_cost": 1.0}, "operators_used": ["logsigmoid", "tanh"]}}, "better_than_baseline": false}
{"generation": 1, "index": 0, "ir": {"name": "Softplus Adaptive Margin Loss", "intuition": "This loss function compares the model's preference (difference in log probabilities) against the ground truth preference (difference in costs). It uses a softplus function to create a smooth, non-negative loss. The key idea is an adaptive margin: the 'target' log probability difference that the model should achieve is not fixed, but scales with the normalized cost difference. If two solutions have very different costs, the model is pushed harder to separate them. If their costs are similar, the model is penalized less for being uncertain. This adaptation is achieved by scaling the log-probability difference by a factor derived from the tanh of the cost difference, which keeps the scaling bounded and stable. Using softplus instead of negative logsigmoid is a common alternative for hinge-like losses.", "pseudocode": "1. For each pair (a, b) where cost(a) < cost(b), calculate the cost difference `delta_cost = cost(b) - cost(a)` and the log probability difference `delta_logp = logp(a) - logp(b)`.\n2. Normalize the cost difference using a tanh function to get a bounded 'cost signal' between 0 and 1: `cost_signal = tanh(delta_cost / temp_cost)`.\n3. Create an adaptive margin `margin_target` that is proportional to this cost signal: `margin_target = alpha * cost_signal`.\n4. The loss is then the softplus of the difference between the adaptive margin and the model's preference: `loss = softplus(margin_target - delta_logp)`. This pushes `delta_logp` to be greater than the margin.\n5. The final loss is the weighted mean of these individual losses over the batch.", "hyperparams": {"alpha": 2.0, "temp_cost": 1.0}, "operators_used": ["softplus", "tanh"], "implementation_hint": {"expects": ["cost_a", "cost_b", "log_prob_w", "log_prob_l", "weight"], "returns": "scalar"}, "code": "import torch\nimport torch.nn.functional as F\n\ndef generated_loss(batch, model_output, extra):\n    \"\"\"\n    Computes the Softplus Adaptive Margin Loss.\n\n    The loss encourages the log probability of the better solution (winner) to be\n    higher than the log probability of the worse solution (loser).\n    The target margin between them is adaptive, scaling with the normalized\n    difference in their costs. This is a conservative mutation of the parent\n    loss, replacing `-logsigmoid(x)` with `softplus(-x)` which are mathematically equivalent.\n\n    Args:\n        batch (dict): A dictionary containing tensors from the dataloader.\n                      Expected keys:\n                      'cost_a', 'cost_b': Costs of the two solutions in a pair. Shape: [N].\n                      'log_prob_w', 'log_prob_l': Model's log probabilities for the\n                                                winner and loser solutions. Shape: [N].\n                      'weight' (optional): Per-sample loss weights. Shape: [N].\n        model_output: Not used in this loss implementation.\n        extra (dict): A dictionary for hyperparameters. Expected keys:\n                      'alpha' (float): A scaling factor for the adaptive margin.\n                      'temp_cost' (float): A temperature for normalizing the cost difference.\n\n    Returns:\n        torch.Tensor: A scalar tensor representing the mean loss over the batch.\n    \"\"\"\n    # For preference learning, we assume the dataset provides pairs where one is strictly better.\n    # Let's define the winner (w) and loser (l) based on costs.\n    # cost_w < cost_l\n    cost_w = torch.min(batch['cost_a'], batch['cost_b'])\n    cost_l = torch.max(batch['cost_a'], batch['cost_b'])\n\n    # The 'log_prob_w' and 'log_prob_l' are assumed to be provided directly by the dataset/wrapper,\n    # corresponding to the winner and loser solutions respectively.\n    log_prob_w = batch['log_prob_w']\n    log_prob_l = batch['log_prob_l']\n\n    # Retrieve hyperparameters\n    alpha = extra.get('alpha', 2.0)\n    temp_cost = extra.get('temp_cost', 1.0)\n\n    # 1. Calculate cost and log probability differences\n    # delta_cost is guaranteed to be >= 0\n    delta_cost = cost_l - cost_w\n    delta_logp = log_prob_w - log_prob_l\n\n    # 2. Normalize the cost difference into a stable, bounded 'cost signal' [0, 1)\n    # Using tanh ensures numerical stability for extreme cost differences.\n    # temp_cost controls the sensitivity of the signal to the cost difference.\n    cost_signal = torch.tanh(delta_cost / temp_cost)\n\n    # 3. Create an adaptive margin based on the cost signal.\n    # The target separation in log-probs is proportional to how different the costs are.\n    # alpha scales the overall magnitude of the margin.\n    adaptive_margin = alpha * cost_signal\n\n    # 4. Compute the core loss using softplus.\n    # This is equivalent to -logsigmoid(delta_logp - adaptive_margin).\n    # The loss is low if delta_logp > adaptive_margin (model agrees and exceeds margin).\n    # The loss is high if delta_logp < adaptive_margin (model disagrees or doesn't meet margin).\n    loss = F.softplus(adaptive_margin - delta_logp)\n\n    # 5. Apply optional weights and compute the mean.\n    weights = batch.get('weight', None)\n    if weights is not None:\n        loss = loss * weights\n        # Use clamp for numerical stability if weights sum to zero\n        return loss.sum() / weights.sum().clamp(min=1e-8)\n    else:\n        return loss.mean()"}, "fitness": {"hf_like_score": 38.20409393310547, "validation_objective": 38.20409393310547, "generalization_penalty": 0.0, "generalization_objectives": {"100": 37.90019989013672}, "train_score_mean": 39.091798800468446, "train_loss_mean": 1.067961990237236, "pair_count": 129023905, "config": {"hf": {"problem": "tsp", "hf_steps": 1000, "train_problem_size": 100, "valid_problem_sizes": [100], "train_batch_size": 64, "pomo_size": 64, "learning_rate": 0.0003, "weight_decay": 1e-06, "alpha": 0.05, "device": "cuda", "seed": 1234, "num_validation_episodes": 128, "validation_batch_size": 64, "generalization_penalty_weight": 1.0, "pool_version": "v0"}, "free_loss": {"f1_steps": 1000, "f2_steps": 100, "f3_enabled": false}}, "loss_ir": {"name": "Softplus Adaptive Margin Loss", "intuition": "This loss function compares the model's preference (difference in log probabilities) against the ground truth preference (difference in costs). It uses a softplus function to create a smooth, non-negative loss. The key idea is an adaptive margin: the 'target' log probability difference that the model should achieve is not fixed, but scales with the normalized cost difference. If two solutions have very different costs, the model is pushed harder to separate them. If their costs are similar, the model is penalized less for being uncertain. This adaptation is achieved by scaling the log-probability difference by a factor derived from the tanh of the cost difference, which keeps the scaling bounded and stable. Using softplus instead of negative logsigmoid is a common alternative for hinge-like losses.", "hyperparams": {"alpha": 2.0, "temp_cost": 1.0}, "operators_used": ["softplus", "tanh"]}}, "better_than_baseline": false}
{"generation": 1, "index": 1, "ir": {"name": "Adaptive Sigmoid Margin Loss with Softplus", "intuition": "This loss function compares the model's preference (difference in log probabilities) against a target margin that adapts to the cost difference. It uses a sigmoid function to create a smooth, bounded loss. The adaptive margin scales with the cost difference, pushing the model harder to separate solutions with large cost gaps. This version replaces `tanh` with `softplus` for normalizing the cost difference. This creates a cost signal that is unbounded but grows sub-linearly, providing a strong signal for large cost differences while still being smooth and non-negative.", "pseudocode": "1. For each pair (a, b) where cost(a) < cost(b), calculate the cost difference `delta_cost = cost(b) - cost(a)` and the log probability difference `delta_logp = logp(a) - logp(b)`.\n2. Normalize the cost difference using a softplus function to get a smooth, non-negative 'cost signal': `cost_signal = softplus(delta_cost / temp_cost)`.\n3. Create an adaptive margin `margin_target` that is proportional to this cost signal: `margin_target = alpha * cost_signal`.\n4. The loss is then the negative log-sigmoid of the difference between the model's preference and the adaptive margin: `loss = -logsigmoid(delta_logp - margin_target)`. This pushes `delta_logp` to be greater than the margin.\n5. The final loss is the weighted mean of these individual losses over the batch.", "hyperparams": {"alpha": 1.0, "temp_cost": 1.0}, "operators_used": ["logsigmoid", "softplus"], "implementation_hint": {"expects": ["cost_a", "cost_b", "log_prob_w", "log_prob_l", "weight"], "returns": "scalar"}, "code": "import torch\nimport torch.nn.functional as F\n\ndef generated_loss(batch, model_output, extra):\n    \"\"\"\n    Computes the Adaptive Sigmoid Margin Loss with a Softplus-based cost signal.\n\n    The loss encourages the log probability of the better solution (winner) to be\n    higher than the log probability of the worse solution (loser).\n    The target margin between them is adaptive, scaling with the normalized\n    difference in their costs.\n\n    Args:\n        batch (dict): A dictionary containing tensors from the dataloader.\n                      Expected keys:\n                      'cost_a', 'cost_b': Costs of the two solutions in a pair. Shape: [N].\n                      'log_prob_w', 'log_prob_l': Model's log probabilities for the\n                                                winner and loser solutions. Shape: [N].\n                      'weight' (optional): Per-sample loss weights. Shape: [N].\n        model_output: Not used in this loss implementation.\n        extra (dict): A dictionary for hyperparameters. Expected keys:\n                      'alpha' (float): A scaling factor for the adaptive margin.\n                      'temp_cost' (float): A temperature for normalizing the cost difference.\n\n    Returns:\n        torch.Tensor: A scalar tensor representing the mean loss over the batch.\n    \"\"\"\n    # For preference learning, we assume the dataset provides pairs where one is strictly better.\n    # Let's define the winner (w) and loser (l) based on costs.\n    # cost_w < cost_l\n    cost_w = torch.min(batch['cost_a'], batch['cost_b'])\n    cost_l = torch.max(batch['cost_a'], batch['cost_b'])\n\n    # The 'log_prob_w' and 'log_prob_l' are assumed to be provided directly by the dataset/wrapper,\n    # corresponding to the winner and loser solutions respectively.\n    log_prob_w = batch['log_prob_w']\n    log_prob_l = batch['log_prob_l']\n\n    # Retrieve hyperparameters\n    alpha = extra.get('alpha', 1.0)\n    temp_cost = extra.get('temp_cost', 1.0)\n\n    # 1. Calculate cost and log probability differences\n    # delta_cost is guaranteed to be >= 0\n    delta_cost = cost_l - cost_w\n    delta_logp = log_prob_w - log_prob_l\n\n    # 2. Normalize the cost difference into a smooth, non-negative 'cost signal'\n    # Using softplus instead of tanh creates an unbounded but sub-linearly growing signal.\n    # temp_cost controls the sensitivity of the signal to the cost difference.\n    cost_signal = F.softplus(delta_cost / temp_cost)\n\n    # 3. Create an adaptive margin based on the cost signal.\n    # The target separation in log-probs is proportional to how different the costs are.\n    # alpha scales the overall magnitude of the margin.\n    adaptive_margin = alpha * cost_signal\n\n    # 4. Compute the core loss using negative log-sigmoid.\n    # This is equivalent to softplus(-(delta_logp - adaptive_margin)).\n    # The loss is low if delta_logp > adaptive_margin (model agrees and exceeds margin).\n    # The loss is high if delta_logp < adaptive_margin (model disagrees or doesn't meet margin).\n    loss = -F.logsigmoid(delta_logp - adaptive_margin)\n\n    # 5. Apply optional weights and compute the mean.\n    weights = batch.get('weight', None)\n    if weights is not None:\n        loss = loss * weights\n        # Use a small epsilon to prevent division by zero if all weights are zero.\n        return loss.sum() / weights.sum().clamp(min=1e-8)\n    else:\n        return loss.mean()"}, "fitness": {"hf_like_score": 39.694929122924805, "validation_objective": 39.691810607910156, "generalization_penalty": 0.0031185150146484375, "generalization_objectives": {"100": 39.694929122924805}, "train_score_mean": 39.400732120513915, "train_loss_mean": 1.349332366347313, "pair_count": 129023923, "config": {"hf": {"problem": "tsp", "hf_steps": 1000, "train_problem_size": 100, "valid_problem_sizes": [100], "train_batch_size": 64, "pomo_size": 64, "learning_rate": 0.0003, "weight_decay": 1e-06, "alpha": 0.05, "device": "cuda", "seed": 1234, "num_validation_episodes": 128, "validation_batch_size": 64, "generalization_penalty_weight": 1.0, "pool_version": "v0"}, "free_loss": {"f1_steps": 1000, "f2_steps": 100, "f3_enabled": false}}, "loss_ir": {"name": "Adaptive Sigmoid Margin Loss with Softplus", "intuition": "This loss function compares the model's preference (difference in log probabilities) against a target margin that adapts to the cost difference. It uses a sigmoid function to create a smooth, bounded loss. The adaptive margin scales with the cost difference, pushing the model harder to separate solutions with large cost gaps. This version replaces `tanh` with `softplus` for normalizing the cost difference. This creates a cost signal that is unbounded but grows sub-linearly, providing a strong signal for large cost differences while still being smooth and non-negative.", "hyperparams": {"alpha": 1.0, "temp_cost": 1.0}, "operators_used": ["logsigmoid", "softplus"]}}, "better_than_baseline": false}
{"generation": 1, "index": 2, "ir": {"name": "Sigmoid Log-Cost Margin Loss", "intuition": "This loss function compares the model's preference (difference in log probabilities) against the ground truth preference (difference in costs). It uses a sigmoid loss structure. The key idea is to define the target margin directly from the logarithm of the cost difference. Using `log(1 + delta_cost)` ensures that the margin is zero when costs are equal, grows as the cost difference increases, and is less sensitive to very large cost differences due to the compressive nature of the logarithm. This provides a natural, parameter-free way to scale the margin based on the magnitude of the cost gap, pushing the model harder to separate pairs with larger cost differences.", "pseudocode": "1. For each pair (a, b) where cost(a) < cost(b), calculate the cost difference `delta_cost = cost(b) - cost(a)` and the log probability difference `delta_logp = logp(a) - logp(b)`.\n2. Define a margin target based on the logarithm of the cost difference: `margin_target = log(1 + delta_cost / temp)`.\n3. The loss is then the negative log-sigmoid of the difference between the model's preference and the log-cost margin: `loss = -logsigmoid(delta_logp - margin_target)`. This pushes `delta_logp` to be greater than the margin.\n4. The final loss is the weighted mean of these individual losses over the batch.", "hyperparams": {"temp": 1.0}, "operators_used": ["logsigmoid", "log"], "implementation_hint": {"expects": ["cost_a", "cost_b", "log_prob_w", "log_prob_l", "weight"], "returns": "scalar"}, "code": "import torch\nimport torch.nn.functional as F\n\ndef generated_loss(batch, model_output, extra):\n    \"\"\"\n    Computes the Sigmoid Log-Cost Margin Loss.\n\n    The loss encourages the log probability of the better solution (winner) to be\n    higher than the log probability of the worse solution (loser).\n    The target margin between them is adaptive, scaling with the logarithm of the\n    cost difference.\n\n    Args:\n        batch (dict): A dictionary containing tensors from the dataloader.\n                      Expected keys:\n                      'cost_a', 'cost_b': Costs of the two solutions in a pair. Shape: [N].\n                      'log_prob_w', 'log_prob_l': Model's log probabilities for the\n                                                winner and loser solutions. Shape: [N].\n                      'weight' (optional): Per-sample loss weights. Shape: [N].\n        model_output: Not used in this loss implementation.\n        extra (dict): A dictionary for hyperparameters. Expected keys:\n                      'temp' (float): A temperature for scaling the cost difference before the log.\n\n    Returns:\n        torch.Tensor: A scalar tensor representing the mean loss over the batch.\n    \"\"\"\n    # For preference learning, we assume the dataset provides pairs where one is strictly better.\n    # cost_w < cost_l\n    cost_w = torch.min(batch['cost_a'], batch['cost_b'])\n    cost_l = torch.max(batch['cost_a'], batch['cost_b'])\n\n    # The 'log_prob_w' and 'log_prob_l' are assumed to be provided directly by the dataset/wrapper,\n    # corresponding to the winner and loser solutions respectively.\n    log_prob_w = batch['log_prob_w']\n    log_prob_l = batch['log_prob_l']\n\n    # Retrieve hyperparameters\n    temp = extra.get('temp', 1.0)\n\n    # 1. Calculate cost and log probability differences\n    # delta_cost is guaranteed to be >= 0\n    delta_cost = cost_l - cost_w\n    delta_logp = log_prob_w - log_prob_l\n\n    # 2. Create an adaptive margin based on the log of the cost difference.\n    # Using log(1 + x) ensures the margin is 0 when delta_cost is 0 and grows monotonically.\n    # The log naturally dampens the effect of very large cost differences.\n    # temp scales the cost difference before the logarithm.\n    log_cost_margin = torch.log(1 + delta_cost / temp)\n\n    # 3. Compute the core loss using negative log-sigmoid.\n    # This is equivalent to softplus(-(delta_logp - log_cost_margin)).\n    # The loss is low if delta_logp > log_cost_margin (model agrees and exceeds margin).\n    # The loss is high if delta_logp < log_cost_margin (model disagrees or doesn't meet margin).\n    loss = -F.logsigmoid(delta_logp - log_cost_margin)\n\n    # 4. Apply optional weights and compute the mean.\n    weights = batch.get('weight', None)\n    if weights is not None:\n        loss = loss * weights\n        # Use a small epsilon to prevent division by zero if all weights are zero.\n        return loss.sum() / weights.sum().clamp(min=1e-8)\n    else:\n        return loss.mean()"}, "fitness": {"hf_like_score": 43.47769737243652, "validation_objective": 43.05756759643555, "generalization_penalty": 0.42012977600097656, "generalization_objectives": {"100": 43.47769737243652}, "train_score_mean": 38.91235827922821, "train_loss_mean": 1.5603323699235916, "pair_count": 129023928, "config": {"hf": {"problem": "tsp", "hf_steps": 1000, "train_problem_size": 100, "valid_problem_sizes": [100], "train_batch_size": 64, "pomo_size": 64, "learning_rate": 0.0003, "weight_decay": 1e-06, "alpha": 0.05, "device": "cuda", "seed": 1234, "num_validation_episodes": 128, "validation_batch_size": 64, "generalization_penalty_weight": 1.0, "pool_version": "v0"}, "free_loss": {"f1_steps": 1000, "f2_steps": 100, "f3_enabled": false}}, "loss_ir": {"name": "Sigmoid Log-Cost Margin Loss", "intuition": "This loss function compares the model's preference (difference in log probabilities) against the ground truth preference (difference in costs). It uses a sigmoid loss structure. The key idea is to define the target margin directly from the logarithm of the cost difference. Using `log(1 + delta_cost)` ensures that the margin is zero when costs are equal, grows as the cost difference increases, and is less sensitive to very large cost differences due to the compressive nature of the logarithm. This provides a natural, parameter-free way to scale the margin based on the magnitude of the cost gap, pushing the model harder to separate pairs with larger cost differences.", "hyperparams": {"temp": 1.0}, "operators_used": ["logsigmoid", "log"]}}, "better_than_baseline": false}
{"generation": 1, "index": 4, "ir": {"name": "Softplus Adaptive Margin Loss", "intuition": "This loss function compares the model's preference (difference in log probabilities) against the ground truth preference (difference in costs). It uses a softplus function to create a smooth, non-negative loss. The key idea is an adaptive margin: the 'target' log probability difference that the model should achieve is not fixed, but scales with the normalized cost difference. If two solutions have very different costs, the model is pushed harder to separate them. If their costs are similar, the model is penalized less for being uncertain. This adaptation is achieved by scaling the log-probability difference by a factor derived from the tanh of the cost difference, which keeps the scaling bounded and stable. The softplus function provides a smooth penalty when the model's preference falls short of this adaptive margin.", "pseudocode": "1. For each pair (a, b) where cost(a) < cost(b), calculate the cost difference `delta_cost = cost(b) - cost(a)` and the log probability difference `delta_logp = logp(a) - logp(b)`.\n2. Normalize the cost difference using a tanh function to get a bounded 'cost signal' between 0 and 1: `cost_signal = tanh(delta_cost / temp_cost)`.\n3. Create an adaptive margin `margin_target` that is proportional to this cost signal: `margin_target = alpha * cost_signal`.\n4. The loss is then the softplus of the negative difference between the model's preference and the adaptive margin: `loss = softplus(margin_target - delta_logp)`. This pushes `delta_logp` to be greater than the margin.\n5. The final loss is the weighted mean of these individual losses over the batch.", "hyperparams": {"alpha": 2.0, "temp_cost": 1.0}, "operators_used": ["softplus", "tanh"], "implementation_hint": {"expects": ["cost_a", "cost_b", "log_prob_w", "log_prob_l", "weight"], "returns": "scalar"}, "code": "import torch\nimport torch.nn.functional as F\n\ndef generated_loss(batch, model_output, extra):\n    \"\"\"\n    Computes the Softplus Adaptive Margin Loss.\n\n    The loss encourages the log probability of the better solution (winner) to be\n    higher than the log probability of the worse solution (loser).\n    The target margin between them is adaptive, scaling with the normalized\n    difference in their costs.\n\n    Args:\n        batch (dict): A dictionary containing tensors from the dataloader.\n                      Expected keys:\n                      'cost_a', 'cost_b': Costs of the two solutions in a pair. Shape: [N].\n                      'log_prob_w', 'log_prob_l': Model's log probabilities for the\n                                                winner and loser solutions. Shape: [N].\n                      'weight' (optional): Per-sample loss weights. Shape: [N].\n        model_output: Not used in this loss implementation.\n        extra (dict): A dictionary for hyperparameters. Expected keys:\n                      'alpha' (float): A scaling factor for the adaptive margin.\n                      'temp_cost' (float): A temperature for normalizing the cost difference.\n\n    Returns:\n        torch.Tensor: A scalar tensor representing the mean loss over the batch.\n    \"\"\"\n    # For preference learning, we assume the dataset provides pairs where one is strictly better.\n    # Let's define the winner (w) and loser (l) based on costs.\n    # cost_w < cost_l\n    cost_w = torch.min(batch['cost_a'], batch['cost_b'])\n    cost_l = torch.max(batch['cost_a'], batch['cost_b'])\n\n    # The 'log_prob_w' and 'log_prob_l' are assumed to be provided directly by the dataset/wrapper,\n    # corresponding to the winner and loser solutions respectively.\n    log_prob_w = batch['log_prob_w']\n    log_prob_l = batch['log_prob_l']\n\n    # Retrieve hyperparameters\n    alpha = extra.get('alpha', 2.0)\n    temp_cost = extra.get('temp_cost', 1.0)\n\n    # 1. Calculate cost and log probability differences\n    # delta_cost is guaranteed to be >= 0\n    delta_cost = cost_l - cost_w\n    delta_logp = log_prob_w - log_prob_l\n\n    # 2. Normalize the cost difference into a stable, bounded 'cost signal' [0, 1)\n    # Using tanh ensures numerical stability for extreme cost differences.\n    # temp_cost controls the sensitivity of the signal to the cost difference.\n    cost_signal = torch.tanh(delta_cost / temp_cost)\n\n    # 3. Create an adaptive margin based on the cost signal.\n    # The target separation in log-probs is proportional to how different the costs are.\n    # alpha scales the overall magnitude of the margin.\n    adaptive_margin = alpha * cost_signal\n\n    # 4. Compute the core loss using softplus.\n    # This is mathematically equivalent to -logsigmoid(x) but can sometimes be\n    # more numerically stable or offer different gradient characteristics.\n    # The loss is low if delta_logp > adaptive_margin (model agrees and exceeds margin).\n    # The loss is high if delta_logp < adaptive_margin (model disagrees or doesn't meet margin).\n    loss = F.softplus(adaptive_margin - delta_logp)\n\n    # 5. Apply optional weights and compute the mean.\n    weights = batch.get('weight', None)\n    if weights is not None:\n        loss = loss * weights\n        # Use a small epsilon to prevent division by zero if all weights are zero.\n        return loss.sum() / weights.sum().clamp(min=1e-8)\n    else:\n        return loss.mean()"}, "fitness": {"hf_like_score": 31.223636627197266, "validation_objective": 31.223636627197266, "generalization_penalty": 0.0, "generalization_objectives": {"100": 30.860966682434082}, "train_score_mean": 39.30589270877838, "train_loss_mean": 1.2036753427386284, "pair_count": 129023924, "config": {"hf": {"problem": "tsp", "hf_steps": 1000, "train_problem_size": 100, "valid_problem_sizes": [100], "train_batch_size": 64, "pomo_size": 64, "learning_rate": 0.0003, "weight_decay": 1e-06, "alpha": 0.05, "device": "cuda", "seed": 1234, "num_validation_episodes": 128, "validation_batch_size": 64, "generalization_penalty_weight": 1.0, "pool_version": "v0"}, "free_loss": {"f1_steps": 1000, "f2_steps": 100, "f3_enabled": false}}, "loss_ir": {"name": "Softplus Adaptive Margin Loss", "intuition": "This loss function compares the model's preference (difference in log probabilities) against the ground truth preference (difference in costs). It uses a softplus function to create a smooth, non-negative loss. The key idea is an adaptive margin: the 'target' log probability difference that the model should achieve is not fixed, but scales with the normalized cost difference. If two solutions have very different costs, the model is pushed harder to separate them. If their costs are similar, the model is penalized less for being uncertain. This adaptation is achieved by scaling the log-probability difference by a factor derived from the tanh of the cost difference, which keeps the scaling bounded and stable. The softplus function provides a smooth penalty when the model's preference falls short of this adaptive margin.", "hyperparams": {"alpha": 2.0, "temp_cost": 1.0}, "operators_used": ["softplus", "tanh"]}}, "better_than_baseline": false}
{"generation": 1, "index": 6, "ir": {"name": "Sigmoid Margin Loss with Softplus Cost Scaling", "intuition": "This loss function compares the model's preference (difference in log probabilities) against a target margin. It uses a sigmoid function to create a smooth, bounded loss. The key idea is an adaptive margin: the 'target' log probability difference scales with the cost difference. Instead of using tanh for normalization, this variant uses a softplus function on the cost difference. Softplus provides a smooth, non-negative, and unbounded scaling factor, meaning that as the cost difference grows very large, the target margin also continues to grow, pushing the model to create an even larger separation in log probabilities for pairs that are very far apart in quality. This contrasts with tanh, which saturates.", "pseudocode": "1. For each pair (a, b) where cost(a) < cost(b), calculate the cost difference `delta_cost = cost(b) - cost(a)` and the log probability difference `delta_logp = logp(a) - logp(b)`.\n2. Transform the cost difference using a softplus function to get a smooth, non-negative 'cost signal': `cost_signal = softplus(delta_cost / temp_cost)`.\n3. Create an adaptive margin `margin_target` that is proportional to this cost signal: `margin_target = alpha * cost_signal`.\n4. The loss is then the negative log-sigmoid of the difference between the model's preference and the adaptive margin: `loss = -logsigmoid(delta_logp - margin_target)`. This pushes `delta_logp` to be greater than the margin.\n5. The final loss is the weighted mean of these individual losses over the batch.", "hyperparams": {"alpha": 1.0, "temp_cost": 1.0}, "operators_used": ["logsigmoid", "softplus"], "implementation_hint": {"expects": ["cost_a", "cost_b", "log_prob_w", "log_prob_l", "weight"], "returns": "scalar"}, "code": "import torch\nimport torch.nn.functional as F\n\ndef generated_loss(batch, model_output, extra):\n    \"\"\"\n    Computes the Sigmoid Margin Loss with Softplus Cost Scaling.\n\n    The loss encourages the log probability of the better solution (winner) to be\n    higher than the log probability of the worse solution (loser).\n    The target margin between them is adaptive, scaling with the softplus-transformed\n    difference in their costs.\n\n    Args:\n        batch (dict): A dictionary containing tensors from the dataloader.\n                      Expected keys:\n                      'cost_a', 'cost_b': Costs of the two solutions in a pair. Shape: [N].\n                      'log_prob_w', 'log_prob_l': Model's log probabilities for the\n                                                winner and loser solutions. Shape: [N].\n                      'weight' (optional): Per-sample loss weights. Shape: [N].\n        model_output: Not used in this loss implementation.\n        extra (dict): A dictionary for hyperparameters. Expected keys:\n                      'alpha' (float): A scaling factor for the adaptive margin.\n                      'temp_cost' (float): A temperature for scaling the cost difference.\n\n    Returns:\n        torch.Tensor: A scalar tensor representing the mean loss over the batch.\n    \"\"\"\n    # For preference learning, we assume the dataset provides pairs where one is strictly better.\n    # Let's define the winner (w) and loser (l) based on costs.\n    # cost_w < cost_l\n    cost_w = torch.min(batch['cost_a'], batch['cost_b'])\n    cost_l = torch.max(batch['cost_a'], batch['cost_b'])\n\n    # The 'log_prob_w' and 'log_prob_l' are assumed to be provided directly by the dataset/wrapper,\n    # corresponding to the winner and loser solutions respectively.\n    log_prob_w = batch['log_prob_w']\n    log_prob_l = batch['log_prob_l']\n\n    # Retrieve hyperparameters\n    alpha = extra.get('alpha', 1.0)\n    temp_cost = extra.get('temp_cost', 1.0)\n\n    # 1. Calculate cost and log probability differences\n    # delta_cost is guaranteed to be >= 0\n    delta_cost = cost_l - cost_w\n    delta_logp = log_prob_w - log_prob_l\n\n    # 2. Transform the cost difference using softplus to get a smooth, non-negative signal.\n    # Unlike tanh, softplus does not saturate, allowing the margin to grow with large cost differences.\n    # temp_cost controls the sensitivity of the signal to the cost difference.\n    cost_signal = F.softplus(delta_cost / temp_cost)\n\n    # 3. Create an adaptive margin based on the cost signal.\n    # The target separation in log-probs is proportional to how different the costs are.\n    # alpha scales the overall magnitude of the margin.\n    adaptive_margin = alpha * cost_signal\n\n    # 4. Compute the core loss using negative log-sigmoid.\n    # This is equivalent to softplus(-(delta_logp - adaptive_margin)).\n    # The loss is low if delta_logp > adaptive_margin (model agrees and exceeds margin).\n    # The loss is high if delta_logp < adaptive_margin (model disagrees or doesn't meet margin).\n    loss = -F.logsigmoid(delta_logp - adaptive_margin)\n\n    # 5. Apply optional weights and compute the mean.\n    weights = batch.get('weight', None)\n    if weights is not None:\n        loss = loss * weights\n        return loss.sum() / weights.sum().clamp(min=1e-8)\n    else:\n        return loss.mean()"}, "fitness": {"hf_like_score": 41.033376693725586, "validation_objective": 41.033376693725586, "generalization_penalty": 0.0, "generalization_objectives": {"100": 40.46025466918945}, "train_score_mean": 38.59072824954987, "train_loss_mean": 1.6508174431324005, "pair_count": 129023935, "config": {"hf": {"problem": "tsp", "hf_steps": 1000, "train_problem_size": 100, "valid_problem_sizes": [100], "train_batch_size": 64, "pomo_size": 64, "learning_rate": 0.0003, "weight_decay": 1e-06, "alpha": 0.05, "device": "cuda", "seed": 1234, "num_validation_episodes": 128, "validation_batch_size": 64, "generalization_penalty_weight": 1.0, "pool_version": "v0"}, "free_loss": {"f1_steps": 1000, "f2_steps": 100, "f3_enabled": false}}, "loss_ir": {"name": "Sigmoid Margin Loss with Softplus Cost Scaling", "intuition": "This loss function compares the model's preference (difference in log probabilities) against a target margin. It uses a sigmoid function to create a smooth, bounded loss. The key idea is an adaptive margin: the 'target' log probability difference scales with the cost difference. Instead of using tanh for normalization, this variant uses a softplus function on the cost difference. Softplus provides a smooth, non-negative, and unbounded scaling factor, meaning that as the cost difference grows very large, the target margin also continues to grow, pushing the model to create an even larger separation in log probabilities for pairs that are very far apart in quality. This contrasts with tanh, which saturates.", "hyperparams": {"alpha": 1.0, "temp_cost": 1.0}, "operators_used": ["logsigmoid", "softplus"]}}, "better_than_baseline": false}
{"generation": 1, "index": 7, "ir": {"name": "Softplus Adaptive Margin Loss", "intuition": "This loss function compares the model's preference (difference in log probabilities) against a target margin that adapts to the ground truth cost difference. The core idea is to use `softplus` to create a smooth, non-negative loss. The target margin for the log probability difference scales with the `softplus` of the cost difference. This means that as the cost gap between two solutions widens, the model is pushed much harder to separate them, but the scaling is smooth and avoids the saturation issues of `tanh`. The `softplus` function on the cost difference ensures the margin is always non-negative and grows approximately linearly for large cost differences.", "pseudocode": "1. For each pair (a, b) where cost(a) < cost(b), calculate the cost difference `delta_cost = cost(b) - cost(a)` and the log probability difference `delta_logp = logp(a) - logp(b)`.\n2. Transform the cost difference using a softplus function to create a smooth, non-negative 'cost signal': `cost_signal = softplus(delta_cost / temp_cost)`.\n3. Create an adaptive margin `margin_target` that is proportional to this cost signal: `margin_target = alpha * cost_signal`.\n4. The loss is then the softplus of the margin minus the model's preference: `loss = softplus(margin_target - delta_logp)`. This is a smooth approximation of `max(0, margin_target - delta_logp)` and penalizes cases where `delta_logp` is less than the target margin.\n5. The final loss is the weighted mean of these individual losses over the batch.", "hyperparams": {"alpha": 1.0, "temp_cost": 1.0}, "operators_used": ["softplus"], "implementation_hint": {"expects": ["cost_a", "cost_b", "log_prob_w", "log_prob_l", "weight"], "returns": "scalar"}, "code": "import torch\nimport torch.nn.functional as F\n\ndef generated_loss(batch, model_output, extra):\n    \"\"\"\n    Computes the Softplus Adaptive Margin Loss.\n\n    The loss encourages the log probability of the better solution (winner) to be\n    higher than the log probability of the worse solution (loser) by an adaptive margin.\n    The target margin scales with the softplus-transformed difference in their costs.\n\n    Args:\n        batch (dict): A dictionary containing tensors from the dataloader.\n                      Expected keys:\n                      'cost_a', 'cost_b': Costs of the two solutions in a pair. Shape: [N].\n                      'log_prob_w', 'log_prob_l': Model's log probabilities for the\n                                                winner and loser solutions. Shape: [N].\n                      'weight' (optional): Per-sample loss weights. Shape: [N].\n        model_output: Not used in this loss implementation.\n        extra (dict): A dictionary for hyperparameters. Expected keys:\n                      'alpha' (float): A scaling factor for the adaptive margin.\n                      'temp_cost' (float): A temperature for normalizing the cost difference.\n\n    Returns:\n        torch.Tensor: A scalar tensor representing the mean loss over the batch.\n    \"\"\"\n    # For preference learning, we assume the dataset provides pairs where one is strictly better.\n    # cost_w < cost_l\n    cost_w = torch.min(batch['cost_a'], batch['cost_b'])\n    cost_l = torch.max(batch['cost_a'], batch['cost_b'])\n\n    # The 'log_prob_w' and 'log_prob_l' are assumed to be provided directly, corresponding\n    # to the winner and loser solutions respectively.\n    log_prob_w = batch['log_prob_w']\n    log_prob_l = batch['log_prob_l']\n\n    # Retrieve hyperparameters\n    alpha = extra.get('alpha', 1.0)\n    temp_cost = extra.get('temp_cost', 1.0)\n\n    # 1. Calculate cost and log probability differences\n    # delta_cost is guaranteed to be >= 0\n    delta_cost = cost_l - cost_w\n    delta_logp = log_prob_w - log_prob_l\n\n    # 2. Transform the cost difference into a smooth, non-negative 'cost signal'.\n    # Using softplus instead of tanh provides a non-saturating signal that grows\n    # approximately linearly for large cost differences.\n    cost_signal = F.softplus(delta_cost / temp_cost)\n\n    # 3. Create an adaptive margin based on the cost signal.\n    # The target separation in log-probs is proportional to how different the costs are.\n    # alpha scales the overall magnitude of the margin.\n    adaptive_margin = alpha * cost_signal\n\n    # 4. Compute the core loss using softplus.\n    # This is a smooth hinge loss: softplus(margin - prediction).\n    # The loss is low if delta_logp > adaptive_margin.\n    # The loss is high if delta_logp < adaptive_margin.\n    loss = F.softplus(adaptive_margin - delta_logp)\n\n    # 5. Apply optional weights and compute the mean.\n    weights = batch.get('weight', None)\n    if weights is not None:\n        loss = loss * weights\n        # Use a small epsilon to prevent division by zero if all weights are zero.\n        return loss.sum() / weights.sum().clamp(min=1e-8)\n    else:\n        return loss.mean()"}, "fitness": {"hf_like_score": 30.139460563659668, "validation_objective": 30.020992279052734, "generalization_penalty": 0.1184682846069336, "generalization_objectives": {"100": 30.139460563659668}, "train_score_mean": 38.78972768974304, "train_loss_mean": 1.3232566705942155, "pair_count": 129023928, "config": {"hf": {"problem": "tsp", "hf_steps": 1000, "train_problem_size": 100, "valid_problem_sizes": [100], "train_batch_size": 64, "pomo_size": 64, "learning_rate": 0.0003, "weight_decay": 1e-06, "alpha": 0.05, "device": "cuda", "seed": 1234, "num_validation_episodes": 128, "validation_batch_size": 64, "generalization_penalty_weight": 1.0, "pool_version": "v0"}, "free_loss": {"f1_steps": 1000, "f2_steps": 100, "f3_enabled": false}}, "loss_ir": {"name": "Softplus Adaptive Margin Loss", "intuition": "This loss function compares the model's preference (difference in log probabilities) against a target margin that adapts to the ground truth cost difference. The core idea is to use `softplus` to create a smooth, non-negative loss. The target margin for the log probability difference scales with the `softplus` of the cost difference. This means that as the cost gap between two solutions widens, the model is pushed much harder to separate them, but the scaling is smooth and avoids the saturation issues of `tanh`. The `softplus` function on the cost difference ensures the margin is always non-negative and grows approximately linearly for large cost differences.", "hyperparams": {"alpha": 1.0, "temp_cost": 1.0}, "operators_used": ["softplus"]}}, "better_than_baseline": false}
{"generation": 2, "index": 7, "ir": {"name": "Adaptive Sigmoid-Weighted Loss", "intuition": "This loss function combines an adaptive margin with a dynamic weighting scheme. The core loss is based on the Bradley-Terry model, using `logsigmoid` of the log-probability difference, which is a standard preference loss. \n\nInherited ideas:\n- From Parent 0/1: The concept of an adaptive margin, `margin = alpha * cost_signal`, is inherited. The model is pushed harder to separate pairs with larger cost differences.\n- From Parent 1: The use of `tanh` to create a bounded `cost_signal` from the cost difference (`delta_cost`) is inherited. This ensures the margin remains stable and doesn't grow infinitely.\n\nNew coupling ideas:\n1.  **Dynamic Loss Weighting:** Instead of a simple hinge-like loss `softplus(margin - delta_logp)`, this child loss introduces a dynamic weight that modulates a standard `logsigmoid` loss. This weight, `loss_weight = 1.0 + sigmoid(margin - delta_logp)`, increases the penalty when the model's preference (`delta_logp`) falls short of the target margin. When the model's preference is much lower than the margin, the sigmoid approaches 1, effectively doubling the loss's importance. When the preference meets or exceeds the margin, the sigmoid approaches 0, and the weight approaches 1, applying a standard penalty.\n2.  **Coupling Margin and Loss Form:** The adaptive margin is not used to set a hard threshold but to dynamically scale the importance of the base `logsigmoid` loss. This creates a smoother penalty landscape, where the model is always encouraged to increase `delta_logp`, but the gradient strength is amplified for pairs where the model's confidence is particularly misaligned with the ground-truth cost difference.", "pseudocode": "1. For each pair (winner, loser), calculate the cost difference `delta_cost = cost(loser) - cost(winner)` and the log probability difference `delta_logp = logp(winner) - logp(loser)`.\n2. Create a bounded 'cost signal' from the cost difference using `tanh`: `cost_signal = tanh(delta_cost / temp_cost)`.\n3. Calculate an adaptive margin target based on this signal: `margin = alpha * cost_signal`.\n4. Calculate the base preference loss using `logsigmoid`: `base_loss = -logsigmoid(delta_logp)`.\n5. Calculate a dynamic weight based on the difference between the target margin and the model's current preference: `loss_weight = 1.0 + sigmoid(margin - delta_logp)`. This weight is close to 2.0 when the model fails to meet the margin and close to 1.0 when it succeeds.\n6. The final loss for the pair is the base loss multiplied by the dynamic weight: `loss = loss_weight * base_loss`.\n7. The final loss is the weighted mean of these individual losses over the batch.", "hyperparams": {"alpha": 2.0, "temp_cost": 1.0}, "operators_used": ["logsigmoid", "sigmoid", "tanh"], "implementation_hint": {"expects": ["cost_a", "cost_b", "logp_w", "logp_l", "weight"], "returns": "scalar"}, "code": "import torch\nimport torch.nn.functional as F\n\ndef generated_loss(batch, model_output, extra):\n    \"\"\"\n    Computes the Adaptive Sigmoid-Weighted Loss.\n\n    This loss uses a standard logsigmoid preference loss, but dynamically weights\n    each sample based on how well the model's log probability difference meets an\n    adaptive margin. The margin is derived from the ground-truth cost difference.\n\n    Args:\n        batch (dict): A dictionary containing tensors from the dataloader.\n                      Expected keys:\n                      'cost_a', 'cost_b': Costs of the two solutions in a pair. Shape: [N].\n                      'log_prob_w', 'log_prob_l': Model's log probabilities for the\n                                                winner and loser solutions. Shape: [N].\n                      'weight' (optional): Per-sample loss weights. Shape: [N].\n        model_output: Not used in this loss implementation.\n        extra (dict): A dictionary for hyperparameters. Expected keys:\n                      'alpha' (float): A scaling factor for the adaptive margin.\n                      'temp_cost' (float): A temperature for normalizing the cost difference.\n\n    Returns:\n        torch.Tensor: A scalar tensor representing the mean loss over the batch.\n    \"\"\"\n    # Assume cost_w < cost_l for winner (w) and loser (l).\n    cost_w = torch.min(batch['cost_a'], batch['cost_b'])\n    cost_l = torch.max(batch['cost_a'], batch['cost_b'])\n\n    log_prob_w = batch['log_prob_w']\n    log_prob_l = batch['log_prob_l']\n\n    # Retrieve hyperparameters\n    alpha = extra.get('alpha', 2.0)\n    temp_cost = extra.get('temp_cost', 1.0)\n\n    # 1. Calculate cost and log probability differences\n    delta_cost = cost_l - cost_w\n    delta_logp = log_prob_w - log_prob_l\n\n    # 2. Inherited from Parent 1: Create a bounded 'cost signal' using tanh.\n    # This signal is in [0, 1) and represents the normalized cost gap.\n    cost_signal = torch.tanh(delta_cost / temp_cost)\n\n    # 3. Inherited from Parent 0/1: Calculate an adaptive margin target.\n    # This is the desired log probability separation for a given cost gap.\n    adaptive_margin = alpha * cost_signal\n\n    # 4. Calculate the base preference loss using logsigmoid (Bradley-Terry model).\n    # This loss is always non-negative and encourages delta_logp to be positive.\n    base_loss = -F.logsigmoid(delta_logp)\n\n    # 5. New Coupling Idea: Create a dynamic weight based on the margin error.\n    # The weight is ~2 when delta_logp << margin, and ~1 when delta_logp >= margin.\n    # This amplifies the loss for samples where the model's confidence is much lower\n    # than what the cost difference suggests it should be.\n    margin_error = adaptive_margin - delta_logp\n    loss_weight = 1.0 + torch.sigmoid(margin_error)\n\n    # 6. Apply the dynamic weight to the base loss.\n    loss = loss_weight * base_loss\n\n    # 7. Apply optional batch weights and compute the mean.\n    weights = batch.get('weight', None)\n    if weights is not None:\n        loss = loss * weights\n        # Use a small epsilon to prevent division by zero if all weights are zero.\n        return loss.sum() / weights.sum().clamp(min=1e-8)\n    else:\n        return loss.mean()"}, "fitness": {"hf_like_score": 32.54754066467285, "validation_objective": 32.54754066467285, "generalization_penalty": 0.0, "generalization_objectives": {"100": 32.211517333984375}, "train_score_mean": 40.14026142215729, "train_loss_mean": 2.325499272227287, "pair_count": 129023919, "config": {"hf": {"problem": "tsp", "hf_steps": 1000, "train_problem_size": 100, "valid_problem_sizes": [100], "train_batch_size": 64, "pomo_size": 64, "learning_rate": 0.0003, "weight_decay": 1e-06, "alpha": 0.05, "device": "cuda", "seed": 1234, "num_validation_episodes": 128, "validation_batch_size": 64, "generalization_penalty_weight": 1.0, "pool_version": "v0"}, "free_loss": {"f1_steps": 1000, "f2_steps": 100, "f3_enabled": false}}, "loss_ir": {"name": "Adaptive Sigmoid-Weighted Loss", "intuition": "This loss function combines an adaptive margin with a dynamic weighting scheme. The core loss is based on the Bradley-Terry model, using `logsigmoid` of the log-probability difference, which is a standard preference loss. \n\nInherited ideas:\n- From Parent 0/1: The concept of an adaptive margin, `margin = alpha * cost_signal`, is inherited. The model is pushed harder to separate pairs with larger cost differences.\n- From Parent 1: The use of `tanh` to create a bounded `cost_signal` from the cost difference (`delta_cost`) is inherited. This ensures the margin remains stable and doesn't grow infinitely.\n\nNew coupling ideas:\n1.  **Dynamic Loss Weighting:** Instead of a simple hinge-like loss `softplus(margin - delta_logp)`, this child loss introduces a dynamic weight that modulates a standard `logsigmoid` loss. This weight, `loss_weight = 1.0 + sigmoid(margin - delta_logp)`, increases the penalty when the model's preference (`delta_logp`) falls short of the target margin. When the model's preference is much lower than the margin, the sigmoid approaches 1, effectively doubling the loss's importance. When the preference meets or exceeds the margin, the sigmoid approaches 0, and the weight approaches 1, applying a standard penalty.\n2.  **Coupling Margin and Loss Form:** The adaptive margin is not used to set a hard threshold but to dynamically scale the importance of the base `logsigmoid` loss. This creates a smoother penalty landscape, where the model is always encouraged to increase `delta_logp`, but the gradient strength is amplified for pairs where the model's confidence is particularly misaligned with the ground-truth cost difference.", "hyperparams": {"alpha": 2.0, "temp_cost": 1.0}, "operators_used": ["logsigmoid", "sigmoid", "tanh"]}}, "better_than_baseline": false}
{"generation": 4, "index": 0, "ir": {"name": "Rank-Gap Scaled Hinge Loss", "intuition": "This loss function combines the adaptive margin concept from both parents with a new rank-based normalization scheme for stability and dynamic scaling. \n\nInherited from Parent 0 and 1:\n- The core loss structure `softplus(margin - delta_logp)` is inherited from both parents. This creates a smooth hinge loss that penalizes the model when its preference `delta_logp` is smaller than a target margin.\n- The idea of an adaptive margin, where the target separation between winner and loser log-probabilities scales with the cost difference, is also inherited from both parents.\n\nNew Coupling Ideas:\n1.  **Rank-Gap Normalization**: Instead of using a static `temp_cost` to normalize the cost difference `delta_cost`, we introduce a dynamic normalization factor. This factor is calculated as the gap between the `rank_p` percentile and `(100 - rank_p)` percentile of the `delta_cost` distribution within the current batch. This makes the margin's sensitivity robust to variations in the scale of costs across different batches and datasets. If the batch contains mostly similar pairs, the normalization factor will be small, making the margin sensitive to small cost differences. If the batch contains a wide range of cost differences, the factor will be larger, preventing the margin from exploding for outliers.\n2.  **Sigmoid Cost Signal**: The normalized cost difference is passed through a `sigmoid` function, inheriting the bounded nature of Parent 1's `tanh` signal but providing a slightly different gradient profile. This ensures the margin target is stable and bounded between 0 and `alpha`.", "pseudocode": "1. For each pair (winner, loser), calculate the cost difference `delta_cost = cost_loser - cost_winner` and the log probability difference `delta_logp = logp_winner - logp_loser`.\n2. In the current batch, calculate the `rank_p` percentile and the `(100 - rank_p)` percentile of all `delta_cost` values.\n3. Compute the `rank_gap` as the difference between these two percentile values. Add a small epsilon for stability to prevent division by zero.\n4. Normalize the `delta_cost` for each pair using this batch-specific `rank_gap`: `normalized_delta_cost = delta_cost / rank_gap`.\n5. Transform the normalized cost difference into a bounded 'cost signal' using a sigmoid function: `cost_signal = sigmoid(normalized_delta_cost)`.\n6. Create the adaptive margin by scaling the cost signal: `margin_target = alpha * cost_signal`.\n7. The final loss for each pair is the softplus of the margin minus the model's preference: `loss = softplus(margin_target - delta_logp)`.\n8. Compute the weighted mean of these individual losses over the batch.", "hyperparams": {"alpha": 2.0, "rank_p": 90.0}, "operators_used": ["softplus", "sigmoid", "rank_gap"], "implementation_hint": {"expects": ["cost_a", "cost_b", "log_prob_w", "log_prob_l", "weight"], "returns": "scalar"}, "code": "import torch\nimport torch.nn.functional as F\n\ndef generated_loss(batch, model_output, extra):\n    \"\"\"\n    Computes the Rank-Gap Scaled Hinge Loss.\n\n    This loss encourages the log probability of the winner to be higher than the loser's\n    by a margin that adapts to the cost difference. The cost difference is dynamically\n    normalized within each batch using a percentile-based rank gap, making the loss\n    robust to varying cost scales.\n\n    Args:\n        batch (dict): A dictionary containing tensors from the dataloader.\n                      Expected keys:\n                      'cost_a', 'cost_b': Costs of the two solutions. Shape: [N].\n                      'log_prob_w', 'log_prob_l': Model's log probabilities for the\n                                                winner and loser. Shape: [N].\n                      'weight' (optional): Per-sample loss weights. Shape: [N].\n        model_output: Not used in this loss implementation.\n        extra (dict): A dictionary for hyperparameters. Expected keys:\n                      'alpha' (float): The maximum margin scaling factor.\n                      'rank_p' (float): The percentile (e.g., 90.0) used to calculate the rank gap for normalization.\n\n    Returns:\n        torch.Tensor: A scalar tensor representing the mean loss over the batch.\n    \"\"\"\n    # Assume cost_w < cost_l for winner (w) and loser (l).\n    cost_w = torch.min(batch['cost_a'], batch['cost_b'])\n    cost_l = torch.max(batch['cost_a'], batch['cost_b'])\n\n    log_prob_w = batch['log_prob_w']\n    log_prob_l = batch['log_prob_l']\n\n    # Retrieve hyperparameters\n    alpha = extra.get('alpha', 2.0)\n    rank_p = extra.get('rank_p', 90.0)\n    epsilon = 1e-6\n\n    # 1. Calculate cost and log probability differences\n    delta_cost = cost_l - cost_w\n    delta_logp = log_prob_w - log_prob_l\n\n    # Detach delta_cost for percentile calculation to prevent gradients from flowing through the normalization factor.\n    delta_cost_detached = delta_cost.detach()\n\n    # 2 & 3. Compute rank-gap for dynamic, batch-wise normalization.\n    # This makes the loss robust to the overall scale of costs in a batch.\n    if delta_cost_detached.numel() > 1:\n        # Convert rank_p to a quantile q in [0, 1]\n        q_high = rank_p / 100.0\n        q_low = 1.0 - q_high\n        \n        # Use torch.quantile, which is differentiable if needed, but we detach.\n        high_percentile = torch.quantile(delta_cost_detached, q_high, interpolation='linear')\n        low_percentile = torch.quantile(delta_cost_detached, q_low, interpolation='linear')\n        \n        rank_gap = high_percentile - low_percentile\n    else:\n        # Handle single-element batch case\n        rank_gap = torch.tensor(1.0, device=delta_cost.device)\n\n    # Add epsilon for numerical stability, especially if all delta_costs are the same.\n    stable_rank_gap = rank_gap.clamp(min=epsilon)\n\n    # 4. Normalize delta_cost\n    normalized_delta_cost = delta_cost / stable_rank_gap\n\n    # 5. Transform into a bounded cost signal using sigmoid\n    cost_signal = torch.sigmoid(normalized_delta_cost)\n\n    # 6. Create the adaptive margin\n    adaptive_margin = alpha * cost_signal\n\n    # 7. Compute the core loss using softplus (smooth hinge loss)\n    loss = F.softplus(adaptive_margin - delta_logp)\n\n    # 8. Apply optional weights and compute the mean.\n    weights = batch.get('weight', None)\n    if weights is not None:\n        loss = loss * weights\n        return loss.sum() / weights.sum().clamp(min=epsilon)\n    else:\n        return loss.mean()"}, "fitness": {"hf_like_score": 46.19156074523926, "validation_objective": 45.992820739746094, "generalization_penalty": 0.19874000549316406, "generalization_objectives": {"100": 46.19156074523926}, "train_score_mean": 39.81867832660675, "train_loss_mean": 1.689635656952858, "pair_count": 129023921, "config": {"hf": {"problem": "tsp", "hf_steps": 1000, "train_problem_size": 100, "valid_problem_sizes": [100], "train_batch_size": 64, "pomo_size": 64, "learning_rate": 0.0003, "weight_decay": 1e-06, "alpha": 0.05, "device": "cuda", "seed": 1234, "num_validation_episodes": 128, "validation_batch_size": 64, "generalization_penalty_weight": 1.0, "pool_version": "v0"}, "free_loss": {"f1_steps": 1000, "f2_steps": 100, "f3_enabled": false}}, "loss_ir": {"name": "Rank-Gap Scaled Hinge Loss", "intuition": "This loss function combines the adaptive margin concept from both parents with a new rank-based normalization scheme for stability and dynamic scaling. \n\nInherited from Parent 0 and 1:\n- The core loss structure `softplus(margin - delta_logp)` is inherited from both parents. This creates a smooth hinge loss that penalizes the model when its preference `delta_logp` is smaller than a target margin.\n- The idea of an adaptive margin, where the target separation between winner and loser log-probabilities scales with the cost difference, is also inherited from both parents.\n\nNew Coupling Ideas:\n1.  **Rank-Gap Normalization**: Instead of using a static `temp_cost` to normalize the cost difference `delta_cost`, we introduce a dynamic normalization factor. This factor is calculated as the gap between the `rank_p` percentile and `(100 - rank_p)` percentile of the `delta_cost` distribution within the current batch. This makes the margin's sensitivity robust to variations in the scale of costs across different batches and datasets. If the batch contains mostly similar pairs, the normalization factor will be small, making the margin sensitive to small cost differences. If the batch contains a wide range of cost differences, the factor will be larger, preventing the margin from exploding for outliers.\n2.  **Sigmoid Cost Signal**: The normalized cost difference is passed through a `sigmoid` function, inheriting the bounded nature of Parent 1's `tanh` signal but providing a slightly different gradient profile. This ensures the margin target is stable and bounded between 0 and `alpha`.", "hyperparams": {"alpha": 2.0, "rank_p": 90.0}, "operators_used": ["softplus", "sigmoid", "rank_gap"]}}, "better_than_baseline": false}
{"generation": 4, "index": 4, "ir": {"name": "Adaptive Sigmoid Margin Loss", "intuition": "This loss function synthesizes ideas from its parents to create a stable and adaptive preference learning objective. \n\nFrom its first parent, it inherits the core loss structure: `softplus(margin - delta_logp)`. This provides a smooth, non-negative penalty when the model's preference (`delta_logp`) falls short of a target margin.\n\nFrom its second parent, it inherits the idea of using a bounded function, `tanh`, to create a normalized 'cost signal' from the cost difference (`delta_cost`). This ensures that the margin remains stable and doesn't grow infinitely, even with extreme cost differences.\n\nA new coupling idea is introduced: instead of using the `tanh`-based cost signal to define a *linear* margin (e.g., `alpha * signal`), it's used to define a *dynamic beta* for a sigmoid-like loss. The loss is formulated as `softplus(beta * (-delta_logp))`. The `beta` term, `1 + alpha * tanh(delta_cost / temp_cost)`, dynamically adjusts the steepness of the loss curve. When costs are very similar (`delta_cost` is near zero), `beta` is close to 1, resulting in a standard Bradley-Terry-like loss (`-logsigmoid(delta_logp)`). When costs are very different, `beta` increases, making the loss function steeper and pushing the model much harder to separate the log probabilities. This creates a margin implicitly, where the required `delta_logp` to achieve low loss increases as `delta_cost` grows, but does so in a stable, bounded manner.", "pseudocode": "1. For each pair (winner, loser), calculate the cost difference `delta_cost = cost_loser - cost_winner` and the log probability difference `delta_logp = logp_winner - logp_loser`.\n2. Inherit from Parent 2: Create a bounded 'cost signal' between 0 and 1 by applying `tanh` to the scaled cost difference: `cost_signal = tanh(delta_cost / temp_cost)`.\n3. New Coupling Idea: Use this cost signal to compute a dynamic scaling factor `beta`. `beta = 1 + alpha * cost_signal`. This `beta` will range from 1 (for identical costs) up to `1 + alpha`.\n4. Inherit from Parent 1: Use the `softplus` function to formulate the core loss. Combine it with the dynamic beta to create the final loss term: `loss = softplus(beta * (-delta_logp))`. This is equivalent to `-logsigmoid(beta * delta_logp)` but uses the parent operator.\n5. The final loss is the weighted mean of these individual losses over the batch.", "hyperparams": {"alpha": 3.0, "temp_cost": 1.0}, "operators_used": ["softplus", "tanh"], "implementation_hint": {"expects": ["cost_a", "cost_b", "logp_w", "logp_l", "weight"], "returns": "scalar"}, "code": "import torch\nimport torch.nn.functional as F\n\ndef generated_loss(batch, model_output, extra):\n    \"\"\"\n    Computes the Adaptive Sigmoid Margin Loss.\n\n    This loss encourages the log probability of the winner to be higher than the loser's.\n    It uses a dynamic scaling factor 'beta', derived from the tanh of the cost difference,\n    to adjust the steepness of the loss landscape. When costs are very different,\n    the loss becomes steeper, implicitly creating a larger margin requirement.\n\n    Args:\n        batch (dict): A dictionary containing tensors from the dataloader.\n                      Expected keys:\n                      'cost_a', 'cost_b': Costs of the two solutions. Shape: [N].\n                      'log_prob_w', 'log_prob_l': Model's log probabilities for the\n                                                winner and loser. Shape: [N].\n                      'weight' (optional): Per-sample loss weights. Shape: [N].\n        model_output: Not used in this loss implementation.\n        extra (dict): A dictionary for hyperparameters. Expected keys:\n                      'alpha' (float): Controls the maximum steepness of the loss.\n                      'temp_cost' (float): Temperature for normalizing the cost difference.\n\n    Returns:\n        torch.Tensor: A scalar tensor representing the mean loss over the batch.\n    \"\"\"\n    # Determine winner (w) and loser (l) based on costs.\n    # cost_w < cost_l\n    cost_w = torch.min(batch['cost_a'], batch['cost_b'])\n    cost_l = torch.max(batch['cost_a'], batch['cost_b'])\n\n    # Log probabilities are assumed to be provided for the winner and loser directly.\n    log_prob_w = batch['log_prob_w']\n    log_prob_l = batch['log_prob_l']\n\n    # Retrieve hyperparameters\n    alpha = extra.get('alpha', 3.0)\n    temp_cost = extra.get('temp_cost', 1.0)\n\n    # 1. Calculate cost and log probability differences\n    delta_cost = cost_l - cost_w\n    delta_logp = log_prob_w - log_prob_l\n\n    # 2. Create a bounded 'cost signal' from the cost difference (Inherited from Parent 2).\n    # This ensures the signal is stable, ranging from [0, 1).\n    cost_signal = torch.tanh(delta_cost / temp_cost)\n\n    # 3. New Coupling: Use the cost signal to compute a dynamic scaling factor 'beta'.\n    # beta ranges from 1 to (1 + alpha), adjusting the loss steepness.\n    # When delta_cost is 0, beta=1 (standard sigmoid loss).\n    # When delta_cost is large, beta -> 1+alpha (steeper loss, larger implicit margin).\n    beta = 1.0 + alpha * cost_signal\n    \n    # Detach beta to prevent gradients from flowing through the cost signal into model parameters,\n    # which could happen if costs were model-dependent. This is a stability trick.\n    beta = beta.detach()\n\n    # 4. Compute the core loss using softplus (Inherited from Parent 1) and the dynamic beta.\n    # This is equivalent to -logsigmoid(beta * delta_logp) but uses the specified operator.\n    # It penalizes cases where delta_logp is not sufficiently positive, with the penalty\n    # scaled by how different the ground truth costs are.\n    loss = F.softplus(-beta * delta_logp)\n\n    # 5. Apply optional weights and compute the mean.\n    weights = batch.get('weight', None)\n    if weights is not None:\n        loss = loss * weights\n        return loss.sum() / weights.sum().clamp(min=1e-8)\n    else:\n        return loss.mean()"}, "fitness": {"hf_like_score": 37.07186508178711, "validation_objective": 36.734825134277344, "generalization_penalty": 0.3370399475097656, "generalization_objectives": {"100": 37.07186508178711}, "train_score_mean": 37.25770998477936, "train_loss_mean": 1.3505465895533562, "pair_count": 129023929, "config": {"hf": {"problem": "tsp", "hf_steps": 1000, "train_problem_size": 100, "valid_problem_sizes": [100], "train_batch_size": 64, "pomo_size": 64, "learning_rate": 0.0003, "weight_decay": 1e-06, "alpha": 0.05, "device": "cuda", "seed": 1234, "num_validation_episodes": 128, "validation_batch_size": 64, "generalization_penalty_weight": 1.0, "pool_version": "v0"}, "free_loss": {"f1_steps": 1000, "f2_steps": 100, "f3_enabled": false}}, "loss_ir": {"name": "Adaptive Sigmoid Margin Loss", "intuition": "This loss function synthesizes ideas from its parents to create a stable and adaptive preference learning objective. \n\nFrom its first parent, it inherits the core loss structure: `softplus(margin - delta_logp)`. This provides a smooth, non-negative penalty when the model's preference (`delta_logp`) falls short of a target margin.\n\nFrom its second parent, it inherits the idea of using a bounded function, `tanh`, to create a normalized 'cost signal' from the cost difference (`delta_cost`). This ensures that the margin remains stable and doesn't grow infinitely, even with extreme cost differences.\n\nA new coupling idea is introduced: instead of using the `tanh`-based cost signal to define a *linear* margin (e.g., `alpha * signal`), it's used to define a *dynamic beta* for a sigmoid-like loss. The loss is formulated as `softplus(beta * (-delta_logp))`. The `beta` term, `1 + alpha * tanh(delta_cost / temp_cost)`, dynamically adjusts the steepness of the loss curve. When costs are very similar (`delta_cost` is near zero), `beta` is close to 1, resulting in a standard Bradley-Terry-like loss (`-logsigmoid(delta_logp)`). When costs are very different, `beta` increases, making the loss function steeper and pushing the model much harder to separate the log probabilities. This creates a margin implicitly, where the required `delta_logp` to achieve low loss increases as `delta_cost` grows, but does so in a stable, bounded manner.", "hyperparams": {"alpha": 3.0, "temp_cost": 1.0}, "operators_used": ["softplus", "tanh"]}}, "better_than_baseline": false}
{"generation": 5, "index": 1, "ir": {"name": "Adaptive Margin Loss with Z-Score Normalization and Rank Gap", "intuition": "This loss function synthesizes ideas from its parents to create a robust, adaptive preference loss. \n\nInherited Ideas:\n- From both parents, it inherits the core structure of an adaptive margin loss: `loss = softplus(margin - delta_logp)`. This smooth hinge loss penalizes the model when its log-probability difference (`delta_logp`) for a preferred solution (`w`) over a less-preferred one (`l`) falls short of a target margin.\n- From one parent, it inherits the use of `tanh` to create a bounded 'cost signal' from the cost difference (`delta_cost`). This prevents extremely large cost differences from creating excessively large, unstable loss values.\n\nNew Coupling Ideas:\n1.  **Z-Score Normalization of Costs**: Before calculating the cost signal, the cost differences (`delta_cost`) for the entire batch are standardized using Z-score normalization. This makes the `temp_cost` hyperparameter less sensitive to the absolute scale of costs in the dataset. The normalized costs are then clamped to prevent outliers from dominating the `tanh` input.\n2.  **Rank Gap Modulation**: The final loss is multiplied by a 'rank gap' term, `rank_gap = 1.0 - exp(-beta * |rank(cost_l) - rank(cost_w)|)`. This term, inspired by ranking losses, dynamically increases the loss weight for pairs that are far apart in the batch's cost-ranking. This encourages the model to prioritize learning the relative ordering of significantly different solutions over those that are already close in cost, adding a new layer of supervision based on the global ranking within the batch.", "pseudocode": "1. For each pair (w, l) in the batch, where cost(w) < cost(l), calculate the cost difference `delta_cost = cost(l) - cost(w)` and the log probability difference `delta_logp = logp(w) - logp(l)`.\n2. Standardize the `delta_cost` vector for the entire batch using Z-score normalization to get `delta_cost_norm`. This makes the process robust to the scale of costs.\n3. Clamp the normalized cost differences to a reasonable range (e.g., [-3, 3]) to mitigate the effect of extreme outliers.\n4. Create a bounded 'cost signal' by applying a `tanh` function to the clamped, normalized cost differences: `cost_signal = tanh(delta_cost_norm / temp_cost)`.\n5. Calculate the adaptive margin based on this signal: `margin = alpha * cost_signal`.\n6. Compute the base per-sample loss using a softplus function: `base_loss = softplus(margin - delta_logp)`.\n7. Calculate a 'rank gap' weight for each pair. First, find the rank of `cost_w` and `cost_l` within the sorted costs of the entire batch. Then compute `rank_gap_weight = 1.0 - exp(-beta * abs(rank(cost_l) - rank(cost_w)))`.\n8. Modulate the base loss with this rank gap weight: `final_loss = base_loss * rank_gap_weight`.\n9. Return the weighted mean of `final_loss` over the batch.", "hyperparams": {"alpha": 1.5, "temp_cost": 1.0, "beta": 0.1, "z_clamp": 3.0}, "operators_used": ["softplus", "tanh", "zscore", "clamp", "exp", "rank_gap"], "implementation_hint": {"expects": ["cost_a", "cost_b", "logp_a", "logp_b"], "returns": "scalar"}, "code": "import torch\nimport torch.nn.functional as F\n\ndef generated_loss(batch, model_output, extra):\n    \"\"\"\n    Computes the Adaptive Margin Loss with Z-Score Normalization and Rank Gap.\n\n    Inherits the softplus(margin - delta_logp) structure and the use of tanh for a bounded cost signal.\n    Introduces two new ideas:\n    1. Z-score normalization of cost differences across the batch for scale invariance.\n    2. A 'rank gap' modulator that up-weights the loss for pairs that are far apart in the batch's cost ranking.\n    \"\"\"\n    # For preference learning, we assume the dataset provides pairs where one is strictly better.\n    cost_w = torch.min(batch['cost_a'], batch['cost_b'])\n    cost_l = torch.max(batch['cost_a'], batch['cost_b'])\n    log_prob_w = batch['log_prob_w']\n    log_prob_l = batch['log_prob_l']\n\n    # Retrieve hyperparameters\n    alpha = extra.get('alpha', 1.5)\n    temp_cost = extra.get('temp_cost', 1.0)\n    beta = extra.get('beta', 0.1)\n    z_clamp = extra.get('z_clamp', 3.0)\n\n    # 1. Calculate cost and log probability differences\n    delta_cost = cost_l - cost_w\n    delta_logp = log_prob_w - log_prob_l\n\n    # Early exit for a batch with no cost difference to avoid division by zero in z-score\n    if torch.all(delta_cost == 0):\n        return torch.tensor(0.0, device=delta_cost.device)\n\n    # 2. New Coupling: Z-score normalization of cost differences for batch-wise scale invariance\n    delta_cost_mean = delta_cost.mean()\n    delta_cost_std = delta_cost.std().clamp(min=1e-8) # Avoid division by zero\n    delta_cost_norm = (delta_cost - delta_cost_mean) / delta_cost_std\n\n    # 3. New Coupling: Clamp to prevent extreme outliers from creating huge signals\n    delta_cost_clamped = torch.clamp(delta_cost_norm, -z_clamp, z_clamp)\n\n    # 4. Inherited: Create a bounded cost signal using tanh\n    cost_signal = torch.tanh(delta_cost_clamped / temp_cost)\n\n    # 5. Inherited: Calculate the adaptive margin\n    adaptive_margin = alpha * cost_signal\n\n    # 6. Inherited: Compute the base per-sample loss using softplus\n    base_loss = F.softplus(adaptive_margin - delta_logp)\n\n    # 7. New Coupling: Calculate rank gap modulation\n    # This requires ranking all costs in the batch\n    all_costs = torch.cat([cost_w, cost_l], dim=0)\n    sorted_costs, _ = torch.sort(all_costs)\n    \n    # Find the rank of each cost_w and cost_l in the sorted list\n    # Note: `torch.searchsorted` is an efficient way to find ranks\n    rank_w = torch.searchsorted(sorted_costs, cost_w)\n    rank_l = torch.searchsorted(sorted_costs, cost_l)\n    \n    rank_diff = (rank_l - rank_w).float()\n    \n    # The rank_gap_weight approaches 1 as rank_diff increases, and is 0 if ranks are the same.\n    rank_gap_weight = 1.0 - torch.exp(-beta * rank_diff)\n\n    # 8. Modulate the base loss with the rank gap weight\n    final_loss = base_loss * rank_gap_weight.detach() # Detach to treat it as a pure weighting factor\n\n    # 9. Apply optional weights and compute the mean.\n    weights = batch.get('weight', None)\n    if weights is not None:\n        final_loss = final_loss * weights\n        return final_loss.sum() / weights.sum().clamp(min=1e-8)\n    else:\n        return final_loss.mean()"}, "fitness": {"hf_like_score": 29.937588691711426, "validation_objective": 29.932374954223633, "generalization_penalty": 0.005213737487792969, "generalization_objectives": {"100": 29.937588691711426}, "train_score_mean": 40.39283141517639, "train_loss_mean": 1.778978935956955, "pair_count": 129023918, "config": {"hf": {"problem": "tsp", "hf_steps": 1000, "train_problem_size": 100, "valid_problem_sizes": [100], "train_batch_size": 64, "pomo_size": 64, "learning_rate": 0.0003, "weight_decay": 1e-06, "alpha": 0.05, "device": "cuda", "seed": 1234, "num_validation_episodes": 128, "validation_batch_size": 64, "generalization_penalty_weight": 1.0, "pool_version": "v0"}, "free_loss": {"f1_steps": 1000, "f2_steps": 100, "f3_enabled": false}}, "loss_ir": {"name": "Adaptive Margin Loss with Z-Score Normalization and Rank Gap", "intuition": "This loss function synthesizes ideas from its parents to create a robust, adaptive preference loss. \n\nInherited Ideas:\n- From both parents, it inherits the core structure of an adaptive margin loss: `loss = softplus(margin - delta_logp)`. This smooth hinge loss penalizes the model when its log-probability difference (`delta_logp`) for a preferred solution (`w`) over a less-preferred one (`l`) falls short of a target margin.\n- From one parent, it inherits the use of `tanh` to create a bounded 'cost signal' from the cost difference (`delta_cost`). This prevents extremely large cost differences from creating excessively large, unstable loss values.\n\nNew Coupling Ideas:\n1.  **Z-Score Normalization of Costs**: Before calculating the cost signal, the cost differences (`delta_cost`) for the entire batch are standardized using Z-score normalization. This makes the `temp_cost` hyperparameter less sensitive to the absolute scale of costs in the dataset. The normalized costs are then clamped to prevent outliers from dominating the `tanh` input.\n2.  **Rank Gap Modulation**: The final loss is multiplied by a 'rank gap' term, `rank_gap = 1.0 - exp(-beta * |rank(cost_l) - rank(cost_w)|)`. This term, inspired by ranking losses, dynamically increases the loss weight for pairs that are far apart in the batch's cost-ranking. This encourages the model to prioritize learning the relative ordering of significantly different solutions over those that are already close in cost, adding a new layer of supervision based on the global ranking within the batch.", "hyperparams": {"alpha": 1.5, "temp_cost": 1.0, "beta": 0.1, "z_clamp": 3.0}, "operators_used": ["softplus", "tanh", "zscore", "clamp", "exp", "rank_gap"]}}, "better_than_baseline": false}
{"generation": 5, "index": 4, "ir": {"name": "Adaptive Margin LogSigmoid Loss with Cost Z-Scoring", "intuition": "This loss function combines the core structure of an adaptive margin loss with robust normalization and a classic sigmoid-based preference formulation. \n\nInherited ideas:\n- From Parent 0 and 1, it inherits the concept of an **adaptive margin**, where the target separation between the winner's and loser's log-probabilities (`delta_logp`) scales with the difference in their costs (`delta_cost`). This ensures that the model is pushed harder to distinguish between pairs with a large quality gap.\n- From Parent 0, it inherits the use of `softplus` to create a **non-saturating cost signal**. This allows the margin to grow approximately linearly for large cost differences, preventing the learning signal from vanishing for easy examples.\n\nNew coupling ideas:\n1. **Z-Score Normalization**: Instead of simple temperature scaling, the `delta_cost` for the entire batch is normalized using z-scoring (`(x - mean) / std`). This makes the margin's scale invariant to the distribution of cost differences in a batch, improving stability and reducing sensitivity to the `temp_cost` hyperparameter. The normalized costs are then clamped to prevent extreme outliers from dominating the loss.\n2. **LogSigmoid Formulation**: The final loss is computed as `-logsigmoid(delta_logp - adaptive_margin)`. This is a common and probabilistically interpretable formulation for preference learning, equivalent to `softplus(adaptive_margin - delta_logp)` but framed differently. It directly maximizes the log-probability of the model's preference `delta_logp` being greater than the target `adaptive_margin`.", "pseudocode": "1. For each pair (winner, loser), calculate the cost difference `delta_cost = cost_loser - cost_winner` and the log probability difference `delta_logp = logp_winner - logp_loser`.\n2. Across the entire batch, compute the mean and standard deviation of `delta_cost`.\n3. Normalize the `delta_cost` values using z-scoring: `norm_delta_cost = (delta_cost - mean) / (std + epsilon)`.\n4. Clamp the `norm_delta_cost` to a reasonable range (e.g., [-3, 3]) to mitigate the effect of extreme outliers.\n5. Create a non-saturating cost signal by applying a softplus function to the clamped, normalized cost difference: `cost_signal = softplus(norm_delta_cost / temp_cost)`.\n6. Calculate the adaptive margin by scaling the cost signal: `adaptive_margin = alpha * cost_signal`.\n7. The loss for each pair is computed using a logsigmoid formulation: `loss = -logsigmoid(delta_logp - adaptive_margin)`.\n8. The final loss is the weighted mean of these individual losses over the batch.", "hyperparams": {"alpha": 1.0, "temp_cost": 1.0, "clamp_range": 3.0}, "operators_used": ["softplus", "logsigmoid", "zscore", "clamp"], "implementation_hint": {"expects": ["cost_a", "cost_b", "logp_chosen", "logp_rejected", "weight"], "returns": "scalar"}, "code": "import torch\nimport torch.nn.functional as F\n\ndef generated_loss(batch, model_output, extra):\n    \"\"\"\n    Computes an adaptive margin loss using z-score normalization for costs and a logsigmoid formulation.\n\n    Inherits the adaptive margin concept from parents, where the target log-probability gap\n    scales with the cost difference. It uses a non-saturating softplus function (like Parent 0)\n    to create the cost signal. A new coupling idea is to normalize the batch-wise cost\n    differences using z-scoring before applying the softplus, making the margin robust to\n    variations in cost scales. The final loss uses the standard logsigmoid preference loss structure.\n\n    Args:\n        batch (dict): A dictionary containing tensors from the dataloader.\n                      Expected keys:\n                      'cost_a', 'cost_b': Costs of the two solutions in a pair. Shape: [N].\n                      'log_prob_w', 'log_prob_l': Model's log probabilities for the\n                                                winner and loser solutions. Shape: [N].\n                      'weight' (optional): Per-sample loss weights. Shape: [N].\n        model_output: Not used in this loss implementation.\n        extra (dict): A dictionary for hyperparameters. Expected keys:\n                      'alpha' (float): Scaling factor for the adaptive margin.\n                      'temp_cost' (float): Temperature for scaling the z-scored cost difference.\n                      'clamp_range' (float): The range (+/-) to clamp z-scored costs.\n\n    Returns:\n        torch.Tensor: A scalar tensor representing the mean loss over the batch.\n    \"\"\"\n    # Assume cost_w < cost_l\n    cost_w = torch.min(batch['cost_a'], batch['cost_b'])\n    cost_l = torch.max(batch['cost_a'], batch['cost_b'])\n    log_prob_w = batch['log_prob_w']\n    log_prob_l = batch['log_prob_l']\n\n    # Retrieve hyperparameters\n    alpha = extra.get('alpha', 1.0)\n    temp_cost = extra.get('temp_cost', 1.0)\n    clamp_range = extra.get('clamp_range', 3.0)\n    epsilon = 1e-8\n\n    # 1. Calculate cost and log probability differences\n    delta_cost = cost_l - cost_w\n    delta_logp = log_prob_w - log_prob_l\n\n    # 2. (New Idea) Z-Score normalize delta_cost across the batch for stability\n    if delta_cost.numel() > 1:\n        cost_mean = delta_cost.mean()\n        cost_std = delta_cost.std()\n        # The 'zscore' operator is implemented here\n        norm_delta_cost = (delta_cost - cost_mean) / (cost_std + epsilon)\n    else:\n        # Handle batch size of 1 to avoid std=0 -> NaN\n        norm_delta_cost = torch.zeros_like(delta_cost)\n\n    # 3. (New Idea) Clamp to prevent outliers from creating extreme margins\n    clamped_norm_cost = torch.clamp(norm_delta_cost, -clamp_range, clamp_range)\n\n    # 4. (Inherited from Parent 0) Create a non-saturating cost signal with softplus\n    cost_signal = F.softplus(clamped_norm_cost / temp_cost)\n\n    # 5. (Inherited from Parents) Create an adaptive margin\n    adaptive_margin = alpha * cost_signal\n\n    # 6. (New Idea) Compute loss using logsigmoid formulation\n    # This is equivalent to softplus(adaptive_margin - delta_logp)\n    loss = -F.logsigmoid(delta_logp - adaptive_margin)\n\n    # 7. Apply optional weights and compute the mean.\n    weights = batch.get('weight', None)\n    if weights is not None:\n        loss = loss * weights\n        return loss.sum() / weights.sum().clamp(min=epsilon)\n    else:\n        return loss.mean()"}, "fitness": {"hf_like_score": 47.15515899658203, "validation_objective": 47.15515899658203, "generalization_penalty": 0.0, "generalization_objectives": {"100": 47.06333541870117}, "train_score_mean": 39.27178464412689, "train_loss_mean": 1.187030460715294, "pair_count": 129023931, "config": {"hf": {"problem": "tsp", "hf_steps": 1000, "train_problem_size": 100, "valid_problem_sizes": [100], "train_batch_size": 64, "pomo_size": 64, "learning_rate": 0.0003, "weight_decay": 1e-06, "alpha": 0.05, "device": "cuda", "seed": 1234, "num_validation_episodes": 128, "validation_batch_size": 64, "generalization_penalty_weight": 1.0, "pool_version": "v0"}, "free_loss": {"f1_steps": 1000, "f2_steps": 100, "f3_enabled": false}}, "loss_ir": {"name": "Adaptive Margin LogSigmoid Loss with Cost Z-Scoring", "intuition": "This loss function combines the core structure of an adaptive margin loss with robust normalization and a classic sigmoid-based preference formulation. \n\nInherited ideas:\n- From Parent 0 and 1, it inherits the concept of an **adaptive margin**, where the target separation between the winner's and loser's log-probabilities (`delta_logp`) scales with the difference in their costs (`delta_cost`). This ensures that the model is pushed harder to distinguish between pairs with a large quality gap.\n- From Parent 0, it inherits the use of `softplus` to create a **non-saturating cost signal**. This allows the margin to grow approximately linearly for large cost differences, preventing the learning signal from vanishing for easy examples.\n\nNew coupling ideas:\n1. **Z-Score Normalization**: Instead of simple temperature scaling, the `delta_cost` for the entire batch is normalized using z-scoring (`(x - mean) / std`). This makes the margin's scale invariant to the distribution of cost differences in a batch, improving stability and reducing sensitivity to the `temp_cost` hyperparameter. The normalized costs are then clamped to prevent extreme outliers from dominating the loss.\n2. **LogSigmoid Formulation**: The final loss is computed as `-logsigmoid(delta_logp - adaptive_margin)`. This is a common and probabilistically interpretable formulation for preference learning, equivalent to `softplus(adaptive_margin - delta_logp)` but framed differently. It directly maximizes the log-probability of the model's preference `delta_logp` being greater than the target `adaptive_margin`.", "hyperparams": {"alpha": 1.0, "temp_cost": 1.0, "clamp_range": 3.0}, "operators_used": ["softplus", "logsigmoid", "zscore", "clamp"]}}, "better_than_baseline": false}
{"generation": 5, "index": 6, "ir": {"name": "Adaptive Margin Loss with Z-Score Normalization", "intuition": "This loss function synergizes two adaptive margin concepts from its parents and introduces a batch-level normalization for enhanced stability and adaptivity.\n\nInherited ideas:\n1.  **From Parent 0 (Softplus Margin):** It inherits the use of `softplus` to create an unbounded, non-negative target margin from the cost difference (`delta_cost`). This ensures that pairs with larger cost gaps receive a proportionally larger target separation, pushing the model harder where the preference is clearest.\n2.  **From Parent 1 (Tanh Margin):** It inherits the use of `tanh` to create a bounded, saturating signal from the cost difference. This is used not for the margin itself, but to dynamically scale the loss, effectively down-weighting the penalty for pairs with extremely large cost differences where the preference is likely already obvious.\n\nNew coupling ideas:\n1.  **Z-Score Normalization:** Instead of relying solely on a fixed temperature `temp_cost`, this loss first normalizes the `delta_cost` across the batch using z-scoring (`(x - mean) / std`). This makes the margin calculation robust to variations in the scale and distribution of cost differences between batches, leading to more stable training. A temperature is still used after z-scoring to control the sensitivity.\n2.  **Dynamic Loss Scaling:** The final loss term is scaled by `(1 - tanh(normalized_delta_cost))`. This acts as a dynamic weight, reducing the loss contribution from pairs with very large cost differences (where `tanh` approaches 1) and focusing the training on more ambiguous pairs with smaller cost gaps.", "pseudocode": "1. For each pair (winner, loser), calculate the cost difference `delta_cost = cost_loser - cost_winner` and the log probability difference `delta_logp = logp_winner - logp_loser`.\n2. Across the entire batch, compute the mean and standard deviation of `delta_cost`.\n3. Normalize `delta_cost` for each pair using z-scoring: `normalized_delta_cost = (delta_cost - mean) / (std + epsilon)`.\n4. Inherit from Parent 0: Calculate an unbounded adaptive margin using the `softplus` function on the normalized cost difference: `margin = alpha * softplus(normalized_delta_cost / temp_cost)`.\n5. Inherit from Parent 1: Calculate a bounded cost signal using the `tanh` function: `cost_signal = tanh(normalized_delta_cost / temp_cost)`.\n6. Compute the core preference violation loss as `softplus(margin - delta_logp)`.\n7. Introduce a new coupling: Dynamically scale the loss using the tanh-based signal. The scaling factor `(1 - cost_signal)` reduces the loss for pairs with very large cost differences, focusing the model on harder examples. The final per-sample loss is `loss = (1 - cost_signal) * softplus(margin - delta_logp)`.\n8. Compute the weighted mean of the per-sample losses over the batch.", "hyperparams": {"alpha": 1.0, "temp_cost": 2.0, "z_score_eps": 1e-06}, "operators_used": ["softplus", "tanh", "zscore"], "implementation_hint": {"expects": ["cost_a", "cost_b", "logp_w", "logp_l", "weight"], "returns": "scalar"}, "code": "import torch\nimport torch.nn.functional as F\n\ndef generated_loss(batch, model_output, extra):\n    \"\"\"\n    Computes an Adaptive Margin Loss with Z-Score Normalization and Dynamic Scaling.\n\n    This loss encourages the log probability of the winner to be higher than the loser's\n    by an adaptive margin. The margin is derived from the z-score normalized cost\n    difference, making it robust to varying cost scales across batches. The final loss\n    is dynamically scaled to de-emphasize pairs with extremely large cost differences.\n\n    Args:\n        batch (dict): A dictionary containing tensors from the dataloader.\n                      Expected keys:\n                      'cost_a', 'cost_b': Costs of the two solutions. Shape: [N].\n                      'log_prob_w', 'log_prob_l': Model's log probabilities for the\n                                                winner and loser. Shape: [N].\n                      'weight' (optional): Per-sample loss weights. Shape: [N].\n        model_output: Not used in this loss implementation.\n        extra (dict): A dictionary for hyperparameters. Expected keys:\n                      'alpha' (float): Scaling factor for the adaptive margin.\n                      'temp_cost' (float): Temperature for scaling normalized costs.\n                      'z_score_eps' (float): Epsilon for stable z-score normalization.\n\n    Returns:\n        torch.Tensor: A scalar tensor representing the mean loss over the batch.\n    \"\"\"\n    # Identify winner (w) and loser (l) costs and log probabilities\n    cost_w = torch.min(batch['cost_a'], batch['cost_b'])\n    cost_l = torch.max(batch['cost_a'], batch['cost_b'])\n    log_prob_w = batch['log_prob_w']\n    log_prob_l = batch['log_prob_l']\n\n    # Retrieve hyperparameters\n    alpha = extra.get('alpha', 1.0)\n    temp_cost = extra.get('temp_cost', 2.0)\n    z_score_eps = extra.get('z_score_eps', 1e-6)\n\n    # 1. Calculate cost and log probability differences\n    delta_cost = cost_l - cost_w\n    delta_logp = log_prob_w - log_prob_l\n\n    # 2. & 3. New Coupling: Z-score normalization of delta_cost across the batch\n    # This makes the loss invariant to the absolute scale of costs in a batch.\n    if delta_cost.numel() > 1:\n        cost_mean = delta_cost.mean()\n        cost_std = delta_cost.std()\n        normalized_delta_cost = (delta_cost - cost_mean) / (cost_std + z_score_eps)\n    else:\n        # Handle batch size of 1 to avoid NaN from std=0\n        normalized_delta_cost = torch.zeros_like(delta_cost)\n\n    # Scale by temperature after normalization\n    scaled_norm_cost = normalized_delta_cost / temp_cost\n\n    # 4. Inherit from Parent 0: Unbounded margin via softplus\n    # The margin is now based on the relative cost difference within the batch.\n    adaptive_margin = alpha * F.softplus(scaled_norm_cost)\n\n    # 5. Inherit from Parent 1: Bounded signal via tanh\n    # This will be used for dynamic loss scaling.\n    cost_signal_tanh = torch.tanh(scaled_norm_cost)\n\n    # 6. Compute the core preference violation loss (smooth hinge loss)\n    core_loss = F.softplus(adaptive_margin - delta_logp)\n\n    # 7. New Coupling: Dynamic loss scaling\n    # The factor (1 - tanh) is close to 1 for small cost differences and\n    # approaches 0 for very large cost differences, focusing training on harder examples.\n    # Clamp is used for stability, though tanh is bounded.\n    loss_scale_factor = 1.0 - cost_signal_tanh.clamp(min=0.0, max=1.0)\n    loss = loss_scale_factor * core_loss\n\n    # 8. Apply optional weights and compute the mean.\n    weights = batch.get('weight', None)\n    if weights is not None:\n        loss = loss * weights\n        # Use a small epsilon to prevent division by zero if all weights are zero.\n        return loss.sum() / weights.sum().clamp(min=1e-8)\n    else:\n        return loss.mean()"}, "fitness": {"hf_like_score": 31.792738914489746, "validation_objective": 31.792738914489746, "generalization_penalty": 0.0, "generalization_objectives": {"100": 31.22359848022461}, "train_score_mean": 39.826229808807376, "train_loss_mean": 1.142056388735771, "pair_count": 129023927, "config": {"hf": {"problem": "tsp", "hf_steps": 1000, "train_problem_size": 100, "valid_problem_sizes": [100], "train_batch_size": 64, "pomo_size": 64, "learning_rate": 0.0003, "weight_decay": 1e-06, "alpha": 0.05, "device": "cuda", "seed": 1234, "num_validation_episodes": 128, "validation_batch_size": 64, "generalization_penalty_weight": 1.0, "pool_version": "v0"}, "free_loss": {"f1_steps": 1000, "f2_steps": 100, "f3_enabled": false}}, "loss_ir": {"name": "Adaptive Margin Loss with Z-Score Normalization", "intuition": "This loss function synergizes two adaptive margin concepts from its parents and introduces a batch-level normalization for enhanced stability and adaptivity.\n\nInherited ideas:\n1.  **From Parent 0 (Softplus Margin):** It inherits the use of `softplus` to create an unbounded, non-negative target margin from the cost difference (`delta_cost`). This ensures that pairs with larger cost gaps receive a proportionally larger target separation, pushing the model harder where the preference is clearest.\n2.  **From Parent 1 (Tanh Margin):** It inherits the use of `tanh` to create a bounded, saturating signal from the cost difference. This is used not for the margin itself, but to dynamically scale the loss, effectively down-weighting the penalty for pairs with extremely large cost differences where the preference is likely already obvious.\n\nNew coupling ideas:\n1.  **Z-Score Normalization:** Instead of relying solely on a fixed temperature `temp_cost`, this loss first normalizes the `delta_cost` across the batch using z-scoring (`(x - mean) / std`). This makes the margin calculation robust to variations in the scale and distribution of cost differences between batches, leading to more stable training. A temperature is still used after z-scoring to control the sensitivity.\n2.  **Dynamic Loss Scaling:** The final loss term is scaled by `(1 - tanh(normalized_delta_cost))`. This acts as a dynamic weight, reducing the loss contribution from pairs with very large cost differences (where `tanh` approaches 1) and focusing the training on more ambiguous pairs with smaller cost gaps.", "hyperparams": {"alpha": 1.0, "temp_cost": 2.0, "z_score_eps": 1e-06}, "operators_used": ["softplus", "tanh", "zscore"]}}, "better_than_baseline": false}
{"generation": 6, "index": 0, "ir": {"name": "Softplus Margin with Z-Score Normalization and Log-Sigmoid Preference", "intuition": "This loss function creates a robust, adaptive preference loss by combining features from its parents with a new stability-focused coupling.\n\nInherited Ideas:\n- From Parent 0, it inherits the use of Z-score normalization for the batch-wise cost differences (`delta_cost`). This makes the margin calculation robust to the absolute scale of costs in a given batch, improving generalization across different problems.\n- From Parent 1, it inherits the use of `softplus` to transform the (now normalized) cost difference into a non-saturating, non-negative margin. Unlike `tanh`, this allows the target margin to grow for pairs with very large cost differences, pushing the model harder to distinguish them.\n\nNew Coupling Ideas:\n1.  **Log-Sigmoid Preference Term**: Instead of using a raw log-probability difference (`delta_logp`), the loss is formulated using `logsigmoid(delta_logp)`. This is a common technique from Bradley-Terry models and DPO-style losses. It reframes the objective as maximizing the log-likelihood of the preferred choice. This term is inherently bounded between (-inf, 0), which improves numerical stability and prevents the model from being overly penalized for extremely confident (but correct) predictions where `delta_logp` might become very large.\n2.  **Dynamic Temperature Scaling**: The temperature parameter `temp_cost`, which scales the normalized cost difference before the `softplus` function, is made dynamic. It is set to the standard deviation of the batch's `delta_logp` values. This couples the margin's sensitivity to the model's current output distribution. If the model's log-probability differences are very spread out (high variance), the temperature increases, softening the margin and preventing excessively large loss values. Conversely, if the model is uncertain (low variance in `delta_logp`), the temperature decreases, creating a sharper, more demanding margin.", "pseudocode": "1. For each pair (w, l) in the batch, where cost(w) < cost(l), calculate the cost difference `delta_cost = cost(l) - cost(w)` and the log probability difference `delta_logp = logp(w) - logp(l)`.\n2. Inherited from Parent 0: Standardize the `delta_cost` vector for the entire batch using Z-score normalization to get `delta_cost_norm`. This makes the margin robust to the scale of costs.\n3. New Coupling: Calculate a dynamic temperature `temp_cost` as the standard deviation of the `delta_logp` values across the batch. Clamp it to a minimum value for stability.\n4. Inherited from Parent 1: Create a non-saturating, adaptive margin by applying a `softplus` function to the scaled, normalized cost difference: `margin = alpha * softplus(delta_cost_norm / temp_cost)`.\n5. New Coupling: Calculate the model's preference term using `logsigmoid(delta_logp)`. This bounds the term and improves stability.\n6. The final loss for each sample is `- (preference_term - margin)`. This encourages the log-sigmoid of the preference to be high, while penalizing it by an amount proportional to the adaptive margin. A larger margin acts as a regularizer, pushing the model to be more confident in its preference.\n7. Return the weighted mean of the loss over the batch.", "hyperparams": {"alpha": 0.1, "min_temp": 0.1}, "operators_used": ["zscore", "softplus", "logsigmoid", "clamp"], "implementation_hint": {"expects": ["cost_a", "cost_b", "logp_a", "logp_b"], "returns": "scalar"}, "code": "import torch\nimport torch.nn.functional as F\n\ndef generated_loss(batch, model_output, extra):\n    \"\"\"\n    Computes the Softplus Margin with Z-Score Normalization and Log-Sigmoid Preference.\n\n    Inherited Ideas:\n    - Z-score normalization of cost differences for scale invariance (from Parent 0).\n    - Softplus transformation to create a non-saturating margin (from Parent 1).\n\n    New Coupling Ideas:\n    1. Log-sigmoid on the log-probability difference for a bounded, stable preference term.\n    2. Dynamic temperature `temp_cost` based on the standard deviation of `delta_logp`,\n       coupling the margin's steepness to the model's current output variance.\n    \"\"\"\n    # For preference learning, we assume the dataset provides pairs where one is strictly better.\n    cost_w = torch.min(batch['cost_a'], batch['cost_b'])\n    cost_l = torch.max(batch['cost_a'], batch['cost_b'])\n    log_prob_w = batch['log_prob_w']\n    log_prob_l = batch['log_prob_l']\n\n    # Retrieve hyperparameters\n    alpha = extra.get('alpha', 0.1)\n    min_temp = extra.get('min_temp', 0.1)\n\n    # 1. Calculate cost and log probability differences\n    delta_cost = cost_l - cost_w\n    delta_logp = log_prob_w - log_prob_l\n\n    # Early exit for a batch with no cost difference to avoid division by zero in z-score\n    if torch.all(delta_cost < 1e-8):\n        # If no cost difference, the margin should be zero.\n        # The loss is just the negative log-likelihood of the preferences.\n        loss = -F.logsigmoid(delta_logp)\n    else:\n        # 2. Inherited from Parent 0: Z-score normalization of cost differences\n        delta_cost_mean = delta_cost.mean()\n        delta_cost_std = delta_cost.std().clamp(min=1e-8) # Avoid division by zero\n        delta_cost_norm = (delta_cost - delta_cost_mean) / delta_cost_std\n\n        # 3. New Coupling: Dynamic temperature based on logp standard deviation\n        with torch.no_grad():\n            temp_cost = delta_logp.std().clamp(min=min_temp)\n\n        # 4. Inherited from Parent 1: Create a non-saturating margin using softplus\n        margin = alpha * F.softplus(delta_cost_norm / temp_cost)\n\n        # 5. New Coupling: Calculate preference term using logsigmoid for stability\n        preference_term = F.logsigmoid(delta_logp)\n\n        # 6. Combine into final loss. We want to maximize `preference_term - margin`,\n        # so we minimize its negative.\n        loss = -(preference_term - margin)\n\n    # 7. Apply optional weights and compute the mean.\n    weights = batch.get('weight', None)\n    if weights is not None:\n        loss = loss * weights\n        return loss.sum() / weights.sum().clamp(min=1e-8)\n    else:\n        return loss.mean()"}, "fitness": {"hf_like_score": 30.927617073059082, "validation_objective": 30.324942588806152, "generalization_penalty": 0.6026744842529297, "generalization_objectives": {"100": 30.927617073059082}, "train_score_mean": 38.498237545013424, "train_loss_mean": 1.1005134001970291, "pair_count": 129023916, "config": {"hf": {"problem": "tsp", "hf_steps": 1000, "train_problem_size": 100, "valid_problem_sizes": [100], "train_batch_size": 64, "pomo_size": 64, "learning_rate": 0.0003, "weight_decay": 1e-06, "alpha": 0.05, "device": "cuda", "seed": 1234, "num_validation_episodes": 128, "validation_batch_size": 64, "generalization_penalty_weight": 1.0, "pool_version": "v0"}, "free_loss": {"f1_steps": 1000, "f2_steps": 100, "f3_enabled": false}}, "loss_ir": {"name": "Softplus Margin with Z-Score Normalization and Log-Sigmoid Preference", "intuition": "This loss function creates a robust, adaptive preference loss by combining features from its parents with a new stability-focused coupling.\n\nInherited Ideas:\n- From Parent 0, it inherits the use of Z-score normalization for the batch-wise cost differences (`delta_cost`). This makes the margin calculation robust to the absolute scale of costs in a given batch, improving generalization across different problems.\n- From Parent 1, it inherits the use of `softplus` to transform the (now normalized) cost difference into a non-saturating, non-negative margin. Unlike `tanh`, this allows the target margin to grow for pairs with very large cost differences, pushing the model harder to distinguish them.\n\nNew Coupling Ideas:\n1.  **Log-Sigmoid Preference Term**: Instead of using a raw log-probability difference (`delta_logp`), the loss is formulated using `logsigmoid(delta_logp)`. This is a common technique from Bradley-Terry models and DPO-style losses. It reframes the objective as maximizing the log-likelihood of the preferred choice. This term is inherently bounded between (-inf, 0), which improves numerical stability and prevents the model from being overly penalized for extremely confident (but correct) predictions where `delta_logp` might become very large.\n2.  **Dynamic Temperature Scaling**: The temperature parameter `temp_cost`, which scales the normalized cost difference before the `softplus` function, is made dynamic. It is set to the standard deviation of the batch's `delta_logp` values. This couples the margin's sensitivity to the model's current output distribution. If the model's log-probability differences are very spread out (high variance), the temperature increases, softening the margin and preventing excessively large loss values. Conversely, if the model is uncertain (low variance in `delta_logp`), the temperature decreases, creating a sharper, more demanding margin.", "hyperparams": {"alpha": 0.1, "min_temp": 0.1}, "operators_used": ["zscore", "softplus", "logsigmoid", "clamp"]}}, "better_than_baseline": false}
{"generation": 6, "index": 1, "ir": {"name": "Rank-Modulated Bradley-Terry Loss with Z-Score Cost Signal", "intuition": "This loss function creates a hybrid between a Bradley-Terry style logistic loss and an adaptive margin loss, incorporating ideas from both parents while introducing a new coupling mechanism.\n\nInherited Ideas:\n- From Parent 1 (Softplus Adaptive Margin Loss), it inherits the core `softplus` structure for the loss calculation, specifically `softplus(margin - delta_logp)`. This provides a smooth and non-negative hinge-like loss.\n- From Parent 0 (Adaptive Margin Loss with Z-Score), it inherits the idea of using Z-score normalization on the batch-wise cost differences (`delta_cost`) to create a scale-invariant cost signal. This makes the loss less sensitive to the absolute magnitude of costs in a given batch.\n\nNew Coupling Ideas:\n1.  **Exponential Cost Signal**: Instead of using the normalized cost signal directly as a linear margin (e.g., `alpha * signal`), the child loss uses it as an exponent in a Bradley-Terry-like formulation: `loss = logsigmoid(delta_logp - exp(alpha * zscore(delta_cost)))`. This can be rewritten as `softplus(exp(alpha * zscore(delta_cost)) - delta_logp)`, connecting it to the inherited `softplus` structure. This coupling creates a margin that grows exponentially with the normalized cost difference, pushing the model much harder to separate pairs with significantly different costs, while having a gentler effect on pairs with similar costs.\n2.  **Rank Gap Modulation**: The child also inherits the rank gap modulation from Parent 0, but applies it in a novel way. The term `rank_gap = 1.0 - exp(-beta * rank_diff)` is used to modulate the strength of the exponential cost signal itself (`alpha * rank_gap_weight * zscore(delta_cost)`). This means that the exponential growth of the margin is amplified for pairs that are far apart in the batch's cost ranking. It focuses the strong exponential penalty on pairs that are not just different in cost value, but also significantly separated in their relative ordering within the batch.", "pseudocode": "1. For each pair (w, l) in the batch, where cost(w) < cost(l), calculate the cost difference `delta_cost = cost(l) - cost(w)` and the log probability difference `delta_logp = logp(w) - logp(l)`.\n2. Standardize the `delta_cost` vector for the entire batch using Z-score normalization to get `delta_cost_norm`. Clamp this to a reasonable range (e.g., [-3, 3]) to prevent outliers from creating extreme values.\n3. Calculate a 'rank gap' weight for each pair. First, find the rank of `cost_w` and `cost_l` within the sorted costs of the entire batch. Then compute `rank_gap_weight = 1.0 - exp(-beta * abs(rank(cost_l) - rank(cost(w))))`.\n4. Create a rank-modulated cost signal by multiplying the normalized cost difference by the rank gap weight: `modulated_signal = alpha * rank_gap_weight * delta_cost_norm`.\n5. Compute an exponential margin from this signal: `exp_margin = exp(modulated_signal)`. This margin grows non-linearly, aggressively penalizing large, rank-separated cost differences.\n6. Calculate the per-sample loss using a softplus function, which is equivalent to a logistic loss: `loss = softplus(exp_margin - delta_logp)`.\n7. Return the weighted mean of the loss over the batch.", "hyperparams": {"alpha": 1.0, "beta": 0.05, "z_clamp": 3.0}, "operators_used": ["softplus", "exp", "zscore", "clamp", "rank_gap"], "implementation_hint": {"expects": ["cost_a", "cost_b", "logp_w", "logp_l"], "returns": "scalar"}, "code": "import torch\nimport torch.nn.functional as F\n\ndef generated_loss(batch, model_output, extra):\n    \"\"\"\n    Computes the Rank-Modulated Bradley-Terry Loss with Z-Score Cost Signal.\n\n    Inherits:\n    - The softplus(margin - delta_logp) structure (Parent 1).\n    - Z-score normalization of cost differences for a scale-invariant signal (Parent 0).\n    \n    Introduces:\n    1. An exponential margin `exp(alpha * ...)` creating a Bradley-Terry style loss where the preference strength grows exponentially with the cost difference.\n    2. Coupling the rank gap weight (from Parent 0) directly into the exponent to modulate the strength of this exponential margin.\n    \"\"\"\n    # For preference learning, we assume the dataset provides pairs where one is strictly better.\n    cost_w = torch.min(batch['cost_a'], batch['cost_b'])\n    cost_l = torch.max(batch['cost_a'], batch['cost_b'])\n    log_prob_w = batch['log_prob_w']\n    log_prob_l = batch['log_prob_l']\n\n    # Retrieve hyperparameters\n    alpha = extra.get('alpha', 1.0)\n    beta = extra.get('beta', 0.05)\n    z_clamp = extra.get('z_clamp', 3.0)\n\n    # 1. Calculate cost and log probability differences\n    delta_cost = cost_l - cost_w\n    delta_logp = log_prob_w - log_prob_l\n\n    # Early exit for a batch with no cost difference to avoid division by zero\n    if torch.all(delta_cost < 1e-6):\n        return torch.tensor(0.0, device=delta_cost.device)\n\n    # 2. Inherited: Z-score normalization of cost differences for batch-wise scale invariance\n    delta_cost_mean = delta_cost.mean()\n    delta_cost_std = delta_cost.std().clamp(min=1e-8)\n    delta_cost_norm = (delta_cost - delta_cost_mean) / delta_cost_std\n    delta_cost_clamped = torch.clamp(delta_cost_norm, -z_clamp, z_clamp)\n\n    # 3. New Coupling Part 1: Calculate rank gap modulation weight\n    all_costs = torch.cat([cost_w, cost_l], dim=0)\n    # Use a stable sort for deterministic ranks\n    sorted_costs, _ = torch.sort(all_costs, stable=True)\n    rank_w = torch.searchsorted(sorted_costs, cost_w)\n    rank_l = torch.searchsorted(sorted_costs, cost_l)\n    rank_diff = (rank_l - rank_w).float()\n    # rank_gap_weight approaches 1 as rank_diff increases\n    rank_gap_weight = 1.0 - torch.exp(-beta * rank_diff)\n\n    # 4. New Coupling Part 2: Modulate the normalized signal with the rank gap weight\n    # This directly couples the rank information into the margin calculation\n    modulated_signal = alpha * rank_gap_weight.detach() * delta_cost_clamped\n\n    # 5. New Coupling Part 3: Compute an exponential margin\n    # This creates a Bradley-Terry-like term where preference strength grows exponentially\n    exp_margin = torch.exp(modulated_signal)\n\n    # 6. Inherited: Compute the core loss using softplus\n    # This is equivalent to -log_sigmoid(delta_logp - log(exp_margin)) but more stable\n    # The final form is softplus(margin - delta_logp)\n    per_sample_loss = F.softplus(exp_margin - delta_logp)\n\n    # 7. Apply optional weights and compute the mean.\n    weights = batch.get('weight', None)\n    if weights is not None:\n        per_sample_loss = per_sample_loss * weights\n        return per_sample_loss.sum() / weights.sum().clamp(min=1e-8)\n    else:\n        return per_sample_loss.mean()"}, "fitness": {"hf_like_score": 38.69381332397461, "validation_objective": 38.58881187438965, "generalization_penalty": 0.10500144958496094, "generalization_objectives": {"100": 38.69381332397461}, "train_score_mean": 40.156808547019956, "train_loss_mean": 1.69420962870121, "pair_count": 129023928, "config": {"hf": {"problem": "tsp", "hf_steps": 1000, "train_problem_size": 100, "valid_problem_sizes": [100], "train_batch_size": 64, "pomo_size": 64, "learning_rate": 0.0003, "weight_decay": 1e-06, "alpha": 0.05, "device": "cuda", "seed": 1234, "num_validation_episodes": 128, "validation_batch_size": 64, "generalization_penalty_weight": 1.0, "pool_version": "v0"}, "free_loss": {"f1_steps": 1000, "f2_steps": 100, "f3_enabled": false}}, "loss_ir": {"name": "Rank-Modulated Bradley-Terry Loss with Z-Score Cost Signal", "intuition": "This loss function creates a hybrid between a Bradley-Terry style logistic loss and an adaptive margin loss, incorporating ideas from both parents while introducing a new coupling mechanism.\n\nInherited Ideas:\n- From Parent 1 (Softplus Adaptive Margin Loss), it inherits the core `softplus` structure for the loss calculation, specifically `softplus(margin - delta_logp)`. This provides a smooth and non-negative hinge-like loss.\n- From Parent 0 (Adaptive Margin Loss with Z-Score), it inherits the idea of using Z-score normalization on the batch-wise cost differences (`delta_cost`) to create a scale-invariant cost signal. This makes the loss less sensitive to the absolute magnitude of costs in a given batch.\n\nNew Coupling Ideas:\n1.  **Exponential Cost Signal**: Instead of using the normalized cost signal directly as a linear margin (e.g., `alpha * signal`), the child loss uses it as an exponent in a Bradley-Terry-like formulation: `loss = logsigmoid(delta_logp - exp(alpha * zscore(delta_cost)))`. This can be rewritten as `softplus(exp(alpha * zscore(delta_cost)) - delta_logp)`, connecting it to the inherited `softplus` structure. This coupling creates a margin that grows exponentially with the normalized cost difference, pushing the model much harder to separate pairs with significantly different costs, while having a gentler effect on pairs with similar costs.\n2.  **Rank Gap Modulation**: The child also inherits the rank gap modulation from Parent 0, but applies it in a novel way. The term `rank_gap = 1.0 - exp(-beta * rank_diff)` is used to modulate the strength of the exponential cost signal itself (`alpha * rank_gap_weight * zscore(delta_cost)`). This means that the exponential growth of the margin is amplified for pairs that are far apart in the batch's cost ranking. It focuses the strong exponential penalty on pairs that are not just different in cost value, but also significantly separated in their relative ordering within the batch.", "hyperparams": {"alpha": 1.0, "beta": 0.05, "z_clamp": 3.0}, "operators_used": ["softplus", "exp", "zscore", "clamp", "rank_gap"]}}, "better_than_baseline": false}
{"generation": 6, "index": 2, "ir": {"name": "Softplus-Sigmoid Adaptive Margin Loss with Z-Score Normalization", "intuition": "This loss function creates a robust, adaptive preference loss by merging concepts from its parents and introducing a new coupling mechanism.\n\nInherited Ideas:\n- From Parent 0, it inherits the use of Z-score normalization for the batch-wise cost differences (`delta_cost`). This makes the margin calculation invariant to the absolute scale of costs within a batch, improving stability and reducing hyperparameter sensitivity.\n- From Parent 1, it inherits the use of `softplus` to create a non-saturating, unbounded 'cost signal' from the (now normalized) cost differences. This ensures that larger cost gaps translate into proportionally larger target margins without a ceiling effect.\n- From both parents, it adopts the fundamental adaptive margin structure: `loss = softplus(margin - delta_logp)`.\n\nNew Coupling Ideas:\n1.  **Sigmoid-Modulated Margin**: A new coupling mechanism is introduced where the `alpha` hyperparameter, which scales the margin, is itself modulated by a sigmoid function of the log-probability difference (`delta_logp`). The margin is now calculated as `margin = (alpha * sigmoid(delta_logp)) * cost_signal`. This creates a dynamic scaling effect: when the model is already confident and correct (`delta_logp` is large and positive), the sigmoid term approaches 1, applying the full margin pressure. Conversely, when the model is very wrong (`delta_logp` is large and negative), the sigmoid term approaches 0, reducing the margin and thus the loss gradient. This acts as a stability trick, preventing excessively large gradients from incorrect but confident predictions and focusing learning on pairs where the model is uncertain or slightly wrong.\n2.  **Clamped Z-Score**: To further enhance stability, the Z-scored cost differences are clamped to a reasonable range (e.g., [-3, 3]) before being passed to the `softplus` function. This prevents extreme cost outliers within a batch from creating disproportionately large margin targets and loss values.", "pseudocode": "1. For each pair (w, l) in the batch where cost(w) < cost(l), calculate the cost difference `delta_cost = cost(l) - cost(w)` and log probability difference `delta_logp = logp(w) - logp(l)`.\n2. Inherited Idea (Parent 0): Standardize the `delta_cost` vector for the entire batch using Z-score normalization to get `delta_cost_norm`.\n3. New Coupling Idea: Clamp the `delta_cost_norm` vector to a fixed range (e.g., [-3, 3]) to mitigate the effect of extreme outliers.\n4. Inherited Idea (Parent 1): Transform the clamped, normalized cost differences into a non-saturating signal using `softplus`: `cost_signal = softplus(clamped_delta_cost_norm / temp_cost)`.\n5. New Coupling Idea: Create a dynamic margin scalar by modulating the `alpha` hyperparameter with a sigmoid function of the model's current preference: `dynamic_alpha = alpha * sigmoid(delta_logp)`.\n6. Combine the cost signal and the dynamic scalar to form the final adaptive margin: `margin = dynamic_alpha * cost_signal`.\n7. Compute the per-sample loss using the core structure from both parents: `loss = softplus(margin - delta_logp)`.\n8. Return the weighted mean of the loss over the batch.", "hyperparams": {"alpha": 1.0, "temp_cost": 1.0, "z_clamp": 3.0}, "operators_used": ["softplus", "sigmoid", "zscore", "clamp"], "implementation_hint": {"expects": ["cost_a", "cost_b", "logp_a", "logp_b"], "returns": "scalar"}, "code": "import torch\nimport torch.nn.functional as F\n\ndef generated_loss(batch, model_output, extra):\n    \"\"\"\n    Computes the Softplus-Sigmoid Adaptive Margin Loss with Z-Score Normalization.\n\n    Inherits Z-score normalization from Parent 0 and the use of softplus for the cost signal from Parent 1.\n    Introduces two new couplings:\n    1. A sigmoid modulation of the margin based on the current log-probability difference (delta_logp),\n       which dampens the loss for very incorrect predictions, improving stability.\n    2. Clamping of the Z-scored costs before the softplus transformation to handle outliers.\n    \"\"\"\n    # For preference learning, we assume the dataset provides pairs where one is strictly better.\n    cost_w = torch.min(batch['cost_a'], batch['cost_b'])\n    cost_l = torch.max(batch['cost_a'], batch['cost_b'])\n    log_prob_w = batch['log_prob_w']\n    log_prob_l = batch['log_prob_l']\n\n    # Retrieve hyperparameters\n    alpha = extra.get('alpha', 1.0)\n    temp_cost = extra.get('temp_cost', 1.0)\n    z_clamp = extra.get('z_clamp', 3.0)\n\n    # 1. Calculate cost and log probability differences\n    delta_cost = cost_l - cost_w\n    delta_logp = log_prob_w - log_prob_l\n\n    # Early exit if there's no cost variation to prevent division by zero in z-score\n    if delta_cost.std() < 1e-8:\n        # If no cost difference, the ideal margin is 0. The loss is softplus(-delta_logp).\n        # This is equivalent to standard logistic loss.\n        loss = F.softplus(-delta_logp)\n    else:\n        # 2. Inherited (Parent 0): Z-score normalization of cost differences\n        delta_cost_mean = delta_cost.mean()\n        delta_cost_std = delta_cost.std()\n        delta_cost_norm = (delta_cost - delta_cost_mean) / delta_cost_std\n\n        # 3. New Coupling: Clamp the normalized costs to prevent outliers from creating extreme margins\n        delta_cost_clamped = torch.clamp(delta_cost_norm, -z_clamp, z_clamp)\n\n        # 4. Inherited (Parent 1): Create a non-saturating cost signal using softplus\n        cost_signal = F.softplus(delta_cost_clamped / temp_cost)\n\n        # 5. New Coupling: Modulate the margin scale with a sigmoid of the model's prediction\n        # This reduces the margin (and gradient) for very wrong predictions (large negative delta_logp)\n        dynamic_alpha = alpha * torch.sigmoid(delta_logp.detach()) # Detach to treat as a dynamic weight\n\n        # 6. Calculate the final adaptive margin\n        adaptive_margin = dynamic_alpha * cost_signal\n\n        # 7. Compute the core loss using softplus (from both parents)\n        loss = F.softplus(adaptive_margin - delta_logp)\n\n    # Apply optional weights and compute the mean.\n    weights = batch.get('weight', None)\n    if weights is not None:\n        loss = loss * weights\n        return loss.sum() / weights.sum().clamp(min=1e-8)\n    else:\n        return loss.mean()"}, "fitness": {"hf_like_score": 35.81654167175293, "validation_objective": 35.81654167175293, "generalization_penalty": 0.0, "generalization_objectives": {"100": 35.59887886047363}, "train_score_mean": 38.76559213542938, "train_loss_mean": 1.1382183606028557, "pair_count": 129023919, "config": {"hf": {"problem": "tsp", "hf_steps": 1000, "train_problem_size": 100, "valid_problem_sizes": [100], "train_batch_size": 64, "pomo_size": 64, "learning_rate": 0.0003, "weight_decay": 1e-06, "alpha": 0.05, "device": "cuda", "seed": 1234, "num_validation_episodes": 128, "validation_batch_size": 64, "generalization_penalty_weight": 1.0, "pool_version": "v0"}, "free_loss": {"f1_steps": 1000, "f2_steps": 100, "f3_enabled": false}}, "loss_ir": {"name": "Softplus-Sigmoid Adaptive Margin Loss with Z-Score Normalization", "intuition": "This loss function creates a robust, adaptive preference loss by merging concepts from its parents and introducing a new coupling mechanism.\n\nInherited Ideas:\n- From Parent 0, it inherits the use of Z-score normalization for the batch-wise cost differences (`delta_cost`). This makes the margin calculation invariant to the absolute scale of costs within a batch, improving stability and reducing hyperparameter sensitivity.\n- From Parent 1, it inherits the use of `softplus` to create a non-saturating, unbounded 'cost signal' from the (now normalized) cost differences. This ensures that larger cost gaps translate into proportionally larger target margins without a ceiling effect.\n- From both parents, it adopts the fundamental adaptive margin structure: `loss = softplus(margin - delta_logp)`.\n\nNew Coupling Ideas:\n1.  **Sigmoid-Modulated Margin**: A new coupling mechanism is introduced where the `alpha` hyperparameter, which scales the margin, is itself modulated by a sigmoid function of the log-probability difference (`delta_logp`). The margin is now calculated as `margin = (alpha * sigmoid(delta_logp)) * cost_signal`. This creates a dynamic scaling effect: when the model is already confident and correct (`delta_logp` is large and positive), the sigmoid term approaches 1, applying the full margin pressure. Conversely, when the model is very wrong (`delta_logp` is large and negative), the sigmoid term approaches 0, reducing the margin and thus the loss gradient. This acts as a stability trick, preventing excessively large gradients from incorrect but confident predictions and focusing learning on pairs where the model is uncertain or slightly wrong.\n2.  **Clamped Z-Score**: To further enhance stability, the Z-scored cost differences are clamped to a reasonable range (e.g., [-3, 3]) before being passed to the `softplus` function. This prevents extreme cost outliers within a batch from creating disproportionately large margin targets and loss values.", "hyperparams": {"alpha": 1.0, "temp_cost": 1.0, "z_clamp": 3.0}, "operators_used": ["softplus", "sigmoid", "zscore", "clamp"]}}, "better_than_baseline": false}
{"generation": 6, "index": 3, "ir": {"name": "Softplus-Sigmoid Adaptive Margin Loss", "intuition": "This child loss function creates a robust, adaptive preference loss by merging concepts from its parents and introducing a new coupling for stability and control.\n\nInherited Ideas:\n- From Parent 1 (`Softplus Adaptive Margin Loss`), it inherits the use of `softplus` to transform the cost difference (`delta_cost`) into a non-saturating, non-negative signal. This ensures that as the cost gap between two solutions widens, the target margin for the model's preference also increases in a smooth, unbounded way.\n- From Parent 0 (`Adaptive Margin Loss with Z-Score...`), it inherits the core loss structure of `softplus(margin - delta_logp)`. This smooth hinge loss penalizes the model when its log-probability difference (`delta_logp`) falls short of the target margin.\n\nNew Coupling Ideas:\n1.  **Log-Sigmoid Transformation of Log-Probabilities**: Instead of directly using the raw `delta_logp`, the loss first applies a `logsigmoid` transformation: `log_preference_ratio = logsigmoid(delta_logp)`. This maps the unbounded `delta_logp` (from -inf to +inf) to a bounded range (from -inf to 0), effectively representing the log-probability of preferring the winning solution. This transformation stabilizes the loss calculation by preventing extremely large `delta_logp` values from causing instability or vanishing gradients, acting as a soft clamp on the model's output within the loss function itself.\n2.  **Margin as a Target Log-Preference Ratio**: The adaptive margin, derived from the `softplus` of the cost difference, is now interpreted as a target for this new `log_preference_ratio` rather than for the raw `delta_logp`. The final loss becomes `softplus(margin - logsigmoid(delta_logp))`. This re-frames the learning objective: the model is pushed to make its log-preference ratio for the correct pair at least as high as the margin dictated by the cost difference.", "pseudocode": "1. For each pair (w, l) in the batch, where cost(w) < cost(l), calculate the cost difference `delta_cost = cost(l) - cost(w)` and the log probability difference `delta_logp = logp(w) - logp(l)`.\n2. (Inherited from Parent 1) Transform the cost difference using a `softplus` function to create a smooth, non-negative, and non-saturating 'cost signal': `cost_signal = softplus(delta_cost / temp_cost)`.\n3. Calculate the adaptive margin, which is proportional to this cost signal: `margin = alpha * cost_signal`.\n4. (New Coupling) Transform the model's log-probability difference using the `logsigmoid` function to get a bounded log-preference ratio: `log_preference_ratio = logsigmoid(delta_logp)`.\n5. (Inherited from Parent 0 & New Coupling) Compute the loss as the softplus of the difference between the target margin and the model's log-preference ratio: `loss = softplus(margin - log_preference_ratio)`.\n6. Return the weighted mean of the loss over the batch.", "hyperparams": {"alpha": 0.5, "temp_cost": 1.0}, "operators_used": ["softplus", "logsigmoid"], "implementation_hint": {"expects": ["cost_a", "cost_b", "logp_a", "logp_b"], "returns": "scalar"}, "code": "import torch\nimport torch.nn.functional as F\n\ndef generated_loss(batch, model_output, extra):\n    \"\"\"\n    Computes the Softplus-Sigmoid Adaptive Margin Loss.\n\n    This loss function combines ideas from its parents to create a stable and adaptive loss.\n    - Inherits the use of `softplus` on the cost difference to create a non-saturating margin (from Parent 1).\n    - Inherits the `softplus(margin - prediction)` core loss structure (from Parent 0).\n    - Introduces a new coupling: it uses `logsigmoid` to transform the model's log-probability difference,\n      stabilizing the loss by mapping the unbounded `delta_logp` to the range (-inf, 0].\n    \"\"\"\n    # For preference learning, we assume the dataset provides pairs where one is strictly better.\n    # cost_w < cost_l\n    cost_w = torch.min(batch['cost_a'], batch['cost_b'])\n    cost_l = torch.max(batch['cost_a'], batch['cost_b'])\n\n    # The 'log_prob_w' and 'log_prob_l' are assumed to be provided directly,\n    # corresponding to the winner and loser solutions respectively.\n    log_prob_w = batch['log_prob_w']\n    log_prob_l = batch['log_prob_l']\n\n    # Retrieve hyperparameters\n    alpha = extra.get('alpha', 0.5)\n    temp_cost = extra.get('temp_cost', 1.0)\n\n    # 1. Calculate cost and log probability differences\n    # delta_cost is guaranteed to be >= 0\n    delta_cost = cost_l - cost_w\n    delta_logp = log_prob_w - log_prob_l\n\n    # 2. Inherited Idea (Parent 1): Transform the cost difference into a smooth, non-negative 'cost signal'.\n    # Using softplus provides a non-saturating signal that grows for large cost differences.\n    cost_signal = F.softplus(delta_cost / temp_cost)\n\n    # 3. Create an adaptive margin based on the cost signal.\n    # The target separation is proportional to how different the costs are.\n    adaptive_margin = alpha * cost_signal\n\n    # 4. New Coupling Idea: Transform the model's log-prob difference using `logsigmoid`.\n    # This maps the unbounded delta_logp to a bounded range (-inf, 0], which stabilizes the loss\n    # by preventing extreme values from causing numerical issues.\n    log_preference_ratio = F.logsigmoid(delta_logp)\n\n    # 5. Inherited Idea (Parent 0) & New Coupling: Compute the core loss using softplus.\n    # The loss is low if log_preference_ratio > adaptive_margin.\n    # Since log_preference_ratio is always <= 0, the margin here is effectively pushing the model's\n    # preference for the winner to be very high (close to 1 in probability space).\n    per_sample_loss = F.softplus(adaptive_margin - log_preference_ratio)\n\n    # 6. Apply optional weights and compute the mean.\n    weights = batch.get('weight', None)\n    if weights is not None:\n        per_sample_loss = per_sample_loss * weights\n        # Use a small epsilon to prevent division by zero if all weights are zero.\n        return per_sample_loss.sum() / weights.sum().clamp(min=1e-8)\n    else:\n        return per_sample_loss.mean()"}, "fitness": {"hf_like_score": 30.61732292175293, "validation_objective": 30.61732292175293, "generalization_penalty": 0.0, "generalization_objectives": {"100": 30.033329010009766}, "train_score_mean": 39.3748005361557, "train_loss_mean": 2.0933321293592453, "pair_count": 129023930, "config": {"hf": {"problem": "tsp", "hf_steps": 1000, "train_problem_size": 100, "valid_problem_sizes": [100], "train_batch_size": 64, "pomo_size": 64, "learning_rate": 0.0003, "weight_decay": 1e-06, "alpha": 0.05, "device": "cuda", "seed": 1234, "num_validation_episodes": 128, "validation_batch_size": 64, "generalization_penalty_weight": 1.0, "pool_version": "v0"}, "free_loss": {"f1_steps": 1000, "f2_steps": 100, "f3_enabled": false}}, "loss_ir": {"name": "Softplus-Sigmoid Adaptive Margin Loss", "intuition": "This child loss function creates a robust, adaptive preference loss by merging concepts from its parents and introducing a new coupling for stability and control.\n\nInherited Ideas:\n- From Parent 1 (`Softplus Adaptive Margin Loss`), it inherits the use of `softplus` to transform the cost difference (`delta_cost`) into a non-saturating, non-negative signal. This ensures that as the cost gap between two solutions widens, the target margin for the model's preference also increases in a smooth, unbounded way.\n- From Parent 0 (`Adaptive Margin Loss with Z-Score...`), it inherits the core loss structure of `softplus(margin - delta_logp)`. This smooth hinge loss penalizes the model when its log-probability difference (`delta_logp`) falls short of the target margin.\n\nNew Coupling Ideas:\n1.  **Log-Sigmoid Transformation of Log-Probabilities**: Instead of directly using the raw `delta_logp`, the loss first applies a `logsigmoid` transformation: `log_preference_ratio = logsigmoid(delta_logp)`. This maps the unbounded `delta_logp` (from -inf to +inf) to a bounded range (from -inf to 0), effectively representing the log-probability of preferring the winning solution. This transformation stabilizes the loss calculation by preventing extremely large `delta_logp` values from causing instability or vanishing gradients, acting as a soft clamp on the model's output within the loss function itself.\n2.  **Margin as a Target Log-Preference Ratio**: The adaptive margin, derived from the `softplus` of the cost difference, is now interpreted as a target for this new `log_preference_ratio` rather than for the raw `delta_logp`. The final loss becomes `softplus(margin - logsigmoid(delta_logp))`. This re-frames the learning objective: the model is pushed to make its log-preference ratio for the correct pair at least as high as the margin dictated by the cost difference.", "hyperparams": {"alpha": 0.5, "temp_cost": 1.0}, "operators_used": ["softplus", "logsigmoid"]}}, "better_than_baseline": false}
{"generation": 6, "index": 4, "ir": {"name": "Sigmoid-Gated Adaptive Margin Loss with Rank Gap", "intuition": "This loss function creates a robust, adaptive preference loss by combining ideas from its parents and introducing a new gating mechanism.\n\nInherited Ideas:\n- From both parents, it inherits the core structure of an adaptive margin loss: `loss = softplus(margin - delta_logp)`. This penalizes the model when its log-probability difference for a preferred solution falls short of a target margin.\n- From Parent 0 (`Adaptive Margin Loss with Z-Score Normalization and Rank Gap`), it inherits the idea of modulating the final loss by a `rank_gap` term. This up-weights the loss for pairs that are far apart in the batch's cost ranking, encouraging the model to prioritize learning more significant preference distinctions.\n- From Parent 1 (`Softplus Adaptive Margin Loss`), it inherits the use of `softplus(delta_cost)` to create a smooth, non-saturating signal from the cost difference. This ensures the margin grows as the cost gap widens, without the upper bound imposed by a `tanh` function.\n\nNew Coupling Ideas:\n1.  **Sigmoid Gating of the Margin**: The adaptive margin, derived from `softplus(delta_cost)`, is multiplied by a sigmoid-transformed version of the log-probability difference (`delta_logp`). The margin is calculated as `margin = alpha * softplus(delta_cost) * sigmoid(-gamma * delta_logp)`. This creates a dynamic margin that is largest when the model is most 'confused' (i.e., `delta_logp` is close to zero or negative). When the model is already correctly preferring the winner by a large margin (`delta_logp` is large and positive), the sigmoid gate approaches zero, effectively reducing the target margin and preventing the loss from punishing the model for not achieving an unnecessarily large separation. This focuses the learning on difficult or incorrectly ranked pairs.\n2.  **Clamping the Cost Signal**: The `softplus(delta_cost)` signal is clamped to a maximum value (`margin_clamp`). This prevents extremely large cost differences (outliers) from creating an unbounded, numerically unstable margin target, adding robustness to the loss calculation.", "pseudocode": "1. For each pair (w, l) in the batch, where cost(w) < cost(l), calculate the cost difference `delta_cost = cost(l) - cost(w)` and the log probability difference `delta_logp = logp(w) - logp(l)`.\n2. Inherited from Parent 1: Create a smooth, non-saturating cost signal using `softplus`: `cost_signal = softplus(delta_cost / temp_cost)`.\n3. New Coupling Idea: Clamp the cost signal to a maximum value to prevent outliers from creating excessively large margins: `clamped_cost_signal = clamp(cost_signal, max=margin_clamp)`.\n4. New Coupling Idea: Create a sigmoid gate that is sensitive to the model's current preference. The gate value is high when the model is wrong or uncertain (`delta_logp` is small/negative) and low when the model is confident and correct (`delta_logp` is large/positive): `gate = sigmoid(-gamma * delta_logp)`.\n5. Calculate the dynamic, gated margin by combining the clamped cost signal and the sigmoid gate: `adaptive_margin = alpha * clamped_cost_signal * gate`.\n6. Compute the per-sample loss using the core structure from both parents: `base_loss = softplus(adaptive_margin - delta_logp)`.\n7. Inherited from Parent 0: Calculate a rank gap weight. First, find the rank of `cost_w` and `cost_l` within the sorted costs of the entire batch. Then compute `rank_gap_weight = 1.0 - exp(-beta * abs(rank(cost_l) - rank(cost_w)))`.\n8. Modulate the base loss with the rank gap weight: `final_loss = base_loss * rank_gap_weight`.\n9. Return the weighted mean of `final_loss` over the batch.", "hyperparams": {"alpha": 2.0, "temp_cost": 1.0, "gamma": 0.5, "beta": 0.05, "margin_clamp": 10.0}, "operators_used": ["softplus", "sigmoid", "clamp", "exp", "rank_gap"], "implementation_hint": {"expects": ["cost_a", "cost_b", "logp_w", "logp_l"], "returns": "scalar"}, "code": "import torch\nimport torch.nn.functional as F\n\ndef generated_loss(batch, model_output, extra):\n    \"\"\"\n    Computes the Sigmoid-Gated Adaptive Margin Loss with Rank Gap.\n\n    Inherits:\n    - The softplus(margin - delta_logp) structure from both parents.\n    - The use of softplus(delta_cost) for a non-saturating cost signal from Parent 1.\n    - The use of a rank_gap modulator from Parent 0.\n\n    Introduces:\n    1. A sigmoid gate on the margin, which reduces the target margin when the model is already confident,\n       focusing learning on difficult pairs.\n    2. Clamping of the cost signal to prevent outliers from creating extreme margins.\n    \"\"\"\n    # For preference learning, we assume the dataset provides pairs where one is strictly better.\n    cost_w = torch.min(batch['cost_a'], batch['cost_b'])\n    cost_l = torch.max(batch['cost_a'], batch['cost_b'])\n    log_prob_w = batch['log_prob_w']\n    log_prob_l = batch['log_prob_l']\n\n    # Retrieve hyperparameters\n    alpha = extra.get('alpha', 2.0)\n    temp_cost = extra.get('temp_cost', 1.0)\n    gamma = extra.get('gamma', 0.5)\n    beta = extra.get('beta', 0.05)\n    margin_clamp = extra.get('margin_clamp', 10.0)\n\n    # 1. Calculate cost and log probability differences\n    delta_cost = cost_l - cost_w\n    delta_logp = log_prob_w - log_prob_l\n\n    # 2. Inherited (Parent 1): Create a non-saturating cost signal with softplus\n    cost_signal = F.softplus(delta_cost / temp_cost)\n\n    # 3. New Coupling: Clamp the cost signal for stability against outliers\n    clamped_cost_signal = torch.clamp(cost_signal, max=margin_clamp)\n\n    # 4. New Coupling: Create a sigmoid gate based on model's current preference (delta_logp)\n    # Gate is ~1 when delta_logp is negative (wrong preference) or small (uncertain).\n    # Gate is ~0 when delta_logp is large and positive (correct preference).\n    gate = torch.sigmoid(-gamma * delta_logp)\n\n    # 5. Calculate the dynamic, gated adaptive margin\n    adaptive_margin = alpha * clamped_cost_signal * gate.detach() # Detach gate to not penalize high delta_logp\n\n    # 6. Inherited (Both Parents): Compute the base per-sample loss\n    base_loss = F.softplus(adaptive_margin - delta_logp)\n\n    # 7. Inherited (Parent 0): Calculate rank gap modulation\n    # This requires ranking all costs in the batch\n    all_costs = torch.cat([cost_w, cost_l], dim=0)\n    sorted_costs, _ = torch.sort(all_costs)\n\n    # Find the rank of each cost_w and cost_l in the sorted list\n    rank_w = torch.searchsorted(sorted_costs, cost_w)\n    rank_l = torch.searchsorted(sorted_costs, cost_l)\n    rank_diff = (rank_l - rank_w).float()\n\n    # rank_gap_weight approaches 1 as rank_diff increases\n    rank_gap_weight = 1.0 - torch.exp(-beta * rank_diff)\n\n    # 8. Modulate the base loss with the rank gap weight\n    final_loss = base_loss * rank_gap_weight.detach() # Detach to treat as a pure weighting factor\n\n    # 9. Apply optional weights and compute the mean.\n    weights = batch.get('weight', None)\n    if weights is not None:\n        final_loss = final_loss * weights\n        return final_loss.sum() / weights.sum().clamp(min=1e-8)\n    else:\n        return final_loss.mean()"}, "fitness": {"hf_like_score": 30.955934524536133, "validation_objective": 30.955934524536133, "generalization_penalty": 0.0, "generalization_objectives": {"100": 30.842860221862793}, "train_score_mean": 39.127545660018924, "train_loss_mean": 1.3679582201242446, "pair_count": 129023905, "config": {"hf": {"problem": "tsp", "hf_steps": 1000, "train_problem_size": 100, "valid_problem_sizes": [100], "train_batch_size": 64, "pomo_size": 64, "learning_rate": 0.0003, "weight_decay": 1e-06, "alpha": 0.05, "device": "cuda", "seed": 1234, "num_validation_episodes": 128, "validation_batch_size": 64, "generalization_penalty_weight": 1.0, "pool_version": "v0"}, "free_loss": {"f1_steps": 1000, "f2_steps": 100, "f3_enabled": false}}, "loss_ir": {"name": "Sigmoid-Gated Adaptive Margin Loss with Rank Gap", "intuition": "This loss function creates a robust, adaptive preference loss by combining ideas from its parents and introducing a new gating mechanism.\n\nInherited Ideas:\n- From both parents, it inherits the core structure of an adaptive margin loss: `loss = softplus(margin - delta_logp)`. This penalizes the model when its log-probability difference for a preferred solution falls short of a target margin.\n- From Parent 0 (`Adaptive Margin Loss with Z-Score Normalization and Rank Gap`), it inherits the idea of modulating the final loss by a `rank_gap` term. This up-weights the loss for pairs that are far apart in the batch's cost ranking, encouraging the model to prioritize learning more significant preference distinctions.\n- From Parent 1 (`Softplus Adaptive Margin Loss`), it inherits the use of `softplus(delta_cost)` to create a smooth, non-saturating signal from the cost difference. This ensures the margin grows as the cost gap widens, without the upper bound imposed by a `tanh` function.\n\nNew Coupling Ideas:\n1.  **Sigmoid Gating of the Margin**: The adaptive margin, derived from `softplus(delta_cost)`, is multiplied by a sigmoid-transformed version of the log-probability difference (`delta_logp`). The margin is calculated as `margin = alpha * softplus(delta_cost) * sigmoid(-gamma * delta_logp)`. This creates a dynamic margin that is largest when the model is most 'confused' (i.e., `delta_logp` is close to zero or negative). When the model is already correctly preferring the winner by a large margin (`delta_logp` is large and positive), the sigmoid gate approaches zero, effectively reducing the target margin and preventing the loss from punishing the model for not achieving an unnecessarily large separation. This focuses the learning on difficult or incorrectly ranked pairs.\n2.  **Clamping the Cost Signal**: The `softplus(delta_cost)` signal is clamped to a maximum value (`margin_clamp`). This prevents extremely large cost differences (outliers) from creating an unbounded, numerically unstable margin target, adding robustness to the loss calculation.", "hyperparams": {"alpha": 2.0, "temp_cost": 1.0, "gamma": 0.5, "beta": 0.05, "margin_clamp": 10.0}, "operators_used": ["softplus", "sigmoid", "clamp", "exp", "rank_gap"]}}, "better_than_baseline": false}
{"generation": 7, "index": 0, "ir": {"name": "Sigmoid-Gated Adaptive Loss with Rank-Gap Normalization", "intuition": "This loss function creates a robust, adaptive preference signal by combining ideas from its parents and introducing novel coupling mechanisms.\n\nInherited Ideas:\n- From Parent 1 (Softplus Adaptive Margin Loss), it inherits the core structure of a softplus-based hinge loss: `softplus(margin - delta_logp)`. This provides a smooth penalty when the model's log-probability difference doesn't meet a target margin.\n- From Parent 0 (Adaptive Margin Loss with Z-Score Normalization and Rank Gap), it inherits the concept of using a rank-based metric to modulate the loss. Instead of using it as a final weight, we integrate it directly into the margin calculation.\n\nNew Coupling Ideas:\n1.  **Sigmoid Gating Mechanism**: The loss introduces a sigmoid gate, `sigmoid(beta * (delta_logp - offset))`, which dynamically re-weights the primary loss term. When the model's preference `delta_logp` is already very high (i.e., the pair is 'easy'), the sigmoid gate approaches 1, effectively turning off the loss for that sample. This allows the model to focus its capacity on more difficult or ambiguous pairs where its preference signal is weak or incorrect, preventing overconfidence on easy examples.\n2.  **Rank-Gap Normalized Cost Signal**: Instead of using raw cost differences or Z-scored costs to create the adaptive margin, we introduce a new normalization scheme. The cost difference `delta_cost` is divided by the rank difference `rank_gap` between the pair within the batch. This `rank_gap` acts as a measure of 'perceptual distance' in the current context. Dividing by it normalizes the cost difference, making the resulting `margin` less sensitive to the absolute scale of costs and more attuned to the relative ordering of solutions in the batch. A `clamp` is used on the normalized signal to ensure stability against pairs with identical ranks.", "pseudocode": "1. For each pair (w, l) where cost(w) < cost(l), calculate the cost difference `delta_cost = cost(l) - cost(w)` and log probability difference `delta_logp = logp(w) - logp(l)`.\n2. Calculate the rank of each `cost_w` and `cost_l` within the sorted costs of the entire batch. Compute the rank difference `rank_gap = rank(l) - rank(w)`.\n3. Create a 'rank-gap normalized' cost signal. Divide `delta_cost` by the `rank_gap`, adding a small epsilon to the denominator for stability. Clamp the result to prevent extreme values, especially when the rank gap is small. This is the new coupling idea.\n4. Inherit the idea of an adaptive margin. Calculate the margin as `margin = alpha * rank_gap_normalized_cost_signal`.\n5. Inherit the core loss structure. Compute the base loss using `base_loss = softplus(margin - delta_logp)`.\n6. Introduce a new sigmoid gating mechanism. Calculate a gate value `gate = sigmoid(beta * (delta_logp - offset))`. This gate value approaches 1 for 'easy' pairs where `delta_logp` is already high, and approaches 0 for 'hard' pairs.\n7. Apply the gate to the loss: `final_loss = (1.0 - gate) * base_loss`. This focuses the training on pairs that the model finds difficult.\n8. Return the weighted mean of `final_loss` over the batch.", "hyperparams": {"alpha": 1.0, "beta": 0.5, "offset": 2.0, "clamp_max": 5.0}, "operators_used": ["softplus", "sigmoid", "clamp", "rank_gap"], "implementation_hint": {"expects": ["cost_a", "cost_b", "log_prob_w", "log_prob_l"], "returns": "scalar"}, "code": "import torch\nimport torch.nn.functional as F\n\ndef generated_loss(batch, model_output, extra):\n    \"\"\"\n    Computes the Sigmoid-Gated Adaptive Loss with Rank-Gap Normalization.\n\n    Inherits:\n    - The softplus(margin - delta_logp) structure from the Softplus Adaptive Margin Loss.\n    - The use of a rank-based metric from the Adaptive Margin Loss with Rank Gap.\n\n    Introduces:\n    1. Rank-Gap Normalization: The cost difference is normalized by the difference in ranks\n       within the batch to create a scale-invariant, context-aware margin.\n    2. Sigmoid Gating: A sigmoid gate diminishes the loss for 'easy' pairs where the model\n       already exhibits a strong preference, focusing training on more difficult examples.\n    \"\"\"\n    cost_w = torch.min(batch['cost_a'], batch['cost_b'])\n    cost_l = torch.max(batch['cost_a'], batch['cost_b'])\n    log_prob_w = batch['log_prob_w']\n    log_prob_l = batch['log_prob_l']\n\n    alpha = extra.get('alpha', 1.0)\n    beta = extra.get('beta', 0.5)\n    offset = extra.get('offset', 2.0)\n    clamp_max = extra.get('clamp_max', 5.0)\n\n    delta_cost = cost_l - cost_w\n    delta_logp = log_prob_w - log_prob_l\n\n    # New Coupling 1: Rank-Gap Normalized Cost Signal\n    # Get ranks of all costs in the batch to find the rank gap\n    all_costs = torch.cat([cost_w, cost_l], dim=0)\n    sorted_costs, _ = torch.sort(all_costs)\n    rank_w = torch.searchsorted(sorted_costs, cost_w).float()\n    rank_l = torch.searchsorted(sorted_costs, cost_l).float()\n    rank_gap = rank_l - rank_w\n\n    # Normalize cost difference by rank gap. Add epsilon for stability.\n    # This makes the signal relative to the batch's cost distribution.\n    rank_gap_stable = rank_gap.clamp(min=1.0) # Prevent division by zero if ranks are the same\n    normalized_cost_signal = delta_cost / rank_gap_stable\n    \n    # Clamp the signal to prevent extreme margin values\n    clamped_signal = torch.clamp(normalized_cost_signal, max=clamp_max)\n\n    # Inherited: Calculate adaptive margin from the new signal\n    adaptive_margin = alpha * clamped_signal\n\n    # Inherited: Compute the core loss using softplus\n    base_loss = F.softplus(adaptive_margin - delta_logp)\n\n    # New Coupling 2: Sigmoid Gating Mechanism\n    # The gate approaches 1 for easy examples (high delta_logp), reducing their loss contribution.\n    # The offset determines the threshold for what is considered 'easy'.\n    gate = torch.sigmoid(beta * (delta_logp.detach() - offset))\n\n    # Apply the gate to focus on hard examples\n    final_loss = (1.0 - gate) * base_loss\n\n    # Apply optional weights and compute the mean\n    weights = batch.get('weight', None)\n    if weights is not None:\n        final_loss = final_loss * weights\n        return final_loss.sum() / weights.sum().clamp(min=1e-8)\n    else:\n        return final_loss.mean()"}, "fitness": {"hf_like_score": 44.00770950317383, "validation_objective": 43.75137138366699, "generalization_penalty": 0.25633811950683594, "generalization_objectives": {"100": 44.00770950317383}, "train_score_mean": 40.47046982574463, "train_loss_mean": 1.0806582278609276, "pair_count": 129023911, "config": {"hf": {"problem": "tsp", "hf_steps": 1000, "train_problem_size": 100, "valid_problem_sizes": [100], "train_batch_size": 64, "pomo_size": 64, "learning_rate": 0.0003, "weight_decay": 1e-06, "alpha": 0.05, "device": "cuda", "seed": 1234, "num_validation_episodes": 128, "validation_batch_size": 64, "generalization_penalty_weight": 1.0, "pool_version": "v0"}, "free_loss": {"f1_steps": 1000, "f2_steps": 100, "f3_enabled": false}}, "loss_ir": {"name": "Sigmoid-Gated Adaptive Loss with Rank-Gap Normalization", "intuition": "This loss function creates a robust, adaptive preference signal by combining ideas from its parents and introducing novel coupling mechanisms.\n\nInherited Ideas:\n- From Parent 1 (Softplus Adaptive Margin Loss), it inherits the core structure of a softplus-based hinge loss: `softplus(margin - delta_logp)`. This provides a smooth penalty when the model's log-probability difference doesn't meet a target margin.\n- From Parent 0 (Adaptive Margin Loss with Z-Score Normalization and Rank Gap), it inherits the concept of using a rank-based metric to modulate the loss. Instead of using it as a final weight, we integrate it directly into the margin calculation.\n\nNew Coupling Ideas:\n1.  **Sigmoid Gating Mechanism**: The loss introduces a sigmoid gate, `sigmoid(beta * (delta_logp - offset))`, which dynamically re-weights the primary loss term. When the model's preference `delta_logp` is already very high (i.e., the pair is 'easy'), the sigmoid gate approaches 1, effectively turning off the loss for that sample. This allows the model to focus its capacity on more difficult or ambiguous pairs where its preference signal is weak or incorrect, preventing overconfidence on easy examples.\n2.  **Rank-Gap Normalized Cost Signal**: Instead of using raw cost differences or Z-scored costs to create the adaptive margin, we introduce a new normalization scheme. The cost difference `delta_cost` is divided by the rank difference `rank_gap` between the pair within the batch. This `rank_gap` acts as a measure of 'perceptual distance' in the current context. Dividing by it normalizes the cost difference, making the resulting `margin` less sensitive to the absolute scale of costs and more attuned to the relative ordering of solutions in the batch. A `clamp` is used on the normalized signal to ensure stability against pairs with identical ranks.", "hyperparams": {"alpha": 1.0, "beta": 0.5, "offset": 2.0, "clamp_max": 5.0}, "operators_used": ["softplus", "sigmoid", "clamp", "rank_gap"]}}, "better_than_baseline": false}
{"generation": 7, "index": 1, "ir": {"name": "Sigmoid-Gated Adaptive Margin Loss with Rank-Gap Normalization", "intuition": "This loss function creates a robust, adaptive preference signal by combining a smooth, non-saturating margin from one parent with a batch-aware weighting scheme from the other, coupled with a new gating mechanism.\n\nInherited Ideas:\n- From 'Softplus Adaptive Margin Loss' (Parent 1), it inherits the core idea of using `softplus(delta_cost)` to create a smooth, unbounded, and non-negative adaptive margin. This ensures that larger cost differences result in a proportionally larger target separation for the model's log probabilities, without the saturation issues of functions like `tanh`.\n- From 'Adaptive Margin Loss with Z-Score Normalization and Rank Gap' (Parent 0), it inherits the concept of using the relative rank of costs within a batch to modulate the loss. Specifically, it uses the rank difference to create a dynamic weighting factor, focusing the model's attention on pairs that are far apart in the batch's cost distribution.\n\nNew Coupling Ideas:\n1.  **Rank-Gap Normalization of Margin**: Instead of just using `rank_gap` as a final loss weight, it is used to directly normalize the adaptive margin itself. The margin is divided by `1 + beta * rank_gap_normalized`, where `rank_gap_normalized` is the rank difference scaled by the batch size. This couples the margin's scale directly to the pair's relative importance in the batch. For pairs with a small rank gap, the margin is larger (less division), pushing the model to learn fine-grained distinctions. For pairs with a large rank gap, the margin is smaller, as the `softplus(delta_cost)` term is already large and a huge margin is not needed, preventing potential gradient explosion.\n2.  **Sigmoid Gating on Log-Probability Difference**: The model's log-probability difference (`delta_logp`) is passed through a sigmoid gate: `delta_logp * sigmoid(delta_logp)`. This serves two purposes: it down-weights the contribution of pairs where the model is already very confident (large positive `delta_logp`), preventing it from wasting capacity on already-learned examples. It also heavily penalizes pairs where the model is confidently wrong (large negative `delta_logp`), as the sigmoid approaches zero, amplifying the negative term.", "pseudocode": "1. For each pair (w, l) where cost(w) < cost(l), calculate the cost difference `delta_cost = cost(l) - cost(w)` and the log probability difference `delta_logp = logp(w) - logp(l)`.\n2. Inherit from Parent 1: Calculate a smooth, non-saturating adaptive margin base using `softplus`: `margin_base = alpha * softplus(delta_cost / temp_cost)`.\n3. Inherit from Parent 0: Determine the cost ranks for all `cost_w` and `cost_l` in the batch. Calculate the absolute rank difference `rank_diff = abs(rank(cost_l) - rank(cost_w))`.\n4. New Coupling (Rank-Gap Normalization): Normalize the rank difference by the batch size. Use this to create a denominator that modulates the margin: `margin_normalizer = 1.0 + beta * (rank_diff / batch_size)`. The modulated margin is `margin = margin_base / margin_normalizer`.\n5. New Coupling (Sigmoid Gating): Apply a sigmoid gate to the model's log-probability difference to modulate its contribution: `gated_delta_logp = delta_logp * sigmoid(delta_logp)`.\n6. Compute the core per-sample loss using the main softplus structure: `loss = softplus(margin - gated_delta_logp)`.\n7. Return the weighted mean of the per-sample losses over the batch.", "hyperparams": {"alpha": 1.0, "temp_cost": 1.0, "beta": 0.5}, "operators_used": ["softplus", "sigmoid", "rank_gap", "log"], "implementation_hint": {"expects": ["cost_w", "cost_l", "log_prob_w", "log_prob_l"], "returns": "scalar"}, "code": "import torch\nimport torch.nn.functional as F\n\ndef generated_loss(batch, model_output, extra):\n    \"\"\"\n    Computes the Sigmoid-Gated Adaptive Margin Loss with Rank-Gap Normalization.\n\n    Inherits:\n    - The use of `softplus(delta_cost)` for a non-saturating margin (from Parent 1).\n    - The use of rank differences for batch-aware modulation (from Parent 0).\n\n    Introduces:\n    - A new coupling where the rank gap normalizes the margin itself, not the final loss.\n    - A sigmoid gate on the model's log-probability difference to focus on uncertain or incorrect predictions.\n    \"\"\"\n    # For preference learning, we assume cost_w < cost_l.\n    cost_w = torch.min(batch['cost_a'], batch['cost_b'])\n    cost_l = torch.max(batch['cost_a'], batch['cost_b'])\n    log_prob_w = batch['log_prob_w']\n    log_prob_l = batch['log_prob_l']\n\n    # Retrieve hyperparameters\n    alpha = extra.get('alpha', 1.0)\n    temp_cost = extra.get('temp_cost', 1.0)\n    beta = extra.get('beta', 0.5)\n\n    # 1. Calculate cost and log probability differences\n    delta_cost = cost_l - cost_w\n    delta_logp = log_prob_w - log_prob_l\n    batch_size = delta_cost.shape[0]\n\n    # 2. Inherited from Parent 1: Calculate a smooth, non-saturating margin base.\n    margin_base = alpha * F.softplus(delta_cost / temp_cost)\n\n    # 3. Inherited from Parent 0: Calculate rank differences.\n    # Combine all costs to establish a batch-wide ranking.\n    all_costs = torch.cat([cost_w, cost_l], dim=0)\n    sorted_costs, _ = torch.sort(all_costs)\n    \n    # Find ranks efficiently using searchsorted.\n    rank_w = torch.searchsorted(sorted_costs, cost_w)\n    rank_l = torch.searchsorted(sorted_costs, cost_l)\n    rank_diff = (rank_l - rank_w).float()\n\n    # 4. New Coupling: Rank-Gap Normalization of the margin.\n    # Normalize rank diff by batch size to keep the term scale-invariant to N.\n    # For pairs far apart in rank, the denominator increases, reducing the margin.\n    # This prevents extremely large margins for pairs that are already easy to distinguish.\n    # Adding 1.0 ensures the denominator is always >= 1.\n    margin_normalizer = 1.0 + beta * (rank_diff / (2 * batch_size))\n    adaptive_margin = margin_base / margin_normalizer.detach() # Detach to treat as a pure weighting factor\n\n    # 5. New Coupling: Sigmoid Gating on delta_logp.\n    # This penalizes confident incorrect predictions more and reduces focus on already-correct ones.\n    gated_delta_logp = delta_logp * torch.sigmoid(delta_logp)\n\n    # 6. Compute the core loss structure.\n    loss = F.softplus(adaptive_margin - gated_delta_logp)\n\n    # 7. Apply optional weights and compute the mean.\n    weights = batch.get('weight', None)\n    if weights is not None:\n        loss = loss * weights\n        return loss.sum() / weights.sum().clamp(min=1e-8)\n    else:\n        return loss.mean()"}, "fitness": {"hf_like_score": 8.128266334533691, "validation_objective": 8.128266334533691, "generalization_penalty": 0.0, "generalization_objectives": {"100": 8.108760833740234}, "train_score_mean": 8.764982051849366, "train_loss_mean": 0.25869439728558064, "pair_count": 129023864, "config": {"hf": {"problem": "tsp", "hf_steps": 1000, "train_problem_size": 100, "valid_problem_sizes": [100], "train_batch_size": 64, "pomo_size": 64, "learning_rate": 0.0003, "weight_decay": 1e-06, "alpha": 0.05, "device": "cuda", "seed": 1234, "num_validation_episodes": 128, "validation_batch_size": 64, "generalization_penalty_weight": 1.0, "pool_version": "v0"}, "free_loss": {"f1_steps": 1000, "f2_steps": 100, "f3_enabled": false}}, "loss_ir": {"name": "Sigmoid-Gated Adaptive Margin Loss with Rank-Gap Normalization", "intuition": "This loss function creates a robust, adaptive preference signal by combining a smooth, non-saturating margin from one parent with a batch-aware weighting scheme from the other, coupled with a new gating mechanism.\n\nInherited Ideas:\n- From 'Softplus Adaptive Margin Loss' (Parent 1), it inherits the core idea of using `softplus(delta_cost)` to create a smooth, unbounded, and non-negative adaptive margin. This ensures that larger cost differences result in a proportionally larger target separation for the model's log probabilities, without the saturation issues of functions like `tanh`.\n- From 'Adaptive Margin Loss with Z-Score Normalization and Rank Gap' (Parent 0), it inherits the concept of using the relative rank of costs within a batch to modulate the loss. Specifically, it uses the rank difference to create a dynamic weighting factor, focusing the model's attention on pairs that are far apart in the batch's cost distribution.\n\nNew Coupling Ideas:\n1.  **Rank-Gap Normalization of Margin**: Instead of just using `rank_gap` as a final loss weight, it is used to directly normalize the adaptive margin itself. The margin is divided by `1 + beta * rank_gap_normalized`, where `rank_gap_normalized` is the rank difference scaled by the batch size. This couples the margin's scale directly to the pair's relative importance in the batch. For pairs with a small rank gap, the margin is larger (less division), pushing the model to learn fine-grained distinctions. For pairs with a large rank gap, the margin is smaller, as the `softplus(delta_cost)` term is already large and a huge margin is not needed, preventing potential gradient explosion.\n2.  **Sigmoid Gating on Log-Probability Difference**: The model's log-probability difference (`delta_logp`) is passed through a sigmoid gate: `delta_logp * sigmoid(delta_logp)`. This serves two purposes: it down-weights the contribution of pairs where the model is already very confident (large positive `delta_logp`), preventing it from wasting capacity on already-learned examples. It also heavily penalizes pairs where the model is confidently wrong (large negative `delta_logp`), as the sigmoid approaches zero, amplifying the negative term.", "hyperparams": {"alpha": 1.0, "temp_cost": 1.0, "beta": 0.5}, "operators_used": ["softplus", "sigmoid", "rank_gap", "log"]}}, "better_than_baseline": false}
{"generation": 7, "index": 2, "ir": {"name": "Sigmoid-Gated Adaptive Margin Loss with Rank-Gap Annealing", "intuition": "This loss function creates an adaptive preference margin by blending ideas from its parents and introducing novel coupling mechanisms for stability and improved learning dynamics.\n\nInherited Ideas:\n- From both parents, it inherits the core structure of a margin-based preference loss: `loss = softplus(margin - delta_logp)`. This smooth hinge loss penalizes the model when the log-probability difference between the winning and losing solutions falls short of a target margin.\n- From Parent 1 (`Adaptive Margin Loss with Z-Score Normalization and Rank Gap`), it inherits the idea of using the batch-wise rank gap between solutions (`rank_diff`) as an important signal. This recognizes that pairs with a larger difference in quality should be treated differently.\n- From Parent 2 (`Softplus Adaptive Margin Loss`), it inherits the use of the raw cost difference (`delta_cost`) to directly influence the margin, ensuring that larger cost gaps demand a larger log-probability separation.\n\nNew Coupling Ideas:\n1.  **Sigmoid Gating Mechanism**: The margin is a product of two components: a base margin derived from the cost difference and a sigmoid gate controlled by the rank difference. The margin is `alpha * (delta_cost / temp_cost) * sigmoid(beta * (rank_diff - rank_shift))`. This gating mechanism ensures that pairs with a very small rank difference (i.e., they are close neighbors in the sorted batch) have their target margin significantly down-weighted. This prevents the model from being aggressively penalized for small preference errors on nearly-indistinguishable pairs, focusing its capacity on learning more significant preference orderings first.\n2.  **Dynamic Rank-Gap Annealing**: The `rank_shift` hyperparameter in the sigmoid gate is not fixed but is scheduled to increase over training steps. It starts at a low value (e.g., 1.0), meaning only pairs that are immediately adjacent in rank get down-weighted. As training progresses, `rank_shift` increases, gradually raising the bar and requiring the model to distinguish between pairs that are further apart in rank. This annealing schedule acts as a curriculum, starting with easy distinctions and moving to harder ones, which can stabilize early training and lead to better final performance.", "pseudocode": "1. For each pair (w, l) where cost(w) < cost(l), calculate the cost difference `delta_cost = cost(l) - cost(w)` and the log probability difference `delta_logp = logp(w) - logp(l)`.\n2. Collect all costs from the batch and determine the rank of each `cost_w` and `cost_l` within the sorted list of all costs.\n3. Calculate the rank difference for each pair: `rank_diff = rank(cost_l) - rank(cost_w)`.\n4. Determine the current `rank_shift` value based on the current training step using a linear annealing schedule from a start to an end value.\n5. Compute a sigmoid gating factor: `gate = sigmoid(beta * (rank_diff - rank_shift))`. This gate approaches 1 for pairs with a large rank difference and approaches 0 for pairs with a rank difference smaller than the current `rank_shift`.\n6. Calculate the adaptive margin by multiplying the cost-based signal with the sigmoid gate: `margin = alpha * (delta_cost / temp_cost) * gate`. This effectively suppresses the target margin for pairs that are too close in rank for the current stage of training.\n7. Compute the per-sample loss using the softplus function: `loss = softplus(margin - delta_logp)`.\n8. Return the weighted mean of the loss over the batch.", "hyperparams": {"alpha": 1.0, "temp_cost": 10.0, "beta": 1.0, "rank_shift_start": 1.0, "rank_shift_end": 5.0, "anneal_steps": 10000}, "operators_used": ["softplus", "sigmoid", "rank_gap"], "implementation_hint": {"expects": ["cost_w", "cost_l", "logp_w", "logp_l"], "returns": "scalar"}, "code": "import torch\nimport torch.nn.functional as F\n\ndef generated_loss(batch, model_output, extra):\n    \"\"\"\n    Computes the Sigmoid-Gated Adaptive Margin Loss with Rank-Gap Annealing.\n\n    Inherits:\n    - The core `softplus(margin - delta_logp)` structure from both parents.\n    - The use of rank differences from Parent 1.\n    - The use of raw cost differences for margin scaling from Parent 2.\n\n    New Couplings:\n    1. Sigmoid Gating: The margin is gated by a sigmoid function of the rank difference,\n       suppressing the margin for pairs that are close in rank.\n    2. Rank-Gap Annealing: The threshold for this gating (`rank_shift`) is annealed over\n       training, creating a curriculum that starts with easy distinctions and moves to harder ones.\n    \"\"\"\n    cost_w = torch.min(batch['cost_a'], batch['cost_b'])\n    cost_l = torch.max(batch['cost_a'], batch['cost_b'])\n    log_prob_w = batch['log_prob_w']\n    log_prob_l = batch['log_prob_l']\n\n    # Retrieve hyperparameters\n    alpha = extra.get('alpha', 1.0)\n    temp_cost = extra.get('temp_cost', 10.0)\n    beta = extra.get('beta', 1.0)\n    rank_shift_start = extra.get('rank_shift_start', 1.0)\n    rank_shift_end = extra.get('rank_shift_end', 5.0)\n    anneal_steps = extra.get('anneal_steps', 10000)\n\n    # Calculate cost and log probability differences\n    delta_cost = cost_l - cost_w\n    delta_logp = log_prob_w - log_prob_l\n\n    # Calculate rank difference (Inherited from Parent 1)\n    all_costs = torch.cat([cost_w, cost_l], dim=0)\n    # Use a stable sort for deterministic ranks\n    sorted_costs, _ = torch.sort(all_costs, stable=True)\n    rank_w = torch.searchsorted(sorted_costs, cost_w)\n    rank_l = torch.searchsorted(sorted_costs, cost_l)\n    rank_diff = (rank_l - rank_w).float()\n\n    # New Coupling 1: Dynamic Rank-Gap Annealing\n    # Get the current training step, default to 0 if not available\n    current_step = extra.get('current_step', 0)\n    progress = min(1.0, current_step / anneal_steps)\n    current_rank_shift = rank_shift_start + (rank_shift_end - rank_shift_start) * progress\n\n    # New Coupling 2: Sigmoid Gating Mechanism\n    # The gate value approaches 0 for pairs with rank_diff << current_rank_shift,\n    # and 1 for pairs with rank_diff >> current_rank_shift.\n    gate = torch.sigmoid(beta * (rank_diff - current_rank_shift))\n\n    # Calculate the adaptive margin\n    # The base margin is proportional to delta_cost (Inherited from Parent 2)\n    base_margin = alpha * (delta_cost / temp_cost)\n    # The final margin is gated by the rank difference\n    adaptive_margin = base_margin * gate.detach() # Detach gate to act as a pure weight\n\n    # Compute the core loss using softplus (Inherited from both)\n    loss = F.softplus(adaptive_margin - delta_logp)\n\n    # Apply optional weights and compute the mean\n    weights = batch.get('weight', None)\n    if weights is not None:\n        loss = loss * weights\n        return loss.sum() / weights.sum().clamp(min=1e-8)\n    else:\n        return loss.mean()"}, "fitness": {"hf_like_score": 33.407920837402344, "validation_objective": 33.39936828613281, "generalization_penalty": 0.00855255126953125, "generalization_objectives": {"100": 33.407920837402344}, "train_score_mean": 38.69051497650146, "train_loss_mean": 1.4474124035835265, "pair_count": 129023934, "config": {"hf": {"problem": "tsp", "hf_steps": 1000, "train_problem_size": 100, "valid_problem_sizes": [100], "train_batch_size": 64, "pomo_size": 64, "learning_rate": 0.0003, "weight_decay": 1e-06, "alpha": 0.05, "device": "cuda", "seed": 1234, "num_validation_episodes": 128, "validation_batch_size": 64, "generalization_penalty_weight": 1.0, "pool_version": "v0"}, "free_loss": {"f1_steps": 1000, "f2_steps": 100, "f3_enabled": false}}, "loss_ir": {"name": "Sigmoid-Gated Adaptive Margin Loss with Rank-Gap Annealing", "intuition": "This loss function creates an adaptive preference margin by blending ideas from its parents and introducing novel coupling mechanisms for stability and improved learning dynamics.\n\nInherited Ideas:\n- From both parents, it inherits the core structure of a margin-based preference loss: `loss = softplus(margin - delta_logp)`. This smooth hinge loss penalizes the model when the log-probability difference between the winning and losing solutions falls short of a target margin.\n- From Parent 1 (`Adaptive Margin Loss with Z-Score Normalization and Rank Gap`), it inherits the idea of using the batch-wise rank gap between solutions (`rank_diff`) as an important signal. This recognizes that pairs with a larger difference in quality should be treated differently.\n- From Parent 2 (`Softplus Adaptive Margin Loss`), it inherits the use of the raw cost difference (`delta_cost`) to directly influence the margin, ensuring that larger cost gaps demand a larger log-probability separation.\n\nNew Coupling Ideas:\n1.  **Sigmoid Gating Mechanism**: The margin is a product of two components: a base margin derived from the cost difference and a sigmoid gate controlled by the rank difference. The margin is `alpha * (delta_cost / temp_cost) * sigmoid(beta * (rank_diff - rank_shift))`. This gating mechanism ensures that pairs with a very small rank difference (i.e., they are close neighbors in the sorted batch) have their target margin significantly down-weighted. This prevents the model from being aggressively penalized for small preference errors on nearly-indistinguishable pairs, focusing its capacity on learning more significant preference orderings first.\n2.  **Dynamic Rank-Gap Annealing**: The `rank_shift` hyperparameter in the sigmoid gate is not fixed but is scheduled to increase over training steps. It starts at a low value (e.g., 1.0), meaning only pairs that are immediately adjacent in rank get down-weighted. As training progresses, `rank_shift` increases, gradually raising the bar and requiring the model to distinguish between pairs that are further apart in rank. This annealing schedule acts as a curriculum, starting with easy distinctions and moving to harder ones, which can stabilize early training and lead to better final performance.", "hyperparams": {"alpha": 1.0, "temp_cost": 10.0, "beta": 1.0, "rank_shift_start": 1.0, "rank_shift_end": 5.0, "anneal_steps": 10000}, "operators_used": ["softplus", "sigmoid", "rank_gap"]}}, "better_than_baseline": false}
{"generation": 7, "index": 3, "ir": {"name": "Rank-Weighted Bradley-Terry Loss with Softplus Cost Scaling", "intuition": "This loss function blends the probabilistic foundation of Bradley-Terry models with adaptive, rank-aware weighting, inheriting key ideas from both parents while introducing new couplings for stability and expressiveness.\n\nInherited Ideas:\n- From the 'Softplus Adaptive Margin Loss' (Parent 1), it inherits the use of `softplus(delta_cost / temp_cost)` to create a smooth, non-saturating, and non-negative signal from the cost difference. This ensures that larger cost gaps translate into stronger learning signals without being overly sensitive to extreme outliers.\n- From the 'Adaptive Margin Loss with Z-Score Normalization and Rank Gap' (Parent 0), it inherits the concept of using a rank-based weighting scheme. This `rank_gap` term up-weights the loss for pairs that are far apart in the batch's cost ranking, focusing the model's attention on learning the most significant relative orderings.\n\nNew Coupling Ideas:\n1. **Bradley-Terry Core with LogSigmoid**: Instead of a margin-based loss like `softplus(margin - delta_logp)`, the core of this loss is based on the Bradley-Terry model: `-logsigmoid(delta_logp)`. This frames the problem as maximizing the log-likelihood of the observed preference, which is a common and stable objective for preference learning.\n2. **Coupling Cost Signal and Rank Gap as a Composite Weight**: The `softplus` cost signal and the `rank_gap` are not used to form a margin. Instead, they are multiplied together (`cost_signal * rank_gap`) to form a composite, adaptive weight for the core `-logsigmoid` loss. This new coupling means the loss for a given pair is amplified based on both its absolute cost difference (via `softplus`) and its relative importance within the batch's ranking (via `rank_gap`). This provides a richer, more nuanced supervision signal than either component alone.", "pseudocode": "1. For each pair (w, l) in the batch, where cost(w) < cost(l), calculate the cost difference `delta_cost = cost(l) - cost(w)` and the log probability difference `delta_logp = logp(w) - logp(l)`.\n2. Compute the base preference loss for each pair using the Bradley-Terry model: `base_loss = -logsigmoid(delta_logp)`.\n3. Inherited from Parent 1: Calculate a smooth, non-negative cost signal by applying `softplus` to the scaled cost difference: `cost_signal = softplus(delta_cost / temp_cost)`.\n4. Inherited from Parent 0: Calculate a 'rank gap' weight. First, find the rank of `cost_w` and `cost_l` within the sorted costs of the entire batch. Then compute `rank_gap = 1.0 - exp(-beta * abs(rank(cost_l) - rank(cost_w)))`.\n5. New Coupling: Create a composite adaptive weight by multiplying the cost signal and the rank gap: `adaptive_weight = cost_signal * rank_gap`.\n6. Modulate the base loss with this composite weight: `final_loss = base_loss * adaptive_weight`.\n7. Return the weighted mean of `final_loss` over the batch.", "hyperparams": {"temp_cost": 1.0, "beta": 0.1}, "operators_used": ["logsigmoid", "softplus", "exp", "rank_gap"], "implementation_hint": {"expects": ["cost_a", "cost_b", "logp_a", "logp_b"], "returns": "scalar"}, "code": "import torch\nimport torch.nn.functional as F\n\ndef generated_loss(batch, model_output, extra):\n    \"\"\"\n    Computes a Rank-Weighted Bradley-Terry Loss with Softplus Cost Scaling.\n\n    Inherits:\n    - The use of `softplus(delta_cost)` for a smooth, non-saturating cost signal (from Parent 1).\n    - The use of a `rank_gap` term to up-weight pairs that are far apart in the batch's cost ranking (from Parent 0).\n\n    New Couplings:\n    1. Uses a Bradley-Terry `-logsigmoid(delta_logp)` core instead of a margin-based loss.\n    2. Couples the cost signal and rank gap by multiplying them to form a single adaptive weight, which then modulates the core loss.\n    \"\"\"\n    cost_w = torch.min(batch['cost_a'], batch['cost_b'])\n    cost_l = torch.max(batch['cost_a'], batch['cost_b'])\n    log_prob_w = batch['log_prob_w']\n    log_prob_l = batch['log_prob_l']\n\n    # Retrieve hyperparameters\n    temp_cost = extra.get('temp_cost', 1.0)\n    beta = extra.get('beta', 0.1)\n\n    # 1. Calculate cost and log probability differences\n    delta_cost = cost_l - cost_w\n    delta_logp = log_prob_w - log_prob_l\n\n    # 2. New Coupling: Use Bradley-Terry model as the core loss\n    # This is equivalent to binary cross-entropy on the preference prediction.\n    base_loss = -F.logsigmoid(delta_logp)\n\n    # 3. Inherited Idea (Parent 1): Create a smooth, non-saturating cost signal\n    cost_signal = F.softplus(delta_cost / temp_cost)\n\n    # 4. Inherited Idea (Parent 0): Calculate rank gap modulation\n    all_costs = torch.cat([cost_w, cost_l], dim=0)\n    # Use torch.sort for differentiability, although ranks themselves are discrete.\n    sorted_costs, _ = torch.sort(all_costs)\n    \n    # Find the rank of each cost_w and cost_l in the sorted list\n    rank_w = torch.searchsorted(sorted_costs, cost_w)\n    rank_l = torch.searchsorted(sorted_costs, cost_l)\n    \n    rank_diff = (rank_l - rank_w).float()\n    \n    # The rank_gap approaches 1 as rank_diff increases, and is 0 if ranks are the same.\n    rank_gap = 1.0 - torch.exp(-beta * rank_diff)\n\n    # 5. New Coupling: Combine cost signal and rank gap into a composite weight\n    # Detach the weight to prevent gradients from flowing through the weighting scheme\n    # back to the model parameters, treating it as a pure loss-shaping mechanism.\n    adaptive_weight = (cost_signal * rank_gap).detach()\n\n    # 6. Modulate the base loss with the composite adaptive weight\n    final_loss = base_loss * adaptive_weight\n\n    # 7. Apply optional weights and compute the mean.\n    weights = batch.get('weight', None)\n    if weights is not None:\n        final_loss = final_loss * weights\n        return final_loss.sum() / weights.sum().clamp(min=1e-8)\n    else:\n        return final_loss.mean()"}, "fitness": {"hf_like_score": 36.94845962524414, "validation_objective": 36.416561126708984, "generalization_penalty": 0.5318984985351562, "generalization_objectives": {"100": 36.94845962524414}, "train_score_mean": 37.913696245193485, "train_loss_mean": 2.3548098022937776, "pair_count": 129023901, "config": {"hf": {"problem": "tsp", "hf_steps": 1000, "train_problem_size": 100, "valid_problem_sizes": [100], "train_batch_size": 64, "pomo_size": 64, "learning_rate": 0.0003, "weight_decay": 1e-06, "alpha": 0.05, "device": "cuda", "seed": 1234, "num_validation_episodes": 128, "validation_batch_size": 64, "generalization_penalty_weight": 1.0, "pool_version": "v0"}, "free_loss": {"f1_steps": 1000, "f2_steps": 100, "f3_enabled": false}}, "loss_ir": {"name": "Rank-Weighted Bradley-Terry Loss with Softplus Cost Scaling", "intuition": "This loss function blends the probabilistic foundation of Bradley-Terry models with adaptive, rank-aware weighting, inheriting key ideas from both parents while introducing new couplings for stability and expressiveness.\n\nInherited Ideas:\n- From the 'Softplus Adaptive Margin Loss' (Parent 1), it inherits the use of `softplus(delta_cost / temp_cost)` to create a smooth, non-saturating, and non-negative signal from the cost difference. This ensures that larger cost gaps translate into stronger learning signals without being overly sensitive to extreme outliers.\n- From the 'Adaptive Margin Loss with Z-Score Normalization and Rank Gap' (Parent 0), it inherits the concept of using a rank-based weighting scheme. This `rank_gap` term up-weights the loss for pairs that are far apart in the batch's cost ranking, focusing the model's attention on learning the most significant relative orderings.\n\nNew Coupling Ideas:\n1. **Bradley-Terry Core with LogSigmoid**: Instead of a margin-based loss like `softplus(margin - delta_logp)`, the core of this loss is based on the Bradley-Terry model: `-logsigmoid(delta_logp)`. This frames the problem as maximizing the log-likelihood of the observed preference, which is a common and stable objective for preference learning.\n2. **Coupling Cost Signal and Rank Gap as a Composite Weight**: The `softplus` cost signal and the `rank_gap` are not used to form a margin. Instead, they are multiplied together (`cost_signal * rank_gap`) to form a composite, adaptive weight for the core `-logsigmoid` loss. This new coupling means the loss for a given pair is amplified based on both its absolute cost difference (via `softplus`) and its relative importance within the batch's ranking (via `rank_gap`). This provides a richer, more nuanced supervision signal than either component alone.", "hyperparams": {"temp_cost": 1.0, "beta": 0.1}, "operators_used": ["logsigmoid", "softplus", "exp", "rank_gap"]}}, "better_than_baseline": false}
{"generation": 7, "index": 4, "ir": {"name": "Rank-Modulated Log-Ratio Loss", "intuition": "This loss function combines the core preference structure of a log-ratio loss with adaptive, rank-based modulation.\n\nInherited Ideas:\n- From Parent 1 (Adaptive Margin Loss with Z-Score Normalization and Rank Gap), it inherits the idea of using a rank-based weighting scheme. The loss for pairs that are far apart in the batch's cost ranking is amplified, focusing the model's attention on learning coarse-grained preferences first.\n- From Parent 2 (Softplus Adaptive Margin Loss), it inherits the use of `softplus` to transform the cost difference (`delta_cost`) into a smooth, non-negative, and non-saturating signal. This ensures that larger cost differences consistently translate to a stronger learning signal.\n\nThe overall loss structure is a log-ratio loss, `log(1 + exp(margin - delta_logp))`, which is equivalent to `softplus(margin - delta_logp)`. This is a common and stable formulation for preference learning.\n\nNew Coupling Ideas:\n1.  **Dynamic Margin Scaling**: The margin is calculated as `margin = softplus(delta_cost / temp_cost)`. This makes the target separation in log-probabilities smoothly and non-linearly increase with the cost difference, avoiding the saturation of `tanh` (from Parent 1) while still being non-negative.\n2.  **Rank-Modulated Log-Probabilities**: Instead of modulating the final loss value, the rank gap weight is applied directly to the log-probability difference (`delta_logp`). The new term becomes `rank_gap * delta_logp`. This reframes the problem: for pairs that are far apart in rank, the model's perceived log-probability difference is amplified, effectively giving the model 'credit' for getting the easy examples right and encouraging it to focus its capacity on harder, closer-ranked pairs where the margin is not yet met. This subtle change shifts the modulation from being a simple loss weight to an integral part of the preference comparison itself.", "pseudocode": "1. For each pair (w, l) in the batch, where cost(w) < cost(l), calculate the cost difference `delta_cost = cost(l) - cost(w)` and the log probability difference `delta_logp = logp(w) - logp(l)`.\n2. Inherited from Parent 2: Create a smooth, non-saturating 'cost signal' by applying a softplus function to the scaled cost difference: `cost_signal = softplus(delta_cost / temp_cost)`.\n3. New Coupling Idea 1: Define the target margin directly from this cost signal: `margin = alpha * cost_signal`.\n4. Inherited from Parent 1: Calculate a 'rank gap' weight for each pair. First, find the rank of `cost_w` and `cost_l` within the sorted costs of the entire batch. Then compute `rank_gap_weight = 1.0 - exp(-beta * abs(rank(cost_l) - rank(cost_w)))`.\n5. New Coupling Idea 2: Modulate the model's log-probability difference directly with the rank gap weight: `modulated_delta_logp = rank_gap_weight * delta_logp`.\n6. Compute the per-sample loss using a log-ratio (or softplus) formulation: `loss = softplus(margin - modulated_delta_logp)`.\n7. Return the weighted mean of the loss over the batch.", "hyperparams": {"alpha": 1.0, "temp_cost": 1.0, "beta": 0.05}, "operators_used": ["softplus", "exp", "log"], "implementation_hint": {"expects": ["cost_a", "cost_b", "log_prob_w", "log_prob_l", "weight"], "returns": "scalar"}, "code": "import torch\nimport torch.nn.functional as F\n\ndef generated_loss(batch, model_output, extra):\n    \"\"\"\n    Computes the Rank-Modulated Log-Ratio Loss.\n\n    Inherits rank-based weighting from Parent 1 and the use of softplus on cost differences from Parent 2.\n    Introduces two new couplings:\n    1. A dynamic margin based directly on the non-saturating softplus of the cost difference.\n    2. A modulation of the log-probability difference by the rank gap, changing the preference term itself.\n    \"\"\"\n    # For preference learning, we assume the dataset provides pairs where one is strictly better.\n    cost_w = torch.min(batch['cost_a'], batch['cost_b'])\n    cost_l = torch.max(batch['cost_a'], batch['cost_b'])\n    log_prob_w = batch['log_prob_w']\n    log_prob_l = batch['log_prob_l']\n\n    # Retrieve hyperparameters\n    alpha = extra.get('alpha', 1.0)\n    temp_cost = extra.get('temp_cost', 1.0)\n    beta = extra.get('beta', 0.05)\n\n    # 1. Calculate cost and log probability differences\n    # delta_cost is guaranteed to be >= 0\n    delta_cost = cost_l - cost_w\n    delta_logp = log_prob_w - log_prob_l\n\n    # 2. Inherited from Parent 2: Create a non-saturating cost signal\n    cost_signal = F.softplus(delta_cost / temp_cost)\n\n    # 3. New Coupling Idea 1: Define the target margin from the cost signal\n    margin = alpha * cost_signal\n\n    # 4. Inherited from Parent 1: Calculate rank gap modulation weight\n    # This requires ranking all costs in the batch\n    all_costs = torch.cat([cost_w, cost_l], dim=0)\n    # Use unique_consecutive to handle ties in ranking gracefully\n    sorted_costs = torch.unique_consecutive(all_costs.sort().values)\n\n    # Find the rank of each cost_w and cost_l in the sorted unique list\n    rank_w = torch.searchsorted(sorted_costs, cost_w, right=True) - 1\n    rank_l = torch.searchsorted(sorted_costs, cost_l, right=True) - 1\n\n    rank_diff = (rank_l - rank_w).float().clamp(min=0)\n\n    # The rank_gap_weight approaches 1 as rank_diff increases, and is 0 if ranks are the same.\n    rank_gap_weight = 1.0 - torch.exp(-beta * rank_diff)\n\n    # 5. New Coupling Idea 2: Modulate delta_logp directly\n    # This amplifies the model's perceived logp difference for widely separated pairs.\n    modulated_delta_logp = rank_gap_weight.detach() * delta_logp\n\n    # 6. Compute the core loss using softplus (equivalent to log(1 + exp(...)))\n    # The loss is low if modulated_delta_logp > margin.\n    loss_per_sample = F.softplus(margin - modulated_delta_logp)\n\n    # 7. Apply optional weights and compute the mean.\n    weights = batch.get('weight', None)\n    if weights is not None:\n        loss_per_sample = loss_per_sample * weights\n        return loss_per_sample.sum() / weights.sum().clamp(min=1e-8)\n    else:\n        return loss_per_sample.mean()"}, "fitness": {"hf_like_score": 28.613067626953125, "validation_objective": 27.96381950378418, "generalization_penalty": 0.6492481231689453, "generalization_objectives": {"100": 28.613067626953125}, "train_score_mean": 39.13591652011871, "train_loss_mean": 1.470458147406578, "pair_count": 129023916, "config": {"hf": {"problem": "tsp", "hf_steps": 1000, "train_problem_size": 100, "valid_problem_sizes": [100], "train_batch_size": 64, "pomo_size": 64, "learning_rate": 0.0003, "weight_decay": 1e-06, "alpha": 0.05, "device": "cuda", "seed": 1234, "num_validation_episodes": 128, "validation_batch_size": 64, "generalization_penalty_weight": 1.0, "pool_version": "v0"}, "free_loss": {"f1_steps": 1000, "f2_steps": 100, "f3_enabled": false}}, "loss_ir": {"name": "Rank-Modulated Log-Ratio Loss", "intuition": "This loss function combines the core preference structure of a log-ratio loss with adaptive, rank-based modulation.\n\nInherited Ideas:\n- From Parent 1 (Adaptive Margin Loss with Z-Score Normalization and Rank Gap), it inherits the idea of using a rank-based weighting scheme. The loss for pairs that are far apart in the batch's cost ranking is amplified, focusing the model's attention on learning coarse-grained preferences first.\n- From Parent 2 (Softplus Adaptive Margin Loss), it inherits the use of `softplus` to transform the cost difference (`delta_cost`) into a smooth, non-negative, and non-saturating signal. This ensures that larger cost differences consistently translate to a stronger learning signal.\n\nThe overall loss structure is a log-ratio loss, `log(1 + exp(margin - delta_logp))`, which is equivalent to `softplus(margin - delta_logp)`. This is a common and stable formulation for preference learning.\n\nNew Coupling Ideas:\n1.  **Dynamic Margin Scaling**: The margin is calculated as `margin = softplus(delta_cost / temp_cost)`. This makes the target separation in log-probabilities smoothly and non-linearly increase with the cost difference, avoiding the saturation of `tanh` (from Parent 1) while still being non-negative.\n2.  **Rank-Modulated Log-Probabilities**: Instead of modulating the final loss value, the rank gap weight is applied directly to the log-probability difference (`delta_logp`). The new term becomes `rank_gap * delta_logp`. This reframes the problem: for pairs that are far apart in rank, the model's perceived log-probability difference is amplified, effectively giving the model 'credit' for getting the easy examples right and encouraging it to focus its capacity on harder, closer-ranked pairs where the margin is not yet met. This subtle change shifts the modulation from being a simple loss weight to an integral part of the preference comparison itself.", "hyperparams": {"alpha": 1.0, "temp_cost": 1.0, "beta": 0.05}, "operators_used": ["softplus", "exp", "log"]}}, "better_than_baseline": false}
{"generation": 7, "index": 5, "ir": {"name": "Rank-Aware Log-Sigmoid Preference Loss", "intuition": "This loss function creates a robust, rank-aware preference signal by combining a classic log-sigmoid structure with adaptive, batch-normalized cost information.\n\nInherited Ideas:\n- From 'Softplus Adaptive Margin Loss' (Parent 1), it inherits the idea of using the raw cost difference `delta_cost` to create a signal that scales the loss. Instead of forming a margin inside a `softplus`, this cost signal now directly scales the log-sigmoid loss term, making pairs with larger cost differences contribute more to the gradient.\n- From 'Adaptive Margin Loss with Z-Score Normalization' (Parent 0), it inherits the technique of using Z-score normalization on the batch-wise `delta_cost`. This makes the scaling factor invariant to the absolute scale of costs in a given batch, improving stability and reducing hyperparameter sensitivity.\n\nNew Coupling Ideas:\n1.  **Log-Sigmoid Core with Adaptive Scaling**: The core of the loss is `logsigmoid(delta_logp)`. However, instead of a simple margin, this term is multiplied by an adaptive weight derived from the cost difference. The final loss is `-weight * logsigmoid(delta_logp)`. This structure is inspired by Bradley-Terry models but is enhanced with dynamic, cost-aware weighting.\n2.  **Softplus-Gated Rank Gap**: The loss incorporates a `rank_gap` term, similar to Parent 0, which emphasizes pairs that are far apart in the batch's cost ranking. The novel coupling is that this rank gap is passed through a `softplus` function. This creates a smooth, non-negative, and non-saturating gate that starts at `log(2)` for items with the same rank and grows approximately linearly as the rank difference increases. This provides a more robust and continuously increasing emphasis on widely separated pairs compared to the `1 - exp(-x)` formulation.", "pseudocode": "1. For each pair (w, l) in the batch where cost(w) < cost(l), calculate the cost difference `delta_cost = cost(l) - cost(w)` and the log probability difference `delta_logp = logp(w) - logp(l)`.\n2. Standardize the `delta_cost` vector for the entire batch using Z-score normalization to get `delta_cost_norm`. This is inherited from Parent 0 for scale invariance.\n3. Create a non-negative, unbounded cost signal by applying `softplus` to the normalized cost differences: `cost_signal = softplus(delta_cost_norm)`. This is inspired by the use of `softplus` in Parent 1.\n4. Calculate a 'rank gap' signal. First, find the rank of `cost_w` and `cost_l` within all sorted costs in the batch. The raw rank difference is `rank_diff = rank(cost_l) - rank(cost_w)`.\n5. Create a smooth, non-saturating rank gate by applying `softplus` to the scaled rank difference: `rank_gate = softplus(beta * rank_diff)`. This is a new coupling.\n6. Combine the cost signal and the rank gate to form a final adaptive weight for each pair: `adaptive_weight = cost_signal * rank_gate`.\n7. Compute the core preference loss for each pair using the log-sigmoid function: `base_loss = -logsigmoid(delta_logp)`.\n8. Modulate the base loss with the adaptive weight: `final_loss = adaptive_weight * base_loss`.\n9. Return the weighted mean of `final_loss` over the batch.", "hyperparams": {"beta": 0.05}, "operators_used": ["logsigmoid", "softplus", "zscore", "rank_gap"], "implementation_hint": {"expects": ["cost_a", "cost_b", "logp_a", "logp_b"], "returns": "scalar"}, "code": "import torch\nimport torch.nn.functional as F\n\ndef generated_loss(batch, model_output, extra):\n    \"\"\"\n    Computes the Rank-Aware Log-Sigmoid Preference Loss.\n\n    Inherits:\n    - The use of Z-score normalization on cost differences for scale-invariance (from Parent 0).\n    - The use of a softplus-transformed cost difference to create an adaptive, non-negative signal (from Parent 1).\n\n    Introduces:\n    - A core log-sigmoid loss, `-w * logsigmoid(delta_logp)`, where `w` is an adaptive weight.\n    - A softplus-gated rank gap, `softplus(beta * rank_diff)`, for a smooth, non-saturating emphasis on widely separated pairs.\n    \"\"\"\n    # For preference learning, we assume cost_w < cost_l.\n    cost_w = torch.min(batch['cost_a'], batch['cost_b'])\n    cost_l = torch.max(batch['cost_a'], batch['cost_b'])\n    log_prob_w = batch['log_prob_w']\n    log_prob_l = batch['log_prob_l']\n\n    # Retrieve hyperparameters\n    beta = extra.get('beta', 0.05)\n\n    # 1. Calculate cost and log probability differences\n    delta_cost = cost_l - cost_w\n    delta_logp = log_prob_w - log_prob_l\n\n    # Early exit for a batch with no cost difference to avoid division by zero in z-score\n    if torch.all(delta_cost < 1e-8):\n        return torch.tensor(0.0, device=delta_cost.device)\n\n    # 2. Inherited: Z-score normalization of cost differences for batch-wise scale invariance\n    delta_cost_mean = delta_cost.mean()\n    delta_cost_std = delta_cost.std().clamp(min=1e-8) # Avoid division by zero\n    delta_cost_norm = (delta_cost - delta_cost_mean) / delta_cost_std\n\n    # 3. Inherited: Create a non-negative, unbounded cost signal using softplus\n    cost_signal = F.softplus(delta_cost_norm)\n\n    # 4. New Coupling: Calculate rank gap\n    # This requires ranking all costs in the batch\n    all_costs = torch.cat([cost_w, cost_l], dim=0)\n    sorted_costs, _ = torch.sort(all_costs)\n    \n    # Find the rank of each cost_w and cost_l in the sorted list\n    rank_w = torch.searchsorted(sorted_costs, cost_w)\n    rank_l = torch.searchsorted(sorted_costs, cost_l)\n    rank_diff = (rank_l - rank_w).float()\n\n    # 5. New Coupling: Create a smooth, non-saturating rank gate using softplus\n    rank_gate = F.softplus(beta * rank_diff)\n    \n    # 6. Combine signals into a final adaptive weight\n    # Detach to ensure weights do not contribute to gradients w.r.t. cost prediction models, if any.\n    adaptive_weight = (cost_signal * rank_gate).detach()\n\n    # 7. New Coupling: Compute the core preference loss using log-sigmoid\n    # This term is maximized when delta_logp is large and positive.\n    base_loss = -F.logsigmoid(delta_logp)\n\n    # 8. Modulate the base loss with the adaptive weight\n    final_loss = adaptive_weight * base_loss\n\n    # 9. Apply optional weights and compute the mean.\n    weights = batch.get('weight', None)\n    if weights is not None:\n        final_loss = final_loss * weights\n        return final_loss.sum() / weights.sum().clamp(min=1e-8)\n    else:\n        return final_loss.mean()"}, "fitness": {"hf_like_score": 8.939128875732422, "validation_objective": 8.939128875732422, "generalization_penalty": 0.0, "generalization_objectives": {"100": 8.896780967712402}, "train_score_mean": 19.074743811607362, "train_loss_mean": 1075.8731213684082, "pair_count": 129023936, "config": {"hf": {"problem": "tsp", "hf_steps": 1000, "train_problem_size": 100, "valid_problem_sizes": [100], "train_batch_size": 64, "pomo_size": 64, "learning_rate": 0.0003, "weight_decay": 1e-06, "alpha": 0.05, "device": "cuda", "seed": 1234, "num_validation_episodes": 128, "validation_batch_size": 64, "generalization_penalty_weight": 1.0, "pool_version": "v0"}, "free_loss": {"f1_steps": 1000, "f2_steps": 100, "f3_enabled": false}}, "loss_ir": {"name": "Rank-Aware Log-Sigmoid Preference Loss", "intuition": "This loss function creates a robust, rank-aware preference signal by combining a classic log-sigmoid structure with adaptive, batch-normalized cost information.\n\nInherited Ideas:\n- From 'Softplus Adaptive Margin Loss' (Parent 1), it inherits the idea of using the raw cost difference `delta_cost` to create a signal that scales the loss. Instead of forming a margin inside a `softplus`, this cost signal now directly scales the log-sigmoid loss term, making pairs with larger cost differences contribute more to the gradient.\n- From 'Adaptive Margin Loss with Z-Score Normalization' (Parent 0), it inherits the technique of using Z-score normalization on the batch-wise `delta_cost`. This makes the scaling factor invariant to the absolute scale of costs in a given batch, improving stability and reducing hyperparameter sensitivity.\n\nNew Coupling Ideas:\n1.  **Log-Sigmoid Core with Adaptive Scaling**: The core of the loss is `logsigmoid(delta_logp)`. However, instead of a simple margin, this term is multiplied by an adaptive weight derived from the cost difference. The final loss is `-weight * logsigmoid(delta_logp)`. This structure is inspired by Bradley-Terry models but is enhanced with dynamic, cost-aware weighting.\n2.  **Softplus-Gated Rank Gap**: The loss incorporates a `rank_gap` term, similar to Parent 0, which emphasizes pairs that are far apart in the batch's cost ranking. The novel coupling is that this rank gap is passed through a `softplus` function. This creates a smooth, non-negative, and non-saturating gate that starts at `log(2)` for items with the same rank and grows approximately linearly as the rank difference increases. This provides a more robust and continuously increasing emphasis on widely separated pairs compared to the `1 - exp(-x)` formulation.", "hyperparams": {"beta": 0.05}, "operators_used": ["logsigmoid", "softplus", "zscore", "rank_gap"]}}, "better_than_baseline": false}
{"generation": 7, "index": 6, "ir": {"name": "Rank-Aware LogSigmoid Loss with Softplus Margin", "intuition": "This loss function combines the core structures of its parents to create a robust, rank-aware preference loss. The design aims to be less sensitive to the absolute scale of costs while still aggressively penalizing misordered pairs that are far apart in cost.\n\nInherited Ideas:\n- From Parent 1 ('Softplus Adaptive Margin Loss'), it inherits the use of `softplus(delta_cost / temp_cost)` to create a smooth, non-saturating adaptive margin. This ensures that as the cost gap between two solutions widens, the target log-probability separation also grows, pushing the model harder to distinguish them.\n- From both parents, it inherits the fundamental structure of a margin-based loss, where the model's log-probability difference `delta_logp` is compared against a target `margin` derived from the cost difference `delta_cost`.\n\nNew Coupling Ideas:\n1.  **LogSigmoid Loss Formulation**: Instead of the common `softplus(margin - delta_logp)`, this child uses a `logsigmoid` formulation: `loss = -logsigmoid(delta_logp - margin)`. This is mathematically equivalent to `softplus(margin - delta_logp)` but can offer slightly better numerical stability. It frames the problem as maximizing the log-likelihood of the model's preference `delta_logp` being greater than the target `margin`.\n2.  **Rank Gap Modulation**: Borrowing the concept from Parent 0, the loss is modulated by a rank-gap weight. This weight, `rank_gap = 1.0 - exp(-beta * |rank(cost_l) - rank(cost_w)|)`, amplifies the loss for pairs that are far apart in the batch's cost ranking. This new coupling directs the model's attention towards correcting significant ranking errors over fine-tuning pairs that are already close in cost, adding a global, batch-aware perspective to the local pairwise comparison.", "pseudocode": "1. For each pair (w, l) in the batch where cost(w) < cost(l), calculate the cost difference `delta_cost = cost(l) - cost(w)` and the log probability difference `delta_logp = logp(w) - logp(l)`.\n2. Inherited from Parent 1: Transform the cost difference using a softplus function to create a smooth, non-saturating 'cost signal': `cost_signal = softplus(delta_cost / temp_cost)`.\n3. Calculate the adaptive margin based on this signal: `margin = alpha * cost_signal`.\n4. New Coupling 1: Compute the base per-sample loss using a numerically stable logsigmoid formulation, which is equivalent to `softplus(margin - delta_logp)`: `base_loss = -logsigmoid(delta_logp - margin)`.\n5. New Coupling 2: Calculate a 'rank gap' weight for each pair. First, find the rank of `cost_w` and `cost_l` within all sorted costs of the batch. Then compute `rank_gap_weight = 1.0 - exp(-beta * abs(rank(cost_l) - rank(cost_w)))`.\n6. Modulate the base loss with the rank gap weight: `final_loss = base_loss * rank_gap_weight`.\n7. Return the weighted mean of `final_loss` over the batch.", "hyperparams": {"alpha": 1.0, "temp_cost": 1.0, "beta": 0.05}, "operators_used": ["logsigmoid", "softplus", "exp", "rank_gap"], "implementation_hint": {"expects": ["cost_a", "cost_b", "log_prob_w", "log_prob_l", "weight"], "returns": "scalar"}, "code": "import torch\nimport torch.nn.functional as F\n\ndef generated_loss(batch, model_output, extra):\n    \"\"\"\n    Computes the Rank-Aware LogSigmoid Loss with Softplus Margin.\n\n    Inherits the softplus-based adaptive margin from Parent 1.\n    Introduces two new ideas:\n    1. A logsigmoid loss formulation, `loss = -logsigmoid(delta_logp - margin)`, which is a stable alternative to `softplus(margin - delta_logp)`.\n    2. A 'rank gap' modulator from Parent 0 that up-weights the loss for pairs that are far apart in the batch's cost ranking.\n    \"\"\"\n    # For preference learning, we assume the dataset provides pairs where one is strictly better.\n    cost_w = torch.min(batch['cost_a'], batch['cost_b'])\n    cost_l = torch.max(batch['cost_a'], batch['cost_b'])\n    log_prob_w = batch['log_prob_w']\n    log_prob_l = batch['log_prob_l']\n\n    # Retrieve hyperparameters\n    alpha = extra.get('alpha', 1.0)\n    temp_cost = extra.get('temp_cost', 1.0)\n    beta = extra.get('beta', 0.05)\n\n    # 1. Calculate cost and log probability differences\n    # delta_cost is guaranteed to be >= 0\n    delta_cost = cost_l - cost_w\n    delta_logp = log_prob_w - log_prob_l\n\n    # 2. Inherited Idea (Parent 1): Create a non-saturating cost signal using softplus.\n    cost_signal = F.softplus(delta_cost / temp_cost)\n\n    # 3. Calculate the adaptive margin.\n    adaptive_margin = alpha * cost_signal\n\n    # 4. New Coupling 1: Compute the base loss using the logsigmoid formulation.\n    # This is equivalent to softplus(adaptive_margin - delta_logp) but can be more stable.\n    base_loss = -F.logsigmoid(delta_logp - adaptive_margin)\n\n    # 5. New Coupling 2: Calculate rank gap modulation.\n    # This requires ranking all costs in the batch.\n    all_costs = torch.cat([cost_w, cost_l], dim=0)\n    # Using searchsorted on the sorted unique values is a robust way to get ranks.\n    sorted_unique_costs = torch.unique(all_costs, sorted=True)\n    \n    # Find the rank of each cost_w and cost_l in the sorted list.\n    rank_w = torch.searchsorted(sorted_unique_costs, cost_w)\n    rank_l = torch.searchsorted(sorted_unique_costs, cost_l)\n    \n    # rank_diff is guaranteed to be >= 0 because cost_l >= cost_w\n    rank_diff = (rank_l - rank_w).float()\n    \n    # The rank_gap_weight approaches 1 as rank_diff increases, and is 0 if ranks are the same.\n    rank_gap_weight = 1.0 - torch.exp(-beta * rank_diff)\n\n    # 6. Modulate the base loss with the rank gap weight.\n    # Detach the weight so it doesn't contribute to gradients w.r.t. costs of other samples.\n    final_loss = base_loss * rank_gap_weight.detach()\n\n    # 7. Apply optional weights and compute the mean.\n    weights = batch.get('weight', None)\n    if weights is not None:\n        final_loss = final_loss * weights\n        # Use a small epsilon to prevent division by zero if all weights are zero.\n        return final_loss.sum() / weights.sum().clamp(min=1e-8)\n    else:\n        return final_loss.mean()"}, "fitness": {"hf_like_score": 37.268951416015625, "validation_objective": 37.268951416015625, "generalization_penalty": 0.0, "generalization_objectives": {"100": 36.997968673706055}, "train_score_mean": 38.59020060062409, "train_loss_mean": 1.3479043372869493, "pair_count": 129023922, "config": {"hf": {"problem": "tsp", "hf_steps": 1000, "train_problem_size": 100, "valid_problem_sizes": [100], "train_batch_size": 64, "pomo_size": 64, "learning_rate": 0.0003, "weight_decay": 1e-06, "alpha": 0.05, "device": "cuda", "seed": 1234, "num_validation_episodes": 128, "validation_batch_size": 64, "generalization_penalty_weight": 1.0, "pool_version": "v0"}, "free_loss": {"f1_steps": 1000, "f2_steps": 100, "f3_enabled": false}}, "loss_ir": {"name": "Rank-Aware LogSigmoid Loss with Softplus Margin", "intuition": "This loss function combines the core structures of its parents to create a robust, rank-aware preference loss. The design aims to be less sensitive to the absolute scale of costs while still aggressively penalizing misordered pairs that are far apart in cost.\n\nInherited Ideas:\n- From Parent 1 ('Softplus Adaptive Margin Loss'), it inherits the use of `softplus(delta_cost / temp_cost)` to create a smooth, non-saturating adaptive margin. This ensures that as the cost gap between two solutions widens, the target log-probability separation also grows, pushing the model harder to distinguish them.\n- From both parents, it inherits the fundamental structure of a margin-based loss, where the model's log-probability difference `delta_logp` is compared against a target `margin` derived from the cost difference `delta_cost`.\n\nNew Coupling Ideas:\n1.  **LogSigmoid Loss Formulation**: Instead of the common `softplus(margin - delta_logp)`, this child uses a `logsigmoid` formulation: `loss = -logsigmoid(delta_logp - margin)`. This is mathematically equivalent to `softplus(margin - delta_logp)` but can offer slightly better numerical stability. It frames the problem as maximizing the log-likelihood of the model's preference `delta_logp` being greater than the target `margin`.\n2.  **Rank Gap Modulation**: Borrowing the concept from Parent 0, the loss is modulated by a rank-gap weight. This weight, `rank_gap = 1.0 - exp(-beta * |rank(cost_l) - rank(cost_w)|)`, amplifies the loss for pairs that are far apart in the batch's cost ranking. This new coupling directs the model's attention towards correcting significant ranking errors over fine-tuning pairs that are already close in cost, adding a global, batch-aware perspective to the local pairwise comparison.", "hyperparams": {"alpha": 1.0, "temp_cost": 1.0, "beta": 0.05}, "operators_used": ["logsigmoid", "softplus", "exp", "rank_gap"]}}, "better_than_baseline": false}
{"generation": 7, "index": 7, "ir": {"name": "Rank-Modulated Bradley-Terry Loss", "intuition": "This loss function creates a hybrid between a Bradley-Terry style preference loss and an adaptive margin loss, modulated by the relative ranking of solutions within a batch.\n\nInherited Ideas:\n- From 'Softplus Adaptive Margin Loss' (Parent 1), it inherits the use of `softplus` on the cost difference (`delta_cost`) to create a smooth, non-saturating signal that defines an adaptive margin. This ensures that pairs with larger cost differences are pushed apart more strongly.\n- From 'Adaptive Margin Loss with Z-Score Normalization and Rank Gap' (Parent 2), it inherits the idea of using the rank difference between the winner and loser costs within a batch to modulate the loss. This 'rank gap' focuses the model's attention on learning the relative ordering of solutions that are significantly far apart in the batch's cost landscape.\n\nNew Coupling Ideas:\n1.  **Bradley-Terry Core with Adaptive Margin**: Instead of the common `softplus(margin - delta_logp)` structure, this loss adopts a Bradley-Terry-like form: `logsigmoid(delta_logp)`. The adaptive margin, derived from the `softplus` of the cost difference, is not used to set a target for `delta_logp`, but rather to scale the entire logsigmoid loss. This means the loss for a mis-ordered pair (`delta_logp < 0`) is amplified proportionally to how different their costs are, but it does not penalize pairs that already exceed a certain margin.\n2.  **Combined Modulation**: The final loss is modulated by the product of the adaptive margin (from costs) and the rank gap weight (from ranks). The expression `(1 + adaptive_margin) * rank_gap_weight` serves as a dynamic, per-sample loss weight. The `1 + ...` term ensures that even pairs with zero cost difference still contribute to the loss (governed by the rank gap), preventing the loss from vanishing for identical-cost pairs.", "pseudocode": "1. For each pair (w, l) in the batch, where cost(w) < cost(l), calculate the cost difference `delta_cost = cost(l) - cost(w)` and the log probability difference `delta_logp = logp(w) - logp(l)`.\n2. Compute the base preference loss using a Bradley-Terry style formulation: `base_loss = -logsigmoid(delta_logp)`. This penalizes cases where `logp(w)` is not sufficiently larger than `logp(l)`.\n3. Inherited Idea 1: Calculate a smooth, non-saturating 'cost signal' by applying softplus to the scaled cost difference: `cost_signal = softplus(delta_cost / temp_cost)`.\n4. Inherited Idea 2: Calculate a 'rank gap' weight. Find the rank of `cost_w` and `cost_l` within the sorted costs of the entire batch. Compute `rank_gap_weight = 1.0 - exp(-beta * abs(rank(l) - rank(w)))`.\n5. New Coupling: Create a combined dynamic weight for each sample. This weight is the product of the rank gap and a term derived from the cost signal: `dynamic_weight = (1.0 + alpha * cost_signal) * rank_gap_weight`.\n6. Modulate the base loss with this dynamic weight: `final_loss = base_loss * dynamic_weight`.\n7. Return the weighted mean of `final_loss` over the batch.", "hyperparams": {"alpha": 1.0, "temp_cost": 1.0, "beta": 0.1}, "operators_used": ["logsigmoid", "softplus", "exp", "rank_gap"], "implementation_hint": {"expects": ["cost_w", "cost_l", "logp_w", "logp_l"], "returns": "scalar"}, "code": "import torch\nimport torch.nn.functional as F\n\ndef generated_loss(batch, model_output, extra):\n    \"\"\"\n    Computes the Rank-Modulated Bradley-Terry Loss.\n\n    This loss combines a Bradley-Terry style preference loss (`-logsigmoid(delta_logp)`)\n    with a dynamic, per-sample weighting scheme.\n\n    Inherited Ideas:\n    - Inherits `softplus(delta_cost)` to create a smooth, non-saturating adaptive margin signal (from Parent 1).\n    - Inherits the use of a 'rank gap' to modulate the loss based on the relative ranking of costs within the batch (from Parent 2).\n\n    New Coupling Ideas:\n    - Uses the adaptive margin to scale a Bradley-Terry loss instead of setting a target in a hinge loss.\n    - Combines the cost-based margin and rank-based gap into a single multiplicative dynamic weight.\n    \"\"\"\n    # For preference learning, we assume the dataset provides pairs where one is strictly better.\n    cost_w = torch.min(batch['cost_a'], batch['cost_b'])\n    cost_l = torch.max(batch['cost_a'], batch['cost_b'])\n    log_prob_w = batch['log_prob_w']\n    log_prob_l = batch['log_prob_l']\n\n    # Retrieve hyperparameters\n    alpha = extra.get('alpha', 1.0)\n    temp_cost = extra.get('temp_cost', 1.0)\n    beta = extra.get('beta', 0.1)\n\n    # 1. Calculate cost and log probability differences\n    delta_cost = cost_l - cost_w\n    delta_logp = log_prob_w - log_prob_l\n\n    # 2. Compute the base loss using a Bradley-Terry style formulation.\n    # The negative sign makes it a loss to be minimized.\n    base_loss = -F.logsigmoid(delta_logp)\n\n    # --- Create Dynamic Weight ---\n\n    # 3. Inherited Idea 1: Calculate adaptive margin signal from cost difference using softplus.\n    cost_signal = F.softplus(delta_cost / temp_cost)\n\n    # 4. Inherited Idea 2: Calculate rank gap modulation.\n    # This requires ranking all costs in the batch.\n    all_costs = torch.cat([cost_w, cost_l], dim=0)\n    sorted_costs, _ = torch.sort(all_costs)\n    \n    # Find the rank of each cost_w and cost_l in the sorted list\n    rank_w = torch.searchsorted(sorted_costs, cost_w)\n    rank_l = torch.searchsorted(sorted_costs, cost_l)\n    \n    rank_diff = (rank_l - rank_w).float()\n    \n    # The rank_gap_weight approaches 1 as rank_diff increases, and is 0 if ranks are the same.\n    rank_gap_weight = 1.0 - torch.exp(-beta * rank_diff)\n\n    # 5. New Coupling: Combine cost signal and rank gap into a single dynamic weight.\n    # The `1.0 + ...` ensures a baseline weight, preventing the weight from being zero if costs are identical.\n    # The weight is detached to ensure it only acts as a scaling factor, not a source of gradients itself.\n    dynamic_weight = ((1.0 + alpha * cost_signal) * rank_gap_weight).detach()\n\n    # 6. Modulate the base loss with the dynamic weight.\n    final_loss = base_loss * dynamic_weight\n\n    # 7. Apply optional batch weights and compute the mean.\n    weights = batch.get('weight', None)\n    if weights is not None:\n        final_loss = final_loss * weights\n        return final_loss.sum() / weights.sum().clamp(min=1e-8)\n    else:\n        return final_loss.mean()"}, "fitness": {"hf_like_score": 35.72786331176758, "validation_objective": 35.640480041503906, "generalization_penalty": 0.08738327026367188, "generalization_objectives": {"100": 35.72786331176758}, "train_score_mean": 39.13307790756225, "train_loss_mean": 1.3442570550441741, "pair_count": 129023929, "config": {"hf": {"problem": "tsp", "hf_steps": 1000, "train_problem_size": 100, "valid_problem_sizes": [100], "train_batch_size": 64, "pomo_size": 64, "learning_rate": 0.0003, "weight_decay": 1e-06, "alpha": 0.05, "device": "cuda", "seed": 1234, "num_validation_episodes": 128, "validation_batch_size": 64, "generalization_penalty_weight": 1.0, "pool_version": "v0"}, "free_loss": {"f1_steps": 1000, "f2_steps": 100, "f3_enabled": false}}, "loss_ir": {"name": "Rank-Modulated Bradley-Terry Loss", "intuition": "This loss function creates a hybrid between a Bradley-Terry style preference loss and an adaptive margin loss, modulated by the relative ranking of solutions within a batch.\n\nInherited Ideas:\n- From 'Softplus Adaptive Margin Loss' (Parent 1), it inherits the use of `softplus` on the cost difference (`delta_cost`) to create a smooth, non-saturating signal that defines an adaptive margin. This ensures that pairs with larger cost differences are pushed apart more strongly.\n- From 'Adaptive Margin Loss with Z-Score Normalization and Rank Gap' (Parent 2), it inherits the idea of using the rank difference between the winner and loser costs within a batch to modulate the loss. This 'rank gap' focuses the model's attention on learning the relative ordering of solutions that are significantly far apart in the batch's cost landscape.\n\nNew Coupling Ideas:\n1.  **Bradley-Terry Core with Adaptive Margin**: Instead of the common `softplus(margin - delta_logp)` structure, this loss adopts a Bradley-Terry-like form: `logsigmoid(delta_logp)`. The adaptive margin, derived from the `softplus` of the cost difference, is not used to set a target for `delta_logp`, but rather to scale the entire logsigmoid loss. This means the loss for a mis-ordered pair (`delta_logp < 0`) is amplified proportionally to how different their costs are, but it does not penalize pairs that already exceed a certain margin.\n2.  **Combined Modulation**: The final loss is modulated by the product of the adaptive margin (from costs) and the rank gap weight (from ranks). The expression `(1 + adaptive_margin) * rank_gap_weight` serves as a dynamic, per-sample loss weight. The `1 + ...` term ensures that even pairs with zero cost difference still contribute to the loss (governed by the rank gap), preventing the loss from vanishing for identical-cost pairs.", "hyperparams": {"alpha": 1.0, "temp_cost": 1.0, "beta": 0.1}, "operators_used": ["logsigmoid", "softplus", "exp", "rank_gap"]}}, "better_than_baseline": false}
{"generation": 8, "index": 0, "ir": {"name": "Softplus-Gated Log-Sigmoid Loss with Rank-Gap Margin", "intuition": "This loss function combines the classic log-sigmoid structure with a dynamic, rank-aware margin and a smooth gating mechanism to create a stable and adaptive preference signal.\n\nInherited Ideas:\n- From 'Rank-Aware Log-Sigmoid Preference Loss' (Parent 1), it inherits the core loss structure of `-logsigmoid(...)`, which is a standard and well-behaved loss for binary preference tasks. It also inherits the idea of using the rank difference between pairs to modulate the loss, focusing more on pairs that are widely separated in the batch's cost distribution.\n- From 'Sigmoid-Gated Adaptive Margin Loss with Rank-Gap Normalization' (Parent 0), it inherits the concept of creating an adaptive margin based on the cost difference (`delta_cost`). This margin is then used to shift the log-sigmoid function, setting a target separation for the model's log probabilities.\n\nNew Coupling Ideas:\n1.  **Rank-Gap as Margin**: Instead of using `softplus(delta_cost)` as the margin, this loss uses the rank difference directly. The margin is defined as `beta * rank_gap_normalized`. This couples the target log-probability separation directly to the relative importance of the pair within the batch, rather than their absolute cost difference. This makes the loss more robust to cost scaling and focuses on relative ordering.\n2.  **Softplus Gating on Loss Term**: The entire loss term, `margin - delta_logp`, is passed through a `softplus` function before being fed into the `-logsigmoid`. The loss becomes `-logsigmoid(softplus(margin - delta_logp))`. This acts as a smooth, non-negative gate. When the model is correct (`delta_logp > margin`), the input to `softplus` is negative and close to zero, resulting in a small loss. When the model is incorrect (`delta_logp < margin`), the `softplus` acts like a linear function, producing a large positive input to `-logsigmoid` and thus a large loss. This gating prevents the loss from becoming negative and stabilizes training by ensuring the argument to `logsigmoid` is always non-negative.", "pseudocode": "1. For each pair (w, l) where cost(w) < cost(l), calculate the log probability difference `delta_logp = logp(w) - logp(l)`.\n2. Inherit from Parent 1: Determine the cost ranks for all `cost_w` and `cost_l` in the batch. Calculate the rank difference `rank_diff = rank(cost_l) - rank(cost_w)`.\n3. New Coupling (Rank-Gap as Margin): Normalize the rank difference by the batch size. Create an adaptive margin directly from this normalized rank gap: `margin = beta * (rank_diff / batch_size)`.\n4. New Coupling (Softplus Gating): Calculate the difference between the target margin and the model's output: `diff = margin - delta_logp`. Apply a softplus gate to this difference to ensure it's non-negative and smooth: `gated_diff = softplus(diff)`.\n5. Inherit from Parent 1: Compute the core per-sample loss using the `-logsigmoid` structure, but with the gated difference as its argument: `loss = -logsigmoid(gated_diff)`.\n6. Return the weighted mean of the per-sample losses over the batch.", "hyperparams": {"beta": 1.0}, "operators_used": ["logsigmoid", "softplus", "rank_gap"], "implementation_hint": {"expects": ["cost_a", "cost_b", "logp_w", "logp_l"], "returns": "scalar"}, "code": "import torch\nimport torch.nn.functional as F\n\ndef generated_loss(batch, model_output, extra):\n    \"\"\"\n    Computes the Softplus-Gated Log-Sigmoid Loss with Rank-Gap Margin.\n\n    Inherits:\n    - The core `-logsigmoid` loss structure (from Parent 1).\n    - The use of rank differences to create a batch-aware signal (from both Parents).\n\n    Introduces:\n    - A new coupling where the normalized rank gap directly forms the margin.\n    - A `softplus` gate on the `margin - delta_logp` term to ensure stability and a non-negative input to the `logsigmoid`.\n    \"\"\"\n    # For preference learning, we assume cost_w < cost_l.\n    cost_w = torch.min(batch['cost_a'], batch['cost_b'])\n    cost_l = torch.max(batch['cost_a'], batch['cost_b'])\n    log_prob_w = batch['log_prob_w']\n    log_prob_l = batch['log_prob_l']\n\n    # Retrieve hyperparameters\n    beta = extra.get('beta', 1.0)\n\n    # 1. Calculate log probability difference\n    delta_logp = log_prob_w - log_prob_l\n    batch_size = delta_logp.shape[0]\n\n    # 2. Inherited: Calculate rank differences\n    # Combine all costs to establish a batch-wide ranking.\n    all_costs = torch.cat([cost_w, cost_l], dim=0)\n    sorted_costs, _ = torch.sort(all_costs)\n    \n    # Find ranks efficiently using searchsorted.\n    rank_w = torch.searchsorted(sorted_costs, cost_w)\n    rank_l = torch.searchsorted(sorted_costs, cost_l)\n    rank_diff = (rank_l - rank_w).float()\n\n    # 3. New Coupling: Rank-Gap as Margin\n    # The margin is proportional to how far apart the items are in the batch's cost ranking.\n    # Normalizing by 2*batch_size (total items) makes it scale-invariant.\n    # Detach to treat the margin as a fixed target for the current step.\n    adaptive_margin = (beta * rank_diff / (2 * batch_size)).detach()\n\n    # 4. New Coupling: Softplus Gating\n    # The term `margin - delta_logp` represents how much the model misses the target separation.\n    # Applying softplus ensures the input to logsigmoid is non-negative, enhancing stability.\n    # It smoothly transitions from near-zero (when model is correct) to linear (when incorrect).\n    gated_diff = F.softplus(adaptive_margin - delta_logp)\n\n    # 5. Inherited: Compute the core loss with the gated difference\n    # This is equivalent to log(1 + exp(-gated_diff)), which is always well-behaved.\n    loss = -F.logsigmoid(gated_diff)\n\n    # 6. Apply optional weights and compute the mean.\n    weights = batch.get('weight', None)\n    if weights is not None:\n        loss = loss * weights\n        # Use clamp to prevent division by zero for an all-zero weight tensor\n        return loss.sum() / weights.sum().clamp(min=1e-8)\n    else:\n        return loss.mean()"}, "fitness": {"hf_like_score": 75.12211608886719, "validation_objective": 74.8690414428711, "generalization_penalty": 0.25307464599609375, "generalization_objectives": {"100": 75.12211608886719}, "train_score_mean": 74.48269640350341, "train_loss_mean": 0.27761865708231925, "pair_count": 129021043, "config": {"hf": {"problem": "tsp", "hf_steps": 1000, "train_problem_size": 100, "valid_problem_sizes": [100], "train_batch_size": 64, "pomo_size": 64, "learning_rate": 0.0003, "weight_decay": 1e-06, "alpha": 0.05, "device": "cuda", "seed": 1234, "num_validation_episodes": 128, "validation_batch_size": 64, "generalization_penalty_weight": 1.0, "pool_version": "v0"}, "free_loss": {"f1_steps": 1000, "f2_steps": 100, "f3_enabled": false}}, "loss_ir": {"name": "Softplus-Gated Log-Sigmoid Loss with Rank-Gap Margin", "intuition": "This loss function combines the classic log-sigmoid structure with a dynamic, rank-aware margin and a smooth gating mechanism to create a stable and adaptive preference signal.\n\nInherited Ideas:\n- From 'Rank-Aware Log-Sigmoid Preference Loss' (Parent 1), it inherits the core loss structure of `-logsigmoid(...)`, which is a standard and well-behaved loss for binary preference tasks. It also inherits the idea of using the rank difference between pairs to modulate the loss, focusing more on pairs that are widely separated in the batch's cost distribution.\n- From 'Sigmoid-Gated Adaptive Margin Loss with Rank-Gap Normalization' (Parent 0), it inherits the concept of creating an adaptive margin based on the cost difference (`delta_cost`). This margin is then used to shift the log-sigmoid function, setting a target separation for the model's log probabilities.\n\nNew Coupling Ideas:\n1.  **Rank-Gap as Margin**: Instead of using `softplus(delta_cost)` as the margin, this loss uses the rank difference directly. The margin is defined as `beta * rank_gap_normalized`. This couples the target log-probability separation directly to the relative importance of the pair within the batch, rather than their absolute cost difference. This makes the loss more robust to cost scaling and focuses on relative ordering.\n2.  **Softplus Gating on Loss Term**: The entire loss term, `margin - delta_logp`, is passed through a `softplus` function before being fed into the `-logsigmoid`. The loss becomes `-logsigmoid(softplus(margin - delta_logp))`. This acts as a smooth, non-negative gate. When the model is correct (`delta_logp > margin`), the input to `softplus` is negative and close to zero, resulting in a small loss. When the model is incorrect (`delta_logp < margin`), the `softplus` acts like a linear function, producing a large positive input to `-logsigmoid` and thus a large loss. This gating prevents the loss from becoming negative and stabilizes training by ensuring the argument to `logsigmoid` is always non-negative.", "hyperparams": {"beta": 1.0}, "operators_used": ["logsigmoid", "softplus", "rank_gap"]}}, "better_than_baseline": false}
{"generation": 8, "index": 1, "ir": {"name": "Rank-Normalized Bradley-Terry Loss with Confidence Gating", "intuition": "This loss function reframes the preference learning problem by directly weighting the classic Bradley-Terry loss (`-logsigmoid(delta_logp)`) with a composite, batch-aware signal. This signal is designed to be robust and focus the model on meaningful pairs.\n\nInherited Ideas:\n- From 'Rank-Aware Log-Sigmoid Preference Loss' (Parent 1), it inherits the core structure of a Bradley-Terry-like loss, `-weight * logsigmoid(delta_logp)`. This provides a probabilistic interpretation and a strong, non-saturating gradient when the model is wrong.\n- From 'Sigmoid-Gated Adaptive Margin Loss with Rank-Gap Normalization' (Parent 0), it inherits the idea of using the rank gap to directly normalize a component of the loss. Instead of normalizing the margin, it now normalizes the `delta_cost` signal itself, making the cost-based weighting adaptive to the pair's relative importance within the batch.\n\nNew Coupling Ideas:\n1.  **Rank-Gap Normalization of Cost Signal**: The `delta_cost` is first passed through a `softplus` to ensure it is non-negative and smooth. This raw signal is then divided by `1 + beta * rank_gap_normalized`. This couples the cost information with rank information directly. For pairs with a large rank gap (easy to distinguish), the denominator grows, down-weighting their contribution and allowing the model to focus on finer-grained, harder examples with small rank gaps.\n2.  **Tanh Confidence Gating**: The final loss is gated by `(1 - tanh(delta_logp))`. This gate has two effects: for pairs the model is already confident about (large positive `delta_logp`), `tanh` approaches 1, and the gate `(1 - tanh)` approaches 0, reducing the loss and preventing the model from over-optimizing on easy pairs. For pairs the model is confidently wrong about (large negative `delta_logp`), `tanh` approaches -1, and the gate approaches 2, amplifying the loss signal and forcing the model to correct its most significant errors.", "pseudocode": "1. For each pair (w, l) where cost(w) < cost(l), calculate the cost difference `delta_cost = cost(l) - cost(w)` and the log probability difference `delta_logp = logp(w) - logp(l)`.\n2. Inherit from Parent 1: Calculate the core loss term using the log-sigmoid function, `base_loss = -logsigmoid(delta_logp)`.\n3. Inherit from Parent 0: Determine the cost ranks for all `cost_w` and `cost_l` in the batch. Calculate the normalized rank difference `rank_gap_norm = rank_diff / (2 * batch_size)`.\n4. New Coupling (Rank-Gap Normalization of Cost Signal): Create a smooth, non-negative cost signal using `softplus(delta_cost / temp_cost)`. Normalize this signal by the rank gap: `adaptive_weight = softplus(...) / (1.0 + beta * rank_gap_norm)`. This weight is large for pairs with small rank gaps and small for pairs with large rank gaps.\n5. New Coupling (Tanh Confidence Gating): Create a confidence gate that down-weights easy examples and up-weights confidently wrong examples: `confidence_gate = 1.0 - tanh(delta_logp)`.\n6. Combine the components: Modulate the base loss with both the adaptive weight and the confidence gate: `loss = confidence_gate * adaptive_weight * base_loss`.\n7. Return the weighted mean of the per-sample losses over the batch.", "hyperparams": {"beta": 1.0, "temp_cost": 1.0}, "operators_used": ["logsigmoid", "softplus", "tanh", "rank_gap"], "implementation_hint": {"expects": ["cost_a", "cost_b", "logp_w", "logp_l"], "returns": "scalar"}, "code": "import torch\nimport torch.nn.functional as F\n\ndef generated_loss(batch, model_output, extra):\n    \"\"\"\n    Computes the Rank-Normalized Bradley-Terry Loss with Confidence Gating.\n\n    Inherits:\n    - The core `-weight * logsigmoid(delta_logp)` structure (from Parent 1).\n    - The use of rank gap for normalization (from Parent 0).\n\n    Introduces:\n    - A new coupling where rank gap normalizes the softplus-transformed cost difference, creating a batch-aware adaptive weight.\n    - A `(1 - tanh(delta_logp))` gate to modulate the loss based on model confidence, focusing on hard or incorrect examples.\n    \"\"\"\n    # For preference learning, we assume cost_w < cost_l.\n    cost_w = torch.min(batch['cost_a'], batch['cost_b'])\n    cost_l = torch.max(batch['cost_a'], batch['cost_b'])\n    log_prob_w = batch['log_prob_w']\n    log_prob_l = batch['log_prob_l']\n\n    # Retrieve hyperparameters\n    beta = extra.get('beta', 1.0)\n    temp_cost = extra.get('temp_cost', 1.0)\n\n    # 1. Calculate cost and log probability differences\n    delta_cost = cost_l - cost_w\n    delta_logp = log_prob_w - log_prob_l\n    batch_size = delta_cost.shape[0]\n\n    # 2. Inherited from Parent 1: Core Bradley-Terry / log-sigmoid loss term\n    base_loss = -F.logsigmoid(delta_logp)\n\n    # 3. Inherited from Parent 0: Calculate rank differences\n    all_costs = torch.cat([cost_w, cost_l], dim=0)\n    sorted_costs, _ = torch.sort(all_costs)\n    rank_w = torch.searchsorted(sorted_costs, cost_w)\n    rank_l = torch.searchsorted(sorted_costs, cost_l)\n    rank_diff = (rank_l - rank_w).float()\n    # Normalize by total number of items to make it independent of batch size\n    rank_gap_norm = rank_diff / (2 * batch_size)\n\n    # 4. New Coupling: Rank-Gap Normalization of Cost Signal\n    # Create a smooth, non-negative signal from the cost difference\n    cost_signal = F.softplus(delta_cost / temp_cost)\n    # Normalize the signal using the rank gap. Pairs far apart in rank (large rank_gap_norm)\n    # will have their weight reduced, focusing the loss on harder, fine-grained pairs.\n    # Adding 1.0 prevents division by zero and ensures the divisor is >= 1.\n    adaptive_weight = cost_signal / (1.0 + beta * rank_gap_norm)\n    # Detach the weight so it doesn't propagate gradients through the cost/rank calculations.\n    adaptive_weight = adaptive_weight.detach()\n\n    # 5. New Coupling: Tanh Confidence Gating\n    # When delta_logp is large & positive (confident & correct), tanh -> 1, gate -> 0. Loss is minimized.\n    # When delta_logp is large & negative (confident & incorrect), tanh -> -1, gate -> 2. Loss is amplified.\n    # When delta_logp is near 0 (uncertain), tanh -> 0, gate -> 1. Loss is unchanged.\n    confidence_gate = 1.0 - torch.tanh(delta_logp)\n\n    # 6. Combine all components for the final loss\n    loss = confidence_gate * adaptive_weight * base_loss\n\n    # 7. Apply optional weights and compute the mean\n    weights = batch.get('weight', None)\n    if weights is not None:\n        loss = loss * weights\n        # Use clamp to prevent division by zero for an all-zero weight tensor\n        return loss.sum() / weights.sum().clamp(min=1e-8)\n    else:\n        return loss.mean()"}, "fitness": {"hf_like_score": 42.36905860900879, "validation_objective": 42.36905860900879, "generalization_penalty": 0.0, "generalization_objectives": {"100": 41.258405685424805}, "train_score_mean": 39.45398051929474, "train_loss_mean": 3.2678703820705413, "pair_count": 129023927, "config": {"hf": {"problem": "tsp", "hf_steps": 1000, "train_problem_size": 100, "valid_problem_sizes": [100], "train_batch_size": 64, "pomo_size": 64, "learning_rate": 0.0003, "weight_decay": 1e-06, "alpha": 0.05, "device": "cuda", "seed": 1234, "num_validation_episodes": 128, "validation_batch_size": 64, "generalization_penalty_weight": 1.0, "pool_version": "v0"}, "free_loss": {"f1_steps": 1000, "f2_steps": 100, "f3_enabled": false}}, "loss_ir": {"name": "Rank-Normalized Bradley-Terry Loss with Confidence Gating", "intuition": "This loss function reframes the preference learning problem by directly weighting the classic Bradley-Terry loss (`-logsigmoid(delta_logp)`) with a composite, batch-aware signal. This signal is designed to be robust and focus the model on meaningful pairs.\n\nInherited Ideas:\n- From 'Rank-Aware Log-Sigmoid Preference Loss' (Parent 1), it inherits the core structure of a Bradley-Terry-like loss, `-weight * logsigmoid(delta_logp)`. This provides a probabilistic interpretation and a strong, non-saturating gradient when the model is wrong.\n- From 'Sigmoid-Gated Adaptive Margin Loss with Rank-Gap Normalization' (Parent 0), it inherits the idea of using the rank gap to directly normalize a component of the loss. Instead of normalizing the margin, it now normalizes the `delta_cost` signal itself, making the cost-based weighting adaptive to the pair's relative importance within the batch.\n\nNew Coupling Ideas:\n1.  **Rank-Gap Normalization of Cost Signal**: The `delta_cost` is first passed through a `softplus` to ensure it is non-negative and smooth. This raw signal is then divided by `1 + beta * rank_gap_normalized`. This couples the cost information with rank information directly. For pairs with a large rank gap (easy to distinguish), the denominator grows, down-weighting their contribution and allowing the model to focus on finer-grained, harder examples with small rank gaps.\n2.  **Tanh Confidence Gating**: The final loss is gated by `(1 - tanh(delta_logp))`. This gate has two effects: for pairs the model is already confident about (large positive `delta_logp`), `tanh` approaches 1, and the gate `(1 - tanh)` approaches 0, reducing the loss and preventing the model from over-optimizing on easy pairs. For pairs the model is confidently wrong about (large negative `delta_logp`), `tanh` approaches -1, and the gate approaches 2, amplifying the loss signal and forcing the model to correct its most significant errors.", "hyperparams": {"beta": 1.0, "temp_cost": 1.0}, "operators_used": ["logsigmoid", "softplus", "tanh", "rank_gap"]}}, "better_than_baseline": false}
{"generation": 8, "index": 2, "ir": {"name": "Rank-Gated Log-Sigmoid Loss with Softplus Margin", "intuition": "This loss function combines the classic Bradley-Terry structure of a log-sigmoid loss with an adaptive, non-saturating margin, gated by a smooth, rank-aware weighting factor. The goal is to create a loss that is sensitive to both the magnitude of cost differences and the relative ranking of pairs within a batch.\n\nInherited Ideas:\n- From 'Rank-Aware Log-Sigmoid Preference Loss' (Parent 1), it inherits the core loss structure `-logsigmoid(delta_logp - margin)`. This is a robust and well-understood preference loss formulation.\n- From 'Sigmoid-Gated Adaptive Margin Loss with Rank-Gap Normalization' (Parent 0), it inherits the idea of using `softplus(delta_cost)` to create a smooth, non-saturating, and adaptive margin that grows with the cost difference. This ensures that pairs with larger cost differences are expected to have a larger log-probability separation.\n\nNew Coupling Ideas:\n1. **Rank-Gap Softplus Gating**: Instead of using the rank gap to normalize the margin (as in Parent 0) or as part of a final loss weight (as in Parent 1), it is used to create a smooth gate that multiplies the entire loss term. The rank difference is passed through `softplus(beta * rank_diff)`, creating a non-negative, non-saturating weight. This smoothly emphasizes pairs that are far apart in the batch's cost distribution, focusing the model's attention on learning the most significant preferences first, while still applying a non-zero loss to all pairs.\n2. **Margin Clamping for Stability**: While the softplus margin is non-saturating, it can grow very large for pairs with extreme cost differences, potentially leading to large gradients. A new stability trick is introduced: `clamp(margin, max=max_margin)`. This caps the target log-probability separation at a reasonable maximum, preventing outlier pairs from dominating the batch gradient and improving training stability without sacrificing the adaptive nature of the margin for most pairs.", "pseudocode": "1. For each pair (w, l) where cost(w) < cost(l), calculate `delta_cost = cost(l) - cost(w)` and `delta_logp = logp(w) - logp(l)`.\n2. Inherit from Parent 0: Calculate a smooth, non-saturating adaptive margin using `softplus`: `margin_base = alpha * softplus(delta_cost / temp_cost)`.\n3. New Coupling (Stability): Clamp the margin to a maximum value to prevent gradient explosion from outlier pairs: `margin = clamp(margin_base, max=max_margin)`.\n4. Determine the cost ranks for all `cost_w` and `cost_l` in the batch. Calculate the rank difference `rank_diff = rank(cost_l) - rank(cost_w)`.\n5. New Coupling (Rank Gating): Create a smooth, non-saturating gate by applying `softplus` to the scaled rank difference: `rank_gate = softplus(beta * rank_diff)`. This gate will weight the final loss.\n6. Inherit from Parent 1: Compute the core preference loss using the log-sigmoid function, incorporating the adaptive margin: `per_sample_loss = -logsigmoid(delta_logp - margin)`.\n7. Apply the rank gate to modulate the loss: `gated_loss = rank_gate * per_sample_loss`.\n8. Return the weighted mean of the `gated_loss` over the batch.", "hyperparams": {"alpha": 1.0, "temp_cost": 1.0, "beta": 0.1, "max_margin": 10.0}, "operators_used": ["logsigmoid", "softplus", "clamp", "rank_gap"], "implementation_hint": {"expects": ["cost_a", "cost_b", "logp_w", "logp_l"], "returns": "scalar"}, "code": "import torch\nimport torch.nn.functional as F\n\ndef generated_loss(batch, model_output, extra):\n    \"\"\"\n    Computes the Rank-Gated Log-Sigmoid Loss with Softplus Margin.\n\n    Inherits:\n    - The use of `softplus(delta_cost)` for a non-saturating margin (from Parent 0).\n    - The core `-logsigmoid(delta_logp - margin)` structure (from Parent 1).\n\n    Introduces:\n    - A softplus-gated rank gap, `softplus(beta * rank_diff)`, to smoothly weight the loss of each pair.\n    - A stability trick by clamping the adaptive margin to a maximum value (`max_margin`).\n    \"\"\"\n    # For preference learning, we assume cost_w < cost_l.\n    cost_w = torch.min(batch['cost_a'], batch['cost_b'])\n    cost_l = torch.max(batch['cost_a'], batch['cost_b'])\n    log_prob_w = batch['log_prob_w']\n    log_prob_l = batch['log_prob_l']\n\n    # Retrieve hyperparameters\n    alpha = extra.get('alpha', 1.0)\n    temp_cost = extra.get('temp_cost', 1.0)\n    beta = extra.get('beta', 0.1)\n    max_margin = extra.get('max_margin', 10.0)\n\n    # 1. Calculate cost and log probability differences\n    delta_cost = cost_l - cost_w\n    delta_logp = log_prob_w - log_prob_l\n\n    # 2. Inherited from Parent 0: Calculate a smooth, non-saturating adaptive margin.\n    # The margin increases as the difference in cost increases.\n    margin_base = alpha * F.softplus(delta_cost / temp_cost)\n\n    # 3. New Coupling (Stability): Clamp the margin to prevent instability from extreme cost differences.\n    adaptive_margin = torch.clamp(margin_base, max=max_margin)\n\n    # 4. Calculate rank differences.\n    # Combine all costs to establish a batch-wide ranking.\n    all_costs = torch.cat([cost_w, cost_l], dim=0)\n    sorted_costs, _ = torch.sort(all_costs)\n    \n    # Find ranks efficiently using searchsorted.\n    rank_w = torch.searchsorted(sorted_costs, cost_w)\n    rank_l = torch.searchsorted(sorted_costs, cost_l)\n    rank_diff = (rank_l - rank_w).float()\n\n    # 5. New Coupling (Rank Gating): Create a smooth, non-saturating gate from rank difference.\n    # This gives more weight to pairs that are far apart in the cost ranking.\n    rank_gate = F.softplus(beta * rank_diff)\n    \n    # 6. Inherited from Parent 1: Compute the core preference loss using log-sigmoid.\n    # The model is encouraged to make `delta_logp > adaptive_margin`.\n    per_sample_loss = -F.logsigmoid(delta_logp - adaptive_margin.detach())\n\n    # 7. Apply the rank gate to the loss.\n    # Detach the gate so it acts as a pure weight, not a gradient path for cost models.\n    gated_loss = rank_gate.detach() * per_sample_loss\n\n    # 8. Apply optional weights and compute the mean.\n    weights = batch.get('weight', None)\n    if weights is not None:\n        gated_loss = gated_loss * weights\n        # Use clamp to avoid division by zero if all weights are zero.\n        return gated_loss.sum() / weights.sum().clamp(min=1e-8)\n    else:\n        return gated_loss.mean()"}, "fitness": {"hf_like_score": 33.1395149230957, "validation_objective": 33.1395149230957, "generalization_penalty": 0.0, "generalization_objectives": {"100": 32.90353775024414}, "train_score_mean": 37.672886675834654, "train_loss_mean": 6913.094963867187, "pair_count": 129023908, "config": {"hf": {"problem": "tsp", "hf_steps": 1000, "train_problem_size": 100, "valid_problem_sizes": [100], "train_batch_size": 64, "pomo_size": 64, "learning_rate": 0.0003, "weight_decay": 1e-06, "alpha": 0.05, "device": "cuda", "seed": 1234, "num_validation_episodes": 128, "validation_batch_size": 64, "generalization_penalty_weight": 1.0, "pool_version": "v0"}, "free_loss": {"f1_steps": 1000, "f2_steps": 100, "f3_enabled": false}}, "loss_ir": {"name": "Rank-Gated Log-Sigmoid Loss with Softplus Margin", "intuition": "This loss function combines the classic Bradley-Terry structure of a log-sigmoid loss with an adaptive, non-saturating margin, gated by a smooth, rank-aware weighting factor. The goal is to create a loss that is sensitive to both the magnitude of cost differences and the relative ranking of pairs within a batch.\n\nInherited Ideas:\n- From 'Rank-Aware Log-Sigmoid Preference Loss' (Parent 1), it inherits the core loss structure `-logsigmoid(delta_logp - margin)`. This is a robust and well-understood preference loss formulation.\n- From 'Sigmoid-Gated Adaptive Margin Loss with Rank-Gap Normalization' (Parent 0), it inherits the idea of using `softplus(delta_cost)` to create a smooth, non-saturating, and adaptive margin that grows with the cost difference. This ensures that pairs with larger cost differences are expected to have a larger log-probability separation.\n\nNew Coupling Ideas:\n1. **Rank-Gap Softplus Gating**: Instead of using the rank gap to normalize the margin (as in Parent 0) or as part of a final loss weight (as in Parent 1), it is used to create a smooth gate that multiplies the entire loss term. The rank difference is passed through `softplus(beta * rank_diff)`, creating a non-negative, non-saturating weight. This smoothly emphasizes pairs that are far apart in the batch's cost distribution, focusing the model's attention on learning the most significant preferences first, while still applying a non-zero loss to all pairs.\n2. **Margin Clamping for Stability**: While the softplus margin is non-saturating, it can grow very large for pairs with extreme cost differences, potentially leading to large gradients. A new stability trick is introduced: `clamp(margin, max=max_margin)`. This caps the target log-probability separation at a reasonable maximum, preventing outlier pairs from dominating the batch gradient and improving training stability without sacrificing the adaptive nature of the margin for most pairs.", "hyperparams": {"alpha": 1.0, "temp_cost": 1.0, "beta": 0.1, "max_margin": 10.0}, "operators_used": ["logsigmoid", "softplus", "clamp", "rank_gap"]}}, "better_than_baseline": false}
{"generation": 8, "index": 3, "ir": {"name": "Rank-Gated Log-Sigmoid Loss with Z-Score Margin", "intuition": "This loss function creates a preference signal that is both robust to the scale of costs and sensitive to the relative ranking of examples within a batch. It combines the classic Bradley-Terry log-sigmoid structure with a dynamically scaled margin that is further modulated by a rank-based gate.\n\nInherited Ideas:\n- From 'Rank-Aware Log-Sigmoid Preference Loss' (Parent 1), it inherits the core loss structure of `-logsigmoid(delta_logp - margin)`. This provides a probabilistic interpretation where the model is encouraged to make the log-probability of the winning candidate greater than the loser's by at least a certain margin.\n- From 'Sigmoid-Gated Adaptive Margin Loss with Rank-Gap Normalization' (Parent 0), it inherits the use of `rank_gap` to modulate the loss, emphasizing pairs that are far apart in the batch's cost distribution. It also inherits the general principle of creating an adaptive margin based on cost differences.\n\nNew Coupling Ideas:\n1.  **Z-Score Normalized Margin**: The margin `M` is derived from the Z-score of the cost differences (`delta_cost`). The margin is calculated as `alpha * softplus(zscore(delta_cost))`. This makes the margin's scale invariant to the absolute magnitude and variance of costs in a batch, leading to more stable training and less sensitivity to the `alpha` hyperparameter. The `softplus` ensures the margin is always non-negative and smooth.\n2.  **Tanh-Gated Rank Weighting**: The final loss is multiplied by a weight derived from the rank difference. This weight is computed as `1.0 + beta * tanh(rank_diff / batch_size)`. The `tanh` function creates a smooth, bounded (between 0 and 2) weight. For pairs with a small rank difference, the weight is close to 1.0. As the rank difference grows, the weight smoothly increases, saturating at `1.0 + beta`. This provides a stable mechanism to up-weight the importance of distinguishing clearly separated pairs without risking the exploding gradients that an unbounded weight (like `softplus` or linear scaling) might cause.", "pseudocode": "1. For each pair (w, l) where cost(w) < cost(l), calculate the cost difference `delta_cost = cost(l) - cost(w)` and the log probability difference `delta_logp = logp(w) - logp(l)`.\n2. New Coupling (Z-Score Margin): Standardize the `delta_cost` vector for the batch using Z-score normalization. Calculate the adaptive margin `M = alpha * softplus(zscore(delta_cost))`. This margin is robust to cost scale.\n3. Inherit from Parent 1: Compute the core loss using a log-sigmoid structure with the adaptive margin: `base_loss = -logsigmoid(delta_logp - M)`.\n4. Inherit from Parent 0: Determine the cost ranks for all `cost_w` and `cost_l` in the batch. Calculate the rank difference `rank_diff = rank(cost_l) - rank(cost_w)`.\n5. New Coupling (Tanh-Gated Rank Weighting): Create a smooth, bounded weight from the rank difference: `rank_weight = 1.0 + beta * tanh(rank_diff / batch_size)`. Dividing by batch size provides scale invariance.\n6. Modulate the base loss with the rank weight: `final_loss = rank_weight * base_loss`.\n7. Return the weighted mean of the per-sample losses over the batch.", "hyperparams": {"alpha": 0.5, "beta": 1.0}, "operators_used": ["logsigmoid", "softplus", "zscore", "rank_gap", "tanh"], "implementation_hint": {"expects": ["cost_a", "cost_b", "logp_a", "logp_b"], "returns": "scalar"}, "code": "import torch\nimport torch.nn.functional as F\n\ndef generated_loss(batch, model_output, extra):\n    \"\"\"\n    Computes the Rank-Gated Log-Sigmoid Loss with Z-Score Margin.\n\n    Inherits:\n    - The core `-logsigmoid(delta_logp - margin)` structure (from Parent 1).\n    - The use of rank differences to modulate the loss signal (from Parent 0).\n\n    Introduces:\n    - A Z-score normalized margin `alpha * softplus(zscore(delta_cost))` for cost scale invariance.\n    - A tanh-gated rank weight `1 + beta * tanh(rank_diff / N)` for smooth, bounded emphasis on separated pairs.\n    \"\"\"\n    # For preference learning, we assume cost_w < cost_l.\n    cost_w = torch.min(batch['cost_a'], batch['cost_b'])\n    cost_l = torch.max(batch['cost_a'], batch['cost_b'])\n    log_prob_w = batch['log_prob_w']\n    log_prob_l = batch['log_prob_l']\n\n    # Retrieve hyperparameters\n    alpha = extra.get('alpha', 0.5)\n    beta = extra.get('beta', 1.0)\n\n    # 1. Calculate cost and log probability differences\n    delta_cost = cost_l - cost_w\n    delta_logp = log_prob_w - log_prob_l\n    batch_size = delta_cost.shape[0]\n\n    # Handle batches with no cost variance to prevent NaN from z-score\n    if delta_cost.std() < 1e-8:\n        margin = torch.zeros_like(delta_cost)\n    else:\n        # 2. New Coupling (Z-Score Margin): Standardize delta_cost and create margin\n        delta_cost_mean = delta_cost.mean()\n        delta_cost_std = delta_cost.std().clamp(min=1e-8)\n        delta_cost_zscore = (delta_cost - delta_cost_mean) / delta_cost_std\n        margin = alpha * F.softplus(delta_cost_zscore)\n\n    # 3. Inherit from Parent 1: Compute the core log-sigmoid loss with the margin\n    # Detach margin so it acts as a target, not part of the backprop path for cost models\n    base_loss = -F.logsigmoid(delta_logp - margin.detach())\n\n    # 4. Inherit from Parent 0: Calculate rank differences\n    all_costs = torch.cat([cost_w, cost_l], dim=0)\n    sorted_costs, _ = torch.sort(all_costs)\n    rank_w = torch.searchsorted(sorted_costs, cost_w)\n    rank_l = torch.searchsorted(sorted_costs, cost_l)\n    rank_diff = (rank_l - rank_w).float()\n\n    # 5. New Coupling (Tanh-Gated Rank Weighting): Create a smooth, bounded weight\n    # The weight ranges from 1.0 to (1.0 + beta), providing stable emphasis.\n    rank_weight = 1.0 + beta * torch.tanh(rank_diff / (2 * batch_size))\n\n    # 6. Modulate the loss with the rank weight\n    # Detach weight to ensure it only scales the loss magnitude\n    final_loss = rank_weight.detach() * base_loss\n\n    # 7. Apply optional weights and compute the mean\n    weights = batch.get('weight', None)\n    if weights is not None:\n        final_loss = final_loss * weights\n        return final_loss.sum() / weights.sum().clamp(min=1e-8)\n    else:\n        return final_loss.mean()"}, "fitness": {"hf_like_score": 36.444732666015625, "validation_objective": 36.12521553039551, "generalization_penalty": 0.3195171356201172, "generalization_objectives": {"100": 36.444732666015625}, "train_score_mean": 38.67688547801971, "train_loss_mean": 1.2986158945560455, "pair_count": 129023914, "config": {"hf": {"problem": "tsp", "hf_steps": 1000, "train_problem_size": 100, "valid_problem_sizes": [100], "train_batch_size": 64, "pomo_size": 64, "learning_rate": 0.0003, "weight_decay": 1e-06, "alpha": 0.05, "device": "cuda", "seed": 1234, "num_validation_episodes": 128, "validation_batch_size": 64, "generalization_penalty_weight": 1.0, "pool_version": "v0"}, "free_loss": {"f1_steps": 1000, "f2_steps": 100, "f3_enabled": false}}, "loss_ir": {"name": "Rank-Gated Log-Sigmoid Loss with Z-Score Margin", "intuition": "This loss function creates a preference signal that is both robust to the scale of costs and sensitive to the relative ranking of examples within a batch. It combines the classic Bradley-Terry log-sigmoid structure with a dynamically scaled margin that is further modulated by a rank-based gate.\n\nInherited Ideas:\n- From 'Rank-Aware Log-Sigmoid Preference Loss' (Parent 1), it inherits the core loss structure of `-logsigmoid(delta_logp - margin)`. This provides a probabilistic interpretation where the model is encouraged to make the log-probability of the winning candidate greater than the loser's by at least a certain margin.\n- From 'Sigmoid-Gated Adaptive Margin Loss with Rank-Gap Normalization' (Parent 0), it inherits the use of `rank_gap` to modulate the loss, emphasizing pairs that are far apart in the batch's cost distribution. It also inherits the general principle of creating an adaptive margin based on cost differences.\n\nNew Coupling Ideas:\n1.  **Z-Score Normalized Margin**: The margin `M` is derived from the Z-score of the cost differences (`delta_cost`). The margin is calculated as `alpha * softplus(zscore(delta_cost))`. This makes the margin's scale invariant to the absolute magnitude and variance of costs in a batch, leading to more stable training and less sensitivity to the `alpha` hyperparameter. The `softplus` ensures the margin is always non-negative and smooth.\n2.  **Tanh-Gated Rank Weighting**: The final loss is multiplied by a weight derived from the rank difference. This weight is computed as `1.0 + beta * tanh(rank_diff / batch_size)`. The `tanh` function creates a smooth, bounded (between 0 and 2) weight. For pairs with a small rank difference, the weight is close to 1.0. As the rank difference grows, the weight smoothly increases, saturating at `1.0 + beta`. This provides a stable mechanism to up-weight the importance of distinguishing clearly separated pairs without risking the exploding gradients that an unbounded weight (like `softplus` or linear scaling) might cause.", "hyperparams": {"alpha": 0.5, "beta": 1.0}, "operators_used": ["logsigmoid", "softplus", "zscore", "rank_gap", "tanh"]}}, "better_than_baseline": false}
{"generation": 8, "index": 4, "ir": {"name": "Rank-Gated Log-Sigmoid Loss with Tanh Margin", "intuition": "This loss function creates a preference signal that is both sensitive to the magnitude of cost differences and robust to the batch-wise distribution of those costs. It combines a classic log-sigmoid structure with a bounded, adaptive margin and a novel rank-based gating mechanism.\n\nInherited Ideas:\n- From 'Rank-Aware Log-Sigmoid Preference Loss' (Parent 1), it inherits the core loss structure of a weighted log-sigmoid: `weight * -logsigmoid(delta_logp - margin)`. This provides a probabilistic interpretation and a strong gradient for misclassified pairs.\n- From 'Sigmoid-Gated Adaptive Margin Loss with Rank-Gap Normalization' (Parent 0), it inherits the use of `rank_gap` to create a batch-aware signal that emphasizes pairs with a large separation in the cost distribution. This helps the model prioritize learning from more distinct pairs within a batch.\n\nNew Coupling Ideas:\n1.  **Tanh Adaptive Margin**: A new margin is created using `tanh(alpha * delta_cost)`. This margin is adaptive to the cost difference but is bounded between 0 and 1. This prevents pairs with extremely large cost differences from creating an unbounded margin, which could lead to gradient explosion and dominate the training. The `tanh` function provides a smooth saturation, balancing sensitivity to small cost differences with stability for large ones.\n2.  **Rank-Gated Loss**: The entire per-sample loss is gated by a rank-based term. The rank difference between the worse and better item is normalized by the batch size and then passed through a `softplus` function: `softplus(beta * normalized_rank_diff)`. This acts as a smooth, non-negative gate that starts near `log(2)` for items with similar ranks and grows as the rank separation increases. This directly couples the batch-wise importance of a pair (its rank gap) to the magnitude of its loss contribution, effectively focusing the training on pairs that are most informative from a batch perspective.", "pseudocode": "1. For each pair (w, l) where cost(w) < cost(l), calculate the cost difference `delta_cost = cost(l) - cost(w)` and the log probability difference `delta_logp = logp(w) - logp(l)`.\n2. New Coupling (Tanh Margin): Calculate a bounded, adaptive margin using the hyperbolic tangent function: `margin = tanh(alpha * delta_cost)`.\n3. Inherit from Parent 1 (Core Structure): Calculate the base loss using a log-sigmoid function, incorporating the new margin: `base_loss = -logsigmoid(delta_logp - margin)`.\n4. Inherit from Parent 0 (Rank Signal): Determine the cost ranks for all `cost_w` and `cost_l` in the batch. Calculate the rank difference `rank_diff = rank(cost_l) - rank(cost_w)`.\n5. New Coupling (Rank-Gated Loss): Normalize the rank difference by the batch size. Apply a `softplus` function to this normalized difference to create a smooth, non-negative gate: `rank_gate = softplus(beta * rank_diff / batch_size)`.\n6. Combine the base loss and the rank gate: `final_loss = rank_gate * base_loss`.\n7. Return the weighted mean of the per-sample losses over the batch.", "hyperparams": {"alpha": 0.5, "beta": 5.0}, "operators_used": ["logsigmoid", "tanh", "softplus", "rank_gap"], "implementation_hint": {"expects": ["cost_a", "cost_b", "log_prob_w", "log_prob_l", "weight"], "returns": "scalar"}, "code": "import torch\nimport torch.nn.functional as F\n\ndef generated_loss(batch, model_output, extra):\n    \"\"\"\n    Computes the Rank-Gated Log-Sigmoid Loss with Tanh Margin.\n\n    Inherits:\n    - The core log-sigmoid loss structure (from Parent 1).\n    - The use of rank differences for batch-aware importance weighting (from Parent 0).\n\n    Introduces:\n    - A new tanh-based adaptive margin that is bounded, preventing gradient explosion from large cost differences.\n    - A new coupling where a softplus-transformed rank gap directly gates the final loss value for each pair.\n    \"\"\"\n    # Ensure cost_w < cost_l for preference learning\n    cost_w = torch.min(batch['cost_a'], batch['cost_b'])\n    cost_l = torch.max(batch['cost_a'], batch['cost_b'])\n    log_prob_w = batch['log_prob_w']\n    log_prob_l = batch['log_prob_l']\n\n    # Retrieve hyperparameters\n    alpha = extra.get('alpha', 0.5)\n    beta = extra.get('beta', 5.0)\n\n    # 1. Calculate cost and log probability differences\n    delta_cost = cost_l - cost_w\n    delta_logp = log_prob_w - log_prob_l\n    batch_size = delta_cost.shape[0]\n\n    # 2. New Coupling: Tanh Adaptive Margin\n    # tanh creates a margin that is sensitive to delta_cost but bounded in [0, 1].\n    # This prevents extremely large cost differences from creating unstable, unbounded margins.\n    margin = torch.tanh(alpha * delta_cost)\n\n    # 3. Inherited from Parent 1: Core log-sigmoid loss structure\n    # The goal is to make (delta_logp - margin) as large and positive as possible.\n    base_loss = -F.logsigmoid(delta_logp - margin.detach())\n\n    # 4. Inherited from Parent 0: Calculate rank differences\n    # Combine all costs to establish a batch-wide ranking.\n    all_costs = torch.cat([cost_w, cost_l], dim=0)\n    sorted_costs, _ = torch.sort(all_costs)\n    \n    # Find ranks efficiently using searchsorted.\n    rank_w = torch.searchsorted(sorted_costs, cost_w)\n    rank_l = torch.searchsorted(sorted_costs, cost_l)\n    rank_diff = (rank_l - rank_w).float()\n\n    # 5. New Coupling: Rank-Gated Loss\n    # Normalize rank_diff by batch size to make it scale-invariant.\n    # softplus creates a smooth, non-negative gate that increases with rank separation.\n    # This emphasizes pairs that are well-separated in the batch's cost distribution.\n    normalized_rank_diff = rank_diff / (2 * batch_size)\n    rank_gate = F.softplus(beta * normalized_rank_diff)\n    \n    # 6. Combine the base loss and the rank gate\n    # The gate acts as a dynamic weight, focusing on important pairs.\n    final_loss = rank_gate.detach() * base_loss\n\n    # 7. Apply optional weights and compute the mean.\n    weights = batch.get('weight', None)\n    if weights is not None:\n        final_loss = final_loss * weights\n        # Use clamp to avoid division by zero for an all-zero weight tensor\n        return final_loss.sum() / weights.sum().clamp(min=1e-8)\n    else:\n        return final_loss.mean()"}, "fitness": {"hf_like_score": 36.589887619018555, "validation_objective": 36.589887619018555, "generalization_penalty": 0.0, "generalization_objectives": {"100": 36.26413536071777}, "train_score_mean": 38.77025053310394, "train_loss_mean": 1.7215479103326798, "pair_count": 129023901, "config": {"hf": {"problem": "tsp", "hf_steps": 1000, "train_problem_size": 100, "valid_problem_sizes": [100], "train_batch_size": 64, "pomo_size": 64, "learning_rate": 0.0003, "weight_decay": 1e-06, "alpha": 0.05, "device": "cuda", "seed": 1234, "num_validation_episodes": 128, "validation_batch_size": 64, "generalization_penalty_weight": 1.0, "pool_version": "v0"}, "free_loss": {"f1_steps": 1000, "f2_steps": 100, "f3_enabled": false}}, "loss_ir": {"name": "Rank-Gated Log-Sigmoid Loss with Tanh Margin", "intuition": "This loss function creates a preference signal that is both sensitive to the magnitude of cost differences and robust to the batch-wise distribution of those costs. It combines a classic log-sigmoid structure with a bounded, adaptive margin and a novel rank-based gating mechanism.\n\nInherited Ideas:\n- From 'Rank-Aware Log-Sigmoid Preference Loss' (Parent 1), it inherits the core loss structure of a weighted log-sigmoid: `weight * -logsigmoid(delta_logp - margin)`. This provides a probabilistic interpretation and a strong gradient for misclassified pairs.\n- From 'Sigmoid-Gated Adaptive Margin Loss with Rank-Gap Normalization' (Parent 0), it inherits the use of `rank_gap` to create a batch-aware signal that emphasizes pairs with a large separation in the cost distribution. This helps the model prioritize learning from more distinct pairs within a batch.\n\nNew Coupling Ideas:\n1.  **Tanh Adaptive Margin**: A new margin is created using `tanh(alpha * delta_cost)`. This margin is adaptive to the cost difference but is bounded between 0 and 1. This prevents pairs with extremely large cost differences from creating an unbounded margin, which could lead to gradient explosion and dominate the training. The `tanh` function provides a smooth saturation, balancing sensitivity to small cost differences with stability for large ones.\n2.  **Rank-Gated Loss**: The entire per-sample loss is gated by a rank-based term. The rank difference between the worse and better item is normalized by the batch size and then passed through a `softplus` function: `softplus(beta * normalized_rank_diff)`. This acts as a smooth, non-negative gate that starts near `log(2)` for items with similar ranks and grows as the rank separation increases. This directly couples the batch-wise importance of a pair (its rank gap) to the magnitude of its loss contribution, effectively focusing the training on pairs that are most informative from a batch perspective.", "hyperparams": {"alpha": 0.5, "beta": 5.0}, "operators_used": ["logsigmoid", "tanh", "softplus", "rank_gap"]}}, "better_than_baseline": false}
{"generation": 8, "index": 5, "ir": {"name": "Rank-Gated Log-Sigmoid Loss with Tanh Margin", "intuition": "This loss function synergizes a classic log-sigmoid structure with a dynamic, rank-aware margin. It aims to be robust by normalizing cost differences while focusing learning on pairs where the model is uncertain or incorrect.\n\nInherited Ideas:\n- From 'Rank-Aware Log-Sigmoid Preference Loss' (Parent 1), it inherits the core loss structure of `-logsigmoid(delta_logp - margin)`. This provides a strong probabilistic foundation, encouraging the model to maximize the log-probability difference, but now with an explicit margin term.\n- From 'Sigmoid-Gated Adaptive Margin Loss with Rank-Gap Normalization' (Parent 0), it inherits the idea of using the rank difference (`rank_gap`) within the batch to directly modulate a key component of the loss. Instead of modulating the margin, here it gates the entire loss value.\n\nNew Coupling Ideas:\n1.  **Tanh-based Adaptive Margin**: The margin is calculated as `alpha * tanh(beta * delta_cost_norm)`. This couples a bounded activation function (`tanh`) with a batch-normalized cost difference (`zscore`). The `tanh` function creates a saturating margin, preventing extremely large cost differences from creating an unstable, unbounded margin. This is a stable alternative to the unbounded `softplus` margin seen in the parents.\n2.  **Rank-Gap Gating of the Loss**: The final per-sample loss is multiplied by a rank-based gate: `loss * (1 + rank_gap / batch_size)`. This directly amplifies the loss for pairs that are far apart in the batch's cost distribution, forcing the model to prioritize learning these more significant preferences. The scaling by `batch_size` ensures the gate's magnitude is consistent across different batch sizes.", "pseudocode": "1. For each pair (w, l), calculate the cost difference `delta_cost = cost(l) - cost(w)` and the log probability difference `delta_logp = logp(w) - logp(l)`.\n2. Normalize the `delta_cost` vector across the batch using Z-score to get `delta_cost_norm`. This makes the margin scale-invariant.\n3. New Coupling (Tanh Margin): Compute a bounded, adaptive margin using the hyperbolic tangent function: `margin = alpha * tanh(beta * delta_cost_norm)`.\n4. Inherited from Parent 1: Calculate the core preference loss using a margin-based log-sigmoid formulation: `base_loss = -logsigmoid(delta_logp - margin)`.\n5. Inherited from Parent 0: Determine the cost ranks for all `cost_w` and `cost_l` in the batch. Calculate the rank difference `rank_gap = rank(cost_l) - rank(cost_w)`.\n6. New Coupling (Rank-Gap Gating): Create a gate that amplifies the loss for pairs with a large rank gap: `rank_gate = 1.0 + (rank_gap / batch_size)`.\n7. Apply the gate to the base loss: `final_loss = base_loss * rank_gate`.\n8. Return the weighted mean of the `final_loss` over the batch.", "hyperparams": {"alpha": 1.0, "beta": 0.5}, "operators_used": ["logsigmoid", "tanh", "zscore", "rank_gap"], "implementation_hint": {"expects": ["cost_a", "cost_b", "logp_a", "logp_b", "weight"], "returns": "scalar"}, "code": "import torch\nimport torch.nn.functional as F\n\ndef generated_loss(batch, model_output, extra):\n    \"\"\"\n    Computes the Rank-Gated Log-Sigmoid Loss with Tanh Margin.\n\n    Inherits:\n    - The core `-logsigmoid(delta_logp - margin)` structure (from Parent 1).\n    - The use of rank differences for batch-aware modulation (from Parent 0).\n\n    Introduces:\n    - A bounded adaptive margin using `tanh` on z-scored cost differences.\n    - A direct gating mechanism where the final loss is amplified by the normalized rank gap.\n    \"\"\"\n    # For preference learning, we assume cost_w < cost_l.\n    cost_w = torch.min(batch['cost_a'], batch['cost_b'])\n    cost_l = torch.max(batch['cost_a'], batch['cost_b'])\n    log_prob_w = batch['log_prob_w']\n    log_prob_l = batch['log_prob_l']\n\n    # Retrieve hyperparameters\n    alpha = extra.get('alpha', 1.0)\n    beta = extra.get('beta', 0.5)\n\n    # 1. Calculate cost and log probability differences\n    delta_cost = cost_l - cost_w\n    delta_logp = log_prob_w - log_prob_l\n    batch_size = delta_cost.shape[0]\n\n    # Early exit for a batch with no cost difference to avoid division by zero in z-score\n    if torch.all(delta_cost < 1e-8):\n        return torch.tensor(0.0, device=delta_cost.device)\n\n    # 2. Normalize delta_cost for scale-invariant margin\n    delta_cost_mean = delta_cost.mean()\n    delta_cost_std = delta_cost.std().clamp(min=1e-8)\n    delta_cost_norm = (delta_cost - delta_cost_mean) / delta_cost_std\n\n    # 3. New Coupling: Compute a bounded, adaptive margin using tanh\n    # This prevents the margin from becoming excessively large for outliers.\n    margin = alpha * torch.tanh(beta * delta_cost_norm)\n\n    # 4. Inherited from Parent 1: Calculate core log-sigmoid loss with margin\n    base_loss = -F.logsigmoid(delta_logp - margin.detach())\n\n    # 5. Inherited from Parent 0: Calculate rank differences\n    all_costs = torch.cat([cost_w, cost_l], dim=0)\n    sorted_costs, _ = torch.sort(all_costs)\n    rank_w = torch.searchsorted(sorted_costs, cost_w)\n    rank_l = torch.searchsorted(sorted_costs, cost_l)\n    rank_gap = (rank_l - rank_w).float()\n\n    # 6. New Coupling: Create a gate to amplify loss for pairs with large rank gaps\n    # Dividing by 2*N (total items) makes it a relative rank gap.\n    # Adding 1.0 ensures the gate is always >= 1.\n    rank_gate = 1.0 + (rank_gap / (2 * batch_size))\n\n    # 7. Apply the rank gate to the loss\n    final_loss = base_loss * rank_gate.detach()\n\n    # 8. Apply optional weights and compute the mean.\n    weights = batch.get('weight', None)\n    if weights is not None:\n        final_loss = final_loss * weights\n        return final_loss.sum() / weights.sum().clamp(min=1e-8)\n    else:\n        return final_loss.mean()"}, "fitness": {"hf_like_score": 30.8274564743042, "validation_objective": 30.8274564743042, "generalization_penalty": 0.0, "generalization_objectives": {"100": 30.66055965423584}, "train_score_mean": 39.50465544128418, "train_loss_mean": 2.3050146623849868, "pair_count": 129023920, "config": {"hf": {"problem": "tsp", "hf_steps": 1000, "train_problem_size": 100, "valid_problem_sizes": [100], "train_batch_size": 64, "pomo_size": 64, "learning_rate": 0.0003, "weight_decay": 1e-06, "alpha": 0.05, "device": "cuda", "seed": 1234, "num_validation_episodes": 128, "validation_batch_size": 64, "generalization_penalty_weight": 1.0, "pool_version": "v0"}, "free_loss": {"f1_steps": 1000, "f2_steps": 100, "f3_enabled": false}}, "loss_ir": {"name": "Rank-Gated Log-Sigmoid Loss with Tanh Margin", "intuition": "This loss function synergizes a classic log-sigmoid structure with a dynamic, rank-aware margin. It aims to be robust by normalizing cost differences while focusing learning on pairs where the model is uncertain or incorrect.\n\nInherited Ideas:\n- From 'Rank-Aware Log-Sigmoid Preference Loss' (Parent 1), it inherits the core loss structure of `-logsigmoid(delta_logp - margin)`. This provides a strong probabilistic foundation, encouraging the model to maximize the log-probability difference, but now with an explicit margin term.\n- From 'Sigmoid-Gated Adaptive Margin Loss with Rank-Gap Normalization' (Parent 0), it inherits the idea of using the rank difference (`rank_gap`) within the batch to directly modulate a key component of the loss. Instead of modulating the margin, here it gates the entire loss value.\n\nNew Coupling Ideas:\n1.  **Tanh-based Adaptive Margin**: The margin is calculated as `alpha * tanh(beta * delta_cost_norm)`. This couples a bounded activation function (`tanh`) with a batch-normalized cost difference (`zscore`). The `tanh` function creates a saturating margin, preventing extremely large cost differences from creating an unstable, unbounded margin. This is a stable alternative to the unbounded `softplus` margin seen in the parents.\n2.  **Rank-Gap Gating of the Loss**: The final per-sample loss is multiplied by a rank-based gate: `loss * (1 + rank_gap / batch_size)`. This directly amplifies the loss for pairs that are far apart in the batch's cost distribution, forcing the model to prioritize learning these more significant preferences. The scaling by `batch_size` ensures the gate's magnitude is consistent across different batch sizes.", "hyperparams": {"alpha": 1.0, "beta": 0.5}, "operators_used": ["logsigmoid", "tanh", "zscore", "rank_gap"]}}, "better_than_baseline": false}
{"generation": 8, "index": 6, "ir": {"name": "Rank-Normalized Bradley-Terry Loss with Gated Margin", "intuition": "This loss function reframes the preference learning problem by adaptively weighting the classic Bradley-Terry (log-sigmoid) loss. It combines a rank-normalized cost signal with a smoothly gated margin to create a stable and expressive objective.\n\nInherited Ideas:\n- From 'Rank-Aware Log-Sigmoid Preference Loss' (Parent 1), it inherits the core structure of a weighted log-sigmoid loss: `-weight * logsigmoid(delta_logp)`. This provides a probabilistic interpretation and a strong gradient signal.\n- From 'Sigmoid-Gated Adaptive Margin Loss with Rank-Gap Normalization' (Parent 0), it inherits the idea of using a rank-based normalization scheme. Instead of normalizing a margin, we normalize the cost difference itself, making the loss scale-invariant with respect to the batch's cost distribution.\n\nNew Coupling Ideas:\n1. **Rank-Gap Normalization of Cost Signal**: The `delta_cost` is normalized by the batch's rank gap (`rank_l - rank_w`). This novel coupling, `delta_cost / (1 + rank_gap)`, makes the cost signal relative. For pairs with a large rank gap (easy to distinguish), the cost signal is down-weighted, preventing them from dominating the loss. For pairs with a small rank gap (hard to distinguish), the cost signal is amplified, focusing the model on fine-grained differences. This is more direct than Parent 0's margin normalization.\n2. **Tanh-Gated Margin**: A margin is introduced to the log-probability difference: `delta_logp - margin`. This margin is dynamically calculated using a `tanh` gate on the normalized cost signal: `gamma * tanh(normalized_cost_signal)`. The `tanh` function creates a bounded, saturating margin. This prevents extremely large cost differences from creating an unbounded margin that could cause gradient explosion, ensuring numerical stability while still providing a strong separation target for most pairs.", "pseudocode": "1. For each pair (w, l) where cost(w) < cost(l), calculate the cost difference `delta_cost = cost(l) - cost(w)` and the log probability difference `delta_logp = logp(w) - logp(l)`.\n2. Inherit from Parent 0: Determine the cost ranks for all `cost_w` and `cost_l` in the batch. Calculate the rank difference `rank_gap = rank(cost_l) - rank(cost_w)`.\n3. New Coupling (Rank-Gap Normalization of Cost): Create a rank-normalized cost signal by dividing the cost difference by the rank gap: `normalized_cost_signal = delta_cost / (1.0 + rank_gap)`. Adding 1.0 to the denominator prevents division by zero.\n4. New Coupling (Tanh-Gated Margin): Compute a bounded, adaptive margin using the `tanh` function on the normalized cost signal: `margin = gamma * tanh(normalized_cost_signal)`. `gamma` is a hyperparameter controlling the maximum margin size.\n5. Inherit from Parent 1: Use the core structure of a weighted log-sigmoid loss. The loss is computed as `loss = -logsigmoid(delta_logp - margin)`.\n6. Create an adaptive weight for each loss term using `softplus` on the normalized cost signal: `adaptive_weight = softplus(normalized_cost_signal)`.\n7. Combine the weight and the core loss: `final_loss = adaptive_weight * loss`.\n8. Return the weighted mean of `final_loss` over the batch.", "hyperparams": {"gamma": 0.5}, "operators_used": ["logsigmoid", "softplus", "tanh", "rank_gap"], "implementation_hint": {"expects": ["cost_a", "cost_b", "log_prob_w", "log_prob_l", "weight"], "returns": "scalar"}, "code": "import torch\nimport torch.nn.functional as F\n\ndef generated_loss(batch, model_output, extra):\n    \"\"\"\n    Computes the Rank-Normalized Bradley-Terry Loss with Gated Margin.\n\n    Inherits:\n    - The core weighted log-sigmoid loss structure from Parent 1.\n    - The use of rank differences for batch-aware normalization from Parent 0.\n\n    Introduces:\n    - A new coupling where the cost difference is directly normalized by the rank gap.\n    - A tanh-gated margin applied to the log-probability difference for stable separation.\n    \"\"\"\n    # For preference learning, we assume cost_w < cost_l.\n    cost_w = torch.min(batch['cost_a'], batch['cost_b'])\n    cost_l = torch.max(batch['cost_a'], batch['cost_b'])\n    log_prob_w = batch['log_prob_w']\n    log_prob_l = batch['log_prob_l']\n\n    # Retrieve hyperparameters\n    gamma = extra.get('gamma', 0.5)\n\n    # 1. Calculate cost and log probability differences\n    delta_cost = cost_l - cost_w\n    delta_logp = log_prob_w - log_prob_l\n\n    # 2. Inherited from Parent 0: Calculate rank differences\n    all_costs = torch.cat([cost_w, cost_l], dim=0)\n    # Use searchsorted for efficient, differentiable ranking\n    sorted_costs, _ = torch.sort(all_costs)\n    rank_w = torch.searchsorted(sorted_costs, cost_w)\n    rank_l = torch.searchsorted(sorted_costs, cost_l)\n    rank_gap = (rank_l - rank_w).float().clamp(min=0) # Ensure non-negative\n\n    # 3. New Coupling: Rank-Gap Normalization of Cost Signal\n    # Add 1.0 to the denominator to prevent division by zero and handle same-rank pairs.\n    # Detach to treat normalization as a fixed target for the current batch.\n    normalized_cost_signal = (delta_cost / (1.0 + rank_gap)).detach()\n\n    # 4. New Coupling: Tanh-Gated Margin\n    # tanh creates a bounded margin, preventing instability from very large cost differences.\n    margin = gamma * torch.tanh(normalized_cost_signal)\n\n    # 5. Inherited from Parent 1: Core log-sigmoid loss, now with a margin\n    # We want to encourage delta_logp > margin.\n    base_loss = -F.logsigmoid(delta_logp - margin)\n\n    # 6. Create an adaptive weight for each loss term\n    # softplus ensures the weight is non-negative and grows with the normalized cost difference.\n    adaptive_weight = F.softplus(normalized_cost_signal)\n\n    # 7. Combine the weight and the core loss\n    final_loss = adaptive_weight * base_loss\n\n    # 8. Apply optional weights and compute the mean\n    weights = batch.get('weight', None)\n    if weights is not None:\n        final_loss = final_loss * weights\n        # Use clamp to prevent division by zero if all weights are zero\n        return final_loss.sum() / weights.sum().clamp(min=1e-8)\n    else:\n        return final_loss.mean()"}, "fitness": {"hf_like_score": 32.64985466003418, "validation_objective": 32.64985466003418, "generalization_penalty": 0.0, "generalization_objectives": {"100": 32.25279998779297}, "train_score_mean": 39.2734341249466, "train_loss_mean": 0.8225363616347313, "pair_count": 129023911, "config": {"hf": {"problem": "tsp", "hf_steps": 1000, "train_problem_size": 100, "valid_problem_sizes": [100], "train_batch_size": 64, "pomo_size": 64, "learning_rate": 0.0003, "weight_decay": 1e-06, "alpha": 0.05, "device": "cuda", "seed": 1234, "num_validation_episodes": 128, "validation_batch_size": 64, "generalization_penalty_weight": 1.0, "pool_version": "v0"}, "free_loss": {"f1_steps": 1000, "f2_steps": 100, "f3_enabled": false}}, "loss_ir": {"name": "Rank-Normalized Bradley-Terry Loss with Gated Margin", "intuition": "This loss function reframes the preference learning problem by adaptively weighting the classic Bradley-Terry (log-sigmoid) loss. It combines a rank-normalized cost signal with a smoothly gated margin to create a stable and expressive objective.\n\nInherited Ideas:\n- From 'Rank-Aware Log-Sigmoid Preference Loss' (Parent 1), it inherits the core structure of a weighted log-sigmoid loss: `-weight * logsigmoid(delta_logp)`. This provides a probabilistic interpretation and a strong gradient signal.\n- From 'Sigmoid-Gated Adaptive Margin Loss with Rank-Gap Normalization' (Parent 0), it inherits the idea of using a rank-based normalization scheme. Instead of normalizing a margin, we normalize the cost difference itself, making the loss scale-invariant with respect to the batch's cost distribution.\n\nNew Coupling Ideas:\n1. **Rank-Gap Normalization of Cost Signal**: The `delta_cost` is normalized by the batch's rank gap (`rank_l - rank_w`). This novel coupling, `delta_cost / (1 + rank_gap)`, makes the cost signal relative. For pairs with a large rank gap (easy to distinguish), the cost signal is down-weighted, preventing them from dominating the loss. For pairs with a small rank gap (hard to distinguish), the cost signal is amplified, focusing the model on fine-grained differences. This is more direct than Parent 0's margin normalization.\n2. **Tanh-Gated Margin**: A margin is introduced to the log-probability difference: `delta_logp - margin`. This margin is dynamically calculated using a `tanh` gate on the normalized cost signal: `gamma * tanh(normalized_cost_signal)`. The `tanh` function creates a bounded, saturating margin. This prevents extremely large cost differences from creating an unbounded margin that could cause gradient explosion, ensuring numerical stability while still providing a strong separation target for most pairs.", "hyperparams": {"gamma": 0.5}, "operators_used": ["logsigmoid", "softplus", "tanh", "rank_gap"]}}, "better_than_baseline": false}
{"generation": 8, "index": 7, "ir": {"name": "Rank-Normalized Margin Loss with Tanh Gate", "intuition": "This loss function creates a stable and adaptive preference signal by combining a classic log-sigmoid structure with a dynamically scaled margin. It focuses learning on difficult pairs by modulating the loss based on the model's current confidence.\n\nInherited Ideas:\n- From 'Rank-Aware Log-Sigmoid Preference Loss' (Parent 1), it inherits the core loss structure of `-logsigmoid(scaled_term)`. This provides a probabilistic interpretation and a strong gradient signal, especially when the model is wrong.\n- From 'Sigmoid-Gated Adaptive Margin Loss' (Parent 0), it inherits the idea of creating an adaptive margin that is normalized by the rank gap. This ensures the margin's scale is directly coupled to the pair's relative importance within the batch, preventing oversized margins for pairs that are already easy to distinguish based on their large cost differences.\n\nNew Coupling Ideas:\n1. **Rank-Normalized Margin within Log-Sigmoid**: Instead of using the rank-normalized margin in a `softplus` structure as in Parent 0, it is coupled directly with the log-probability difference inside the `logsigmoid` function. The core term becomes `-logsigmoid(delta_logp - margin)`. This frames the problem as ensuring the log-probability difference `delta_logp` exceeds the `margin`, which itself is dynamically scaled by the rank gap. This provides a clear, margin-based objective within a probabilistic loss framework.\n2. **Tanh Confidence Gating**: The final loss is multiplied by a gate: `1.0 - tanh(clamp(delta_logp, min=0))`. This gate has two effects: for pairs the model already correctly prefers (positive `delta_logp`), the `tanh` approaches 1, and the gate `(1 - tanh)` approaches 0, reducing the loss contribution and preventing the model from wasting capacity on already-learned examples. For pairs the model incorrectly prefers (negative `delta_logp`), the `clamp` makes the term 0, `tanh(0)` is 0, and the gate is 1, applying the full loss. This creates a one-sided focus mechanism on misclassified or uncertain pairs, improving training efficiency and stability.", "pseudocode": "1. For each pair (w, l) where cost(w) < cost(l), calculate the cost difference `delta_cost = cost(l) - cost(w)` and the log probability difference `delta_logp = logp(w) - logp(l)`.\n2. Inherit from Parent 0: Determine the cost ranks for `cost_w` and `cost_l` in the batch. Calculate the rank difference `rank_diff = rank(cost_l) - rank(cost_w)`.\n3. Inherit from Parent 0: Create a base margin using `softplus(delta_cost)`. Normalize this margin using the rank difference to create the `adaptive_margin = (alpha * softplus(delta_cost)) / (1.0 + beta * rank_diff)`.\n4. New Coupling (Margin in Log-Sigmoid): Combine the model's log-probability difference with the adaptive margin to form the core loss argument: `argument = delta_logp - adaptive_margin`.\n5. Inherit from Parent 1: Calculate the base preference loss using the `logsigmoid` function: `base_loss = -logsigmoid(argument)`.\n6. New Coupling (Tanh Gating): Create a confidence gate that down-weights already correct predictions. Calculate `gate = 1.0 - tanh(relu(delta_logp))`. This gate is close to 1 for incorrect/uncertain pairs and close to 0 for confident, correct pairs.\n7. Compute the final per-sample loss by applying the gate: `final_loss = gate * base_loss`.\n8. Return the weighted mean of the per-sample losses over the batch.", "hyperparams": {"alpha": 1.0, "beta": 0.1}, "operators_used": ["logsigmoid", "softplus", "tanh", "relu", "rank_gap"], "implementation_hint": {"expects": ["cost_a", "cost_b", "logp_a", "logp_b"], "returns": "scalar"}, "code": "import torch\nimport torch.nn.functional as F\n\ndef generated_loss(batch, model_output, extra):\n    \"\"\"\n    Computes the Rank-Normalized Margin Loss with Tanh Gate.\n\n    Inherits:\n    - The core `-logsigmoid()` structure for a probabilistic loss (from Parent 1).\n    - The use of a rank-gap-normalized margin to make the target separation batch-aware (from Parent 0).\n\n    Introduces:\n    - A new coupling where the rank-normalized margin is used *inside* the logsigmoid, as `-logsigmoid(delta_logp - margin)`.\n    - A Tanh-based confidence gate, `1 - tanh(relu(delta_logp))`, to focus loss on misclassified or uncertain pairs.\n    \"\"\"\n    # For preference learning, we assume cost_w < cost_l.\n    cost_w = torch.min(batch['cost_a'], batch['cost_b'])\n    cost_l = torch.max(batch['cost_a'], batch['cost_b'])\n    log_prob_w = batch['log_prob_w']\n    log_prob_l = batch['log_prob_l']\n\n    # Retrieve hyperparameters\n    alpha = extra.get('alpha', 1.0)\n    beta = extra.get('beta', 0.1)\n\n    # 1. Calculate cost and log probability differences\n    delta_cost = cost_l - cost_w\n    delta_logp = log_prob_w - log_prob_l\n    batch_size = delta_cost.shape[0]\n\n    # 2. Inherited from Parent 0: Calculate rank differences.\n    all_costs = torch.cat([cost_w, cost_l], dim=0)\n    sorted_costs, _ = torch.sort(all_costs)\n    rank_w = torch.searchsorted(sorted_costs, cost_w)\n    rank_l = torch.searchsorted(sorted_costs, cost_l)\n    rank_diff = (rank_l - rank_w).float()\n\n    # 3. Inherited from Parent 0: Create a rank-normalized adaptive margin.\n    # The margin is larger for bigger cost differences but is tempered by the rank gap.\n    margin_base = alpha * F.softplus(delta_cost)\n    # Add 1.0 to the denominator to prevent division by zero and ensure it's always >= 1.\n    margin_normalizer = 1.0 + beta * rank_diff\n    adaptive_margin = margin_base / margin_normalizer.detach()\n\n    # 4. New Coupling: Place the margin inside the logsigmoid term.\n    argument = delta_logp - adaptive_margin\n\n    # 5. Inherited from Parent 1: Use the core logsigmoid loss structure.\n    base_loss = -F.logsigmoid(argument)\n\n    # 6. New Coupling: Tanh Confidence Gating.\n    # For delta_logp > 0 (correct preference), relu(delta_logp) > 0, tanh(...) approaches 1, gate approaches 0.\n    # For delta_logp <= 0 (incorrect preference), relu(delta_logp) = 0, tanh(0) = 0, gate is 1.\n    # This focuses the loss on samples the model gets wrong.\n    confidence_gate = 1.0 - torch.tanh(F.relu(delta_logp.detach()))\n\n    # 7. Compute the final loss.\n    final_loss = confidence_gate * base_loss\n\n    # 8. Apply optional weights and compute the mean.\n    weights = batch.get('weight', None)\n    if weights is not None:\n        final_loss = final_loss * weights\n        # Use clamp to prevent division by zero for an all-zero weight tensor.\n        return final_loss.sum() / weights.sum().clamp(min=1e-8)\n    else:\n        return final_loss.mean()"}, "fitness": {"hf_like_score": 48.35121726989746, "validation_objective": 48.25134086608887, "generalization_penalty": 0.09987640380859375, "generalization_objectives": {"100": 48.35121726989746}, "train_score_mean": 39.49951585483551, "train_loss_mean": 1.4367490932941436, "pair_count": 129023921, "config": {"hf": {"problem": "tsp", "hf_steps": 1000, "train_problem_size": 100, "valid_problem_sizes": [100], "train_batch_size": 64, "pomo_size": 64, "learning_rate": 0.0003, "weight_decay": 1e-06, "alpha": 0.05, "device": "cuda", "seed": 1234, "num_validation_episodes": 128, "validation_batch_size": 64, "generalization_penalty_weight": 1.0, "pool_version": "v0"}, "free_loss": {"f1_steps": 1000, "f2_steps": 100, "f3_enabled": false}}, "loss_ir": {"name": "Rank-Normalized Margin Loss with Tanh Gate", "intuition": "This loss function creates a stable and adaptive preference signal by combining a classic log-sigmoid structure with a dynamically scaled margin. It focuses learning on difficult pairs by modulating the loss based on the model's current confidence.\n\nInherited Ideas:\n- From 'Rank-Aware Log-Sigmoid Preference Loss' (Parent 1), it inherits the core loss structure of `-logsigmoid(scaled_term)`. This provides a probabilistic interpretation and a strong gradient signal, especially when the model is wrong.\n- From 'Sigmoid-Gated Adaptive Margin Loss' (Parent 0), it inherits the idea of creating an adaptive margin that is normalized by the rank gap. This ensures the margin's scale is directly coupled to the pair's relative importance within the batch, preventing oversized margins for pairs that are already easy to distinguish based on their large cost differences.\n\nNew Coupling Ideas:\n1. **Rank-Normalized Margin within Log-Sigmoid**: Instead of using the rank-normalized margin in a `softplus` structure as in Parent 0, it is coupled directly with the log-probability difference inside the `logsigmoid` function. The core term becomes `-logsigmoid(delta_logp - margin)`. This frames the problem as ensuring the log-probability difference `delta_logp` exceeds the `margin`, which itself is dynamically scaled by the rank gap. This provides a clear, margin-based objective within a probabilistic loss framework.\n2. **Tanh Confidence Gating**: The final loss is multiplied by a gate: `1.0 - tanh(clamp(delta_logp, min=0))`. This gate has two effects: for pairs the model already correctly prefers (positive `delta_logp`), the `tanh` approaches 1, and the gate `(1 - tanh)` approaches 0, reducing the loss contribution and preventing the model from wasting capacity on already-learned examples. For pairs the model incorrectly prefers (negative `delta_logp`), the `clamp` makes the term 0, `tanh(0)` is 0, and the gate is 1, applying the full loss. This creates a one-sided focus mechanism on misclassified or uncertain pairs, improving training efficiency and stability.", "hyperparams": {"alpha": 1.0, "beta": 0.1}, "operators_used": ["logsigmoid", "softplus", "tanh", "relu", "rank_gap"]}}, "better_than_baseline": false}
{"generation": 9, "index": 0, "ir": {"name": "Tanh-Gated Log-Sigmoid Loss with Rank-Aware Margin", "intuition": "This loss function combines the classic log-sigmoid structure for preference learning with a dynamic, rank-aware margin, and introduces a novel gating mechanism to stabilize training and focus learning on difficult examples.\n\nInherited Ideas:\n- From 'Rank-Aware Log-Sigmoid Preference Loss' (Parent 1), it inherits the core loss structure of `-logsigmoid(X)`, where `X` is a function of the log-probability difference. This provides a probabilistic interpretation rooted in the Bradley-Terry model.\n- From 'Sigmoid-Gated Adaptive Margin Loss with Rank-Gap Normalization' (Parent 0), it inherits the idea of using `softplus(delta_cost)` to create a smooth, non-saturating, and non-negative adaptive margin that scales with the difference in cost between the preferred and dispreferred candidates.\n\nNew Coupling Ideas:\n1.  **Rank-Aware Margin in Log-Sigmoid**: Instead of using cost information to weight the final loss (as in Parent 1), the `softplus(delta_cost)` term is integrated *inside* the `logsigmoid` as a margin. The core term becomes `-logsigmoid(delta_logp - margin)`. This directly enforces a separation in log-probabilities that is proportional to the cost difference, making the optimization target more explicit.\n2.  **Tanh Gating on Log-Probability Difference**: The model's log-probability difference `delta_logp` is passed through a `tanh` gate: `delta_logp * tanh(gamma * delta_logp)`. This serves two key purposes for stability and focus: (a) For pairs where the model is already very confident and correct (large positive `delta_logp`), the `tanh` term saturates to 1, preventing the loss from being driven by excessively large log-probability differences and avoiding potential gradient explosion. (b) For pairs where the model is confidently wrong (large negative `delta_logp`), the gate also saturates, ensuring a strong but bounded penalty, unlike an unbounded linear term.", "pseudocode": "1. For each pair (w, l) where cost(w) < cost(l), calculate the cost difference `delta_cost = cost(l) - cost(w)` and the log probability difference `delta_logp = logp(w) - logp(l)`.\n2. Inherit from Parent 0: Calculate a smooth, non-saturating adaptive margin using `softplus`: `margin = alpha * softplus(delta_cost / temp_cost)`. Detach this margin to treat it as a fixed target for each pair.\n3. New Coupling (Tanh Gating): Apply a scaled `tanh` gate to the model's log-probability difference to create a bounded, stabilized signal: `gated_delta_logp = delta_logp * tanh(gamma * delta_logp)`.\n4. New Coupling (Rank-Aware Margin in Log-Sigmoid): Combine the gated log-probability difference and the adaptive margin inside a `logsigmoid` function. This is inherited from Parent 1's use of a `logsigmoid` core.\n5. Compute the final per-sample loss: `loss = -logsigmoid(gated_delta_logp - margin)`.\n6. Return the weighted mean of the per-sample losses over the batch.", "hyperparams": {"alpha": 1.0, "temp_cost": 1.0, "gamma": 1.0}, "operators_used": ["logsigmoid", "softplus", "tanh", "log"], "implementation_hint": {"expects": ["cost_a", "cost_b", "logp_a", "logp_b"], "returns": "scalar"}, "code": "import torch\nimport torch.nn.functional as F\n\ndef generated_loss(batch, model_output, extra):\n    \"\"\"\n    Computes the Tanh-Gated Log-Sigmoid Loss with Rank-Aware Margin.\n\n    Inherits:\n    - The core `-logsigmoid(X)` structure from Parent 1.\n    - The use of `softplus(delta_cost)` for an adaptive margin from Parent 0.\n\n    Introduces:\n    - A new coupling where the adaptive margin is placed *inside* the logsigmoid, creating a direct separation target.\n    - A `tanh` gate on the log-probability difference to stabilize gradients and prevent blowup from overly confident predictions.\n    \"\"\"\n    # For preference learning, we assume cost_w < cost_l.\n    cost_w = torch.min(batch['cost_a'], batch['cost_b'])\n    cost_l = torch.max(batch['cost_a'], batch['cost_b'])\n    log_prob_w = batch['log_prob_w']\n    log_prob_l = batch['log_prob_l']\n\n    # Retrieve hyperparameters\n    alpha = extra.get('alpha', 1.0)\n    temp_cost = extra.get('temp_cost', 1.0)\n    gamma = extra.get('gamma', 1.0)\n\n    # 1. Calculate cost and log probability differences\n    delta_cost = cost_l - cost_w\n    delta_logp = log_prob_w - log_prob_l\n\n    # 2. Inherited from Parent 0: Calculate a smooth, non-saturating adaptive margin.\n    # We detach the margin as it serves as a target, not something to be optimized through.\n    margin = alpha * F.softplus(delta_cost / temp_cost).detach()\n\n    # 3. New Coupling: Tanh Gating on delta_logp.\n    # This bounds the influence of delta_logp, preventing gradient explosion from pairs where the model is already very confident.\n    # The gate is `delta_logp * tanh(gamma * delta_logp)`.\n    gated_delta_logp = delta_logp * torch.tanh(gamma * delta_logp)\n\n    # 4. New Coupling & Inheritance from Parent 1: Combine into a logsigmoid loss.\n    # The margin is now inside the logsigmoid, creating a direct separation target.\n    # The loss is minimized when `gated_delta_logp > margin`.\n    loss = -F.logsigmoid(gated_delta_logp - margin)\n\n    # 5. Apply optional weights and compute the mean.\n    weights = batch.get('weight', None)\n    if weights is not None:\n        loss = loss * weights\n        # Use clamp to avoid division by zero for an all-zero weight tensor\n        return loss.sum() / weights.sum().clamp(min=1e-8)\n    else:\n        return loss.mean()"}, "fitness": {"hf_like_score": 49.746524810791016, "validation_objective": 49.487709045410156, "generalization_penalty": 0.2588157653808594, "generalization_objectives": {"100": 49.746524810791016}, "train_score_mean": 34.92274066162109, "train_loss_mean": 0.03920233099162579, "pair_count": 129023942, "config": {"hf": {"problem": "tsp", "hf_steps": 1000, "train_problem_size": 100, "valid_problem_sizes": [100], "train_batch_size": 64, "pomo_size": 64, "learning_rate": 0.0003, "weight_decay": 1e-06, "alpha": 0.05, "device": "cuda", "seed": 1234, "num_validation_episodes": 128, "validation_batch_size": 64, "generalization_penalty_weight": 1.0, "pool_version": "v0"}, "free_loss": {"f1_steps": 1000, "f2_steps": 100, "f3_enabled": false}}, "loss_ir": {"name": "Tanh-Gated Log-Sigmoid Loss with Rank-Aware Margin", "intuition": "This loss function combines the classic log-sigmoid structure for preference learning with a dynamic, rank-aware margin, and introduces a novel gating mechanism to stabilize training and focus learning on difficult examples.\n\nInherited Ideas:\n- From 'Rank-Aware Log-Sigmoid Preference Loss' (Parent 1), it inherits the core loss structure of `-logsigmoid(X)`, where `X` is a function of the log-probability difference. This provides a probabilistic interpretation rooted in the Bradley-Terry model.\n- From 'Sigmoid-Gated Adaptive Margin Loss with Rank-Gap Normalization' (Parent 0), it inherits the idea of using `softplus(delta_cost)` to create a smooth, non-saturating, and non-negative adaptive margin that scales with the difference in cost between the preferred and dispreferred candidates.\n\nNew Coupling Ideas:\n1.  **Rank-Aware Margin in Log-Sigmoid**: Instead of using cost information to weight the final loss (as in Parent 1), the `softplus(delta_cost)` term is integrated *inside* the `logsigmoid` as a margin. The core term becomes `-logsigmoid(delta_logp - margin)`. This directly enforces a separation in log-probabilities that is proportional to the cost difference, making the optimization target more explicit.\n2.  **Tanh Gating on Log-Probability Difference**: The model's log-probability difference `delta_logp` is passed through a `tanh` gate: `delta_logp * tanh(gamma * delta_logp)`. This serves two key purposes for stability and focus: (a) For pairs where the model is already very confident and correct (large positive `delta_logp`), the `tanh` term saturates to 1, preventing the loss from being driven by excessively large log-probability differences and avoiding potential gradient explosion. (b) For pairs where the model is confidently wrong (large negative `delta_logp`), the gate also saturates, ensuring a strong but bounded penalty, unlike an unbounded linear term.", "hyperparams": {"alpha": 1.0, "temp_cost": 1.0, "gamma": 1.0}, "operators_used": ["logsigmoid", "softplus", "tanh", "log"]}}, "better_than_baseline": false}
{"generation": 9, "index": 1, "ir": {"name": "Rank-Gated Log-Sigmoid Loss with Tanh-Normalized Margin", "intuition": "This loss function blends the stability of the classic log-sigmoid framework with a dynamic, bounded margin, creating a robust preference signal that adapts to both the cost and rank distribution within a batch.\n\nInherited Ideas:\n- From 'Rank-Aware Log-Sigmoid Preference Loss' (Parent 1), it inherits the core structure of a weighted log-sigmoid loss: `-weight * logsigmoid(delta_logp)`. This provides a strong probabilistic foundation and avoids the potential for vanishing gradients when the model is already correct, unlike margin-based losses.\n- From 'Sigmoid-Gated Adaptive Margin Loss with Rank-Gap Normalization' (Parent 0), it inherits the use of a rank-gap signal to modulate the loss. This focuses the model's attention on pairs that are far apart in the batch's cost distribution, treating them as more significant learning signals.\n\nNew Coupling Ideas:\n1.  **Tanh-Normalized Adaptive Margin**: Instead of directly weighting the loss with cost differences, this child loss introduces a dynamic margin inside the `logsigmoid` term: `logsigmoid(delta_logp - margin)`. The margin itself is a novel construction: `margin = alpha * tanh(beta * softplus(zscore(delta_cost)))`. This creates a margin that is adaptive to the batch's cost differences (via `zscore` and `softplus`) but is smoothly bounded between 0 and `alpha` by the `tanh` function. This prevents excessively large margins from causing numerical instability or overly aggressive updates for pairs with huge cost differences.\n2.  **Softplus Rank Gating**: The final loss is gated by a rank-based weight, `rank_gate * -logsigmoid(...)`. The gate is calculated as `softplus(gamma * rank_diff)`. This smoothly and non-saturatingly increases the weight for pairs with larger rank separation, providing a more robust and continuous emphasis compared to linear or exponential weighting schemes.", "pseudocode": "1. For each pair (w, l) where cost(w) < cost(l), calculate the cost difference `delta_cost = cost(l) - cost(w)` and the log probability difference `delta_logp = logp(w) - logp(l)`.\n2. Inherit from Parent 1 and Parent 0: Standardize the `delta_cost` vector for the entire batch using Z-score normalization to get `delta_cost_norm` for scale invariance.\n3. New Coupling (Tanh-Normalized Margin): Create an adaptive, bounded margin. First, apply `softplus` to the normalized cost difference. Then, scale this by `beta` and pass it through a `tanh` function. Finally, scale by `alpha` to control the maximum margin: `margin = alpha * tanh(beta * softplus(delta_cost_norm))`.\n4. Inherit from Parent 0: Calculate the rank difference `rank_diff = rank(cost_l) - rank(cost_w)` based on the positions of `cost_w` and `cost_l` within all sorted costs in the batch.\n5. New Coupling (Softplus Rank Gating): Create a smooth, non-negative, and non-saturating weight from the rank difference: `rank_gate = softplus(gamma * rank_diff)`.\n6. Inherit from Parent 1: Compute the core preference loss using the log-sigmoid function, but now incorporating the adaptive margin: `base_loss = -logsigmoid(delta_logp - margin)`.\n7. Modulate the base loss with the rank gate: `final_loss = rank_gate * base_loss`.\n8. Return the weighted mean of `final_loss` over the batch.", "hyperparams": {"alpha": 2.0, "beta": 0.5, "gamma": 0.1}, "operators_used": ["logsigmoid", "softplus", "tanh", "zscore", "rank_gap"], "implementation_hint": {"expects": ["cost_a", "cost_b", "log_prob_w", "log_prob_l", "weight"], "returns": "scalar"}, "code": "import torch\nimport torch.nn.functional as F\n\ndef generated_loss(batch, model_output, extra):\n    \"\"\"\n    Computes the Rank-Gated Log-Sigmoid Loss with Tanh-Normalized Margin.\n\n    Inherits:\n    - The core weighted log-sigmoid structure from Parent 1.\n    - The use of rank differences for batch-aware modulation from Parent 0.\n\n    Introduces:\n    - A tanh-normalized adaptive margin inside the logsigmoid, providing a bounded target separation.\n    - A softplus-gated rank difference as a final loss weight for smooth, non-saturating emphasis.\n    \"\"\"\n    # For preference learning, we assume cost_w < cost_l.\n    cost_w = torch.min(batch['cost_a'], batch['cost_b'])\n    cost_l = torch.max(batch['cost_a'], batch['cost_b'])\n    log_prob_w = batch['log_prob_w']\n    log_prob_l = batch['log_prob_l']\n\n    # Retrieve hyperparameters\n    alpha = extra.get('alpha', 2.0)\n    beta = extra.get('beta', 0.5)\n    gamma = extra.get('gamma', 0.1)\n\n    # 1. Calculate cost and log probability differences\n    delta_cost = cost_l - cost_w\n    delta_logp = log_prob_w - log_prob_l\n\n    # Handle batches with no cost variance to prevent division by zero in z-score\n    if delta_cost.std() < 1e-8:\n        delta_cost_norm = torch.zeros_like(delta_cost)\n    else:\n        # 2. Inherited: Z-score normalization of cost differences for batch-wise scale invariance\n        delta_cost_mean = delta_cost.mean()\n        delta_cost_std = delta_cost.std().clamp(min=1e-8)\n        delta_cost_norm = (delta_cost - delta_cost_mean) / delta_cost_std\n\n    # 3. New Coupling: Tanh-Normalized Adaptive Margin\n    # softplus ensures the input to tanh is non-negative, so the margin is always positive.\n    # tanh ensures the margin is smoothly bounded between 0 and alpha.\n    margin = alpha * torch.tanh(beta * F.softplus(delta_cost_norm))\n\n    # 4. Inherited: Calculate rank difference\n    all_costs = torch.cat([cost_w, cost_l], dim=0)\n    sorted_costs, _ = torch.sort(all_costs)\n    rank_w = torch.searchsorted(sorted_costs, cost_w)\n    rank_l = torch.searchsorted(sorted_costs, cost_l)\n    rank_diff = (rank_l - rank_w).float()\n\n    # 5. New Coupling: Softplus Rank Gating\n    # Creates a smooth, non-saturating weight based on rank separation.\n    rank_gate = F.softplus(gamma * rank_diff)\n\n    # 6. Inherited: Compute core log-sigmoid loss with the new margin\n    base_loss = -F.logsigmoid(delta_logp - margin.detach())\n\n    # 7. Modulate the base loss with the rank gate\n    final_loss = rank_gate.detach() * base_loss\n\n    # 8. Apply optional weights and compute the mean.\n    weights = batch.get('weight', None)\n    if weights is not None:\n        final_loss = final_loss * weights\n        return final_loss.sum() / weights.sum().clamp(min=1e-8)\n    else:\n        return final_loss.mean()"}, "fitness": {"hf_like_score": 35.45460891723633, "validation_objective": 35.45460891723633, "generalization_penalty": 0.0, "generalization_objectives": {"100": 35.154624938964844}, "train_score_mean": 38.460824007034304, "train_loss_mean": 7899.814846191406, "pair_count": 129023918, "config": {"hf": {"problem": "tsp", "hf_steps": 1000, "train_problem_size": 100, "valid_problem_sizes": [100], "train_batch_size": 64, "pomo_size": 64, "learning_rate": 0.0003, "weight_decay": 1e-06, "alpha": 0.05, "device": "cuda", "seed": 1234, "num_validation_episodes": 128, "validation_batch_size": 64, "generalization_penalty_weight": 1.0, "pool_version": "v0"}, "free_loss": {"f1_steps": 1000, "f2_steps": 100, "f3_enabled": false}}, "loss_ir": {"name": "Rank-Gated Log-Sigmoid Loss with Tanh-Normalized Margin", "intuition": "This loss function blends the stability of the classic log-sigmoid framework with a dynamic, bounded margin, creating a robust preference signal that adapts to both the cost and rank distribution within a batch.\n\nInherited Ideas:\n- From 'Rank-Aware Log-Sigmoid Preference Loss' (Parent 1), it inherits the core structure of a weighted log-sigmoid loss: `-weight * logsigmoid(delta_logp)`. This provides a strong probabilistic foundation and avoids the potential for vanishing gradients when the model is already correct, unlike margin-based losses.\n- From 'Sigmoid-Gated Adaptive Margin Loss with Rank-Gap Normalization' (Parent 0), it inherits the use of a rank-gap signal to modulate the loss. This focuses the model's attention on pairs that are far apart in the batch's cost distribution, treating them as more significant learning signals.\n\nNew Coupling Ideas:\n1.  **Tanh-Normalized Adaptive Margin**: Instead of directly weighting the loss with cost differences, this child loss introduces a dynamic margin inside the `logsigmoid` term: `logsigmoid(delta_logp - margin)`. The margin itself is a novel construction: `margin = alpha * tanh(beta * softplus(zscore(delta_cost)))`. This creates a margin that is adaptive to the batch's cost differences (via `zscore` and `softplus`) but is smoothly bounded between 0 and `alpha` by the `tanh` function. This prevents excessively large margins from causing numerical instability or overly aggressive updates for pairs with huge cost differences.\n2.  **Softplus Rank Gating**: The final loss is gated by a rank-based weight, `rank_gate * -logsigmoid(...)`. The gate is calculated as `softplus(gamma * rank_diff)`. This smoothly and non-saturatingly increases the weight for pairs with larger rank separation, providing a more robust and continuous emphasis compared to linear or exponential weighting schemes.", "hyperparams": {"alpha": 2.0, "beta": 0.5, "gamma": 0.1}, "operators_used": ["logsigmoid", "softplus", "tanh", "zscore", "rank_gap"]}}, "better_than_baseline": false}
{"generation": 9, "index": 2, "ir": {"name": "Rank-Gated Log-Sigmoid Loss with Z-Score Margin", "intuition": "This loss function creates a stable, rank-aware preference signal by combining a classic log-sigmoid structure with a dynamically scaled margin. It focuses learning on pairs that are either closely ranked or where the model is confidently wrong.\n\nInherited Ideas:\n- From 'Rank-Aware Log-Sigmoid Preference Loss' (Parent 1), it inherits the core loss structure of `-logsigmoid(delta_logp - margin)`. This provides a probabilistic interpretation and a strong gradient signal, especially when the model's preference is incorrect.\n- From 'Sigmoid-Gated Adaptive Margin Loss with Rank-Gap Normalization' (Parent 0), it inherits the idea of using the rank difference (`rank_gap`) between pairs to modulate the loss, focusing attention on pairs with specific relationships within the batch's cost distribution.\n\nNew Coupling Ideas:\n1.  **Z-Scored Cost Difference as Margin**: Instead of creating a complex margin from softplus-transformed costs, this loss uses the batch-normalized (Z-scored) cost difference directly as the margin: `margin = alpha * zscore(cost_l - cost_w)`. This couples the margin directly to the relative cost difference within the batch, making it robust to the absolute scale of costs and adaptive to the current batch's statistics. `alpha` controls the strength of this margin.\n2.  **Inverse Rank Gating**: The loss is weighted by an inverse rank gate: `weight = 1.0 / (1.0 + beta * rank_gap)`. This is a novel coupling where pairs with a *small* rank difference (i.e., similar costs) are given higher weight. This forces the model to learn fine-grained distinctions between closely competing candidates, which are often the hardest and most important examples. The denominator prevents division by zero and smooths the weighting.", "pseudocode": "1. For each pair (w, l) where cost(w) < cost(l), calculate the cost difference `delta_cost = cost(l) - cost(w)` and the log probability difference `delta_logp = logp(w) - logp(l)`.\n2. New Coupling (Z-Score Margin): Standardize the `delta_cost` vector for the batch using Z-score normalization. This normalized value, scaled by `alpha`, becomes the adaptive margin: `margin = alpha * zscore(delta_cost)`.\n3. Inherit from Parent 0 (Rank Modulation): Determine the cost ranks for all `cost_w` and `cost_l` in the batch. Calculate the rank difference `rank_gap = rank(cost_l) - rank(cost_w)`.\n4. New Coupling (Inverse Rank Gating): Create a weight that emphasizes pairs with small rank gaps. Calculate `weight = 1.0 / (1.0 + beta * rank_gap)`. This focuses the loss on fine-grained distinctions.\n5. Inherit from Parent 1 (Log-Sigmoid Core): Compute the core per-sample loss using the log-sigmoid function with the adaptive margin: `loss = -logsigmoid(delta_logp - margin)`.\n6. Combine the loss with the rank-based weight: `final_loss = weight * loss`.\n7. Return the mean of the final losses over the batch, applying optional external weights if provided.", "hyperparams": {"alpha": 0.5, "beta": 0.1}, "operators_used": ["logsigmoid", "zscore", "rank_gap"], "implementation_hint": {"expects": ["cost_a", "cost_b", "logp_a", "logp_b"], "returns": "scalar"}, "code": "import torch\nimport torch.nn.functional as F\n\ndef generated_loss(batch, model_output, extra):\n    \"\"\"\n    Computes the Rank-Gated Log-Sigmoid Loss with Z-Score Margin.\n\n    Inherits:\n    - The core `-logsigmoid(delta_logp - margin)` structure from Parent 1.\n    - The use of rank differences for batch-aware modulation from Parent 0.\n\n    Introduces:\n    - A new margin coupling where the margin is the Z-scored cost difference.\n    - An inverse rank gate `1 / (1 + beta * rank_gap)` to up-weight pairs with similar costs.\n    \"\"\"\n    # For preference learning, we assume cost_w < cost_l.\n    cost_w = torch.min(batch['cost_a'], batch['cost_b'])\n    cost_l = torch.max(batch['cost_a'], batch['cost_b'])\n    log_prob_w = batch['log_prob_w']\n    log_prob_l = batch['log_prob_l']\n\n    # Retrieve hyperparameters\n    alpha = extra.get('alpha', 0.5)\n    beta = extra.get('beta', 0.1)\n\n    # 1. Calculate cost and log probability differences\n    delta_cost = cost_l - cost_w\n    delta_logp = log_prob_w - log_prob_l\n    batch_size = delta_cost.shape[0]\n\n    # Handle batches with no cost variance to prevent NaN from z-scoring\n    if delta_cost.std() < 1e-8:\n        margin = torch.zeros_like(delta_cost)\n    else:\n        # 2. New Coupling: Z-Scored Cost Difference as Margin\n        delta_cost_mean = delta_cost.mean()\n        delta_cost_std = delta_cost.std()\n        delta_cost_norm = (delta_cost - delta_cost_mean) / delta_cost_std.clamp(min=1e-8)\n        margin = alpha * delta_cost_norm\n\n    # 3. Inherited: Calculate rank gap\n    all_costs = torch.cat([cost_w, cost_l], dim=0)\n    # Use torch.sort for compatibility and clarity\n    sorted_costs_unique = torch.unique_consecutive(torch.sort(all_costs).values)\n    rank_w = torch.searchsorted(sorted_costs_unique, cost_w, right=True) - 1\n    rank_l = torch.searchsorted(sorted_costs_unique, cost_l, right=True) - 1\n    rank_gap = (rank_l - rank_w).float().clamp(min=0)\n\n    # 4. New Coupling: Inverse Rank Gating\n    # This weight is higher for pairs with a small rank gap, focusing on fine-grained examples.\n    rank_weight = 1.0 / (1.0 + beta * rank_gap)\n\n    # 5. Inherited: Compute the core log-sigmoid loss\n    # Detach margin and weight so they act as targets/weights, not variables to be optimized through\n    # in the context of a model that might predict costs.\n    loss = -F.logsigmoid(delta_logp - margin.detach())\n\n    # 6. Apply the rank-based weight\n    final_loss = rank_weight.detach() * loss\n\n    # 7. Apply optional batch weights and compute the mean.\n    weights = batch.get('weight', None)\n    if weights is not None:\n        final_loss = final_loss * weights\n        return final_loss.sum() / weights.sum().clamp(min=1e-8)\n    else:\n        return final_loss.mean()"}, "fitness": {"hf_like_score": 41.66746520996094, "validation_objective": 41.66746520996094, "generalization_penalty": 0.0, "generalization_objectives": {"100": 41.04034423828125}, "train_score_mean": 39.68769811058044, "train_loss_mean": 0.030255129430443048, "pair_count": 129023935, "config": {"hf": {"problem": "tsp", "hf_steps": 1000, "train_problem_size": 100, "valid_problem_sizes": [100], "train_batch_size": 64, "pomo_size": 64, "learning_rate": 0.0003, "weight_decay": 1e-06, "alpha": 0.05, "device": "cuda", "seed": 1234, "num_validation_episodes": 128, "validation_batch_size": 64, "generalization_penalty_weight": 1.0, "pool_version": "v0"}, "free_loss": {"f1_steps": 1000, "f2_steps": 100, "f3_enabled": false}}, "loss_ir": {"name": "Rank-Gated Log-Sigmoid Loss with Z-Score Margin", "intuition": "This loss function creates a stable, rank-aware preference signal by combining a classic log-sigmoid structure with a dynamically scaled margin. It focuses learning on pairs that are either closely ranked or where the model is confidently wrong.\n\nInherited Ideas:\n- From 'Rank-Aware Log-Sigmoid Preference Loss' (Parent 1), it inherits the core loss structure of `-logsigmoid(delta_logp - margin)`. This provides a probabilistic interpretation and a strong gradient signal, especially when the model's preference is incorrect.\n- From 'Sigmoid-Gated Adaptive Margin Loss with Rank-Gap Normalization' (Parent 0), it inherits the idea of using the rank difference (`rank_gap`) between pairs to modulate the loss, focusing attention on pairs with specific relationships within the batch's cost distribution.\n\nNew Coupling Ideas:\n1.  **Z-Scored Cost Difference as Margin**: Instead of creating a complex margin from softplus-transformed costs, this loss uses the batch-normalized (Z-scored) cost difference directly as the margin: `margin = alpha * zscore(cost_l - cost_w)`. This couples the margin directly to the relative cost difference within the batch, making it robust to the absolute scale of costs and adaptive to the current batch's statistics. `alpha` controls the strength of this margin.\n2.  **Inverse Rank Gating**: The loss is weighted by an inverse rank gate: `weight = 1.0 / (1.0 + beta * rank_gap)`. This is a novel coupling where pairs with a *small* rank difference (i.e., similar costs) are given higher weight. This forces the model to learn fine-grained distinctions between closely competing candidates, which are often the hardest and most important examples. The denominator prevents division by zero and smooths the weighting.", "hyperparams": {"alpha": 0.5, "beta": 0.1}, "operators_used": ["logsigmoid", "zscore", "rank_gap"]}}, "better_than_baseline": false}
{"generation": 9, "index": 3, "ir": {"name": "Rank-Gated Log-Sigmoid Loss with Tanh-Scaled Margin", "intuition": "This loss function creates a stable and adaptive preference signal by combining a classic log-sigmoid structure with a dynamically scaled margin. It focuses on difficult examples by using rank information to gate the loss.\n\nInherited Ideas:\n- From 'Rank-Aware Log-Sigmoid Preference Loss' (Parent 1), it inherits the core loss structure of `-logsigmoid(delta_logp - margin)`. This is a classic preference loss formulation that encourages the log-probability difference to be greater than a margin.\n- From 'Sigmoid-Gated Adaptive Margin Loss with Rank-Gap Normalization' (Parent 0), it inherits the use of `rank_gap` to modulate the loss. Instead of normalizing a margin or being part of a weight, here the rank gap is used as a direct gate on the final loss value, focusing the model on pairs with a small rank difference (i.e., those that are harder to distinguish).\n\nNew Coupling Ideas:\n1.  **Tanh-Scaled Adaptive Margin**: The margin is not static or simply proportional to the cost difference. It's calculated as `alpha * tanh(beta * delta_cost)`. This couples the margin to the cost difference in a bounded way. For small `delta_cost`, the margin is approximately linear (`alpha * beta * delta_cost`), but for very large `delta_cost`, the margin saturates at `alpha`. This prevents extremely large cost differences from creating excessively large margins, which could lead to gradient explosion and numerical instability, while still providing an adaptive signal.\n2.  **Inverse Rank-Gap Gating**: The final loss is multiplied by `1 / (1 + gamma * rank_gap)`. This is a new form of gating. It heavily down-weights the loss for pairs that are far apart in the batch's cost ranking (large `rank_gap`), effectively telling the model to ignore pairs that are already easy to distinguish. Conversely, it preserves the full loss signal for pairs with a small rank difference, focusing training on fine-grained distinctions and difficult cases. The `+1` in the denominator ensures stability and that the gate is always between 0 and 1.", "pseudocode": "1. For each pair (w, l) where cost(w) < cost(l), calculate the cost difference `delta_cost = cost(l) - cost(w)` and the log probability difference `delta_logp = logp(w) - logp(l)`.\n2. New Coupling (Tanh-Scaled Margin): Calculate a bounded, adaptive margin using `margin = alpha * tanh(beta * delta_cost)`. This prevents the margin from growing uncontrollably with large cost differences.\n3. Inherit from Parent 1: Compute the base preference loss using a log-sigmoid structure with the adaptive margin: `base_loss = -logsigmoid(delta_logp - margin)`.\n4. Inherit from Parent 0: Determine the cost ranks for all `cost_w` and `cost_l` in the batch. Calculate the normalized rank difference `rank_gap = abs(rank(cost_l) - rank(cost_w)) / (2 * batch_size)`.\n5. New Coupling (Inverse Rank-Gap Gating): Create a gating factor that down-weights easy-to-distinguish pairs: `rank_gate = 1.0 / (1.0 + gamma * rank_gap)`.\n6. Apply the gate to the base loss: `final_loss = rank_gate * base_loss`.\n7. Return the weighted mean of the final loss over the batch.", "hyperparams": {"alpha": 1.0, "beta": 0.1, "gamma": 5.0}, "operators_used": ["logsigmoid", "tanh", "rank_gap"], "implementation_hint": {"expects": ["cost_w", "cost_l", "logp_w", "logp_l"], "returns": "scalar"}, "code": "import torch\nimport torch.nn.functional as F\n\ndef generated_loss(batch, model_output, extra):\n    \"\"\"\n    Computes the Rank-Gated Log-Sigmoid Loss with Tanh-Scaled Margin.\n\n    Inherits:\n    - The core `-logsigmoid(delta_logp - margin)` structure from Parent 1.\n    - The use of rank_gap for modulation, inspired by Parent 0.\n\n    Introduces:\n    - A tanh-scaled adaptive margin `alpha * tanh(beta * delta_cost)` for stability.\n    - An inverse rank-gap gate `1 / (1 + gamma * rank_gap)` to focus on hard-to-distinguish pairs.\n    \"\"\"\n    # For preference learning, we assume cost_w < cost_l.\n    cost_w = torch.min(batch['cost_a'], batch['cost_b'])\n    cost_l = torch.max(batch['cost_a'], batch['cost_b'])\n    log_prob_w = batch['log_prob_w']\n    log_prob_l = batch['log_prob_l']\n\n    # Retrieve hyperparameters\n    alpha = extra.get('alpha', 1.0)\n    beta = extra.get('beta', 0.1)\n    gamma = extra.get('gamma', 5.0)\n\n    # 1. Calculate cost and log probability differences\n    delta_cost = cost_l - cost_w\n    delta_logp = log_prob_w - log_prob_l\n    batch_size = delta_cost.shape[0]\n\n    # 2. New Coupling: Tanh-Scaled Adaptive Margin\n    # This creates a margin that grows with delta_cost but is bounded by alpha,\n    # preventing numerical instability from very large cost differences.\n    adaptive_margin = alpha * torch.tanh(beta * delta_cost)\n\n    # 3. Inherited from Parent 1: Core log-sigmoid loss structure\n    # The model is encouraged to make delta_logp > adaptive_margin.\n    base_loss = -F.logsigmoid(delta_logp - adaptive_margin.detach())\n\n    # 4. Inherited from Parent 0: Calculate rank gap\n    # Combine all costs to establish a batch-wide ranking.\n    all_costs = torch.cat([cost_w, cost_l], dim=0)\n    # Sorting is needed to establish ranks\n    sorted_indices = torch.argsort(all_costs)\n    ranks = torch.empty_like(sorted_indices)\n    ranks[sorted_indices] = torch.arange(len(all_costs), device=all_costs.device)\n    rank_w = ranks[:batch_size]\n    rank_l = ranks[batch_size:]\n    \n    # Calculate normalized rank difference\n    rank_diff = (rank_l - rank_w).float()\n    # Normalize by the max possible rank difference (2*N-1) for scale invariance\n    # We use 2*N as a simple, stable approximation.\n    normalized_rank_gap = rank_diff / (2 * batch_size)\n    \n    # 5. New Coupling: Inverse Rank-Gap Gating\n    # This gate is close to 1 for small rank gaps (hard pairs) and approaches 0 for large gaps (easy pairs).\n    # This focuses training on fine-grained distinctions.\n    rank_gate = 1.0 / (1.0 + gamma * normalized_rank_gap.detach())\n\n    # 6. Apply the gate to the loss\n    final_loss = rank_gate * base_loss\n\n    # 7. Apply optional weights and compute the mean\n    weights = batch.get('weight', None)\n    if weights is not None:\n        final_loss = final_loss * weights\n        # Use clamp to avoid division by zero if all weights are zero\n        return final_loss.sum() / weights.sum().clamp(min=1e-8)\n    else:\n        return final_loss.mean()"}, "fitness": {"hf_like_score": 31.103219985961914, "validation_objective": 31.103219985961914, "generalization_penalty": 0.0, "generalization_objectives": {"100": 30.882488250732422}, "train_score_mean": 39.46310137653351, "train_loss_mean": 0.746367000669241, "pair_count": 129023926, "config": {"hf": {"problem": "tsp", "hf_steps": 1000, "train_problem_size": 100, "valid_problem_sizes": [100], "train_batch_size": 64, "pomo_size": 64, "learning_rate": 0.0003, "weight_decay": 1e-06, "alpha": 0.05, "device": "cuda", "seed": 1234, "num_validation_episodes": 128, "validation_batch_size": 64, "generalization_penalty_weight": 1.0, "pool_version": "v0"}, "free_loss": {"f1_steps": 1000, "f2_steps": 100, "f3_enabled": false}}, "loss_ir": {"name": "Rank-Gated Log-Sigmoid Loss with Tanh-Scaled Margin", "intuition": "This loss function creates a stable and adaptive preference signal by combining a classic log-sigmoid structure with a dynamically scaled margin. It focuses on difficult examples by using rank information to gate the loss.\n\nInherited Ideas:\n- From 'Rank-Aware Log-Sigmoid Preference Loss' (Parent 1), it inherits the core loss structure of `-logsigmoid(delta_logp - margin)`. This is a classic preference loss formulation that encourages the log-probability difference to be greater than a margin.\n- From 'Sigmoid-Gated Adaptive Margin Loss with Rank-Gap Normalization' (Parent 0), it inherits the use of `rank_gap` to modulate the loss. Instead of normalizing a margin or being part of a weight, here the rank gap is used as a direct gate on the final loss value, focusing the model on pairs with a small rank difference (i.e., those that are harder to distinguish).\n\nNew Coupling Ideas:\n1.  **Tanh-Scaled Adaptive Margin**: The margin is not static or simply proportional to the cost difference. It's calculated as `alpha * tanh(beta * delta_cost)`. This couples the margin to the cost difference in a bounded way. For small `delta_cost`, the margin is approximately linear (`alpha * beta * delta_cost`), but for very large `delta_cost`, the margin saturates at `alpha`. This prevents extremely large cost differences from creating excessively large margins, which could lead to gradient explosion and numerical instability, while still providing an adaptive signal.\n2.  **Inverse Rank-Gap Gating**: The final loss is multiplied by `1 / (1 + gamma * rank_gap)`. This is a new form of gating. It heavily down-weights the loss for pairs that are far apart in the batch's cost ranking (large `rank_gap`), effectively telling the model to ignore pairs that are already easy to distinguish. Conversely, it preserves the full loss signal for pairs with a small rank difference, focusing training on fine-grained distinctions and difficult cases. The `+1` in the denominator ensures stability and that the gate is always between 0 and 1.", "hyperparams": {"alpha": 1.0, "beta": 0.1, "gamma": 5.0}, "operators_used": ["logsigmoid", "tanh", "rank_gap"]}}, "better_than_baseline": false}
{"generation": 9, "index": 4, "ir": {"name": "Adaptive Log-Sigmoid Loss with Tanh-Gated Rank Margin", "intuition": "This loss function synergizes the classic log-sigmoid preference structure with a dynamic, rank-aware margin, stabilized by batch normalization and a smooth gating mechanism.\n\nInherited Ideas:\n- From 'Rank-Aware Log-Sigmoid Preference Loss' (Parent 1), it inherits the core loss structure of `-logsigmoid(delta_logp - margin)`. This provides a probabilistic interpretation (Bradley-Terry) while allowing for a margin, which is a common and effective technique for robust preference learning.\n- From 'Sigmoid-Gated Adaptive Margin Loss with Rank-Gap Normalization' (Parent 0), it inherits the idea of using the rank difference between costs within a batch (`rank_gap`) to create a dynamic, batch-aware signal. This focuses the model on distinguishing between items that are far apart in the batch's cost distribution.\n\nNew Coupling Ideas:\n1.  **Rank-Gap Margin with Tanh Gating**: Instead of using the cost difference (`delta_cost`) to define the margin, this loss uses the normalized rank difference. The margin is `alpha * tanh(beta * rank_gap_norm)`. The `rank_gap` is normalized by the batch size to make it scale-invariant. The `tanh` function is then applied to create a smooth, bounded margin between `[-alpha, alpha]`. This couples the margin directly to the relative importance of a pair within the batch, preventing extreme margin values that could arise from outlier cost differences, thus improving stability.\n2.  **Z-Score Normalization of Log-Probability Difference**: The log-probability difference (`delta_logp`) is standardized using Z-score normalization across the batch before being used in the loss. This stabilizes training by preventing extreme `delta_logp` values (from either a very confident or very uncertain model) from producing exploding gradients. It recenters the model's output distribution for the batch, making the fixed `tanh`-gated margin more effective and reducing sensitivity to the overall scale of the model's logits.", "pseudocode": "1. For each pair (w, l) where cost(w) < cost(l), calculate the log probability difference `delta_logp = logp(w) - logp(l)`.\n2. Inherit from Parent 0: Determine the cost ranks for all `cost_w` and `cost_l` in the batch. Calculate the rank difference `rank_diff = rank(cost_l) - rank(cost_w)`.\n3. New Coupling (Rank-Gap Margin with Tanh Gating): Normalize the rank difference by the batch size. Apply a scaled `tanh` function to this normalized rank difference to create a smooth, bounded, and adaptive margin: `margin = alpha * tanh(beta * rank_diff / batch_size)`.\n4. New Coupling (Z-Score Normalization): Standardize the `delta_logp` vector for the entire batch using Z-score normalization to get `delta_logp_norm`. This stabilizes the loss calculation against outliers in model predictions.\n5. Inherit from Parent 1: Compute the core loss using the log-sigmoid structure, incorporating the normalized log-probability difference and the new adaptive margin: `loss = -logsigmoid(delta_logp_norm - margin)`.\n6. Return the weighted mean of the per-sample losses over the batch.", "hyperparams": {"alpha": 1.0, "beta": 5.0}, "operators_used": ["logsigmoid", "tanh", "zscore", "rank_gap"], "implementation_hint": {"expects": ["cost_a", "cost_b", "log_prob_w", "log_prob_l", "weight"], "returns": "scalar"}, "code": "import torch\nimport torch.nn.functional as F\n\ndef generated_loss(batch, model_output, extra):\n    \"\"\"\n    Computes the Adaptive Log-Sigmoid Loss with Tanh-Gated Rank Margin.\n\n    Inherits:\n    - The core `-logsigmoid(delta_logp - margin)` structure (from Parent 1).\n    - The use of rank differences for a batch-aware signal (from Parent 0).\n\n    Introduces:\n    - A new margin coupling based on `tanh(rank_gap)`, making it smooth, bounded, and derived from relative batch importance rather than absolute cost.\n    - Z-score normalization of `delta_logp` to stabilize training against extreme model outputs.\n    \"\"\"\n    # For preference learning, cost_w < cost_l is assumed.\n    cost_w = torch.min(batch['cost_a'], batch['cost_b'])\n    cost_l = torch.max(batch['cost_a'], batch['cost_b'])\n    log_prob_w = batch['log_prob_w']\n    log_prob_l = batch['log_prob_l']\n\n    # Retrieve hyperparameters\n    alpha = extra.get('alpha', 1.0) # Controls the max margin size\n    beta = extra.get('beta', 5.0)   # Controls the steepness of the tanh curve\n\n    # 1. Calculate log probability difference\n    delta_logp = log_prob_w - log_prob_l\n    batch_size = delta_logp.shape[0]\n\n    # 2. Inherited from Parent 0: Calculate rank differences\n    all_costs = torch.cat([cost_w, cost_l], dim=0)\n    sorted_costs, _ = torch.sort(all_costs)\n    rank_w = torch.searchsorted(sorted_costs, cost_w)\n    rank_l = torch.searchsorted(sorted_costs, cost_l)\n    rank_diff = (rank_l - rank_w).float()\n\n    # 3. New Coupling: Tanh-gated rank-gap margin\n    # Normalizing by 2*batch_size (total elements) makes it scale-invariant.\n    # The tanh creates a smooth, bounded margin from [-alpha, alpha].\n    # The margin is detached as it's a target, not something to be optimized through.\n    normalized_rank_diff = rank_diff / (2 * batch_size)\n    margin = alpha * torch.tanh(beta * normalized_rank_diff)\n    margin = margin.detach()\n\n    # 4. New Coupling: Z-score normalization of delta_logp for stability\n    if batch_size > 1:\n        delta_logp_mean = delta_logp.mean()\n        delta_logp_std = delta_logp.std().clamp(min=1e-8)\n        delta_logp_norm = (delta_logp - delta_logp_mean) / delta_logp_std\n    else:\n        # Cannot compute z-score for a single element, so we just center it.\n        delta_logp_norm = delta_logp - delta_logp.mean()\n\n    # 5. Inherited from Parent 1: Compute the core log-sigmoid loss\n    # The goal is to make `delta_logp_norm` > `margin`.\n    loss = -F.logsigmoid(delta_logp_norm - margin)\n\n    # 6. Apply optional weights and compute the mean.\n    weights = batch.get('weight', None)\n    if weights is not None:\n        loss = loss * weights\n        # Use clamp to prevent division by zero if all weights are zero.\n        return loss.sum() / weights.sum().clamp(min=1e-8)\n    else:\n        return loss.mean()"}, "fitness": {"hf_like_score": 40.61764907836914, "validation_objective": 40.61764907836914, "generalization_penalty": 0.0, "generalization_objectives": {"100": 40.53561592102051}, "train_score_mean": 27.057448506355286, "train_loss_mean": 0.8187287545800209, "pair_count": 129023928, "config": {"hf": {"problem": "tsp", "hf_steps": 1000, "train_problem_size": 100, "valid_problem_sizes": [100], "train_batch_size": 64, "pomo_size": 64, "learning_rate": 0.0003, "weight_decay": 1e-06, "alpha": 0.05, "device": "cuda", "seed": 1234, "num_validation_episodes": 128, "validation_batch_size": 64, "generalization_penalty_weight": 1.0, "pool_version": "v0"}, "free_loss": {"f1_steps": 1000, "f2_steps": 100, "f3_enabled": false}}, "loss_ir": {"name": "Adaptive Log-Sigmoid Loss with Tanh-Gated Rank Margin", "intuition": "This loss function synergizes the classic log-sigmoid preference structure with a dynamic, rank-aware margin, stabilized by batch normalization and a smooth gating mechanism.\n\nInherited Ideas:\n- From 'Rank-Aware Log-Sigmoid Preference Loss' (Parent 1), it inherits the core loss structure of `-logsigmoid(delta_logp - margin)`. This provides a probabilistic interpretation (Bradley-Terry) while allowing for a margin, which is a common and effective technique for robust preference learning.\n- From 'Sigmoid-Gated Adaptive Margin Loss with Rank-Gap Normalization' (Parent 0), it inherits the idea of using the rank difference between costs within a batch (`rank_gap`) to create a dynamic, batch-aware signal. This focuses the model on distinguishing between items that are far apart in the batch's cost distribution.\n\nNew Coupling Ideas:\n1.  **Rank-Gap Margin with Tanh Gating**: Instead of using the cost difference (`delta_cost`) to define the margin, this loss uses the normalized rank difference. The margin is `alpha * tanh(beta * rank_gap_norm)`. The `rank_gap` is normalized by the batch size to make it scale-invariant. The `tanh` function is then applied to create a smooth, bounded margin between `[-alpha, alpha]`. This couples the margin directly to the relative importance of a pair within the batch, preventing extreme margin values that could arise from outlier cost differences, thus improving stability.\n2.  **Z-Score Normalization of Log-Probability Difference**: The log-probability difference (`delta_logp`) is standardized using Z-score normalization across the batch before being used in the loss. This stabilizes training by preventing extreme `delta_logp` values (from either a very confident or very uncertain model) from producing exploding gradients. It recenters the model's output distribution for the batch, making the fixed `tanh`-gated margin more effective and reducing sensitivity to the overall scale of the model's logits.", "hyperparams": {"alpha": 1.0, "beta": 5.0}, "operators_used": ["logsigmoid", "tanh", "zscore", "rank_gap"]}}, "better_than_baseline": false}
{"generation": 9, "index": 5, "ir": {"name": "Rank-Gated Log-Sigmoid Loss with Tanh Margin", "intuition": "This loss function combines the classic log-sigmoid structure with a dynamic, rank-aware margin, creating a preference signal that is both stable and sensitive to the relative importance of pairs within a batch.\n\nInherited Ideas:\n- From 'Rank-Aware Log-Sigmoid Preference Loss' (Parent 1), it inherits the core loss structure of `-logsigmoid(delta_logp - margin)`. This provides a strong probabilistic foundation, encouraging the model to maximize the log-likelihood of the preferred choice.\n- From 'Sigmoid-Gated Adaptive Margin Loss with Rank-Gap Normalization' (Parent 0), it inherits the use of a sigmoid gate on the log-probability difference, `delta_logp * sigmoid(delta_logp)`. This serves as a stability mechanism, down-weighting the loss contribution from pairs where the model is already very confident and correct, allowing it to focus on more difficult or incorrectly classified examples.\n\nNew Coupling Ideas:\n1.  **Tanh-Scaled Rank Gap Margin**: A new adaptive margin is created by coupling the rank difference with a `tanh` function: `margin = alpha * tanh(beta * rank_gap_normalized)`. The rank gap is first normalized by the batch size to ensure scale invariance. The `tanh` function then smoothly maps this normalized rank gap to a bounded margin between 0 and `alpha`. This ensures that pairs with a larger rank difference (more important to get right) have a larger margin, but it prevents the margin from growing uncontrollably, which enhances numerical stability.\n2.  **Adaptive Loss Weighting with Z-Scored Cost**: The final per-sample loss is weighted by an adaptive factor derived from the cost difference. The `delta_cost` is first standardized using a Z-score across the batch. This normalized score is then passed through a `softplus` function, `softplus(delta_cost_zscore)`, creating a non-negative, unbounded weight. This couples the magnitude of the loss to the magnitude of the cost difference in a scale-invariant manner, ensuring that pairs with larger real-world cost differences contribute more significantly to the training signal.", "pseudocode": "1. For each pair (w, l) where cost(w) < cost(l), calculate the cost difference `delta_cost = cost(l) - cost(w)` and the log probability difference `delta_logp = logp(w) - logp(l)`.\n2. Inherit from Parent 0: Apply a sigmoid gate to the log-probability difference: `gated_delta_logp = delta_logp * sigmoid(delta_logp)`.\n3. New Coupling (Tanh Margin): Determine the cost ranks for all items in the batch. Calculate the normalized rank difference `rank_gap_norm = (rank(l) - rank(w)) / (2 * batch_size)`. Create a bounded, adaptive margin using `margin = alpha * tanh(beta * rank_gap_norm)`.\n4. New Coupling (Adaptive Weighting): Standardize the `delta_cost` vector for the batch using Z-score to get `delta_cost_zscore`. Create an adaptive weight using `weight_adaptive = softplus(delta_cost_zscore)`.\n5. Inherit from Parent 1: Compute the core loss using a log-sigmoid structure, incorporating the new margin and gated log-probabilities: `base_loss = -logsigmoid(gated_delta_logp - margin)`.\n6. Combine the base loss with the adaptive weight: `per_sample_loss = weight_adaptive * base_loss`.\n7. Return the mean of the per-sample losses over the batch, applying optional external weights if provided.", "hyperparams": {"alpha": 2.0, "beta": 5.0}, "operators_used": ["logsigmoid", "sigmoid", "tanh", "softplus", "zscore", "rank_gap"], "implementation_hint": {"expects": ["cost_w", "cost_l", "logp_w", "logp_l", "weight"], "returns": "scalar"}, "code": "import torch\nimport torch.nn.functional as F\n\ndef generated_loss(batch, model_output, extra):\n    \"\"\"\n    Computes the Rank-Gated Log-Sigmoid Loss with Tanh Margin.\n\n    Inherits:\n    - The core `-logsigmoid(delta_logp - margin)` structure (from Parent 1).\n    - A sigmoid gate on `delta_logp` for stability and focus (from Parent 0).\n\n    Introduces:\n    - A new tanh-based adaptive margin from the normalized rank gap.\n    - An adaptive loss weight based on the z-scored cost difference.\n    \"\"\"\n    # For preference learning, we assume cost_w < cost_l.\n    cost_w = torch.min(batch['cost_a'], batch['cost_b'])\n    cost_l = torch.max(batch['cost_a'], batch['cost_b'])\n    log_prob_w = batch['log_prob_w']\n    log_prob_l = batch['log_prob_l']\n\n    # Retrieve hyperparameters\n    alpha = extra.get('alpha', 2.0) # Controls the maximum margin\n    beta = extra.get('beta', 5.0)   # Controls the steepness of the tanh margin\n\n    # 1. Calculate base differences\n    delta_cost = cost_l - cost_w\n    delta_logp = log_prob_w - log_prob_l\n    batch_size = delta_cost.shape[0]\n\n    # 2. Inherited from Parent 0: Sigmoid gating on delta_logp\n    # This down-weights pairs where the model is already very confident and correct.\n    gated_delta_logp = delta_logp * torch.sigmoid(delta_logp)\n\n    # 3. New Coupling: Tanh-based adaptive margin from rank gap\n    # Combine all costs to establish a batch-wide ranking.\n    all_costs = torch.cat([cost_w, cost_l], dim=0)\n    sorted_costs, _ = torch.sort(all_costs)\n    \n    # Find ranks efficiently using searchsorted.\n    rank_w = torch.searchsorted(sorted_costs, cost_w)\n    rank_l = torch.searchsorted(sorted_costs, cost_l)\n    rank_diff = (rank_l - rank_w).float()\n    \n    # Normalize rank diff by total number of items to make it batch-size invariant.\n    rank_gap_norm = rank_diff / (2.0 * batch_size)\n    \n    # Tanh creates a smooth, bounded margin from the rank gap.\n    margin = alpha * torch.tanh(beta * rank_gap_norm)\n\n    # 4. New Coupling: Adaptive loss weighting from z-scored cost difference\n    if torch.all(delta_cost < 1e-8):\n        # If all costs are the same, z-score is undefined and weight is neutral.\n        delta_cost_zscore = torch.zeros_like(delta_cost)\n    else:\n        delta_cost_mean = delta_cost.mean()\n        delta_cost_std = delta_cost.std().clamp(min=1e-8)\n        delta_cost_zscore = (delta_cost - delta_cost_mean) / delta_cost_std\n    \n    # Softplus creates a non-negative, unbounded weight.\n    adaptive_weight = F.softplus(delta_cost_zscore)\n\n    # 5. Inherited from Parent 1: Core log-sigmoid loss structure\n    # We want to maximize `gated_delta_logp - margin`, so we minimize its negative log-sigmoid.\n    base_loss = -F.logsigmoid(gated_delta_logp - margin.detach())\n\n    # 6. Modulate the base loss with the adaptive weight\n    per_sample_loss = adaptive_weight.detach() * base_loss\n\n    # 7. Apply optional external weights and compute the mean.\n    weights = batch.get('weight', None)\n    if weights is not None:\n        per_sample_loss = per_sample_loss * weights\n        return per_sample_loss.sum() / weights.sum().clamp(min=1e-8)\n    else:\n        return per_sample_loss.mean()"}, "fitness": {"hf_like_score": 8.140677452087402, "validation_objective": 8.140677452087402, "generalization_penalty": 0.0, "generalization_objectives": {"100": 8.12071704864502}, "train_score_mean": 9.334088541030884, "train_loss_mean": 0.09636632823944091, "pair_count": 129023888, "config": {"hf": {"problem": "tsp", "hf_steps": 1000, "train_problem_size": 100, "valid_problem_sizes": [100], "train_batch_size": 64, "pomo_size": 64, "learning_rate": 0.0003, "weight_decay": 1e-06, "alpha": 0.05, "device": "cuda", "seed": 1234, "num_validation_episodes": 128, "validation_batch_size": 64, "generalization_penalty_weight": 1.0, "pool_version": "v0"}, "free_loss": {"f1_steps": 1000, "f2_steps": 100, "f3_enabled": false}}, "loss_ir": {"name": "Rank-Gated Log-Sigmoid Loss with Tanh Margin", "intuition": "This loss function combines the classic log-sigmoid structure with a dynamic, rank-aware margin, creating a preference signal that is both stable and sensitive to the relative importance of pairs within a batch.\n\nInherited Ideas:\n- From 'Rank-Aware Log-Sigmoid Preference Loss' (Parent 1), it inherits the core loss structure of `-logsigmoid(delta_logp - margin)`. This provides a strong probabilistic foundation, encouraging the model to maximize the log-likelihood of the preferred choice.\n- From 'Sigmoid-Gated Adaptive Margin Loss with Rank-Gap Normalization' (Parent 0), it inherits the use of a sigmoid gate on the log-probability difference, `delta_logp * sigmoid(delta_logp)`. This serves as a stability mechanism, down-weighting the loss contribution from pairs where the model is already very confident and correct, allowing it to focus on more difficult or incorrectly classified examples.\n\nNew Coupling Ideas:\n1.  **Tanh-Scaled Rank Gap Margin**: A new adaptive margin is created by coupling the rank difference with a `tanh` function: `margin = alpha * tanh(beta * rank_gap_normalized)`. The rank gap is first normalized by the batch size to ensure scale invariance. The `tanh` function then smoothly maps this normalized rank gap to a bounded margin between 0 and `alpha`. This ensures that pairs with a larger rank difference (more important to get right) have a larger margin, but it prevents the margin from growing uncontrollably, which enhances numerical stability.\n2.  **Adaptive Loss Weighting with Z-Scored Cost**: The final per-sample loss is weighted by an adaptive factor derived from the cost difference. The `delta_cost` is first standardized using a Z-score across the batch. This normalized score is then passed through a `softplus` function, `softplus(delta_cost_zscore)`, creating a non-negative, unbounded weight. This couples the magnitude of the loss to the magnitude of the cost difference in a scale-invariant manner, ensuring that pairs with larger real-world cost differences contribute more significantly to the training signal.", "hyperparams": {"alpha": 2.0, "beta": 5.0}, "operators_used": ["logsigmoid", "sigmoid", "tanh", "softplus", "zscore", "rank_gap"]}}, "better_than_baseline": false}
{"generation": 9, "index": 6, "ir": {"name": "Rank-Normalized Bradley-Terry Loss", "intuition": "This loss function reframes the classic Bradley-Terry model (log-sigmoid loss) by introducing an adaptive, rank-aware margin. The goal is to create a loss that is both statistically grounded and sensitive to the relative importance of pairs within a batch.\n\nInherited Ideas:\n- From 'Rank-Aware Log-Sigmoid Preference Loss' (Parent 1), it inherits the core `logsigmoid` structure, which is a standard and well-behaved loss for pairwise preferences. This provides a probabilistic interpretation of the model's output.\n- From 'Sigmoid-Gated Adaptive Margin Loss with Rank-Gap Normalization' (Parent 0), it inherits the idea of using `softplus(delta_cost)` to create a smooth, non-negative, and non-saturating adaptive margin. This ensures that pairs with larger cost differences are expected to have a larger log-probability difference.\n\nNew Coupling Ideas:\n1. **Rank-Normalized Margin**: Instead of just using the rank gap to weight the final loss, it is used to directly normalize the adaptive margin itself. The margin `softplus(delta_cost)` is divided by `log(1 + rank_gap)`. This couples the margin's scale directly to the pair's relative importance. For pairs with a small rank gap, the denominator is small, preserving a larger margin and pushing the model to learn fine-grained distinctions. For pairs with a large rank gap, the denominator grows, reducing the margin. This prevents the margin from becoming excessively large for pairs that are already easy to distinguish based on their large cost difference, which helps stabilize gradients.\n2. **Log-Sigmoid with Adaptive Margin**: The core loss structure is `logsigmoid(delta_logp - margin)`. This is a novel coupling that directly incorporates the rank-normalized adaptive margin into the log-sigmoid function. This is different from Parent 1, which used cost/rank as an external weight, and Parent 0, which used a hinge-like `softplus(margin - delta_logp)` structure. This formulation maintains the probabilistic interpretation of the log-sigmoid loss while making the target log-probability difference dependent on both the absolute cost difference and the relative rank difference.", "pseudocode": "1. For each pair (w, l) where cost(w) < cost(l), calculate the cost difference `delta_cost = cost(l) - cost(w)` and the log probability difference `delta_logp = logp(w) - logp(l)`.\n2. Inherit from Parent 0: Calculate a smooth, non-saturating base margin using `margin_base = softplus(delta_cost / temp_cost)`.\n3. Determine the cost ranks for all `cost_w` and `cost_l` in the batch. Calculate the rank difference `rank_gap = rank(cost_l) - rank(cost_w)`.\n4. New Coupling (Rank-Normalized Margin): Create a rank-based normalizer using the natural logarithm: `rank_normalizer = log(1.0 + beta * rank_gap)`. Add a small epsilon for stability. The final adaptive margin is `margin = margin_base / rank_normalizer`.\n5. New Coupling (Log-Sigmoid with Adaptive Margin): Compute the core per-sample loss by incorporating the adaptive margin directly into the log-sigmoid function, which is inherited from Parent 1. The loss is `-logsigmoid(delta_logp - margin)`.\n6. Return the weighted mean of the per-sample losses over the batch.", "hyperparams": {"temp_cost": 1.0, "beta": 0.1}, "operators_used": ["logsigmoid", "softplus", "log", "rank_gap"], "implementation_hint": {"expects": ["cost_a", "cost_b", "log_prob_w", "log_prob_l", "weight"], "returns": "scalar"}, "code": "import torch\nimport torch.nn.functional as F\n\ndef generated_loss(batch, model_output, extra):\n    \"\"\"\n    Computes the Rank-Normalized Bradley-Terry Loss.\n\n    Inherits:\n    - The core `logsigmoid` structure for pairwise preference (from Parent 1).\n    - The use of `softplus(delta_cost)` for a smooth, adaptive margin (from Parent 0).\n\n    Introduces:\n    - A novel coupling where the rank gap normalizes the margin via `log(1 + beta * rank_gap)`.\n    - A direct integration of this adaptive margin into the log-sigmoid function: `-logsigmoid(delta_logp - margin)`.\n    \"\"\"\n    # For preference learning, cost_w < cost_l.\n    cost_w = torch.min(batch['cost_a'], batch['cost_b'])\n    cost_l = torch.max(batch['cost_a'], batch['cost_b'])\n    log_prob_w = batch['log_prob_w']\n    log_prob_l = batch['log_prob_l']\n\n    # Retrieve hyperparameters\n    temp_cost = extra.get('temp_cost', 1.0)\n    beta = extra.get('beta', 0.1)\n\n    # 1. Calculate cost and log probability differences\n    delta_cost = cost_l - cost_w\n    delta_logp = log_prob_w - log_prob_l\n    batch_size = delta_cost.shape[0]\n\n    # 2. Inherited from Parent 0: Calculate a smooth, non-saturating base margin.\n    # Using a temperature parameter to control the margin's sensitivity to cost differences.\n    margin_base = F.softplus(delta_cost / temp_cost)\n\n    # 3. Calculate rank differences (rank_gap).\n    # Combine all costs to establish a batch-wide ranking.\n    all_costs = torch.cat([cost_w, cost_l], dim=0)\n    sorted_costs, _ = torch.sort(all_costs)\n    \n    # Find ranks efficiently using searchsorted.\n    rank_w = torch.searchsorted(sorted_costs, cost_w)\n    rank_l = torch.searchsorted(sorted_costs, cost_l)\n    rank_gap = (rank_l - rank_w).float()\n\n    # 4. New Coupling: Rank-Normalized Margin.\n    # The log function provides a smooth, non-linear normalization.\n    # For small rank_gap, log(1+x) ~ x, providing gentle scaling.\n    # For large rank_gap, the log growth slows down, preventing over-suppression of the margin.\n    # The 1.0 ensures the argument to log is always >= 1.\n    # A small epsilon prevents division by zero if rank_gap is 0.\n    rank_normalizer = torch.log(1.0 + beta * rank_gap).clamp(min=1e-8)\n    adaptive_margin = (margin_base / rank_normalizer).detach() # Detach to treat margin as a target\n\n    # 5. New Coupling: Log-Sigmoid with Adaptive Margin.\n    # This encourages `delta_logp` to be greater than the `adaptive_margin`.\n    # The structure is `log(1 + exp(-(delta_logp - margin)))`, which is minimized as `delta_logp` increases.\n    loss = -F.logsigmoid(delta_logp - adaptive_margin)\n\n    # 6. Apply optional weights and compute the mean.\n    weights = batch.get('weight', None)\n    if weights is not None:\n        loss = loss * weights\n        # Use clamp to avoid division by zero if all weights are zero.\n        return loss.sum() / weights.sum().clamp(min=1e-8)\n    else:\n        return loss.mean()"}, "fitness": {"hf_like_score": 33.24443435668945, "validation_objective": 33.24443435668945, "generalization_penalty": 0.0, "generalization_objectives": {"100": 32.72842597961426}, "train_score_mean": 39.882491435050966, "train_loss_mean": 1.2867837154865265, "pair_count": 129023912, "config": {"hf": {"problem": "tsp", "hf_steps": 1000, "train_problem_size": 100, "valid_problem_sizes": [100], "train_batch_size": 64, "pomo_size": 64, "learning_rate": 0.0003, "weight_decay": 1e-06, "alpha": 0.05, "device": "cuda", "seed": 1234, "num_validation_episodes": 128, "validation_batch_size": 64, "generalization_penalty_weight": 1.0, "pool_version": "v0"}, "free_loss": {"f1_steps": 1000, "f2_steps": 100, "f3_enabled": false}}, "loss_ir": {"name": "Rank-Normalized Bradley-Terry Loss", "intuition": "This loss function reframes the classic Bradley-Terry model (log-sigmoid loss) by introducing an adaptive, rank-aware margin. The goal is to create a loss that is both statistically grounded and sensitive to the relative importance of pairs within a batch.\n\nInherited Ideas:\n- From 'Rank-Aware Log-Sigmoid Preference Loss' (Parent 1), it inherits the core `logsigmoid` structure, which is a standard and well-behaved loss for pairwise preferences. This provides a probabilistic interpretation of the model's output.\n- From 'Sigmoid-Gated Adaptive Margin Loss with Rank-Gap Normalization' (Parent 0), it inherits the idea of using `softplus(delta_cost)` to create a smooth, non-negative, and non-saturating adaptive margin. This ensures that pairs with larger cost differences are expected to have a larger log-probability difference.\n\nNew Coupling Ideas:\n1. **Rank-Normalized Margin**: Instead of just using the rank gap to weight the final loss, it is used to directly normalize the adaptive margin itself. The margin `softplus(delta_cost)` is divided by `log(1 + rank_gap)`. This couples the margin's scale directly to the pair's relative importance. For pairs with a small rank gap, the denominator is small, preserving a larger margin and pushing the model to learn fine-grained distinctions. For pairs with a large rank gap, the denominator grows, reducing the margin. This prevents the margin from becoming excessively large for pairs that are already easy to distinguish based on their large cost difference, which helps stabilize gradients.\n2. **Log-Sigmoid with Adaptive Margin**: The core loss structure is `logsigmoid(delta_logp - margin)`. This is a novel coupling that directly incorporates the rank-normalized adaptive margin into the log-sigmoid function. This is different from Parent 1, which used cost/rank as an external weight, and Parent 0, which used a hinge-like `softplus(margin - delta_logp)` structure. This formulation maintains the probabilistic interpretation of the log-sigmoid loss while making the target log-probability difference dependent on both the absolute cost difference and the relative rank difference.", "hyperparams": {"temp_cost": 1.0, "beta": 0.1}, "operators_used": ["logsigmoid", "softplus", "log", "rank_gap"]}}, "better_than_baseline": false}
{"generation": 9, "index": 7, "ir": {"name": "Adaptive Log-Sigmoid Loss with Tanh-Gated Rank Margin", "intuition": "This loss function creates a stable, rank-aware preference signal by combining a classic log-sigmoid structure with a novel, bounded margin that is dynamically modulated by both cost differences and batch-wise rank information.\n\nInherited Ideas:\n- From 'Rank-Aware Log-Sigmoid Preference Loss' (Parent 1), it inherits the core loss structure of `-logsigmoid(delta_logp - margin)`. This is a robust, probabilistic formulation that encourages the log-probability difference to exceed a certain margin.\n- From 'Sigmoid-Gated Adaptive Margin Loss with Rank-Gap Normalization' (Parent 0), it inherits the use of a `softplus`-transformed cost difference (`softplus(delta_cost)`) as a key component of the margin. This ensures the margin is always non-negative and grows smoothly as the cost gap between a pair increases.\n\nNew Coupling Ideas:\n1.  **Tanh-Gated Rank Margin**: The margin is constructed by coupling the cost-based signal with a rank-based signal through a `tanh` gate. The base margin is `alpha * softplus(delta_cost / temp_cost)`. This base margin is then multiplied by `tanh(beta * rank_gap)`. This gating mechanism has a desirable property: for pairs with a small rank difference, the margin is small (as `tanh(x)  x` for small x), forcing the model to learn fine-grained distinctions. For pairs with a large rank difference, the `tanh` function saturates at 1, preventing the rank gap from creating an excessively large, numerically unstable margin. This creates a bounded, rank-aware margin that focuses learning without risking gradient explosion.\n2.  **Sigmoid-Weighted Loss**: The final per-sample loss is multiplied by `sigmoid(delta_cost)`. This acts as a confidence weight. For pairs with a very small cost difference (close to zero), the sigmoid weight is close to 0.5, giving them moderate importance. For pairs with a large cost difference, the weight approaches 1.0, emphasizing these 'obvious' pairs and ensuring the model learns the broad preference landscape correctly. This smoothly de-emphasizes pairs with negligible cost differences, which might otherwise introduce noise into the training process.", "pseudocode": "1. For each pair (w, l) where cost(w) < cost(l), calculate the cost difference `delta_cost = cost(l) - cost(w)` and the log probability difference `delta_logp = logp(w) - logp(l)`.\n2. Inherit from Parent 0: Calculate a smooth, non-negative base margin signal using `softplus`: `margin_base = alpha * softplus(delta_cost / temp_cost)`.\n3. Determine the cost ranks for all `cost_w` and `cost_l` in the batch. Calculate the normalized rank difference `rank_gap = (rank(cost_l) - rank(cost_w)) / (2 * batch_size)`.\n4. New Coupling (Tanh-Gated Rank Margin): Create a bounded rank-based gate using `tanh(beta * rank_gap)`. Combine this with the base margin to form the final adaptive margin: `adaptive_margin = margin_base * tanh(beta * rank_gap)`.\n5. Inherit from Parent 1: Compute the core preference loss using a log-sigmoid structure with the adaptive margin: `base_loss = -logsigmoid(delta_logp - adaptive_margin)`.\n6. New Coupling (Sigmoid-Weighted Loss): Calculate a confidence weight for each pair based on the cost difference: `confidence_weight = sigmoid(delta_cost)`. This weight de-emphasizes pairs with very small cost differences.\n7. Modulate the base loss with the confidence weight: `final_loss = confidence_weight * base_loss`.\n8. Return the weighted mean of the per-sample losses over the batch.", "hyperparams": {"alpha": 1.0, "temp_cost": 1.0, "beta": 5.0}, "operators_used": ["logsigmoid", "softplus", "tanh", "sigmoid", "rank_gap"], "implementation_hint": {"expects": ["cost_a", "cost_b", "log_prob_w", "log_prob_l"], "returns": "scalar"}, "code": "import torch\nimport torch.nn.functional as F\n\ndef generated_loss(batch, model_output, extra):\n    \"\"\"\n    Computes the Adaptive Log-Sigmoid Loss with Tanh-Gated Rank Margin.\n\n    Inherits:\n    - The core `-logsigmoid(delta_logp - margin)` structure (from Parent 1).\n    - The use of `softplus(delta_cost)` to create a smooth, non-negative margin component (from Parent 0).\n\n    Introduces:\n    - A Tanh-gated rank margin, `softplus(delta_cost) * tanh(rank_gap)`, which creates a bounded but responsive margin.\n    - A sigmoid weight, `sigmoid(delta_cost)`, applied to the final loss to de-emphasize pairs with negligible cost differences.\n    \"\"\"\n    # For preference learning, we assume cost_w < cost_l.\n    cost_w = torch.min(batch['cost_a'], batch['cost_b'])\n    cost_l = torch.max(batch['cost_a'], batch['cost_b'])\n    log_prob_w = batch['log_prob_w']\n    log_prob_l = batch['log_prob_l']\n\n    # Retrieve hyperparameters\n    alpha = extra.get('alpha', 1.0)\n    temp_cost = extra.get('temp_cost', 1.0)\n    beta = extra.get('beta', 5.0)\n\n    # 1. Calculate cost and log probability differences\n    delta_cost = cost_l - cost_w\n    delta_logp = log_prob_w - log_prob_l\n    batch_size = delta_cost.shape[0]\n\n    # 2. Inherited from Parent 0: Calculate a smooth, non-negative base margin signal.\n    margin_base = alpha * F.softplus(delta_cost / temp_cost)\n\n    # 3. Calculate rank differences.\n    all_costs = torch.cat([cost_w, cost_l], dim=0)\n    # Use torch.sort for compatibility and clarity\n    sorted_costs, _ = torch.sort(all_costs)\n    rank_w = torch.searchsorted(sorted_costs, cost_w)\n    rank_l = torch.searchsorted(sorted_costs, cost_l)\n    rank_diff = (rank_l - rank_w).float()\n\n    # 4. New Coupling: Tanh-Gated Rank Margin.\n    # tanh provides a smooth gate that saturates, preventing the margin from exploding for large rank gaps.\n    # The rank difference is used directly, as tanh naturally bounds its influence.\n    rank_gate = torch.tanh(beta * rank_diff)\n    adaptive_margin = margin_base * rank_gate\n\n    # 5. Inherited from Parent 1: Compute the core loss using the log-sigmoid structure.\n    # The loss encourages delta_logp > adaptive_margin.\n    # Detach margin to treat it as a target, not part of the model's computation graph wrt cost prediction.\n    base_loss = -F.logsigmoid(delta_logp - adaptive_margin.detach())\n\n    # 6. New Coupling: Sigmoid-Weighted Loss.\n    # This weight smoothly de-emphasizes pairs with near-zero cost differences,\n    # which can be noisy, while emphasizing clear preferences.\n    confidence_weight = torch.sigmoid(delta_cost).detach()\n\n    # 7. Modulate the base loss.\n    loss = confidence_weight * base_loss\n\n    # 8. Apply optional weights and compute the mean.\n    weights = batch.get('weight', None)\n    if weights is not None:\n        loss = loss * weights\n        # Use clamp to prevent division by zero for an all-zero weight tensor\n        return loss.sum() / weights.sum().clamp(min=1e-8)\n    else:\n        return loss.mean()"}, "fitness": {"hf_like_score": 38.086212158203125, "validation_objective": 38.086212158203125, "generalization_penalty": 0.0, "generalization_objectives": {"100": 37.70858573913574}, "train_score_mean": 39.69097891139984, "train_loss_mean": 1.063348751783371, "pair_count": 129023920, "config": {"hf": {"problem": "tsp", "hf_steps": 1000, "train_problem_size": 100, "valid_problem_sizes": [100], "train_batch_size": 64, "pomo_size": 64, "learning_rate": 0.0003, "weight_decay": 1e-06, "alpha": 0.05, "device": "cuda", "seed": 1234, "num_validation_episodes": 128, "validation_batch_size": 64, "generalization_penalty_weight": 1.0, "pool_version": "v0"}, "free_loss": {"f1_steps": 1000, "f2_steps": 100, "f3_enabled": false}}, "loss_ir": {"name": "Adaptive Log-Sigmoid Loss with Tanh-Gated Rank Margin", "intuition": "This loss function creates a stable, rank-aware preference signal by combining a classic log-sigmoid structure with a novel, bounded margin that is dynamically modulated by both cost differences and batch-wise rank information.\n\nInherited Ideas:\n- From 'Rank-Aware Log-Sigmoid Preference Loss' (Parent 1), it inherits the core loss structure of `-logsigmoid(delta_logp - margin)`. This is a robust, probabilistic formulation that encourages the log-probability difference to exceed a certain margin.\n- From 'Sigmoid-Gated Adaptive Margin Loss with Rank-Gap Normalization' (Parent 0), it inherits the use of a `softplus`-transformed cost difference (`softplus(delta_cost)`) as a key component of the margin. This ensures the margin is always non-negative and grows smoothly as the cost gap between a pair increases.\n\nNew Coupling Ideas:\n1.  **Tanh-Gated Rank Margin**: The margin is constructed by coupling the cost-based signal with a rank-based signal through a `tanh` gate. The base margin is `alpha * softplus(delta_cost / temp_cost)`. This base margin is then multiplied by `tanh(beta * rank_gap)`. This gating mechanism has a desirable property: for pairs with a small rank difference, the margin is small (as `tanh(x)  x` for small x), forcing the model to learn fine-grained distinctions. For pairs with a large rank difference, the `tanh` function saturates at 1, preventing the rank gap from creating an excessively large, numerically unstable margin. This creates a bounded, rank-aware margin that focuses learning without risking gradient explosion.\n2.  **Sigmoid-Weighted Loss**: The final per-sample loss is multiplied by `sigmoid(delta_cost)`. This acts as a confidence weight. For pairs with a very small cost difference (close to zero), the sigmoid weight is close to 0.5, giving them moderate importance. For pairs with a large cost difference, the weight approaches 1.0, emphasizing these 'obvious' pairs and ensuring the model learns the broad preference landscape correctly. This smoothly de-emphasizes pairs with negligible cost differences, which might otherwise introduce noise into the training process.", "hyperparams": {"alpha": 1.0, "temp_cost": 1.0, "beta": 5.0}, "operators_used": ["logsigmoid", "softplus", "tanh", "sigmoid", "rank_gap"]}}, "better_than_baseline": false}
