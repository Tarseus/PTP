{"generation": 0, "index": 0, "ir": {"name": "Adaptive Margin Hinge Loss with Rank-Gap Scaling", "intuition": "This loss function uses a hinge-like structure, penalizing the model only when its preference (`logp_w - log_p_l`) is smaller than a desired margin. The key idea is that the margin is not fixed but adapts to the difficulty of the comparison, as measured by the rank-normalized cost difference (`rank_gap`). For pairs with a large, obvious cost difference, the margin is large, demanding a confident prediction from the model. For pairs that are very close in cost, the margin is small, tolerating more uncertainty. The `tanh` function is used to smoothly scale the log-probability difference, preventing extreme values from dominating the gradient and ensuring numerical stability.", "pseudocode": "1. Calculate the cost difference: delta_cost = cost_l - cost_w.\n2. Calculate the log-probability difference: delta_logp = logp_w - logp_l.\n3. Normalize the cost differences across the batch to get a rank-based gap between 0 and 1: cost_rank_gap = rank_gap(delta_cost).\n4. Compute an adaptive margin by scaling the rank gap: margin = alpha * cost_rank_gap.\n5. Scale the log-probability difference using tanh for stability: scaled_delta_logp = beta * tanh(delta_logp / beta).\n6. Calculate the hinge loss for each pair: loss_per_item = relu(margin - scaled_delta_logp).\n7. Apply optional sample weights if provided.\n8. Return the mean loss over the batch.", "hyperparams": {"alpha": 1.0, "beta": 5.0}, "operators_used": ["rank_gap", "tanh", "relu"], "implementation_hint": {"expects": ["cost_w", "cost_l", "logp_w", "logp_l"], "returns": "scalar"}, "code": "import torch\nimport torch.nn.functional as F\n\ndef rank_gap(x):\n    \"\"\"Numerically stable rank normalization to a [0, 1] gap.\"\"\"\n    if x.numel() <= 1:\n        return torch.zeros_like(x)\n    ranks = x.argsort().argsort().float()\n    # Avoid division by zero if all values are the same\n    denominator = x.numel() - 1.0\n    if denominator < 1.0:\n        denominator = 1.0\n    return ranks / denominator\n\ndef generated_loss(batch, model_output, extra):\n    \"\"\"\n    Adaptive Margin Hinge Loss with Rank-Gap Scaling.\n\n    Args:\n        batch (dict): Dictionary containing tensors from the dataloader.\n                      Expected keys: 'cost_w', 'cost_l', 'weight' (optional).\n                      'cost_w' is the cost of the preferred solution (winner).\n                      'cost_l' is the cost of the dispreferred solution (loser).\n        model_output (dict): Dictionary containing model outputs.\n                             Expected keys: 'log_prob_w', 'log_prob_l'.\n        extra (dict): Dictionary for hyperparameters, e.g., {'alpha': 1.0, 'beta': 5.0}.\n\n    Returns:\n        torch.Tensor: A scalar loss value.\n    \"\"\"\n    cost_w = batch['cost_w']\n    cost_l = batch['cost_l']\n    logp_w = model_output['log_prob_w']\n    logp_l = model_output['log_prob_l']\n    \n    # Hyperparameters from the 'extra' dictionary\n    alpha = extra.get('alpha', 1.0)\n    beta = extra.get('beta', 5.0)\n\n    # 1. Calculate cost and log-probability differences\n    # cost_w < cost_l, so delta_cost is always non-negative\n    delta_cost = cost_l - cost_w\n    delta_logp = logp_w - logp_l\n\n    # 2. Normalize the cost differences using a rank-based gap\n    # This makes the margin robust to the scale of costs\n    # rank_gap maps the delta_cost values to a [0, 1] range based on their rank\n    cost_rank_gap = rank_gap(delta_cost)\n\n    # 3. Compute an adaptive margin based on the normalized cost gap\n    # For easy pairs (large cost_rank_gap), the margin is large.\n    # For hard pairs (small cost_rank_gap), the margin is small.\n    margin = alpha * cost_rank_gap\n\n    # 4. Scale the log-probability difference for stability\n    # tanh squashes the difference into a [-beta, beta] range, preventing extreme gradients\n    # and making the loss less sensitive to outliers.\n    scaled_delta_logp = beta * torch.tanh(delta_logp / beta)\n\n    # 5. Calculate the hinge loss\n    # We want scaled_delta_logp to be greater than the margin.\n    # Loss is incurred only if scaled_delta_logp < margin.\n    loss_per_item = F.relu(margin - scaled_delta_logp)\n\n    # 6. Apply optional sample weights\n    weights = batch.get('weight', None)\n    if weights is not None:\n        loss_per_item = loss_per_item * weights\n\n    # 7. Return the mean loss over the batch\n    return loss_per_item.mean()"}, "static_ok": true, "static_reason": "", "dynamic_ok": false, "dynamic_reason": "forward_error: 'cost_w'", "loss_value": null, "grad_norm": null}
{"generation": 0, "index": 1, "ir": {"name": "Adaptive Margin Sigmoid Loss with Cost Rank Gating", "intuition": "This loss uses a sigmoid function on the log-probability difference, similar to a logistic loss, but with an adaptive margin determined by the normalized cost difference. The key idea is to use the rank-based gap of the cost difference within the batch (normalized to [0, 1]) to modulate the margin. For pairs with a very large cost gap (relative to others in the batch), the margin is large, demanding a strong preference from the model. For pairs with a small cost gap, the margin is small, allowing the model more flexibility. This prevents overfitting on noisy or near-indistinguishable pairs. The entire loss is gated by the sign of the cost difference, ensuring that the learning signal is only applied when the model's preference contradicts the ground-truth cost ordering. This makes it a one-sided, hinge-like loss.", "pseudocode": "1. For a batch of pairs (a, b), calculate the cost difference `delta_cost = cost_a - cost_b` and log-probability difference `delta_logp = logp_a - logp_b`.\n2. Normalize the absolute cost differences across the batch to get a rank-based gap `cost_rank_gap` in the range [0, 1]. This measures the relative significance of each pair's cost difference.\n3. Define an adaptive margin as `margin = beta * cost_rank_gap`, where `beta` is a hyperparameter scaling the margin's influence.\n4. Calculate the core loss term using a sigmoid function: `core_loss = sigmoid(margin - delta_logp)`. This term is high when the model's preference `delta_logp` is smaller than the required margin.\n5. Create a binary gate `is_wrong_preference` which is 1 if `cost_a < cost_b` and `logp_a < logp_b`, and 0 otherwise. This ensures loss is only applied for incorrectly ordered pairs.\n6. The final loss for a pair is `is_wrong_preference * core_loss`.\n7. Return the (optionally weighted) mean of this loss over the batch.", "hyperparams": {"beta": {"type": "float", "default": 1.0, "description": "Scales the adaptive margin derived from the cost rank gap. Higher beta means larger margins and stronger preference enforcement for pairs with significant cost differences."}}, "operators_used": ["sigmoid", "rank_gap"], "implementation_hint": {"expects": ["cost_a", "cost_b", "logp_a", "logp_b"], "returns": "scalar"}, "code": "import torch\nimport torch.nn.functional as F\n\ndef rank_gap(x):\n    \"\"\"Normalizes a tensor by its rank to the range [0, 1].\"\"\"\n    if x.numel() <= 1:\n        return torch.ones_like(x)\n    ranks = x.argsort().argsort().float()\n    return ranks / (x.numel() - 1)\n\ndef generated_loss(batch, model_output, extra):\n    \"\"\"\n    Calculates the Adaptive Margin Sigmoid Loss with Cost Rank Gating.\n    It uses a sigmoid loss where the margin is dynamically set by the\n    rank-normalized cost difference within the batch.\n    \"\"\"\n    # Read hyperparameters\n    hyperparams = extra.get('hyperparams', {})\n    beta = hyperparams.get('beta', 1.0)\n\n    # Unpack data from batch dictionary\n    # Assuming cost_a < cost_b is the preferred outcome\n    cost_a = batch['cost_a']\n    cost_b = batch['cost_b']\n    logp_a = batch['log_prob_w'] # log prob of winner (a)\n    logp_b = batch['log_prob_l'] # log prob of loser (b)\n    weight = batch.get('weight', None)\n\n    # Assert ground truth preference: cost_a should be less than cost_b\n    # This loss is designed for a fixed preference direction (w, l)\n    # If not, the logic needs to be adapted, but we follow the prompt's implied setup.\n    # assert torch.all(cost_a <= cost_b)\n\n    # 1. Calculate differences\n    delta_cost = cost_b - cost_a  # Should be non-negative\n    delta_logp = logp_a - logp_b\n\n    # 2. Normalize cost differences using rank_gap\n    # We use the absolute difference for ranking to handle potential float noise\n    # The rank_gap function is stable and returns values in [0, 1]\n    cost_rank_gap = rank_gap(torch.abs(delta_cost))\n\n    # 3. Define an adaptive margin\n    # Margin is scaled by beta and is proportional to the cost gap's significance\n    margin = beta * cost_rank_gap\n\n    # 4. Calculate the core loss term\n    # We use F.sigmoid, which is numerically stable.\n    # The argument is `margin - delta_logp`. The loss is high if `delta_logp` is less than `margin`.\n    # This is equivalent to log(1 + exp(margin - delta_logp)) but using sigmoid form.\n    core_loss = F.sigmoid(margin - delta_logp)\n\n    # 5. Gate the loss (one-sided hinge-like behavior)\n    # The loss should only be non-zero if the model prefers the wrong solution, i.e., delta_logp < 0.\n    # Since we want to push delta_logp to be at least `margin`, we apply the loss\n    # whenever delta_logp < margin. The sigmoid formulation already handles this gracefully.\n    # However, a hard gate for delta_logp < 0 (wrong preference sign) makes it more hinge-like.\n    # Here, we keep the smooth sigmoid version which penalizes failing to meet the margin.\n    # A key insight is that the prompt's `log_prob_w` and `log_prob_l` implies a fixed\n    # (winner, loser) pair based on cost, so we don't need an explicit gate.\n    # The loss naturally penalizes `logp_a < logp_b` (i.e., `delta_logp < 0`).\n\n    loss_per_item = core_loss\n\n    # Apply optional weights\n    if weight is not None:\n        loss_per_item = loss_per_item * weight\n\n    # 6. Return the mean loss\n    return loss_per_item.mean()"}, "static_ok": true, "static_reason": "", "dynamic_ok": false, "dynamic_reason": "forward_error: name 'rank_gap' is not defined", "loss_value": null, "grad_norm": null}
{"generation": 0, "index": 2, "ir": {"name": "Adaptive Margin Log-Ratio Loss", "intuition": "This loss function conceptualizes the preference learning problem as aligning two ratios: the model's preference ratio, expressed by the log-probability difference, and the ground-truth quality ratio, derived from the cost difference. It introduces an adaptive margin that is proportional to the rank-normalized cost gap. This margin encourages the model to not just correctly rank the pair, but to assign a log-probability difference that is commensurate with the magnitude of the cost difference. The use of `tanh` and `logsigmoid` ensures that the loss is bounded and numerically stable, preventing extreme gradients from very large cost or log-probability gaps. The loss is high when the model's preference strongly contradicts a large quality gap, and low when they are aligned.", "pseudocode": "1. Calculate the log-probability difference: `logp_diff = logp_winner - logp_loser`.\n2. Calculate the cost difference: `cost_diff = cost_loser - cost_winner` (always non-negative).\n3. Normalize the cost difference using a rank-based z-score (`rank_gap`) to get a stable measure of its magnitude: `normalized_cost_gap`.\n4. Create an adaptive margin that scales with this normalized cost gap: `margin = margin_scale * softplus(normalized_cost_gap)`.\n5. The core of the loss is `logsigmoid(logp_diff - margin)`. This pushes `logp_diff` to be greater than the adaptive margin.\n6. To handle potential instability from large `logp_diff`, apply a `tanh` function to it, creating a bounded 'preference score': `bounded_logp_diff = tanh_scale * tanh(logp_diff / tanh_scale)`.\n7. The final loss for a pair is `-logsigmoid(bounded_logp_diff - margin)`.\n8. Compute the weighted mean of this value across the entire batch.", "hyperparams": {"margin_scale": 1.0, "tanh_scale": 5.0}, "operators_used": ["logsigmoid", "softplus", "tanh", "rank_gap"], "implementation_hint": {"expects": ["cost_a", "cost_b", "logp_a", "logp_b"], "returns": "scalar"}, "code": "import torch\nimport torch.nn.functional as F\n\ndef rank_gap(x, eps=1e-9):\n    \"\"\"Computes a z-score over the ranks of the input tensor.\"\"\"\n    ranks = x.argsort().argsort().float()\n    ranks_z = (ranks - ranks.mean()) / (ranks.std() + eps)\n    return ranks_z\n\ndef generated_loss(batch, model_output, hyperparams):\n    \"\"\"\n    Computes the Adaptive Margin Log-Ratio Loss.\n\n    Args:\n        batch (dict): A dictionary containing tensors from the data loader.\n                      Expected keys: 'cost_a', 'cost_b'.\n        model_output (dict): A dictionary containing tensors from the model.\n                             Expected keys: 'log_prob_a', 'log_prob_b'.\n        hyperparams (dict): A dictionary of hyperparameters for the loss function.\n                           Expected keys: 'margin_scale', 'tanh_scale'.\n\n    Returns:\n        torch.Tensor: A scalar tensor representing the mean loss.\n    \"\"\"\n    cost_a, cost_b = batch['cost_a'], batch['cost_b']\n    logp_a, logp_b = model_output['log_prob_a'], model_output['log_prob_b']\n    weight = batch.get('weight', None)\n\n    # Identify winner (w) and loser (l) based on cost (lower is better)\n    is_b_winner = (cost_b < cost_a).float()\n\n    # Select costs and log_probs for winners and losers\n    cost_w = torch.where(is_b_winner.bool(), cost_b, cost_a)\n    cost_l = torch.where(is_b_winner.bool(), cost_a, cost_b)\n    logp_w = torch.where(is_b_winner.bool(), logp_b, logp_a)\n    logp_l = torch.where(is_b_winner.bool(), logp_a, logp_b)\n\n    # 1. Calculate log-probability difference\n    logp_diff = logp_w - logp_l\n\n    # 2. Calculate non-negative cost difference\n    cost_diff = cost_l - cost_w\n\n    # 3. Normalize the cost difference using a stable rank-based method\n    # This converts absolute cost gaps into a statistical measure of their significance\n    # within the batch, making the margin robust to cost scaling.\n    with torch.no_grad():\n        normalized_cost_gap = rank_gap(cost_diff)\n\n    # 4. Create an adaptive margin that scales with the normalized cost gap\n    # softplus ensures the margin is non-negative and smooth.\n    margin_scale = hyperparams.get('margin_scale', 1.0)\n    margin = margin_scale * F.softplus(normalized_cost_gap)\n\n    # 5. Apply a scaled tanh to the logp_diff to bound its influence and prevent\n    # extreme values from dominating the gradient. This improves numerical stability.\n    tanh_scale = hyperparams.get('tanh_scale', 5.0)\n    bounded_logp_diff = tanh_scale * torch.tanh(logp_diff / tanh_scale)\n\n    # 6. Compute the core preference loss using logsigmoid\n    # This is equivalent to a logistic loss. The goal is to make `bounded_logp_diff`\n    # larger than the adaptive `margin`.\n    # loss = -logsigmoid(x) is equivalent to softplus(-x)\n    losses = -F.logsigmoid(bounded_logp_diff - margin)\n\n    # 7. Apply optional sample weights and compute the mean loss\n    if weight is not None:\n        loss = (losses * weight).sum() / weight.sum().clamp(min=1e-9)\n    else:\n        loss = losses.mean()\n\n    return loss"}, "static_ok": true, "static_reason": "", "dynamic_ok": false, "dynamic_reason": "forward_error: 'log_prob_a'", "loss_value": null, "grad_norm": null}
{"generation": 0, "index": 3, "ir": {"name": "Adaptive Margin Hinge Loss with Cost-Gap Normalization", "intuition": "This loss function adapts the hinge loss concept to preference learning. It enforces a margin between the log-probability of the better solution and the worse one. The size of this margin is not fixed but adapts proportionally to the normalized cost difference between the two solutions. This means that when the cost gap is large, the model is pushed more strongly to prefer the better solution by a wide log-probability margin. Conversely, for small, potentially noisy cost differences, the required margin is small, preventing the model from overfitting to minor variations. The hinge mechanism (ReLU) ensures that no loss is incurred if the preference margin is already satisfied, focusing the training effort on 'mistakes'. A tanh function is used to squash the log-probability difference, ensuring numerical stability for extreme logit gaps.", "pseudocode": "1. For each pair (a, b) with cost(a) < cost(b), calculate the cost difference delta_cost = cost(b) - cost(a).\n2. Normalize the cost differences across the batch, for example, using z-score normalization, to get norm_delta_cost. This makes the margin scale invariant to the absolute cost values.\n3. Compute the adaptive margin as margin = alpha * softplus(norm_delta_cost), where alpha is a scaling hyperparameter. Using softplus ensures the margin is always positive and smooth.\n4. Calculate the model's preference score difference, logp_diff = logp(a) - logp(b).\n5. Use the tanh function to squash the logp_diff into a stable range: squashed_diff = tanh_scale * tanh(logp_diff / tanh_scale). This prevents extreme logit differences from causing instability.\n6. The core loss is a hinge-like term: relu(margin - squashed_diff). This penalizes the model only if the preference difference is smaller than the required adaptive margin.\n7. Apply optional sample weights and compute the mean loss across the batch.", "hyperparams": {"alpha": 1.0, "tanh_scale": 10.0, "epsilon": 1e-08}, "operators_used": ["zscore", "softplus", "tanh", "relu"], "implementation_hint": {"expects": ["cost_a", "cost_b", "logp_a", "logp_b"], "returns": "scalar"}, "code": "import torch\nimport torch.nn.functional as F\n\ndef generated_loss(batch, model_output, extra):\n    \"\"\"\n    Computes an adaptive margin hinge loss.\n\n    The loss enforces that the log-probability of the winning solution should be greater\n    than the log-probability of the losing one by a margin proportional to the\n    normalized cost difference.\n    \"\"\"\n    # Hyperparameters from the 'extra' dict, with defaults\n    alpha = extra.get(\"alpha\", 1.0)\n    tanh_scale = extra.get(\"tanh_scale\", 10.0)\n    epsilon = extra.get(\"epsilon\", 1e-8)\n\n    # In this setting, (a, b) is a pair where cost(a) is not necessarily < cost(b).\n    # The provided tensors are log_prob_w (winner) and log_prob_l (loser).\n    # We assume cost_w < cost_l, so we can directly use the provided tensors.\n    # For clarity, let's use the (a, b) notation where a is the winner and b is the loser.\n    cost_a = batch['cost_w']\n    cost_b = batch['cost_l']\n    logp_a = batch['log_prob_w']\n    logp_b = batch['log_prob_l']\n\n    # 1. Calculate cost difference (always positive since cost_l > cost_w)\n    cost_gap = cost_b - cost_a\n\n    # 2. Normalize the cost gap (z-score normalization for batch-level adaptation)\n    # This makes the margin robust to the scale of costs.\n    mean_gap = cost_gap.mean()\n    std_gap = cost_gap.std()\n    norm_cost_gap = (cost_gap - mean_gap) / (std_gap + epsilon)\n\n    # 3. Compute the adaptive margin using softplus for a smooth, non-negative margin\n    # The margin increases as the normalized cost gap increases.\n    adaptive_margin = alpha * F.softplus(norm_cost_gap)\n\n    # 4. Calculate the log-probability difference\n    logp_diff = logp_a - logp_b\n\n    # 5. Squash the log-probability difference using a scaled tanh for stability\n    # This prevents extreme logit differences from producing huge loss values or gradients.\n    squashed_logp_diff = tanh_scale * torch.tanh(logp_diff / (tanh_scale + epsilon))\n\n    # 6. Compute the hinge loss: max(0, margin - difference)\n    # This penalizes the model if the logp_diff is not greater than the adaptive_margin.\n    loss_per_item = F.relu(adaptive_margin - squashed_logp_diff)\n\n    # 7. Apply optional weights and compute the final mean loss\n    weights = batch.get('weight', None)\n    if weights is not None:\n        loss = (loss_per_item * weights).sum() / (weights.sum() + epsilon)\n    else:\n        loss = loss_per_item.mean()\n\n    return loss"}, "static_ok": true, "static_reason": "", "dynamic_ok": false, "dynamic_reason": "forward_error: 'cost_w'", "loss_value": null, "grad_norm": null}
{"generation": 0, "index": 4, "ir": {"name": "Adaptive Margin Sigmoid Loss with Cost Rank Normalization", "intuition": "This loss function uses a sigmoid-based structure similar to a logistic loss, but with an adaptive margin. The margin is determined by the rank-normalized cost difference between two solutions. Normalizing the cost difference by its rank within the batch (via `rank_gap`) makes the margin robust to outliers and varying cost scales across different problem instances. The `tanh` function is applied to the log-probability difference to squash extreme values, preventing overly confident predictions from dominating the loss and ensuring numerical stability. The overall effect is a loss that focuses on getting the relative ordering correct, with a learning signal strength proportional to how 'significant' the cost difference is, but without being thrown off by extreme values in either costs or model logits.", "pseudocode": "1. For a batch of solution pairs (a, b), calculate the cost difference `delta_cost = cost(a) - cost(b)`.\n2. Calculate the log-probability difference `delta_logp = logp(a) - logp(b)`.\n3. Normalize the cost differences within the batch using the `rank_gap` operator to get a stable, rank-based margin `m_rank`.\n4. Scale the rank-based margin by a hyperparameter `alpha`.\n5. Squash the log-probability difference using `tanh` for stability.\n6. The core of the loss is `logsigmoid(m_rank - tanh(delta_logp))`. This penalizes the model if `tanh(delta_logp)` is not greater than the adaptive margin `m_rank`.\n7. The final loss is the negative of this value, averaged over the batch.", "hyperparams": {"alpha": 1.0}, "operators_used": ["logsigmoid", "tanh", "rank_gap"], "implementation_hint": {"expects": ["cost_a", "cost_b", "logp_a", "logp_b"], "returns": "scalar"}, "code": "import torch\nimport torch.nn.functional as F\n\ndef rank_gap(x, eps=1e-9):\n    \"\"\"Normalizes a tensor by its rank, scaled to [-1, 1].\"\"\"\n    # Sort the absolute values of x to get ranks\n    sorted_indices = torch.argsort(torch.abs(x))\n    ranks = torch.empty_like(sorted_indices, dtype=x.dtype)\n    ranks[sorted_indices] = torch.arange(len(x), device=x.device, dtype=x.dtype)\n    \n    # Normalize ranks to [0, 1]\n    normalized_ranks = ranks / (len(x) - 1 + eps)\n    \n    # Apply original sign and scale to [-1, 1]\n    return torch.sign(x) * (2 * normalized_ranks - 1)\n\ndef generated_loss(batch, model_output, extra):\n    \"\"\"\n    Adaptive Margin Sigmoid Loss with Cost Rank Normalization.\n\n    Assumes batch['cost_a'] < batch['cost_b'] is the desired preference.\n    The model should learn to produce logp(a) > logp(b).\n    The provided log probabilities are for the 'winner' and 'loser' solutions.\n    \"\"\"\n    # Read hyperparameters\n    alpha = extra.get('alpha', 1.0)\n\n    # Inputs are structured as (winner, loser) pairs\n    # We want log_prob_w > log_prob_l\n    cost_w = batch['cost_a']\n    cost_l = batch['cost_b']\n    log_prob_w = batch['log_prob_w']\n    log_prob_l = batch['log_prob_l']\n\n    # 1. Calculate cost and log-probability differences\n    # delta_cost is negative since cost_w < cost_l\n    delta_cost = cost_w - cost_l\n    delta_logp = log_prob_w - log_prob_l\n\n    # 2. Compute the adaptive margin using rank normalization\n    # rank_gap(delta_cost) will be negative. We want a positive margin.\n    # The margin is larger for larger (more negative) cost gaps.\n    margin = -rank_gap(delta_cost)\n\n    # 3. Scale the margin\n    scaled_margin = alpha * margin\n    \n    # 4. Squash the log-probability difference for stability\n    squashed_delta_logp = torch.tanh(delta_logp)\n\n    # 5. Compute the core logistic-style loss\n    # The argument to logsigmoid is `margin - prediction`.\n    # We want squashed_delta_logp > scaled_margin.\n    # If this holds, `scaled_margin - squashed_delta_logp` is negative, and logsigmoid is close to 0.\n    # If it doesn't hold, the argument is positive, and logsigmoid is negative (loss is positive).\n    loss_per_item = -F.logsigmoid(squashed_delta_logp - scaled_margin)\n\n    # 6. Apply optional weights and compute the mean loss\n    if 'weight' in batch and batch['weight'] is not None:\n        loss = (loss_per_item * batch['weight']).mean()\n    else:\n        loss = loss_per_item.mean()\n\n    return loss"}, "static_ok": true, "static_reason": "", "dynamic_ok": false, "dynamic_reason": "forward_error: name 'rank_gap' is not defined", "loss_value": null, "grad_norm": null}
{"generation": 0, "index": 5, "ir": {"name": "Adaptive Margin Loss with Cost-Gap Ranking", "intuition": "This loss adapts its margin and weighting based on the relative significance of the cost difference within a batch. It uses rank normalization on the cost gaps to prevent outliers (very large or very small cost differences) from dominating the gradients. The core idea is to create a dynamic margin: for pairs with a 'significant' cost gap (high rank), the model is pushed harder to align its preferences, while for pairs with a negligible cost gap (low rank), the penalty for misalignment is softened. This makes the training more robust to the scale of cost values and focuses learning on the most informative preference pairs.", "pseudocode": "1. For a batch of solution pairs (a, b), calculate the cost difference: delta_cost = cost(b) - cost(a).\n2. Calculate the log-probability difference: delta_logp = logp(a) - logp(b).\n3. Use the 'rank_gap' function to transform delta_cost into a normalized rank-based score between 0 and 1. This score, 'cost_rank_gap', represents the percentile rank of the cost difference in the batch.\n4. Define an adaptive margin as a function of this rank score, e.g., margin = alpha * cost_rank_gap. This makes the target separation between log-probabilities proportional to the relative importance of the cost gap.\n5. The loss is computed using a logistic (logsigmoid) formulation: loss = -logsigmoid(delta_logp - margin). This penalizes the model when its log-probability difference doesn't exceed the adaptive margin that reflects the cost-based preference strength.\n6. An optional re-weighting term, also derived from the rank score, can be applied to further emphasize pairs with significant cost gaps.\n7. The final loss is the (optionally weighted) mean of the per-pair losses.", "hyperparams": {"alpha": 2.0, "beta": 0.5}, "operators_used": ["logsigmoid", "rank_gap"], "implementation_hint": {"expects": ["cost_a", "cost_b", "logp_a", "logp_b"], "returns": "scalar"}, "code": "import torch\nimport torch.nn.functional as F\n\ndef rank_gap(x):\n    \"\"\"Transforms a tensor of values into their percentile ranks scaled to [0, 1].\"\"\"\n    if x.numel() <= 1:\n        return torch.ones_like(x) * 0.5\n    ranks = torch.empty_like(x, dtype=torch.float)\n    ranks[x.argsort()] = torch.arange(x.numel(), device=x.device, dtype=torch.float)\n    # Scale ranks to be in [0, 1]\n    normalized_ranks = ranks / (x.numel() - 1)\n    return normalized_ranks\n\ndef generated_loss(batch, model_output, extra):\n    \"\"\"\n    Adaptive Margin Loss with Cost-Gap Ranking.\n\n    This loss function uses the rank of the cost difference within a batch\n    to dynamically set the margin for the preference comparison. This makes\n    the loss robust to the scale of costs and focuses learning on pairs with\n    a meaningful performance gap.\n\n    Args:\n        batch (dict): A dictionary containing tensors.\n            'cost_a': Costs of the first set of solutions (preferred). Shape [N].\n            'cost_b': Costs of the second set of solutions (not preferred). Shape [N].\n            'weight' (optional): Per-pair loss weights. Shape [N].\n        model_output (dict): A dictionary containing model outputs.\n            'log_prob_a': Log probabilities of the first solutions. Shape [N].\n            'log_prob_b': Log probabilities of the second solutions. Shape [N].\n        extra (dict): A dictionary for additional parameters, like hyperparameters.\n            'alpha' (float): Controls the maximum margin size.\n            'beta' (float): Controls the strength of re-weighting based on rank.\n\n    Returns:\n        torch.Tensor: A scalar tensor representing the mean loss.\n    \"\"\"\n    cost_a, cost_b = batch['cost_a'], batch['cost_b']\n    logp_a, logp_b = model_output['log_prob_a'], model_output['log_prob_b']\n    weights = batch.get('weight', None)\n\n    alpha = extra.get('alpha', 2.0)\n    beta = extra.get('beta', 0.5)\n\n    with torch.no_grad():\n        # Lower cost is better, so cost_b - cost_a should be positive for (a preferred over b)\n        cost_diff = cost_b - cost_a\n        \n        # Use rank_gap to get a normalized, robust measure of preference strength\n        # This value is in [0, 1], where 1 means the largest cost gap in the batch\n        cost_rank_score = rank_gap(cost_diff)\n\n    # The log probability difference should be positive if the model prefers a over b\n    logp_diff = logp_a - logp_b\n\n    # The margin is now adaptive: a larger ranked cost gap demands a larger logp difference\n    adaptive_margin = alpha * cost_rank_score\n\n    # Core logistic loss: we want logp_diff to be greater than the adaptive_margin\n    # -logsigmoid(x) is equivalent to softplus(-x)\n    losses = -F.logsigmoid(logp_diff - adaptive_margin)\n\n    # Optional: re-weight the loss to focus more on pairs with a significant cost gap\n    # The weight is 1 + beta * score, so it ranges from 1 to 1+beta\n    if beta > 0:\n        reweighting_factor = 1.0 + beta * cost_rank_score\n        losses = losses * reweighting_factor\n\n    # Apply external weights if provided\n    if weights is not None:\n        losses = losses * weights\n\n    return losses.mean()"}, "static_ok": true, "static_reason": "", "dynamic_ok": false, "dynamic_reason": "forward_error: 'log_prob_a'", "loss_value": null, "grad_norm": null}
{"generation": 0, "index": 6, "ir": {"name": "Rank-Aware Margin Loss", "intuition": "This loss function uses the rank-normalized cost difference to set a dynamic, adaptive margin. Instead of treating all cost gaps equally, it focuses more on pairs with a significant, non-trivial difference in quality, while being less sensitive to noise or tiny cost variations. The margin is scaled by the rank-transformed cost gap, which makes the loss robust to outliers in the cost distribution. A softplus function is used to ensure the loss is smooth, bounded below by zero, and avoids the vanishing gradients of a sigmoid for large negative inputs.", "pseudocode": "1. Compute the cost difference: delta_cost = cost(b) - cost(a).\n2. Compute the log probability difference: delta_logp = logp(a) - logp(b).\n3. Normalize the cost differences across the batch using a rank-based z-score (rank_gap) to get normalized_delta_cost. This makes the margin robust to the scale and distribution of costs.\n4. Calculate an adaptive margin, m, which is a scaled version of the normalized_delta_cost, clamped to a reasonable range. The margin is positive when cost(a) < cost(b).\n5. The core loss is softplus(-delta_logp + m). This penalizes the model if its preference (delta_logp) does not exceed the adaptive margin (m).\n6. Apply optional sample weights and compute the mean loss over the batch.", "hyperparams": {"margin_scale": 1.0, "margin_min": 0.0, "margin_max": 5.0}, "operators_used": ["rank_gap", "clamp", "softplus"], "implementation_hint": {"expects": ["cost_a", "cost_b", "logp_a", "logp_b"], "returns": "scalar"}, "code": "import torch\nimport torch.nn.functional as F\n\ndef rank_gap(x, y):\n    \"\"\"Calculates the rank-based z-score of the gap (y - x).\"\"\"\n    gap = y - x\n    # Assign ranks to the gap values\n    sorted_gap, indices = torch.sort(gap)\n    ranks = torch.empty_like(indices, dtype=torch.float)\n    ranks[indices] = torch.arange(len(gap), device=gap.device, dtype=torch.float)\n    # Z-score normalize the ranks\n    ranks_mean = ranks.mean()\n    ranks_std = ranks.std()\n    # Add a small epsilon to prevent division by zero for single-element batches\n    normalized_ranks = (ranks - ranks_mean) / (ranks_std + 1e-8)\n    return normalized_ranks\n\ndef generated_loss(batch, hyperparams, extra):\n    \"\"\"\n    Computes a Rank-Aware Margin Loss.\n\n    Args:\n        batch (dict): A dictionary containing tensors:\n            'cost_a': Costs of the first solutions in pairs (shape [N]).\n            'cost_b': Costs of the second solutions in pairs (shape [N]).\n            'logp_a': Log probabilities of the first solutions (shape [N]).\n            'logp_b': Log probabilities of the second solutions (shape [N]).\n            'weight': Optional sample weights (shape [N]).\n        hyperparams (dict): A dictionary of hyperparameters for the loss function.\n            'margin_scale': Scaling factor for the adaptive margin.\n            'margin_min': Minimum value for the margin.\n            'margin_max': Maximum value for the margin.\n        extra (dict): Extra parameters (not used here).\n\n    Returns:\n        torch.Tensor: A scalar tensor representing the mean loss.\n    \"\"\"\n    # Unpack inputs from the batch dictionary\n    cost_a = batch['cost_a']\n    cost_b = batch['cost_b']\n    logp_a = batch['logp_a']\n    logp_b = batch['logp_b']\n    weight = batch.get('weight')\n\n    # Unpack hyperparameters\n    margin_scale = hyperparams.get('margin_scale', 1.0)\n    margin_min = hyperparams.get('margin_min', 0.0)\n    margin_max = hyperparams.get('margin_max', 5.0)\n\n    # For preference learning, a is preferred over b if cost_a < cost_b.\n    # We want logp_a > logp_b in this case.\n    delta_logp = logp_a - logp_b\n\n    # Compute the rank-normalized cost gap. This serves as a robust, non-parametric\n    # measure of how significant the cost difference is within the batch.\n    # rank_gap(cost_a, cost_b) will be large when cost_b is much larger than cost_a relative to other pairs.\n    normalized_delta_cost = rank_gap(cost_a, cost_b)\n\n    # Create an adaptive margin based on the normalized cost difference.\n    # The margin is scaled and clamped to prevent extreme values.\n    margin = torch.clamp(\n        margin_scale * normalized_delta_cost,\n        min=margin_min,\n        max=margin_max\n    )\n\n    # The core of the loss function. We want delta_logp to be greater than the margin.\n    # The loss is softplus(margin - delta_logp), which is a smooth version of relu(margin - delta_logp).\n    # This penalizes cases where logp_a - logp_b < margin.\n    # When cost_a < cost_b, margin > 0, so we push delta_logp up.\n    # When cost_a > cost_b, margin is small or negative (due to rank_gap), relaxing the constraint.\n    losses = F.softplus(margin - delta_logp)\n\n    # Apply sample weights if provided\n    if weight is not None:\n        losses = losses * weight\n\n    # Return the mean loss over the batch\n    return losses.mean()"}, "static_ok": true, "static_reason": "", "dynamic_ok": false, "dynamic_reason": "forward_error: 'logp_a'", "loss_value": null, "grad_norm": null}
{"generation": 0, "index": 7, "ir": {"name": "Adaptive Margin Hinge Loss with Rank-Gap Scaling", "intuition": "This loss uses a hinge-like structure to enforce a margin between the log-probabilities of preferred and non-preferred solutions. The margin is not fixed; instead, it adaptively scales with the normalized cost difference (rank-gap), demanding a larger log-probability gap for solutions with a larger quality difference. A tanh function is applied to the core term to smoothly bound the loss and its gradients, preventing instability from extreme log-probability or cost differences. This combines the clear separation goal of a hinge loss with a dynamic, cost-aware target that is robust to outliers.", "pseudocode": "1. Calculate the cost difference: delta_cost = cost_b - cost_a. Note: a is the preferred solution, so delta_cost is positive.\n2. Normalize the cost difference using a rank-gap transformation to get a stable, scaled value between 0 and 1. This represents the 'importance' of the preference.\n3. Compute the log-probability difference: delta_logp = logp_a - logp_b.\n4. Define an adaptive margin, which is the normalized cost difference scaled by a hyperparameter, beta.\n5. The core of the loss is the difference between the adaptive margin and the log-probability difference: margin - delta_logp.\n6. Apply a non-linearity (e.g., tanh) to this core term. This bounds the loss and its gradient, ensuring numerical stability.\n7. Use a relu (or softplus for a smoother version) on the output of the tanh to create a one-sided hinge-like loss, only penalizing cases where the model's preference (delta_logp) is smaller than the target margin.\n8. Scale the final loss by an optional weight and compute the mean over the batch.", "hyperparams": {"beta": 10.0, "tau": 1.0, "epsilon": 1e-08}, "operators_used": ["rank_gap", "tanh", "relu", "clamp"], "implementation_hint": {"expects": ["cost_a", "cost_b", "logp_a", "logp_b"], "returns": "scalar"}, "code": "import torch\nimport torch.nn.functional as F\n\ndef rank_gap(x, tau=1.0, epsilon=1e-8):\n    \"\"\"Numerically stable rank-gap normalization based on tanh.\"\"\"\n    # Clamp to prevent tanh from saturating too quickly with extreme tau\n    scaled_x = torch.clamp(x / (tau + epsilon), min=-20.0, max=20.0)\n    return (torch.tanh(scaled_x) + 1.0) / 2.0\n\ndef generated_loss(batch, model_output, extra):\n    \"\"\"Adaptive Margin Hinge Loss with Rank-Gap Scaling.\"\"\"\n    # Hyperparameters from the extra config dict\n    beta = extra.get('beta', 10.0)\n    tau = extra.get('tau', 1.0)\n    epsilon = extra.get('epsilon', 1e-8)\n\n    # Unpack tensors from batch\n    # By convention, 'a' is the winner (preferred) and 'b' is the loser (not preferred)\n    cost_a = batch['cost_a']\n    cost_b = batch['cost_b']\n    logp_a = batch['log_prob_w'] # log probability of the winner\n    logp_b = batch['log_prob_l'] # log probability of the loser\n    weight = batch.get('weight', None)\n\n    # Ensure cost_a <= cost_b, so delta_cost is non-negative\n    delta_cost = cost_b - cost_a\n    # Clamp to avoid issues with identical costs if epsilon is zero\n    delta_cost = torch.clamp(delta_cost, min=0.0)\n\n    # 1. Normalize the cost difference using rank-gap to get a stable signal\n    # normalized_cost_gap will be in [0, 1]\n    normalized_cost_gap = rank_gap(delta_cost, tau=tau, epsilon=epsilon)\n\n    # 2. Define an adaptive margin based on the normalized cost gap\n    # A larger quality difference demands a larger log-probability separation\n    adaptive_margin = beta * normalized_cost_gap\n\n    # 3. Compute the log-probability difference\n    # We want logp_a to be greater than logp_b\n    delta_logp = logp_a - logp_b\n\n    # 4. Core loss term: how far is the logp difference from the target margin?\n    # A positive value means the model fails to meet the margin.\n    # We use tanh to bound this term, ensuring gradients do not explode.\n    # The term inside tanh is (margin - logp_diff). We want this to be negative.\n    bounded_term = torch.tanh(adaptive_margin - delta_logp)\n\n    # 5. Hinge loss structure: only penalize when the model's preference is insufficient.\n    # relu(bounded_term) is non-zero only if (adaptive_margin > delta_logp),\n    # which is exactly the violation condition of a hinge loss.\n    # The tanh ensures the penalty is smoothly bounded between 0 and 1.\n    loss = F.relu(bounded_term)\n\n    # 6. Apply optional sample weights and compute the mean\n    if weight is not None:\n        loss = loss * weight\n\n    return loss.mean()"}, "static_ok": true, "static_reason": "", "dynamic_ok": false, "dynamic_reason": "forward_error: name 'rank_gap' is not defined", "loss_value": null, "grad_norm": null}
{"generation": 1, "index": 0, "ir": {"name": "Adaptive Margin Ranking Loss with Cost-Gap Scaling", "intuition": "This loss adapts the classical ranking loss by introducing a dynamic margin that grows with the relative cost difference between two solutions. It uses the `tanh` function to smoothly scale the log-probability difference, preventing extreme gradients from dominating the learning process. The core idea is to enforce a larger separation in log-probabilities for pairs with a larger quality gap, while being robust to outliers in both cost and log-probability spaces.", "pseudocode": "1. Calculate the cost difference: delta_cost = cost(b) - cost(a).\n2. Normalize the cost differences across the batch using z-score to make the margin scale invariant to the absolute cost scale of the problem instance.\n3. Compute an adaptive margin: margin = alpha * softplus(normalized_delta_cost). The softplus ensures the margin is non-negative and grows smoothly.\n4. Calculate the log-probability difference: delta_logp = logp(a) - logp(b).\n5. Scale the log-probability difference using tanh to bound its influence: scaled_delta_logp = beta * tanh(delta_logp / beta). This acts as a robust regularizer.\n6. Compute the ranking loss for a pair: loss = relu(margin - scaled_delta_logp).\n7. Apply optional sample weights and compute the mean loss over the batch.", "hyperparams": {"alpha": 1.0, "beta": 5.0, "epsilon": 1e-08}, "operators_used": ["softplus", "tanh", "relu", "zscore"], "implementation_hint": {"expects": ["cost_a", "cost_b", "log_prob_w", "log_prob_l"], "returns": "scalar"}, "code": "import torch\nimport torch.nn.functional as F\n\ndef zscore(x, epsilon=1e-8):\n    \"\"\"Numerically stable z-score normalization.\"\"\"\n    if x.numel() <= 1:\n        return torch.zeros_like(x)\n    mean = x.mean()\n    std = x.std()\n    return (x - mean) / (std + epsilon)\n\ndef generated_loss(batch, model_output, hyperparams):\n    \"\"\"\n    Computes an adaptive margin ranking loss.\n    Assumes cost_a is the better solution (winner) and cost_b is the worse (loser).\n    Thus, we expect log_prob_w > log_prob_l.\n    \"\"\"\n    # Unpack inputs from the batch dictionary\n    # In this context, 'a' is the winner (w) and 'b' is the loser (l)\n    cost_w = batch['cost_a']\n    cost_l = batch['cost_b']\n    logp_w = batch['log_prob_w']\n    logp_l = batch['log_prob_l']\n    weights = batch.get('weight', None)\n\n    # Retrieve hyperparameters\n    alpha = hyperparams.get('alpha', 1.0)\n    beta = hyperparams.get('beta', 5.0)\n    epsilon = hyperparams.get('epsilon', 1e-8)\n\n    # 1. Calculate the cost difference (should be positive since cost_l > cost_w)\n    delta_cost = cost_l - cost_w\n\n    # 2. Normalize cost differences for stable margin calculation\n    # This makes the margin scale invariant to the problem's cost range\n    normalized_delta_cost = zscore(delta_cost, epsilon=epsilon)\n\n    # 3. Compute an adaptive, non-negative margin\n    # The margin grows as the normalized quality gap increases\n    margin = alpha * F.softplus(normalized_delta_cost)\n\n    # 4. Calculate the log-probability difference\n    delta_logp = logp_w - logp_l\n\n    # 5. Scale the log-probability difference using tanh for robustness\n    # This prevents very large logp differences from causing extreme gradients\n    # and acts as a soft clamp.\n    scaled_delta_logp = beta * torch.tanh(delta_logp / beta)\n\n    # 6. Compute the core ranking loss\n    # The loss is positive only if scaled_delta_logp < margin\n    # This pushes logp_w to be greater than logp_l by at least the margin.\n    pair_loss = F.relu(margin - scaled_delta_logp)\n\n    # 7. Apply sample weights if provided\n    if weights is not None:\n        pair_loss = pair_loss * weights\n\n    # Return the mean loss over the batch\n    return pair_loss.mean()"}, "static_ok": true, "static_reason": "", "dynamic_ok": false, "dynamic_reason": "forward_error: name 'zscore' is not defined", "loss_value": null, "grad_norm": null}
{"generation": 1, "index": 1, "ir": {"name": "Adaptive Margin Sigmoid Loss", "intuition": "This loss adapts the learning signal based on both the cost difference and the model's current confidence. It uses a sigmoid function to create a bounded loss, preventing extreme gradients from large log-probability differences. A dynamic margin, derived from the normalized cost gap, is added. If the model is already correct (logp(a) > logp(b) for cost(a) < cost(b)), the loss is pushed towards zero. If the model is incorrect, the margin requires the log-probability difference to not just be positive, but to exceed a threshold proportional to how much better solution 'a' is than 'b'. A tanh-based reweighting factor further focuses learning on pairs where the model is confident but wrong, while down-weighting pairs that are either too easy (already correct) or too hard (large cost gap but model is very wrong).", "pseudocode": "1. For each pair (a, b), calculate the cost difference 'cost_gap' = cost(b) - cost(a).\n2. Normalize 'cost_gap' across the batch to get 'normalized_cost_gap' (e.g., using z-score or rank-based normalization).\n3. Calculate the log-probability difference 'logp_diff' = logp(a) - logp(b).\n4. Create an adaptive margin 'margin' = alpha * softplus(normalized_cost_gap), where alpha is a hyperparameter. This ensures the margin is non-negative and scales with the relative cost improvement.\n5. Compute the core loss term using a sigmoid on the scaled log-probability difference and margin: loss_term = logsigmoid(-(logp_diff - margin)). This is a logistic loss where the model is penalized if logp_diff is not greater than the margin.\n6. Compute a dynamic weight 'focus_weight' = 0.5 * (1 + tanh(beta * logp_diff * sign(cost_gap))). This weight is close to 1 when the model's preference (sign of logp_diff) mismatches the true preference (sign of cost_gap), and close to 0 when they align. 'beta' controls the sharpness of this focus.\n7. The final loss for the pair is focus_weight * loss_term. This selectively applies the loss, concentrating on correcting mistakes.\n8. Average the loss over all pairs in the batch.", "hyperparams": {"alpha": 1.0, "beta": 2.0, "normalize_mode": "rank_gap", "clamp_min": -5.0, "clamp_max": 5.0}, "operators_used": ["logsigmoid", "softplus", "tanh", "clamp", "rank_gap", "zscore"], "implementation_hint": {"expects": ["cost_a", "cost_b", "log_prob_w", "log_prob_l"], "returns": "scalar"}, "code": "import torch\nimport torch.nn.functional as F\n\n# Helper functions for normalization, assumed to be available\ndef zscore(x):\n    if x.numel() <= 1:\n        return torch.zeros_like(x)\n    return (x - x.mean()) / (x.std() + 1e-8)\n\ndef rank_gap(x):\n    if x.numel() <= 1:\n        return torch.zeros_like(x)\n    # A simple rank-based normalization to [-1, 1]\n    ranks = torch.empty_like(x).scatter_(0, x.argsort(), torch.arange(x.numel(), device=x.device, dtype=x.dtype))\n    return (ranks / (x.numel() - 1) * 2 - 1)\n\ndef generated_loss(batch, model_output, extra):\n    \"\"\"\n    Adaptive Margin Sigmoid Loss.\n\n    Args:\n        batch (dict): A dictionary containing tensors from the dataloader.\n                      Expected keys: 'cost_a', 'cost_b'.\n        model_output (dict): A dictionary containing model outputs.\n                             Expected keys: 'log_prob_w', 'log_prob_l'.\n        extra (dict): A dictionary containing hyperparameters and other info.\n                      Expected keys: 'alpha', 'beta', 'normalize_mode', 'clamp_min', 'clamp_max'.\n\n    Returns:\n        torch.Tensor: A scalar tensor representing the mean loss.\n    \"\"\"\n    cost_a, cost_b = batch['cost_a'], batch['cost_b']\n    logp_w, logp_l = model_output['log_prob_w'], model_output['log_prob_l']\n\n    # Hyperparameters from the 'extra' dict\n    alpha = extra.get('alpha', 1.0)\n    beta = extra.get('beta', 2.0)\n    normalize_mode = extra.get('normalize_mode', 'rank_gap')\n    clamp_min = extra.get('clamp_min', -5.0)\n    clamp_max = extra.get('clamp_max', 5.0)\n\n    # Ensure winner (w) has lower cost than loser (l)\n    # This loss assumes cost(w) < cost(l) by convention.\n    # logp_a corresponds to the better solution, logp_b to the worse one.\n    logp_a = torch.where(cost_a < cost_b, logp_w, logp_l)\n    logp_b = torch.where(cost_a < cost_b, logp_l, logp_w)\n\n    cost_w = torch.min(cost_a, cost_b)\n    cost_l = torch.max(cost_a, cost_b)\n\n    # 1. Calculate cost gap (always positive by construction)\n    cost_gap = cost_l - cost_w\n\n    # 2. Normalize the cost gap\n    if normalize_mode == 'zscore':\n        normalized_cost_gap = zscore(cost_gap)\n    elif normalize_mode == 'rank_gap':\n        normalized_cost_gap = rank_gap(cost_gap)\n    else: # Default to no normalization\n        normalized_cost_gap = cost_gap\n\n    # Clamp normalized gap for stability\n    normalized_cost_gap = torch.clamp(normalized_cost_gap, min=clamp_min, max=clamp_max)\n\n    # 3. Calculate log-probability difference\n    logp_diff = logp_a - logp_b\n\n    # 4. Create an adaptive, non-negative margin\n    margin = alpha * F.softplus(normalized_cost_gap)\n\n    # 5. Core logistic loss term with adaptive margin\n    # The loss is -log(sigmoid(logp_diff - margin))\n    # This encourages logp_diff > margin\n    loss_term = -F.logsigmoid(logp_diff - margin)\n\n    # 6. Compute a dynamic 'focus' weight using tanh\n    # This weight is ~1 when logp_diff is negative (model is wrong) and ~0 when positive (model is right)\n    # We use -logp_diff because tanh(negative) -> -1, making the weight large.\n    # tanh(positive) -> 1, making the weight small.\n    focus_weight = 0.5 * (1 - torch.tanh(beta * logp_diff))\n\n    # 7. Apply the focus weight to the loss term\n    final_loss = focus_weight * loss_term\n\n    # Optional: Apply sample weights if provided\n    if 'weight' in batch and batch['weight'] is not None:\n        final_loss = final_loss * batch['weight']\n\n    # 8. Return the mean loss over the batch\n    return final_loss.mean()"}, "static_ok": true, "static_reason": "", "dynamic_ok": false, "dynamic_reason": "forward_error: 'log_prob_w'", "loss_value": null, "grad_norm": null}
{"generation": 1, "index": 2, "ir": {"name": "Adaptive Sigmoid Margin Loss", "intuition": "This loss function refines the standard Bradley-Terry model by introducing a dynamic margin that adapts to the magnitude of the cost difference. It uses a sigmoid function to scale the cost gap, creating a 'soft margin' that is small for negligible cost differences (reducing noise) and saturates for very large differences (preventing instability). This ensures the model focuses on learning meaningful preferences while remaining robust to extreme cost outliers. The loss is framed as a logistic loss on the difference between the model's log-probability gap and this adaptive cost-based margin.", "pseudocode": "1. Calculate the cost difference: delta_cost = cost(b) - cost(a).\n2. Normalize the cost differences across the batch using z-score normalization for stability.\n3. Compute an adaptive margin by passing the normalized cost difference through a scaled and shifted sigmoid function. This creates a margin that is close to zero for small cost gaps and approaches a maximum value for large gaps.\n4. Calculate the log-probability difference: delta_logp = logp(a) - logp(b).\n5. The core of the loss is a logistic loss (logsigmoid) applied to the difference between the adaptive margin and the log-probability gap. This penalizes the model when its probability gap does not align with the cost-derived margin.\n6. The final loss is the mean of these individual loss values over the batch.", "hyperparams": {"margin_scale": 10.0, "margin_bias": 0.0, "epsilon": 1e-08}, "operators_used": ["logsigmoid", "sigmoid", "zscore"], "implementation_hint": {"expects": ["cost_a", "cost_b", "logp_a", "logp_b"], "returns": "scalar"}, "code": "import torch\nimport torch.nn.functional as F\n\ndef generated_loss(batch, model_output, extra):\n    \"\"\"\n    Adaptive Sigmoid Margin Loss.\n\n    The loss encourages the log-probability gap to match a dynamic margin\n    derived from the normalized cost difference.\n    We assume cost_w < cost_l, so log_prob_w should be > log_prob_l.\n    The preference is (w, l), where w is preferred over l.\n    \"\"\"\n    # Read hyperparameters with defaults\n    margin_scale = extra.get('margin_scale', 10.0)\n    margin_bias = extra.get('margin_bias', 0.0)\n    epsilon = extra.get('epsilon', 1e-8)\n\n    # Unpack data from the batch\n    # In preference learning, we have a winner (w) and a loser (l)\n    # where cost(w) < cost(l)\n    cost_w = batch['cost_a']\n    cost_l = batch['cost_b']\n    logp_w = batch['log_prob_w']\n    logp_l = batch['log_prob_l']\n    weights = batch.get('weight', None)\n\n    # Ensure cost_w is indeed better (lower) than cost_l\n    # The loss is designed for pairs where a clear preference exists.\n    # delta_cost will be positive since cost_l > cost_w\n    delta_cost = cost_l - cost_w\n\n    # 1. Normalize the cost difference for stability (z-score)\n    # This makes the margin scale less sensitive to the absolute scale of costs.\n    if delta_cost.numel() > 1:\n        cost_mean = delta_cost.mean()\n        cost_std = delta_cost.std()\n        normalized_delta_cost = (delta_cost - cost_mean) / (cost_std + epsilon)\n    else:\n        # Avoid division by zero for a single-element batch\n        normalized_delta_cost = torch.zeros_like(delta_cost)\n\n    # 2. Compute the adaptive margin using a scaled sigmoid\n    # The sigmoid squashes the normalized cost difference into a (0, 1) range.\n    # Scaling this provides a bounded, non-linear margin.\n    # The margin is small for small cost differences and saturates for large ones.\n    adaptive_margin = margin_scale * torch.sigmoid(normalized_delta_cost + margin_bias)\n\n    # 3. Calculate the log-probability difference\n    # This should be positive if the model correctly prefers w over l.\n    delta_logp = logp_w - logp_l\n\n    # 4. Compute the logistic loss\n    # The loss is log(1 + exp(-(delta_logp - adaptive_margin)))\n    # which is equivalent to -logsigmoid(delta_logp - adaptive_margin).\n    # This encourages delta_logp to be greater than the adaptive_margin.\n    # We use the negative because we want to minimize the loss.\n    loss = -F.logsigmoid(delta_logp - adaptive_margin)\n\n    # 5. Apply optional weights and return the mean loss\n    if weights is not None:\n        loss = loss * weights\n\n    return loss.mean()"}, "static_ok": true, "static_reason": "", "dynamic_ok": false, "dynamic_reason": "loss 5.0067 outside soft range [-5.0, 5.0]", "loss_value": 5.006715297698975, "grad_norm": 0.0}
{"generation": 1, "index": 3, "ir": {"name": "Adaptive Hinge Loss with Rank-Gap Normalization", "intuition": "This loss function adapts the hinge loss margin based on the rank-normalized cost difference between two solutions. It treats the preference learning problem as a classification task: correctly ranking the better solution (a) over the worse one (b) by a margin. The margin itself is not fixed but is scaled by how significant the cost difference is relative to other pairs in the batch. This prevents extreme cost gaps from dominating the gradient and allows the model to focus on subtle differences when costs are similar. The use of `tanh` on the log-probability difference creates a bounded, saturated error signal, which enhances numerical stability and prevents overly aggressive updates when the model is already very confident or very wrong.", "pseudocode": "1. For a batch of solution pairs (a, b), calculate the cost difference c = cost(b) - cost(a) and log-probability difference lp = logp(a) - logp(b).\n2. Normalize the cost differences across the batch using a rank-based method (e.g., `rank_gap`) to get a stable, scaled measure `norm_gap` between 0 and 1. This represents the relative importance of the cost gap.\n3. Define an adaptive margin `m = alpha * norm_gap`, where `alpha` is a hyperparameter controlling the maximum margin size.\n4. Calculate the core loss term using a hinge-like structure: `relu(m - tanh(beta * lp))`. The `tanh` function bounds the log-prob difference to prevent extreme values from causing instability, with `beta` controlling the steepness. The `relu` function ensures that no loss is incurred if the model's preference `tanh(beta * lp)` already exceeds the adaptive margin `m`.\n5. If weights are provided, multiply the loss for each pair by its corresponding weight.\n6. The final loss is the mean of these values over the batch.", "hyperparams": {"alpha": {"type": "float", "default": 1.0, "description": "Scales the maximum margin. A higher value creates a stronger push for separation."}, "beta": {"type": "float", "default": 1.0, "description": "Temperature parameter for the tanh saturation. A higher value makes the tanh transition sharper, approximating a sign function."}}, "operators_used": ["tanh", "relu", "rank_gap"], "implementation_hint": {"expects": ["cost_a", "cost_b", "logp_a", "logp_b"], "returns": "scalar"}, "code": "import torch\nimport torch.nn.functional as F\n\ndef rank_gap(x):\n    \"\"\"Normalizes a tensor by its rank-based ordering to a [0, 1] range.\"\"\"\n    if x.numel() <= 1:\n        return torch.ones_like(x) * 0.5\n    \n    # Get the rank of each element\n    # Note: sorting can be non-differentiable w.r.t. x, but we only need gradients w.r.t. model params.\n    # The values of x (cost differences) are constants during the backward pass for the loss.\n    ranks = x.argsort().argsort().float()\n    \n    # Normalize ranks to [0, 1]\n    normalized_ranks = ranks / (x.numel() - 1)\n    return normalized_ranks\n\ndef generated_loss(batch, model_output, extra):\n    \"\"\"\n    Adaptive Hinge Loss with Rank-Gap Normalization.\n\n    Args:\n        batch (dict): A dictionary containing tensors.\n            - 'cost_a': Cost of the first solution in the pair (shape [N]).\n            - 'cost_b': Cost of the second solution in the pair (shape [N]).\n        model_output (dict): A dictionary from the model.\n            - 'log_prob_a': Log probability of the first solution (shape [N]).\n            - 'log_prob_b': Log probability of the second solution (shape [N]).\n        extra (dict): A dictionary for hyperparameters.\n            - 'alpha': The maximum margin size (float).\n            - 'beta': The temperature for tanh saturation (float).\n\n    Returns:\n        torch.Tensor: A scalar loss value.\n    \"\"\"\n    # Hyperparameters from the 'extra' dict\n    alpha = extra.get('alpha', 1.0)\n    beta = extra.get('beta', 1.0)\n\n    # For preference data, 'a' is preferred to 'b', so cost_a < cost_b\n    # We use this convention to define the winner (w) and loser (l)\n    cost_w, cost_l = batch['cost_a'], batch['cost_b']\n    logp_w, logp_l = model_output['log_prob_a'], model_output['log_prob_b']\n    \n    # Ensure inputs are in the correct order for the loss logic\n    # Loss is defined for cost_w < cost_l\n    is_flipped = cost_w > cost_l\n    if torch.any(is_flipped):\n        # For pairs where cost_a > cost_b, we flip them\n        # The loss logic remains the same, but the roles of 'a' and 'b' are swapped\n        temp_cost_w = torch.where(is_flipped, cost_l, cost_w)\n        cost_l = torch.where(is_flipped, cost_w, cost_l)\n        cost_w = temp_cost_w\n        \n        temp_logp_w = torch.where(is_flipped, logp_l, logp_w)\n        logp_l = torch.where(is_flipped, logp_w, logp_l)\n        logp_w = temp_logp_w\n\n    # 1. Calculate cost and log-probability differences\n    # cost is positive by convention (cost_l > cost_w)\n    delta_cost = cost_l - cost_w\n    # logp should be positive for correct preference\n    delta_logp = logp_w - logp_l\n\n    # 2. Normalize cost difference using rank_gap for a stable margin signal\n    # This is detached as cost differences are constants w.r.t. model parameters\n    with torch.no_grad():\n        # Add a small epsilon to avoid division by zero if all costs are identical\n        normalized_cost_gap = rank_gap(delta_cost + 1e-9)\n\n    # 3. Define the adaptive margin\n    margin = alpha * normalized_cost_gap\n\n    # 4. Calculate the core hinge-like loss\n    # tanh bounds the logp difference to [-1, 1], scaled by beta\n    # This makes the loss robust to extreme logit differences\n    saturated_logp_diff = torch.tanh(beta * delta_logp)\n    \n    # The loss is the amount by which the saturated preference fails to meet the margin\n    # F.relu is equivalent to max(0, x)\n    loss_per_item = F.relu(margin - saturated_logp_diff)\n\n    # 5. Apply optional sample weights\n    weights = batch.get('weight', None)\n    if weights is not None:\n        loss_per_item = loss_per_item * weights\n\n    # 6. Return the mean loss\n    return loss_per_item.mean()"}, "static_ok": true, "static_reason": "", "dynamic_ok": false, "dynamic_reason": "forward_error: 'log_prob_a'", "loss_value": null, "grad_norm": null}
{"generation": 1, "index": 4, "ir": {"name": "Adaptive Sigmoid Margin Loss", "intuition": "This loss function uses a dynamic, cost-sensitive margin to separate preferred and non-preferred solutions. The margin is not fixed but is a sigmoid function of the normalized cost difference. When the cost difference is small, the margin is also small, allowing the model to be uncertain. As the cost difference grows, the margin increases towards a maximum value, demanding a stronger preference signal from the model. This adaptive margin is combined with a standard logistic loss (logsigmoid) on the log-probability difference. The entire term is then scaled by a softplus function of the cost difference, which amplifies the loss for pairs with larger cost gaps, focusing the training on more significant preference signals while ensuring numerical stability and bounded gradients.", "pseudocode": "1. Calculate the cost difference: delta_cost = cost(b) - cost(a).\n2. Normalize the cost difference across the batch (e.g., z-score) to get norm_delta_cost. This makes the loss invariant to the scale of the costs.\n3. Compute an adaptive margin: margin = max_margin * sigmoid(norm_delta_cost). The margin is small for small cost gaps and approaches max_margin for large gaps.\n4. Calculate the log-probability difference: delta_logp = logp(a) - logp(b).\n5. Compute the core preference loss: core_loss = -logsigmoid(delta_logp - margin). This is a logistic loss that pushes delta_logp to be greater than the adaptive margin.\n6. Calculate a cost-based weight: weight_scale = softplus(beta * delta_cost). This amplifies the loss for pairs with larger cost differences, making the training focus on more impactful preferences.\n7. Combine them: final_loss = weight_scale * core_loss.\n8. Return the mean of final_loss over the batch.", "hyperparams": {"beta": 1.0, "max_margin": 2.0, "eps": 1e-08}, "operators_used": ["logsigmoid", "softplus", "sigmoid", "zscore"], "implementation_hint": {"expects": ["cost_a", "cost_b", "logp_a", "logp_b"], "returns": "scalar"}, "code": "import torch\nimport torch.nn.functional as F\n\ndef generated_loss(batch, model_output, extra):\n    \"\"\"\n    Adaptive Sigmoid Margin Loss.\n\n    Args:\n        batch (dict): A dictionary containing tensors from the data loader.\n                      Expected keys: 'cost_a', 'cost_b', 'log_prob_w', 'log_prob_l'.\n                      'cost_a' and 'log_prob_w' correspond to the preferred solution (winner).\n                      'cost_b' and 'log_prob_l' correspond to the non-preferred solution (loser).\n                      An optional 'weight' tensor can be used for sample weighting.\n        model_output (dict): Output from the model (not used in this loss structure).\n        extra (dict): A dictionary for hyperparameters, e.g., {'beta': 1.0, 'max_margin': 2.0, 'eps': 1e-8}.\n\n    Returns:\n        torch.Tensor: A scalar tensor representing the mean loss.\n    \"\"\"\n    # Read hyperparameters\n    beta = extra.get('beta', 1.0)\n    max_margin = extra.get('max_margin', 2.0)\n    eps = extra.get('eps', 1e-8)\n\n    # In our setting, 'a' is the winner (w) and 'b' is the loser (l).\n    # cost_a < cost_b\n    cost_a = batch['cost_a']\n    cost_b = batch['cost_b']\n    logp_a = batch['log_prob_w']\n    logp_b = batch['log_prob_l']\n\n    # 1. Calculate cost and log-probability differences\n    # delta_cost should be positive since cost_b > cost_a\n    delta_cost = cost_b - cost_a\n    delta_logp = logp_a - logp_b\n\n    # 2. Normalize the cost difference (z-score for batch-level adaptation)\n    # Ensure we only normalize if there's more than one element and some variance\n    if delta_cost.numel() > 1 and delta_cost.std() > eps:\n        mean = delta_cost.mean()\n        std = delta_cost.std()\n        norm_delta_cost = (delta_cost - mean) / (std + eps)\n    else:\n        norm_delta_cost = torch.zeros_like(delta_cost)\n\n    # 3. Compute the adaptive sigmoid margin\n    # The margin is a function of the normalized cost difference.\n    margin = max_margin * torch.sigmoid(norm_delta_cost)\n\n    # 4. Compute the core preference loss (Bradley-Terry style with margin)\n    # We want logp_a > logp_b + margin, so (logp_a - logp_b - margin) should be positive.\n    # The logsigmoid of a large positive number is close to 0.\n    core_loss = -F.logsigmoid(delta_logp - margin)\n\n    # 5. Compute a cost-based scaling factor using softplus for stability\n    # This amplifies the loss for pairs with larger raw cost differences.\n    weight_scale = F.softplus(beta * delta_cost)\n\n    # 6. Combine core loss and scaling factor\n    loss_per_sample = weight_scale * core_loss\n\n    # 7. Apply optional sample weights if provided\n    sample_weights = batch.get('weight', None)\n    if sample_weights is not None:\n        loss_per_sample = loss_per_sample * sample_weights\n\n    # 8. Return the mean loss over the batch\n    return loss_per_sample.mean()"}, "static_ok": true, "static_reason": "", "dynamic_ok": true, "dynamic_reason": "ok", "loss_value": 1.7246562242507935, "grad_norm": 0.0}
{"generation": 1, "index": 5, "ir": {"name": "Adaptive Margin Log-Ratio Loss", "intuition": "This loss function uses a log-ratio of sigmoid-transformed log-probabilities, which provides a bounded and symmetric measure of preference strength. It incorporates an adaptive margin that scales with the normalized rank gap of the costs. This makes the loss more sensitive to significant cost differences while being robust to outliers. The margin is applied within a softplus function to ensure it's always positive and smooth. The overall structure is inspired by contrastive learning, aiming to push the log-probability of the better solution further away from the worse one, with a target separation determined by the cost difference.", "pseudocode": "1. For each pair (a, b) with cost(a) < cost(b), calculate the cost difference `delta_cost = cost(b) - cost(a)`.\n2. Normalize `delta_cost` across the batch using the `rank_gap` operator to get a robust, scale-invariant measure of relative improvement, `normalized_margin`.\n3. Compute the log-probability difference `delta_logp = logp(a) - logp(b)`.\n4. Transform `delta_logp` into a preference score using `log(sigmoid(delta_logp)) - log(sigmoid(-delta_logp))`, which is equivalent to `delta_logp`. This is a placeholder for more complex ratio-based ideas but simplifies to the log-probability difference for stability and directness.\n5. The loss for the pair is `softplus(-delta_logp + alpha * normalized_margin)`. This aims to make `delta_logp` greater than the adaptive margin `alpha * normalized_margin`.\n6. Compute the weighted mean of this loss over the entire batch.", "hyperparams": {"alpha": 1.0, "beta": 1.0}, "operators_used": ["softplus", "rank_gap"], "implementation_hint": {"expects": ["cost_a", "cost_b", "logp_a", "logp_b"], "returns": "scalar"}, "code": "import torch\nimport torch.nn.functional as F\n\ndef rank_gap(x):\n    \"\"\"Numerically stable rank-based normalization.\n    Converts a tensor of values to their percentile ranks and then to a gap.\n    Assumes x >= 0. For x > 0, output is in [-1, 1]. For x=0, output is 0.\n    \"\"\"\n    ranks = torch.argsort(torch.argsort(x).float()).float()\n    # Normalize ranks to [0, 1]\n    normalized_ranks = ranks / (ranks.size(0) - 1 + 1e-8)\n    # Scale to [-1, 1] range\n    return 2 * normalized_ranks - 1\n\ndef generated_loss(batch, model_output, extra):\n    \"\"\"\n    Adaptive Margin Log-Ratio Loss.\n\n    Args:\n        batch (dict): A dictionary containing tensors from the data loader.\n                      Expected keys: 'cost_a', 'cost_b', 'log_prob_w', 'log_prob_l'.\n                      'log_prob_w' corresponds to the better solution (winner).\n                      'log_prob_l' corresponds to the worse solution (loser).\n        model_output: Not used in this loss formulation, as log-probs are in the batch.\n        extra (dict): A dictionary for hyperparameters, e.g., {'alpha': 1.0, 'beta': 1.0}.\n\n    Returns:\n        torch.Tensor: A scalar loss value.\n    \"\"\"\n    # Hyperparameters\n    alpha = extra.get('alpha', 1.0)\n\n    # In the provided setting, (a) is the winner and (b) is the loser.\n    # So, cost_a < cost_b.\n    cost_w, cost_l = batch['cost_a'], batch['cost_b']\n    logp_w, logp_l = batch['log_prob_w'], batch['log_prob_l']\n\n    # Ensure cost_w < cost_l for correct margin calculation\n    delta_cost = cost_l - cost_w\n    # Clamp to avoid issues with identical costs, though this should be rare.\n    delta_cost = torch.clamp(delta_cost, min=1e-8)\n\n    # 1. Calculate the adaptive margin based on the normalized cost difference.\n    # rank_gap provides a robust, non-parametric normalization of the cost gap.\n    # It maps the cost differences to a percentile-based scale in [-1, 1].\n    # Since delta_cost > 0, the output will be in [0, 1] if we use a variant\n    # or we can just use the standard one as its ordering is preserved.\n    # We use a simplified version here for directness.\n    with torch.no_grad():\n        # Using rank_gap to create a robust, scale-invariant margin.\n        # The output is in [-1, 1], but for delta_cost > 0, it's effectively [0, 1].\n        normalized_margin = rank_gap(delta_cost)\n        # Ensure margin is non-negative since cost_l > cost_w\n        normalized_margin = torch.clamp(normalized_margin, min=0.0)\n\n    # 2. Calculate the log-probability difference.\n    # This is the core signal from the model's preference.\n    delta_logp = logp_w - logp_l\n\n    # 3. Compute the hinge-like loss with the adaptive margin.\n    # The loss is softplus(margin - delta_logp), which encourages delta_logp > margin.\n    # softplus(x) = log(1 + exp(x)), a smooth version of ReLU.\n    # We want to minimize softplus(alpha * normalized_margin - delta_logp)\n    # which is equivalent to softplus(-(delta_logp - alpha * normalized_margin)).\n    loss_per_item = F.softplus(-delta_logp + alpha * normalized_margin)\n\n    # 4. Apply sample weights if provided.\n    weights = batch.get('weight', None)\n    if weights is not None:\n        loss = (loss_per_item * weights).mean()\n    else:\n        loss = loss_per_item.mean()\n\n    return loss"}, "static_ok": true, "static_reason": "", "dynamic_ok": false, "dynamic_reason": "forward_error: name 'rank_gap' is not defined", "loss_value": null, "grad_norm": null}
{"generation": 1, "index": 6, "ir": {"name": "Adaptive Margin Contrastive Loss", "intuition": "This loss treats preference learning as a contrastive task. It aims to pull the model's log-probability for the better solution (winner) above the log-probability for the worse solution (loser) by an adaptive margin. The margin is dynamically scaled by the normalized rank-gap of the cost difference, making the loss more aggressive for pairs with a large, meaningful quality gap and more lenient for pairs with similar costs. A softplus function ensures the loss is non-negative and only penalizes incorrect preferences (logp(winner) < logp(loser)), while a tanh squashing function on the log-probability difference ensures numerical stability and prevents gradients from exploding with large logit gaps.", "pseudocode": "1. For each pair (a, b), determine the winner (w) and loser (l) based on their costs.\n2. Calculate the cost difference: delta_cost = cost(l) - cost(w).\n3. Normalize the cost differences across the batch using a z-score or similar method to get normalized_delta_cost.\n4. Compute an adaptive margin, which is a scaled and clamped version of the normalized_delta_cost. This makes the target separation proportional to the significance of the cost gap.\n5. Calculate the log-probability difference: delta_logp = logp(w) - logp(l).\n6. Apply a tanh function to delta_logp to bound its influence and prevent extreme gradients.\n7. The core loss is softplus(margin - tanh(delta_logp)). This penalizes cases where delta_logp is smaller than the target margin.\n8. Apply optional instance weights and compute the mean loss over the batch.", "hyperparams": {"margin_scale": 1.0, "margin_clamp_min": 0.0, "margin_clamp_max": 5.0, "zscore_eps": 1e-08}, "operators_used": ["softplus", "tanh", "clamp", "zscore"], "implementation_hint": {"expects": ["cost_a", "cost_b", "logp_a", "logp_b"], "returns": "scalar"}, "code": "import torch\nimport torch.nn.functional as F\n\ndef zscore(x, eps=1e-8):\n    \"\"\"Calculates the z-score of a tensor, ensuring numerical stability.\"\"\"\n    if x.numel() <= 1:\n        return torch.zeros_like(x)\n    mean = x.mean()\n    std = x.std()\n    return (x - mean) / (std + eps)\n\ndef generated_loss(batch, model_output, hyperparams):\n    \"\"\"\n    Computes the Adaptive Margin Contrastive Loss.\n\n    Args:\n        batch (dict): A dictionary containing tensors from the data loader.\n                      Expected keys: 'cost_a', 'cost_b'.\n        model_output (dict): A dictionary containing tensors from the model.\n                             Expected keys: 'logp_a', 'logp_b'.\n        hyperparams (dict): A dictionary of hyperparameters for the loss function.\n                           Expected keys: 'margin_scale', 'margin_clamp_min',\n                           'margin_clamp_max', 'zscore_eps'.\n\n    Returns:\n        torch.Tensor: A scalar tensor representing the mean loss.\n    \"\"\"\n    cost_a = batch['cost_a']\n    cost_b = batch['cost_b']\n    logp_a = model_output['logp_a']\n    logp_b = model_output['logp_b']\n    weights = batch.get('weight', None)\n\n    # 1. Identify winner (w) and loser (l) for each pair\n    # Winner has lower cost\n    is_a_winner = (cost_a < cost_b).float()\n    \n    logp_w = is_a_winner * logp_a + (1 - is_a_winner) * logp_b\n    logp_l = (1 - is_a_winner) * logp_a + is_a_winner * logp_b\n\n    cost_w = is_a_winner * cost_a + (1 - is_a_winner) * cost_b\n    cost_l = (1 - is_a_winner) * cost_a + is_a_winner * cost_b\n\n    # 2. Calculate cost difference (always non-negative)\n    delta_cost = cost_l - cost_w\n\n    # 3. Normalize the cost difference to get a stable signal\n    # Using z-score for normalization across the batch\n    normalized_delta_cost = zscore(delta_cost, eps=hyperparams.get('zscore_eps', 1e-8))\n\n    # 4. Compute an adaptive margin based on the normalized cost gap\n    # The margin is larger for pairs with a more significant cost difference\n    margin = torch.clamp(\n        hyperparams.get('margin_scale', 1.0) * normalized_delta_cost,\n        min=hyperparams.get('margin_clamp_min', 0.0),\n        max=hyperparams.get('margin_clamp_max', 5.0)\n    )\n\n    # 5. Calculate the log-probability difference\n    delta_logp = logp_w - logp_l\n\n    # 6. Squash the log-probability difference with tanh for stability\n    # This bounds the input to the loss, preventing overflow/underflow\n    # and taming the gradients for extreme logit differences.\n    squashed_delta_logp = torch.tanh(delta_logp)\n\n    # 7. Compute the core contrastive loss using softplus for a smooth hinge-like effect\n    # The loss is softplus(margin - squashed_delta_logp).\n    # This penalizes cases where squashed_delta_logp < margin.\n    # It's zero when the model's preference (delta_logp) correctly exceeds the margin.\n    per_instance_loss = F.softplus(margin - squashed_delta_logp)\n\n    # 8. Apply optional instance weights\n    if weights is not None:\n        per_instance_loss = per_instance_loss * weights\n\n    # Return the mean loss over the batch\n    return per_instance_loss.mean()"}, "static_ok": true, "static_reason": "", "dynamic_ok": false, "dynamic_reason": "forward_error: 'logp_a'", "loss_value": null, "grad_norm": null}
{"generation": 1, "index": 7, "ir": {"name": "Adaptive Margin Hinge Loss with Rank-Gap Scaling", "intuition": "This loss combines a hinge-loss structure with an adaptive, cost-dependent margin. It aims to enforce that the log-probability of the better solution (`logp_w`) exceeds that of the worse one (`logp_l`) by at least a certain margin. The margin itself is scaled by a non-linear function of the normalized cost difference (rank gap), making the learning signal stronger for pairs with a large, meaningful cost difference, while being more lenient for pairs that are close in quality. A `tanh` function is used to create a smooth, bounded 'reward' for correct preferences, preventing excessively large negative losses for easy pairs and ensuring stability.", "pseudocode": "1. Calculate the cost difference `delta_cost = cost_l - cost_w` and the log-probability difference `delta_logp = logp_l - logp_w`.\n2. Normalize the `delta_cost` across the batch to get a rank-gap score, e.g., using z-scoring. Clamp to prevent extreme values.\n3. Compute an adaptive margin `m = alpha * rank_gap_score`, where `alpha` is a hyperparameter. The margin is now larger for pairs with a significant cost difference.\n4. Calculate the hinge term `hinge = relu(delta_logp + m)`. This term is positive (a penalty) only if the model's preference is incorrect (`logp_l > logp_w`) or correct but by an insufficient margin.\n5. Calculate a reward term `reward = beta * tanh(relu(-delta_logp))`. This term provides a bounded negative loss (a reward) when the model's preference is correct, encouraging it to further separate the log-probabilities. The `relu` ensures reward is only given for correct preferences.\n6. Combine the hinge penalty and the reward: `loss = hinge - reward`.\n7. If batch weights are provided, apply them. Return the mean loss.", "hyperparams": {"alpha": 1.0, "beta": 0.5, "z_clamp": 3.0}, "operators_used": ["relu", "tanh", "zscore", "clamp"], "implementation_hint": {"expects": ["cost_a", "cost_b", "logp_a", "logp_b"], "returns": "scalar"}, "code": "import torch\nimport torch.nn.functional as F\n\ndef zscore(x, eps=1e-8):\n    \"\"\"Custom z-score implementation for whitelisting.\"\"\"\n    if x.numel() <= 1:\n        return torch.zeros_like(x)\n    mean = x.mean()\n    std = x.std()\n    return (x - mean) / (std + eps)\n\ndef generated_loss(batch, model_output, extra):\n    \"\"\" \n    Adaptive Margin Hinge Loss with Rank-Gap Scaling.\n    batch: A dictionary containing tensors.\n        - cost_a, cost_b: Costs of the two solutions in a pair (lower is better).\n        - log_prob_w, log_prob_l: Model's log probabilities for the winner and loser solutions.\n        - weight: Optional weights for each pair.\n    model_output: Not used in this loss formulation, but part of the required signature.\n    extra: A dictionary that may contain hyperparameters.\n    \"\"\"\n    # Read hyperparameters, with defaults\n    alpha = extra.get('alpha', 1.0)\n    beta = extra.get('beta', 0.5)\n    z_clamp = extra.get('z_clamp', 3.0)\n\n    # Unpack data from the batch dictionary\n    cost_w = batch['cost_a'] # Assuming a is the winner, b is the loser\n    cost_l = batch['cost_b']\n    logp_w = batch['log_prob_w']\n    logp_l = batch['log_prob_l']\n    weights = batch.get('weight', None)\n\n    # Ensure inputs are tensors\n    cost_w = torch.as_tensor(cost_w, device=logp_w.device)\n    cost_l = torch.as_tensor(cost_l, device=logp_l.device)\n\n    # 1. Calculate cost and log-probability differences\n    # delta_cost should be positive since cost_l > cost_w\n    delta_cost = cost_l - cost_w\n    # delta_logp should be negative for a correct model preference (logp_w > logp_l)\n    delta_logp = logp_l - logp_w\n\n    # 2. Normalize cost difference to get a stable rank-gap signal\n    # Using z-score for normalization, followed by clamping for stability\n    with torch.no_grad():\n        rank_gap_score = zscore(delta_cost)\n        rank_gap_score = torch.clamp(rank_gap_score, min=-z_clamp, max=z_clamp)\n\n    # 3. Compute an adaptive margin based on the rank gap\n    # The margin is larger for pairs with a more significant cost difference\n    margin = alpha * rank_gap_score\n\n    # 4. Calculate the hinge term (penalty for incorrect/insufficient preference)\n    # This is positive only if logp_l > logp_w - margin\n    hinge_penalty = F.relu(delta_logp + margin)\n\n    # 5. Calculate a bounded reward for correct preferences\n    # We want to reward when delta_logp is negative. relu(-delta_logp) captures this.\n    # tanh provides a smooth, bounded reward signal, preventing extreme negative loss values.\n    correct_preference_strength = F.relu(-delta_logp)\n    reward = beta * torch.tanh(correct_preference_strength)\n\n    # 6. Combine penalty and reward\n    # The loss is the penalty for being wrong, minus the reward for being right.\n    pair_loss = hinge_penalty - reward\n\n    # 7. Apply optional weights and compute the mean\n    if weights is not None:\n        pair_loss = pair_loss * weights\n    \n    return pair_loss.mean()"}, "static_ok": true, "static_reason": "", "dynamic_ok": false, "dynamic_reason": "forward_error: name 'zscore' is not defined", "loss_value": null, "grad_norm": null}
{"generation": 2, "index": 0, "ir": {"name": "Adaptive Sigmoid Margin Loss with Tanh Scaling", "intuition": "This loss function modifies the parent by replacing the unbounded `softplus` scaling with a bounded `tanh` scaling. The core idea remains the same: use a dynamic, cost-sensitive margin based on a sigmoid function of the normalized cost difference. This adaptive margin is used in a standard logistic loss. However, instead of amplifying the loss indefinitely with `softplus(beta * delta_cost)`, this version uses `tanh(beta * delta_cost)` to scale the loss. This bounds the influence of any single data point, especially outliers with extremely large cost differences, which can prevent gradient explosion and lead to more stable training. The loss is still amplified for larger cost gaps, but this amplification saturates, making the training process more robust.", "pseudocode": "1. Calculate the cost difference: delta_cost = cost(b) - cost(a).\n2. Normalize the cost difference across the batch (e.g., z-score) to get norm_delta_cost. This makes the loss invariant to the scale of the costs.\n3. Compute an adaptive margin: margin = max_margin * sigmoid(norm_delta_cost). The margin is small for small cost gaps and approaches max_margin for large gaps.\n4. Calculate the log-probability difference: delta_logp = logp(a) - logp(b).\n5. Compute the core preference loss: core_loss = -logsigmoid(delta_logp - margin). This is a logistic loss that pushes delta_logp to be greater than the adaptive margin.\n6. Calculate a cost-based weight: weight_scale = tanh(beta * delta_cost). This amplifies the loss for pairs with larger cost differences but bounds the scaling factor between 0 and 1, preventing instability from outliers.\n7. Combine them: final_loss = weight_scale * core_loss.\n8. Return the mean of final_loss over the batch.", "hyperparams": {"beta": 1.0, "max_margin": 2.0, "eps": 1e-08}, "operators_used": ["logsigmoid", "tanh", "sigmoid", "zscore"], "implementation_hint": {"expects": ["cost_a", "cost_b", "logp_a", "logp_b"], "returns": "scalar"}, "code": "import torch\nimport torch.nn.functional as F\n\ndef generated_loss(batch, model_output, extra):\n    \"\"\"\n    Adaptive Sigmoid Margin Loss with Tanh Scaling.\n\n    Args:\n        batch (dict): A dictionary containing tensors from the data loader.\n                      Expected keys: 'cost_a', 'cost_b', 'log_prob_w', 'log_prob_l'.\n                      'cost_a' and 'log_prob_w' correspond to the preferred solution (winner).\n                      'cost_b' and 'log_prob_l' correspond to the non-preferred solution (loser).\n                      An optional 'weight' tensor can be used for sample weighting.\n        model_output (dict): Output from the model (not used in this loss structure).\n        extra (dict): A dictionary for hyperparameters, e.g., {'beta': 1.0, 'max_margin': 2.0, 'eps': 1e-8}.\n\n    Returns:\n        torch.Tensor: A scalar tensor representing the mean loss.\n    \"\"\"\n    # Read hyperparameters\n    beta = extra.get('beta', 1.0)\n    max_margin = extra.get('max_margin', 2.0)\n    eps = extra.get('eps', 1e-8)\n\n    # In our setting, 'a' is the winner (w) and 'b' is the loser (l).\n    # cost_a < cost_b\n    cost_a = batch['cost_a']\n    cost_b = batch['cost_b']\n    logp_a = batch['log_prob_w']\n    logp_b = batch['log_prob_l']\n\n    # 1. Calculate cost and log-probability differences\n    # delta_cost should be positive since cost_b > cost_a\n    delta_cost = cost_b - cost_a\n    delta_logp = logp_a - logp_b\n\n    # 2. Normalize the cost difference (z-score for batch-level adaptation)\n    # Ensure we only normalize if there's more than one element and some variance\n    if delta_cost.numel() > 1 and delta_cost.std() > eps:\n        mean = delta_cost.mean()\n        std = delta_cost.std()\n        norm_delta_cost = (delta_cost - mean) / (std + eps)\n    else:\n        norm_delta_cost = torch.zeros_like(delta_cost)\n\n    # 3. Compute the adaptive sigmoid margin\n    # The margin is a function of the normalized cost difference.\n    margin = max_margin * torch.sigmoid(norm_delta_cost)\n\n    # 4. Compute the core preference loss (Bradley-Terry style with margin)\n    # We want logp_a > logp_b + margin, so (logp_a - logp_b - margin) should be positive.\n    # The logsigmoid of a large positive number is close to 0.\n    core_loss = -F.logsigmoid(delta_logp - margin)\n\n    # 5. Compute a cost-based scaling factor using tanh for bounded scaling\n    # This amplifies the loss for pairs with larger raw cost differences but bounds the effect.\n    weight_scale = torch.tanh(beta * delta_cost)\n\n    # 6. Combine core loss and scaling factor\n    loss_per_sample = weight_scale * core_loss\n\n    # 7. Apply optional sample weights if provided\n    sample_weights = batch.get('weight', None)\n    if sample_weights is not None:\n        loss_per_sample = loss_per_sample * sample_weights\n\n    # 8. Return the mean loss over the batch\n    return loss_per_sample.mean()"}, "static_ok": true, "static_reason": "", "dynamic_ok": true, "dynamic_reason": "ok", "loss_value": 1.00017249584198, "grad_norm": 0.0}
{"generation": 2, "index": 1, "ir": {"name": "Adaptive Sigmoid Margin Loss with Rank-Gap Normalization", "intuition": "This loss function uses a dynamic, cost-sensitive margin to separate preferred and non-preferred solutions. The margin is not fixed but is a sigmoid function of the rank-normalized cost difference. Using rank-gap normalization makes the margin robust to outliers in the cost distribution within a batch. When the cost difference is small relative to others in the batch, the margin is also small, allowing the model to be uncertain. As the cost difference becomes larger, the margin increases towards a maximum value, demanding a stronger preference signal. This adaptive margin is combined with a standard logistic loss (logsigmoid) on the log-probability difference. The entire term is then scaled by a softplus function of the cost difference, which amplifies the loss for pairs with larger cost gaps, focusing training on more significant preference signals.", "pseudocode": "1. Calculate the cost difference: delta_cost = cost(b) - cost(a).\n2. Normalize the cost difference across the batch using rank-gap normalization to get norm_delta_cost. This maps cost differences to a stable [-1, 1] range, making it robust to outliers.\n3. Compute an adaptive margin: margin = max_margin * sigmoid(norm_delta_cost). The margin is small for small cost gaps and approaches max_margin for large gaps.\n4. Calculate the log-probability difference: delta_logp = logp(a) - logp(b).\n5. Compute the core preference loss: core_loss = -logsigmoid(delta_logp - margin). This is a logistic loss that pushes delta_logp to be greater than the adaptive margin.\n6. Calculate a cost-based weight: weight_scale = softplus(beta * delta_cost). This amplifies the loss for pairs with larger cost differences, making the training focus on more impactful preferences.\n7. Combine them: final_loss = weight_scale * core_loss.\n8. Return the mean of final_loss over the batch.", "hyperparams": {"beta": 1.0, "max_margin": 2.0, "eps": 1e-08}, "operators_used": ["logsigmoid", "softplus", "sigmoid", "rank_gap"], "implementation_hint": {"expects": ["cost_a", "cost_b", "logp_a", "logp_b"], "returns": "scalar"}, "code": "import torch\nimport torch.nn.functional as F\n\ndef rank_gap(x, eps=1e-8):\n    \"\"\"Rank-based normalization that maps values to the range [-1, 1].\"\"\"\n    if x.numel() <= 1:\n        return torch.zeros_like(x)\n    ranks = x.argsort().argsort().float()\n    # Shift ranks to be centered around 0\n    ranks = ranks - (x.numel() - 1) / 2.0\n    # Scale to [-1, 1] range\n    ranks = ranks / (x.numel() / 2.0 + eps)\n    return ranks\n\ndef generated_loss(batch, model_output, extra):\n    \"\"\"\n    Adaptive Sigmoid Margin Loss with Rank-Gap Normalization.\n\n    Args:\n        batch (dict): A dictionary containing tensors from the data loader.\n                      Expected keys: 'cost_a', 'cost_b', 'log_prob_w', 'log_prob_l'.\n                      'cost_a' and 'log_prob_w' correspond to the preferred solution (winner).\n                      'cost_b' and 'log_prob_l' correspond to the non-preferred solution (loser).\n                      An optional 'weight' tensor can be used for sample weighting.\n        model_output (dict): Output from the model (not used in this loss structure).\n        extra (dict): A dictionary for hyperparameters, e.g., {'beta': 1.0, 'max_margin': 2.0, 'eps': 1e-8}.\n\n    Returns:\n        torch.Tensor: A scalar tensor representing the mean loss.\n    \"\"\"\n    # Read hyperparameters\n    beta = extra.get('beta', 1.0)\n    max_margin = extra.get('max_margin', 2.0)\n    eps = extra.get('eps', 1e-8)\n\n    # In our setting, 'a' is the winner (w) and 'b' is the loser (l).\n    # cost_a < cost_b\n    cost_a = batch['cost_a']\n    cost_b = batch['cost_b']\n    logp_a = batch['log_prob_w']\n    logp_b = batch['log_prob_l']\n\n    # 1. Calculate cost and log-probability differences\n    # delta_cost should be positive since cost_b > cost_a\n    delta_cost = cost_b - cost_a\n    delta_logp = logp_a - logp_b\n\n    # 2. Normalize the cost difference using rank-gap normalization for outlier robustness\n    norm_delta_cost = rank_gap(delta_cost, eps=eps)\n\n    # 3. Compute the adaptive sigmoid margin\n    # The margin is a function of the rank-normalized cost difference.\n    margin = max_margin * torch.sigmoid(norm_delta_cost)\n\n    # 4. Compute the core preference loss (Bradley-Terry style with margin)\n    # We want logp_a > logp_b + margin, so (logp_a - logp_b - margin) should be positive.\n    # The logsigmoid of a large positive number is close to 0.\n    core_loss = -F.logsigmoid(delta_logp - margin)\n\n    # 5. Compute a cost-based scaling factor using softplus for stability\n    # This amplifies the loss for pairs with larger raw cost differences.\n    weight_scale = F.softplus(beta * delta_cost)\n\n    # 6. Combine core loss and scaling factor\n    loss_per_sample = weight_scale * core_loss\n\n    # 7. Apply optional sample weights if provided\n    sample_weights = batch.get('weight', None)\n    if sample_weights is not None:\n        loss_per_sample = loss_per_sample * sample_weights\n\n    # 8. Return the mean loss over the batch\n    return loss_per_sample.mean()"}, "static_ok": true, "static_reason": "", "dynamic_ok": false, "dynamic_reason": "forward_error: name 'rank_gap' is not defined", "loss_value": null, "grad_norm": null}
{"generation": 2, "index": 2, "ir": {"name": "Adaptive Tanh Margin Loss", "intuition": "This loss function refines the adaptive margin concept by using the `tanh` function instead of `sigmoid`. The margin, which separates preferred and non-preferred solutions, is now proportional to `tanh` of the normalized cost difference. The `tanh` function is symmetric around zero and has a steeper slope, which might make the margin more responsive to small cost differences around the mean. The loss still uses a standard logistic objective (`logsigmoid`) and is scaled by a `softplus` function of the cost difference, preserving the parent's core structure of focusing on more significant preferences while ensuring stability. The switch to `tanh` offers a different margin dynamic, potentially improving learning for pairs with small to medium cost gaps.", "pseudocode": "1. Calculate the cost difference: delta_cost = cost(b) - cost(a).\n2. Normalize the cost difference across the batch (e.g., z-score) to get norm_delta_cost. This makes the loss invariant to the scale of the costs.\n3. Compute an adaptive margin using tanh: margin = max_margin * tanh(norm_delta_cost). The margin is small for small cost gaps and approaches max_margin for large gaps, with a steeper response around the mean compared to sigmoid.\n4. Calculate the log-probability difference: delta_logp = logp(a) - logp(b).\n5. Compute the core preference loss: core_loss = -logsigmoid(delta_logp - margin). This is a logistic loss that pushes delta_logp to be greater than the adaptive margin.\n6. Calculate a cost-based weight: weight_scale = softplus(beta * delta_cost). This amplifies the loss for pairs with larger cost differences, focusing training on more impactful preferences.\n7. Combine them: final_loss = weight_scale * core_loss.\n8. Return the mean of final_loss over the batch.", "hyperparams": {"beta": 1.0, "max_margin": 1.0, "eps": 1e-08}, "operators_used": ["logsigmoid", "softplus", "tanh", "zscore"], "implementation_hint": {"expects": ["cost_a", "cost_b", "logp_a", "logp_b"], "returns": "scalar"}, "code": "import torch\nimport torch.nn.functional as F\n\ndef generated_loss(batch, model_output, extra):\n    \"\"\"\n    Adaptive Tanh Margin Loss.\n\n    Args:\n        batch (dict): A dictionary containing tensors from the data loader.\n                      Expected keys: 'cost_a', 'cost_b', 'log_prob_w', 'log_prob_l'.\n                      'cost_a' and 'log_prob_w' correspond to the preferred solution (winner).\n                      'cost_b' and 'log_prob_l' correspond to the non-preferred solution (loser).\n                      An optional 'weight' tensor can be used for sample weighting.\n        model_output (dict): Output from the model (not used in this loss structure).\n        extra (dict): A dictionary for hyperparameters, e.g., {'beta': 1.0, 'max_margin': 1.0, 'eps': 1e-8}.\n\n    Returns:\n        torch.Tensor: A scalar tensor representing the mean loss.\n    \"\"\"\n    # Read hyperparameters\n    beta = extra.get('beta', 1.0)\n    max_margin = extra.get('max_margin', 1.0)\n    eps = extra.get('eps', 1e-8)\n\n    # In our setting, 'a' is the winner (w) and 'b' is the loser (l).\n    # cost_a < cost_b\n    cost_a = batch['cost_a']\n    cost_b = batch['cost_b']\n    logp_a = batch['log_prob_w']\n    logp_b = batch['log_prob_l']\n\n    # 1. Calculate cost and log-probability differences\n    # delta_cost should be positive since cost_b > cost_a\n    delta_cost = cost_b - cost_a\n    delta_logp = logp_a - logp_b\n\n    # 2. Normalize the cost difference (z-score for batch-level adaptation)\n    # Ensure we only normalize if there's more than one element and some variance\n    if delta_cost.numel() > 1 and delta_cost.std() > eps:\n        mean = delta_cost.mean()\n        std = delta_cost.std()\n        norm_delta_cost = (delta_cost - mean) / (std + eps)\n    else:\n        norm_delta_cost = torch.zeros_like(delta_cost)\n\n    # 3. Compute the adaptive tanh margin\n    # The margin is a function of the normalized cost difference using tanh.\n    margin = max_margin * torch.tanh(norm_delta_cost)\n\n    # 4. Compute the core preference loss (Bradley-Terry style with margin)\n    # We want logp_a > logp_b + margin, so (logp_a - logp_b - margin) should be positive.\n    # The logsigmoid of a large positive number is close to 0.\n    core_loss = -F.logsigmoid(delta_logp - margin)\n\n    # 5. Compute a cost-based scaling factor using softplus for stability\n    # This amplifies the loss for pairs with larger raw cost differences.\n    weight_scale = F.softplus(beta * delta_cost)\n\n    # 6. Combine core loss and scaling factor\n    loss_per_sample = weight_scale * core_loss\n\n    # 7. Apply optional sample weights if provided\n    sample_weights = batch.get('weight', None)\n    if sample_weights is not None:\n        loss_per_sample = loss_per_sample * sample_weights\n\n    # 8. Return the mean loss over the batch\n    return loss_per_sample.mean()"}, "static_ok": true, "static_reason": "", "dynamic_ok": true, "dynamic_reason": "ok", "loss_value": 0.910283625125885, "grad_norm": 0.0}
{"generation": 2, "index": 3, "ir": {"name": "Adaptive Sigmoid Margin Loss with Rank-Gap Normalization", "intuition": "This loss function uses a dynamic, cost-sensitive margin to separate preferred and non-preferred solutions. The margin is a sigmoid function of the cost difference, but instead of using z-score normalization, it uses rank-gap normalization. This non-parametric approach is more robust to outliers in cost differences and provides a stable, bounded scale from -1 to 1 for the margin calculation. A larger cost difference results in a larger rank-gap, pushing the margin towards its maximum value and demanding a stronger preference signal. This adaptive margin is combined with a standard logistic loss. The entire term is then scaled by a softplus function of the cost difference, amplifying the loss for pairs with larger cost gaps.", "pseudocode": "1. Calculate the cost difference: delta_cost = cost(b) - cost(a).\n2. Normalize the cost difference using rank-gap normalization to get norm_delta_cost. This maps the cost differences to a robust [-1, 1] range.\n3. Compute an adaptive margin: margin = max_margin * sigmoid(norm_delta_cost). The margin is small for small cost gaps and approaches max_margin for large gaps.\n4. Calculate the log-probability difference: delta_logp = logp(a) - logp(b).\n5. Compute the core preference loss: core_loss = -logsigmoid(delta_logp - margin). This is a logistic loss that pushes delta_logp to be greater than the adaptive margin.\n6. Calculate a cost-based weight: weight_scale = softplus(beta * delta_cost). This amplifies the loss for pairs with larger cost differences.\n7. Combine them: final_loss = weight_scale * core_loss.\n8. Return the mean of final_loss over the batch.", "hyperparams": {"beta": 1.0, "max_margin": 2.0}, "operators_used": ["logsigmoid", "softplus", "sigmoid", "rank_gap"], "implementation_hint": {"expects": ["cost_a", "cost_b", "logp_a", "logp_b"], "returns": "scalar"}, "code": "import torch\nimport torch.nn.functional as F\n\ndef rank_gap(x):\n    \"\"\"Non-parametric normalization based on rank, mapping to [-1, 1].\"\"\"\n    if x.numel() <= 1:\n        return torch.zeros_like(x)\n    # Get the rank of each element\n    # The permutation `p` gives the indices that would sort `x`.\n    # `p.argsort()` gives the rank of each element in the original `x`.\n    ranks = x.argsort().argsort().float()\n    # Scale ranks to [0, 1]\n    normalized_ranks = ranks / (ranks.size(0) - 1)\n    # Scale to [-1, 1]\n    return 2 * normalized_ranks - 1\n\ndef generated_loss(batch, model_output, extra):\n    \"\"\"\n    Adaptive Sigmoid Margin Loss with Rank-Gap Normalization.\n\n    Args:\n        batch (dict): A dictionary containing tensors from the data loader.\n                      Expected keys: 'cost_a', 'cost_b', 'log_prob_w', 'log_prob_l'.\n                      'cost_a' and 'log_prob_w' correspond to the preferred solution (winner).\n                      'cost_b' and 'log_prob_l' correspond to the non-preferred solution (loser).\n                      An optional 'weight' tensor can be used for sample weighting.\n        model_output (dict): Output from the model (not used in this loss structure).\n        extra (dict): A dictionary for hyperparameters, e.g., {'beta': 1.0, 'max_margin': 2.0}.\n\n    Returns:\n        torch.Tensor: A scalar tensor representing the mean loss.\n    \"\"\"\n    # Read hyperparameters\n    beta = extra.get('beta', 1.0)\n    max_margin = extra.get('max_margin', 2.0)\n\n    # In our setting, 'a' is the winner (w) and 'b' is the loser (l).\n    # cost_a < cost_b\n    cost_a = batch['cost_a']\n    cost_b = batch['cost_b']\n    logp_a = batch['log_prob_w']\n    logp_b = batch['log_prob_l']\n\n    # 1. Calculate cost and log-probability differences\n    # delta_cost should be positive since cost_b > cost_a\n    delta_cost = cost_b - cost_a\n    delta_logp = logp_a - logp_b\n\n    # 2. Normalize the cost difference using rank-gap normalization\n    # This is more robust to outliers than z-scoring.\n    norm_delta_cost = rank_gap(delta_cost)\n\n    # 3. Compute the adaptive sigmoid margin\n    # The margin is a function of the normalized cost difference.\n    margin = max_margin * torch.sigmoid(norm_delta_cost)\n\n    # 4. Compute the core preference loss (Bradley-Terry style with margin)\n    # We want logp_a > logp_b + margin, so (logp_a - logp_b - margin) should be positive.\n    # The logsigmoid of a large positive number is close to 0.\n    core_loss = -F.logsigmoid(delta_logp - margin)\n\n    # 5. Compute a cost-based scaling factor using softplus for stability\n    # This amplifies the loss for pairs with larger raw cost differences.\n    weight_scale = F.softplus(beta * delta_cost)\n\n    # 6. Combine core loss and scaling factor\n    loss_per_sample = weight_scale * core_loss\n\n    # 7. Apply optional sample weights if provided\n    sample_weights = batch.get('weight', None)\n    if sample_weights is not None:\n        loss_per_sample = loss_per_sample * sample_weights\n\n    # 8. Return the mean loss over the batch\n    return loss_per_sample.mean()"}, "static_ok": true, "static_reason": "", "dynamic_ok": false, "dynamic_reason": "forward_error: name 'rank_gap' is not defined", "loss_value": null, "grad_norm": null}
{"generation": 2, "index": 4, "ir": {"name": "Adaptive Sigmoid Margin Loss with Tanh", "intuition": "This loss function uses a dynamic, cost-sensitive margin to separate preferred and non-preferred solutions. The margin is not fixed but is a tanh function of the normalized cost difference. When the cost difference is small, the margin is also small, allowing the model to be uncertain. As the cost difference grows, the margin increases towards a maximum value, demanding a stronger preference signal from the model. Using tanh instead of sigmoid provides a symmetric margin around zero for the normalized cost difference. This adaptive margin is combined with a standard logistic loss (logsigmoid) on the log-probability difference. The entire term is then scaled by a softplus function of the cost difference, which amplifies the loss for pairs with larger cost gaps, focusing the training on more significant preference signals while ensuring numerical stability and bounded gradients.", "pseudocode": "1. Calculate the cost difference: delta_cost = cost(b) - cost(a).\n2. Normalize the cost difference across the batch (e.g., z-score) to get norm_delta_cost. This makes the loss invariant to the scale of the costs.\n3. Compute an adaptive margin: margin = max_margin * tanh(norm_delta_cost). The margin is small for small cost gaps and approaches max_margin for large positive gaps.\n4. Calculate the log-probability difference: delta_logp = logp(a) - logp(b).\n5. Compute the core preference loss: core_loss = -logsigmoid(delta_logp - margin). This is a logistic loss that pushes delta_logp to be greater than the adaptive margin.\n6. Calculate a cost-based weight: weight_scale = softplus(beta * delta_cost). This amplifies the loss for pairs with larger cost differences, making the training focus on more impactful preferences.\n7. Combine them: final_loss = weight_scale * core_loss.\n8. Return the mean of final_loss over the batch.", "hyperparams": {"beta": 1.0, "max_margin": 1.5, "eps": 1e-08}, "operators_used": ["logsigmoid", "softplus", "tanh", "zscore"], "implementation_hint": {"expects": ["cost_a", "cost_b", "logp_a", "logp_b"], "returns": "scalar"}, "code": "import torch\nimport torch.nn.functional as F\n\ndef generated_loss(batch, model_output, extra):\n    \"\"\"\n    Adaptive Sigmoid Margin Loss with Tanh.\n\n    Args:\n        batch (dict): A dictionary containing tensors from the data loader.\n                      Expected keys: 'cost_a', 'cost_b', 'log_prob_w', 'log_prob_l'.\n                      'cost_a' and 'log_prob_w' correspond to the preferred solution (winner).\n                      'cost_b' and 'log_prob_l' correspond to the non-preferred solution (loser).\n                      An optional 'weight' tensor can be used for sample weighting.\n        model_output (dict): Output from the model (not used in this loss structure).\n        extra (dict): A dictionary for hyperparameters, e.g., {'beta': 1.0, 'max_margin': 1.5, 'eps': 1e-8}.\n\n    Returns:\n        torch.Tensor: A scalar tensor representing the mean loss.\n    \"\"\"\n    # Read hyperparameters\n    beta = extra.get('beta', 1.0)\n    max_margin = extra.get('max_margin', 1.5)\n    eps = extra.get('eps', 1e-8)\n\n    # In our setting, 'a' is the winner (w) and 'b' is the loser (l).\n    # cost_a < cost_b\n    cost_a = batch['cost_a']\n    cost_b = batch['cost_b']\n    logp_a = batch['log_prob_w']\n    logp_b = batch['log_prob_l']\n\n    # 1. Calculate cost and log-probability differences\n    # delta_cost should be positive since cost_b > cost_a\n    delta_cost = cost_b - cost_a\n    delta_logp = logp_a - logp_b\n\n    # 2. Normalize the cost difference (z-score for batch-level adaptation)\n    # Ensure we only normalize if there's more than one element and some variance\n    if delta_cost.numel() > 1 and delta_cost.std() > eps:\n        mean = delta_cost.mean()\n        std = delta_cost.std()\n        norm_delta_cost = (delta_cost - mean) / (std + eps)\n    else:\n        norm_delta_cost = torch.zeros_like(delta_cost)\n\n    # 3. Compute the adaptive tanh margin\n    # The margin is a function of the normalized cost difference.\n    margin = max_margin * torch.tanh(norm_delta_cost)\n\n    # 4. Compute the core preference loss (Bradley-Terry style with margin)\n    # We want logp_a > logp_b + margin, so (logp_a - logp_b - margin) should be positive.\n    # The logsigmoid of a large positive number is close to 0.\n    core_loss = -F.logsigmoid(delta_logp - margin)\n\n    # 5. Compute a cost-based scaling factor using softplus for stability\n    # This amplifies the loss for pairs with larger raw cost differences.\n    weight_scale = F.softplus(beta * delta_cost)\n\n    # 6. Combine core loss and scaling factor\n    loss_per_sample = weight_scale * core_loss\n\n    # 7. Apply optional sample weights if provided\n    sample_weights = batch.get('weight', None)\n    if sample_weights is not None:\n        loss_per_sample = loss_per_sample * sample_weights\n\n    # 8. Return the mean loss over the batch\n    return loss_per_sample.mean()"}, "static_ok": true, "static_reason": "", "dynamic_ok": true, "dynamic_reason": "ok", "loss_value": 0.910283625125885, "grad_norm": 0.0}
{"generation": 2, "index": 5, "ir": {"name": "Adaptive Margin Loss with Rank-Gap Normalization", "intuition": "This loss function refines the adaptive margin concept by replacing batch-level z-score normalization with a more robust rank-based normalization (rank_gap). The margin, which separates preferred and non-preferred solutions, is now a sigmoid function of the rank-normalized cost difference. This makes the margin less sensitive to outlier cost values and more dependent on the relative ordering of cost differences within the batch. The core loss remains a logistic objective, pushing the log-probability difference to exceed this rank-aware margin. The loss is further scaled by a softplus of the raw cost difference, ensuring that pairs with larger cost separations contribute more significantly to the training objective.", "pseudocode": "1. Calculate the cost difference: delta_cost = cost(b) - cost(a).\n2. Normalize the cost difference using rank-gap normalization: rank_norm_delta_cost = rank_gap(delta_cost). This maps the cost differences to a stable range based on their rank in the batch.\n3. Compute an adaptive margin: margin = max_margin * sigmoid(rank_norm_delta_cost). The margin is determined by the relative ranking of the cost gap.\n4. Calculate the log-probability difference: delta_logp = logp(a) - logp(b).\n5. Compute the core preference loss: core_loss = -logsigmoid(delta_logp - margin). This is a logistic loss that pushes delta_logp to be greater than the adaptive margin.\n6. Calculate a cost-based weight: weight_scale = softplus(beta * delta_cost). This amplifies the loss for pairs with larger cost differences.\n7. Combine them: final_loss = weight_scale * core_loss.\n8. Return the mean of final_loss over the batch.", "hyperparams": {"beta": 1.0, "max_margin": 2.0, "eps": 1e-08}, "operators_used": ["logsigmoid", "softplus", "sigmoid", "rank_gap"], "implementation_hint": {"expects": ["cost_a", "cost_b", "logp_a", "logp_b"], "returns": "scalar"}, "code": "import torch\nimport torch.nn.functional as F\n\ndef rank_gap(x, eps=1e-8):\n    \"\"\"Computes the rank-gap normalization of a 1D tensor.\"\"\"\n    if x.numel() <= 1:\n        return torch.zeros_like(x)\n    # Get ranks (1-based)\n    ranks = x.argsort().argsort().float() + 1.0\n    # Scale ranks to [0, 1]\n    scaled_ranks = (ranks - 1.0) / (x.numel() - 1.0)\n    # Apply inverse normal CDF (probit function) for spread\n    # Clamp input to avoid +/- inf from ppf\n    # The range (-1+eps, 1-eps) is mapped to a finite range by the inverse CDF\n    return torch.special.ndtri(scaled_ranks.clamp(eps, 1.0 - eps))\n\ndef generated_loss(batch, model_output, extra):\n    \"\"\"\n    Adaptive Margin Loss with Rank-Gap Normalization.\n\n    Args:\n        batch (dict): A dictionary containing tensors from the data loader.\n                      Expected keys: 'cost_a', 'cost_b', 'log_prob_w', 'log_prob_l'.\n                      'cost_a' and 'log_prob_w' correspond to the preferred solution (winner).\n                      'cost_b' and 'log_prob_l' correspond to the non-preferred solution (loser).\n                      An optional 'weight' tensor can be used for sample weighting.\n        model_output (dict): Output from the model (not used in this loss structure).\n        extra (dict): A dictionary for hyperparameters, e.g., {'beta': 1.0, 'max_margin': 2.0, 'eps': 1e-8}.\n\n    Returns:\n        torch.Tensor: A scalar tensor representing the mean loss.\n    \"\"\"\n    # Read hyperparameters\n    beta = extra.get('beta', 1.0)\n    max_margin = extra.get('max_margin', 2.0)\n    eps = extra.get('eps', 1e-8)\n\n    # In our setting, 'a' is the winner (w) and 'b' is the loser (l).\n    # cost_a < cost_b\n    cost_a = batch['cost_a']\n    cost_b = batch['cost_b']\n    logp_a = batch['log_prob_w']\n    logp_b = batch['log_prob_l']\n\n    # 1. Calculate cost and log-probability differences\n    delta_cost = cost_b - cost_a\n    delta_logp = logp_a - logp_b\n\n    # 2. Normalize the cost difference using rank-gap normalization\n    # This is more robust to outliers than z-score normalization.\n    norm_delta_cost = rank_gap(delta_cost, eps=eps)\n\n    # 3. Compute the adaptive sigmoid margin\n    # The margin is a function of the rank-normalized cost difference.\n    margin = max_margin * torch.sigmoid(norm_delta_cost)\n\n    # 4. Compute the core preference loss (Bradley-Terry style with margin)\n    core_loss = -F.logsigmoid(delta_logp - margin)\n\n    # 5. Compute a cost-based scaling factor using softplus for stability\n    # This amplifies the loss for pairs with larger raw cost differences.\n    weight_scale = F.softplus(beta * delta_cost)\n\n    # 6. Combine core loss and scaling factor\n    loss_per_sample = weight_scale * core_loss\n\n    # 7. Apply optional sample weights if provided\n    sample_weights = batch.get('weight', None)\n    if sample_weights is not None:\n        loss_per_sample = loss_per_sample * sample_weights\n\n    # 8. Return the mean loss over the batch\n    return loss_per_sample.mean()"}, "static_ok": true, "static_reason": "", "dynamic_ok": false, "dynamic_reason": "forward_error: name 'rank_gap' is not defined", "loss_value": null, "grad_norm": null}
{"generation": 2, "index": 6, "ir": {"name": "Adaptive Sigmoid Margin Loss with Tanh Normalization", "intuition": "This loss function uses a dynamic, cost-sensitive margin to separate preferred and non-preferred solutions. The margin is not fixed but is a sigmoid function of the normalized cost difference. When the cost difference is small, the margin is also small, allowing the model to be uncertain. As the cost difference grows, the margin increases towards a maximum value, demanding a stronger preference signal from the model. Instead of z-score normalization which is batch-dependent, this version uses a scaled tanh function on the cost difference, which provides a bounded, batch-independent normalization. This adaptive margin is combined with a standard logistic loss (logsigmoid) on the log-probability difference. The entire term is then scaled by a softplus function of the cost difference, which amplifies the loss for pairs with larger cost gaps, focusing the training on more significant preference signals.", "pseudocode": "1. Calculate the cost difference: delta_cost = cost(b) - cost(a).\n2. Normalize the cost difference using a scaled tanh function: norm_delta_cost = tanh(delta_cost / scale). This provides a bounded, batch-independent normalization of the cost gap.\n3. Compute an adaptive margin: margin = max_margin * sigmoid(norm_delta_cost). The margin is small for small cost gaps and approaches max_margin for large gaps.\n4. Calculate the log-probability difference: delta_logp = logp(a) - logp(b).\n5. Compute the core preference loss: core_loss = -logsigmoid(delta_logp - margin). This is a logistic loss that pushes delta_logp to be greater than the adaptive margin.\n6. Calculate a cost-based weight: weight_scale = softplus(beta * delta_cost). This amplifies the loss for pairs with larger cost differences, making the training focus on more impactful preferences.\n7. Combine them: final_loss = weight_scale * core_loss.\n8. Return the mean of final_loss over the batch.", "hyperparams": {"beta": 1.0, "max_margin": 2.0, "norm_scale": 10.0}, "operators_used": ["logsigmoid", "softplus", "sigmoid", "tanh"], "implementation_hint": {"expects": ["cost_a", "cost_b", "log_prob_w", "log_prob_l"], "returns": "scalar"}, "code": "import torch\nimport torch.nn.functional as F\n\ndef generated_loss(batch, model_output, extra):\n    \"\"\"\n    Adaptive Sigmoid Margin Loss with Tanh Normalization.\n\n    Args:\n        batch (dict): A dictionary containing tensors from the data loader.\n                      Expected keys: 'cost_a', 'cost_b', 'log_prob_w', 'log_prob_l'.\n                      'cost_a' and 'log_prob_w' correspond to the preferred solution (winner).\n                      'cost_b' and 'log_prob_l' correspond to the non-preferred solution (loser).\n                      An optional 'weight' tensor can be used for sample weighting.\n        model_output (dict): Output from the model (not used in this loss structure).\n        extra (dict): A dictionary for hyperparameters, e.g., {'beta': 1.0, 'max_margin': 2.0, 'norm_scale': 10.0}.\n\n    Returns:\n        torch.Tensor: A scalar tensor representing the mean loss.\n    \"\"\"\n    # Read hyperparameters\n    beta = extra.get('beta', 1.0)\n    max_margin = extra.get('max_margin', 2.0)\n    norm_scale = extra.get('norm_scale', 10.0)\n\n    # In our setting, 'a' is the winner (w) and 'b' is the loser (l).\n    # cost_a < cost_b\n    cost_a = batch['cost_a']\n    cost_b = batch['cost_b']\n    logp_a = batch['log_prob_w']\n    logp_b = batch['log_prob_l']\n\n    # 1. Calculate cost and log-probability differences\n    # delta_cost should be positive since cost_b > cost_a\n    delta_cost = cost_b - cost_a\n    delta_logp = logp_a - logp_b\n\n    # 2. Normalize the cost difference using a scaled tanh function\n    # This provides a bounded (-1, 1) and batch-independent normalization\n    norm_delta_cost = torch.tanh(delta_cost / norm_scale)\n\n    # 3. Compute the adaptive sigmoid margin\n    # The margin is a function of the normalized cost difference.\n    margin = max_margin * torch.sigmoid(norm_delta_cost)\n\n    # 4. Compute the core preference loss (Bradley-Terry style with margin)\n    # We want logp_a > logp_b + margin, so (logp_a - logp_b - margin) should be positive.\n    # The logsigmoid of a large positive number is close to 0.\n    core_loss = -F.logsigmoid(delta_logp - margin)\n\n    # 5. Compute a cost-based scaling factor using softplus for stability\n    # This amplifies the loss for pairs with larger raw cost differences.\n    weight_scale = F.softplus(beta * delta_cost)\n\n    # 6. Combine core loss and scaling factor\n    loss_per_sample = weight_scale * core_loss\n\n    # 7. Apply optional sample weights if provided\n    sample_weights = batch.get('weight', None)\n    if sample_weights is not None:\n        loss_per_sample = loss_per_sample * sample_weights\n\n    # 8. Return the mean loss over the batch\n    return loss_per_sample.mean()"}, "static_ok": true, "static_reason": "", "dynamic_ok": true, "dynamic_reason": "ok", "loss_value": 1.7727785110473633, "grad_norm": 0.0}
{"generation": 2, "index": 7, "ir": {"name": "Clipped Adaptive Sigmoid Margin Loss", "intuition": "This loss function refines the adaptive margin concept by introducing a clipping mechanism for numerical stability and to prevent extreme values from dominating the batch. The margin is a sigmoid function of the normalized cost difference, making it sensitive to the relative cost improvement within the batch. By clamping the normalized cost difference before applying the sigmoid function, we ensure the margin signal remains within a controlled range, preventing outliers from creating excessively large or small margins. This makes the training process more stable. The core loss remains a logistic objective, scaled by a softplus function of the raw cost difference to emphasize pairs with more significant cost gaps.", "pseudocode": "1. Calculate the cost difference: delta_cost = cost(b) - cost(a).\n2. Normalize the cost difference across the batch using z-score: norm_delta_cost.\n3. Clip the normalized cost difference to a range [-clip_value, clip_value] to prevent extreme values from having an outsized effect.\n4. Compute an adaptive margin based on the clipped, normalized cost difference: margin = max_margin * sigmoid(clipped_norm_delta_cost).\n5. Calculate the log-probability difference: delta_logp = logp(a) - logp(b).\n6. Compute the core preference loss: core_loss = -logsigmoid(delta_logp - margin). This pushes delta_logp to be greater than the adaptive margin.\n7. Calculate a cost-based weight: weight_scale = softplus(beta * delta_cost). This amplifies the loss for pairs with larger cost differences.\n8. Combine them: final_loss = weight_scale * core_loss.\n9. Return the mean of final_loss over the batch.", "hyperparams": {"beta": 1.0, "max_margin": 2.0, "clip_value": 3.0, "eps": 1e-08}, "operators_used": ["logsigmoid", "softplus", "sigmoid", "zscore", "clamp"], "implementation_hint": {"expects": ["cost_a", "cost_b", "logp_a", "logp_b"], "returns": "scalar"}, "code": "import torch\nimport torch.nn.functional as F\n\ndef generated_loss(batch, model_output, extra):\n    \"\"\"\n    Clipped Adaptive Sigmoid Margin Loss.\n\n    Args:\n        batch (dict): A dictionary containing tensors from the data loader.\n                      Expected keys: 'cost_a', 'cost_b', 'log_prob_w', 'log_prob_l'.\n                      'cost_a' and 'log_prob_w' correspond to the preferred solution (winner).\n                      'cost_b' and 'log_prob_l' correspond to the non-preferred solution (loser).\n                      An optional 'weight' tensor can be used for sample weighting.\n        model_output (dict): Output from the model (not used in this loss structure).\n        extra (dict): A dictionary for hyperparameters, e.g., {'beta': 1.0, 'max_margin': 2.0, 'clip_value': 3.0, 'eps': 1e-8}.\n\n    Returns:\n        torch.Tensor: A scalar tensor representing the mean loss.\n    \"\"\"\n    # Read hyperparameters\n    beta = extra.get('beta', 1.0)\n    max_margin = extra.get('max_margin', 2.0)\n    clip_value = extra.get('clip_value', 3.0)\n    eps = extra.get('eps', 1e-8)\n\n    # In our setting, 'a' is the winner (w) and 'b' is the loser (l).\n    # cost_a < cost_b\n    cost_a = batch['cost_a']\n    cost_b = batch['cost_b']\n    logp_a = batch['log_prob_w']\n    logp_b = batch['log_prob_l']\n\n    # 1. Calculate cost and log-probability differences\n    # delta_cost should be positive since cost_b > cost_a\n    delta_cost = cost_b - cost_a\n    delta_logp = logp_a - logp_b\n\n    # 2. Normalize the cost difference (z-score for batch-level adaptation)\n    if delta_cost.numel() > 1 and delta_cost.std() > eps:\n        mean = delta_cost.mean()\n        std = delta_cost.std()\n        norm_delta_cost = (delta_cost - mean) / (std + eps)\n    else:\n        norm_delta_cost = torch.zeros_like(delta_cost)\n\n    # 3. Clip the normalized cost difference for stability\n    clipped_norm_delta_cost = torch.clamp(norm_delta_cost, -clip_value, clip_value)\n\n    # 4. Compute the adaptive sigmoid margin from the clipped value\n    margin = max_margin * torch.sigmoid(clipped_norm_delta_cost)\n\n    # 5. Compute the core preference loss (Bradley-Terry style with margin)\n    core_loss = -F.logsigmoid(delta_logp - margin)\n\n    # 6. Compute a cost-based scaling factor using softplus for stability\n    weight_scale = F.softplus(beta * delta_cost)\n\n    # 7. Combine core loss and scaling factor\n    loss_per_sample = weight_scale * core_loss\n\n    # 8. Apply optional sample weights if provided\n    sample_weights = batch.get('weight', None)\n    if sample_weights is not None:\n        loss_per_sample = loss_per_sample * sample_weights\n\n    # 9. Return the mean loss over the batch\n    return loss_per_sample.mean()"}, "static_ok": true, "static_reason": "", "dynamic_ok": true, "dynamic_reason": "ok", "loss_value": 1.7246562242507935, "grad_norm": 0.0}
{"generation": 3, "index": 0, "ir": {"name": "Clipped Tanh Margin Loss with Cost-Gap Gating", "intuition": "This loss function synthesizes the stable, batch-independent normalization from Parent 0 with the outlier-resistant clipping from Parent 1, and introduces a novel cost-gap gating mechanism. \n\nInherited ideas:\n- **Tanh Normalization (from Parent 0):** It uses a scaled `tanh` function to normalize the cost difference (`cost_b - cost_a`). This provides a bounded, batch-independent normalization, making the loss less sensitive to the specific composition of a training batch compared to z-scoring.\n- **Clipping (from Parent 1):** Before computing the margin, the normalized cost difference is clipped. This idea is inherited from Parent 1, but applied to the output of `tanh` rather than z-score. It prevents extreme values from creating excessively large margins, enhancing numerical stability.\n\nNew coupling ideas:\n1. **Cost-Gap Gating:** The core innovation is a gating mechanism that modulates the influence of the log-probability difference (`delta_logp`). This gate is a `sigmoid` function of the raw cost difference. For pairs with a very small cost difference, the gate's output is close to 0.5, effectively shrinking `delta_logp` and reducing the learning signal. This allows the model to be uncertain when the preference is weak. As the cost difference grows, the gate approaches 1, allowing the full `delta_logp` to contribute to the loss. This creates a smoother, more proportional response to the cost gap than the all-or-nothing scaling seen in the parents.\n2. **Unified Scaling:** Instead of using a separate `softplus` weight for the entire loss term, the cost-gap information is integrated directly into the core `logsigmoid` argument via the gating mechanism. This provides a more direct and potentially more stable way to scale the learning signal based on the magnitude of the preference.", "pseudocode": "1. Calculate the cost difference: delta_cost = cost(b) - cost(a).\n2. Calculate the log-probability difference: delta_logp = logp(a) - logp(b).\n3. Normalize the cost difference using a scaled tanh function (inherited from Parent 0): norm_delta_cost = tanh(delta_cost / norm_scale).\n4. Clip the normalized cost difference to prevent extreme values (inspired by Parent 1): clipped_norm_cost = clamp(norm_delta_cost, -clip_value, clip_value).\n5. Compute an adaptive margin based on the clipped, normalized cost: margin = max_margin * sigmoid(clipped_norm_cost).\n6. (New Coupling 1) Create a cost-gap gate: gate = sigmoid(gate_beta * delta_cost). This gate value is between 0.5 and 1, scaling up as the cost difference increases.\n7. (New Coupling 2) Apply the gate to the log-probability difference, modulating its effect: gated_delta_logp = gate * delta_logp.\n8. Compute the final loss using a single logsigmoid term that combines the gated probability difference and the adaptive margin: loss = -logsigmoid(gated_delta_logp - margin).\n9. Return the mean loss over the batch.", "hyperparams": {"max_margin": 1.5, "norm_scale": 10.0, "clip_value": 0.95, "gate_beta": 0.5}, "operators_used": ["logsigmoid", "sigmoid", "tanh", "clamp"], "implementation_hint": {"expects": ["cost_a", "cost_b", "logp_a", "logp_b", "weight"], "returns": "scalar"}, "code": "import torch\nimport torch.nn.functional as F\n\ndef generated_loss(batch, model_output, extra):\n    \"\"\"\n    Clipped Tanh Margin Loss with Cost-Gap Gating.\n\n    Args:\n        batch (dict): A dictionary containing tensors from the data loader.\n                      Expected keys: 'cost_a', 'cost_b', 'log_prob_w', 'log_prob_l'.\n                      'cost_a' and 'log_prob_w' correspond to the preferred solution (winner).\n                      'cost_b' and 'log_prob_l' correspond to the non-preferred solution (loser).\n                      An optional 'weight' tensor can be used for sample weighting.\n        model_output (dict): Output from the model (not used in this loss structure).\n        extra (dict): A dictionary for hyperparameters, e.g., {'max_margin': 1.5, 'norm_scale': 10.0, 'clip_value': 0.95, 'gate_beta': 0.5}.\n\n    Returns:\n        torch.Tensor: A scalar tensor representing the mean loss.\n    \"\"\"\n    # Read hyperparameters\n    max_margin = extra.get('max_margin', 1.5)\n    norm_scale = extra.get('norm_scale', 10.0)\n    clip_value = extra.get('clip_value', 0.95) # Clip the tanh output, so should be <= 1.0\n    gate_beta = extra.get('gate_beta', 0.5)\n\n    # In our setting, 'a' is the winner (w) and 'b' is the loser (l).\n    # cost_a < cost_b\n    cost_a = batch['cost_a']\n    cost_b = batch['cost_b']\n    logp_a = batch['log_prob_w']\n    logp_b = batch['log_prob_l']\n\n    # 1. Calculate cost and log-probability differences\n    delta_cost = cost_b - cost_a\n    delta_logp = logp_a - logp_b\n\n    # 2. Normalize cost difference with tanh (from Parent 0)\n    norm_delta_cost = torch.tanh(delta_cost / norm_scale)\n\n    # 3. Clip the normalized cost for stability (inspired by Parent 1)\n    # Since tanh is bounded by [-1, 1], clip_value should be <= 1.\n    clipped_norm_cost = torch.clamp(norm_delta_cost, -clip_value, clip_value)\n\n    # 4. Compute the adaptive margin from the clipped, normalized cost\n    margin = max_margin * torch.sigmoid(clipped_norm_cost)\n\n    # 5. (New Coupling) Create a cost-gap gate to modulate the logp difference\n    # For delta_cost -> 0, gate -> 0.5. For delta_cost -> inf, gate -> 1.\n    # This smoothly scales the importance of delta_logp based on the cost gap.\n    gate = torch.sigmoid(gate_beta * delta_cost) \n    gated_delta_logp = gate * delta_logp\n\n    # 6. Compute the final loss\n    # The objective is to make (gated_delta_logp - margin) large and positive.\n    loss_per_sample = -F.logsigmoid(gated_delta_logp - margin)\n\n    # 7. Apply optional sample weights if provided\n    sample_weights = batch.get('weight', None)\n    if sample_weights is not None:\n        loss_per_sample = loss_per_sample * sample_weights\n\n    # 8. Return the mean loss over the batch\n    return loss_per_sample.mean()"}, "static_ok": true, "static_reason": "", "dynamic_ok": true, "dynamic_reason": "ok", "loss_value": 1.1623859405517578, "grad_norm": 0.0}
{"generation": 3, "index": 1, "ir": {"name": "Rank-Gated Tanh-Normalized Margin Loss", "intuition": "This loss function combines the stability of tanh normalization from Parent 0 with the outlier-robustness of clipping from Parent 1, but introduces a new coupling mechanism based on rank-gating. \n\nInherited Ideas:\n- From Parent 0 (Adaptive Sigmoid Margin Loss with Tanh Normalization), it inherits the use of `tanh` for a bounded, batch-independent normalization of the cost difference. This avoids sensitivity to batch statistics unlike z-score. It also inherits the `softplus` scaling of the loss by the raw cost difference, focusing training on more significant preferences.\n- From Parent 1 (Clipped Adaptive Sigmoid Margin Loss), it inherits the concept of controlling the influence of the normalized cost difference, but replaces explicit `clamp` with a smoother, rank-based gating mechanism.\n\nNew Coupling Ideas:\n1.  **Rank-Gating:** Instead of hard-clipping the normalized cost difference, this loss introduces a 'gate' based on the percentile rank of the cost difference within the batch. The `rank_gap` operator computes a value from -1 to 1 based on a sample's rank. This gate is then used to modulate the `tanh`-normalized cost difference. For pairs with a median cost gap, the gate is near zero, effectively nullifying the margin and simplifying the loss to a standard logistic loss. For pairs with very high or low cost gaps relative to the batch, the gate approaches 1 or -1, allowing the full `tanh`-normalized signal to contribute to the margin. This smoothly focuses the adaptive margin on the most and least significant preferences within the batch, providing a dynamic form of outlier control.\n2.  **Sigmoid Blending:** The final loss is a blend between the rank-gated margin loss and a simple logistic loss. The blending weight is a `sigmoid` function of the raw cost difference. For very small cost differences, the loss defaults almost entirely to the simple logistic loss, preventing noisy or unstable margins from affecting pairs where the preference is weak. As the cost difference grows, the contribution from the more complex rank-gated margin loss increases.", "pseudocode": "1. Calculate the cost difference: delta_cost = cost(b) - cost(a).\n2. Calculate the log-probability difference: delta_logp = logp(a) - logp(b).\n3. Normalize the cost difference using a scaled tanh function for a bounded signal: tanh_norm_cost = tanh(delta_cost / norm_scale). (Inherited from Parent 0)\n4. Compute a rank-based gate for each sample in the batch using the `rank_gap` operator on delta_cost. This yields a value in [-1, 1] indicating the percentile rank. (New Coupling Idea 1)\n5. Modulate the normalized cost with the rank gate: gated_norm_cost = rank_gate * tanh_norm_cost. This focuses the margin on non-median preferences.\n6. Compute the adaptive margin: margin = max_margin * sigmoid(gated_norm_cost).\n7. Compute the core margin-based loss term: margin_loss_term = -logsigmoid(delta_logp - margin).\n8. Compute a simple logistic loss term for a stable baseline: simple_loss_term = -logsigmoid(delta_logp).\n9. Compute a blending factor using a sigmoid on the raw cost difference: blend_weight = sigmoid((delta_cost - blend_midpoint) / blend_steepness). (New Coupling Idea 2)\n10. Linearly interpolate between the simple and margin-based losses using the blend weight: core_loss = (1 - blend_weight) * simple_loss_term + blend_weight * margin_loss_term.\n11. Calculate a final weight scale based on the raw cost difference to amplify the loss for significant pairs: weight_scale = softplus(beta * delta_cost). (Inherited from Parent 0)\n12. Combine them: final_loss = weight_scale * core_loss.\n13. Return the mean of final_loss over the batch.", "hyperparams": {"beta": 1.0, "max_margin": 2.0, "norm_scale": 10.0, "blend_midpoint": 0.5, "blend_steepness": 2.0, "eps": 1e-08}, "operators_used": ["logsigmoid", "softplus", "sigmoid", "tanh", "rank_gap"], "implementation_hint": {"expects": ["cost_a", "cost_b", "logp_a", "logp_b"], "returns": "scalar"}, "code": "import torch\nimport torch.nn.functional as F\n\ndef _rank_gap(x, eps=1e-8):\n    \"\"\"Computes a value in [-1, 1] based on the rank of x.\n\n    The value is 2 * (rank / (N - 1)) - 1, where rank is 0-indexed.\n    This maps the lowest value to -1, the highest to 1, and the median to ~0.\n    Handles batch size of 1 by returning 0.\n    \"\"\"\n    if x.numel() <= 1:\n        return torch.zeros_like(x)\n    # Get the rank of each element in ascending order\n    ranks = x.argsort().argsort().float()\n    # Normalize ranks to [0, 1]\n    normalized_ranks = ranks / (x.numel() - 1 + eps)\n    # Scale to [-1, 1]\n    return 2 * normalized_ranks - 1\n\ndef generated_loss(batch, model_output, extra):\n    \"\"\"\n    Rank-Gated Tanh-Normalized Margin Loss.\n\n    Args:\n        batch (dict): A dictionary containing tensors from the data loader.\n                      Expected keys: 'cost_a', 'cost_b', 'log_prob_w', 'log_prob_l'.\n                      An optional 'weight' tensor can be used for sample weighting.\n        model_output (dict): Output from the model (not used).\n        extra (dict): A dictionary for hyperparameters.\n\n    Returns:\n        torch.Tensor: A scalar tensor representing the mean loss.\n    \"\"\"\n    # Read hyperparameters\n    beta = extra.get('beta', 1.0)\n    max_margin = extra.get('max_margin', 2.0)\n    norm_scale = extra.get('norm_scale', 10.0)\n    blend_midpoint = extra.get('blend_midpoint', 0.5)\n    blend_steepness = extra.get('blend_steepness', 2.0)\n    eps = extra.get('eps', 1e-8)\n\n    # Unpack batch data\n    cost_a = batch['cost_a']\n    cost_b = batch['cost_b']\n    logp_a = batch['log_prob_w']\n    logp_b = batch['log_prob_l']\n\n    # 1. Calculate cost and log-probability differences\n    delta_cost = cost_b - cost_a\n    delta_logp = logp_a - logp_b\n\n    # 2. Normalize cost difference with tanh (from Parent 0)\n    tanh_norm_cost = torch.tanh(delta_cost / norm_scale)\n\n    # 3. Compute rank-based gate (New Coupling Idea 1)\n    rank_gate = _rank_gap(delta_cost, eps=eps)\n\n    # 4. Modulate the normalized cost with the rank gate\n    gated_norm_cost = rank_gate * tanh_norm_cost\n\n    # 5. Compute the adaptive margin\n    margin = max_margin * torch.sigmoid(gated_norm_cost)\n\n    # 6. Compute the core margin-based loss term\n    margin_loss_term = -F.logsigmoid(delta_logp - margin)\n\n    # 7. Compute a simple logistic loss for blending\n    simple_loss_term = -F.logsigmoid(delta_logp)\n\n    # 8. Compute a blending weight based on raw cost difference (New Coupling Idea 2)\n    blend_weight = torch.sigmoid((delta_cost - blend_midpoint) / blend_steepness)\n\n    # 9. Blend the two loss terms\n    core_loss = torch.lerp(simple_loss_term, margin_loss_term, blend_weight)\n\n    # 10. Apply final cost-based scaling (from Parent 0)\n    weight_scale = F.softplus(beta * delta_cost)\n    loss_per_sample = weight_scale * core_loss\n\n    # 11. Apply optional sample weights\n    sample_weights = batch.get('weight', None)\n    if sample_weights is not None:\n        loss_per_sample = loss_per_sample * sample_weights\n\n    # 12. Return the mean loss\n    return loss_per_sample.mean()"}, "static_ok": true, "static_reason": "", "dynamic_ok": false, "dynamic_reason": "forward_error: name '_rank_gap' is not defined", "loss_value": null, "grad_norm": null}
{"generation": 3, "index": 2, "ir": {"name": "Clipped Tanh-Normalized Adaptive Margin Loss", "intuition": "This loss function combines the stability of tanh normalization with a clipping mechanism to create a robust, adaptive margin for preference learning. The core objective is a logistic loss encouraging the model's log-probability difference to exceed a dynamic margin.\n\nInherited ideas:\n- From 'Adaptive Sigmoid Margin Loss with Tanh Normalization' (Parent 0), it inherits the use of a scaled `tanh` function to normalize the cost difference. This provides a bounded, batch-independent measure of the cost gap, which is inherently more stable than batch-dependent z-scoring.\n- From 'Clipped Adaptive Sigmoid Margin Loss' (Parent 1), it inherits the idea of `clamping` the normalized value before it is used to compute the margin. This prevents extreme cost differences from creating overly aggressive margin targets, thereby stabilizing training.\n- Both parents contribute the general structure: a logistic loss (`-logsigmoid`) with a cost-sensitive margin, and a `softplus` scaling of the final loss to emphasize pairs with larger raw cost differences.\n\nNew coupling ideas:\n1.  **Dual-Stage Normalization (Tanh + Clamp):** The child loss couples the `tanh` normalization from Parent 0 with the `clamp` operator from Parent 1. While `tanh` already bounds its output to (-1, 1), applying a clamp (e.g., `clamp(-0.9, 0.9)`) before the sigmoid function provides a finer-grained control over the margin's sensitivity. It ensures that the effective margin never quite reaches its theoretical maximum or minimum, preventing the sigmoid from operating in its saturated regions and thus maintaining a healthier gradient flow.\n2.  **Adaptive Beta Scaling:** The `beta` hyperparameter, which scales the final loss weighting, is made adaptive. It is scaled by the sigmoid of the clipped, normalized cost difference. This means that for pairs with a very small normalized cost gap, the `beta` scaling is attenuated, reducing the loss contribution from ambiguous or noisy preference pairs. Conversely, for pairs with a clear and significant normalized cost gap, the `beta` scaling is stronger, focusing the model's attention on confident preference signals.", "pseudocode": "1. Calculate the cost difference: delta_cost = cost(b) - cost(a).\n2. Normalize the cost difference using a scaled tanh function for bounded, batch-independent normalization: tanh_norm_delta_cost = tanh(delta_cost / norm_scale).\n3. Clip the tanh-normalized cost difference to a range [-clip_value, clip_value] to control the margin's sensitivity and prevent saturation: clipped_norm_cost = clamp(tanh_norm_delta_cost, -clip_value, clip_value).\n4. Compute an adaptive margin based on the clipped, normalized cost: margin = max_margin * sigmoid(clipped_norm_cost).\n5. Calculate the log-probability difference: delta_logp = logp(a) - logp(b).\n6. Compute the core preference loss: core_loss = -logsigmoid(delta_logp - margin).\n7. Compute an adaptive beta scaling factor: adaptive_beta = beta * sigmoid(clipped_norm_cost). This attenuates the loss weight for pairs with small normalized cost gaps.\n8. Calculate the final cost-based weight: weight_scale = softplus(adaptive_beta * delta_cost).\n9. Combine them: final_loss = weight_scale * core_loss.\n10. Return the mean of final_loss over the batch.", "hyperparams": {"beta": 1.0, "max_margin": 2.0, "norm_scale": 10.0, "clip_value": 0.9}, "operators_used": ["logsigmoid", "softplus", "sigmoid", "tanh", "clamp"], "implementation_hint": {"expects": ["cost_a", "cost_b", "logp_a", "logp_b"], "returns": "scalar"}, "code": "import torch\nimport torch.nn.functional as F\n\ndef generated_loss(batch, model_output, extra):\n    \"\"\"\n    Clipped Tanh-Normalized Adaptive Margin Loss.\n\n    Args:\n        batch (dict): A dictionary containing tensors from the data loader.\n                      Expected keys: 'cost_a', 'cost_b', 'log_prob_w', 'log_prob_l'.\n                      'cost_a' and 'log_prob_w' correspond to the preferred solution (winner).\n                      'cost_b' and 'log_prob_l' correspond to the non-preferred solution (loser).\n                      An optional 'weight' tensor can be used for sample weighting.\n        model_output (dict): Output from the model (not used in this loss structure).\n        extra (dict): A dictionary for hyperparameters, e.g., {'beta': 1.0, 'max_margin': 2.0, 'norm_scale': 10.0, 'clip_value': 0.9}.\n\n    Returns:\n        torch.Tensor: A scalar tensor representing the mean loss.\n    \"\"\"\n    # Read hyperparameters\n    beta = extra.get('beta', 1.0)\n    max_margin = extra.get('max_margin', 2.0)\n    norm_scale = extra.get('norm_scale', 10.0)\n    clip_value = extra.get('clip_value', 0.9)\n\n    # In our setting, 'a' is the winner (w) and 'b' is the loser (l).\n    # cost_a < cost_b\n    cost_a = batch['cost_a']\n    cost_b = batch['cost_b']\n    logp_a = batch['log_prob_w']\n    logp_b = batch['log_prob_l']\n\n    # 1. Calculate cost and log-probability differences\n    # delta_cost should be positive since cost_b > cost_a\n    delta_cost = cost_b - cost_a\n    delta_logp = logp_a - logp_b\n\n    # 2. Inherited Idea (Parent 0): Normalize cost difference using tanh for stability.\n    tanh_norm_delta_cost = torch.tanh(delta_cost / norm_scale)\n\n    # 3. New Coupling Idea (1): Couple tanh normalization with clamping (from Parent 1) to control margin sensitivity.\n    # Ensures the input to sigmoid is not in saturated regions.\n    clipped_norm_cost = torch.clamp(tanh_norm_delta_cost, -clip_value, clip_value)\n\n    # 4. Compute the adaptive sigmoid margin from the clipped, normalized value.\n    margin = max_margin * torch.sigmoid(clipped_norm_cost)\n\n    # 5. Compute the core preference loss (Bradley-Terry style with margin).\n    core_loss = -F.logsigmoid(delta_logp - margin)\n\n    # 6. New Coupling Idea (2): Create an adaptive beta scaling factor.\n    # This reduces the loss weight for pairs with small normalized cost gaps.\n    adaptive_beta = beta * torch.sigmoid(clipped_norm_cost)\n\n    # 7. Compute a cost-based scaling factor using softplus for stability.\n    # This now uses the adaptive beta.\n    weight_scale = F.softplus(adaptive_beta * delta_cost)\n\n    # 8. Combine core loss and scaling factor.\n    loss_per_sample = weight_scale * core_loss\n\n    # 9. Apply optional sample weights if provided.\n    sample_weights = batch.get('weight', None)\n    if sample_weights is not None:\n        loss_per_sample = loss_per_sample * sample_weights\n\n    # 10. Return the mean loss over the batch.\n    return loss_per_sample.mean()"}, "static_ok": true, "static_reason": "", "dynamic_ok": true, "dynamic_reason": "ok", "loss_value": 1.3359289169311523, "grad_norm": 0.0}
{"generation": 3, "index": 3, "ir": {"name": "Adaptive Margin Loss with Tanh-Clipped Normalization and Rank Gap Scaling", "intuition": "This loss function combines the stability and batch-independent normalization of tanh from one parent with the outlier-robustness of clipping from the other. It inherits the core logistic loss structure (-logsigmoid) and the concept of an adaptive, cost-sensitive margin. \n\nInherited ideas:\n- From Parent 0 (Adaptive Sigmoid Margin Loss with Tanh Normalization), it inherits the use of `tanh` for a bounded, batch-independent normalization of the cost difference, which avoids sensitivity to batch statistics.\n- From Parent 1 (Clipped Adaptive Sigmoid Margin Loss), it inherits the idea of `clamping` the normalized values. This prevents extreme cost differences from creating overly saturated margin signals (close to 0 or max_margin), ensuring the model receives a meaningful gradient even for outliers.\n\nNew coupling ideas:\n1.  **Tanh-Clip Coupling**: Instead of using z-score then clipping, we apply `tanh` for normalization and then `clamp` the result. This creates a highly stable normalization scheme where the effective range is controlled by the clip bounds, preventing the `tanh` from fully saturating and killing gradients for large cost differences.\n2.  **Rank Gap Scaling**: The final loss is scaled not by the raw cost difference (which can be unbounded), but by a `softplus` function of the `rank_gap` of the cost difference. `rank_gap(x)` maps a vector `x` to `[0, 1]`, preserving the order. This focuses training on the relative importance of preferences within the batch (i.e., how significant a cost gap is compared to others) rather than their absolute magnitude, making the loss scaling more robust to cost distributions across different batches.", "pseudocode": "1. Calculate the cost difference: delta_cost = cost(b) - cost(a).\n2. Calculate the log-probability difference: delta_logp = logp(a) - logp(b).\n3. Normalize the cost difference using a scaled tanh function: tanh_norm_cost = tanh(delta_cost / norm_scale).\n4. Clip the tanh-normalized cost difference to a stable range [-clip_value, clip_value]. This is the new Tanh-Clip coupling.\n5. Compute an adaptive margin based on the clipped, normalized value: margin = max_margin * sigmoid(clipped_norm_cost).\n6. Compute the core preference loss: core_loss = -logsigmoid(delta_logp - margin).\n7. Compute the rank gap of the cost differences in the batch, which maps them to a [0, 1] range while preserving order. This is the new Rank Gap Scaling.\n8. Calculate a loss weight using a softplus function on the scaled rank gap: weight_scale = softplus(beta * rank_gap_delta_cost).\n9. Combine them: final_loss = weight_scale * core_loss.\n10. Return the mean of final_loss over the batch.", "hyperparams": {"beta": 5.0, "max_margin": 2.0, "norm_scale": 10.0, "clip_value": 0.95}, "operators_used": ["logsigmoid", "softplus", "sigmoid", "tanh", "clamp", "rank_gap"], "implementation_hint": {"expects": ["cost_a", "cost_b", "logp_a", "logp_b"], "returns": "scalar"}, "code": "import torch\nimport torch.nn.functional as F\n\ndef _rank_gap(x):\n    \"\"\"Helper to compute the rank gap of a tensor, mapping it to [0, 1].\"\"\"\n    if x.numel() <= 1:\n        return torch.ones_like(x) * 0.5\n    # Returns the indices that would sort the tensor\n    sorted_indices = torch.argsort(x)\n    # Creates a tensor where the value at each original index is its rank\n    ranks = torch.zeros_like(x, dtype=torch.float)\n    ranks[sorted_indices] = torch.arange(x.numel(), device=x.device, dtype=torch.float)\n    # Normalize ranks to [0, 1]\n    normalized_ranks = ranks / (x.numel() - 1)\n    return normalized_ranks\n\ndef generated_loss(batch, model_output, extra):\n    \"\"\"\n    Adaptive Margin Loss with Tanh-Clipped Normalization and Rank Gap Scaling.\n\n    Args:\n        batch (dict): A dictionary containing tensors from the data loader.\n                      Expected keys: 'cost_a', 'cost_b', 'log_prob_w', 'log_prob_l'.\n                      An optional 'weight' tensor can be used for sample weighting.\n        model_output (dict): Output from the model (not used).\n        extra (dict): A dictionary for hyperparameters, e.g., {'beta': 5.0, 'max_margin': 2.0, 'norm_scale': 10.0, 'clip_value': 0.95}.\n\n    Returns:\n        torch.Tensor: A scalar tensor representing the mean loss.\n    \"\"\"\n    # Read hyperparameters\n    beta = extra.get('beta', 5.0)\n    max_margin = extra.get('max_margin', 2.0)\n    norm_scale = extra.get('norm_scale', 10.0)\n    clip_value = extra.get('clip_value', 0.95) # Should be <= 1.0\n\n    # In our setting, 'a' is the winner (w) and 'b' is the loser (l).\n    # cost_a < cost_b\n    cost_a = batch['cost_a']\n    cost_b = batch['cost_b']\n    logp_a = batch['log_prob_w']\n    logp_b = batch['log_prob_l']\n\n    # 1. Calculate cost and log-probability differences\n    delta_cost = cost_b - cost_a\n    delta_logp = logp_a - logp_b\n\n    # 2. Inherit Tanh normalization from Parent 0\n    tanh_norm_cost = torch.tanh(delta_cost / norm_scale)\n\n    # 3. Inherit clipping from Parent 1, coupling it with Tanh\n    clipped_norm_cost = torch.clamp(tanh_norm_cost, -clip_value, clip_value)\n\n    # 4. Compute the adaptive sigmoid margin\n    margin = max_margin * torch.sigmoid(clipped_norm_cost)\n\n    # 5. Compute the core preference loss (Bradley-Terry style with margin)\n    core_loss = -F.logsigmoid(delta_logp - margin)\n\n    # 6. New Coupling: Use rank gap for loss scaling\n    # This makes scaling robust to the absolute magnitude of costs in a batch.\n    with torch.no_grad():\n        rank_gap_delta_cost = _rank_gap(delta_cost)\n    \n    # 7. Compute scaling factor based on the rank gap\n    weight_scale = F.softplus(beta * rank_gap_delta_cost)\n\n    # 8. Combine core loss and the new rank-based scaling factor\n    loss_per_sample = weight_scale * core_loss\n\n    # 9. Apply optional sample weights if provided\n    sample_weights = batch.get('weight', None)\n    if sample_weights is not None:\n        loss_per_sample = loss_per_sample * sample_weights\n\n    # 10. Return the mean loss over the batch\n    return loss_per_sample.mean()"}, "static_ok": true, "static_reason": "", "dynamic_ok": false, "dynamic_reason": "forward_error: name '_rank_gap' is not defined", "loss_value": null, "grad_norm": null}
{"generation": 3, "index": 4, "ir": {"name": "Rank-Gated Tanh-Normalized Margin Loss", "intuition": "This loss function synthesizes ideas from its parents by using an adaptive, cost-sensitive margin within a logistic loss framework, while introducing a new coupling mechanism to gate the loss based on the rank of the cost difference within the batch.\n\nInherited Ideas:\n- From `Adaptive Sigmoid Margin Loss with Tanh Normalization` (Parent 0), it inherits the use of a scaled `tanh` function to create a bounded, batch-independent normalization of the cost difference (`delta_cost`). This avoids the batch-dependency of z-score normalization and provides inherent stability.\n- From `Clipped Adaptive Sigmoid Margin Loss` (Parent 1) and Parent 0, it inherits the core structure of a logistic loss (`-logsigmoid`) with an adaptive margin that is a `sigmoid` function of the normalized cost difference. This makes the required log-probability gap larger for pairs with a larger cost gap.\n\nNew Coupling Ideas:\n1.  **Rank-Gating:** Instead of scaling the loss directly by the cost difference (e.g., via `softplus(beta * delta_cost)`), this child loss introduces a gating mechanism based on the percentile rank of the cost difference within the batch. A `rank_gap` operator computes the percentile of each `delta_cost`. This rank is then passed through a `softplus` function. This 'rank-gate' focuses training on the most significant preference pairs in the *current batch* in a relative sense, rather than relying on absolute cost values which can vary widely. It makes the loss more robust to shifts in the distribution of cost differences across different stages of training.\n2.  **Margin Clipping:** Although `tanh` normalization is already bounded, we explicitly `clamp` the normalized cost difference before computing the sigmoid margin, as inspired by Parent 1. This provides an additional layer of stability and control, ensuring the input to the sigmoid function (and thus the resulting margin) stays within a well-defined range, preventing potential saturation issues if `norm_scale` is poorly chosen.", "pseudocode": "1. Calculate the cost difference: delta_cost = cost(b) - cost(a).\n2. Normalize the cost difference using a scaled tanh function for bounded, batch-independent normalization: norm_delta_cost = tanh(delta_cost / norm_scale).\n3. Clip the normalized cost difference to a range [-clip_value, clip_value] for added stability.\n4. Compute an adaptive margin from the clipped, normalized cost difference: margin = max_margin * sigmoid(clipped_norm_delta_cost).\n5. Calculate the log-probability difference: delta_logp = logp(a) - logp(b).\n6. Compute the core preference loss: core_loss = -logsigmoid(delta_logp - margin).\n7. Compute a batch-relative 'rank-gate' weight: First, find the percentile rank of each delta_cost within the batch (rank_gap). Then, scale this rank and pass it through a softplus function: rank_gate = softplus(beta * rank_gap(delta_cost)).\n8. Combine them: final_loss = rank_gate * core_loss. This amplifies the loss for pairs with a higher relative cost difference in the batch.\n9. Return the mean of final_loss over the batch.", "hyperparams": {"beta": 5.0, "max_margin": 2.5, "norm_scale": 15.0, "clip_value": 0.95}, "operators_used": ["logsigmoid", "softplus", "sigmoid", "tanh", "clamp", "rank_gap"], "implementation_hint": {"expects": ["cost_a", "cost_b", "logp_a", "logp_b"], "returns": "scalar"}, "code": "import torch\nimport torch.nn.functional as F\n\ndef rank_gap(x):\n    \"\"\"Computes the percentile rank of each element in a 1D tensor.\"\"\"\n    if x.numel() <= 1:\n        return torch.ones_like(x)\n    # stable_rank sorts and returns ranks, adding average of ties\n    # This is a simplified version; torch.argsort().argsort() is a common way\n    # but we need to handle ties properly for a percentile rank.\n    # A simple implementation:\n    sorted_indices = torch.argsort(x)\n    ranks = torch.empty_like(x, dtype=torch.float)\n    ranks[sorted_indices] = torch.arange(len(x), device=x.device, dtype=torch.float)\n    # Normalize to [0, 1] range\n    return ranks / (len(x) - 1 + 1e-8)\n\ndef generated_loss(batch, model_output, extra):\n    \"\"\"\n    Rank-Gated Tanh-Normalized Margin Loss.\n\n    Args:\n        batch (dict): A dictionary containing tensors from the data loader.\n                      Expected keys: 'cost_a', 'cost_b', 'log_prob_w', 'log_prob_l'.\n                      'cost_a'/'log_prob_w' are preferred (winner), 'cost_b'/'log_prob_l' are not (loser).\n                      An optional 'weight' tensor can be used for sample weighting.\n        model_output (dict): Output from the model (not used).\n        extra (dict): Hyperparameters, e.g., {'beta': 5.0, 'max_margin': 2.5, 'norm_scale': 15.0, 'clip_value': 0.95}.\n\n    Returns:\n        torch.Tensor: A scalar tensor representing the mean loss.\n    \"\"\"\n    # Read hyperparameters\n    beta = extra.get('beta', 5.0)\n    max_margin = extra.get('max_margin', 2.5)\n    norm_scale = extra.get('norm_scale', 15.0)\n    clip_value = extra.get('clip_value', 0.95)\n\n    # Unpack batch tensors\n    cost_a = batch['cost_a']\n    cost_b = batch['cost_b']\n    logp_a = batch['log_prob_w']\n    logp_b = batch['log_prob_l']\n\n    # Ensure cost_a < cost_b for valid preference pairs\n    delta_cost = cost_b - cost_a\n    delta_logp = logp_a - logp_b\n\n    # 1. Inherited Idea (Parent 0): Tanh normalization for a bounded, batch-independent signal\n    norm_delta_cost = torch.tanh(delta_cost / norm_scale)\n\n    # 2. New Coupling Idea: Explicitly clamp the normalized cost for added stability\n    # This is inspired by the clipping in Parent 1, but applied to the tanh output.\n    clipped_norm_delta_cost = torch.clamp(norm_delta_cost, -clip_value, clip_value)\n\n    # 3. Inherited Idea (Parents 0 & 1): Adaptive sigmoid margin\n    margin = max_margin * torch.sigmoid(clipped_norm_delta_cost)\n\n    # 4. Core Loss (Parents 0 & 1): Logistic loss with the adaptive margin\n    core_loss = -F.logsigmoid(delta_logp - margin)\n\n    # 5. New Coupling Idea: Rank-gating mechanism\n    # Instead of scaling by raw delta_cost, we scale by its percentile rank in the batch.\n    # This focuses training on the relatively most important pairs in the current batch.\n    with torch.no_grad(): # rank_gap is non-differentiable w.r.t. its input, used as a weight\n        cost_ranks = rank_gap(delta_cost)\n    rank_gate = F.softplus(beta * cost_ranks)\n\n    # 6. Combine the core loss with the rank-gate\n    loss_per_sample = rank_gate * core_loss\n\n    # 7. Apply optional sample weights if provided\n    sample_weights = batch.get('weight', None)\n    if sample_weights is not None:\n        loss_per_sample = loss_per_sample * sample_weights\n\n    # 8. Return the mean loss over the batch\n    return loss_per_sample.mean()"}, "static_ok": true, "static_reason": "", "dynamic_ok": false, "dynamic_reason": "forward_error: name 'rank_gap' is not defined", "loss_value": null, "grad_norm": null}
{"generation": 3, "index": 5, "ir": {"name": "Clipped Tanh-Normalized Adaptive Margin Loss", "intuition": "This loss function creates a robust, cost-sensitive preference objective by merging ideas from its parents and introducing a new coupling mechanism. \n\nInherited Ideas:\n- From Parent 0, it inherits the use of a scaled `tanh` function for batch-independent normalization of the cost difference. This provides a bounded and stable signal of the cost gap's magnitude.\n- From Parent 1, it inherits the idea of clipping the normalized signal (`clamp`) before it's used to compute the margin. This prevents extreme cost differences from creating overly dominant margin values, enhancing training stability.\n- Both parents contribute the core structure: a logistic loss (`-logsigmoid`) on the log-probability difference, an adaptive margin that is a `sigmoid` function of the (now clipped and tanh-normalized) cost difference, and a `softplus` weighting based on the raw cost difference to focus on more significant preference pairs.\n\nNew Coupling Idea:\n- The new idea is to couple the clipping range with the `tanh` normalization scale. The `clamp` bounds are set to `[-1.0, 1.0]`, which is the natural output range of the `tanh` function. This creates a more principled interaction: the `tanh` function smoothly squashes the cost difference into this range, and the `clamp` acts as a hard guard at the boundaries, ensuring perfect numerical stability and preventing any values from exceeding the theoretical `tanh` limits due to floating-point inaccuracies. This tight coupling makes the `clip_value` hyperparameter unnecessary, simplifying the loss design.", "pseudocode": "1. Calculate the cost difference: delta_cost = cost(b) - cost(a).\n2. Normalize the cost difference using a scaled tanh function: norm_delta_cost = tanh(delta_cost / norm_scale). This is inherited from Parent 0 for batch-independent normalization.\n3. Clip the normalized cost difference to the range [-1.0, 1.0]. This is inherited from Parent 1, but the range is now coupled with the output range of the tanh function, which is a new modification.\n4. Compute an adaptive margin: margin = max_margin * sigmoid(clipped_norm_delta_cost). This combines the sigmoid margin from both parents with the new clipped signal.\n5. Calculate the log-probability difference: delta_logp = logp(a) - logp(b).\n6. Compute the core preference loss: core_loss = -logsigmoid(delta_logp - margin). This is the standard logistic loss structure from both parents.\n7. Calculate a cost-based weight: weight_scale = softplus(beta * delta_cost). This weighting scheme is also inherited from both parents.\n8. Combine them: final_loss = weight_scale * core_loss.\n9. Return the mean of final_loss over the batch.", "hyperparams": {"beta": 1.0, "max_margin": 2.0, "norm_scale": 10.0}, "operators_used": ["logsigmoid", "softplus", "sigmoid", "tanh", "clamp"], "implementation_hint": {"expects": ["cost_a", "cost_b", "logp_a", "logp_b"], "returns": "scalar"}, "code": "import torch\nimport torch.nn.functional as F\n\ndef generated_loss(batch, model_output, extra):\n    \"\"\"\n    Clipped Tanh-Normalized Adaptive Margin Loss.\n\n    Args:\n        batch (dict): A dictionary containing tensors from the data loader.\n                      Expected keys: 'cost_a', 'cost_b', 'log_prob_w', 'log_prob_l'.\n                      'cost_a' and 'log_prob_w' correspond to the preferred solution (winner).\n                      'cost_b' and 'log_prob_l' correspond to the non-preferred solution (loser).\n                      An optional 'weight' tensor can be used for sample weighting.\n        model_output (dict): Output from the model (not used in this loss structure).\n        extra (dict): A dictionary for hyperparameters, e.g., {'beta': 1.0, 'max_margin': 2.0, 'norm_scale': 10.0}.\n\n    Returns:\n        torch.Tensor: A scalar tensor representing the mean loss.\n    \"\"\"\n    # Read hyperparameters\n    beta = extra.get('beta', 1.0)\n    max_margin = extra.get('max_margin', 2.0)\n    norm_scale = extra.get('norm_scale', 10.0)\n\n    # In our setting, 'a' is the winner (w) and 'b' is the loser (l).\n    # cost_a < cost_b\n    cost_a = batch['cost_a']\n    cost_b = batch['cost_b']\n    logp_a = batch['log_prob_w']\n    logp_b = batch['log_prob_l']\n\n    # 1. Calculate cost and log-probability differences\n    # delta_cost should be positive since cost_b > cost_a\n    delta_cost = cost_b - cost_a\n    delta_logp = logp_a - logp_b\n\n    # 2. Normalize the cost difference using a scaled tanh function (from Parent 0)\n    # This provides a bounded (-1, 1) and batch-independent normalization\n    norm_delta_cost = torch.tanh(delta_cost / norm_scale)\n\n    # 3. Clip the normalized cost difference for stability (from Parent 1)\n    # NEW COUPLING: The clip range is coupled with the tanh output range.\n    clipped_norm_delta_cost = torch.clamp(norm_delta_cost, -1.0, 1.0)\n\n    # 4. Compute the adaptive sigmoid margin from the clipped, normalized value (from both parents)\n    margin = max_margin * torch.sigmoid(clipped_norm_delta_cost)\n\n    # 5. Compute the core preference loss (from both parents)\n    core_loss = -F.logsigmoid(delta_logp - margin)\n\n    # 6. Compute a cost-based scaling factor using softplus (from both parents)\n    weight_scale = F.softplus(beta * delta_cost)\n\n    # 7. Combine core loss and scaling factor\n    loss_per_sample = weight_scale * core_loss\n\n    # 8. Apply optional sample weights if provided\n    sample_weights = batch.get('weight', None)\n    if sample_weights is not None:\n        loss_per_sample = loss_per_sample * sample_weights\n\n    # 9. Return the mean loss over the batch\n    return loss_per_sample.mean()"}, "static_ok": true, "static_reason": "", "dynamic_ok": true, "dynamic_reason": "ok", "loss_value": 1.7727785110473633, "grad_norm": 0.0}
{"generation": 3, "index": 6, "ir": {"name": "Clipped Tanh-Normalized Adaptive Margin Loss", "intuition": "This loss function combines the stable, batch-independent normalization from the 'Adaptive Sigmoid Margin Loss with Tanh Normalization' (Parent 0) with the robustness-enhancing clipping mechanism from the 'Clipped Adaptive Sigmoid Margin Loss' (Parent 1). Specifically, it inherits the use of a scaled `tanh` function to normalize the cost difference, which provides a bounded and predictable normalization that is not dependent on batch statistics. From Parent 1, it inherits the `clamp` operator, which is applied to the output of the tanh function before it is used to compute the margin. This acts as a secondary stability control, ensuring the input to the sigmoid function is well-behaved, even if the tanh scaling is miscalibrated. The core loss remains a logistic objective (`-logsigmoid`) where the model's log-probability difference is pushed to exceed this adaptive margin, and the entire loss term is scaled by a `softplus` of the raw cost difference, an idea common to both parents.\n\nAs a new coupling idea, this child loss introduces a dynamic `beta` schedule based on the tanh-normalized cost difference. The `beta` parameter, which scales the importance of the loss based on the raw cost gap, is now modulated by `tanh(delta_cost)`. This means that for pairs with very small cost differences, the `beta` scaling is reduced, preventing the model from being over-penalized on noisy or insignificant preferences. As the cost difference grows, `beta`'s effect ramps up towards its maximum configured value, smoothly focusing the training on more meaningful preference pairs. This creates a more nuanced weighting scheme than the fixed `beta` used by the parents.", "pseudocode": "1. Calculate the cost difference: delta_cost = cost(b) - cost(a).\n2. Normalize the cost difference using a scaled tanh function: norm_delta_cost = tanh(delta_cost / norm_scale). This is inherited from Parent 0.\n3. Clip the normalized cost difference to a range [-clip_value, clip_value] for added stability. This idea is inherited from Parent 1.\n4. Compute an adaptive margin based on the clipped, normalized value: margin = max_margin * sigmoid(clipped_norm_delta_cost).\n5. Calculate the log-probability difference: delta_logp = logp(a) - logp(b).\n6. Compute the core preference loss: core_loss = -logsigmoid(delta_logp - margin).\n7. (New Coupling) Compute a dynamic beta schedule: dynamic_beta = base_beta * norm_delta_cost. This reduces the weight for pairs with small cost differences.\n8. Calculate a cost-based weight using the dynamic beta: weight_scale = softplus(dynamic_beta * delta_cost). This is a modification of the weighting scheme from both parents.\n9. Combine them: final_loss = weight_scale * core_loss.\n10. Return the mean of final_loss over the batch.", "hyperparams": {"base_beta": 1.0, "max_margin": 2.0, "norm_scale": 10.0, "clip_value": 0.99}, "operators_used": ["logsigmoid", "softplus", "sigmoid", "tanh", "clamp"], "implementation_hint": {"expects": ["cost_a", "cost_b", "log_prob_w", "log_prob_l", "weight"], "returns": "scalar"}, "code": "import torch\nimport torch.nn.functional as F\n\ndef generated_loss(batch, model_output, extra):\n    \"\"\"\n    Clipped Tanh-Normalized Adaptive Margin Loss.\n\n    Args:\n        batch (dict): A dictionary containing tensors from the data loader.\n                      Expected keys: 'cost_a', 'cost_b', 'log_prob_w', 'log_prob_l'.\n                      'cost_a' and 'log_prob_w' correspond to the preferred solution (winner).\n                      'cost_b' and 'log_prob_l' correspond to the non-preferred solution (loser).\n                      An optional 'weight' tensor can be used for sample weighting.\n        model_output (dict): Output from the model (not used in this loss structure).\n        extra (dict): A dictionary for hyperparameters, e.g., {'base_beta': 1.0, 'max_margin': 2.0, 'norm_scale': 10.0, 'clip_value': 0.99}.\n\n    Returns:\n        torch.Tensor: A scalar tensor representing the mean loss.\n    \"\"\"\n    # Read hyperparameters\n    base_beta = extra.get('base_beta', 1.0)\n    max_margin = extra.get('max_margin', 2.0)\n    norm_scale = extra.get('norm_scale', 10.0)\n    clip_value = extra.get('clip_value', 0.99) # Clip inside tanh's range\n\n    # In our setting, 'a' is the winner (w) and 'b' is the loser (l).\n    # cost_a < cost_b\n    cost_a = batch['cost_a']\n    cost_b = batch['cost_b']\n    logp_a = batch['log_prob_w']\n    logp_b = batch['log_prob_l']\n\n    # 1. Calculate cost and log-probability differences\n    delta_cost = cost_b - cost_a\n    delta_logp = logp_a - logp_b\n\n    # 2. Normalize cost difference with tanh (inherited from Parent 0)\n    norm_delta_cost = torch.tanh(delta_cost / norm_scale)\n\n    # 3. Clip the normalized cost for stability (inherited from Parent 1)\n    # The clip_value should be <= 1 to be effective with tanh output.\n    clipped_norm_delta_cost = torch.clamp(norm_delta_cost, -clip_value, clip_value)\n\n    # 4. Compute the adaptive sigmoid margin\n    margin = max_margin * torch.sigmoid(clipped_norm_delta_cost)\n\n    # 5. Compute the core preference loss (Bradley-Terry style with margin)\n    core_loss = -F.logsigmoid(delta_logp - margin)\n\n    # 6. (New Coupling) Compute a dynamic beta, modulated by the normalized cost difference.\n    # This reduces the weight for pairs with very small cost differences, making beta's effect smoother.\n    dynamic_beta = base_beta * norm_delta_cost.abs() # Use abs to ensure beta is positive\n\n    # 7. Compute a cost-based scaling factor using dynamic beta and softplus\n    weight_scale = F.softplus(dynamic_beta * delta_cost)\n\n    # 8. Combine core loss and scaling factor\n    loss_per_sample = weight_scale * core_loss\n\n    # 9. Apply optional sample weights if provided\n    sample_weights = batch.get('weight', None)\n    if sample_weights is not None:\n        loss_per_sample = loss_per_sample * sample_weights\n\n    # 10. Return the mean loss over the batch\n    return loss_per_sample.mean()"}, "static_ok": true, "static_reason": "", "dynamic_ok": true, "dynamic_reason": "ok", "loss_value": 1.0046294927597046, "grad_norm": 0.0}
{"generation": 3, "index": 7, "ir": {"name": "Hybrid-Normalized Clipped Margin Loss with Rank-Gap Scaling", "intuition": "This loss function creates a robust preference signal by combining stable normalization techniques and dynamic loss scaling. It inherits the core concept of an adaptive, cost-sensitive margin from both parents, where the required log-probability gap increases with the cost difference. \n\nFrom Parent 1 (Adaptive Sigmoid Margin Loss with Tanh Normalization), it inherits the use of `tanh` for a bounded, batch-independent normalization of the cost difference, which prevents extreme cost gaps from creating unstable margin values. \n\nFrom Parent 2 (Clipped Adaptive Sigmoid Margin Loss), it inherits the `clamp` operator to clip the normalized cost difference. This adds an extra layer of stability, ensuring the input to the sigmoid function for margin calculation remains in a well-behaved range. \n\nTwo new coupling ideas are introduced: \n1. **Hybrid Normalization:** Instead of choosing between batch-dependent z-score and batch-independent tanh, we use both. The `tanh` provides a stable baseline normalization, and then a batch-wise `zscore` is applied to this already-bounded value. This allows the model to adapt to the distribution of cost gaps within the current batch while still being protected from extreme outlier values in the raw costs. \n2. **Rank-Gap Loss Scaling:** Instead of scaling the loss by the raw cost difference (via `softplus`), we use a `rank_gap` function. This discretizes the cost difference into a small number of ranks (e.g., 0, 1, 2). The loss for pairs with a small cost difference receives a baseline weight, while pairs with medium or large cost differences receive progressively higher weights. This focuses training on meaningfully different pairs without being overly sensitive to the exact magnitude of the cost gap, which can be noisy.", "pseudocode": "1. Calculate the cost difference: delta_cost = cost(b) - cost(a).\n2. Apply a bounded normalization to the cost difference using a scaled `tanh` function. This is inherited from Parent 1.\n3. Apply a batch-wise z-score normalization to the result of the previous step. This is a new coupling idea, creating a hybrid normalization.\n4. Clip the hybrid-normalized cost difference to a range [-clip_value, clip_value] for stability. This is inherited from Parent 2.\n5. Compute an adaptive margin based on the clipped, hybrid-normalized cost difference: margin = max_margin * sigmoid(clipped_norm_delta_cost).\n6. Calculate the log-probability difference: delta_logp = logp(a) - logp(b).\n7. Compute the core preference loss: core_loss = -logsigmoid(delta_logp - margin). This pushes delta_logp to be greater than the adaptive margin.\n8. Calculate a discrete loss scaling factor using the `rank_gap` function on the raw cost difference. This is a new coupling idea that replaces the continuous `softplus` scaling. For example, `weight_scale = rank_gap(delta_cost, thresholds=[t1, t2]) + 1`.\n9. Combine them: final_loss = weight_scale * core_loss.\n10. Return the mean of final_loss over the batch.", "hyperparams": {"max_margin": 2.0, "tanh_scale": 10.0, "clip_value": 3.0, "rank_thresholds": [0.1, 1.0], "eps": 1e-08}, "operators_used": ["logsigmoid", "sigmoid", "tanh", "zscore", "clamp", "rank_gap"], "implementation_hint": {"expects": ["cost_a", "cost_b", "logp_a", "logp_b", "weight"], "returns": "scalar"}, "code": "import torch\nimport torch.nn.functional as F\n\ndef rank_gap(x, thresholds):\n    \"\"\"Discretizes a tensor into ranks based on thresholds.\"\"\"\n    ranks = torch.zeros_like(x, dtype=torch.long)\n    for i, t in enumerate(thresholds):\n        ranks += (x > t).long()\n    return ranks.float()\n\ndef zscore(x, eps=1e-8):\n    \"\"\"Computes z-score for a tensor.\"\"\"\n    if x.numel() > 1 and x.std() > eps:\n        mean = x.mean()\n        std = x.std()\n        return (x - mean) / (std + eps)\n    return torch.zeros_like(x)\n\ndef generated_loss(batch, model_output, extra):\n    \"\"\"\n    Hybrid-Normalized Clipped Margin Loss with Rank-Gap Scaling.\n\n    Args:\n        batch (dict): A dictionary containing tensors from the data loader.\n                      Expected keys: 'cost_a', 'cost_b', 'log_prob_w', 'log_prob_l'.\n                      'cost_a' and 'log_prob_w' correspond to the preferred solution (winner).\n                      'cost_b' and 'log_prob_l' correspond to the non-preferred solution (loser).\n                      An optional 'weight' tensor can be used for sample weighting.\n        model_output (dict): Output from the model (not used in this loss structure).\n        extra (dict): A dictionary for hyperparameters.\n\n    Returns:\n        torch.Tensor: A scalar tensor representing the mean loss.\n    \"\"\"\n    # Read hyperparameters\n    max_margin = extra.get('max_margin', 2.0)\n    tanh_scale = extra.get('tanh_scale', 10.0)\n    clip_value = extra.get('clip_value', 3.0)\n    rank_thresholds = extra.get('rank_thresholds', [0.1, 1.0])\n    eps = extra.get('eps', 1e-8)\n\n    cost_a = batch['cost_a']\n    cost_b = batch['cost_b']\n    logp_a = batch['log_prob_w']\n    logp_b = batch['log_prob_l']\n\n    # 1. Calculate cost and log-probability differences\n    delta_cost = cost_b - cost_a\n    delta_logp = logp_a - logp_b\n\n    # 2. Inherit Tanh normalization from Parent 1\n    tanh_norm_delta_cost = torch.tanh(delta_cost / tanh_scale)\n\n    # 3. New Coupling: Apply z-score on top of tanh-normalized costs for hybrid normalization\n    hybrid_norm_delta_cost = zscore(tanh_norm_delta_cost, eps=eps)\n\n    # 4. Inherit Clipping from Parent 2 for stability\n    clipped_norm_delta_cost = torch.clamp(hybrid_norm_delta_cost, -clip_value, clip_value)\n\n    # 5. Compute adaptive sigmoid margin (idea from both parents)\n    margin = max_margin * torch.sigmoid(clipped_norm_delta_cost)\n\n    # 6. Compute the core preference loss (Bradley-Terry style with margin)\n    core_loss = -F.logsigmoid(delta_logp - margin)\n\n    # 7. New Coupling: Use rank_gap for discrete loss scaling instead of softplus\n    # The base weight is 1.0, increasing by 1 for each threshold crossed.\n    with torch.no_grad():\n        weight_scale = 1.0 + rank_gap(delta_cost, thresholds=rank_thresholds)\n    \n    # 8. Combine core loss and scaling factor\n    loss_per_sample = weight_scale * core_loss\n\n    # 9. Apply optional sample weights if provided\n    sample_weights = batch.get('weight', None)\n    if sample_weights is not None:\n        loss_per_sample = loss_per_sample * sample_weights\n\n    # 10. Return the mean loss over the batch\n    return loss_per_sample.mean()"}, "static_ok": true, "static_reason": "", "dynamic_ok": false, "dynamic_reason": "forward_error: name 'zscore' is not defined", "loss_value": null, "grad_norm": null}
{"generation": 4, "index": 0, "ir": {"name": "Progressive Rank-Gap Margin Loss", "intuition": "This loss function synthesizes a stable, adaptive margin with a novel progressive weighting scheme. It inherits the core logistic loss structure (`-logsigmoid`) and the use of a `tanh` function for cost normalization from both parents. This ensures the loss is grounded in a probabilistic framework and that the cost gap is mapped to a bounded, batch-independent range.\n\nTwo new coupling ideas are introduced. First, instead of a fixed margin or a simple sigmoid-based margin, this loss uses a `rank_gap` function on the tanh-normalized cost difference to create a step-wise, progressive margin. This means that pairs are categorized into discrete 'importance tiers' based on their cost difference (e.g., low, medium, high). The margin is constant within each tier but increases across tiers. This encourages the model to first separate easily distinguishable pairs before focusing on finer-grained preferences, creating a curriculum effect. Second, the loss introduces a new weighting mechanism based on the `softplus` of the log-probability difference (`delta_logp`). This `logp-aware` weighting scheme dynamically increases the penalty on pairs where the model is confidently wrong (i.e., `delta_logp` is large and negative), while reducing the penalty on pairs where the model is already close to correct or uncertain. This focuses training effort on correcting significant errors in the model's probability assignments, rather than just on the magnitude of the cost gap.", "pseudocode": "1. Calculate the cost difference: delta_cost = cost(b) - cost(a).\n2. Calculate the log-probability difference: delta_logp = logp(a) - logp(b).\n3. (Inherited) Normalize the cost difference using a scaled tanh function: norm_delta_cost = tanh(delta_cost / norm_scale).\n4. (New Coupling 1) Compute a progressive, step-wise margin using the `rank_gap` function on the normalized cost difference. The margin increases in discrete steps as the cost difference crosses predefined thresholds (quantiles).\n5. (Inherited) Calculate the core preference loss: core_loss = -logsigmoid(delta_logp - margin).\n6. (New Coupling 2) Compute a 'logp-aware' weight. The weight is calculated as softplus(-delta_logp * beta). This increases the loss for pairs where the model is confidently wrong (delta_logp is very negative).\n7. Combine the core loss and the weight: final_loss = weight * core_loss.\n8. Return the mean of final_loss over the batch.", "hyperparams": {"beta": 0.5, "norm_scale": 10.0, "margin_steps": [0.0, 0.5, 1.5, 3.0], "margin_quantiles": [0.0, 0.5, 0.8]}, "operators_used": ["logsigmoid", "softplus", "tanh", "rank_gap"], "implementation_hint": {"expects": ["cost_a", "cost_b", "log_prob_w", "log_prob_l"], "returns": "scalar"}, "code": "import torch\nimport torch.nn.functional as F\n\ndef rank_gap(x, quantiles, steps):\n    \"\"\" Helper to compute a step-wise function based on quantiles. \"\"\"\n    # Ensure quantiles are sorted and start with 0.0\n    sorted_quantiles = sorted(quantiles)\n    if sorted_quantiles[0] != 0.0:\n        sorted_quantiles = [0.0] + sorted_quantiles\n\n    # Ensure steps has one more element than quantiles\n    if len(steps) != len(sorted_quantiles) + 1:\n        raise ValueError(\"steps must have len(quantiles) + 1 elements\")\n\n    # Convert quantiles to a tensor\n    q = torch.tensor(sorted_quantiles, device=x.device, dtype=x.dtype)\n\n    # Find which quantile bucket each element of x falls into\n    # The result of searchsorted is the index of the quantile\n    indices = torch.searchsorted(q, x.abs(), right=True)\n\n    # Use these indices to select the corresponding step value\n    s = torch.tensor(steps, device=x.device, dtype=x.dtype)\n    return s[indices]\n\ndef generated_loss(batch, model_output, extra):\n    \"\"\"\n    Progressive Rank-Gap Margin Loss.\n\n    Args:\n        batch (dict): A dictionary containing tensors from the data loader.\n                      Expected keys: 'cost_a', 'cost_b', 'log_prob_w', 'log_prob_l'.\n                      An optional 'weight' tensor can be used for sample weighting.\n        model_output (dict): Output from the model (not used).\n        extra (dict): A dictionary for hyperparameters.\n\n    Returns:\n        torch.Tensor: A scalar tensor representing the mean loss.\n    \"\"\"\n    # Read hyperparameters\n    beta = extra.get('beta', 0.5)\n    norm_scale = extra.get('norm_scale', 10.0)\n    margin_steps = extra.get('margin_steps', [0.0, 0.5, 1.5, 3.0])\n    margin_quantiles = extra.get('margin_quantiles', [0.0, 0.5, 0.8])\n\n    # In our setting, 'a' is the winner (w) and 'b' is the loser (l).\n    cost_a = batch['cost_a']\n    cost_b = batch['cost_b']\n    logp_a = batch['log_prob_w']\n    logp_b = batch['log_prob_l']\n\n    # 1. Calculate cost and log-probability differences\n    delta_cost = cost_b - cost_a\n    delta_logp = logp_a - logp_b\n\n    # 2. Normalize cost difference with tanh (Inherited from both parents)\n    norm_delta_cost = torch.tanh(delta_cost / norm_scale)\n\n    # 3. (New Coupling 1) Compute progressive margin using rank_gap\n    # The margin steps up as the normalized cost difference crosses quantile thresholds.\n    margin = rank_gap(norm_delta_cost, margin_quantiles, margin_steps)\n\n    # 4. Compute the core preference loss (Inherited)\n    core_loss = -F.logsigmoid(delta_logp - margin)\n\n    # 5. (New Coupling 2) Compute 'logp-aware' weight\n    # This weight increases when the model is confidently wrong (delta_logp << 0),\n    # focusing training on correcting large prediction errors.\n    # softplus(-x) is a smooth approximation of relu(-x).\n    weight = F.softplus(-delta_logp * beta)\n\n    # 6. Combine core loss and the new weight\n    loss_per_sample = weight * core_loss\n\n    # 7. Apply optional sample weights if provided\n    sample_weights = batch.get('weight', None)\n    if sample_weights is not None:\n        loss_per_sample = loss_per_sample * sample_weights\n\n    # 8. Return the mean loss over the batch\n    return loss_per_sample.mean()"}, "static_ok": true, "static_reason": "", "dynamic_ok": false, "dynamic_reason": "forward_error: name 'rank_gap' is not defined", "loss_value": null, "grad_norm": null}
{"generation": 4, "index": 1, "ir": {"name": "Rank-Gapped Adaptive Sigmoid Loss", "intuition": "This loss function synthesizes the adaptive margin concept from both parents with a new, robust normalization and weighting scheme. It inherits the core structure of an adaptive margin loss, where the margin is a function of the cost difference, and this margin is used within a logistic loss (`-logsigmoid`). The margin is computed via `max_margin * sigmoid(...)`, a concept present in both parents, which ensures that a larger cost gap demands a larger log-probability separation.\n\nThis child introduces two key coupling ideas. First, instead of using `tanh` or `zscore` for normalization, it uses `rank_gap`. This non-parametric normalization method converts the raw cost differences into ranks within the batch and then scales them to a [-1, 1] range. This makes the margin adaptive not only to the magnitude of the cost difference but also to its relative importance within the current batch, providing robustness against outliers and varying cost scales. Second, it introduces a new weighting mechanism. The `softplus(beta * delta_cost)` weighting from the parents is replaced with a `softplus` applied to the *log* of the cost difference. This `softplus(beta * log(1 + delta_cost))` weighting dampens the influence of extremely large cost differences, preventing a few outlier pairs from dominating the batch gradient, while still emphasizing more significant preferences over less significant ones. The `log` transformation creates a more balanced weighting across the batch.", "pseudocode": "1. Calculate the cost difference: delta_cost = cost(b) - cost(a).\n2. (New Coupling 1) Normalize the cost difference using rank-gap normalization across the batch: norm_delta_cost = rank_gap(delta_cost). This maps cost differences to a [-1, 1] range based on their batch-wise rank, making the normalization robust to outliers.\n3. (Inherited) Compute an adaptive margin based on the normalized cost: margin = max_margin * sigmoid(norm_delta_cost). This idea is inherited from both parents.\n4. Calculate the log-probability difference: delta_logp = logp(a) - logp(b).\n5. Compute the core preference loss: core_loss = -logsigmoid(delta_logp - margin).\n6. (New Coupling 2) Compute a log-dampened cost-based weight: weight_scale = softplus(beta * log(1 + delta_cost)). This weighting scheme increases with the cost gap but at a decelerating rate, reducing the dominance of pairs with very large cost differences.\n7. Combine them: final_loss = weight_scale * core_loss.\n8. Return the mean of final_loss over the batch.", "hyperparams": {"beta": 1.0, "max_margin": 2.0}, "operators_used": ["logsigmoid", "softplus", "sigmoid", "log", "rank_gap"], "implementation_hint": {"expects": ["cost_a", "cost_b", "logp_a", "logp_b"], "returns": "scalar"}, "code": "import torch\nimport torch.nn.functional as F\n\ndef rank_gap(x):\n    \"\"\"Non-parametric normalization based on batch ranks.\"\"\"\n    if x.numel() <= 1:\n        return torch.zeros_like(x)\n    ranks = x.argsort().argsort().float()\n    # Scale ranks to [-1, 1]\n    return 2 * (ranks / (x.numel() - 1)) - 1\n\ndef generated_loss(batch, model_output, extra):\n    \"\"\"\n    Rank-Gapped Adaptive Sigmoid Loss.\n\n    Args:\n        batch (dict): A dictionary containing tensors from the data loader.\n                      Expected keys: 'cost_a', 'cost_b', 'log_prob_w', 'log_prob_l'.\n                      'cost_a' and 'log_prob_w' correspond to the preferred solution (winner).\n                      'cost_b' and 'log_prob_l' correspond to the non-preferred solution (loser).\n                      An optional 'weight' tensor can be used for sample weighting.\n        model_output (dict): Output from the model (not used in this loss structure).\n        extra (dict): A dictionary for hyperparameters, e.g., {'beta': 1.0, 'max_margin': 2.0}.\n\n    Returns:\n        torch.Tensor: A scalar tensor representing the mean loss.\n    \"\"\"\n    # Read hyperparameters\n    beta = extra.get('beta', 1.0)\n    max_margin = extra.get('max_margin', 2.0)\n\n    # In our setting, 'a' is the winner (w) and 'b' is the loser (l).\n    # cost_a < cost_b\n    cost_a = batch['cost_a']\n    cost_b = batch['cost_b']\n    logp_a = batch['log_prob_w']\n    logp_b = batch['log_prob_l']\n\n    # 1. Calculate cost and log-probability differences\n    delta_cost = cost_b - cost_a\n    delta_logp = logp_a - logp_b\n\n    # 2. (New Coupling 1) Normalize cost difference using rank_gap for outlier robustness.\n    # This is a non-parametric, batch-dependent normalization.\n    with torch.no_grad():\n        norm_delta_cost = rank_gap(delta_cost)\n\n    # 3. (Inherited) Compute the adaptive sigmoid margin.\n    # The margin is a function of the rank-normalized cost difference.\n    margin = max_margin * torch.sigmoid(norm_delta_cost)\n\n    # 4. Compute the core preference loss (Bradley-Terry style with margin)\n    core_loss = -F.logsigmoid(delta_logp - margin)\n\n    # 5. (New Coupling 2) Compute a log-dampened scaling factor.\n    # This reduces the influence of extreme cost differences compared to linear scaling.\n    # Add 1 for numerical stability (log(0) is -inf).\n    weight_scale = F.softplus(beta * torch.log1p(delta_cost))\n\n    # 6. Combine core loss and scaling factor\n    loss_per_sample = weight_scale * core_loss\n\n    # 7. Apply optional sample weights if provided\n    sample_weights = batch.get('weight', None)\n    if sample_weights is not None:\n        loss_per_sample = loss_per_sample * sample_weights\n\n    # 8. Return the mean loss over the batch\n    return loss_per_sample.mean()"}, "static_ok": true, "static_reason": "", "dynamic_ok": false, "dynamic_reason": "forward_error: name 'rank_gap' is not defined", "loss_value": null, "grad_norm": null}
{"generation": 4, "index": 2, "ir": {"name": "Progressive Rank-Gap Margin Loss", "intuition": "This loss function synthesizes ideas from its parents to create a robust, cost-sensitive preference objective, and introduces two new coupling mechanisms for improved stability and learning dynamics.\n\nInherited Ideas:\n- **Adaptive Sigmoid Margin (from both parents):** It inherits the core concept of an adaptive margin that is a sigmoid function of a normalized cost difference. This allows the required separation between preferred and non-preferred solutions to scale with the magnitude of their cost difference.\n- **Tanh Normalization (from both parents):** It uses a scaled `tanh` function to normalize the cost difference. This provides a stable, batch-independent normalization that maps the cost gap to a bounded range, which is crucial for the adaptive margin calculation.\n\nNew Coupling Ideas:\n1.  **Progressive Beta Scaling:** Instead of a fixed `beta` for weighting the loss, this child introduces a progressive scaling mechanism. `beta` is modulated by `sigmoid(delta_logp)`, the model's current confidence in the preference. When the model is very certain (high `delta_logp`), `beta`'s effect is maximized, focusing on refining high-confidence predictions. When the model is uncertain or wrong (low `delta_logp`), `beta` is down-weighted, preventing large, potentially unstable gradients from pairs the model struggles with. This acts as a form of gradient modulation based on model confidence.\n2.  **Rank-Gap Margin Coupling:** The second innovation is to couple the adaptive margin with the batch-wise rank gap of the cost differences. The `rank_gap` operator computes the difference in the percentile rank of `cost_b` and `cost_a` within the batch. This rank-based term is added to the `tanh`-normalized cost gap before it is fed into the sigmoid function for the margin. This makes the margin sensitive not only to the absolute cost difference (`tanh` term) but also to the *relative* importance of that difference within the current batch (`rank_gap` term). A pair with a large cost difference that is also one of the largest in the batch will be assigned a significantly higher margin, creating a stronger separation target.", "pseudocode": "1. Calculate the cost difference: delta_cost = cost(b) - cost(a).\n2. Calculate the log-probability difference: delta_logp = logp(a) - logp(b).\n3. (Inherited) Normalize the cost difference using a scaled tanh function: tanh_norm_cost = tanh(delta_cost / norm_scale).\n4. (New Coupling) Compute the batch-wise rank gap of the costs: rank_gap_cost = rank_gap(cost_b, cost_a).\n5. Combine the absolute and relative normalizations: combined_norm = tanh_norm_cost + rank_gap_cost.\n6. (Inherited) Compute an adaptive margin based on the combined normalization: margin = max_margin * sigmoid(combined_norm).\n7. Compute the core preference loss: core_loss = -logsigmoid(delta_logp - margin).\n8. (New Coupling) Compute a progressive beta based on model confidence: progressive_beta = base_beta * sigmoid(delta_logp).\n9. Calculate a cost-based weight using the progressive beta: weight_scale = softplus(progressive_beta * delta_cost).\n10. Combine them: final_loss = weight_scale * core_loss.\n11. Return the mean of final_loss over the batch.", "hyperparams": {"base_beta": 1.0, "max_margin": 2.0, "norm_scale": 10.0}, "operators_used": ["logsigmoid", "softplus", "sigmoid", "tanh", "rank_gap"], "implementation_hint": {"expects": ["cost_a", "cost_b", "logp_a", "logp_b"], "returns": "scalar"}, "code": "import torch\nimport torch.nn.functional as F\n\ndef rank_gap(x, y):\n    \"\"\"Computes the percentile rank difference between elements of x and y in the combined batch.\"\"\"\n    # Detach to treat costs as fixed for ranking, preventing gradients through the ranking process itself.\n    x_detached = x.detach()\n    y_detached = y.detach()\n    combined = torch.cat([x_detached, y_detached])\n    # Sort and find ranks\n    sorted_vals, _ = torch.sort(combined)\n    # Add a small epsilon for stability with searchsorted\n    rank_x = torch.searchsorted(sorted_vals, x_detached, right=True).float() / len(combined)\n    rank_y = torch.searchsorted(sorted_vals, y_detached, right=True).float() / len(combined)\n    # The gap should be positive since y (cost_b) > x (cost_a)\n    return (rank_y - rank_x).clamp(min=0)\n\ndef generated_loss(batch, model_output, extra):\n    \"\"\"\n    Progressive Rank-Gap Margin Loss.\n\n    Args:\n        batch (dict): A dictionary containing tensors from the data loader.\n                      Expected keys: 'cost_a', 'cost_b', 'log_prob_w', 'log_prob_l'.\n                      'cost_a' and 'log_prob_w' correspond to the preferred solution (winner).\n                      'cost_b' and 'log_prob_l' correspond to the non-preferred solution (loser).\n                      An optional 'weight' tensor can be used for sample weighting.\n        model_output (dict): Output from the model (not used in this loss structure).\n        extra (dict): A dictionary for hyperparameters, e.g., {'base_beta': 1.0, 'max_margin': 2.0, 'norm_scale': 10.0}.\n\n    Returns:\n        torch.Tensor: A scalar tensor representing the mean loss.\n    \"\"\"\n    # Read hyperparameters\n    base_beta = extra.get('base_beta', 1.0)\n    max_margin = extra.get('max_margin', 2.0)\n    norm_scale = extra.get('norm_scale', 10.0)\n\n    # In our setting, 'a' is the winner (w) and 'b' is the loser (l).\n    # cost_a < cost_b\n    cost_a = batch['cost_a']\n    cost_b = batch['cost_b']\n    logp_a = batch['log_prob_w']\n    logp_b = batch['log_prob_l']\n\n    # 1. Calculate cost and log-probability differences\n    delta_cost = cost_b - cost_a\n    delta_logp = logp_a - logp_b\n\n    # 2. Inherited Idea: Normalize cost difference with tanh\n    tanh_norm_cost = torch.tanh(delta_cost / norm_scale)\n\n    # 3. New Coupling: Compute batch-wise rank gap of costs\n    # This makes the margin sensitive to the relative cost difference within the batch.\n    rank_gap_cost = rank_gap(cost_b, cost_a)\n\n    # 4. Combine absolute (tanh) and relative (rank_gap) normalizations for the margin input\n    combined_norm = tanh_norm_cost + rank_gap_cost\n\n    # 5. Inherited Idea: Compute an adaptive sigmoid margin using the combined normalization\n    margin = max_margin * torch.sigmoid(combined_norm)\n\n    # 6. Compute the core preference loss (Bradley-Terry style with margin)\n    core_loss = -F.logsigmoid(delta_logp - margin)\n\n    # 7. New Coupling: Progressive beta scaling based on model confidence (sigmoid(delta_logp))\n    # We detach delta_logp here so this only acts as a gradient *magnitude* modulator, not a source of gradients.\n    progressive_beta = base_beta * torch.sigmoid(delta_logp.detach())\n\n    # 8. Compute a cost-based scaling factor using softplus and the progressive beta\n    weight_scale = F.softplus(progressive_beta * delta_cost)\n\n    # 9. Combine core loss and scaling factor\n    loss_per_sample = weight_scale * core_loss\n\n    # 10. Apply optional sample weights if provided\n    sample_weights = batch.get('weight', None)\n    if sample_weights is not None:\n        loss_per_sample = loss_per_sample * sample_weights\n\n    # 11. Return the mean loss over the batch\n    return loss_per_sample.mean()"}, "static_ok": true, "static_reason": "", "dynamic_ok": false, "dynamic_reason": "forward_error: name 'rank_gap' is not defined", "loss_value": null, "grad_norm": null}
{"generation": 4, "index": 3, "ir": {"name": "Rank-Gap Modulated Tanh-Margin Loss", "intuition": "This loss function synthesizes a stable, adaptive margin approach with a novel rank-based modulation scheme. It inherits the core logistic loss structure (`-logsigmoid`) and the use of a `tanh`-normalized cost difference to create a bounded, adaptive margin, a robust technique present in both parents ('Clipped Tanh-Normalized Adaptive Margin Loss' and 'Adaptive Sigmoid Margin Loss with Tanh Normalization'). This ensures the required log-probability gap smoothly scales with the cost difference.\n\nAs a new coupling idea, this child loss introduces two significant modifications. First, it replaces the `softplus` weighting with a direct, bounded `tanh` weighting on the raw cost difference (`tanh(beta * delta_cost)`). This prevents extremely large cost differences from creating disproportionately massive loss values, improving numerical stability and preventing a few outlier pairs from dominating the gradient. Second, and more importantly, it introduces a `rank_gap` modulation on the log-probability difference itself. We calculate the `rank_gap` of the model's log-probabilities within the batch, which measures the percentile difference between the winner and loser. This rank gap is then used to scale down the `delta_logp` term. The effect is that if a 'winner' is already ranked much higher than the 'loser' within the batch context, the loss signal for this pair is attenuated. This new coupling encourages the model to focus its capacity on correcting mis-ordered pairs or those where the preference is not yet clearly established in the model's current ranking, rather than continuing to push apart pairs that are already well-separated.", "pseudocode": "1. Calculate the cost difference: delta_cost = cost(b) - cost(a).\n2. Calculate the log-probability difference: delta_logp = logp(a) - logp(b).\n3. (Inherited from both parents) Normalize the cost difference using a scaled tanh function to create a bounded signal: norm_delta_cost = tanh(delta_cost / norm_scale).\n4. (Inherited from both parents) Compute an adaptive margin based on the normalized cost: margin = max_margin * sigmoid(norm_delta_cost).\n5. (New Coupling 1) Compute a batch-wise rank gap for the log-probabilities: rank_gap_val = rank_gap(logp(a), logp(b)). This value is between 0 and 1.\n6. (New Coupling 1) Modulate the log-probability difference by the rank gap: modulated_delta_logp = delta_logp * (1.0 - rank_gap_val). This reduces the effective logp difference for pairs that are already well-separated in the batch ranking.\n7. Compute the core preference loss using the modulated logp difference: core_loss = -logsigmoid(modulated_delta_logp - margin).\n8. (New Coupling 2) Compute a bounded, cost-based weight using a tanh function instead of softplus for stability: weight_scale = tanh(beta * delta_cost).\n9. Combine them: final_loss = weight_scale * core_loss.\n10. Return the mean of final_loss over the batch.", "hyperparams": {"beta": 0.1, "max_margin": 2.0, "norm_scale": 10.0}, "operators_used": ["logsigmoid", "sigmoid", "tanh", "rank_gap"], "implementation_hint": {"expects": ["cost_a", "cost_b", "logp_a", "logp_b"], "returns": "scalar"}, "code": "import torch\nimport torch.nn.functional as F\n\ndef rank_gap(win_scores, lose_scores):\n    \"\"\"Calculates the rank gap between winner and loser scores in a batch.\"\"\"\n    scores = torch.cat([win_scores, lose_scores])\n    # Invert scores so that higher scores get lower ranks (e.g., rank 1 is best)\n    ranks = (-scores).argsort().argsort()\n    win_ranks = ranks[:len(win_scores)]\n    lose_ranks = ranks[len(win_scores):]\n    # Rank gap is the difference in percentile ranks.\n    # A small gap means they are close in the batch-wise ranking.\n    # A large gap (near 1.0) means they are far apart.\n    return (lose_ranks - win_ranks).float() / (len(scores) - 1)\n\ndef generated_loss(batch, model_output, extra):\n    \"\"\"\n    Rank-Gap Modulated Tanh-Margin Loss.\n\n    Args:\n        batch (dict): A dictionary containing tensors from the data loader.\n                      Expected keys: 'cost_a', 'cost_b', 'log_prob_w', 'log_prob_l'.\n                      'cost_a' and 'log_prob_w' correspond to the preferred solution (winner).\n                      'cost_b' and 'log_prob_l' correspond to the non-preferred solution (loser).\n                      An optional 'weight' tensor can be used for sample weighting.\n        model_output (dict): Output from the model (not used in this loss structure).\n        extra (dict): A dictionary for hyperparameters, e.g., {'beta': 0.1, 'max_margin': 2.0, 'norm_scale': 10.0}.\n\n    Returns:\n        torch.Tensor: A scalar tensor representing the mean loss.\n    \"\"\"\n    # Read hyperparameters\n    beta = extra.get('beta', 0.1)\n    max_margin = extra.get('max_margin', 2.0)\n    norm_scale = extra.get('norm_scale', 10.0)\n\n    # In our setting, 'a' is the winner (w) and 'b' is the loser (l).\n    cost_a = batch['cost_a']\n    cost_b = batch['cost_b']\n    logp_a = batch['log_prob_w']\n    logp_b = batch['log_prob_l']\n\n    # 1. Calculate cost and log-probability differences\n    delta_cost = cost_b - cost_a\n    delta_logp = logp_a - logp_b\n\n    # 2. (Inherited) Normalize cost diff with tanh for a bounded signal\n    norm_delta_cost = torch.tanh(delta_cost / norm_scale)\n\n    # 3. (Inherited) Compute adaptive margin from normalized cost\n    margin = max_margin * torch.sigmoid(norm_delta_cost)\n\n    # 4. (New Coupling 1) Modulate delta_logp based on rank gap\n    # If a pair is already well-separated in rank, reduce the pressure to push them further.\n    with torch.no_grad(): # Don't backprop through the rank calculation itself\n        rg = rank_gap(logp_a, logp_b)\n    # The (1 - rg) factor is close to 1 for mis-ordered pairs, and close to 0 for well-ordered pairs.\n    modulated_delta_logp = delta_logp * (1.0 - rg)\n\n    # 5. Compute the core preference loss with the modulated logp difference\n    core_loss = -F.logsigmoid(modulated_delta_logp - margin)\n\n    # 6. (New Coupling 2) Use a bounded tanh weighting instead of unbounded softplus\n    # This stabilizes the loss by preventing huge weights from large cost differences.\n    weight_scale = torch.tanh(beta * delta_cost)\n\n    # 7. Combine core loss and scaling factor\n    loss_per_sample = weight_scale * core_loss\n\n    # 8. Apply optional sample weights if provided\n    sample_weights = batch.get('weight', None)\n    if sample_weights is not None:\n        loss_per_sample = loss_per_sample * sample_weights\n\n    # 9. Return the mean loss over the batch\n    return loss_per_sample.mean()"}, "static_ok": true, "static_reason": "", "dynamic_ok": false, "dynamic_reason": "forward_error: name 'rank_gap' is not defined", "loss_value": null, "grad_norm": null}
{"generation": 4, "index": 4, "ir": {"name": "Rank-Gapped Adaptive Tanh Margin Loss", "intuition": "This loss function synthesizes a stable, adaptive margin with a rank-aware weighting scheme. It inherits the core logistic loss (`-logsigmoid`) and the concept of an adaptive margin from both parents. Specifically, it adopts the `tanh` normalization of the cost difference from Parent 1 to create a bounded, batch-independent signal for the margin, and the `clamp` operator from Parent 0 to ensure this signal remains in a stable range before being used.\n\nTwo new coupling ideas are introduced. First, instead of using `softplus` to weight the loss, we introduce a `rank_gap` based weighting. The `rank_gap` of the cost difference is computed across the batch, which measures the percentile rank of a given cost difference among all others. This value is then passed through a `softplus` function. This novel weighting scheme prioritizes pairs with a relatively large cost difference *within the current batch*, making the loss focus on the most informative comparisons dynamically. Second, the `beta` parameter, which scales this rank-based weight, is now modulated by the training progress (e.g., step count). This `beta` schedule starts low and increases over time, allowing the model to learn basic preferences first before being more aggressively pushed to respect the relative importance of different preference pairs.", "pseudocode": "1. Calculate the cost difference: delta_cost = cost(b) - cost(a).\n2. Calculate the log-probability difference: delta_logp = logp(a) - logp(b).\n3. (Inherited from Parent 1) Normalize the cost difference using a scaled tanh function: norm_delta_cost = tanh(delta_cost / norm_scale).\n4. (Inherited from Parent 0) Clip the normalized cost difference for stability: clipped_norm_delta_cost = clamp(norm_delta_cost, -clip_value, clip_value).\n5. Compute an adaptive margin based on the clipped, normalized value: margin = max_margin * sigmoid(clipped_norm_delta_cost).\n6. Compute the core preference loss: core_loss = -logsigmoid(delta_logp - margin).\n7. (New Coupling 1) Compute a rank-based weight. First, find the rank percentile of each delta_cost within the batch using `rank_gap`. Then, scale this rank using `softplus` to create a smooth, non-negative weight: weight_scale = softplus(beta * rank_gap(delta_cost)).\n8. (New Coupling 2) The `beta` hyperparameter is now scheduled. It is calculated as a function of the current training step, starting low and increasing towards a `max_beta` value, e.g., `beta = max_beta * tanh(current_step / schedule_steps)`.\n9. Combine the core loss and the rank-based weight: final_loss = weight_scale * core_loss.\n10. Return the mean of final_loss over the batch.", "hyperparams": {"max_beta": 2.0, "max_margin": 2.0, "norm_scale": 10.0, "clip_value": 0.99, "beta_schedule_steps": 10000}, "operators_used": ["logsigmoid", "softplus", "sigmoid", "tanh", "clamp", "rank_gap"], "implementation_hint": {"expects": ["cost_a", "cost_b", "logp_a", "logp_b"], "returns": "scalar"}, "code": "import torch\nimport torch.nn.functional as F\n\ndef rank_gap(x):\n    \"\"\"Computes the rank percentile of each element in a 1D tensor.\"\"\"\n    if x.numel() <= 1:\n        return torch.ones_like(x)\n    # Sort the tensor and find the rank indices\n    _, sorted_indices = torch.sort(x)\n    ranks = torch.empty_like(sorted_indices, dtype=x.dtype)\n    ranks[sorted_indices] = torch.arange(x.numel(), device=x.device, dtype=x.dtype)\n    # Normalize ranks to [0, 1]\n    normalized_ranks = ranks / (x.numel() - 1)\n    return normalized_ranks\n\ndef generated_loss(batch, model_output, extra):\n    \"\"\"\n    Rank-Gapped Adaptive Tanh Margin Loss.\n\n    Args:\n        batch (dict): A dictionary containing tensors from the data loader.\n                      Expected keys: 'cost_a', 'cost_b', 'log_prob_w', 'log_prob_l'.\n                      'cost_a' and 'log_prob_w' correspond to the preferred solution (winner).\n                      'cost_b' and 'log_prob_l' correspond to the non-preferred solution (loser).\n                      An optional 'weight' tensor can be used for sample weighting.\n        model_output (dict): Output from the model (not used in this loss structure).\n        extra (dict): A dictionary for hyperparameters and dynamic values like 'current_step'.\n\n    Returns:\n        torch.Tensor: A scalar tensor representing the mean loss.\n    \"\"\"\n    # Read hyperparameters\n    max_beta = extra.get('max_beta', 2.0)\n    max_margin = extra.get('max_margin', 2.0)\n    norm_scale = extra.get('norm_scale', 10.0)\n    clip_value = extra.get('clip_value', 0.99)\n    beta_schedule_steps = extra.get('beta_schedule_steps', 10000)\n    current_step = extra.get('current_step', beta_schedule_steps) # Default to max beta if step not provided\n\n    # In our setting, 'a' is the winner (w) and 'b' is the loser (l).\n    cost_a = batch['cost_a']\n    cost_b = batch['cost_b']\n    logp_a = batch['log_prob_w']\n    logp_b = batch['log_prob_l']\n\n    # 1. Calculate cost and log-probability differences\n    delta_cost = cost_b - cost_a\n    delta_logp = logp_a - logp_b\n\n    # 2. Normalize cost difference with tanh (Inherited from Parent 1)\n    norm_delta_cost = torch.tanh(delta_cost / norm_scale)\n\n    # 3. Clip the normalized cost for stability (Inherited from Parent 0)\n    clipped_norm_delta_cost = torch.clamp(norm_delta_cost, -clip_value, clip_value)\n\n    # 4. Compute the adaptive sigmoid margin\n    margin = max_margin * torch.sigmoid(clipped_norm_delta_cost)\n\n    # 5. Compute the core preference loss (Bradley-Terry style with margin)\n    core_loss = -F.logsigmoid(delta_logp - margin)\n\n    # 6. (New Coupling 1) Beta schedule\n    # Beta increases as training progresses, focusing more on rank later.\n    schedule_progress = torch.tensor(current_step / beta_schedule_steps, device=delta_cost.device)\n    beta = max_beta * torch.tanh(schedule_progress)\n\n    # 7. (New Coupling 2) Compute a rank-based scaling factor.\n    # This amplifies the loss for pairs with a larger cost difference relative to the batch.\n    cost_rank = rank_gap(delta_cost)\n    weight_scale = F.softplus(beta * cost_rank)\n\n    # 8. Combine core loss and scaling factor\n    loss_per_sample = weight_scale * core_loss\n\n    # 9. Apply optional sample weights if provided\n    sample_weights = batch.get('weight', None)\n    if sample_weights is not None:\n        loss_per_sample = loss_per_sample * sample_weights\n\n    # 10. Return the mean loss over the batch\n    return loss_per_sample.mean()"}, "static_ok": true, "static_reason": "", "dynamic_ok": false, "dynamic_reason": "forward_error: name 'rank_gap' is not defined", "loss_value": null, "grad_norm": null}
{"generation": 4, "index": 5, "ir": {"name": "Rank-Gapped Exponential Margin Loss", "intuition": "This loss function synthesizes ideas from its parents to create a robust, cost-sensitive preference objective, while introducing new coupling mechanisms for improved stability and learning dynamics. \n\nInherited Ideas:\n- From both parents, it inherits the core structure of a logistic loss (`-logsigmoid`) where the model's log-probability difference is pushed to exceed a dynamic margin. \n- From both parents, it also inherits the idea of weighting the loss by a function of the raw cost difference (`delta_cost`), although it uses a different functional form (`exp` instead of `softplus`) for the weighting. \n\nNew Couplings & Modifications:\n1.  **Exponential Margin:** Instead of a sigmoid-based margin, this child uses an exponential function: `margin = exp(norm_delta_cost) - 1`. This creates a margin that grows more aggressively as the cost difference increases, strongly enforcing preferences for pairs with large cost gaps. The `-1` term ensures the margin is zero when the cost difference is zero. \n2.  **Rank-Gap Normalization:** Instead of `tanh` normalization, it introduces a novel batch-aware normalization using `rank_gap`. The cost difference `delta_cost` is first converted to its rank percentile within the batch, and then `rank_gap` is applied to this rank. This makes the normalization robust to outliers in the raw cost values and focuses on the relative importance of a preference pair within the current batch. The `rank_gap` operator (defined as `2*rank - 1`) maps the rank percentile `[0, 1]` to a `[-1, 1]` range, similar to `tanh`, providing a stable input for the exponential margin calculation. This couples the margin of each sample to the distribution of cost differences in the batch.", "pseudocode": "1. Calculate the cost difference: delta_cost = cost(b) - cost(a).\n2. Calculate the log-probability difference: delta_logp = logp(a) - logp(b).\n3. (New Coupling) Normalize the cost difference using rank-gap: \n   a. Compute the rank percentile of each delta_cost within the batch.\n   b. Apply the rank_gap operator (2*rank - 1) to map ranks to the [-1, 1] range. This gives norm_delta_cost.\n4. (New Coupling) Compute an exponential margin: margin = exp(norm_delta_cost) - 1. This is inherited from neither parent and replaces the sigmoid-based margin.\n5. Compute the core preference loss: core_loss = -logsigmoid(delta_logp - margin). This structure is inherited from both parents.\n6. Calculate a cost-based weight: weight_scale = exp(beta * norm_delta_cost). This is a modification of the softplus weighting from the parents, using the normalized cost difference instead of the raw one to control the weight's scale.\n7. Combine them: final_loss = weight_scale * core_loss.\n8. Return the mean of final_loss over the batch.", "hyperparams": {"beta": 0.5}, "operators_used": ["logsigmoid", "exp", "rank_gap"], "implementation_hint": {"expects": ["cost_a", "cost_b", "logp_a", "logp_b"], "returns": "scalar"}, "code": "import torch\nimport torch.nn.functional as F\n\n# Helper for the custom operator\ndef rank_gap(x):\n    \"\"\"Computes the rank percentile of each element and maps it to [-1, 1].\"\"\"\n    if x.numel() == 0:\n        return x\n    # Compute ranks (handle ties by averaging)\n    ranks = x.argsort().argsort().float()\n    # Normalize to [0, 1] rank percentile\n    if x.numel() > 1:\n        rank_percentile = ranks / (x.numel() - 1)\n    else:\n        rank_percentile = torch.zeros_like(ranks)\n    # Map to [-1, 1]\n    return 2 * rank_percentile - 1\n\ndef generated_loss(batch, model_output, extra):\n    \"\"\"\n    Rank-Gapped Exponential Margin Loss.\n\n    Args:\n        batch (dict): A dictionary containing tensors from the data loader.\n                      Expected keys: 'cost_a', 'cost_b', 'log_prob_w', 'log_prob_l'.\n                      An optional 'weight' tensor can be used for sample weighting.\n        model_output (dict): Output from the model (not used in this loss structure).\n        extra (dict): A dictionary for hyperparameters, e.g., {'beta': 0.5}.\n\n    Returns:\n        torch.Tensor: A scalar tensor representing the mean loss.\n    \"\"\"\n    # Read hyperparameters\n    beta = extra.get('beta', 0.5)\n\n    # In our setting, 'a' is the winner (w) and 'b' is the loser (l).\n    cost_a = batch['cost_a']\n    cost_b = batch['cost_b']\n    logp_a = batch['log_prob_w']\n    logp_b = batch['log_prob_l']\n\n    # 1. Calculate cost and log-probability differences\n    delta_cost = cost_b - cost_a\n    delta_logp = logp_a - logp_b\n\n    # 2. (New Coupling) Normalize cost difference using rank_gap\n    # This makes the normalization robust to outliers and batch-distribution-aware.\n    with torch.no_grad(): # Ranks are non-differentiable but act as a fixed transform\n        norm_delta_cost = rank_gap(delta_cost)\n\n    # 3. (New Coupling) Compute an exponential margin\n    # This creates a more aggressive margin for higher-ranked cost differences.\n    # exp(x)-1 ensures margin is 0 for rank 0.5 (norm_delta_cost=0) and positive for >0.5.\n    margin = torch.exp(norm_delta_cost) - 1.0\n\n    # 4. Compute the core preference loss (structure inherited from parents)\n    core_loss = -F.logsigmoid(delta_logp - margin)\n\n    # 5. Compute a cost-based scaling factor (modified from parents)\n    # Uses an exponential weight based on the normalized cost difference.\n    # This prevents extreme weights from raw delta_cost outliers.\n    weight_scale = torch.exp(beta * norm_delta_cost)\n\n    # 6. Combine core loss and scaling factor\n    loss_per_sample = weight_scale * core_loss\n\n    # 7. Apply optional sample weights if provided\n    sample_weights = batch.get('weight', None)\n    if sample_weights is not None:\n        loss_per_sample = loss_per_sample * sample_weights\n\n    # 8. Return the mean loss over the batch\n    return loss_per_sample.mean()"}, "static_ok": true, "static_reason": "", "dynamic_ok": false, "dynamic_reason": "forward_error: name 'rank_gap' is not defined", "loss_value": null, "grad_norm": null}
{"generation": 4, "index": 6, "ir": {"name": "Adaptive Rank-Gap Margin Loss", "intuition": "This loss function creates a dynamic, cost-sensitive margin that is robust to outliers in cost distributions. It inherits two key ideas from its parents: the use of `tanh` for a bounded, batch-independent normalization of the cost difference (from Parent 1), and the application of a `clamp` operator for enhanced stability (from Parent 0). The core of the loss remains a logistic objective (`-logsigmoid`) where the model's log-probability difference is encouraged to exceed this adaptive margin.\n\nTwo new coupling ideas are introduced. First, instead of a sigmoid function, the margin is computed using the `rank_gap` operator applied to the tanh-normalized cost differences. This makes the margin for a given pair dependent on its relative cost difference within the batch, providing a form of adaptive batch-level normalization. This focuses the model on correctly ordering the preferences within the current batch, making the margin more context-aware. Second, a dynamic `beta` schedule is introduced, where `beta` (the scaling factor for the loss) is modulated by `softplus` of the log-probability difference. This means that when the model is already very confident about a preference (large `delta_logp`), the loss is up-weighted to reinforce this correct signal. Conversely, when the model is uncertain or wrong (small or negative `delta_logp`), the loss is down-weighted, preventing large, potentially unstable gradients from incorrect or noisy pairs. This focuses training on refining already good predictions and avoids over-penalizing initial uncertainty.", "pseudocode": "1. Calculate the cost difference: delta_cost = cost(b) - cost(a).\n2. (Inherited) Normalize the cost difference using a scaled tanh function for bounded, batch-independent normalization: norm_delta_cost = tanh(delta_cost / norm_scale).\n3. (Inherited) Clip the normalized cost difference to a range [-clip_value, clip_value] for added stability: clipped_norm_delta_cost = clamp(norm_delta_cost, -clip_value, clip_value).\n4. (New Coupling 1) Compute an adaptive margin using the rank_gap operator on the clipped, normalized cost differences. This makes the margin for each pair dependent on its relative cost gap within the batch: margin = max_margin * rank_gap(clipped_norm_delta_cost).\n5. Calculate the log-probability difference: delta_logp = logp(a) - logp(b).\n6. Compute the core preference loss: core_loss = -logsigmoid(delta_logp - margin).\n7. (New Coupling 2) Compute a dynamic beta schedule based on the model's confidence. This increases the loss weight for confident, correct predictions: dynamic_beta = base_beta * softplus(delta_logp).\n8. Combine them: final_loss = dynamic_beta * core_loss. Note that the traditional cost-based weighting is replaced by this confidence-based weighting.\n9. Return the mean of final_loss over the batch.", "hyperparams": {"base_beta": 0.5, "max_margin": 1.5, "norm_scale": 15.0, "clip_value": 0.99}, "operators_used": ["logsigmoid", "softplus", "tanh", "clamp", "rank_gap"], "implementation_hint": {"expects": ["cost_a", "cost_b", "log_prob_w", "log_prob_l"], "returns": "scalar"}, "code": "import torch\nimport torch.nn.functional as F\n\ndef rank_gap(x):\n    \"\"\"Computes the rank-based gap: (rank(x) - 1) / (len(x) - 1) for x in [0, 1].\"\"\"\n    if x.numel() <= 1:\n        return torch.zeros_like(x)\n    # Sort the tensor and get the original indices\n    _, sorted_indices = torch.sort(x)\n    # Create a tensor of ranks (0 to N-1)\n    ranks = torch.empty_like(sorted_indices, dtype=torch.float)\n    ranks[sorted_indices] = torch.arange(x.numel(), device=x.device, dtype=x.dtype)\n    # Normalize ranks to be in [0, 1]\n    normalized_ranks = ranks / (x.numel() - 1)\n    return normalized_ranks\n\ndef generated_loss(batch, model_output, extra):\n    \"\"\"\n    Adaptive Rank-Gap Margin Loss.\n\n    Args:\n        batch (dict): A dictionary containing tensors from the data loader.\n                      Expected keys: 'cost_a', 'cost_b', 'log_prob_w', 'log_prob_l'.\n                      'cost_a' and 'log_prob_w' correspond to the preferred solution (winner).\n                      'cost_b' and 'log_prob_l' correspond to the non-preferred solution (loser).\n                      An optional 'weight' tensor can be used for sample weighting.\n        model_output (dict): Output from the model (not used in this loss structure).\n        extra (dict): A dictionary for hyperparameters, e.g., {'base_beta': 0.5, 'max_margin': 1.5, 'norm_scale': 15.0, 'clip_value': 0.99}.\n\n    Returns:\n        torch.Tensor: A scalar tensor representing the mean loss.\n    \"\"\"\n    # Read hyperparameters\n    base_beta = extra.get('base_beta', 0.5)\n    max_margin = extra.get('max_margin', 1.5)\n    norm_scale = extra.get('norm_scale', 15.0)\n    clip_value = extra.get('clip_value', 0.99)\n\n    # In our setting, 'a' is the winner (w) and 'b' is the loser (l).\n    # cost_a < cost_b\n    cost_a = batch['cost_a']\n    cost_b = batch['cost_b']\n    logp_a = batch['log_prob_w']\n    logp_b = batch['log_prob_l']\n\n    # 1. Calculate cost and log-probability differences\n    delta_cost = cost_b - cost_a\n    delta_logp = logp_a - logp_b\n\n    # 2. Normalize cost difference with tanh (inherited)\n    norm_delta_cost = torch.tanh(delta_cost / norm_scale)\n    \n    # 3. Clip the normalized cost for stability (inherited)\n    clipped_norm_delta_cost = torch.clamp(norm_delta_cost, -clip_value, clip_value)\n\n    # 4. (New Coupling 1) Compute adaptive margin using rank_gap on the batch\n    # This makes the margin context-aware within the batch.\n    margin = max_margin * rank_gap(clipped_norm_delta_cost)\n\n    # 5. Compute the core preference loss (Bradley-Terry style with margin)\n    core_loss = -F.logsigmoid(delta_logp - margin)\n\n    # 6. (New Coupling 2) Compute a dynamic beta based on model's confidence (delta_logp)\n    # This up-weights pairs where the model is already confident and correct.\n    # softplus(x) is a smooth approximation of relu(x), ensuring the weight is non-negative.\n    dynamic_beta = base_beta * F.softplus(delta_logp)\n    \n    # 7. Combine core loss and dynamic beta. Detach beta to avoid influencing gradients through it.\n    loss_per_sample = dynamic_beta.detach() * core_loss\n\n    # 8. Apply optional sample weights if provided\n    sample_weights = batch.get('weight', None)\n    if sample_weights is not None:\n        loss_per_sample = loss_per_sample * sample_weights\n\n    # 9. Return the mean loss over the batch\n    return loss_per_sample.mean()"}, "static_ok": true, "static_reason": "", "dynamic_ok": false, "dynamic_reason": "forward_error: name 'rank_gap' is not defined", "loss_value": null, "grad_norm": null}
{"generation": 4, "index": 7, "ir": {"name": "Tanh-Normalized Margin Loss with Dynamic Beta and Rank-Gap Modulation", "intuition": "This loss function synthesizes the stable normalization and adaptive margin concepts from its parents while introducing two new coupling mechanisms for improved robustness and training dynamics.\n\nInherited Ideas:\n- From Parent 1 (Adaptive Sigmoid Margin Loss with Tanh Normalization), it inherits the core idea of using a scaled `tanh` function to normalize the cost difference (`delta_cost`). This provides a smooth, bounded, and batch-independent normalization that is robust to cost outliers.\n- From Parent 0 (Clipped Tanh-Normalized Adaptive Margin Loss), it inherits the concept of a dynamic `beta` that is modulated by the normalized cost difference. This allows the loss to focus more on significant preferences and less on pairs with very small, potentially noisy cost gaps.\n\nNew Coupling Ideas:\n1.  **Rank-Gap Modulation on Margin**: Instead of using the normalized cost difference directly to create the margin (via `sigmoid`), this child loss first computes the `rank_gap` of the normalized cost differences across the batch. The margin is then made a function of this rank-gap. This makes the margin adaptive not only to the magnitude of the cost difference but also to its relative standing within the batch. Pairs that are 'easy' (large cost gap relative to others in the batch) will have a higher margin, while 'hard' pairs (small cost gap relative to others) will have a lower one. This focuses the model's capacity on distinguishing between competitively-costed solutions.\n2.  **Softplus-Clipped Beta**: The dynamic beta, inherited from Parent 0, is further stabilized. After being calculated, it is passed through a `softplus` function and then `clamp`ed to a maximum value. This ensures beta is always positive and prevents it from growing uncontrollably for pairs with extremely large cost differences, which could otherwise lead to exploding gradients and unstable training.", "pseudocode": "1. Calculate the cost difference: delta_cost = cost(b) - cost(a).\n2. (Inherited from Parent 1) Normalize the cost difference using a scaled tanh function: norm_delta_cost = tanh(delta_cost / norm_scale).\n3. (New Coupling 1) Calculate the rank-gap of the normalized cost differences across the batch: cost_rank_gap = rank_gap(norm_delta_cost). This value is between 0 and 1.\n4. Compute the adaptive margin based on the rank-gap: margin = max_margin * cost_rank_gap. This makes the margin dependent on the relative difficulty of the preference pair within the batch.\n5. Calculate the log-probability difference: delta_logp = logp(a) - logp(b).\n6. Compute the core preference loss: core_loss = -logsigmoid(delta_logp - margin).\n7. (Inherited from Parent 0) Compute a dynamic beta based on the normalized cost difference: raw_dynamic_beta = base_beta * norm_delta_cost.\n8. (New Coupling 2) Stabilize the dynamic beta by applying softplus and then clamping it to a maximum value: dynamic_beta = clamp(softplus(raw_dynamic_beta), max=max_beta).\n9. Calculate a cost-based weight using the stabilized dynamic beta: weight_scale = dynamic_beta * delta_cost.\n10. Combine them: final_loss = weight_scale * core_loss.\n11. Return the mean of final_loss over the batch.", "hyperparams": {"base_beta": 1.0, "max_beta": 5.0, "max_margin": 2.0, "norm_scale": 10.0}, "operators_used": ["logsigmoid", "softplus", "tanh", "clamp", "rank_gap"], "implementation_hint": {"expects": ["cost_a", "cost_b", "logp_a", "logp_b"], "returns": "scalar"}, "code": "import torch\nimport torch.nn.functional as F\n\ndef rank_gap(x):\n    \"\"\"Computes the rank-based gap (0 to 1) for each element in a 1D tensor.\"\"\"\n    if x.numel() <= 1:\n        return torch.ones_like(x)\n    ranks = torch.argsort(torch.argsort(x)).float()\n    return ranks / (x.numel() - 1)\n\ndef generated_loss(batch, model_output, extra):\n    \"\"\"\n    Tanh-Normalized Margin Loss with Dynamic Beta and Rank-Gap Modulation.\n\n    Args:\n        batch (dict): A dictionary containing tensors from the data loader.\n                      Expected keys: 'cost_a', 'cost_b', 'log_prob_w', 'log_prob_l'.\n                      An optional 'weight' tensor can be used for sample weighting.\n        model_output (dict): Output from the model (not used).\n        extra (dict): Hyperparameters, e.g., {'base_beta': 1.0, 'max_beta': 5.0, 'max_margin': 2.0, 'norm_scale': 10.0}.\n\n    Returns:\n        torch.Tensor: A scalar tensor representing the mean loss.\n    \"\"\"\n    # Read hyperparameters\n    base_beta = extra.get('base_beta', 1.0)\n    max_beta = extra.get('max_beta', 5.0)\n    max_margin = extra.get('max_margin', 2.0)\n    norm_scale = extra.get('norm_scale', 10.0)\n\n    # 'a' is the winner (w), 'b' is the loser (l), so cost_a < cost_b\n    cost_a = batch['cost_a']\n    cost_b = batch['cost_b']\n    logp_a = batch['log_prob_w']\n    logp_b = batch['log_prob_l']\n\n    # Ensure tensors are float for stability\n    delta_cost = (cost_b - cost_a).float()\n    delta_logp = (logp_a - logp_b).float()\n\n    # 1. (Inherited from Parent 1) Tanh-normalize the cost difference\n    norm_delta_cost = torch.tanh(delta_cost / norm_scale)\n\n    # 2. (New Coupling 1) Compute margin from the rank-gap of normalized costs\n    # This makes the margin relative to batch difficulty\n    cost_rank_gap = rank_gap(norm_delta_cost)\n    margin = max_margin * cost_rank_gap\n    \n    # 3. Compute core preference loss with the rank-gap based margin\n    core_loss = -F.logsigmoid(delta_logp - margin)\n\n    # 4. (Inherited from Parent 0) Compute a dynamic beta\n    raw_dynamic_beta = base_beta * norm_delta_cost\n\n    # 5. (New Coupling 2) Stabilize beta with softplus and clamp\n    # This ensures beta is positive and prevents it from exploding\n    dynamic_beta = torch.clamp(F.softplus(raw_dynamic_beta), max=max_beta)\n\n    # 6. Compute a cost-based scaling factor using the stabilized dynamic beta\n    # Unlike parents' softplus(beta*cost), this is a direct scaling, but beta is already smooth\n    weight_scale = dynamic_beta * delta_cost\n\n    # 7. Combine core loss and scaling factor\n    loss_per_sample = weight_scale * core_loss\n    \n    # 8. Apply optional sample weights if provided\n    sample_weights = batch.get('weight', None)\n    if sample_weights is not None:\n        loss_per_sample = loss_per_sample * sample_weights.float()\n\n    # 9. Return the mean loss over the batch\n    return loss_per_sample.mean()"}, "static_ok": true, "static_reason": "", "dynamic_ok": false, "dynamic_reason": "forward_error: name 'rank_gap' is not defined", "loss_value": null, "grad_norm": null}
{"generation": 5, "index": 0, "ir": {"name": "Rank-Gapped Adaptive Tanh-Margin Loss", "intuition": "This loss function synthesizes the stable, batch-independent normalization and adaptive margin concepts from its parents, while introducing two new coupling ideas for improved robustness and focus. \n\nInherited Ideas:\n- From both parents, it inherits the core idea of an adaptive margin that is a function of the cost difference (`delta_cost = cost_b - cost_a`). This allows the loss to be lenient on pairs with small cost gaps and stricter on pairs with large gaps.\n- It specifically inherits the use of a scaled `tanh` function (`tanh(delta_cost / norm_scale)`) for normalizing the cost difference (from Parent 1). This provides a smooth, bounded, and batch-independent normalization, which is more stable than batch-level statistics.\n- The core loss structure remains a logistic loss (`-logsigmoid(delta_logp - margin)`), pushing the model's log-probability difference to exceed the computed margin.\n\nNew Coupling Ideas:\n1.  **Rank-Gap Normalization of Log-Probabilities:** Instead of using the raw log-probability difference `delta_logp`, the loss first normalizes the batch-wise `delta_logp` values using a `rank_gap` operator. This operator rescales the values to be in the range [0, 1] based on their rank order. This new coupling makes the loss less sensitive to the absolute magnitude of log-probability differences and more focused on their relative ordering within the batch. It helps prevent outlier log-probability values from dominating the gradient and stabilizes training, especially in early stages or with poorly calibrated models.\n2.  **Margin Clipping for Stability:** A `clamp` operation is applied to the final margin value. This is a stability trick that ensures the margin does not become excessively large, even if `max_margin` is set high or the `tanh` function saturates. It caps the 'difficulty' of any given preference pair, preventing the model from being forced to produce an impractically large log-probability separation and avoiding potential gradient explosions.", "pseudocode": "1. Calculate the cost difference: delta_cost = cost(b) - cost(a).\n2. Calculate the log-probability difference: raw_delta_logp = logp(a) - logp(b).\n3. (New Coupling 1) Normalize the log-probability differences across the batch using rank-gap normalization: delta_logp_norm = rank_gap(raw_delta_logp). This rescales the differences to [0, 1] based on their rank.\n4. (Inherited) Normalize the cost difference using a scaled tanh function: norm_delta_cost = tanh(delta_cost / norm_scale).\n5. (Inherited) Compute an adaptive margin based on the normalized cost difference: margin = max_margin * norm_delta_cost.\n6. (New Coupling 2) Clip the margin to a maximum value for stability: clipped_margin = clamp(margin, min=0, max=clip_margin).\n7. Compute the core preference loss using the normalized log-probability difference and the clipped margin: core_loss = -logsigmoid(delta_logp_norm - clipped_margin).\n8. (Inherited) Calculate a cost-based weight to amplify loss on more significant pairs: weight_scale = softplus(beta * delta_cost).\n9. Combine them: final_loss = weight_scale * core_loss.\n10. Return the mean of final_loss over the batch.", "hyperparams": {"beta": 1.0, "max_margin": 1.5, "norm_scale": 10.0, "clip_margin": 1.0}, "operators_used": ["logsigmoid", "softplus", "tanh", "clamp", "rank_gap"], "implementation_hint": {"expects": ["cost_a", "cost_b", "logp_a", "logp_b"], "returns": "scalar"}, "code": "import torch\nimport torch.nn.functional as F\n\n# Helper function for the new operator\ndef rank_gap(x):\n    \"\"\"Normalizes a tensor by its rank to the range [0, 1].\"\"\"\n    if x.numel() <= 1:\n        return torch.ones_like(x) * 0.5 # Return midpoint for single-element or empty batch\n    # argsort twice gives the rank of each element\n    ranks = x.argsort().argsort().float()\n    # Normalize ranks to [0, 1]\n    # Subtract 0.5 and divide by N-1 to handle single-element case gracefully\n    # and map ranks [0, N-1] to a range centered around 0.5\n    return ranks / (x.numel() - 1 + 1e-9)\n\ndef generated_loss(batch, model_output, extra):\n    \"\"\"\n    Rank-Gapped Adaptive Tanh-Margin Loss.\n\n    Args:\n        batch (dict): A dictionary containing tensors from the data loader.\n                      Expected keys: 'cost_a', 'cost_b', 'log_prob_w', 'log_prob_l'.\n                      'cost_a' and 'log_prob_w' correspond to the preferred solution (winner).\n                      'cost_b' and 'log_prob_l' correspond to the non-preferred solution (loser).\n                      An optional 'weight' tensor can be used for sample weighting.\n        model_output (dict): Output from the model (not used in this loss structure).\n        extra (dict): A dictionary for hyperparameters, e.g., {'beta': 1.0, 'max_margin': 1.5, 'norm_scale': 10.0, 'clip_margin': 1.0}.\n\n    Returns:\n        torch.Tensor: A scalar tensor representing the mean loss.\n    \"\"\"\n    # Read hyperparameters\n    beta = extra.get('beta', 1.0)\n    max_margin = extra.get('max_margin', 1.5)\n    norm_scale = extra.get('norm_scale', 10.0)\n    clip_margin = extra.get('clip_margin', 1.0)\n\n    # In our setting, 'a' is the winner (w) and 'b' is the loser (l).\n    # cost_a < cost_b\n    cost_a = batch['cost_a']\n    cost_b = batch['cost_b']\n    logp_a = batch['log_prob_w']\n    logp_b = batch['log_prob_l']\n\n    # 1. Calculate cost and raw log-probability differences\n    delta_cost = cost_b - cost_a\n    raw_delta_logp = logp_a - logp_b\n\n    # 2. (New Coupling 1) Normalize logp difference using rank_gap\n    # This focuses on the relative ordering of logp differences in the batch.\n    delta_logp_norm = rank_gap(raw_delta_logp)\n\n    # 3. (Inherited) Normalize the cost difference using a scaled tanh function\n    norm_delta_cost = torch.tanh(delta_cost / norm_scale)\n\n    # 4. (Inherited) Compute the adaptive margin based on normalized cost\n    # We use norm_delta_cost directly, as it's already in [-1, 1]\n    margin = max_margin * norm_delta_cost\n\n    # 5. (New Coupling 2) Clip the margin for stability\n    # Ensures the margin target is not excessively large.\n    clipped_margin = torch.clamp(margin, min=0.0, max=clip_margin)\n\n    # 6. Compute the core preference loss with normalized logp and clipped margin\n    # The target is for the rank-normalized logp diff to exceed the margin.\n    core_loss = -F.logsigmoid(delta_logp_norm - clipped_margin)\n\n    # 7. (Inherited) Compute a cost-based scaling factor using softplus\n    weight_scale = F.softplus(beta * delta_cost)\n\n    # 8. Combine core loss and scaling factor\n    loss_per_sample = weight_scale * core_loss\n\n    # 9. Apply optional sample weights if provided\n    sample_weights = batch.get('weight', None)\n    if sample_weights is not None:\n        loss_per_sample = loss_per_sample * sample_weights\n\n    # 10. Return the mean loss over the batch\n    return loss_per_sample.mean()"}, "static_ok": true, "static_reason": "", "dynamic_ok": false, "dynamic_reason": "forward_error: name 'rank_gap' is not defined", "loss_value": null, "grad_norm": null}
{"generation": 5, "index": 1, "ir": {"name": "Adaptive Logit-Scaled Margin Loss with Tanh Normalization", "intuition": "This loss function aims to create a more direct and interpretable link between the cost difference and the learning signal. It inherits two core ideas from its parents: 1) The use of a scaled `tanh` function to normalize the cost difference, providing a stable, batch-independent measure of preference strength (from Parent 1, 'Adaptive Sigmoid Margin Loss with Tanh Normalization'). 2) The overall structure of a cost-weighted logistic loss, where `softplus(beta * delta_cost)` scales the core loss term, focusing training on pairs with larger cost gaps (common to both parents).\n\nThe first new coupling idea is the margin calculation. Instead of using a sigmoid or another bounded function, the margin is computed by applying an inverse sigmoid (logit) transformation to the tanh-normalized cost difference. Specifically, `margin = log(norm_delta_cost / (1 - norm_delta_cost))`. This creates a margin that grows much more rapidly and becomes unbounded as the normalized cost difference approaches 1. The intuition is that for very confident preferences (large cost gap), the model should be pushed to create an extremely large log-probability separation, rather than just exceeding a bounded margin. A `clamp` is used on the input to the `log` to ensure numerical stability when `norm_delta_cost` is close to 0 or 1.\n\nThe second new coupling idea is a dynamic `beta` schedule that is inversely related to the log-probability difference. We calculate `dynamic_beta = base_beta * exp(-gamma * |delta_logp|)`. When the model is already very confident about a preference pair (large `|delta_logp|`), `dynamic_beta` is down-weighted. Conversely, when the model is uncertain (small `|delta_logp|`), `dynamic_beta` is larger. This focuses the `softplus` weighting mechanism on the examples where the model's predictions are still weak or incorrect, preventing the loss from being dominated by already well-learned examples that have large cost differences.", "pseudocode": "1. Calculate the cost difference: delta_cost = cost(b) - cost(a).\n2. Calculate the log-probability difference: delta_logp = logp(a) - logp(b).\n3. (Inherited from Parent 1) Normalize the cost difference using a scaled tanh function: norm_delta_cost = tanh(delta_cost / norm_scale).\n4. (New Coupling 1) Compute an adaptive margin using an inverse sigmoid (logit-like) transformation. First, clamp the normalized cost to avoid log(0) or division by zero: clamped_norm = clamp(norm_delta_cost, eps, 1-eps). Then, compute margin = log(clamped_norm / (1 - clamped_norm)). This creates a margin that grows rapidly as the cost gap increases.\n5. Compute the core preference loss: core_loss = -logsigmoid(delta_logp - margin).\n6. (New Coupling 2) Compute a dynamic beta that decreases as the model's confidence (`|delta_logp|`) increases: dynamic_beta = base_beta * exp(-gamma * abs(delta_logp)).\n7. (Inherited from both parents) Calculate a cost-based weight using the dynamic beta: weight_scale = softplus(dynamic_beta * delta_cost).\n8. Combine them: final_loss = weight_scale * core_loss.\n9. Return the mean of final_loss over the batch.", "hyperparams": {"base_beta": 1.0, "norm_scale": 10.0, "gamma": 0.5, "eps": 1e-08}, "operators_used": ["logsigmoid", "softplus", "tanh", "clamp", "log", "exp"], "implementation_hint": {"expects": ["cost_a", "cost_b", "logp_a", "logp_b"], "returns": "scalar"}, "code": "import torch\nimport torch.nn.functional as F\n\ndef generated_loss(batch, model_output, extra):\n    \"\"\"\n    Adaptive Logit-Scaled Margin Loss with Tanh Normalization.\n\n    Args:\n        batch (dict): A dictionary containing tensors from the data loader.\n                      Expected keys: 'cost_a', 'cost_b', 'log_prob_w', 'log_prob_l'.\n                      'cost_a' and 'log_prob_w' correspond to the preferred solution (winner).\n                      'cost_b' and 'log_prob_l' correspond to the non-preferred solution (loser).\n                      An optional 'weight' tensor can be used for sample weighting.\n        model_output (dict): Output from the model (not used in this loss structure).\n        extra (dict): A dictionary for hyperparameters, e.g., {'base_beta': 1.0, 'norm_scale': 10.0, 'gamma': 0.5, 'eps': 1e-8}.\n\n    Returns:\n        torch.Tensor: A scalar tensor representing the mean loss.\n    \"\"\"\n    # Read hyperparameters\n    base_beta = extra.get('base_beta', 1.0)\n    norm_scale = extra.get('norm_scale', 10.0)\n    gamma = extra.get('gamma', 0.5)\n    eps = extra.get('eps', 1e-8)\n\n    # In our setting, 'a' is the winner (w) and 'b' is the loser (l).\n    # cost_a < cost_b\n    cost_a = batch['cost_a']\n    cost_b = batch['cost_b']\n    logp_a = batch['log_prob_w']\n    logp_b = batch['log_prob_l']\n\n    # 1. Calculate cost and log-probability differences\n    delta_cost = cost_b - cost_a\n    delta_logp = logp_a - logp_b\n\n    # 2. Normalize cost difference with tanh (inherited from Parent 1)\n    # This maps the cost difference to (0, 1) since delta_cost > 0\n    norm_delta_cost = torch.tanh(delta_cost / norm_scale)\n\n    # 3. (New Coupling 1) Compute an adaptive margin using a logit-like transformation.\n    # Clamp is crucial for numerical stability near 0 and 1.\n    clamped_norm = torch.clamp(norm_delta_cost, min=eps, max=1.0 - eps)\n    margin = torch.log(clamped_norm / (1.0 - clamped_norm))\n\n    # 4. Compute the core preference loss (Bradley-Terry style with margin)\n    core_loss = -F.logsigmoid(delta_logp - margin)\n\n    # 5. (New Coupling 2) Compute a dynamic beta based on model confidence.\n    # This down-weights examples where the model is already confident.\n    dynamic_beta = base_beta * torch.exp(-gamma * torch.abs(delta_logp.detach()))\n\n    # 6. Compute a cost-based scaling factor using dynamic beta and softplus (inherited from both parents)\n    weight_scale = F.softplus(dynamic_beta * delta_cost)\n\n    # 7. Combine core loss and scaling factor\n    loss_per_sample = weight_scale * core_loss\n\n    # 8. Apply optional sample weights if provided\n    sample_weights = batch.get('weight', None)\n    if sample_weights is not None:\n        loss_per_sample = loss_per_sample * sample_weights\n\n    # 9. Return the mean loss over the batch\n    return loss_per_sample.mean()"}, "static_ok": true, "static_reason": "", "dynamic_ok": true, "dynamic_reason": "ok", "loss_value": 0.1378815621137619, "grad_norm": 0.0}
{"generation": 5, "index": 2, "ir": {"name": "Adaptive Margin Loss with Tanh-Softplus Coupling", "intuition": "This loss function synthesizes the core concepts of its parents while introducing a novel coupling mechanism for improved stability and training dynamics. It inherits the foundational Bradley-Terry structure where the log-probability difference is encouraged to exceed a margin, specifically using `-logsigmoid(delta_logp - margin)` as seen in both parents. It also adopts the idea of an adaptive margin that is a function of the cost difference, and the `softplus(beta * delta_cost)` weighting scheme, both common to its parents. The use of `tanh` for cost normalization is also inherited from Parent 1 (`Adaptive Sigmoid Margin Loss with Tanh Normalization`).\n\nThe first new coupling idea is in the margin calculation. Instead of `sigmoid(tanh(delta_cost))`, this child uses `softplus(tanh(delta_cost))`. This creates a margin that is non-saturating for positive cost differences, allowing the required log-probability gap to grow more naturally with the cost gap, while still being bounded at zero for negative or zero cost gaps. This prevents the model from being forced to learn a large, potentially noisy, separation for pairs with extremely large cost differences, which `sigmoid` would enforce by saturating near `max_margin`.\n\nAs a second new coupling, the loss introduces a `log_prob_gap` term. This term, `log(1 + exp(delta_logp))`, is subtracted from the final loss. It acts as a gentle regularizer that penalizes the model for making the log-probability difference excessively large, especially when the preference is already correctly satisfied. This encourages the model to allocate its probability mass more efficiently, preventing it from becoming overconfident on easy examples and improving generalization. This term is inspired by the idea of preventing overfitting but is implemented in a novel, smooth, and differentiable way.", "pseudocode": "1. Calculate the cost difference: delta_cost = cost(b) - cost(a).\n2. Calculate the log-probability difference: delta_logp = logp(a) - logp(b).\n3. (Inherited from Parent 1) Normalize the cost difference using a scaled tanh function: norm_delta_cost = tanh(delta_cost / norm_scale).\n4. (New Coupling 1) Compute an adaptive margin using a softplus activation on the normalized cost: margin = max_margin * softplus(norm_delta_cost). This creates a non-saturating margin for positive cost gaps.\n5. (Inherited from both) Compute the core preference loss: core_loss = -logsigmoid(delta_logp - margin).\n6. (Inherited from both) Calculate a cost-based weight: weight_scale = softplus(beta * delta_cost).\n7. Compute the scaled loss: scaled_loss = weight_scale * core_loss.\n8. (New Coupling 2) Compute a log-probability gap regularizer: log_prob_gap = log(1 + exp(delta_logp)). This term is approximately delta_logp for large positive delta_logp, and near zero otherwise.\n9. Combine the scaled loss and the regularizer: final_loss = scaled_loss - regularizer_strength * log_prob_gap.\n10. Return the mean of final_loss over the batch.", "hyperparams": {"beta": 1.0, "max_margin": 1.0, "norm_scale": 10.0, "regularizer_strength": 0.01}, "operators_used": ["logsigmoid", "softplus", "tanh", "log", "exp"], "implementation_hint": {"expects": ["cost_a", "cost_b", "logp_a", "logp_b"], "returns": "scalar"}, "code": "import torch\nimport torch.nn.functional as F\n\ndef generated_loss(batch, model_output, extra):\n    \"\"\"\n    Adaptive Margin Loss with Tanh-Softplus Coupling and a Log-Prob Gap Regularizer.\n\n    Args:\n        batch (dict): A dictionary containing tensors from the data loader.\n                      Expected keys: 'cost_a', 'cost_b', 'log_prob_w', 'log_prob_l'.\n                      'cost_a' and 'log_prob_w' correspond to the preferred solution (winner).\n                      'cost_b' and 'log_prob_l' correspond to the non-preferred solution (loser).\n                      An optional 'weight' tensor can be used for sample weighting.\n        model_output (dict): Output from the model (not used in this loss structure).\n        extra (dict): A dictionary for hyperparameters, e.g., {'beta': 1.0, 'max_margin': 1.0, 'norm_scale': 10.0, 'regularizer_strength': 0.01}.\n\n    Returns:\n        torch.Tensor: A scalar tensor representing the mean loss.\n    \"\"\"\n    # Read hyperparameters\n    beta = extra.get('beta', 1.0)\n    max_margin = extra.get('max_margin', 1.0)\n    norm_scale = extra.get('norm_scale', 10.0)\n    regularizer_strength = extra.get('regularizer_strength', 0.01)\n\n    # In our setting, 'a' is the winner (w) and 'b' is the loser (l).\n    # cost_a < cost_b\n    cost_a = batch['cost_a']\n    cost_b = batch['cost_b']\n    logp_a = batch['log_prob_w']\n    logp_b = batch['log_prob_l']\n\n    # 1. Calculate cost and log-probability differences\n    delta_cost = cost_b - cost_a\n    delta_logp = logp_a - logp_b\n\n    # 2. Normalize cost difference with tanh (inherited from Parent 1)\n    norm_delta_cost = torch.tanh(delta_cost / norm_scale)\n\n    # 3. (New Coupling 1) Compute adaptive margin using softplus on the tanh-normalized cost.\n    # This creates a non-saturating margin for positive delta_cost, unlike the sigmoid margin in parents.\n    margin = max_margin * F.softplus(norm_delta_cost)\n\n    # 4. Compute the core preference loss (inherited from both parents)\n    core_loss = -F.logsigmoid(delta_logp - margin)\n\n    # 5. Compute a cost-based scaling factor (inherited from both parents)\n    weight_scale = F.softplus(beta * delta_cost)\n\n    # 6. Combine core loss and scaling factor\n    scaled_loss = weight_scale * core_loss\n\n    # 7. (New Coupling 2) Compute a log-prob gap regularizer to prevent overconfidence.\n    # log(1 + exp(x)) is a smooth approximation of relu(x). It penalizes large positive delta_logp.\n    # It can be computed more stably as F.softplus(delta_logp).\n    log_prob_gap_regularizer = F.softplus(delta_logp)\n\n    # 8. Combine the scaled loss with the regularizer.\n    # We subtract the regularizer as we want to penalize (increase loss for) large delta_logp.\n    # The main loss term is negative, so adding a positive penalty is equivalent to subtracting it from the -logsigmoid term.\n    # To keep the logic clear, we subtract a positive term from the positive loss.\n    loss_per_sample = scaled_loss + regularizer_strength * log_prob_gap_regularizer\n\n    # 9. Apply optional sample weights if provided\n    sample_weights = batch.get('weight', None)\n    if sample_weights is not None:\n        loss_per_sample = loss_per_sample * sample_weights\n\n    # 10. Return the mean loss over the batch\n    return loss_per_sample.mean()"}, "static_ok": true, "static_reason": "", "dynamic_ok": true, "dynamic_reason": "ok", "loss_value": 1.494791865348816, "grad_norm": 0.0}
{"generation": 5, "index": 3, "ir": {"name": "Rank-Modulated Tanh-Margin Loss", "intuition": "This loss function synthesizes an adaptive margin with a novel rank-based modulation. It inherits the core structure of a logistic loss with a cost-sensitive margin from both parents ('Clipped Tanh-Normalized Adaptive Margin Loss' and 'Adaptive Sigmoid Margin Loss with Tanh Normalization'). Specifically, the margin is computed as a function of the cost difference, normalized by a scaled `tanh` function to ensure batch-independent, bounded behavior (an idea from Parent 1). The entire loss is also scaled by a `softplus` of the cost difference, focusing training on more significant preference pairs, which is a common technique in both parents.\n\nAs a new coupling idea, this child loss introduces a dynamic, rank-based modulation of the log-probability difference (`delta_logp`). Instead of using `delta_logp` directly, it is scaled by `exp(-rank_gap(delta_cost))`. The `rank_gap` operator computes the percentile rank of each cost difference within the batch, normalized to [0, 1]. This means pairs with a small `delta_cost` relative to the current batch will have their `delta_logp` term down-weighted, effectively relaxing the preference constraint. Conversely, pairs with a large `delta_cost` (high rank) will have their `delta_logp` term fully weighted. This allows the model to be more flexible on ambiguous or low-stakes pairs while enforcing strong separation on clear, high-stakes preferences, adapting dynamically to the distribution of cost gaps in each batch. A second, minor coupling is the use of a temperature parameter `tau` inside the `tanh` normalization, allowing finer control over the normalization's saturation.", "pseudocode": "1. Calculate the cost difference: delta_cost = cost(b) - cost(a).\n2. (Inherited) Normalize the cost difference using a scaled tanh function: norm_delta_cost = tanh(delta_cost / tau). This is adapted from both parents.\n3. (Inherited) Compute an adaptive margin based on the normalized cost: margin = max_margin * sigmoid(norm_delta_cost).\n4. Calculate the log-probability difference: delta_logp = logp(a) - logp(b).\n5. (New Coupling 1) Compute the rank-based modulation factor. First, find the percentile rank of each delta_cost within the batch using `rank_gap`. Then, compute the modulation as `mod_factor = exp(-rank_gap(delta_cost))`. This factor will be close to 1 for high-rank (large) cost differences and closer to `exp(-1)` for low-rank (small) cost differences.\n6. Modulate the log-probability difference: `modulated_delta_logp = delta_logp * mod_factor`.\n7. Compute the core preference loss with the modulated logp difference: `core_loss = -logsigmoid(modulated_delta_logp - margin)`.\n8. (Inherited) Calculate a cost-based weight: `weight_scale = softplus(beta * delta_cost)`. This amplifies loss for large cost gaps, a technique from both parents.\n9. Combine them: `final_loss = weight_scale * core_loss`.\n10. Return the mean of final_loss over the batch.", "hyperparams": {"beta": 0.5, "max_margin": 1.5, "tau": 50.0}, "operators_used": ["logsigmoid", "softplus", "sigmoid", "tanh", "exp", "rank_gap"], "implementation_hint": {"expects": ["cost_a", "cost_b", "log_prob_w", "log_prob_l"], "returns": "scalar"}, "code": "import torch\nimport torch.nn.functional as F\n\ndef rank_gap(x):\n    \"\"\"Computes the percentile rank of each element in a 1D tensor, scaled to [0, 1].\"\"\"\n    if x.numel() <= 1:\n        return torch.zeros_like(x)\n    # Sort the tensor and get the indices\n    _, sorted_indices = torch.sort(x)\n    # Create a tensor of ranks [0, 1, 2, ...]\n    ranks = torch.arange(len(x), device=x.device, dtype=x.dtype)\n    # Create an empty tensor to store the final ranks in original order\n    original_order_ranks = torch.zeros_like(x)\n    # Use the sorted indices to place the ranks in the correct original positions\n    original_order_ranks[sorted_indices] = ranks\n    # Normalize ranks to be in [0, 1]\n    return original_order_ranks / (len(x) - 1)\n\ndef generated_loss(batch, model_output, extra):\n    \"\"\"\n    Rank-Modulated Tanh-Margin Loss.\n\n    Args:\n        batch (dict): A dictionary containing tensors from the data loader.\n                      Expected keys: 'cost_a', 'cost_b', 'log_prob_w', 'log_prob_l'.\n                      'cost_a' and 'log_prob_w' correspond to the preferred solution (winner).\n                      'cost_b' and 'log_prob_l' correspond to the non-preferred solution (loser).\n                      An optional 'weight' tensor can be used for sample weighting.\n        model_output (dict): Output from the model (not used in this loss structure).\n        extra (dict): A dictionary for hyperparameters, e.g., {'beta': 0.5, 'max_margin': 1.5, 'tau': 50.0}.\n\n    Returns:\n        torch.Tensor: A scalar tensor representing the mean loss.\n    \"\"\"\n    # Read hyperparameters\n    beta = extra.get('beta', 0.5)\n    max_margin = extra.get('max_margin', 1.5)\n    tau = extra.get('tau', 50.0) # Temperature for tanh normalization\n\n    # In our setting, 'a' is the winner (w) and 'b' is the loser (l).\n    cost_a = batch['cost_a']\n    cost_b = batch['cost_b']\n    logp_a = batch['log_prob_w']\n    logp_b = batch['log_prob_l']\n\n    # 1. Calculate cost and log-probability differences\n    delta_cost = cost_b - cost_a\n    delta_logp = logp_a - logp_b\n\n    # 2. (Inherited) Normalize cost difference with tanh\n    norm_delta_cost = torch.tanh(delta_cost / tau)\n\n    # 3. (Inherited) Compute the adaptive sigmoid margin\n    margin = max_margin * torch.sigmoid(norm_delta_cost)\n\n    # 4. (New Coupling) Compute rank-based modulation for delta_logp\n    # The rank_gap is low for small delta_cost, high for large delta_cost.\n    # We want to down-weight the loss for low-rank pairs.\n    # exp(-rank_gap) will be ~1 for high-rank pairs and <1 for low-rank pairs.\n    # Using rank_gap(delta_cost) directly means small cost gaps get small weights.\n    with torch.no_grad(): # Ranking is non-differentiable and used as a weight\n        cost_ranks = rank_gap(delta_cost)\n    mod_factor = torch.exp(-cost_ranks)\n\n    # 5. Modulate the log-probability difference\n    modulated_delta_logp = delta_logp * mod_factor\n\n    # 6. Compute the core preference loss with the modulated term\n    core_loss = -F.logsigmoid(modulated_delta_logp - margin)\n\n    # 7. (Inherited) Compute a cost-based scaling factor using softplus\n    weight_scale = F.softplus(beta * delta_cost)\n\n    # 8. Combine core loss and scaling factor\n    loss_per_sample = weight_scale * core_loss\n\n    # 9. Apply optional sample weights if provided\n    sample_weights = batch.get('weight', None)\n    if sample_weights is not None:\n        loss_per_sample = loss_per_sample * sample_weights\n\n    # 10. Return the mean loss over the batch\n    return loss_per_sample.mean()"}, "static_ok": true, "static_reason": "", "dynamic_ok": false, "dynamic_reason": "forward_error: name 'rank_gap' is not defined", "loss_value": null, "grad_norm": null}
{"generation": 5, "index": 4, "ir": {"name": "Rank-Gapped Exponential Margin Loss", "intuition": "This loss function synthesizes ideas from its parents to create a novel loss that is both robust and sensitive to the relative importance of preference pairs. It inherits the core logistic loss structure `-logsigmoid(delta_logp - margin)` and the `softplus` weighting of the raw cost difference, `softplus(beta * delta_cost)`, which are common principles in both parents for ensuring a stable, cost-sensitive learning signal.\n\nFrom Parent 1 ('Adaptive Sigmoid Margin Loss with Tanh Normalization'), it inherits the concept of batch-independent normalization using a scaled `tanh` function to process the cost difference. This ensures the margin calculation is predictable and not skewed by batch statistics.\n\nFrom Parent 0 ('Clipped Tanh-Normalized Adaptive Margin Loss'), it inherits the use of `clamp` for stability, although it is repurposed here. Instead of clipping the normalized cost, it clips the final margin to prevent extreme values from destabilizing training.\n\nTwo new coupling ideas are introduced. First, the margin is computed using an `exp` function instead of `sigmoid`: `margin = max_margin * exp(norm_delta_cost - 1)`. This creates a margin that grows exponentially with the normalized cost difference, strongly penalizing the model for misranking pairs with large cost gaps. The `-1` term ensures the margin is `max_margin` when `norm_delta_cost` is at its maximum of 1.\n\nSecond, it introduces a dynamic beta scaling based on the batch-wise rank of the cost differences. Using a new `rank_gap` operator, it computes a weight for each sample based on how its `delta_cost` ranks within the batch. This weight is then multiplied by the base `beta` hyperparameter. The effect is that pairs with a higher cost difference relative to others in the same batch are given a larger `beta`, thus amplifying their contribution to the loss. This focuses the model's attention on the most 'egregious' or 'important' preference pairs in each training step.", "pseudocode": "1. Calculate the cost difference: delta_cost = cost(b) - cost(a).\n2. (Inherited from Parent 1) Normalize the cost difference using a scaled tanh function for batch-independent scaling: norm_delta_cost = tanh(delta_cost / norm_scale).\n3. (New Coupling 1) Compute an exponential margin: margin = max_margin * exp(norm_delta_cost - 1). This margin grows exponentially with the cost gap, emphasizing large differences.\n4. (Inherited from Parent 0, repurposed) Clip the margin to a safe range for stability: clipped_margin = clamp(margin, 0, max_margin).\n5. Calculate the log-probability difference: delta_logp = logp(a) - logp(b).\n6. (Inherited from both) Compute the core logistic loss: core_loss = -logsigmoid(delta_logp - clipped_margin).\n7. (New Coupling 2) Compute a rank-based weight for each sample in the batch: rank_weight = rank_gap(delta_cost). This returns a value in [0, 1] based on the rank of delta_cost.\n8. Compute a dynamic beta: dynamic_beta = base_beta * (1 + rank_weight). This increases beta for higher-ranked cost differences.\n9. (Inherited from both) Calculate a cost-based weight using the dynamic beta: weight_scale = softplus(dynamic_beta * delta_cost).\n10. Combine them: final_loss = weight_scale * core_loss.\n11. Return the mean of final_loss over the batch.", "hyperparams": {"base_beta": 0.5, "max_margin": 3.0, "norm_scale": 15.0}, "operators_used": ["logsigmoid", "softplus", "exp", "tanh", "clamp", "rank_gap"], "implementation_hint": {"expects": ["cost_a", "cost_b", "logp_a", "logp_b"], "returns": "scalar"}, "code": "import torch\nimport torch.nn.functional as F\n\ndef rank_gap(x: torch.Tensor) -> torch.Tensor:\n    \"\"\"Normalizes a tensor by its rank within the batch. Output is in [0, 1].\"\"\"\n    if x.numel() <= 1:\n        return torch.ones_like(x)\n    ranks = x.argsort().argsort().float()\n    return ranks / (x.numel() - 1)\n\ndef generated_loss(batch, model_output, extra):\n    \"\"\"\n    Rank-Gapped Exponential Margin Loss.\n\n    Args:\n        batch (dict): A dictionary containing tensors from the data loader.\n                      Expected keys: 'cost_a', 'cost_b', 'log_prob_w', 'log_prob_l'.\n        model_output (dict): Output from the model (not used).\n        extra (dict): A dictionary for hyperparameters.\n\n    Returns:\n        torch.Tensor: A scalar tensor representing the mean loss.\n    \"\"\"\n    # Read hyperparameters\n    base_beta = extra.get('base_beta', 0.5)\n    max_margin = extra.get('max_margin', 3.0)\n    norm_scale = extra.get('norm_scale', 15.0)\n\n    # Unpack batch data\n    cost_a = batch['cost_a']\n    cost_b = batch['cost_b']\n    logp_a = batch['log_prob_w']\n    logp_b = batch['log_prob_l']\n\n    # Ensure cost_b >= cost_a for valid preference pairs\n    delta_cost = torch.relu(cost_b - cost_a)\n    delta_logp = logp_a - logp_b\n\n    # 1. Inherit tanh normalization from Parent 1\n    norm_delta_cost = torch.tanh(delta_cost / norm_scale)\n\n    # 2. New Coupling 1: Exponential margin calculation\n    # The margin grows exponentially, emphasizing pairs with large cost gaps.\n    # The '-1' term ensures margin is max_margin when tanh output is 1.\n    margin = max_margin * torch.exp(norm_delta_cost - 1)\n\n    # 3. Inherit clamp from Parent 0 for stability, applied to the margin\n    clipped_margin = torch.clamp(margin, min=0.0, max=max_margin)\n\n    # 4. Inherit core logsigmoid loss from both parents\n    core_loss = -F.logsigmoid(delta_logp - clipped_margin)\n\n    # 5. New Coupling 2: Dynamic beta based on batch-wise cost rank\n    # This focuses training on the most significant preferences in each batch.\n    with torch.no_grad():\n        rank_weight = rank_gap(delta_cost)\n    dynamic_beta = base_beta * (1.0 + rank_weight)\n\n    # 6. Inherit softplus weighting from both parents, using the dynamic beta\n    weight_scale = F.softplus(dynamic_beta * delta_cost)\n\n    # 7. Combine into the final loss per sample\n    loss_per_sample = weight_scale * core_loss\n\n    # 8. Apply optional sample weights if provided\n    sample_weights = batch.get('weight', None)\n    if sample_weights is not None:\n        loss_per_sample = loss_per_sample * sample_weights\n\n    # 9. Return the mean loss\n    return loss_per_sample.mean()"}, "static_ok": true, "static_reason": "", "dynamic_ok": false, "dynamic_reason": "forward_error: name 'rank_gap' is not defined", "loss_value": null, "grad_norm": null}
{"generation": 5, "index": 5, "ir": {"name": "Rank-Modulated Tanh Margin Loss", "intuition": "This loss function synthesizes ideas from its parents to create a robust, adaptive margin loss, and introduces a novel rank-based modulation scheme. It inherits the core logistic loss structure (`-logsigmoid`) and the use of a `tanh`-normalized cost difference to create an adaptive margin, an idea central to both parents. This ensures the margin is bounded, batch-independent, and sensitive to the magnitude of the cost difference. It also inherits the `softplus` weighting of the raw cost difference, which focuses training on pairs with more significant cost gaps.\n\nAs a new coupling idea, this child loss introduces a dynamic, rank-based modulation of the model's log-probability difference (`delta_logp`). First, the `delta_logp` values within a batch are converted to ranks, and a `rank_gap` is computed, representing the relative ordering of the model's confidence across the batch. This rank gap is then transformed by a `tanh` function and used to scale the original `delta_logp`. This has two effects: (1) it normalizes the influence of log-probability differences, preventing outliers with extremely high or low `delta_logp` from dominating the batch gradient, and (2) it regularizes the model by encouraging a consistent ordering of preferences within the batch, not just pairwise correctness. This 'rank-pressure' acts as a regularizer, coupling the learning signals for all pairs in a batch.\n\nA second new idea is a dynamic `beta` schedule that depends on the training progress (e.g., step count). This allows `beta`, which controls the `softplus` weighting, to be annealed over time. It can start low to allow for exploration and then increase to focus on high-cost-gap pairs as the model matures, providing a curriculum learning effect.", "pseudocode": "1. Calculate the cost difference: delta_cost = cost(b) - cost(a).\n2. Calculate the log-probability difference: delta_logp = logp(a) - logp(b).\n3. (Inherited from both parents) Normalize the cost difference using a scaled tanh function: norm_delta_cost = tanh(delta_cost / norm_scale).\n4. (Inherited from both parents) Compute an adaptive margin based on the normalized cost: margin = max_margin * sigmoid(norm_delta_cost).\n5. (New Coupling 1) Modulate the log-probability difference using batch-wise ranks. First, compute the rank_gap of delta_logp across the batch. Then, scale the original delta_logp by the tanh of this rank gap: modulated_delta_logp = delta_logp * tanh(rank_gap(delta_logp)).\n6. Compute the core preference loss using the modulated log-probability difference: core_loss = -logsigmoid(modulated_delta_logp - margin).\n7. (New Coupling 2) Compute a dynamic beta based on the current training step: dynamic_beta = initial_beta + (final_beta - initial_beta) * clamp(current_step / total_steps, 0, 1).\n8. (Inherited from both parents) Calculate a cost-based weight using the dynamic beta: weight_scale = softplus(dynamic_beta * delta_cost).\n9. Combine them: final_loss = weight_scale * core_loss.\n10. Return the mean of final_loss over the batch.", "hyperparams": {"initial_beta": 0.5, "final_beta": 1.5, "total_steps": 10000, "max_margin": 2.0, "norm_scale": 10.0}, "operators_used": ["logsigmoid", "softplus", "sigmoid", "tanh", "clamp", "rank_gap"], "implementation_hint": {"expects": ["cost_a", "cost_b", "logp_a", "logp_b"], "returns": "scalar"}, "code": "import torch\nimport torch.nn.functional as F\n\ndef rank_gap(x):\n    \"\"\"Computes the gap between the rank of each element and the median rank.\"\"\"\n    if x.numel() <= 1:\n        return torch.zeros_like(x)\n    ranks = x.argsort().argsort().float()\n    scaled_ranks = (ranks / (x.numel() - 1)) * 2 - 1 # Scale ranks to [-1, 1]\n    return scaled_ranks\n\ndef generated_loss(batch, model_output, extra):\n    \"\"\"\n    Rank-Modulated Tanh Margin Loss.\n\n    Args:\n        batch (dict): A dictionary containing tensors from the data loader.\n                      Expected keys: 'cost_a', 'cost_b', 'log_prob_w', 'log_prob_l'.\n        model_output (dict): Output from the model (not used in this loss structure).\n        extra (dict): A dictionary for hyperparameters and auxiliary info like 'step'.\n\n    Returns:\n        torch.Tensor: A scalar tensor representing the mean loss.\n    \"\"\"\n    # Read hyperparameters\n    initial_beta = extra.get('initial_beta', 0.5)\n    final_beta = extra.get('final_beta', 1.5)\n    total_steps = extra.get('total_steps', 10000)\n    max_margin = extra.get('max_margin', 2.0)\n    norm_scale = extra.get('norm_scale', 10.0)\n    current_step = extra.get('step', 0)\n\n    cost_a = batch['cost_a']\n    cost_b = batch['cost_b']\n    logp_a = batch['log_prob_w']\n    logp_b = batch['log_prob_l']\n\n    # 1. Calculate cost and log-probability differences\n    delta_cost = cost_b - cost_a\n    delta_logp = logp_a - logp_b\n\n    # 2. Inherited: Normalize cost difference with tanh for adaptive margin\n    norm_delta_cost = torch.tanh(delta_cost / norm_scale)\n\n    # 3. Inherited: Compute the adaptive sigmoid margin\n    margin = max_margin * torch.sigmoid(norm_delta_cost)\n    \n    # 4. New Coupling 1: Modulate delta_logp based on its batch-wise rank\n    # This normalizes the influence of logp differences and adds a regularization pressure\n    # for the model to learn a consistent ranking within the batch.\n    if delta_logp.numel() > 1:\n        logp_rank_gap = rank_gap(delta_logp)\n        # The tanh ensures the scaling factor is in [-1, 1] and is well-behaved.\n        rank_modulator = torch.tanh(logp_rank_gap)\n        modulated_delta_logp = delta_logp * rank_modulator.detach() # Detach to treat as a target modulator, not part of the AD graph for itself\n    else:\n        modulated_delta_logp = delta_logp\n\n    # 5. Compute the core preference loss with modulated logp difference\n    core_loss = -F.logsigmoid(modulated_delta_logp - margin)\n\n    # 6. New Coupling 2: Dynamic beta schedule for curriculum effect\n    progress = torch.clamp(torch.tensor(current_step / total_steps, device=delta_cost.device), 0.0, 1.0)\n    dynamic_beta = initial_beta + (final_beta - initial_beta) * progress\n\n    # 7. Inherited: Compute a cost-based scaling factor using softplus\n    weight_scale = F.softplus(dynamic_beta * delta_cost)\n\n    # 8. Combine core loss and scaling factor\n    loss_per_sample = weight_scale * core_loss\n\n    # 9. Apply optional sample weights if provided\n    sample_weights = batch.get('weight', None)\n    if sample_weights is not None:\n        loss_per_sample = loss_per_sample * sample_weights\n\n    # 10. Return the mean loss over the batch\n    return loss_per_sample.mean()"}, "static_ok": true, "static_reason": "", "dynamic_ok": false, "dynamic_reason": "forward_error: name 'rank_gap' is not defined", "loss_value": null, "grad_norm": null}
{"generation": 5, "index": 6, "ir": {"name": "Rank-Gapped Adaptive Tanh-Margin Loss", "intuition": "This loss function synthesizes ideas from its parents to create a robust, adaptive preference learning objective, while introducing novel coupling mechanisms for improved stability and focus.\n\nInherited Ideas:\n- From both parents, it inherits the core structure of an adaptive margin loss: `loss = -logsigmoid(delta_logp - margin)`, where the margin is a function of the cost difference. This ensures the model is pushed harder on pairs with a larger quality gap.\n- From Parent 1 (`Adaptive Sigmoid Margin Loss with Tanh Normalization`), it inherits the use of a scaled `tanh` function to normalize the cost difference. This provides a stable, batch-independent normalization that maps the cost gap to a bounded range, making the margin calculation predictable.\n\nNew Couplings & Modifications:\n1.  **Direct Tanh Margin:** Instead of using `sigmoid(tanh(delta_cost))` to form the margin, this child simplifies the mechanism by using the output of `tanh` directly: `margin = max_margin * tanh(delta_cost / norm_scale)`. This is more direct, preserves the sign of the cost difference, and maintains the bounded, adaptive nature of the margin.\n2.  **Rank-Gap Weighting:** A key new idea is the introduction of a `rank_gap` operator for weighting the loss. Instead of scaling the loss by `softplus(beta * delta_cost)` (which can grow unbounded and cause instability with large cost differences), we compute the rank of the cost differences within the batch. The loss for each sample is then scaled by `log(1 + rank_gap)`, where `rank_gap` is the difference between the rank of the current sample's cost difference and the median rank. This focuses training on samples with unusually high or low cost differences relative to the current batch, effectively acting as a form of batch-aware curriculum learning. The `log` function ensures this weighting is non-aggressive and numerically stable. This approach is more robust to outliers in the raw cost values than the parents' `softplus` scaling.", "pseudocode": "1. Calculate the cost difference: delta_cost = cost(b) - cost(a).\n2. Calculate the log-probability difference: delta_logp = logp(a) - logp(b).\n3. (Inherited from Parent 1) Compute a normalized cost difference using a scaled tanh function: norm_delta_cost = tanh(delta_cost / norm_scale).\n4. (New Coupling 1: Direct Tanh Margin) Compute an adaptive margin directly from the normalized cost difference: margin = max_margin * norm_delta_cost.\n5. (Inherited from both) Compute the core preference loss: core_loss = -logsigmoid(delta_logp - margin).\n6. (New Coupling 2: Rank-Gap Weighting) Within the batch, compute the rank of each delta_cost. Then, calculate the rank_gap for each sample as the absolute difference between its rank and the median rank of the batch.\n7. Calculate a stable, rank-based weight: weight_scale = log(1 + rank_gap).\n8. Combine the core loss and the rank-based weight: final_loss = weight_scale * core_loss.\n9. Return the mean of final_loss over the batch.", "hyperparams": {"max_margin": 2.5, "norm_scale": 15.0}, "operators_used": ["logsigmoid", "tanh", "log", "rank_gap"], "implementation_hint": {"expects": ["cost_a", "cost_b", "logp_a", "logp_b"], "returns": "scalar"}, "code": "import torch\nimport torch.nn.functional as F\n\ndef rank_gap(x):\n    \"\"\"Computes the gap from the median rank for each element in a 1D tensor.\"\"\"\n    ranks = x.argsort().argsort().float()\n    median_rank = torch.median(ranks)\n    return torch.abs(ranks - median_rank)\n\ndef generated_loss(batch, model_output, extra):\n    \"\"\"\n    Rank-Gapped Adaptive Tanh-Margin Loss.\n\n    Args:\n        batch (dict): A dictionary containing tensors from the data loader.\n                      Expected keys: 'cost_a', 'cost_b', 'log_prob_w', 'log_prob_l'.\n                      'cost_a' and 'log_prob_w' correspond to the preferred solution (winner).\n                      'cost_b' and 'log_prob_l' correspond to the non-preferred solution (loser).\n                      An optional 'weight' tensor can be used for sample weighting.\n        model_output (dict): Output from the model (not used in this loss structure).\n        extra (dict): A dictionary for hyperparameters, e.g., {'max_margin': 2.5, 'norm_scale': 15.0}.\n\n    Returns:\n        torch.Tensor: A scalar tensor representing the mean loss.\n    \"\"\"\n    # Read hyperparameters\n    max_margin = extra.get('max_margin', 2.5)\n    norm_scale = extra.get('norm_scale', 15.0)\n\n    # In our setting, 'a' is the winner (w) and 'b' is the loser (l).\n    # cost_a < cost_b is the preference.\n    cost_a = batch['cost_a']\n    cost_b = batch['cost_b']\n    logp_a = batch['log_prob_w']\n    logp_b = batch['log_prob_l']\n\n    # 1. Calculate cost and log-probability differences\n    delta_cost = cost_b - cost_a\n    delta_logp = logp_a - logp_b\n    \n    # Ensure delta_cost is not zero to avoid numerical issues if norm_scale is also zero.\n    # This is a good practice for stability.\n    delta_cost = delta_cost.clamp(min=1e-6)\n\n    # 2. (Inherited) Normalize cost difference with tanh for a bounded value.\n    norm_delta_cost = torch.tanh(delta_cost / norm_scale)\n\n    # 3. (New Coupling 1) Compute margin directly from the tanh-normalized cost.\n    # This is a simpler and more direct adaptive margin than the parents' sigmoid(tanh(...)).\n    margin = max_margin * norm_delta_cost\n\n    # 4. (Inherited) Compute the core preference loss.\n    core_loss = -F.logsigmoid(delta_logp - margin)\n\n    # 5. (New Coupling 2) Compute a rank-based weight.\n    # This replaces the parents' softplus(beta * delta_cost) scaling.\n    # It's more robust to outliers in raw cost values.\n    with torch.no_grad(): # Ranks should not contribute to gradients.\n        r_gap = rank_gap(delta_cost)\n    \n    # Scale by log(1 + rank_gap) for stable, non-aggressive weighting.\n    weight_scale = torch.log(1.0 + r_gap)\n    \n    # 6. Combine core loss and the new rank-based scaling factor.\n    loss_per_sample = weight_scale * core_loss\n\n    # 7. Apply optional sample weights if provided\n    sample_weights = batch.get('weight', None)\n    if sample_weights is not None:\n        loss_per_sample = loss_per_sample * sample_weights\n\n    # 8. Return the mean loss over the batch\n    return loss_per_sample.mean()"}, "static_ok": true, "static_reason": "", "dynamic_ok": false, "dynamic_reason": "forward_error: name 'rank_gap' is not defined", "loss_value": null, "grad_norm": null}
{"generation": 5, "index": 7, "ir": {"name": "Rank-Gapped Adaptive Tanh Loss", "intuition": "This loss function creates a robust preference learning objective by blending ideas from its parents and introducing novel couplings for stability and focused learning. \n\nInherited Ideas:\n- From both parents, it inherits the core concept of an adaptive margin that grows with the cost difference. This is achieved by making the margin a function of the normalized cost gap, allowing the model to be uncertain on close pairs while demanding a strong signal on clearly distinct pairs.\n- From Parent 0 (`Clipped Tanh-Normalized Adaptive Margin Loss`), it inherits the idea of applying a `clamp` operation for stability, although it is repurposed here.\n\nNew Couplings & Modifications:\n1. **Tanh-based Margin**: Instead of using a sigmoid function to create the margin, this child loss applies `tanh` directly to the normalized cost difference. This creates a margin that is symmetric around zero and scales between `-max_margin` and `+max_margin`, which is a more direct mapping from the normalized cost gap.\n2. **Rank-Gap Normalization**: A new batch-level normalization scheme, `rank_gap`, is introduced. This operator normalizes the cost differences based on their rank within the batch, making the normalization robust to outliers in cost values. The normalized cost differences are then clamped to prevent extreme values before being used to compute the margin. This replaces the simple `tanh(delta_cost / scale)` from the parents.\n3. **Dynamic Beta Scaling**: The `beta` parameter, which scales the loss importance based on the cost gap, is made dynamic. It is modulated by `softplus` of the rank-gapped normalized cost difference. This ensures that pairs with a very small rank-gap (i.e., insignificant preference) have their `beta` value down-weighted, focusing training on more meaningful preferences within the batch context.", "pseudocode": "1. Calculate the cost difference for the batch: delta_cost = cost(b) - cost(a).\n2. (New Coupling) Normalize the cost difference using rank-gap normalization across the batch: rank_norm_cost = rank_gap(delta_cost). This makes the normalization robust to outliers.\n3. (Inherited Idea) Clamp the rank-normalized cost to a stable range, e.g., [-clamp_value, clamp_value]. This ensures the input to the margin function is well-behaved.\n4. (New Coupling) Compute an adaptive margin using a scaled tanh function on the clamped, normalized cost: margin = max_margin * tanh(clamped_rank_norm_cost).\n5. Calculate the log-probability difference: delta_logp = logp(a) - logp(b).\n6. Compute the core preference loss: core_loss = -logsigmoid(delta_logp - margin).\n7. (New Coupling) Compute a dynamic beta: dynamic_beta = base_beta * softplus(rank_norm_cost). This scales beta based on the significance of the preference pair within the batch.\n8. Calculate a cost-based weight using the dynamic beta: weight_scale = softplus(dynamic_beta * delta_cost).\n9. Combine them: final_loss = weight_scale * core_loss.\n10. Return the mean of final_loss over the batch.", "hyperparams": {"base_beta": 0.5, "max_margin": 1.5, "clamp_value": 3.0}, "operators_used": ["logsigmoid", "softplus", "tanh", "clamp", "rank_gap"], "implementation_hint": {"expects": ["cost_a", "cost_b", "logp_a", "logp_b"], "returns": "scalar"}, "code": "import torch\nimport torch.nn.functional as F\n\ndef _rank_gap(x):\n    \"\"\"Normalizes a tensor by its rank, scaled to [-0.5, 0.5].\"\"\"\n    if x.numel() <= 1:\n        return torch.zeros_like(x)\n    ranks = x.argsort().argsort().float()\n    # Scale ranks to be in the range [-0.5, 0.5]\n    # Subtracting 0.5 from the denominator prevents division by zero for a single element\n    # and ensures that for N > 1, the max rank (N-1) maps to 0.5.\n    return ranks / (x.numel() - 1 + 1e-8) - 0.5\n\ndef generated_loss(batch, model_output, extra):\n    \"\"\"\n    Rank-Gapped Adaptive Tanh Loss.\n\n    Args:\n        batch (dict): A dictionary containing tensors from the data loader.\n                      Expected keys: 'cost_a', 'cost_b', 'log_prob_w', 'log_prob_l'.\n                      'cost_a' and 'log_prob_w' correspond to the preferred solution (winner).\n                      'cost_b' and 'log_prob_l' correspond to the non-preferred solution (loser).\n                      An optional 'weight' tensor can be used for sample weighting.\n        model_output (dict): Output from the model (not used in this loss structure).\n        extra (dict): A dictionary for hyperparameters, e.g., {'base_beta': 0.5, 'max_margin': 1.5, 'clamp_value': 3.0}.\n\n    Returns:\n        torch.Tensor: A scalar tensor representing the mean loss.\n    \"\"\"\n    # Read hyperparameters\n    base_beta = extra.get('base_beta', 0.5)\n    max_margin = extra.get('max_margin', 1.5)\n    clamp_value = extra.get('clamp_value', 3.0)\n\n    # In our setting, 'a' is the winner (w) and 'b' is the loser (l).\n    cost_a = batch['cost_a']\n    cost_b = batch['cost_b']\n    logp_a = batch['log_prob_w']\n    logp_b = batch['log_prob_l']\n\n    # 1. Calculate cost and log-probability differences\n    delta_cost = cost_b - cost_a\n    delta_logp = logp_a - logp_b\n\n    # 2. (New Coupling) Normalize cost difference using rank-gap normalization.\n    # This is robust to outliers in cost magnitudes.\n    rank_norm_cost = _rank_gap(delta_cost)\n\n    # 3. (Inherited Idea) Clamp the normalized cost for stability.\n    # This prevents the tanh from saturating too quickly if ranks are scaled aggressively.\n    clamped_rank_norm_cost = torch.clamp(rank_norm_cost, -clamp_value, clamp_value)\n\n    # 4. (New Coupling) Compute adaptive margin using tanh, creating a symmetric margin.\n    margin = max_margin * torch.tanh(clamped_rank_norm_cost)\n\n    # 5. Compute the core preference loss\n    core_loss = -F.logsigmoid(delta_logp - margin)\n\n    # 6. (New Coupling) Compute dynamic beta based on the rank-normalized cost significance.\n    # softplus ensures the multiplier is positive and smooth.\n    dynamic_beta = base_beta * F.softplus(rank_norm_cost)\n\n    # 7. Compute a cost-based scaling factor using the dynamic beta.\n    weight_scale = F.softplus(dynamic_beta * delta_cost)\n\n    # 8. Combine core loss and scaling factor\n    loss_per_sample = weight_scale * core_loss\n\n    # 9. Apply optional sample weights if provided\n    sample_weights = batch.get('weight', None)\n    if sample_weights is not None:\n        loss_per_sample = loss_per_sample * sample_weights\n\n    # 10. Return the mean loss over the batch\n    return loss_per_sample.mean()"}, "static_ok": true, "static_reason": "", "dynamic_ok": false, "dynamic_reason": "forward_error: name '_rank_gap' is not defined", "loss_value": null, "grad_norm": null}
{"generation": 6, "index": 0, "ir": {"name": "Progressive Rank-Gap Margin Loss", "intuition": "This loss function combines the robust, batch-independent normalization of Parent 1 with a novel margin mechanism. It inherits the use of `tanh(delta_cost / norm_scale)` to produce a stable, bounded normalization of the cost difference, ensuring predictable behavior across different batches. It also inherits the `softplus(beta * delta_cost)` weighting scheme, common to both parents, which upweights pairs with larger cost differences, focusing training on more significant preferences.\n\nAs a first new coupling idea, this child introduces a 'progressive margin' based on the model's own confidence. The margin is not static but is calculated as `margin_scale * softplus(delta_logp)`. This means that as the model becomes more confident about its preference (i.e., `delta_logp` is large and positive), the margin it must overcome also increases. This self-regulating mechanism encourages the model to continually improve its discrimination, preventing it from becoming complacent with a fixed margin.\n\nAs a second new coupling idea, the loss incorporates a batch-wise rank-gap term, `rank_gap(delta_logp)`. This term compares the rank of each `delta_logp` within the batch to the rank of the corresponding `delta_cost`. The loss is then scaled by `exp(-rank_gap_strength * rank_gap)`. This penalizes the model less if its log-probability differences are rank-ordered consistently with the cost differences within the batch, even if the absolute values are not perfect. It encourages the model to first learn the correct relative ordering of preferences before perfecting the absolute magnitudes, providing a smoother learning curriculum.", "pseudocode": "1. Calculate the cost difference `delta_cost = cost(b) - cost(a)` and log-probability difference `delta_logp = logp(a) - logp(b)`.\n2. (Inherited from Parent 1) Normalize the cost difference using a scaled tanh function: `norm_delta_cost = tanh(delta_cost / norm_scale)`.\n3. (New Coupling 1) Compute a progressive, self-adjusting margin based on the model's current log-probability difference: `margin = margin_scale * softplus(delta_logp)`.\n4. Combine the normalized cost and the progressive margin to form the argument for the main loss: `loss_argument = norm_delta_cost * delta_logp - margin`.\n5. Compute the core preference loss using logsigmoid: `core_loss = -logsigmoid(loss_argument)`. The goal is to make `norm_delta_cost * delta_logp` larger than the margin.\n6. (Inherited from both parents) Calculate a cost-based weight: `weight_scale = softplus(beta * delta_cost)`.\n7. (New Coupling 2) Calculate the batch-wise rank gap between `delta_logp` and `delta_cost`: `rg = rank_gap(delta_logp, delta_cost)`. This measures how well the model's preference strengths are ordered relative to the true cost differences.\n8. Compute a rank-based scaling factor: `rank_scale = exp(-rank_gap_strength * rg)`. This reduces the loss for pairs where the model's ranking is consistent with the cost ranking.\n9. Combine all components: `final_loss = rank_scale * weight_scale * core_loss`.\n10. Return the mean of `final_loss` over the batch.", "hyperparams": {"beta": 0.5, "norm_scale": 20.0, "margin_scale": 0.1, "rank_gap_strength": 0.2}, "operators_used": ["logsigmoid", "softplus", "tanh", "exp", "rank_gap"], "implementation_hint": {"expects": ["cost_a", "cost_b", "logp_a", "logp_b"], "returns": "scalar"}, "code": "import torch\nimport torch.nn.functional as F\n\ndef rank_gap(scores, targets):\n    \"\"\"Helper to compute the rank gap between scores and targets.\"\"\"\n    # Higher scores/targets should have higher ranks\n    score_ranks = scores.argsort().argsort().float()\n    target_ranks = targets.argsort().argsort().float()\n    return torch.abs(score_ranks - target_ranks) / len(scores)\n\ndef generated_loss(batch, model_output, extra):\n    \"\"\"\n    Progressive Rank-Gap Margin Loss.\n\n    Args:\n        batch (dict): A dictionary containing tensors from the data loader.\n                      Expected keys: 'cost_a', 'cost_b', 'log_prob_w', 'log_prob_l'.\n                      'cost_a' and 'log_prob_w' correspond to the preferred solution (winner).\n                      'cost_b' and 'log_prob_l' correspond to the non-preferred solution (loser).\n                      An optional 'weight' tensor can be used for sample weighting.\n        model_output (dict): Output from the model (not used in this loss structure).\n        extra (dict): A dictionary for hyperparameters.\n\n    Returns:\n        torch.Tensor: A scalar tensor representing the mean loss.\n    \"\"\"\n    # Read hyperparameters\n    beta = extra.get('beta', 0.5)\n    norm_scale = extra.get('norm_scale', 20.0)\n    margin_scale = extra.get('margin_scale', 0.1)\n    rank_gap_strength = extra.get('rank_gap_strength', 0.2)\n\n    # In our setting, 'a' is the winner (w) and 'b' is the loser (l).\n    cost_a = batch['cost_a']\n    cost_b = batch['cost_b']\n    logp_a = batch['log_prob_w']\n    logp_b = batch['log_prob_l']\n\n    # 1. Calculate cost and log-probability differences\n    delta_cost = cost_b - cost_a\n    delta_logp = logp_a - logp_b\n\n    # Ensure delta_cost is non-negative for stability\n    delta_cost = torch.clamp(delta_cost, min=0.0)\n\n    # 2. (Inherited) Normalize cost difference with tanh for a bounded signal\n    norm_delta_cost = torch.tanh(delta_cost / norm_scale)\n\n    # 3. (New Coupling 1) Progressive margin based on model's own confidence\n    # As delta_logp grows, the margin it needs to overcome also grows.\n    progressive_margin = margin_scale * F.softplus(delta_logp)\n\n    # 4. Form the argument for the logsigmoid loss\n    # The model is encouraged to make (norm_delta_cost * delta_logp) > progressive_margin\n    loss_argument = norm_delta_cost * delta_logp - progressive_margin\n\n    # 5. Compute the core preference loss\n    core_loss = -F.logsigmoid(loss_argument)\n\n    # 6. (Inherited) Compute a cost-based scaling factor\n    weight_scale = F.softplus(beta * delta_cost)\n\n    # 7. (New Coupling 2) Compute batch-wise rank gap\n    # This measures ordinal inconsistency between model preference and cost difference.\n    # Detach to avoid gradients flowing through the ranking process itself.\n    rg = rank_gap(delta_logp.detach(), delta_cost.detach())\n\n    # 8. Compute a rank-based scaling factor. Reduces loss for ordinally-correct pairs.\n    rank_scale = torch.exp(-rank_gap_strength * rg)\n\n    # 9. Combine all components\n    loss_per_sample = rank_scale * weight_scale * core_loss\n\n    # 10. Apply optional sample weights if provided\n    sample_weights = batch.get('weight', None)\n    if sample_weights is not None:\n        loss_per_sample = loss_per_sample * sample_weights\n\n    # 11. Return the mean loss over the batch\n    return loss_per_sample.mean()"}, "static_ok": true, "static_reason": "", "dynamic_ok": false, "dynamic_reason": "forward_error: name 'rank_gap' is not defined", "loss_value": null, "grad_norm": null}
{"generation": 6, "index": 1, "ir": {"name": "Soft-Clipped Exponential Margin Loss", "intuition": "This loss function creates a novel preference objective by blending ideas from its parents and introducing new coupling mechanisms for stability and focus. \n\nInherited Ideas:\n- From both parents, it inherits the core concept of an adaptive margin that depends on the cost difference (`delta_cost = cost_b - cost_a`). This ensures that pairs with a larger cost gap demand a larger separation in log-probabilities.\n- It also inherits the `softplus(beta * delta_cost)` weighting scheme, which up-weights pairs that are more distinct in cost, focusing the training on clear-cut preferences.\n\nNew Couplings & Modifications:\n1. **Tanh-based Soft Clipping of Log-Probabilities**: Instead of clipping the normalized cost to compute the margin, we apply a `tanh` function directly to the log-probability difference (`delta_logp`). `tanh(delta_logp / tau)` acts as a soft clipping mechanism, preventing extremely large `delta_logp` values from causing instability or gradient explosion. The `tau` hyperparameter controls the sensitivity of this clipping. This is a stability trick that regularizes the model's output space.\n2. **Exponential Margin**: The margin is now an exponential function of the cost difference, `exp(alpha * delta_cost) - 1`. This creates a more aggressive margin that grows non-linearly with the cost gap, strongly pushing the model to differentiate pairs with high cost differences. The `-1` ensures the margin is zero when the cost gap is zero.\n\nOverall, the loss becomes `softplus(beta * delta_cost) * -logsigmoid(tanh(delta_logp / tau) - (exp(alpha * delta_cost) - 1))`. This design combines an aggressive exponential margin with a stabilizing soft-clip on the model's log-probability difference, all weighted by the significance of the cost gap.", "pseudocode": "1. Calculate the cost difference: delta_cost = cost(b) - cost(a).\n2. Calculate the log-probability difference: delta_logp = logp(a) - logp(b).\n3. (New Coupling) Compute an exponential margin: margin = exp(alpha * delta_cost) - 1. This margin grows aggressively with the cost gap.\n4. (New Coupling) Apply a soft-clipping function to the log-probability difference for stability: soft_clipped_delta_logp = tanh(delta_logp / tau).\n5. Compute the core preference loss using the soft-clipped log-probability difference and the exponential margin: core_loss = -logsigmoid(soft_clipped_delta_logp - margin).\n6. (Inherited) Calculate a cost-based weight to emphasize significant pairs: weight_scale = softplus(beta * delta_cost).\n7. Combine the weight and core loss: final_loss = weight_scale * core_loss.\n8. Return the mean of final_loss over the batch.", "hyperparams": {"beta": 0.5, "alpha": 0.1, "tau": 4.0}, "operators_used": ["logsigmoid", "softplus", "tanh", "exp"], "implementation_hint": {"expects": ["cost_a", "cost_b", "logp_a", "logp_b"], "returns": "scalar"}, "code": "import torch\nimport torch.nn.functional as F\n\ndef generated_loss(batch, model_output, extra):\n    \"\"\"\n    Soft-Clipped Exponential Margin Loss.\n\n    Args:\n        batch (dict): A dictionary containing tensors from the data loader.\n                      Expected keys: 'cost_a', 'cost_b', 'log_prob_w', 'log_prob_l'.\n                      'cost_a' and 'log_prob_w' correspond to the preferred solution (winner).\n                      'cost_b' and 'log_prob_l' correspond to the non-preferred solution (loser).\n                      An optional 'weight' tensor can be used for sample weighting.\n        model_output (dict): Output from the model (not used in this loss structure).\n        extra (dict): A dictionary for hyperparameters, e.g., {'beta': 0.5, 'alpha': 0.1, 'tau': 4.0}.\n\n    Returns:\n        torch.Tensor: A scalar tensor representing the mean loss.\n    \"\"\"\n    # Read hyperparameters\n    beta = extra.get('beta', 0.5)\n    alpha = extra.get('alpha', 0.1)\n    tau = extra.get('tau', 4.0)\n\n    # In our setting, 'a' is the winner (w) and 'b' is the loser (l).\n    # cost_a < cost_b\n    cost_a = batch['cost_a']\n    cost_b = batch['cost_b']\n    logp_a = batch['log_prob_w']\n    logp_b = batch['log_prob_l']\n\n    # 1. Calculate cost and log-probability differences\n    delta_cost = cost_b - cost_a\n    delta_logp = logp_a - logp_b\n\n    # 2. (New Coupling) Compute an exponential margin based on the cost difference.\n    # This margin grows non-linearly, demanding stronger separation for high-cost-gap pairs.\n    # The -1 ensures margin is 0 when delta_cost is 0.\n    margin = torch.exp(alpha * delta_cost) - 1.0\n\n    # 3. (New Coupling) Apply a soft-clipping function (tanh) to the log-probability difference.\n    # This acts as a stabilizer, preventing extreme delta_logp values from causing gradient issues.\n    # The tau parameter controls the saturation point.\n    soft_clipped_delta_logp = torch.tanh(delta_logp / tau)\n\n    # 4. Compute the core preference loss using the soft-clipped logp and exponential margin.\n    core_loss = -F.logsigmoid(soft_clipped_delta_logp - margin)\n\n    # 5. (Inherited) Compute a cost-based scaling factor to up-weight significant preferences.\n    weight_scale = F.softplus(beta * delta_cost)\n\n    # 6. Combine core loss and scaling factor\n    loss_per_sample = weight_scale * core_loss\n\n    # 7. Apply optional sample weights if provided\n    sample_weights = batch.get('weight', None)\n    if sample_weights is not None:\n        loss_per_sample = loss_per_sample * sample_weights\n\n    # 8. Return the mean loss over the batch\n    return loss_per_sample.mean()"}, "static_ok": true, "static_reason": "", "dynamic_ok": true, "dynamic_reason": "ok", "loss_value": 0.7277472019195557, "grad_norm": 0.0}
{"generation": 6, "index": 2, "ir": {"name": "Adaptive Rank-Gap Margin Loss", "intuition": "This loss function creates a dynamic, cost-sensitive margin that is robust to outliers in cost distributions. It inherits the core logistic loss structure `logsigmoid(delta_logp - margin)` and the `softplus(beta * delta_cost)` weighting scheme from both parents, which ensures that pairs with larger cost differences contribute more to the total loss. It also inherits the concept of an adaptive margin from both parents, where the required separation in log-probabilities grows with the cost difference.\n\nThe first new coupling idea is the use of `rank_gap` normalization for the cost difference. Instead of using `tanh` or `zscore` which are sensitive to the absolute scale or batch statistics of costs, `rank_gap` normalizes the cost difference based on its percentile rank within the batch. This makes the margin adaptive not just to the cost value itself, but to its relative importance within the current batch, providing robustness against cost outliers and shifting cost distributions during training.\n\nThe second new coupling idea is a `softplus`-based margin function. Instead of a `sigmoid` which saturates at a maximum value, `softplus` provides a margin that is non-negative, strictly increasing, and unbounded. This allows the model to be continuously pushed to achieve larger log-probability gaps for pairs with extremely large (rank-normalized) cost differences, avoiding the saturation plateau of sigmoid-based margins. The combination of rank-gap normalization and a softplus margin creates a uniquely responsive and robust learning signal.", "pseudocode": "1. Calculate the cost difference: delta_cost = cost(b) - cost(a).\n2. (New Coupling) Normalize the cost difference using rank-gap normalization: rank_norm_cost = rank_gap(delta_cost). This maps the cost difference to a value in [-1, 1] based on its rank in the batch, providing batch-aware, robust normalization.\n3. (New Coupling) Compute an adaptive margin using a softplus function: margin = margin_scale * softplus(rank_norm_cost). This creates a non-negative, non-saturating margin that grows with the relative importance of the cost gap.\n4. Calculate the log-probability difference: delta_logp = logp(a) - logp(b).\n5. Compute the core preference loss: core_loss = -logsigmoid(delta_logp - margin). This is inherited from both parents.\n6. Calculate a cost-based weight: weight_scale = softplus(beta * delta_cost). This is also inherited from both parents, amplifying the loss for pairs with larger raw cost differences.\n7. Combine them: final_loss = weight_scale * core_loss.\n8. Return the mean of final_loss over the batch.", "hyperparams": {"beta": 1.0, "margin_scale": 1.0}, "operators_used": ["logsigmoid", "softplus", "rank_gap"], "implementation_hint": {"expects": ["cost_a", "cost_b", "logp_a", "logp_b"], "returns": "scalar"}, "code": "import torch\nimport torch.nn.functional as F\n\ndef rank_gap(x):\n    \"\"\"Computes the rank-gap normalization of a 1D tensor x.\"\"\"\n    if x.numel() <= 1:\n        return torch.zeros_like(x)\n    ranks = torch.empty_like(x).float()\n    ranks[x.argsort()] = torch.arange(len(x), device=x.device, dtype=x.dtype)\n    ranks = (2 * ranks / (len(x) - 1)) - 1\n    return ranks\n\ndef generated_loss(batch, model_output, extra):\n    \"\"\"\n    Adaptive Rank-Gap Margin Loss.\n\n    Args:\n        batch (dict): A dictionary containing tensors from the data loader.\n                      Expected keys: 'cost_a', 'cost_b', 'log_prob_w', 'log_prob_l'.\n                      'cost_a' and 'log_prob_w' correspond to the preferred solution (winner).\n                      'cost_b' and 'log_prob_l' correspond to the non-preferred solution (loser).\n                      An optional 'weight' tensor can be used for sample weighting.\n        model_output (dict): Output from the model (not used in this loss structure).\n        extra (dict): A dictionary for hyperparameters, e.g., {'beta': 1.0, 'margin_scale': 1.0}.\n\n    Returns:\n        torch.Tensor: A scalar tensor representing the mean loss.\n    \"\"\"\n    # Read hyperparameters\n    beta = extra.get('beta', 1.0)\n    margin_scale = extra.get('margin_scale', 1.0)\n\n    # In our setting, 'a' is the winner (w) and 'b' is the loser (l).\n    # cost_a < cost_b\n    cost_a = batch['cost_a']\n    cost_b = batch['cost_b']\n    logp_a = batch['log_prob_w']\n    logp_b = batch['log_prob_l']\n\n    # 1. Calculate cost and log-probability differences\n    delta_cost = cost_b - cost_a\n    delta_logp = logp_a - logp_b\n\n    # 2. (New Coupling) Normalize cost difference using rank-gap\n    # This provides batch-aware normalization that is robust to outliers.\n    rank_norm_cost = rank_gap(delta_cost)\n\n    # 3. (New Coupling) Compute adaptive margin with softplus\n    # This creates a non-saturating margin that grows with the rank of the cost gap.\n    margin = margin_scale * F.softplus(rank_norm_cost)\n\n    # 4. Compute core preference loss (inherited from parents)\n    core_loss = -F.logsigmoid(delta_logp - margin)\n\n    # 5. Compute cost-based weight (inherited from parents)\n    weight_scale = F.softplus(beta * delta_cost)\n\n    # 6. Combine core loss and scaling factor\n    loss_per_sample = weight_scale * core_loss\n\n    # 7. Apply optional sample weights if provided\n    sample_weights = batch.get('weight', None)\n    if sample_weights is not None:\n        loss_per_sample = loss_per_sample * sample_weights\n\n    # 8. Return the mean loss over the batch\n    return loss_per_sample.mean()"}, "static_ok": true, "static_reason": "", "dynamic_ok": false, "dynamic_reason": "forward_error: name 'rank_gap' is not defined", "loss_value": null, "grad_norm": null}
{"generation": 6, "index": 3, "ir": {"name": "Softplus-Scaled Sigmoid Margin Loss with Tanh-Rank Normalization", "intuition": "This loss function synthesizes the adaptive margin concept from both parents with a novel normalization and weighting scheme. \n\nInherited Ideas:\n- From both parents, it inherits the core structure of an adaptive margin loss: `loss = -logsigmoid(delta_logp - margin)`. The margin is not fixed but is a function of the cost difference, making the required separation between winner and loser log-probabilities dependent on the magnitude of their cost gap.\n- From both parents, it also inherits the idea of scaling the entire loss by a `softplus` of the cost difference (`softplus(beta * delta_cost)`). This focuses training on pairs with more significant cost differences.\n\nNew Coupling Ideas:\n1.  **Tanh-Rank Normalization**: Instead of normalizing the raw cost difference directly (which can be sensitive to outliers), this loss first computes the `rank_gap` of the cost differences within the batch. This rank is then scaled and passed through a `tanh` function. This approach makes the margin's calculation robust to the absolute scale and distribution of costs, depending only on the relative ordering of cost differences in the batch. It combines the batch-relative strength of ranking with the smooth, bounded properties of `tanh`.\n2.  **Dynamic Log-Probability Scaling**: A second coupling is introduced by dynamically scaling the log-probability difference (`delta_logp`) itself. The scaling factor is `softplus(gamma * norm_delta_cost)`, where `norm_delta_cost` is the normalized cost difference. For pairs with small cost differences, this factor is close to 1, leaving `delta_logp` mostly unchanged. For pairs with large cost differences, it amplifies `delta_logp`, effectively increasing the learning rate on the model's predictions for more 'obvious' preferences. This encourages the model to be more confident about high-stakes decisions without destabilizing learning on subtle ones.", "pseudocode": "1. Calculate the raw cost difference for the batch: delta_cost = cost(b) - cost(a).\n2. (New Coupling 1) Normalize the cost difference using Tanh-Rank Normalization:\n   a. Compute the rank gap of delta_cost within the batch: rank_gaps = rank_gap(delta_cost).\n   b. Normalize the rank gaps using a scaled tanh function: norm_delta_cost = tanh(rank_gaps / norm_scale).\n3. (Inherited) Compute the adaptive margin based on the normalized cost: margin = max_margin * sigmoid(norm_delta_cost).\n4. Calculate the raw log-probability difference: raw_delta_logp = logp(a) - logp(b).\n5. (New Coupling 2) Compute a dynamic log-probability scaling factor: logp_scale = softplus(gamma * norm_delta_cost).\n6. Apply the scaling to the log-probability difference: delta_logp = logp_scale * raw_delta_logp.\n7. (Inherited) Compute the core preference loss: core_loss = -logsigmoid(delta_logp - margin).\n8. (Inherited) Calculate a cost-based weight: weight_scale = softplus(beta * delta_cost).\n9. Combine them: final_loss = weight_scale * core_loss.\n10. Return the mean of final_loss over the batch.", "hyperparams": {"beta": 1.0, "max_margin": 2.0, "norm_scale": 0.5, "gamma": 0.5}, "operators_used": ["logsigmoid", "softplus", "sigmoid", "tanh", "rank_gap"], "implementation_hint": {"expects": ["cost_a", "cost_b", "logp_a", "logp_b"], "returns": "scalar"}, "code": "import torch\nimport torch.nn.functional as F\n\ndef rank_gap(x):\n    \"\"\"Computes the rank of each element in a 1D tensor and normalizes it to [-1, 1].\"\"\"\n    ranks = torch.empty_like(x, dtype=torch.float)\n    ranks[x.argsort()] = torch.arange(len(x), device=x.device, dtype=torch.float)\n    # Normalize ranks to be in the range [-1, 1]\n    # Subtracting 0.5 * (len(x) - 1) centers the ranks around 0\n    # Dividing by 0.5 * (len(x) - 1) scales them to [-1, 1]\n    # Add a small epsilon to the denominator to avoid division by zero for a batch of size 1.\n    n = len(x)\n    if n > 1:\n        ranks = (ranks - 0.5 * (n - 1)) / (0.5 * (n - 1) + 1e-8)\n    else:\n        ranks.fill_(0.0) # A single element has rank 0\n    return ranks\n\ndef generated_loss(batch, model_output, extra):\n    \"\"\"\n    Softplus-Scaled Sigmoid Margin Loss with Tanh-Rank Normalization.\n\n    Args:\n        batch (dict): A dictionary containing tensors from the data loader.\n                      Expected keys: 'cost_a', 'cost_b', 'log_prob_w', 'log_prob_l'.\n                      'cost_a' and 'log_prob_w' correspond to the preferred solution (winner).\n                      'cost_b' and 'log_prob_l' correspond to the non-preferred solution (loser).\n                      An optional 'weight' tensor can be used for sample weighting.\n        model_output (dict): Output from the model (not used in this loss structure).\n        extra (dict): A dictionary for hyperparameters, e.g., {'beta': 1.0, 'max_margin': 2.0, 'norm_scale': 0.5, 'gamma': 0.5}.\n\n    Returns:\n        torch.Tensor: A scalar tensor representing the mean loss.\n    \"\"\"\n    # Read hyperparameters\n    beta = extra.get('beta', 1.0)\n    max_margin = extra.get('max_margin', 2.0)\n    norm_scale = extra.get('norm_scale', 0.5)\n    gamma = extra.get('gamma', 0.5)\n\n    # 'a' is the winner (w), 'b' is the loser (l). cost_a < cost_b\n    cost_a = batch['cost_a']\n    cost_b = batch['cost_b']\n    logp_a = batch['log_prob_w']\n    logp_b = batch['log_prob_l']\n\n    delta_cost = cost_b - cost_a\n    raw_delta_logp = logp_a - logp_b\n\n    # 1. (New Coupling 1) Tanh-Rank Normalization\n    # This is robust to cost outliers and scale, focusing on relative preference strength.\n    with torch.no_grad():\n        cost_rank_gaps = rank_gap(delta_cost)\n    norm_delta_cost = torch.tanh(cost_rank_gaps / norm_scale)\n\n    # 2. (Inherited) Compute adaptive margin from normalized cost ranks\n    margin = max_margin * torch.sigmoid(norm_delta_cost)\n\n    # 3. (New Coupling 2) Dynamically scale delta_logp\n    # This amplifies the learning signal for pairs with a higher rank-gap in cost.\n    logp_scale = F.softplus(gamma * norm_delta_cost)\n    delta_logp = logp_scale * raw_delta_logp\n\n    # 4. (Inherited) Compute the core preference loss\n    core_loss = -F.logsigmoid(delta_logp - margin)\n\n    # 5. (Inherited) Compute a cost-based scaling factor\n    weight_scale = F.softplus(beta * delta_cost)\n\n    # 6. Combine core loss and scaling factor\n    loss_per_sample = weight_scale * core_loss\n\n    # 7. Apply optional sample weights if provided\n    sample_weights = batch.get('weight', None)\n    if sample_weights is not None:\n        loss_per_sample = loss_per_sample * sample_weights\n\n    # 8. Return the mean loss over the batch\n    return loss_per_sample.mean()"}, "static_ok": true, "static_reason": "", "dynamic_ok": false, "dynamic_reason": "forward_error: name 'rank_gap' is not defined", "loss_value": null, "grad_norm": null}
{"generation": 6, "index": 4, "ir": {"name": "Adaptive Logit-Margin Loss with Exponential Weighting", "intuition": "This loss function synthesizes and extends ideas from its parents to create a robust and adaptive preference learning objective. It inherits the core logistic loss structure (`-logsigmoid`) and the use of an adaptive, cost-sensitive margin from both parents. The margin's adaptivity is achieved by making it a function of the cost difference between the preferred and non-preferred solutions. From both parents, it also inherits the idea of scaling the loss based on the magnitude of the cost difference, focusing training on more significant preference pairs.\n\nThe first new coupling idea is a different method for calculating the margin. Instead of a sigmoid function, it uses a 'logit-margin' approach: `margin = max_margin * log(1 + exp(delta_cost / norm_scale)) / (1 + log(1 + exp(delta_cost / norm_scale)))`. This creates a margin that grows from 0 towards `max_margin` but has a gentler slope for small cost differences compared to sigmoid, while still being bounded. This can make the model less sensitive to noisy preferences near zero cost difference.\n\nThe second new coupling idea is a novel weighting scheme. Instead of using `softplus` on the cost difference, which can grow linearly, this loss uses an exponential function `exp(beta * tanh(delta_cost / norm_scale))`. The `tanh` normalization inside the exponential ensures the weight is bounded (between 1 and `exp(beta)`), preventing extremely large cost differences from creating unstable, exploding loss values. This provides a stable, bounded amplification of the loss for more significant preferences, combining the stability of `tanh` normalization (from Parent 1) with a powerful exponential weighting.", "pseudocode": "1. Calculate the cost difference: delta_cost = cost(b) - cost(a).\n2. Calculate the log-probability difference: delta_logp = logp(a) - logp(b).\n3. (Inherited) Define the base logistic loss structure: loss = -(logp(a) - logp(b) - margin).\n4. (New Coupling 1) Compute an adaptive 'logit-margin'. First, calculate a softplus-like term: term = log(1 + exp(delta_cost / norm_scale)). Then compute the margin: margin = max_margin * term / (1 + term). This creates a margin that smoothly grows from 0 to max_margin.\n5. (New Coupling 2) Compute a bounded exponential weight. First, normalize the cost difference with tanh: norm_delta_cost = tanh(delta_cost / norm_scale). Then compute the weight: weight_scale = exp(beta * norm_delta_cost). This weight is bounded between exp(0)=1 and exp(beta).\n6. Combine the components into the final loss expression: final_loss = weight_scale * logsigmoid(margin - delta_logp). This is equivalent to weight_scale * -logsigmoid(delta_logp - margin) but can be more stable.\n7. Return the mean of final_loss over the batch.", "hyperparams": {"beta": 1.0, "max_margin": 2.0, "norm_scale": 10.0}, "operators_used": ["logsigmoid", "exp", "log", "tanh"], "implementation_hint": {"expects": ["cost_a", "cost_b", "logp_a", "logp_b"], "returns": "scalar"}, "code": "import torch\nimport torch.nn.functional as F\n\ndef generated_loss(batch, model_output, extra):\n    \"\"\"\n    Adaptive Logit-Margin Loss with Exponential Weighting.\n\n    Args:\n        batch (dict): A dictionary containing tensors from the data loader.\n                      Expected keys: 'cost_a', 'cost_b', 'log_prob_w', 'log_prob_l'.\n                      'cost_a' and 'log_prob_w' correspond to the preferred solution (winner).\n                      'cost_b' and 'log_prob_l' correspond to the non-preferred solution (loser).\n                      An optional 'weight' tensor can be used for sample weighting.\n        model_output (dict): Output from the model (not used in this loss structure).\n        extra (dict): A dictionary for hyperparameters, e.g., {'beta': 1.0, 'max_margin': 2.0, 'norm_scale': 10.0}.\n\n    Returns:\n        torch.Tensor: A scalar tensor representing the mean loss.\n    \"\"\"\n    # Read hyperparameters\n    beta = extra.get('beta', 1.0)\n    max_margin = extra.get('max_margin', 2.0)\n    norm_scale = extra.get('norm_scale', 10.0)\n\n    # In our setting, 'a' is the winner (w) and 'b' is the loser (l).\n    # cost_a < cost_b\n    cost_a = batch['cost_a']\n    cost_b = batch['cost_b']\n    logp_a = batch['log_prob_w']\n    logp_b = batch['log_prob_l']\n\n    # 1. Calculate cost and log-probability differences\n    # delta_cost should be positive since cost_b > cost_a\n    delta_cost = cost_b - cost_a\n    delta_logp = logp_a - logp_b\n\n    # 2. (New Coupling 1) Compute the adaptive 'logit-margin'\n    # This margin function grows smoothly from 0 to max_margin.\n    # It is a variation of softplus, normalized to be bounded.\n    # term = log(1 + exp(delta_cost / norm_scale)) is equivalent to softplus(delta_cost / norm_scale)\n    # We use the log-sum-exp trick for stability: log(exp(0) + exp(x)) = max(0,x) + log(1+exp(-|x|))\n    x = delta_cost / norm_scale\n    term = torch.max(torch.zeros_like(x), x) + torch.log1p(torch.exp(-torch.abs(x)))\n    margin = max_margin * term / (1.0 + term)\n\n    # 3. (New Coupling 2) Compute a bounded exponential weight\n    # The tanh normalization ensures the weight is bounded between 1 and exp(beta),\n    # preventing exploding loss values from very large cost differences.\n    norm_delta_cost = torch.tanh(delta_cost / norm_scale)\n    weight_scale = torch.exp(beta * norm_delta_cost)\n\n    # 4. Compute the core preference loss (Bradley-Terry style with margin)\n    # We want logp_a > logp_b + margin, so (delta_logp - margin) should be positive.\n    # The loss is -logsigmoid(delta_logp - margin).\n    # This is equivalent to logsigmoid(-(delta_logp - margin)) = logsigmoid(margin - delta_logp).\n    core_loss = F.logsigmoid(margin - delta_logp)\n\n    # 5. Combine weight and loss. The negative sign is included here.\n    loss_per_sample = -weight_scale * core_loss\n\n    # 6. Apply optional sample weights if provided\n    sample_weights = batch.get('weight', None)\n    if sample_weights is not None:\n        loss_per_sample = loss_per_sample * sample_weights\n\n    # 7. Return the mean loss over the batch\n    return loss_per_sample.mean()"}, "static_ok": true, "static_reason": "", "dynamic_ok": true, "dynamic_reason": "ok", "loss_value": 0.3920140266418457, "grad_norm": 0.0}
{"generation": 6, "index": 5, "ir": {"name": "Rank-Modulated Tanh-Margin Loss", "intuition": "This loss function creates a robust preference learning objective by combining ideas from its parents and introducing a new rank-based modulation.\n\nInherited Ideas:\n- From 'Adaptive Sigmoid Margin Loss with Tanh Normalization' (Parent 1), it inherits the core concept of an adaptive margin that is a function of a normalized cost difference. Specifically, it uses a scaled `tanh` function to normalize the cost gap, creating a bounded, batch-independent signal.\n- From 'Clipped Tanh-Normalized Adaptive Margin Loss' (Parent 0), it inherits the idea of scaling the entire loss term to emphasize more significant preferences. However, instead of using `softplus` on the raw cost difference, it adapts this idea to use a `softplus` scaling on a normalized, rank-aware signal for better stability and focus.\n\nNew Coupling Ideas:\n1.  **Rank-Gap Normalization**: Instead of normalizing the raw cost difference directly, this loss first computes the `rank_gap` of the costs within the batch. This transforms the costs into a more uniform distribution, making the subsequent `tanh` normalization less sensitive to outliers or skewed cost distributions. This focuses the loss on the relative ranking of preferences within a batch, not just their absolute cost magnitudes.\n2.  **Margin as Direct Tanh Output**: Instead of passing the normalized value through a `sigmoid` to create the margin, this loss uses the output of the `tanh` function directly as the margin (scaled by `max_margin`). This provides a symmetric margin around zero. For pairs with very similar costs (rank gap near zero), the margin is near zero, requiring almost no preference. For pairs with large cost differences, the margin approaches `+/- max_margin`, demanding a strong separation.\n\nThe final loss is a standard logistic loss (`-logsigmoid`) where the log-probability difference `(logp(a) - logp(b))` is pushed to exceed this rank-aware, tanh-based margin. The entire term is then weighted by a `softplus` of the same normalized rank gap, smoothly amplifying the loss for pairs that are further apart in the batch's preference ranking.", "pseudocode": "1. For the entire batch, calculate the rank-gap of the costs: rank_gaps = rank_gap(cost_b) - rank_gap(cost_a). This is a new coupling idea that normalizes costs based on their relative standing in the batch.\n2. Normalize the rank-gap using a scaled tanh function: norm_rank_gap = tanh(rank_gaps / norm_scale). This is inspired by Parent 1's tanh normalization but applied to ranks.\n3. Compute an adaptive margin directly from the normalized rank-gap: margin = max_margin * norm_rank_gap. This is a new coupling idea, using tanh output directly for a symmetric margin.\n4. Calculate the log-probability difference: delta_logp = logp(a) - logp(b).\n5. Compute the core preference loss: core_loss = -logsigmoid(delta_logp - margin).\n6. Calculate a rank-based weight: weight_scale = softplus(beta * norm_rank_gap). This is an adaptation of the weighting idea from both parents, now based on the normalized rank gap.\n7. Combine them: final_loss = weight_scale * core_loss.\n8. Return the mean of final_loss over the batch.", "hyperparams": {"beta": 1.0, "max_margin": 1.5, "norm_scale": 0.5}, "operators_used": ["logsigmoid", "softplus", "tanh", "rank_gap"], "implementation_hint": {"expects": ["cost_a", "cost_b", "logp_a", "logp_b"], "returns": "scalar"}, "code": "import torch\nimport torch.nn.functional as F\n\ndef rank_gap(x):\n    \"\"\"Computes the rank of each element in x and normalizes it to [-0.5, 0.5].\"\"\"\n    # Sort the tensor and get the indices\n    sorted_indices = torch.argsort(x)\n    \n    # Create a tensor to store the ranks\n    ranks = torch.empty_like(x, dtype=torch.float)\n    \n    # Assign ranks based on sorted order. The rank is the position in the sorted list.\n    # The clone().scatter_() pattern is a stable way to do this on GPU.\n    ranks.scatter_(0, sorted_indices, torch.arange(len(x), device=x.device, dtype=torch.float))\n    \n    # Normalize ranks to be in the range [-0.5, 0.5] for better scaling with tanh\n    # Subtracting 0.5 from the standard [0, 1] normalization centers it around 0.\n    return ranks / (len(x) - 1 + 1e-9) - 0.5\n\ndef generated_loss(batch, model_output, extra):\n    \"\"\"\n    Rank-Modulated Tanh-Margin Loss.\n\n    Args:\n        batch (dict): A dictionary containing tensors from the data loader.\n                      Expected keys: 'cost_a', 'cost_b', 'log_prob_w', 'log_prob_l'.\n                      'cost_a' and 'log_prob_w' correspond to the preferred solution (winner).\n                      'cost_b' and 'log_prob_l' correspond to the non-preferred solution (loser).\n                      An optional 'weight' tensor can be used for sample weighting.\n        model_output (dict): Output from the model (not used in this loss structure).\n        extra (dict): A dictionary for hyperparameters, e.g., {'beta': 1.0, 'max_margin': 1.5, 'norm_scale': 0.5}.\n\n    Returns:\n        torch.Tensor: A scalar tensor representing the mean loss.\n    \"\"\"\n    # Read hyperparameters\n    beta = extra.get('beta', 1.0)\n    max_margin = extra.get('max_margin', 1.5)\n    norm_scale = extra.get('norm_scale', 0.5)\n\n    # In our setting, 'a' is the winner (w) and 'b' is the loser (l).\n    # cost_a < cost_b\n    cost_a = batch['cost_a']\n    cost_b = batch['cost_b']\n    logp_a = batch['log_prob_w']\n    logp_b = batch['log_prob_l']\n\n    # 1. (New Coupling) Compute rank-gap of costs within the batch.\n    # This makes the loss robust to the scale of costs.\n    # We concatenate to rank all costs together.\n    all_costs = torch.cat([cost_a, cost_b])\n    all_ranks = rank_gap(all_costs)\n    rank_a, rank_b = torch.chunk(all_ranks, 2)\n    delta_rank = rank_b - rank_a # Should be positive\n\n    # 2. Normalize the rank difference with tanh (inspired by Parent 1)\n    norm_rank_gap = torch.tanh(delta_rank / norm_scale)\n\n    # 3. (New Coupling) Compute margin directly from tanh output\n    margin = max_margin * norm_rank_gap\n\n    # 4. Calculate log-probability difference\n    delta_logp = logp_a - logp_b\n\n    # 5. Compute the core preference loss (Bradley-Terry style with margin)\n    core_loss = -F.logsigmoid(delta_logp - margin)\n\n    # 6. Compute a rank-based scaling factor (inspired by both parents)\n    weight_scale = F.softplus(beta * norm_rank_gap)\n\n    # 7. Combine core loss and scaling factor\n    loss_per_sample = weight_scale * core_loss\n\n    # 8. Apply optional sample weights if provided\n    sample_weights = batch.get('weight', None)\n    if sample_weights is not None:\n        loss_per_sample = loss_per_sample * sample_weights\n\n    # 9. Return the mean loss over the batch\n    return loss_per_sample.mean()"}, "static_ok": true, "static_reason": "", "dynamic_ok": false, "dynamic_reason": "forward_error: name 'rank_gap' is not defined", "loss_value": null, "grad_norm": null}
{"generation": 6, "index": 6, "ir": {"name": "Soft-Clipped Tanh-Margin Loss with Rank-Gap Normalization", "intuition": "This loss function creates a robust preference objective by inheriting and combining key features from its parents, while introducing novel coupling ideas for improved stability and adaptivity.\n\nInherited Ideas:\n- From Parent 1 (Adaptive Sigmoid Margin Loss with Tanh Normalization), it inherits the core concept of an adaptive margin, where the required separation in log-probabilities (`delta_logp`) scales with the cost difference. This is achieved via `margin = max_margin * sigmoid(...)`.\n- From Parent 0 (Clipped Tanh-Normalized Adaptive Margin Loss), it inherits the idea of bounding the influence of the cost difference on the margin. However, instead of a hard `clamp`, this child uses a `tanh` function as a soft-clipping mechanism on the normalized cost, providing a smooth, differentiable bound.\n\nNew Coupling Ideas:\n1. **Rank-Gap Normalization**: Instead of using a simple `tanh` on the raw cost difference, this loss first normalizes the cost difference using `rank_gap`. `rank_gap(delta_cost)` transforms the cost differences into a new space where the values are sensitive to their relative ranking within the batch. This makes the normalization robust to outliers and varying scales of cost differences across different batches. The output of `rank_gap` is then passed through a `tanh` function, coupling batch-relative information with a smooth, bounded activation.\n2. **Dynamic Beta from Normalized Rank-Gap**: The `beta` parameter, which scales the overall loss magnitude, is made dynamic. It is modulated by the absolute value of the tanh-normalized rank-gap (`tanh(rank_gap(delta_cost))`). This means that pairs with a small relative cost difference (a small rank-gap) within the batch are down-weighted, focusing the training effort on the most clearly separated preference pairs in that specific batch. This prevents the model from being over-penalized for small or noisy preferences relative to the current batch context.", "pseudocode": "1. Calculate the cost difference: delta_cost = cost(b) - cost(a).\n2. (New Coupling) Normalize the cost difference using rank-gap normalization: rank_gap_cost = rank_gap(delta_cost). This makes the normalization robust to outliers and sensitive to the relative ordering of cost differences within the batch.\n3. (Inherited from Parent 0, modified) Apply a soft-clipping mechanism using tanh on the normalized cost: clipped_norm_cost = tanh(rank_gap_cost). This smoothly bounds the influence of the cost difference.\n4. (Inherited from Parent 1) Compute an adaptive margin based on the sigmoid of the clipped, normalized value: margin = max_margin * sigmoid(clipped_norm_cost).\n5. Calculate the log-probability difference: delta_logp = logp(a) - logp(b).\n6. Compute the core preference loss using a standard logistic objective: core_loss = -logsigmoid(delta_logp - margin).\n7. (New Coupling) Compute a dynamic beta schedule based on the normalized rank-gap: dynamic_beta = base_beta * abs(clipped_norm_cost). This down-weights pairs with a small relative cost difference in the batch.\n8. Calculate a cost-based weight using the dynamic beta and softplus: weight_scale = softplus(dynamic_beta * delta_cost).\n9. Combine them: final_loss = weight_scale * core_loss.\n10. Return the mean of final_loss over the batch.", "hyperparams": {"base_beta": 1.0, "max_margin": 2.0}, "operators_used": ["logsigmoid", "softplus", "sigmoid", "tanh", "rank_gap"], "implementation_hint": {"expects": ["cost_a", "cost_b", "logp_a", "logp_b"], "returns": "scalar"}, "code": "import torch\nimport torch.nn.functional as F\n\ndef rank_gap(x):\n    \"\"\"Computes the rank-normalized gap for a tensor x.\"\"\"\n    if x.numel() <= 1:\n        return torch.zeros_like(x) # Avoid division by zero for single-element batches\n    \n    # Sort the tensor and get the indices\n    sorted_indices = torch.argsort(x)\n    \n    # Create ranks from 0 to N-1\n    ranks = torch.empty_like(sorted_indices, dtype=torch.float)\n    ranks[sorted_indices] = torch.arange(x.numel(), device=x.device, dtype=torch.float)\n    \n    # Normalize ranks to be in the range [-0.5, 0.5]\n    normalized_ranks = (ranks / (x.numel() - 1)) - 0.5\n    \n    # Scale to [-3, 3] to cover the sensitive range of tanh\n    return normalized_ranks * 6.0\n\ndef generated_loss(batch, model_output, extra):\n    \"\"\"\n    Soft-Clipped Tanh-Margin Loss with Rank-Gap Normalization.\n\n    Args:\n        batch (dict): A dictionary containing tensors from the data loader.\n                      Expected keys: 'cost_a', 'cost_b', 'log_prob_w', 'log_prob_l'.\n                      'cost_a' and 'log_prob_w' correspond to the preferred solution (winner).\n                      'cost_b' and 'log_prob_l' correspond to the non-preferred solution (loser).\n                      An optional 'weight' tensor can be used for sample weighting.\n        model_output (dict): Output from the model (not used in this loss structure).\n        extra (dict): A dictionary for hyperparameters, e.g., {'base_beta': 1.0, 'max_margin': 2.0}.\n\n    Returns:\n        torch.Tensor: A scalar tensor representing the mean loss.\n    \"\"\"\n    # Read hyperparameters\n    base_beta = extra.get('base_beta', 1.0)\n    max_margin = extra.get('max_margin', 2.0)\n\n    # In our setting, 'a' is the winner (w) and 'b' is the loser (l).\n    cost_a = batch['cost_a']\n    cost_b = batch['cost_b']\n    logp_a = batch['log_prob_w']\n    logp_b = batch['log_prob_l']\n\n    # 1. Calculate cost and log-probability differences\n    delta_cost = cost_b - cost_a\n    delta_logp = logp_a - logp_b\n\n    # 2. (New Coupling) Normalize cost difference using rank_gap for batch-relative scaling\n    rank_gap_cost = rank_gap(delta_cost)\n\n    # 3. (Inherited from Parent 0, modified) Apply soft-clipping with tanh\n    clipped_norm_cost = torch.tanh(rank_gap_cost)\n\n    # 4. (Inherited from Parent 1) Compute the adaptive sigmoid margin\n    margin = max_margin * torch.sigmoid(clipped_norm_cost)\n\n    # 5. Compute the core preference loss\n    core_loss = -F.logsigmoid(delta_logp - margin)\n\n    # 6. (New Coupling) Compute dynamic beta from the normalized rank-gap\n    dynamic_beta = base_beta * clipped_norm_cost.abs()\n\n    # 7. Compute a cost-based scaling factor using dynamic beta and softplus\n    weight_scale = F.softplus(dynamic_beta * delta_cost)\n\n    # 8. Combine core loss and scaling factor\n    loss_per_sample = weight_scale * core_loss\n\n    # 9. Apply optional sample weights if provided\n    sample_weights = batch.get('weight', None)\n    if sample_weights is not None:\n        loss_per_sample = loss_per_sample * sample_weights\n\n    # 10. Return the mean loss over the batch\n    return loss_per_sample.mean()"}, "static_ok": true, "static_reason": "", "dynamic_ok": false, "dynamic_reason": "forward_error: name 'rank_gap' is not defined", "loss_value": null, "grad_norm": null}
{"generation": 6, "index": 7, "ir": {"name": "Softplus-Normalized Sigmoid Margin Loss with Exponential Weighting", "intuition": "This loss function synthesizes ideas from its parents to create a robust, cost-sensitive preference objective. It inherits the core structure of an adaptive margin loss from both parents, where the margin that the model's log-probability difference must overcome is a function of the cost difference between the preferred and non-preferred solutions. From both parents, it also inherits the use of a logistic loss (`-logsigmoid`) as the fundamental preference objective.\n\nThe first new coupling idea is a different normalization scheme. Instead of `tanh` (Parent 1) or clipping (Parent 0), this child uses a scaled `softplus` function to normalize the cost difference. This creates a non-negative, unbounded normalization that grows smoothly and non-linearly with the cost gap, offering a different inductive bias compared to the bounded `tanh` normalization. This `softplus` normalized value is then used within a `sigmoid` function to compute the adaptive margin, similar to both parents.\n\nThe second new coupling idea is a novel weighting mechanism. Instead of scaling the loss with `softplus(beta * delta_cost)` as both parents do, this child uses an exponential weighting `exp(beta * norm_delta_cost)`. This couples the loss weighting directly to the *normalized* cost difference. Using `exp` creates a sharper penalty gradient for pairs with larger normalized cost gaps, aggressively focusing the model on learning from the most significant preference signals as defined by the `softplus` normalization. This avoids potential numerical instability from `exp(raw_delta_cost)` while still providing strong, exponential emphasis.", "pseudocode": "1. Calculate the cost difference: delta_cost = cost(b) - cost(a).\n2. (New Coupling 1) Normalize the cost difference using a scaled softplus function: norm_delta_cost = softplus(delta_cost / norm_scale). This provides a smooth, non-negative, and unbounded normalization.\n3. (Inherited) Compute an adaptive margin based on the normalized cost difference: margin = max_margin * sigmoid(norm_delta_cost). The margin is small for small cost gaps and grows towards max_margin.\n4. (Inherited) Calculate the log-probability difference: delta_logp = logp(a) - logp(b).\n5. (Inherited) Compute the core preference loss: core_loss = -logsigmoid(delta_logp - margin).\n6. (New Coupling 2) Calculate an exponential weight based on the normalized cost difference: weight_scale = exp(beta * norm_delta_cost). This sharply increases the loss weight for pairs with larger normalized cost gaps.\n7. Combine them: final_loss = weight_scale * core_loss.\n8. Return the mean of final_loss over the batch.", "hyperparams": {"beta": 0.5, "max_margin": 2.0, "norm_scale": 10.0}, "operators_used": ["logsigmoid", "softplus", "sigmoid", "exp"], "implementation_hint": {"expects": ["cost_a", "cost_b", "logp_a", "logp_b", "weight"], "returns": "scalar"}, "code": "import torch\nimport torch.nn.functional as F\n\ndef generated_loss(batch, model_output, extra):\n    \"\"\"\n    Softplus-Normalized Sigmoid Margin Loss with Exponential Weighting.\n\n    Args:\n        batch (dict): A dictionary containing tensors from the data loader.\n                      Expected keys: 'cost_a', 'cost_b', 'log_prob_w', 'log_prob_l'.\n                      'cost_a' and 'log_prob_w' correspond to the preferred solution (winner).\n                      'cost_b' and 'log_prob_l' correspond to the non-preferred solution (loser).\n                      An optional 'weight' tensor can be used for sample weighting.\n        model_output (dict): Output from the model (not used in this loss structure).\n        extra (dict): A dictionary for hyperparameters, e.g., {'beta': 0.5, 'max_margin': 2.0, 'norm_scale': 10.0}.\n\n    Returns:\n        torch.Tensor: A scalar tensor representing the mean loss.\n    \"\"\"\n    # Read hyperparameters\n    beta = extra.get('beta', 0.5)\n    max_margin = extra.get('max_margin', 2.0)\n    norm_scale = extra.get('norm_scale', 10.0)\n\n    # In our setting, 'a' is the winner (w) and 'b' is the loser (l).\n    # cost_a < cost_b\n    cost_a = batch['cost_a']\n    cost_b = batch['cost_b']\n    logp_a = batch['log_prob_w']\n    logp_b = batch['log_prob_l']\n\n    # 1. Calculate cost and log-probability differences\n    # delta_cost should be positive since cost_b > cost_a\n    delta_cost = cost_b - cost_a\n    delta_logp = logp_a - logp_b\n\n    # 2. (New Coupling 1) Normalize the cost difference using a scaled softplus function.\n    # This provides a smooth, non-negative, and unbounded normalization.\n    norm_delta_cost = F.softplus(delta_cost / norm_scale)\n\n    # 3. (Inherited) Compute the adaptive sigmoid margin.\n    # The margin is a function of the normalized cost difference.\n    margin = max_margin * torch.sigmoid(norm_delta_cost)\n\n    # 4. (Inherited) Compute the core preference loss (Bradley-Terry style with margin).\n    core_loss = -F.logsigmoid(delta_logp - margin)\n\n    # 5. (New Coupling 2) Compute an exponential scaling factor based on the *normalized* cost difference.\n    # This sharply amplifies the loss for pairs with larger normalized cost differences.\n    weight_scale = torch.exp(beta * norm_delta_cost)\n\n    # 6. Combine core loss and scaling factor\n    loss_per_sample = weight_scale * core_loss\n\n    # 7. Apply optional sample weights if provided\n    sample_weights = batch.get('weight', None)\n    if sample_weights is not None:\n        loss_per_sample = loss_per_sample * sample_weights\n\n    # 8. Return the mean loss over the batch\n    return loss_per_sample.mean()"}, "static_ok": true, "static_reason": "", "dynamic_ok": true, "dynamic_reason": "ok", "loss_value": 2.3000099658966064, "grad_norm": 0.0}
{"generation": 7, "index": 0, "ir": {"name": "Adaptive Logit-Scaled Margin Loss with Tanh-Softplus Weighting", "intuition": "This loss function creates a dynamic, cost-sensitive margin that is directly proportional to the log-odds (logit) of a sigmoid-transformed cost difference. This inherits the core idea of an adaptive margin from both parents, but reframes its calculation. Instead of using the sigmoid output directly, we use `log(sigmoid(x) / (1 - sigmoid(x)))`, which is simply `x`. This means the margin is `max_margin * tanh(delta_cost / norm_scale)`, creating a bounded, linear-in-the-middle relationship between the normalized cost gap and the required log-probability separation.\n\nFrom both parents, it inherits the use of a scaled `tanh` function to provide a batch-independent normalization of the cost difference (`delta_cost`). It also inherits the overall structure of a Bradley-Terry style loss, `logsigmoid(delta_logp - margin)`, and the concept of weighting the loss by the magnitude of the cost difference.\n\nAs a new coupling idea, this child loss introduces a novel weighting mechanism. Instead of scaling the loss by `softplus(beta * delta_cost)` as the parents do, it scales the loss by `softplus(tanh(delta_cost / norm_scale))`. This has two benefits: 1) It makes the weighting factor less sensitive to extreme outliers in `delta_cost`, as the `tanh` function bounds its input. 2) It couples the weighting scale directly to the same normalized cost representation used for the margin, creating a more consistent and stable response to the cost gap. A second new idea is the introduction of a `margin_offset` hyperparameter, which adds a small, constant baseline to the margin. This ensures that even for pairs with a near-zero cost difference, the model is still encouraged to maintain a minimal separation, preventing complete indifference.", "pseudocode": "1. Calculate the cost difference: delta_cost = cost(b) - cost(a).\n2. Calculate the log-probability difference: delta_logp = logp(a) - logp(b).\n3. (Inherited) Normalize the cost difference using a scaled tanh function: norm_delta_cost = tanh(delta_cost / norm_scale).\n4. (New Coupling 1) Compute the adaptive margin. Instead of a sigmoid, the margin is now directly proportional to the tanh-normalized cost, plus a small offset: margin = margin_offset + max_margin * norm_delta_cost. This simplifies the margin calculation while retaining the desired adaptive behavior.\n5. (New Coupling 2) Compute a bounded, cost-based weight. The weight is a softplus of the tanh-normalized cost difference, making it robust to cost outliers: weight_scale = softplus(beta * norm_delta_cost).\n6. Compute the core preference loss using the adaptive margin: core_loss = -logsigmoid(delta_logp - margin).\n7. Combine the core loss and the new weight: final_loss = weight_scale * core_loss.\n8. Return the mean of final_loss over the batch.", "hyperparams": {"beta": 1.0, "max_margin": 2.0, "norm_scale": 10.0, "margin_offset": 0.05}, "operators_used": ["logsigmoid", "softplus", "tanh"], "implementation_hint": {"expects": ["cost_a", "cost_b", "log_prob_w", "log_prob_l", "weight"], "returns": "scalar"}, "code": "import torch\nimport torch.nn.functional as F\n\ndef generated_loss(batch, model_output, extra):\n    \"\"\"\n    Adaptive Logit-Scaled Margin Loss with Tanh-Softplus Weighting.\n\n    Args:\n        batch (dict): A dictionary containing tensors from the data loader.\n                      Expected keys: 'cost_a', 'cost_b', 'log_prob_w', 'log_prob_l'.\n                      'cost_a' and 'log_prob_w' correspond to the preferred solution (winner).\n                      'cost_b' and 'log_prob_l' correspond to the non-preferred solution (loser).\n                      An optional 'weight' tensor can be used for sample weighting.\n        model_output (dict): Output from the model (not used in this loss structure).\n        extra (dict): A dictionary for hyperparameters, e.g., {'beta': 1.0, 'max_margin': 2.0, 'norm_scale': 10.0, 'margin_offset': 0.05}.\n\n    Returns:\n        torch.Tensor: A scalar tensor representing the mean loss.\n    \"\"\"\n    # Read hyperparameters from the extra dictionary\n    beta = extra.get('beta', 1.0)\n    max_margin = extra.get('max_margin', 2.0)\n    norm_scale = extra.get('norm_scale', 10.0)\n    margin_offset = extra.get('margin_offset', 0.05)\n\n    # In our setting, 'a' is the winner (w) and 'b' is the loser (l).\n    # Therefore, we expect cost_a < cost_b.\n    cost_a = batch['cost_a']\n    cost_b = batch['cost_b']\n    logp_a = batch['log_prob_w']\n    logp_b = batch['log_prob_l']\n\n    # Calculate cost and log-probability differences\n    # delta_cost should be positive since cost_b > cost_a\n    delta_cost = cost_b - cost_a\n    delta_logp = logp_a - logp_b\n\n    # Inherited Idea: Normalize the cost difference using a scaled tanh function\n    # This provides a bounded (-1, 1) and batch-independent normalization\n    norm_delta_cost = torch.tanh(delta_cost / norm_scale)\n\n    # New Coupling 1: Compute the adaptive margin directly from the tanh-normalized cost\n    # This is equivalent to a logit-scaling of a sigmoid, but simpler.\n    # A small offset ensures a non-zero margin even for tiny cost differences.\n    margin = margin_offset + max_margin * norm_delta_cost\n\n    # The core preference loss (Bradley-Terry style with an adaptive margin)\n    core_loss = -F.logsigmoid(delta_logp - margin)\n\n    # New Coupling 2: Compute a bounded weight scale using softplus on the normalized cost\n    # This makes the weighting robust to extreme delta_cost values, unlike softplus(beta * delta_cost).\n    weight_scale = F.softplus(beta * norm_delta_cost)\n\n    # Combine core loss and the new bounded scaling factor\n    loss_per_sample = weight_scale * core_loss\n\n    # Apply optional sample weights if provided\n    sample_weights = batch.get('weight', None)\n    if sample_weights is not None:\n        loss_per_sample = loss_per_sample * sample_weights\n\n    # Return the mean loss over the batch\n    return loss_per_sample.mean()"}, "static_ok": true, "static_reason": "", "dynamic_ok": true, "dynamic_reason": "ok", "loss_value": 0.6144047379493713, "grad_norm": 0.0}
{"generation": 7, "index": 1, "ir": {"name": "Rank-Aware Adaptive Margin Loss", "intuition": "This loss function creates a hybrid approach by inheriting the core structure of an adaptive sigmoid margin from both parents, while introducing a novel, rank-based normalization scheme and a dynamic beta schedule.\n\nInherited Ideas:\n1.  **Adaptive Sigmoid Margin** (from Parent 0 and 1): The core idea of using a margin that grows with the cost difference is preserved. The margin is calculated as `max_margin * sigmoid(...)`, ensuring that pairs with larger cost differences require a larger log-probability gap to satisfy the loss.\n2.  **Softplus Weighting** (from Parent 0 and 1): The final loss for each sample is weighted by `softplus(beta * delta_cost)`. This common technique amplifies the loss signal for pairs with more significant cost differences, focusing the model's attention on clear-cut preferences.\n\nNew Coupling Ideas:\n1.  **Rank Gap Normalization**: Instead of the `tanh` normalization from the parents, this child introduces a batch-aware `rank_gap` normalization. It computes `(rank(cost_b) - rank(cost_a)) / N`, where N is the batch size. This normalizes the cost difference based on its relative standing within the current batch, making the margin less sensitive to the absolute scale of costs and more attuned to their ordinal relationships. This can improve robustness to cost outliers or shifts in cost distribution between batches.\n2.  **Dynamic Beta from Log-Prob Difference**: A new coupling is introduced where the `beta` parameter, which scales the loss, is modulated by the model's own confidence. Specifically, `dynamic_beta = base_beta * exp(-abs(delta_logp))`. When the model is very confident (large `abs(delta_logp)`), the beta is down-weighted. Conversely, when the model is uncertain (small `abs(delta_logp)`), the beta is larger. This acts as a form of self-paced learning, increasing the penalty on pairs where the model is struggling to establish a clear preference, while reducing the penalty on pairs it already confidently separates.", "pseudocode": "1. For each pair (a, b) in the batch, calculate the cost difference: delta_cost = cost(b) - cost(a).\n2. Calculate the log-probability difference: delta_logp = logp(a) - logp(b).\n3. (New Coupling 1) Normalize the cost difference using rank-gap normalization within the batch: norm_delta_cost = rank_gap(cost_a, cost_b). This maps the difference to a value that reflects its relative magnitude in the batch.\n4. (Inherited) Compute an adaptive margin based on the rank-normalized cost difference: margin = max_margin * sigmoid(norm_scale * norm_delta_cost).\n5. Compute the core preference loss using a logistic objective: core_loss = -logsigmoid(delta_logp - margin).\n6. (New Coupling 2) Compute a dynamic beta based on the model's log-probability difference: dynamic_beta = base_beta * exp(-abs(delta_logp)). This increases the weight for uncertain pairs.\n7. (Inherited) Calculate a cost-based weight using the dynamic beta: weight_scale = softplus(dynamic_beta * delta_cost).\n8. Combine the components: final_loss = weight_scale * core_loss.\n9. Return the mean of final_loss over the batch.", "hyperparams": {"base_beta": 1.0, "max_margin": 2.0, "norm_scale": 5.0}, "operators_used": ["logsigmoid", "softplus", "sigmoid", "exp", "rank_gap"], "implementation_hint": {"expects": ["cost_a", "cost_b", "logp_a", "logp_b"], "returns": "scalar"}, "code": "import torch\nimport torch.nn.functional as F\n\ndef _rank_gap(cost_a, cost_b):\n    \"\"\"Helper for rank_gap normalization.\"\"\"\n    costs = torch.cat([cost_a, cost_b])\n    # stable_rank sorts and assigns ranks, avg for ties\n    ranks = costs.to(torch.float32).argsort().argsort().float()\n    rank_a, rank_b = ranks.chunk(2)\n    # Normalize by batch size (N) which is len(cost_a)\n    return (rank_b - rank_a) / float(cost_a.size(0))\n\ndef generated_loss(batch, model_output, extra):\n    \"\"\"\n    Rank-Aware Adaptive Margin Loss.\n\n    Args:\n        batch (dict): A dictionary containing tensors from the data loader.\n                      Expected keys: 'cost_a', 'cost_b', 'log_prob_w', 'log_prob_l'.\n                      'cost_a'/'log_prob_w' are preferred (winner), 'cost_b'/'log_prob_l' are not (loser).\n                      An optional 'weight' tensor can be used for sample weighting.\n        model_output (dict): Output from the model (not used).\n        extra (dict): A dictionary for hyperparameters, e.g., {'base_beta': 1.0, 'max_margin': 2.0, 'norm_scale': 5.0}.\n\n    Returns:\n        torch.Tensor: A scalar tensor representing the mean loss.\n    \"\"\"\n    # Read hyperparameters\n    base_beta = extra.get('base_beta', 1.0)\n    max_margin = extra.get('max_margin', 2.0)\n    norm_scale = extra.get('norm_scale', 5.0)\n\n    # In our setting, 'a' is the winner (w) and 'b' is the loser (l).\n    # cost_a < cost_b\n    cost_a = batch['cost_a']\n    cost_b = batch['cost_b']\n    logp_a = batch['log_prob_w']\n    logp_b = batch['log_prob_l']\n\n    # 1. Calculate cost and log-probability differences\n    delta_cost = cost_b - cost_a\n    delta_logp = logp_a - logp_b\n\n    # 2. (New Coupling 1) Normalize cost difference using rank_gap\n    with torch.no_grad(): # Ranks are non-differentiable piecewise constants\n        norm_delta_cost = _rank_gap(cost_a, cost_b)\n\n    # 3. (Inherited) Compute the adaptive sigmoid margin\n    margin = max_margin * torch.sigmoid(norm_scale * norm_delta_cost)\n\n    # 4. Compute the core preference loss\n    core_loss = -F.logsigmoid(delta_logp - margin)\n\n    # 5. (New Coupling 2) Compute a dynamic beta based on model uncertainty\n    # When delta_logp is near zero (uncertain), exp() is near 1. When large, exp() is near 0.\n    dynamic_beta = base_beta * torch.exp(-torch.abs(delta_logp.detach()))\n\n    # 6. (Inherited) Compute a cost-based scaling factor using softplus\n    weight_scale = F.softplus(dynamic_beta * delta_cost)\n\n    # 7. Combine core loss and scaling factor\n    loss_per_sample = weight_scale * core_loss\n\n    # 8. Apply optional sample weights if provided\n    sample_weights = batch.get('weight', None)\n    if sample_weights is not None:\n        loss_per_sample = loss_per_sample * sample_weights\n\n    # 9. Return the mean loss over the batch\n    return loss_per_sample.mean()"}, "static_ok": true, "static_reason": "", "dynamic_ok": false, "dynamic_reason": "forward_error: name '_rank_gap' is not defined", "loss_value": null, "grad_norm": null}
{"generation": 7, "index": 2, "ir": {"name": "Soft-Clipped Exponential Margin Loss with Dynamic Beta", "intuition": "This loss function synthesizes ideas from its parents to create a robust, cost-sensitive preference objective, while introducing novel coupling mechanisms for improved stability and learning dynamics.\n\nInherited Ideas:\n- From Parent 1 ('Adaptive Sigmoid Margin Loss with Tanh Normalization'), it inherits the concept of an adaptive margin that grows with the cost difference. However, instead of a sigmoid, this child uses an exponential function (`exp`) to create the margin, which can provide a stronger separation signal for pairs with large cost differences.\n- From Parent 0 ('Clipped Tanh-Normalized Adaptive Margin Loss'), it inherits the use of a `softplus` function to weight the overall loss. This `softplus(beta * delta_cost)` term smoothly amplifies the loss for pairs with larger cost differences, focusing training on more significant preference signals.\n\nNew Coupling Ideas:\n1.  **Soft-Clipping with `tanh`:** The exponentially growing margin can become numerically unstable. To counteract this, the child introduces a 'soft-clipping' mechanism. The output of the `exp` function is passed through a `tanh` function. This allows the margin to grow exponentially for small-to-moderate cost differences but smoothly caps it at a maximum value (`max_margin`), preventing instability while retaining the steep initial growth.\n2.  **Dynamic Beta from Log-Probability Gap:** Instead of a fixed `beta` or one based on cost, this child introduces a `beta` that is dynamically adjusted based on the model's current log-probability gap (`delta_logp`). Specifically, `dynamic_beta = base_beta * exp(-delta_logp.detach())`. When the model is already confident (large positive `delta_logp`), `beta` is down-weighted, reducing the gradient and preventing over-confidence. When the model is wrong or uncertain (negative or small `delta_logp`), `beta` is increased, amplifying the learning signal. Using `.detach()` ensures this dynamic scaling doesn't introduce unwanted gradients through the `beta` term itself, focusing the gradient purely on the primary loss objective.", "pseudocode": "1. Calculate the cost difference: delta_cost = cost(b) - cost(a).\n2. Calculate the log-probability difference: delta_logp = logp(a) - logp(b).\n3. (Inherited from Parent 1, modified) Compute a base margin using an exponential function of the cost difference: base_margin = exp(delta_cost / norm_scale) - 1. This provides a rapidly increasing margin.\n4. (New Coupling 1) Apply 'soft-clipping' to the margin using tanh for stability: margin = max_margin * tanh(base_margin). This caps the margin smoothly.\n5. (New Coupling 2) Compute a dynamic beta based on the current log-probability gap to modulate loss weight: dynamic_beta = base_beta * exp(-delta_logp.detach()). This increases the weight for misclassified or uncertain pairs.\n6. (Inherited from Parent 0) Calculate a cost-based weight using the dynamic beta: weight_scale = softplus(dynamic_beta * delta_cost).\n7. Compute the core preference loss: core_loss = -logsigmoid(delta_logp - margin).\n8. Combine them: final_loss = weight_scale * core_loss.\n9. Return the mean of final_loss over the batch.", "hyperparams": {"base_beta": 0.5, "max_margin": 4.0, "norm_scale": 5.0}, "operators_used": ["logsigmoid", "softplus", "exp", "tanh"], "implementation_hint": {"expects": ["cost_a", "cost_b", "logp_a", "logp_b"], "returns": "scalar"}, "code": "import torch\nimport torch.nn.functional as F\n\ndef generated_loss(batch, model_output, extra):\n    \"\"\"\n    Soft-Clipped Exponential Margin Loss with Dynamic Beta.\n\n    Args:\n        batch (dict): A dictionary containing tensors from the data loader.\n                      Expected keys: 'cost_a', 'cost_b', 'log_prob_w', 'log_prob_l'.\n                      'cost_a' and 'log_prob_w' correspond to the preferred solution (winner).\n                      'cost_b' and 'log_prob_l' correspond to the non-preferred solution (loser).\n                      An optional 'weight' tensor can be used for sample weighting.\n        model_output (dict): Output from the model (not used in this loss structure).\n        extra (dict): A dictionary for hyperparameters, e.g., {'base_beta': 0.5, 'max_margin': 4.0, 'norm_scale': 5.0}.\n\n    Returns:\n        torch.Tensor: A scalar tensor representing the mean loss.\n    \"\"\"\n    # Read hyperparameters\n    base_beta = extra.get('base_beta', 0.5)\n    max_margin = extra.get('max_margin', 4.0)\n    norm_scale = extra.get('norm_scale', 5.0)\n\n    # In our setting, 'a' is the winner (w) and 'b' is the loser (l).\n    # cost_a < cost_b\n    cost_a = batch['cost_a']\n    cost_b = batch['cost_b']\n    logp_a = batch['log_prob_w']\n    logp_b = batch['log_prob_l']\n\n    # 1. Calculate cost and log-probability differences\n    delta_cost = cost_b - cost_a\n    delta_logp = logp_a - logp_b\n\n    # 2. (Inherited/Modified) Compute an exponential margin base\n    # The `-1` ensures the margin is zero when delta_cost is zero.\n    base_margin = torch.exp(delta_cost / norm_scale) - 1.0\n\n    # 3. (New Coupling 1) Soft-clip the margin using tanh for stability\n    # This allows exponential growth but smoothly caps it at max_margin.\n    margin = max_margin * torch.tanh(base_margin)\n\n    # 4. (New Coupling 2) Dynamic beta based on log-prob gap\n    # When model is wrong (delta_logp < 0), exp term is large, increasing beta.\n    # When model is correct (delta_logp > 0), exp term is small, decreasing beta.\n    # .detach() is used to prevent this from affecting the gradient direction of delta_logp.\n    with torch.no_grad():\n        dynamic_beta = base_beta * torch.exp(-delta_logp)\n\n    # 5. (Inherited) Compute cost-based weight using softplus and dynamic beta\n    weight_scale = F.softplus(dynamic_beta * delta_cost)\n\n    # 6. Compute the core preference loss\n    core_loss = -F.logsigmoid(delta_logp - margin)\n\n    # 7. Combine core loss and scaling factor\n    loss_per_sample = weight_scale * core_loss\n\n    # 8. Apply optional sample weights if provided\n    sample_weights = batch.get('weight', None)\n    if sample_weights is not None:\n        loss_per_sample = loss_per_sample * sample_weights\n\n    # 9. Return the mean loss over the batch\n    return loss_per_sample.mean()"}, "static_ok": true, "static_reason": "", "dynamic_ok": true, "dynamic_reason": "ok", "loss_value": 1.1892682313919067, "grad_norm": 0.0}
{"generation": 7, "index": 3, "ir": {"name": "Adaptive Margin Loss with Decaying Beta and Rank-Gap Normalization", "intuition": "This loss function creates a dynamic learning signal by combining an adaptive margin with a novel weighting scheme. It inherits the core concept of an adaptive margin, which scales with the cost difference, from both parents. Specifically, the margin is a `sigmoid` function of a normalized cost gap, ensuring that pairs with larger cost differences require a larger log-probability separation. It also inherits the `softplus` weighting of the core loss by the raw cost difference, which focuses training on more significant preferences.\n\nThis child introduces two key coupling ideas. First, instead of the `tanh` normalization used by the parents, it employs batch-wise `rank_gap` normalization. This normalizes the cost differences based on their relative ranks within the batch, making the margin less sensitive to the absolute scale of costs and more attuned to the relative importance of preferences in the current training step. Second, it introduces a new `beta` schedule. The `beta` parameter, which scales the loss, is now an exponentially decaying function of the model's own confidence, specifically `exp(-logp_diff)`. When the model is already confident (large `logp_diff`), `beta` is small, reducing the loss contribution and preventing overfitting on 'easy' pairs. When the model is unconfident or wrong (small or negative `logp_diff`), `beta` is large, amplifying the learning signal and focusing on 'hard' or misclassified pairs. This creates a self-regulating mechanism that adapts the loss weight based on the model's current performance.", "pseudocode": "1. Calculate the cost difference: delta_cost = cost(b) - cost(a).\n2. Calculate the log-probability difference: logp_diff = logp(a) - logp(b).\n3. (New Coupling) Normalize the cost difference using rank-gap normalization across the batch: rank_norm_cost = rank_gap(delta_cost). This is a new normalization scheme.\n4. (Inherited) Compute an adaptive margin based on the normalized cost: margin = max_margin * sigmoid(rank_norm_cost).\n5. Compute the core logistic loss: core_loss = -logsigmoid(logp_diff - margin).\n6. (New Coupling) Compute a dynamic beta that decays with model confidence: dynamic_beta = base_beta * exp(-logp_diff).\n7. (Inherited) Calculate a cost-based weight using the dynamic beta: weight_scale = softplus(dynamic_beta * delta_cost).\n8. Combine them: final_loss = weight_scale * core_loss.\n9. Return the mean of final_loss over the batch.", "hyperparams": {"base_beta": 1.0, "max_margin": 2.0}, "operators_used": ["logsigmoid", "softplus", "sigmoid", "exp", "rank_gap"], "implementation_hint": {"expects": ["cost_a", "cost_b", "logp_a", "logp_b"], "returns": "scalar"}, "code": "import torch\nimport torch.nn.functional as F\n\ndef rank_gap(x):\n    \"\"\"Normalizes a tensor by its rank-based gaps.\n    Sorts the tensor, computes differences between sorted values, and maps them back.\n    \"\"\"\n    if x.numel() <= 1:\n        return torch.zeros_like(x)\n    sorted_x, indices = torch.sort(x)\n    # Compute gaps between adjacent sorted values\n    gaps = torch.cat([sorted_x[0:1], sorted_x[1:] - sorted_x[:-1]])\n    # Normalize gaps to [0, 1] range for stability with sigmoid\n    max_gap = gaps.max()\n    if max_gap > 1e-6:\n        gaps = gaps / max_gap\n    # Unsort the gaps to match original tensor order\n    unsorted_gaps = torch.zeros_like(gaps)\n    unsorted_gaps.scatter_(0, indices, gaps)\n    return unsorted_gaps\n\ndef generated_loss(batch, model_output, extra):\n    \"\"\"\n    Adaptive Margin Loss with Decaying Beta and Rank-Gap Normalization.\n\n    Args:\n        batch (dict): A dictionary containing tensors from the data loader.\n                      Expected keys: 'cost_a', 'cost_b', 'log_prob_w', 'log_prob_l'.\n        model_output (dict): Output from the model (not used).\n        extra (dict): A dictionary for hyperparameters, e.g., {'base_beta': 1.0, 'max_margin': 2.0}.\n\n    Returns:\n        torch.Tensor: A scalar tensor representing the mean loss.\n    \"\"\"\n    # Read hyperparameters\n    base_beta = extra.get('base_beta', 1.0)\n    max_margin = extra.get('max_margin', 2.0)\n\n    # 'a' is the winner (w), 'b' is the loser (l). cost_a < cost_b\n    cost_a = batch['cost_a']\n    cost_b = batch['cost_b']\n    logp_a = batch['log_prob_w']\n    logp_b = batch['log_prob_l']\n\n    # 1. Calculate cost and log-probability differences\n    delta_cost = cost_b - cost_a\n    logp_diff = logp_a - logp_b\n\n    # 2. (New Coupling) Normalize cost difference using rank-gap normalization\n    with torch.no_grad(): # Prevent gradients from flowing through the ranking process\n        rank_norm_cost = rank_gap(delta_cost)\n\n    # 3. (Inherited) Compute an adaptive margin using a sigmoid function\n    margin = max_margin * torch.sigmoid(rank_norm_cost)\n\n    # 4. Compute the core preference loss\n    core_loss = -F.logsigmoid(logp_diff - margin)\n\n    # 5. (New Coupling) Compute a dynamic beta that decays with model confidence (logp_diff)\n    # When logp_diff is large (confident), beta is small. When logp_diff is small/negative, beta is large.\n    # Detach logp_diff to prevent it from directly influencing its own gradient via beta.\n    with torch.no_grad():\n        dynamic_beta = base_beta * torch.exp(-logp_diff)\n\n    # 6. (Inherited) Compute a cost-based scaling factor using softplus\n    weight_scale = F.softplus(dynamic_beta * delta_cost)\n\n    # 7. Combine core loss and scaling factor\n    loss_per_sample = weight_scale * core_loss\n\n    # 8. Apply optional sample weights if provided\n    sample_weights = batch.get('weight', None)\n    if sample_weights is not None:\n        loss_per_sample = loss_per_sample * sample_weights\n\n    # 9. Return the mean loss over the batch\n    return loss_per_sample.mean()"}, "static_ok": true, "static_reason": "", "dynamic_ok": false, "dynamic_reason": "forward_error: name 'rank_gap' is not defined", "loss_value": null, "grad_norm": null}
{"generation": 7, "index": 4, "ir": {"name": "Adaptive Log-Barrier Margin Loss with Rank-Gap Normalization", "intuition": "This loss function introduces a novel margin concept inspired by interior-point or barrier methods in optimization, coupled with a robust, rank-based normalization scheme.\n\nInherited Ideas:\n- From both parents, it inherits the core logistic loss structure `-logsigmoid(delta_logp - margin)`, which forms a stable and probabilistically sound basis for preference learning.\n- Also from both parents, it inherits the idea of scaling the overall loss by a `softplus` function of the cost difference (`softplus(beta * delta_cost)`). This prioritizes training on pairs with more significant cost gaps, focusing the model's capacity where it matters most.\n\nNew Couplings & Modifications:\n1.  **Log-Barrier Margin (New Coupling 1):** Instead of a sigmoid-based margin that saturates, this child loss computes the margin as `margin = -margin_strength * log(1 - rank_gap)`. This creates a 'barrier' effect. For pairs with a small rank-gap (i.e., `cost_b` is only slightly more expensive than `cost_a`), the margin is small. As the rank-gap approaches 1 (its maximum), the margin grows towards infinity. This aggressively pushes the model to strongly prefer `a` over `b` when the cost difference is verifiably large according to the rank-gap, preventing the model from being indifferent to clear preferences.\n2.  **Rank-Gap Normalization (New Coupling 2):** The cost difference is normalized using a new `rank_gap` operator. This operator computes `(rank(cost_b) - rank(cost_a)) / N`, where `N` is the batch size. This normalization is non-parametric, robust to outliers in cost values, and purely ordinal. It measures how many other samples in the batch fall between the costs of `a` and `b`. This provides a more stable and relative measure of preference strength compared to direct value-based normalization like `tanh`, which can be sensitive to the `norm_scale` hyperparameter.", "pseudocode": "1. For the entire batch, calculate the rank of each `cost_a` and `cost_b`.\n2. Calculate the cost difference: delta_cost = cost(b) - cost(a).\n3. (New Coupling) Normalize the preference strength using the rank-gap: rank_gap = (rank(cost_b) - rank(cost_a)) / batch_size. This value is naturally in the range [0, 1).\n4. Clamp the rank_gap to a small epsilon away from 1.0 to prevent `log(0)` in the next step.\n5. (New Coupling) Compute the log-barrier adaptive margin: margin = -margin_strength * log(1 - rank_gap). This margin grows towards infinity as the rank-gap approaches 1.\n6. Calculate the log-probability difference: delta_logp = logp(a) - logp(b).\n7. Compute the core preference loss using a logistic objective (inherited from both parents): core_loss = -logsigmoid(delta_logp - margin).\n8. Calculate a cost-based weight using a softplus function (inherited from both parents): weight_scale = softplus(beta * delta_cost).\n9. Combine them: final_loss = weight_scale * core_loss.\n10. Return the mean of final_loss over the batch.", "hyperparams": {"beta": 1.0, "margin_strength": 0.5, "log_eps": 1e-06}, "operators_used": ["logsigmoid", "softplus", "log", "clamp", "rank_gap"], "implementation_hint": {"expects": ["cost_a", "cost_b", "logp_a", "logp_b"], "returns": "scalar"}, "code": "import torch\nimport torch.nn.functional as F\n\ndef rank_gap(cost_a, cost_b):\n    \"\"\"Computes the normalized rank gap between cost_b and cost_a.\"\"\"\n    # Concatenate all costs to establish a global ranking within the batch\n    all_costs = torch.cat([cost_a, cost_b], dim=0)\n    sorted_costs, _ = torch.sort(all_costs)\n    \n    # Find the rank of each cost. Rank is its position in the sorted list.\n    # searchsorted returns the index where the element would be inserted to maintain order.\n    rank_a = torch.searchsorted(sorted_costs, cost_a).float()\n    rank_b = torch.searchsorted(sorted_costs, cost_b).float()\n    \n    # The maximum possible rank is N-1 where N is the number of elements.\n    # Normalization factor is the total number of items.\n    num_items = float(all_costs.shape[0])\n    \n    # Normalized difference in ranks. Since cost_a < cost_b, this is positive.\n    return (rank_b - rank_a) / num_items\n\ndef generated_loss(batch, model_output, extra):\n    \"\"\"\n    Adaptive Log-Barrier Margin Loss with Rank-Gap Normalization.\n\n    Args:\n        batch (dict): A dictionary containing tensors from the data loader.\n                      Expected keys: 'cost_a', 'cost_b', 'log_prob_w', 'log_prob_l'.\n                      'cost_a' and 'log_prob_w' correspond to the preferred solution (winner).\n                      'cost_b' and 'log_prob_l' correspond to the non-preferred solution (loser).\n                      An optional 'weight' tensor can be used for sample weighting.\n        model_output (dict): Output from the model (not used in this loss structure).\n        extra (dict): A dictionary for hyperparameters, e.g., {'beta': 1.0, 'margin_strength': 0.5, 'log_eps': 1e-6}.\n\n    Returns:\n        torch.Tensor: A scalar tensor representing the mean loss.\n    \"\"\"\n    # Read hyperparameters\n    beta = extra.get('beta', 1.0)\n    margin_strength = extra.get('margin_strength', 0.5)\n    log_eps = extra.get('log_eps', 1e-6)\n\n    # In our setting, 'a' is the winner (w) and 'b' is the loser (l).\n    # cost_a < cost_b\n    cost_a = batch['cost_a']\n    cost_b = batch['cost_b']\n    logp_a = batch['log_prob_w']\n    logp_b = batch['log_prob_l']\n\n    # 1. Calculate cost and log-probability differences\n    delta_cost = cost_b - cost_a\n    delta_logp = logp_a - logp_b\n\n    # 2. (New Coupling) Normalize using rank-gap\n    # This is a non-parametric, outlier-robust normalization based on relative standing in the batch.\n    norm_pref_strength = rank_gap(cost_a, cost_b)\n\n    # 3. Clamp to prevent log(0). rank_gap is <= (N-1)/N, so max value is < 1.\n    # Clamping at 1.0 - log_eps is a safeguard.\n    norm_pref_strength = torch.clamp(norm_pref_strength, min=0.0, max=1.0 - log_eps)\n\n    # 4. (New Coupling) Compute the log-barrier adaptive margin\n    # Margin approaches infinity as the normalized strength approaches 1.\n    margin = -margin_strength * torch.log(1.0 - norm_pref_strength)\n\n    # 5. Compute the core preference loss (Inherited from parents)\n    core_loss = -F.logsigmoid(delta_logp - margin)\n\n    # 6. Compute a cost-based scaling factor (Inherited from parents)\n    weight_scale = F.softplus(beta * delta_cost)\n\n    # 7. Combine core loss and scaling factor\n    loss_per_sample = weight_scale * core_loss\n\n    # 8. Apply optional sample weights if provided\n    sample_weights = batch.get('weight', None)\n    if sample_weights is not None:\n        loss_per_sample = loss_per_sample * sample_weights\n\n    # 9. Return the mean loss over the batch\n    return loss_per_sample.mean()"}, "static_ok": true, "static_reason": "", "dynamic_ok": false, "dynamic_reason": "forward_error: name 'rank_gap' is not defined", "loss_value": null, "grad_norm": null}
{"generation": 7, "index": 5, "ir": {"name": "Rank-Modulated Tanh-Normalized Margin Loss", "intuition": "This loss function synthesizes a stable, adaptive margin from its parents with a novel rank-based modulation scheme. It inherits the core idea of an adaptive margin based on a `tanh`-normalized cost difference from both parents ('Clipped Tanh-Normalized Adaptive Margin Loss' and 'Adaptive Sigmoid Margin Loss with Tanh Normalization'). This provides a bounded, batch-independent margin that grows with the cost difference. From the 'Clipped Tanh-Normalized' parent, it inherits the use of `clamp` on the normalized cost difference, which adds a layer of robustness by preventing extreme values from affecting the margin calculation.\n\nTwo new coupling ideas are introduced. First, a 'rank-gap' normalization is applied to the log-probability difference (`delta_logp`). This operator transforms `delta_logp` based on its rank within the batch, making the loss less sensitive to the absolute scale of log-probabilities and more focused on their relative ordering. This helps stabilize training when log-probabilities have high variance. Second, the loss introduces a dynamic `beta` schedule that is also modulated by this rank-gap. The `beta` parameter, which scales the loss importance via `softplus(beta * delta_cost)`, is now multiplied by the rank-gap of the cost difference. This means that preference pairs with a cost difference that is unusually large *relative to the rest of the batch* are given a higher weight, focusing the model's attention on the most salient examples in each batch.", "pseudocode": "1. Calculate the cost difference: delta_cost = cost(b) - cost(a).\n2. Calculate the log-probability difference: delta_logp = logp(a) - logp(b).\n3. (Inherited from Parent 0 & 1) Normalize the cost difference using a scaled tanh function: norm_delta_cost = tanh(delta_cost / norm_scale).\n4. (Inherited from Parent 0) Clip the normalized cost difference for stability: clipped_norm_delta_cost = clamp(norm_delta_cost, -clip_value, clip_value).\n5. (Inherited from Parent 0 & 1) Compute an adaptive margin based on the clipped, normalized value: margin = max_margin * sigmoid(clipped_norm_delta_cost).\n6. (New Coupling 1) Normalize the log-probability difference using a rank-gap transformation: ranked_delta_logp = rank_gap(delta_logp). This focuses on relative ordering within the batch.\n7. (New Coupling 2) Compute a dynamic beta, modulated by the rank-gap of the cost difference: dynamic_beta = base_beta * rank_gap(delta_cost). This up-weights pairs with a relatively large cost gap in the current batch.\n8. Compute a cost-based weight using the dynamic beta: weight_scale = softplus(dynamic_beta * delta_cost).\n9. Compute the core preference loss using the rank-normalized log-probabilities: core_loss = -logsigmoid(ranked_delta_logp - margin).\n10. Combine them: final_loss = weight_scale * core_loss.\n11. Return the mean of final_loss over the batch.", "hyperparams": {"base_beta": 1.0, "max_margin": 2.0, "norm_scale": 10.0, "clip_value": 0.99}, "operators_used": ["logsigmoid", "softplus", "sigmoid", "tanh", "clamp", "rank_gap"], "implementation_hint": {"expects": ["cost_a", "cost_b", "log_prob_w", "log_prob_l"], "returns": "scalar"}, "code": "import torch\nimport torch.nn.functional as F\n\ndef rank_gap(x):\n    \"\"\"Computes (x - x_median) / (x_p75 - x_p25 + epsilon).\"\"\"\n    if x.numel() < 2:\n        return torch.zeros_like(x)\n    q = torch.quantile(x, torch.tensor([0.25, 0.5, 0.75], device=x.device))\n    q25, q50, q75 = q[0], q[1], q[2]\n    iqr = q75 - q25\n    # Add epsilon for numerical stability if IQR is zero\n    return (x - q50) / (iqr + 1e-6)\n\ndef generated_loss(batch, model_output, extra):\n    \"\"\"\n    Rank-Modulated Tanh-Normalized Margin Loss.\n\n    Args:\n        batch (dict): A dictionary containing tensors from the data loader.\n                      Expected keys: 'cost_a', 'cost_b', 'log_prob_w', 'log_prob_l'.\n                      'cost_a' and 'log_prob_w' correspond to the preferred solution (winner).\n                      'cost_b' and 'log_prob_l' correspond to the non-preferred solution (loser).\n                      An optional 'weight' tensor can be used for sample weighting.\n        model_output (dict): Output from the model (not used in this loss structure).\n        extra (dict): A dictionary for hyperparameters, e.g., {'base_beta': 1.0, 'max_margin': 2.0, 'norm_scale': 10.0, 'clip_value': 0.99}.\n\n    Returns:\n        torch.Tensor: A scalar tensor representing the mean loss.\n    \"\"\"\n    # Read hyperparameters\n    base_beta = extra.get('base_beta', 1.0)\n    max_margin = extra.get('max_margin', 2.0)\n    norm_scale = extra.get('norm_scale', 10.0)\n    clip_value = extra.get('clip_value', 0.99)\n\n    # In our setting, 'a' is the winner (w) and 'b' is the loser (l).\n    cost_a = batch['cost_a']\n    cost_b = batch['cost_b']\n    logp_a = batch['log_prob_w']\n    logp_b = batch['log_prob_l']\n\n    # Calculate cost and log-probability differences\n    delta_cost = cost_b - cost_a\n    delta_logp = logp_a - logp_b\n\n    # Inherited: Normalize cost difference with tanh (from Parent 0 & 1)\n    norm_delta_cost = torch.tanh(delta_cost / norm_scale)\n\n    # Inherited: Clip the normalized cost for stability (from Parent 0)\n    clipped_norm_delta_cost = torch.clamp(norm_delta_cost, -clip_value, clip_value)\n\n    # Inherited: Compute the adaptive sigmoid margin (from Parent 0 & 1)\n    margin = max_margin * torch.sigmoid(clipped_norm_delta_cost)\n\n    # New Coupling 1: Normalize log-prob difference via rank-gap\n    ranked_delta_logp = rank_gap(delta_logp)\n\n    # New Coupling 2: Modulate beta by the rank-gap of the cost difference\n    # This gives higher weight to pairs with a relatively large cost gap in the batch.\n    # Using relu to ensure the modulator is non-negative.\n    cost_rank_modulator = F.relu(rank_gap(delta_cost))\n    dynamic_beta = base_beta * cost_rank_modulator\n\n    # Compute cost-based scaling factor using the dynamic beta\n    weight_scale = F.softplus(dynamic_beta * delta_cost)\n\n    # Compute the core preference loss with rank-normalized log-probs\n    core_loss = -F.logsigmoid(ranked_delta_logp - margin)\n\n    # Combine core loss and scaling factor\n    loss_per_sample = weight_scale * core_loss\n\n    # Apply optional sample weights if provided\n    sample_weights = batch.get('weight', None)\n    if sample_weights is not None:\n        loss_per_sample = loss_per_sample * sample_weights\n\n    # Return the mean loss over the batch\n    return loss_per_sample.mean()"}, "static_ok": true, "static_reason": "", "dynamic_ok": false, "dynamic_reason": "forward_error: name 'rank_gap' is not defined", "loss_value": null, "grad_norm": null}
{"generation": 7, "index": 6, "ir": {"name": "Adaptive Margin Loss with Decoupled Log-Probability Normalization", "intuition": "This loss function refines the adaptive margin concept present in both parents. It inherits the core idea of using a dynamic, cost-sensitive margin from Parent 1 (`Adaptive Sigmoid Margin Loss with Tanh Normalization`). This margin, derived from a `tanh`-normalized cost difference, allows the loss to demand a stronger preference signal for pairs with larger cost gaps. It also inherits the `softplus` weighting of the final loss by the raw cost difference, a common feature in both parents that focuses training on more significant preferences.\n\nAs a new coupling idea, this child loss introduces a decoupled normalization of the model's log-probability difference (`delta_logp`). Instead of directly using `delta_logp` in the `logsigmoid` term, it first normalizes it using the `zscore` operator across the batch. This has two benefits: 1) It stabilizes training by preventing extremely large or small `delta_logp` values from causing gradient explosion or vanishing, especially early in training. 2) It makes the loss invariant to the absolute scale of the model's log-probabilities, focusing instead on the relative ranking of log-probabilities within the batch. To preserve the original scale information, which is important, the normalized `delta_logp` is then re-scaled by a `softplus` of the original `delta_logp`'s standard deviation. This re-scaling is a second new coupling, ensuring that as the model becomes more confident (higher variance in `delta_logp`), the effective learning signal is amplified, while still benefiting from the stability of the initial z-score normalization.", "pseudocode": "1. Calculate the cost difference: delta_cost = cost(b) - cost(a).\n2. Calculate the log-probability difference: delta_logp = logp(a) - logp(b).\n3. (Inherited from Parent 1) Normalize the cost difference using a scaled tanh function: norm_delta_cost = tanh(delta_cost / norm_scale).\n4. (Inherited from Parent 1) Compute an adaptive margin based on the normalized cost: margin = max_margin * sigmoid(norm_delta_cost).\n5. (New Coupling 1) Normalize the log-probability difference across the batch using z-score: z_delta_logp = zscore(delta_logp). This stabilizes the input to the logsigmoid.\n6. (New Coupling 2) Compute a dynamic re-scaling factor for the normalized log-probabilities based on the batch's log-probability variance: logp_scale = softplus(std(delta_logp)). This re-introduces confidence information.\n7. Compute the normalized and re-scaled log-probability term: scaled_delta_logp = z_delta_logp * logp_scale.\n8. Compute the core preference loss using the scaled term: core_loss = -logsigmoid(scaled_delta_logp - margin).\n9. (Inherited from both parents) Calculate a cost-based weight: weight_scale = softplus(beta * delta_cost).\n10. Combine them: final_loss = weight_scale * core_loss.\n11. Return the mean of final_loss over the batch.", "hyperparams": {"beta": 1.0, "max_margin": 2.0, "norm_scale": 10.0}, "operators_used": ["logsigmoid", "softplus", "sigmoid", "tanh", "zscore"], "implementation_hint": {"expects": ["cost_a", "cost_b", "log_prob_w", "log_prob_l", "weight"], "returns": "scalar"}, "code": "import torch\nimport torch.nn.functional as F\n\ndef zscore(x, eps=1e-8):\n    \"\"\"Helper to compute z-score normalization.\"\"\"\n    if x.numel() <= 1:\n        return x - x.mean()\n    mean = x.mean()\n    std = x.std()\n    return (x - mean) / (std + eps)\n\ndef generated_loss(batch, model_output, extra):\n    \"\"\"\n    Adaptive Margin Loss with Decoupled Log-Probability Normalization.\n\n    Args:\n        batch (dict): A dictionary containing tensors from the data loader.\n                      Expected keys: 'cost_a', 'cost_b', 'log_prob_w', 'log_prob_l'.\n                      'cost_a' and 'log_prob_w' correspond to the preferred solution (winner).\n                      'cost_b' and 'log_prob_l' correspond to the non-preferred solution (loser).\n                      An optional 'weight' tensor can be used for sample weighting.\n        model_output (dict): Output from the model (not used in this loss structure).\n        extra (dict): A dictionary for hyperparameters, e.g., {'beta': 1.0, 'max_margin': 2.0, 'norm_scale': 10.0}.\n\n    Returns:\n        torch.Tensor: A scalar tensor representing the mean loss.\n    \"\"\"\n    # Read hyperparameters\n    beta = extra.get('beta', 1.0)\n    max_margin = extra.get('max_margin', 2.0)\n    norm_scale = extra.get('norm_scale', 10.0)\n\n    # In our setting, 'a' is the winner (w) and 'b' is the loser (l).\n    cost_a = batch['cost_a']\n    cost_b = batch['cost_b']\n    logp_a = batch['log_prob_w']\n    logp_b = batch['log_prob_l']\n\n    # 1. Calculate cost and log-probability differences\n    delta_cost = cost_b - cost_a\n    delta_logp = logp_a - logp_b\n\n    # 2. (Inherited) Normalize cost difference with tanh for the margin calculation\n    norm_delta_cost = torch.tanh(delta_cost / norm_scale)\n\n    # 3. (Inherited) Compute the adaptive sigmoid margin\n    margin = max_margin * torch.sigmoid(norm_delta_cost)\n\n    # 4. (New Coupling 1) Normalize delta_logp using z-score for stability\n    z_delta_logp = zscore(delta_logp)\n\n    # 5. (New Coupling 2) Create a scale factor from the standard deviation of the original delta_logp.\n    # This re-introduces the model's confidence (variance) into the loss signal.\n    # .detach() is used to prevent gradients from flowing through the stddev calculation, \n    # as we only want to use it as a scaling factor, not optimize for higher variance directly.\n    logp_scale = F.softplus(delta_logp.std().detach())\n\n    # 6. Apply the scaling to the z-scored logp difference\n    scaled_delta_logp = z_delta_logp * logp_scale\n    \n    # 7. Compute the core preference loss with the scaled logp difference\n    core_loss = -F.logsigmoid(scaled_delta_logp - margin)\n\n    # 8. (Inherited) Compute a cost-based scaling factor using softplus\n    weight_scale = F.softplus(beta * delta_cost)\n\n    # 9. Combine core loss and scaling factor\n    loss_per_sample = weight_scale * core_loss\n\n    # 10. Apply optional sample weights if provided\n    sample_weights = batch.get('weight', None)\n    if sample_weights is not None:\n        loss_per_sample = loss_per_sample * sample_weights\n\n    # 11. Return the mean loss over the batch\n    return loss_per_sample.mean()"}, "static_ok": true, "static_reason": "", "dynamic_ok": false, "dynamic_reason": "forward_error: name 'zscore' is not defined", "loss_value": null, "grad_norm": null}
{"generation": 7, "index": 7, "ir": {"name": "Rank-Gapped Exponential Margin Loss", "intuition": "This loss function creates a hybrid approach by combining the core logistic loss structure from both parents with a novel margin and weighting scheme. \n\nInherited Ideas:\n1. From both parents ('Clipped Tanh-Normalized Adaptive Margin Loss' and 'Adaptive Sigmoid Margin Loss with Tanh Normalization'), it inherits the fundamental Bradley-Terry style objective of using `-logsigmoid(delta_logp - margin)` to encourage the log-probability difference to exceed a certain margin.\n2. Also from both parents, it inherits the idea of scaling the overall loss by a function of the cost difference to emphasize more significant preference pairs. However, it adapts this idea significantly.\n\nNew Coupling Ideas:\n1. **Rank-Gapped Normalization for Margin**: Instead of using `tanh` or `zscore` normalization on the raw cost values, this loss introduces `rank_gap` normalization. This operator computes the difference in the fractional ranks of `cost_a` and `cost_b` within the batch. This makes the margin robust to outliers in cost values and focuses on the relative ordering of costs within the batch, rather than their absolute magnitudes. The margin is then computed as an exponential function of this rank gap (`margin = max_margin * exp(rank_gap - 1)`), which creates a small margin for adjacent-rank pairs and a rapidly increasing margin for pairs that are far apart in the batch's cost ordering.\n2. **Log-Ratio Cost Weighting**: The loss introduces a new weighting mechanism. Instead of `softplus(beta * delta_cost)`, it uses `log(1 + beta * (cost_b / cost_a - 1))`, assuming costs are positive. This `log` of the scaled relative cost difference provides a scale-invariant weight. For example, the pair (1, 2) gets the same weight as (10, 20). This prevents pairs with very large absolute costs from dominating the loss gradient, focusing instead on the proportional improvement.", "pseudocode": "1. For each pair (cost_a, cost_b) in the batch, calculate the rank gap: rg = rank_gap(cost_a, cost_b). This is a batch-wise operation where rg is the difference in fractional ranks. rg is expected to be in [0, 1].\n2. (New Coupling) Compute an adaptive margin using an exponential function of the rank gap: margin = max_margin * exp(beta_margin * (rg - 1)). This ensures the margin is 0 for rg=1 (large gap) and grows as rg approaches 0 (small gap), but is scaled by max_margin. Let's refine this: margin = max_margin * (1 - rg). This is simpler and more stable. The margin is 0 for the largest possible rank gap (1) and max_margin for the smallest possible gap (0).\n3. Calculate the log-probability difference: delta_logp = logp(a) - logp(b).\n4. (Inherited) Compute the core preference loss using a logistic objective: core_loss = -logsigmoid(delta_logp - margin).\n5. (New Coupling) Compute a scale-invariant weight based on the log of the relative cost difference: weight_scale = log(1 + beta_weight * (cost_b / cost_a - 1)). A small epsilon is added to cost_a for stability.\n6. (Inherited) Combine the core loss and the new weight: final_loss = weight_scale * core_loss.\n7. Return the mean of final_loss over the batch.", "hyperparams": {"beta_weight": 1.0, "max_margin": 1.0, "epsilon": 1e-08}, "operators_used": ["logsigmoid", "log", "rank_gap"], "implementation_hint": {"expects": ["cost_a", "cost_b", "logp_a", "logp_b"], "returns": "scalar"}, "code": "import torch\nimport torch.nn.functional as F\n\ndef rank_gap(cost_a, cost_b):\n    \"\"\"Computes the fractional rank difference between cost_a and cost_b.\"\"\"\n    costs = torch.cat([cost_a, cost_b])\n    # Stable ranking using sorted indices\n    sorted_indices = torch.argsort(costs)\n    ranks = torch.empty_like(sorted_indices, dtype=torch.float)\n    ranks[sorted_indices] = torch.arange(len(costs), device=costs.device, dtype=torch.float)\n    # Normalize ranks to [0, 1]\n    normalized_ranks = ranks / (len(costs) - 1) if len(costs) > 1 else torch.zeros_like(ranks)\n    \n    rank_a, rank_b = torch.chunk(normalized_ranks, 2)\n    # Since cost_a < cost_b, we expect rank_b > rank_a. The gap is rank_b - rank_a.\n    return rank_b - rank_a\n\ndef generated_loss(batch, model_output, extra):\n    \"\"\"\n    Rank-Gapped Exponential Margin Loss.\n\n    Args:\n        batch (dict): A dictionary containing tensors from the data loader.\n                      Expected keys: 'cost_a', 'cost_b', 'log_prob_w', 'log_prob_l'.\n        model_output (dict): Output from the model (not used).\n        extra (dict): A dictionary for hyperparameters.\n\n    Returns:\n        torch.Tensor: A scalar tensor representing the mean loss.\n    \"\"\"\n    # Read hyperparameters\n    beta_weight = extra.get('beta_weight', 1.0)\n    max_margin = extra.get('max_margin', 1.0)\n    epsilon = extra.get('epsilon', 1e-8)\n\n    # cost_a is for the winner (w), cost_b is for the loser (l).\n    # cost_a < cost_b\n    cost_a = batch['cost_a']\n    cost_b = batch['cost_b']\n    logp_a = batch['log_prob_w']\n    logp_b = batch['log_prob_l']\n\n    # 1. (New Coupling) Compute rank gap for margin calculation\n    # This is a batch-dependent normalization robust to cost outliers.\n    # rank_gap_val is in [0, 1] where 1 is the largest possible gap.\n    rank_gap_val = rank_gap(cost_a, cost_b)\n    \n    # 2. Compute margin based on rank gap. A larger gap (closer to 1) means an easier pair, so smaller margin.\n    # margin = max_margin * (1 - rank_gap_val) ensures margin is in [0, max_margin].\n    margin = max_margin * (1.0 - rank_gap_val)\n\n    # 3. Calculate log-probability difference\n    delta_logp = logp_a - logp_b\n\n    # 4. (Inherited) Compute the core preference loss\n    core_loss = -F.logsigmoid(delta_logp - margin)\n\n    # 5. (New Coupling) Compute a scale-invariant weight using log of relative cost difference.\n    # Add epsilon to cost_a to prevent division by zero.\n    # The term (cost_b / cost_a - 1) is the relative improvement.\n    relative_cost_diff = (cost_b / (cost_a + epsilon)) - 1.0\n    # Clamp to prevent log(negative) if cost_b < cost_a due to noise.\n    weight_scale = torch.log(1.0 + beta_weight * F.relu(relative_cost_diff))\n\n    # 6. (Inherited) Combine core loss and new weighting scheme\n    loss_per_sample = weight_scale * core_loss\n\n    # 7. Apply optional sample weights if provided\n    sample_weights = batch.get('weight', None)\n    if sample_weights is not None:\n        loss_per_sample = loss_per_sample * sample_weights\n\n    # 8. Return the mean loss\n    return loss_per_sample.mean()"}, "static_ok": true, "static_reason": "", "dynamic_ok": false, "dynamic_reason": "forward_error: name 'rank_gap' is not defined", "loss_value": null, "grad_norm": null}
{"generation": 8, "index": 0, "ir": {"name": "Rank-Modulated Tanh-Margin Loss", "intuition": "This loss function creates a novel synthesis of its parents' core ideas while introducing a rank-based modulation for improved batch-wise adaptation. \n\nIt inherits the following key concepts:\n- From Parent 1 (`Adaptive Sigmoid Margin Loss with Tanh Normalization`): It adopts the use of a scaled `tanh` function to normalize the cost difference (`delta_cost`). This provides a stable, bounded, and batch-independent normalization that maps the cost gap into a predictable [-1, 1] range.\n- From both parents: It uses a `softplus` function of the raw cost difference (`beta * delta_cost`) to scale the final loss. This is a robust mechanism to amplify the learning signal for pairs with a larger, more significant cost gap, focusing the model's attention on clear-cut preferences.\n\nAs a new coupling idea, this child loss introduces two significant modifications:\n1. **Rank-Modulated Margin:** Instead of using a `sigmoid` function to create the margin, it directly uses the `tanh`-normalized cost difference. This creates a simpler, linear relationship between the normalized cost gap and the margin. More importantly, this margin is then modulated by the `rank_gap` of the costs within the batch. The `rank_gap` measures the percentile difference between `cost_a` and `cost_b` in the batch. This coupling means that for a given cost difference, the margin will be larger if the two costs are far apart in the batch's rank distribution, and smaller if they are close. This helps the model prioritize learning from pairs that are not only different in absolute cost but also significantly separated in the context of the current batch.\n2. **Log-Probability Clipping:** To prevent extreme gradients from unstable log-probability differences (`delta_logp`), the loss applies a `clamp` to `delta_logp` itself before it's used in the core `logsigmoid` calculation. This acts as a stability trick, ensuring that outlier predictions do not dominate the gradient updates, making training more robust.", "pseudocode": "1. Calculate the cost difference: delta_cost = cost(b) - cost(a).\n2. (Inherited from Parent 1) Normalize the cost difference using a scaled tanh function: norm_delta_cost = tanh(delta_cost / norm_scale).\n3. (New Coupling 1) Calculate the rank gap for the costs within the batch: cost_rank_gap = rank_gap(cost_a, cost_b).\n4. (New Coupling 1) Compute a rank-modulated adaptive margin: margin = max_margin * norm_delta_cost * cost_rank_gap. The margin now depends on both the magnitude of the cost difference and its relative ranking in the batch.\n5. Calculate the log-probability difference: delta_logp = logp(a) - logp(b).\n6. (New Coupling 2) Clip the log-probability difference for stability: clipped_delta_logp = clamp(delta_logp, min=-logp_clip, max=logp_clip).\n7. Compute the core preference loss using the clipped log-probability difference: core_loss = -logsigmoid(clipped_delta_logp - margin).\n8. (Inherited from both parents) Calculate a cost-based weight: weight_scale = softplus(beta * delta_cost).\n9. Combine them: final_loss = weight_scale * core_loss.\n10. Return the mean of final_loss over the batch.", "hyperparams": {"beta": 0.5, "max_margin": 4.0, "norm_scale": 20.0, "logp_clip": 10.0}, "operators_used": ["logsigmoid", "softplus", "tanh", "clamp", "rank_gap"], "implementation_hint": {"expects": ["cost_a", "cost_b", "logp_a", "logp_b"], "returns": "scalar"}, "code": "import torch\nimport torch.nn.functional as F\n\ndef rank_gap(a, b):\n    \"\"\"Helper to compute the rank gap between two tensors within the batch.\"\"\"\n    combined = torch.cat([a, b])\n    ranks = torch.empty_like(combined)\n    ranks[combined.argsort()] = torch.arange(len(combined), device=a.device)\n    ranks_a = ranks[:len(a)]\n    ranks_b = ranks[len(a):]\n    # Normalize ranks to [0, 1]\n    normalized_ranks_a = ranks_a / (len(combined) - 1)\n    normalized_ranks_b = ranks_b / (len(combined) - 1)\n    # Since cost_a < cost_b, we expect ranks_b > ranks_a. Gap is in [0, 1].\n    return normalized_ranks_b - normalized_ranks_a\n\ndef generated_loss(batch, model_output, extra):\n    \"\"\"\n    Rank-Modulated Tanh-Margin Loss.\n\n    Args:\n        batch (dict): A dictionary containing tensors from the data loader.\n                      Expected keys: 'cost_a', 'cost_b', 'log_prob_w', 'log_prob_l'.\n                      'cost_a' and 'log_prob_w' correspond to the preferred solution (winner).\n                      'cost_b' and 'log_prob_l' correspond to the non-preferred solution (loser).\n                      An optional 'weight' tensor can be used for sample weighting.\n        model_output (dict): Output from the model (not used in this loss structure).\n        extra (dict): A dictionary for hyperparameters, e.g., {'beta': 0.5, 'max_margin': 4.0, 'norm_scale': 20.0, 'logp_clip': 10.0}.\n\n    Returns:\n        torch.Tensor: A scalar tensor representing the mean loss.\n    \"\"\"\n    # Read hyperparameters\n    beta = extra.get('beta', 0.5)\n    max_margin = extra.get('max_margin', 4.0)\n    norm_scale = extra.get('norm_scale', 20.0)\n    logp_clip = extra.get('logp_clip', 10.0)\n\n    # In our setting, 'a' is the winner (w) and 'b' is the loser (l).\n    # cost_a < cost_b\n    cost_a = batch['cost_a']\n    cost_b = batch['cost_b']\n    logp_a = batch['log_prob_w']\n    logp_b = batch['log_prob_l']\n\n    # 1. Calculate cost and log-probability differences\n    delta_cost = cost_b - cost_a\n    delta_logp = logp_a - logp_b\n\n    # 2. Normalize cost difference with tanh (Inherited from Parent 1)\n    norm_delta_cost = torch.tanh(delta_cost / norm_scale)\n\n    # 3. (New Coupling 1) Calculate rank gap of costs in the batch\n    with torch.no_grad():\n        cost_rank_gap = rank_gap(cost_a, cost_b)\n\n    # 4. (New Coupling 1) Compute rank-modulated margin\n    margin = max_margin * norm_delta_cost * cost_rank_gap\n\n    # 5. (New Coupling 2) Clip logp difference for stability\n    clipped_delta_logp = torch.clamp(delta_logp, -logp_clip, logp_clip)\n\n    # 6. Compute the core preference loss\n    core_loss = -F.logsigmoid(clipped_delta_logp - margin)\n\n    # 7. Scale loss by softplus of cost difference (Inherited from both parents)\n    weight_scale = F.softplus(beta * delta_cost)\n\n    # 8. Combine core loss and scaling factor\n    loss_per_sample = weight_scale * core_loss\n\n    # 9. Apply optional sample weights if provided\n    sample_weights = batch.get('weight', None)\n    if sample_weights is not None:\n        loss_per_sample = loss_per_sample * sample_weights\n\n    # 10. Return the mean loss over the batch\n    return loss_per_sample.mean()"}, "static_ok": true, "static_reason": "", "dynamic_ok": false, "dynamic_reason": "forward_error: name 'rank_gap' is not defined", "loss_value": null, "grad_norm": null}
{"generation": 8, "index": 1, "ir": {"name": "Rank-Aware Log-Ratio Margin Loss", "intuition": "This loss function creates a dynamic, cost-sensitive margin that adapts not only to the magnitude of the cost difference between a pair but also to its relative rank within the batch. \n\nIt inherits two key ideas from its parents:\n1.  **Adaptive Margin (from both parents):** The core concept of a margin that grows with the cost difference (`delta_cost`) is inherited. A larger cost gap should demand a stronger preference signal from the model.\n2.  **Tanh Normalization (from Parent 1):** The use of `tanh(delta_cost / norm_scale)` is inherited to create a smooth, bounded, and batch-independent normalization of the cost difference. This ensures the margin calculation is stable and predictable.\n\nIt introduces two new coupling ideas:\n1.  **Rank-Aware Margin Scaling:** The primary innovation is to make the margin sensitive to the *rank* of the cost difference within the batch. We compute `rank_gap`, which is the rank of `delta_cost` divided by the batch size. This value (from 0 to 1) is then used to scale the `tanh`-normalized cost. The result is that pairs with a relatively small `delta_cost` *for that specific batch* will have a smaller margin, while pairs with a large `delta_cost` *for that batch* will have a larger one. This allows the loss to dynamically focus on the most significant preference pairs in each batch, rather than relying on a global, absolute scale.\n2.  **Log-Ratio Weighting:** Instead of using `softplus` to weight the loss, this child uses `log(1 + beta * delta_cost)`. This weighting scheme also up-weights pairs with larger cost differences but does so with a logarithmic curve. This provides a strong initial signal for small `delta_cost` values but grows more slowly for very large `delta_cost`, preventing extreme outliers from dominating the batch gradient.", "pseudocode": "1. Calculate the cost difference: delta_cost = cost(b) - cost(a).\n2. (New Coupling) Calculate the rank-gap of the cost difference within the batch: rank_gap = rank(delta_cost) / N, where N is the batch size. This provides a normalized rank from ~0 to 1.\n3. (Inherited from Parent 1) Normalize the cost difference using a scaled tanh function: norm_delta_cost = tanh(delta_cost / norm_scale).\n4. (New Coupling) Couple the rank-gap and normalized cost to create a rank-aware signal: rank_aware_signal = rank_gap * norm_delta_cost. This signal is large only when both the absolute cost gap and its relative rank in the batch are high.\n5. (Inherited from both parents) Compute an adaptive margin based on this rank-aware signal: margin = max_margin * rank_aware_signal.\n6. Calculate the log-probability difference: delta_logp = logp(a) - logp(b).\n7. Compute the core preference loss: core_loss = -logsigmoid(delta_logp - margin).\n8. (New Coupling) Calculate a log-ratio weight: weight_scale = log(1 + beta * delta_cost). This up-weights pairs with larger cost differences with a diminishing-returns effect.\n9. Combine them: final_loss = weight_scale * core_loss.\n10. Return the mean of final_loss over the batch.", "hyperparams": {"beta": 1.0, "max_margin": 3.0, "norm_scale": 10.0, "eps": 1e-08}, "operators_used": ["logsigmoid", "tanh", "log", "rank_gap"], "implementation_hint": {"expects": ["cost_a", "cost_b", "logp_a", "logp_b"], "returns": "scalar"}, "code": "import torch\nimport torch.nn.functional as F\n\n# Helper function for rank_gap operator\ndef rank_gap(x):\n    \"\"\"Computes the rank of each element in x and normalizes by the total count.\"\"\"\n    return x.argsort().argsort().float() / (len(x) - 1 + 1e-8)\n\ndef generated_loss(batch, model_output, extra):\n    \"\"\"\n    Rank-Aware Log-Ratio Margin Loss.\n\n    Args:\n        batch (dict): A dictionary containing tensors from the data loader.\n                      Expected keys: 'cost_a', 'cost_b', 'log_prob_w', 'log_prob_l'.\n                      'cost_a' and 'log_prob_w' correspond to the preferred solution (winner).\n                      'cost_b' and 'log_prob_l' correspond to the non-preferred solution (loser).\n                      An optional 'weight' tensor can be used for sample weighting.\n        model_output (dict): Output from the model (not used in this loss structure).\n        extra (dict): A dictionary for hyperparameters, e.g., {'beta': 1.0, 'max_margin': 3.0, 'norm_scale': 10.0, 'eps': 1e-8}.\n\n    Returns:\n        torch.Tensor: A scalar tensor representing the mean loss.\n    \"\"\"\n    # Read hyperparameters\n    beta = extra.get('beta', 1.0)\n    max_margin = extra.get('max_margin', 3.0)\n    norm_scale = extra.get('norm_scale', 10.0)\n    eps = extra.get('eps', 1e-8)\n\n    # In our setting, 'a' is the winner (w) and 'b' is the loser (l).\n    # cost_a < cost_b\n    cost_a = batch['cost_a']\n    cost_b = batch['cost_b']\n    logp_a = batch['log_prob_w']\n    logp_b = batch['log_prob_l']\n\n    # 1. Calculate cost and log-probability differences\n    delta_cost = cost_b - cost_a\n    delta_logp = logp_a - logp_b\n\n    # 2. (New Coupling) Compute the rank-gap of delta_cost within the batch.\n    # This gives a batch-relative measure of the cost difference's importance.\n    # Detach to treat ranks as fixed constants for the gradient calculation of this step.\n    r_gap = rank_gap(delta_cost.detach())\n\n    # 3. (Inherited) Normalize cost difference with tanh for a bounded signal.\n    norm_delta_cost = torch.tanh(delta_cost / norm_scale)\n\n    # 4. (New Coupling) Couple rank-gap with the normalized cost to form the margin signal.\n    # The margin is now sensitive to both the absolute cost gap and its relative rank.\n    rank_aware_signal = r_gap * norm_delta_cost\n    \n    # 5. (Inherited) Compute the adaptive margin.\n    margin = max_margin * rank_aware_signal\n\n    # 6. Compute the core preference loss.\n    core_loss = -F.logsigmoid(delta_logp - margin)\n\n    # 7. (New Coupling) Compute a log-ratio weight.\n    # This up-weights larger cost differences with a sub-linear (log) scaling.\n    # Add eps for numerical stability if delta_cost is zero.\n    weight_scale = torch.log(1 + beta * delta_cost + eps)\n\n    # 8. Combine core loss and scaling factor.\n    loss_per_sample = weight_scale * core_loss\n\n    # 9. Apply optional sample weights if provided\n    sample_weights = batch.get('weight', None)\n    if sample_weights is not None:\n        loss_per_sample = loss_per_sample * sample_weights\n\n    # 10. Return the mean loss over the batch\n    return loss_per_sample.mean()"}, "static_ok": true, "static_reason": "", "dynamic_ok": false, "dynamic_reason": "forward_error: name 'rank_gap' is not defined", "loss_value": null, "grad_norm": null}
{"generation": 8, "index": 2, "ir": {"name": "Soft-Clipped Exponential Margin Loss", "intuition": "This loss function synthesizes the adaptive margin concept from both parents into a new exponential form, while introducing a novel soft-clipping mechanism for stability and a dynamic beta schedule.\n\nInherited Ideas:\n- From both Parent 0 and Parent 1, it inherits the core idea of an adaptive margin that grows with the cost difference (`delta_cost`). The margin's purpose is to demand a larger `delta_logp` for pairs with more significant cost gaps.\n- From both parents, it also inherits the use of `softplus(beta * delta_cost)` to scale the final loss, which emphasizes training on pairs with larger cost differences in a smooth, non-negative way.\n\nNew Couplings & Modifications:\n1. **Exponential Margin:** Instead of a `sigmoid` or `tanh`-based margin, this child uses an `exp`-based margin: `margin = margin_scale * (exp(norm_delta_cost) - 1)`. This creates a margin that grows exponentially with the normalized cost difference, strongly penalizing the model for failing to distinguish pairs with large cost gaps, while being very lenient on pairs with small gaps. This is a departure from the bounded sigmoid margin of the parents.\n2. **Soft-Clipping with `tanh`:** To control the potentially explosive exponential margin, a new coupling is introduced: the margin is passed through a `tanh` function, `final_margin = max_margin * tanh(margin)`. This acts as a 'soft-clip', smoothly bounding the margin at `max_margin` while preserving gradients. It provides stability without the hard cutoff of a `clamp` operator.\n3. **Dynamic `beta` based on `rank_gap`:** A second new idea is to make the `beta` parameter dynamic. It is modulated by the `rank_gap` of the cost differences within the batch. `dynamic_beta = base_beta * rank_gap(delta_cost)`. This means that pairs with a higher relative cost difference *within the current batch* are given a higher `beta`, focusing the model's attention on the most discriminative examples in each training step. This is more adaptive than the fixed `beta` in the parents.", "pseudocode": "1. Calculate the cost difference: delta_cost = cost(b) - cost(a).\n2. Calculate the log-probability difference: delta_logp = logp(a) - logp(b).\n3. (Inherited) Normalize the cost difference to a [0, 1] range using batch-rank normalization: norm_delta_cost = rank_gap(delta_cost).\n4. (New Idea 1: Exponential Margin) Compute a preliminary margin that grows exponentially with the normalized cost gap: margin = margin_scale * (exp(norm_delta_cost) - 1).\n5. (New Idea 2: Soft-Clipping) Pass the margin through a scaled tanh function to smoothly bound it for stability: final_margin = max_margin * tanh(margin).\n6. Compute the core logistic loss using this soft-clipped exponential margin: core_loss = -logsigmoid(delta_logp - final_margin).\n7. (New Idea 3: Dynamic Beta) Modulate beta based on the rank-normalized cost difference: dynamic_beta = base_beta * norm_delta_cost.\n8. (Inherited) Calculate a cost-based weight using the dynamic beta and softplus: weight_scale = softplus(dynamic_beta * delta_cost).\n9. Combine them: final_loss = weight_scale * core_loss.\n10. Return the mean of final_loss over the batch.", "hyperparams": {"base_beta": 0.5, "max_margin": 5.0, "margin_scale": 1.0}, "operators_used": ["logsigmoid", "softplus", "exp", "tanh", "rank_gap"], "implementation_hint": {"expects": ["cost_a", "cost_b", "logp_a", "logp_b"], "returns": "scalar"}, "code": "import torch\nimport torch.nn.functional as F\n\ndef rank_gap(x):\n    \"\"\"Computes the rank of each element in x and normalizes it to [0, 1].\"\"\"\n    if x.numel() < 2:\n        return torch.ones_like(x) * 0.5\n    # stable ranking\n    sorter = torch.argsort(x)\n    inv_sorter = torch.empty_like(sorter)\n    inv_sorter[sorter] = torch.arange(len(x), device=x.device, dtype=x.dtype)\n    # normalize to [0, 1]\n    return inv_sorter / (len(x) - 1)\n\ndef generated_loss(batch, model_output, extra):\n    \"\"\"\n    Soft-Clipped Exponential Margin Loss with dynamic beta.\n\n    Args:\n        batch (dict): A dictionary containing tensors from the data loader.\n                      Expected keys: 'cost_a', 'cost_b', 'log_prob_w', 'log_prob_l'.\n                      'cost_a' and 'log_prob_w' correspond to the preferred solution (winner).\n                      'cost_b' and 'log_prob_l' correspond to the non-preferred solution (loser).\n                      An optional 'weight' tensor can be used for sample weighting.\n        model_output (dict): Output from the model (not used in this loss structure).\n        extra (dict): A dictionary for hyperparameters, e.g., {'base_beta': 0.5, 'max_margin': 5.0, 'margin_scale': 1.0}.\n\n    Returns:\n        torch.Tensor: A scalar tensor representing the mean loss.\n    \"\"\"\n    # Read hyperparameters\n    base_beta = extra.get('base_beta', 0.5)\n    max_margin = extra.get('max_margin', 5.0)\n    margin_scale = extra.get('margin_scale', 1.0)\n\n    # In our setting, 'a' is the winner (w) and 'b' is the loser (l).\n    cost_a = batch['cost_a']\n    cost_b = batch['cost_b']\n    logp_a = batch['log_prob_w']\n    logp_b = batch['log_prob_l']\n\n    # Ensure costs are non-negative for stability with rank_gap\n    delta_cost = (cost_b - cost_a).clamp(min=1e-6)\n    delta_logp = logp_a - logp_b\n\n    # 1. Normalize cost difference using rank_gap (idea inherited from parents' normalization concept)\n    # This gives a [0, 1] normalized value based on batch-relative cost differences.\n    norm_delta_cost = rank_gap(delta_cost)\n\n    # 2. (New Coupling 1) Compute an exponential margin.\n    # margin_scale controls how quickly the margin grows. exp(0)-1=0, exp(1)-1=1.718\n    # This creates a margin that grows faster than linearly with the normalized cost.\n    exp_margin = margin_scale * (torch.exp(norm_delta_cost) - 1.0)\n\n    # 3. (New Coupling 2) Apply a 'soft-clip' using tanh for stability.\n    # This smoothly bounds the potentially large exponential margin at `max_margin`.\n    final_margin = max_margin * torch.tanh(exp_margin)\n\n    # 4. Compute the core preference loss (Bradley-Terry style with adaptive margin)\n    core_loss = -F.logsigmoid(delta_logp - final_margin)\n\n    # 5. (New Coupling 3) Compute a dynamic beta based on the rank-normalized cost difference.\n    # This focuses weighting on pairs with higher relative cost gaps within the batch.\n    dynamic_beta = base_beta * norm_delta_cost\n\n    # 6. Compute a cost-based scaling factor using softplus (idea inherited from parents)\n    # This amplifies the loss for pairs with larger raw cost differences.\n    weight_scale = F.softplus(dynamic_beta * delta_cost)\n\n    # 7. Combine core loss and scaling factor\n    loss_per_sample = weight_scale * core_loss\n\n    # 8. Apply optional sample weights if provided\n    sample_weights = batch.get('weight', None)\n    if sample_weights is not None:\n        loss_per_sample = loss_per_sample * sample_weights\n\n    # 9. Return the mean loss over the batch\n    return loss_per_sample.mean()"}, "static_ok": true, "static_reason": "", "dynamic_ok": false, "dynamic_reason": "forward_error: name 'rank_gap' is not defined", "loss_value": null, "grad_norm": null}
{"generation": 8, "index": 3, "ir": {"name": "Rank-Gap Weighted Adaptive Tanh Margin Loss", "intuition": "This loss function creates a hybrid objective by inheriting key ideas from both parents while introducing a novel weighting scheme. \n\nInherited Ideas:\n- From Parent 1 (Adaptive Sigmoid Margin Loss with Tanh Normalization), it inherits the core concept of an adaptive margin that is a function of a normalized cost difference. Specifically, it uses `tanh` to create a bounded, batch-independent normalization of the cost gap, `tanh((cost_b - cost_a) / norm_scale)`. This ensures the margin smoothly increases with the cost difference.\n- From Parent 0 (Clipped Tanh-Normalized Adaptive Margin Loss), it inherits the use of `softplus` to scale the entire loss term based on the raw cost difference, `softplus(beta * delta_cost)`. This focuses training on pairs with more significant cost gaps.\n\nNew Coupling Ideas:\n1.  **Rank-Gap Based Margin Scaling**: Instead of using `sigmoid` to shape the margin, this child loss uses the `rank_gap` function, which is defined as `x / (1 + |x|)`. Applying this to the tanh-normalized cost difference (`norm_delta_cost`) creates a margin that is less sensitive to the `max_margin` hyperparameter for small cost differences and grows linearly at first before saturating. This provides a different, potentially more stable, margin profile compared to the exponential growth of `sigmoid`.\n2.  **Dynamic Beta based on Log-Probability Difference**: The `beta` parameter, which controls the strength of the `softplus` weighting, is made dynamic. It is modulated by `exp(-abs(delta_logp))`. This new coupling idea reduces the weight of the loss for pairs where the model is already very confident (large `abs(delta_logp)`) or very uncertain (large negative `delta_logp`). This helps prevent overfitting on easy examples and reduces the influence of pairs where the model's current predictions are extremely far from the desired outcome, which could cause instability. It encourages the model to focus its learning capacity on moderately difficult examples.", "pseudocode": "1. Calculate the cost difference: delta_cost = cost_b - cost_a.\n2. Calculate the log-probability difference: delta_logp = logp_a - logp_b.\n3. (Inherited from Parent 1) Normalize the cost difference using a scaled tanh function: norm_delta_cost = tanh(delta_cost / norm_scale).\n4. (New Coupling 1) Compute an adaptive margin using the rank_gap function on the normalized cost: margin = max_margin * rank_gap(norm_delta_cost).\n5. Compute the core preference loss: core_loss = -logsigmoid(delta_logp - margin).\n6. (New Coupling 2) Compute a dynamic beta that is down-weighted by the model's confidence: dynamic_beta = base_beta * exp(-abs(delta_logp)).\n7. (Inherited from Parent 0) Calculate a cost-based weight using the dynamic beta: weight_scale = softplus(dynamic_beta * delta_cost).\n8. Combine them: final_loss = weight_scale * core_loss.\n9. Return the mean of final_loss over the batch.", "hyperparams": {"base_beta": 1.0, "max_margin": 1.5, "norm_scale": 10.0}, "operators_used": ["logsigmoid", "softplus", "tanh", "exp", "rank_gap"], "implementation_hint": {"expects": ["cost_a", "cost_b", "logp_a", "logp_b"], "returns": "scalar"}, "code": "import torch\nimport torch.nn.functional as F\n\n# Define the rank_gap operator as a helper function\ndef rank_gap(x):\n    \"\"\"Implements the rank_gap function: x / (1 + |x|).\"\"\"\n    return x / (1.0 + torch.abs(x))\n\ndef generated_loss(batch, model_output, extra):\n    \"\"\"\n    Rank-Gap Weighted Adaptive Tanh Margin Loss.\n\n    Args:\n        batch (dict): A dictionary containing tensors from the data loader.\n                      Expected keys: 'cost_a', 'cost_b', 'log_prob_w', 'log_prob_l'.\n                      'cost_a' and 'log_prob_w' correspond to the preferred solution (winner).\n                      'cost_b' and 'log_prob_l' correspond to the non-preferred solution (loser).\n                      An optional 'weight' tensor can be used for sample weighting.\n        model_output (dict): Output from the model (not used in this loss structure).\n        extra (dict): A dictionary for hyperparameters, e.g., {'base_beta': 1.0, 'max_margin': 1.5, 'norm_scale': 10.0}.\n\n    Returns:\n        torch.Tensor: A scalar tensor representing the mean loss.\n    \"\"\"\n    # Read hyperparameters\n    base_beta = extra.get('base_beta', 1.0)\n    max_margin = extra.get('max_margin', 1.5)\n    norm_scale = extra.get('norm_scale', 10.0)\n\n    # In our setting, 'a' is the winner (w) and 'b' is the loser (l).\n    # cost_a < cost_b\n    cost_a = batch['cost_a']\n    cost_b = batch['cost_b']\n    logp_a = batch['log_prob_w']\n    logp_b = batch['log_prob_l']\n\n    # 1. Calculate cost and log-probability differences\n    delta_cost = cost_b - cost_a\n    delta_logp = logp_a - logp_b\n\n    # 2. Normalize cost difference with tanh (Inherited from Parent 1)\n    norm_delta_cost = torch.tanh(delta_cost / norm_scale)\n\n    # 3. (New Coupling 1) Compute adaptive margin using rank_gap function\n    margin = max_margin * rank_gap(norm_delta_cost)\n\n    # 4. Compute the core preference loss (Bradley-Terry style with margin)\n    core_loss = -F.logsigmoid(delta_logp - margin)\n\n    # 5. (New Coupling 2) Compute a dynamic beta based on model confidence (delta_logp)\n    # This reduces weight for pairs where the model is already very confident or very wrong.\n    dynamic_beta = base_beta * torch.exp(-torch.abs(delta_logp.detach())) # detach to avoid influencing gradients through beta\n\n    # 6. Compute cost-based scaling factor using softplus (Inherited from Parent 0)\n    weight_scale = F.softplus(dynamic_beta * delta_cost)\n\n    # 7. Combine core loss and scaling factor\n    loss_per_sample = weight_scale * core_loss\n\n    # 8. Apply optional sample weights if provided\n    sample_weights = batch.get('weight', None)\n    if sample_weights is not None:\n        loss_per_sample = loss_per_sample * sample_weights\n\n    # 9. Return the mean loss over the batch\n    return loss_per_sample.mean()"}, "static_ok": true, "static_reason": "", "dynamic_ok": false, "dynamic_reason": "forward_error: name 'rank_gap' is not defined", "loss_value": null, "grad_norm": null}
{"generation": 8, "index": 4, "ir": {"name": "Rank-Modulated Tanh-Normalized Margin Loss", "intuition": "This loss function creates a robust preference learning objective by integrating ideas from its parents and introducing novel coupling mechanisms. \n\nIt inherits the core concept of an adaptive margin from both parents ('Clipped Tanh-Normalized Adaptive Margin Loss' and 'Adaptive Sigmoid Margin Loss with Tanh Normalization'). This margin is computed as a function of the cost difference between the preferred and non-preferred solutions. Specifically, it inherits the use of a scaled `tanh` function to normalize the cost difference, which creates a bounded, batch-independent signal that feeds into the margin calculation. The fundamental loss structure is a logistic loss (`-logsigmoid`) that pushes the model's log-probability difference to exceed this adaptive margin.\n\nAs a new coupling idea, this child loss introduces two modifications:\n1. **Rank-Gap Modulation**: Instead of scaling the margin with a simple `sigmoid` of the normalized cost difference, it uses a rank-based function, `rank_gap(norm_delta_cost)`. This function is designed to be flat (zero) for very small cost differences, preventing the model from being forced to learn a margin for noisy or insignificant preferences. It then ramps up linearly before saturating. This creates a 'dead zone' for tiny cost gaps, enhancing stability and focusing training on more meaningful pairs. The `rank_gap` is inspired by ranking losses but applied here as a margin modulator.\n2. **Dynamic Beta with Softplus-Exp Coupling**: The loss for each sample is weighted by `softplus(beta * delta_cost)`, a concept present in both parents. However, this child introduces a dynamic `beta` that is itself a function of the normalized cost difference: `beta = base_beta * exp(norm_delta_cost - 1)`. This `exp` coupling ensures that `beta` is small for small cost gaps (approaching `base_beta / e`) and grows exponentially towards `base_beta` as the normalized cost gap approaches its maximum of 1. This creates a smoother and more aggressive focus on high-confidence preference pairs compared to the linear or absolute value-based dynamic betas of the parents.", "pseudocode": "1. Calculate the cost difference: delta_cost = cost_b - cost_a.\n2. Calculate the log-probability difference: delta_logp = logp_a - logp_b.\n3. (Inherited) Normalize the cost difference using a scaled tanh function: norm_delta_cost = tanh(delta_cost / norm_scale).\n4. (New Coupling 1) Compute an adaptive margin using a rank-gap function on the normalized cost. The margin is zero for small norm_delta_cost, then grows linearly: margin = max_margin * rank_gap(norm_delta_cost, threshold=margin_deadzone).\n5. (New Coupling 2) Compute a dynamic beta that grows exponentially with the normalized cost difference: dynamic_beta = base_beta * exp(norm_delta_cost - 1).\n6. (Inherited) Calculate a cost-based weight using the dynamic beta: weight_scale = softplus(dynamic_beta * delta_cost).\n7. Compute the core preference loss with the adaptive margin: core_loss = -logsigmoid(delta_logp - margin).\n8. Combine the core loss and the weight scale: final_loss = weight_scale * core_loss.\n9. Return the mean of final_loss over the batch.", "hyperparams": {"base_beta": 1.0, "max_margin": 1.5, "norm_scale": 20.0, "margin_deadzone": 0.1}, "operators_used": ["logsigmoid", "softplus", "tanh", "exp", "rank_gap"], "implementation_hint": {"expects": ["cost_a", "cost_b", "logp_a", "logp_b"], "returns": "scalar"}, "code": "import torch\nimport torch.nn.functional as F\n\n# Define the custom operator within the scope accessible to the loss function\ndef rank_gap(x, threshold=0.1, scale=1.0):\n    \"\"\"A function that is 0 for x < threshold, then ramps up linearly.\n       It's a simplified, one-sided version of a hinge or ranking function.\n       Ensures the output is clamped between 0 and 1.\n    \"\"\"\n    # Ensure the denominator is not zero\n    denominator = 1.0 - threshold\n    if denominator <= 1e-6:\n        denominator = 1e-6\n    # Linear ramp from threshold to 1\n    ramped_val = (x - threshold) / denominator\n    # Clamp ensures it's 0 below threshold and doesn't exceed 1\n    return torch.clamp(ramped_val, 0.0, 1.0) * scale\n\ndef generated_loss(batch, model_output, extra):\n    \"\"\"\n    Rank-Modulated Tanh-Normalized Margin Loss.\n\n    Args:\n        batch (dict): A dictionary containing tensors from the data loader.\n                      Expected keys: 'cost_a', 'cost_b', 'log_prob_w', 'log_prob_l'.\n                      'cost_a' and 'log_prob_w' correspond to the preferred solution (winner).\n                      'cost_b' and 'log_prob_l' correspond to the non-preferred solution (loser).\n                      An optional 'weight' tensor can be used for sample weighting.\n        model_output (dict): Output from the model (not used in this loss structure).\n        extra (dict): A dictionary for hyperparameters.\n\n    Returns:\n        torch.Tensor: A scalar tensor representing the mean loss.\n    \"\"\"\n    # Read hyperparameters\n    base_beta = extra.get('base_beta', 1.0)\n    max_margin = extra.get('max_margin', 1.5)\n    norm_scale = extra.get('norm_scale', 20.0)\n    margin_deadzone = extra.get('margin_deadzone', 0.1)\n\n    # 'a' is the winner (w), 'b' is the loser (l). cost_a < cost_b.\n    cost_a = batch['cost_a']\n    cost_b = batch['cost_b']\n    logp_a = batch['log_prob_w']\n    logp_b = batch['log_prob_l']\n\n    # Ensure costs are positive for stability, though delta_cost is the key driver\n    delta_cost = cost_b - cost_a\n    delta_logp = logp_a - logp_b\n\n    # 1. (Inherited) Normalize cost difference with tanh for a bounded signal in [0, 1]\n    # Since cost_b > cost_a, delta_cost > 0, so tanh will be in [0, 1)\n    norm_delta_cost = torch.tanh(delta_cost / norm_scale)\n\n    # 2. (New Coupling 1) Compute an adaptive margin using the rank_gap operator.\n    # This creates a dead zone for small cost differences, improving robustness to noise.\n    margin = max_margin * rank_gap(norm_delta_cost, threshold=margin_deadzone)\n\n    # 3. (New Coupling 2) Compute a dynamic beta that grows exponentially with the normalized cost.\n    # The `exp(x-1)` form ensures beta is `base_beta/e` at x=0 and `base_beta` at x=1.\n    dynamic_beta = base_beta * torch.exp(norm_delta_cost - 1.0)\n\n    # 4. (Inherited) Compute a cost-based scaling factor using softplus for stability.\n    weight_scale = F.softplus(dynamic_beta * delta_cost)\n\n    # 5. Compute the core preference loss (Bradley-Terry style with adaptive margin)\n    core_loss = -F.logsigmoid(delta_logp - margin)\n\n    # 6. Combine core loss and scaling factor\n    loss_per_sample = weight_scale * core_loss\n\n    # 7. Apply optional sample weights if provided\n    sample_weights = batch.get('weight', None)\n    if sample_weights is not None:\n        loss_per_sample = loss_per_sample * sample_weights\n\n    # 8. Return the mean loss over the batch\n    return loss_per_sample.mean()"}, "static_ok": true, "static_reason": "", "dynamic_ok": false, "dynamic_reason": "forward_error: name 'rank_gap' is not defined", "loss_value": null, "grad_norm": null}
{"generation": 8, "index": 5, "ir": {"name": "Rank-Gated Adaptive Margin Loss", "intuition": "This loss function creates a hybrid objective that combines a cost-sensitive adaptive margin with a rank-based stability mechanism. It inherits the core idea of an adaptive margin from both parents, where the required log-probability separation between a preferred and non-preferred solution is a function of their cost difference. Specifically, it uses a scaled `tanh` function to normalize the cost gap, which then feeds into a `sigmoid` to produce a margin that smoothly increases with the cost difference (inherited from Parent 1). The entire loss is also scaled by a `softplus` of the cost difference, amplifying the learning signal for more significant preference pairs, another idea common to both parents.\n\nThis child loss introduces two new coupling ideas. First, it incorporates a 'rank-gating' mechanism. The core adaptive margin loss is only applied to a subset of the batchspecifically, the pairs with the largest `k` cost differences. For the rest of the pairs (those with smaller cost gaps), a simpler, non-margin logistic loss (`-logsigmoid(delta_logp)`) is applied. This is achieved using a `rank_gap` operator to identify the top-k pairs. This gating focuses the more complex margin-based learning on the most informative examples, while still providing a basic preference signal for less certain pairs, improving training stability and efficiency. Second, the `beta` parameter, which scales the loss, is dynamically scheduled based on the rank-gated mask. For the top-k pairs, a higher `beta_high` is used, while a lower `beta_low` is used for the others. This further emphasizes the importance of the high-cost-gap pairs.", "pseudocode": "1. Calculate the cost difference: delta_cost = cost(b) - cost(a).\n2. Calculate the log-probability difference: delta_logp = logp(a) - logp(b).\n3. (Inherited from Parent 1) Normalize the cost difference using a scaled tanh function: norm_delta_cost = tanh(delta_cost / norm_scale).\n4. (Inherited from Parent 1) Compute an adaptive margin based on the normalized cost: margin = max_margin * sigmoid(norm_delta_cost).\n5. (New Coupling 1: Rank-Gating) Identify the top `k` percent of pairs with the largest `delta_cost` using a `rank_gap` operator. This creates a binary mask, `is_top_k`.\n6. Compute the adaptive margin loss for all pairs: margin_loss = -logsigmoid(delta_logp - margin).\n7. Compute a simpler, non-margin logistic loss for all pairs: base_loss = -logsigmoid(delta_logp).\n8. Combine the losses using the rank-gating mask: core_loss = where(is_top_k, margin_loss, base_loss).\n9. (New Coupling 2: Rank-based Beta) Define a dynamic beta based on the same mask: dynamic_beta = where(is_top_k, beta_high, beta_low).\n10. (Inherited from both) Calculate a cost-based weight using softplus: weight_scale = softplus(dynamic_beta * delta_cost).\n11. Combine the weight and the core loss: final_loss = weight_scale * core_loss.\n12. Return the mean of final_loss over the batch.", "hyperparams": {"beta_high": 1.5, "beta_low": 0.5, "max_margin": 2.0, "norm_scale": 10.0, "top_k_frac": 0.5}, "operators_used": ["logsigmoid", "softplus", "sigmoid", "tanh", "rank_gap"], "implementation_hint": {"expects": ["cost_a", "cost_b", "logp_a", "logp_b"], "returns": "scalar"}, "code": "import torch\nimport torch.nn.functional as F\n\n# Helper for rank_gap operator\ndef _rank_gap(x, k_frac):\n    \"\"\"Returns a binary mask where 1 indicates elements in the top k_frac of x.\"\"\"\n    if k_frac >= 1.0:\n        return torch.ones_like(x, dtype=torch.bool)\n    if k_frac <= 0.0:\n        return torch.zeros_like(x, dtype=torch.bool)\n    \n    num_elements = x.numel()\n    k = int(num_elements * k_frac)\n    if k == 0:\n        return torch.zeros_like(x, dtype=torch.bool)\n\n    # Find the threshold value for the top-k elements\n    # kthvalue is 1-indexed for k, but we need 0-indexed for topk\n    k_val = torch.kthvalue(x, num_elements - k + 1).values\n\n    # Create a mask for elements >= k_val\n    # This handles ties by including all tied values\n    return x >= k_val\n\ndef generated_loss(batch, model_output, extra):\n    \"\"\"\n    Rank-Gated Adaptive Margin Loss.\n\n    Args:\n        batch (dict): A dictionary containing tensors from the data loader.\n                      Expected keys: 'cost_a', 'cost_b', 'log_prob_w', 'log_prob_l'.\n                      'cost_a'/'log_prob_w' are for the winner, 'cost_b'/'log_prob_l' for the loser.\n                      An optional 'weight' tensor can be used for sample weighting.\n        model_output (dict): Output from the model (not used).\n        extra (dict): A dictionary for hyperparameters, e.g., {'beta_high': 1.5, 'beta_low': 0.5, 'max_margin': 2.0, 'norm_scale': 10.0, 'top_k_frac': 0.5}.\n\n    Returns:\n        torch.Tensor: A scalar tensor representing the mean loss.\n    \"\"\"\n    # Read hyperparameters\n    beta_high = extra.get('beta_high', 1.5)\n    beta_low = extra.get('beta_low', 0.5)\n    max_margin = extra.get('max_margin', 2.0)\n    norm_scale = extra.get('norm_scale', 10.0)\n    top_k_frac = extra.get('top_k_frac', 0.5)\n\n    # In our setting, 'a' is the winner (w) and 'b' is the loser (l).\n    cost_a = batch['cost_a']\n    cost_b = batch['cost_b']\n    logp_a = batch['log_prob_w']\n    logp_b = batch['log_prob_l']\n\n    # 1. Calculate cost and log-probability differences (should be positive)\n    delta_cost = cost_b - cost_a\n    delta_logp = logp_a - logp_b\n\n    # 2. (Inherited) Compute adaptive margin components\n    norm_delta_cost = torch.tanh(delta_cost / norm_scale)\n    margin = max_margin * torch.sigmoid(norm_delta_cost)\n\n    # 3. (New Coupling 1: Rank-Gating) Identify top-k pairs by cost difference\n    # The 'rank_gap' operator is implemented here via a helper\n    is_top_k = _rank_gap(delta_cost, top_k_frac)\n\n    # 4. Compute two potential loss terms\n    margin_loss = -F.logsigmoid(delta_logp - margin)\n    base_loss = -F.logsigmoid(delta_logp) # Simple logistic loss without margin\n\n    # 5. Apply the rank-gating to select the loss term for each sample\n    core_loss = torch.where(is_top_k, margin_loss, base_loss)\n\n    # 6. (New Coupling 2: Rank-based Beta) Define a dynamic beta based on the rank\n    dynamic_beta = torch.where(is_top_k, beta_high, beta_low)\n    \n    # 7. (Inherited) Compute a cost-based scaling factor using softplus\n    weight_scale = F.softplus(dynamic_beta * delta_cost)\n\n    # 8. Combine core loss and scaling factor\n    loss_per_sample = weight_scale * core_loss\n\n    # 9. Apply optional sample weights if provided\n    sample_weights = batch.get('weight', None)\n    if sample_weights is not None:\n        loss_per_sample = loss_per_sample * sample_weights\n\n    # 10. Return the mean loss over the batch\n    return loss_per_sample.mean()"}, "static_ok": true, "static_reason": "", "dynamic_ok": false, "dynamic_reason": "forward_error: name '_rank_gap' is not defined", "loss_value": null, "grad_norm": null}
{"generation": 8, "index": 6, "ir": {"name": "Rank-Gap Scaled Adaptive Margin Loss", "intuition": "This loss function synthesizes ideas from its parents to create a robust, adaptive preference learning objective, while introducing a novel normalization scheme and a dynamic loss scaling mechanism.\n\nInherited Ideas:\n- From both parents, it inherits the core structure of a logistic preference loss (`-logsigmoid`) where the model's log-probability difference is encouraged to surpass a dynamic, cost-sensitive margin. This establishes a baseline probabilistic model for preferences.\n- From Parent 1 (Adaptive Sigmoid Margin Loss with Tanh Normalization), it inherits the idea of using a sigmoid function to transform a normalized cost difference into a margin. This ensures the margin is small for negligible cost differences and grows towards a maximum for significant ones.\n\nNew Couplings & Modifications:\n- **New Normalization (Coupling 1):** Instead of `tanh` or `zscore`, this child uses a new `rank_gap` normalization. The cost differences (`delta_cost`) are first converted to their ranks within the batch. The rank difference between each sample and the batch median rank is then scaled to a [-1, 1] range. This makes the margin robust to absolute cost scaling and outliers, focusing instead on the relative importance of a preference pair within the current batch. This is a theoretically sound way to normalize as it preserves the monotonic relationship between cost gap and the normalized value.\n- **Dynamic Loss Scaling (Coupling 2):** The loss for each sample is scaled by `exp(logp_l - logp_w)`, which is `p(l)/p(w)`. This is a stability trick and a form of self-paced learning. When the model is already correctly ranking a pair (i.e., `logp_w` is much larger than `logp_l`), the scaling factor is small, reducing the gradient and preventing the model from over-optimizing on 'easy' pairs. Conversely, when the model is wrong (`logp_l > logp_w`), the scaling factor is large, focusing the training effort on correcting these mistakes. This is a modification of the parents' `softplus(beta * delta_cost)` scaling, replacing a cost-based focus with a model-confidence-based focus.", "pseudocode": "1. For the entire batch, calculate the cost difference: delta_cost = cost(b) - cost(a).\n2. (New Coupling 1) Normalize the cost difference using rank-gap normalization. First, find the ranks of delta_cost within the batch. Then, compute the signed, scaled distance of each rank from the median rank, mapping it to a [-1, 1] range. Let's call this `rank_norm_delta_cost`.\n3. (Inherited) Compute an adaptive margin based on the rank-normalized cost: margin = max_margin * sigmoid(rank_norm_delta_cost). The margin is now sensitive to the relative importance of the cost gap within the batch.\n4. Calculate the log-probability difference: delta_logp = logp(a) - logp(b).\n5. (Inherited) Compute the core preference loss: core_loss = -logsigmoid(delta_logp - margin).\n6. (New Coupling 2) Compute a dynamic loss scale based on the model's current predictions: dynamic_scale = exp(logp(b) - logp(a)). This scale is large when the model is wrong and small when it is correct.\n7. Combine them: final_loss = dynamic_scale * core_loss.\n8. Return the mean of final_loss over the batch.", "hyperparams": {"max_margin": 1.5}, "operators_used": ["logsigmoid", "sigmoid", "exp", "rank_gap"], "implementation_hint": {"expects": ["cost_a", "cost_b", "logp_a", "logp_b"], "returns": "scalar"}, "code": "import torch\nimport torch.nn.functional as F\n\n# Helper function for rank_gap normalization, as it's a new operator\ndef rank_gap(x):\n    \"\"\"Scales the input tensor based on its rank within the batch to a [-1, 1] range.\"\"\"\n    if x.numel() <= 1:\n        return torch.zeros_like(x)\n    ranks = x.argsort().argsort().float()\n    # Scale ranks to [0, 1]\n    scaled_ranks = ranks / (x.numel() - 1)\n    # Shift and scale to [-1, 1]\n    return 2.0 * scaled_ranks - 1.0\n\ndef generated_loss(batch, model_output, extra):\n    \"\"\"\n    Rank-Gap Scaled Adaptive Margin Loss.\n\n    Args:\n        batch (dict): A dictionary containing tensors from the data loader.\n                      Expected keys: 'cost_a', 'cost_b', 'log_prob_w', 'log_prob_l'.\n                      'cost_a' and 'log_prob_w' correspond to the preferred solution (winner).\n                      'cost_b' and 'log_prob_l' correspond to the non-preferred solution (loser).\n                      An optional 'weight' tensor can be used for sample weighting.\n        model_output (dict): Output from the model (not used in this loss structure).\n        extra (dict): A dictionary for hyperparameters, e.g., {'max_margin': 1.5}.\n\n    Returns:\n        torch.Tensor: A scalar tensor representing the mean loss.\n    \"\"\"\n    # Read hyperparameters\n    max_margin = extra.get('max_margin', 1.5)\n\n    # In our setting, 'a' is the winner (w) and 'b' is the loser (l).\n    # cost_a < cost_b\n    cost_a = batch['cost_a']\n    cost_b = batch['cost_b']\n    logp_a = batch['log_prob_w']\n    logp_b = batch['log_prob_l']\n\n    # 1. Calculate cost and log-probability differences\n    delta_cost = cost_b - cost_a\n    delta_logp = logp_a - logp_b\n\n    # 2. (New Coupling 1) Normalize cost difference using rank_gap\n    with torch.no_grad(): # Ranks are non-differentiable and should not be part of the graph\n        rank_norm_delta_cost = rank_gap(delta_cost)\n\n    # 3. (Inherited) Compute adaptive margin using sigmoid on the rank-normalized cost\n    margin = max_margin * torch.sigmoid(rank_norm_delta_cost)\n\n    # 4. (Inherited) Compute the core preference loss\n    core_loss = -F.logsigmoid(delta_logp - margin)\n\n    # 5. (New Coupling 2) Compute dynamic loss scale based on model confidence\n    # This is exp(logp_b - logp_a) = p_b / p_a. It's > 1 when the model is wrong.\n    # Detach to use it as a weighting factor, not for gradient propagation through the scale itself.\n    dynamic_scale = torch.exp(logp_b - logp_a).detach()\n\n    # 6. Combine core loss and dynamic scale\n    loss_per_sample = dynamic_scale * core_loss\n\n    # 7. Apply optional sample weights if provided\n    sample_weights = batch.get('weight', None)\n    if sample_weights is not None:\n        loss_per_sample = loss_per_sample * sample_weights\n\n    # 8. Return the mean loss over the batch\n    return loss_per_sample.mean()"}, "static_ok": true, "static_reason": "", "dynamic_ok": false, "dynamic_reason": "forward_error: name 'rank_gap' is not defined", "loss_value": null, "grad_norm": null}
{"generation": 8, "index": 7, "ir": {"name": "Adaptive Margin Loss with Tanh-Normalized Log-Probabilities", "intuition": "This loss function creates a robust preference learning objective by combining an adaptive margin with a novel normalization scheme for the model's log-probabilities. \n\nIt inherits two key ideas from its parents:\n1.  From 'Adaptive Sigmoid Margin Loss with Tanh Normalization' (Parent 1), it inherits the concept of an **adaptive margin** that is a sigmoid function of the normalized cost difference (`margin = max_margin * sigmoid(...)`). This makes the required separation between winner and loser log-probabilities dependent on the magnitude of their cost difference.\n2.  From both parents, it inherits the use of a **cost-based weighting scheme** (`softplus(beta * delta_cost)`), which scales the loss to focus training on pairs with more significant cost gaps.\n\nThis child loss introduces two new coupling ideas:\n1.  **Log-Probability Normalization:** Instead of directly using the raw log-probability difference (`logp_a - logp_b`), it first normalizes this difference using a scaled `tanh` function: `norm_delta_logp = tanh((logp_a - logp_b) / logp_norm_scale)`. This bounds the influence of the model's output, preventing extremely confident (and potentially erroneous) predictions from generating huge gradients, thus improving training stability. The core loss then becomes a comparison between this normalized log-prob difference and the adaptive margin.\n2.  **Margin Clipping:** To further enhance stability, the adaptive margin is clipped to a maximum value using `clamp`. This prevents the `tanh`-normalized log-prob difference from being pushed towards an unnecessarily large target value, especially if the `max_margin` hyperparameter is set aggressively. This acts as a safeguard, ensuring the target for `norm_delta_logp` remains within a sensible range.", "pseudocode": "1. Calculate the cost difference: delta_cost = cost(b) - cost(a).\n2. Calculate the log-probability difference: delta_logp = logp(a) - logp(b).\n3. (Inherited from Parent 1) Normalize the cost difference using a scaled tanh function: norm_delta_cost = tanh(delta_cost / cost_norm_scale).\n4. (Inherited from Parent 1) Compute an adaptive margin based on the normalized cost difference: adaptive_margin = max_margin * sigmoid(norm_delta_cost).\n5. (New Coupling) Clip the adaptive margin to a maximum value for stability: margin = clamp(adaptive_margin, min=0, max=margin_clip).\n6. (New Coupling) Normalize the log-probability difference using a scaled tanh function: norm_delta_logp = tanh(delta_logp / logp_norm_scale). This bounds the model's output contribution to the loss.\n7. Compute the core preference loss by comparing the normalized log-prob difference to the clipped, adaptive margin: core_loss = -logsigmoid(norm_delta_logp - margin).\n8. (Inherited from both parents) Calculate a cost-based weight: weight_scale = softplus(beta * delta_cost).\n9. Combine them: final_loss = weight_scale * core_loss.\n10. Return the mean of final_loss over the batch.", "hyperparams": {"beta": 1.0, "max_margin": 0.8, "margin_clip": 0.95, "cost_norm_scale": 10.0, "logp_norm_scale": 5.0}, "operators_used": ["logsigmoid", "softplus", "sigmoid", "tanh", "clamp"], "implementation_hint": {"expects": ["cost_a", "cost_b", "logp_a", "logp_b"], "returns": "scalar"}, "code": "import torch\nimport torch.nn.functional as F\n\ndef generated_loss(batch, model_output, extra):\n    \"\"\"\n    Adaptive Margin Loss with Tanh-Normalized Log-Probabilities.\n\n    Args:\n        batch (dict): A dictionary containing tensors from the data loader.\n                      Expected keys: 'cost_a', 'cost_b', 'log_prob_w', 'log_prob_l'.\n                      'cost_a' and 'log_prob_w' correspond to the preferred solution (winner).\n                      'cost_b' and 'log_prob_l' correspond to the non-preferred solution (loser).\n                      An optional 'weight' tensor can be used for sample weighting.\n        model_output (dict): Output from the model (not used in this loss structure).\n        extra (dict): A dictionary for hyperparameters.\n\n    Returns:\n        torch.Tensor: A scalar tensor representing the mean loss.\n    \"\"\"\n    # Read hyperparameters\n    beta = extra.get('beta', 1.0)\n    max_margin = extra.get('max_margin', 0.8)\n    margin_clip = extra.get('margin_clip', 0.95)\n    cost_norm_scale = extra.get('cost_norm_scale', 10.0)\n    logp_norm_scale = extra.get('logp_norm_scale', 5.0)\n\n    # In our setting, 'a' is the winner (w) and 'b' is the loser (l).\n    cost_a = batch['cost_a']\n    cost_b = batch['cost_b']\n    logp_a = batch['log_prob_w']\n    logp_b = batch['log_prob_l']\n\n    # 1. Calculate cost and log-probability differences\n    delta_cost = cost_b - cost_a\n    delta_logp = logp_a - logp_b\n\n    # 2. (Inherited) Normalize cost difference with tanh for the margin calculation\n    norm_delta_cost = torch.tanh(delta_cost / cost_norm_scale)\n\n    # 3. (Inherited) Compute the adaptive sigmoid margin\n    adaptive_margin = max_margin * torch.sigmoid(norm_delta_cost)\n\n    # 4. (New Coupling) Clip the margin to a max value for stability. \n    # The target for tanh(delta_logp/scale) should not exceed 1.\n    margin = torch.clamp(adaptive_margin, min=0, max=margin_clip)\n    \n    # 5. (New Coupling) Normalize the log-probability difference with tanh\n    # This bounds the model's contribution to the loss, enhancing stability.\n    norm_delta_logp = torch.tanh(delta_logp / logp_norm_scale)\n\n    # 6. Compute the core preference loss using normalized values\n    # We push the normalized logp difference to exceed the margin.\n    core_loss = -F.logsigmoid(norm_delta_logp - margin)\n\n    # 7. (Inherited) Compute a cost-based scaling factor using softplus\n    weight_scale = F.softplus(beta * delta_cost)\n\n    # 8. Combine core loss and scaling factor\n    loss_per_sample = weight_scale * core_loss\n\n    # 9. Apply optional sample weights if provided\n    sample_weights = batch.get('weight', None)\n    if sample_weights is not None:\n        loss_per_sample = loss_per_sample * sample_weights\n\n    # 10. Return the mean loss over the batch\n    return loss_per_sample.mean()"}, "static_ok": true, "static_reason": "", "dynamic_ok": true, "dynamic_reason": "ok", "loss_value": 1.2147499322891235, "grad_norm": 0.0}
{"generation": 9, "index": 0, "ir": {"name": "Rank-Gap Modulated Tanh-Margin Loss", "intuition": "This loss function synthesizes a stable, adaptive margin from its parents and introduces two new coupling ideas for improved robustness and training dynamics.\n\nInherited Ideas:\n- From both parents, it inherits the core concept of a logistic loss (`-logsigmoid`) where the model's log-probability difference (`logp_a - logp_b`) must exceed an adaptive margin. This margin is a function of the cost difference, making the required separation between `logp_a` and `logp_b` dependent on the magnitude of the preference.\n- It also inherits the use of a scaled `tanh` function to normalize the cost difference (`delta_cost`). This provides a stable, bounded, and batch-independent normalization, as seen in both parents.\n\nNew Coupling and Modifications:\n1. **Rank-Gap Margin Modulation:** The first new coupling idea modifies how the margin is derived. Instead of using the raw `tanh` output, the loss first calculates the 'rank gap' of the `tanh`-normalized cost differences within the batch. This `rank_gap` operator transforms the values into a uniform distribution based on their ranks, making the margin less sensitive to outliers or a skewed distribution of cost differences. The margin is then a `sigmoid` function of this rank-gap-normalized value, ensuring a smooth and well-distributed set of targets for the model across the batch.\n2. **Log-Barrier Weighting:** The second new coupling idea replaces the `softplus(beta * delta_cost)` weighting scheme from the parents with a log-barrier function: `log(1 + beta * delta_cost)`. This weighting still amplifies the loss for pairs with larger cost differences, but it does so less aggressively than `softplus` (which behaves linearly for large inputs). The logarithmic scaling prevents extremely large cost differences from dominating the batch gradient, leading to more stable training while still emphasizing more significant preference pairs.", "pseudocode": "1. Calculate the cost difference: delta_cost = cost(b) - cost(a).\n2. Calculate the log-probability difference: delta_logp = logp(a) - logp(b).\n3. (Inherited) Normalize the cost difference using a scaled tanh function: norm_delta_cost = tanh(delta_cost / norm_scale).\n4. (New Coupling 1) Apply rank-gap normalization to the tanh-normalized costs within the batch: ranked_norm_cost = rank_gap(norm_delta_cost). This maps the values to a uniform distribution based on their rank, improving robustness to outliers.\n5. (Inherited) Compute an adaptive margin based on the rank-gap-normalized value: margin = max_margin * sigmoid(ranked_norm_cost).\n6. Compute the core preference loss: core_loss = -logsigmoid(delta_logp - margin).\n7. (New Coupling 2) Compute a log-barrier weight: weight_scale = log(1 + beta * delta_cost). This provides a less aggressive, more stable weighting for large cost differences compared to the parents' softplus weighting.\n8. Combine them: final_loss = weight_scale * core_loss.\n9. Return the mean of final_loss over the batch.", "hyperparams": {"beta": 1.0, "max_margin": 2.0, "norm_scale": 10.0}, "operators_used": ["logsigmoid", "sigmoid", "tanh", "log", "rank_gap"], "implementation_hint": {"expects": ["cost_a", "cost_b", "logp_a", "logp_b"], "returns": "scalar"}, "code": "import torch\nimport torch.nn.functional as F\n\n# Helper function for rank_gap as it's not a standard torch operator\ndef rank_gap(x):\n    \"\"\"Computes the rank-based gap normalization for a 1D tensor.\"\"\"\n    # Sort the tensor and get the indices\n    sorted_indices = torch.argsort(x)\n    # Create a tensor of ranks (0, 1, 2, ...)\n    ranks = torch.empty_like(x, dtype=x.dtype)\n    ranks[sorted_indices] = torch.arange(len(x), device=x.device, dtype=x.dtype)\n    # Normalize ranks to [-1, 1]\n    # Avoid division by zero if batch size is 1\n    if len(x) > 1:\n        return 2 * (ranks / (len(x) - 1)) - 1\n    else:\n        return torch.zeros_like(x)\n\ndef generated_loss(batch, model_output, extra):\n    \"\"\"\n    Rank-Gap Modulated Tanh-Margin Loss.\n\n    Args:\n        batch (dict): A dictionary containing tensors from the data loader.\n                      Expected keys: 'cost_a', 'cost_b', 'log_prob_w', 'log_prob_l'.\n                      'cost_a' and 'log_prob_w' correspond to the preferred solution (winner).\n                      'cost_b' and 'log_prob_l' correspond to the non-preferred solution (loser).\n                      An optional 'weight' tensor can be used for sample weighting.\n        model_output (dict): Output from the model (not used in this loss structure).\n        extra (dict): A dictionary for hyperparameters, e.g., {'beta': 1.0, 'max_margin': 2.0, 'norm_scale': 10.0}.\n\n    Returns:\n        torch.Tensor: A scalar tensor representing the mean loss.\n    \"\"\"\n    # Read hyperparameters\n    beta = extra.get('beta', 1.0)\n    max_margin = extra.get('max_margin', 2.0)\n    norm_scale = extra.get('norm_scale', 10.0)\n\n    # In our setting, 'a' is the winner (w) and 'b' is the loser (l).\n    # cost_a < cost_b\n    cost_a = batch['cost_a']\n    cost_b = batch['cost_b']\n    logp_a = batch['log_prob_w']\n    logp_b = batch['log_prob_l']\n\n    # 1. Calculate cost and log-probability differences\n    delta_cost = cost_b - cost_a\n    delta_logp = logp_a - logp_b\n\n    # 2. (Inherited) Normalize cost difference with tanh for batch-independent scaling\n    norm_delta_cost = torch.tanh(delta_cost / norm_scale)\n\n    # 3. (New Coupling 1) Apply rank-gap normalization for robustness to outliers\n    ranked_norm_cost = rank_gap(norm_delta_cost)\n\n    # 4. (Inherited) Compute adaptive margin using sigmoid on the robust, ranked value\n    margin = max_margin * torch.sigmoid(ranked_norm_cost)\n\n    # 5. Compute the core preference loss (Bradley-Terry style with margin)\n    core_loss = -F.logsigmoid(delta_logp - margin)\n\n    # 6. (New Coupling 2) Compute a log-barrier weight for more stable gradient contribution\n    # Add a small epsilon for stability in case beta * delta_cost is exactly -1, though unlikely.\n    weight_scale = torch.log(1 + beta * delta_cost + 1e-8)\n\n    # 7. Combine core loss and scaling factor\n    loss_per_sample = weight_scale * core_loss\n\n    # 8. Apply optional sample weights if provided\n    sample_weights = batch.get('weight', None)\n    if sample_weights is not None:\n        loss_per_sample = loss_per_sample * sample_weights\n\n    # 9. Return the mean loss over the batch\n    return loss_per_sample.mean()"}, "static_ok": true, "static_reason": "", "dynamic_ok": false, "dynamic_reason": "forward_error: name 'rank_gap' is not defined", "loss_value": null, "grad_norm": null}
{"generation": 9, "index": 1, "ir": {"name": "Rank-Modulated Tanh-Margin Loss", "intuition": "This loss function synthesizes ideas from its parents while introducing a novel rank-based modulation for stability and focus. It inherits the core structure of an adaptive margin loss from both parents, where the margin is a function of the cost difference (`delta_cost`). Specifically, it adopts the `tanh` normalization of `delta_cost` from Parent 1 to create a bounded, batch-independent signal representing preference strength. It also inherits the `softplus` weighting from both parents, which scales the loss by the magnitude of the cost difference, emphasizing more significant preference pairs.\n\nTwo new coupling ideas are introduced. First, instead of using `sigmoid` to shape the margin, this child uses a direct, scaled `tanh` of the cost difference. This creates a margin that is symmetric around zero and approaches `max_margin` for large positive `delta_cost` and `-max_margin` for large negative `delta_cost`, providing a consistent signal. The primary innovation is the second coupling: a rank-based modulation of the `softplus` weighting term. The `beta` parameter, which controls the amplification of the loss, is scaled by the batch-wise `rank_gap` of the `delta_logp` values. The `rank_gap` operator normalizes the rank of each sample's `delta_logp` to a [-1, 1] range. By multiplying `beta` with `(1 - rank_gap)`, the loss focuses more intensely on pairs where the model is most 'wrong' (i.e., `delta_logp` is small or negative, leading to a low rank and a high modulation factor), and less on pairs it already confidently ranks correctly. This dynamic focusing mechanism adapts the learning pressure based on the model's current performance on the batch, improving stability and learning efficiency.", "pseudocode": "1. Calculate the cost difference: delta_cost = cost(b) - cost(a).\n2. Calculate the log-probability difference: delta_logp = logp(a) - logp(b).\n3. (Inherited from Parent 1) Normalize the cost difference using a scaled tanh function: norm_delta_cost = tanh(delta_cost / norm_scale).\n4. (New Coupling 1) Compute a symmetric adaptive margin directly from the normalized cost difference: margin = max_margin * norm_delta_cost.\n5. Compute the core preference loss: core_loss = -logsigmoid(delta_logp - margin).\n6. (New Coupling 2) Compute a rank-based modulation factor for beta. First, calculate the rank gap of delta_logp across the batch: r_gap = rank_gap(delta_logp). Then, compute the modulation: rank_mod = 1.0 - r_gap. This factor is close to 2 for the worst-ranked pairs and close to 0 for the best-ranked pairs.\n7. (Inherited from both parents) Compute a cost-based weight, now modulated by the rank factor: weight_scale = softplus(beta * rank_mod * delta_cost).\n8. Combine them: final_loss = weight_scale * core_loss.\n9. Return the mean of final_loss over the batch.", "hyperparams": {"beta": 1.0, "max_margin": 1.5, "norm_scale": 10.0}, "operators_used": ["logsigmoid", "softplus", "tanh", "rank_gap"], "implementation_hint": {"expects": ["cost_a", "cost_b", "logp_a", "logp_b"], "returns": "scalar"}, "code": "import torch\nimport torch.nn.functional as F\n\ndef rank_gap(x):\n    \"\"\"Computes the rank of each element in x and normalizes it to [-1, 1].\"\"\"\n    ranks = torch.argsort(torch.argsort(x)).float()\n    # Normalize ranks to [0, 1]\n    normalized_ranks = ranks / (len(x) - 1)\n    # Scale to [-1, 1]\n    return 2 * normalized_ranks - 1\n\ndef generated_loss(batch, model_output, extra):\n    \"\"\"\n    Rank-Modulated Tanh-Margin Loss.\n\n    Args:\n        batch (dict): A dictionary containing tensors from the data loader.\n                      Expected keys: 'cost_a', 'cost_b', 'log_prob_w', 'log_prob_l'.\n                      An optional 'weight' tensor can be used for sample weighting.\n        model_output (dict): Output from the model (not used in this loss structure).\n        extra (dict): A dictionary for hyperparameters, e.g., {'beta': 1.0, 'max_margin': 1.5, 'norm_scale': 10.0}.\n\n    Returns:\n        torch.Tensor: A scalar tensor representing the mean loss.\n    \"\"\"\n    # Read hyperparameters\n    beta = extra.get('beta', 1.0)\n    max_margin = extra.get('max_margin', 1.5)\n    norm_scale = extra.get('norm_scale', 10.0)\n\n    # 'a' is the winner (w), 'b' is the loser (l). cost_a < cost_b.\n    cost_a = batch['cost_a']\n    cost_b = batch['cost_b']\n    logp_a = batch['log_prob_w']\n    logp_b = batch['log_prob_l']\n\n    # 1. Calculate cost and log-probability differences\n    delta_cost = cost_b - cost_a\n    delta_logp = logp_a - logp_b\n\n    # 2. (Inherited) Normalize cost difference with tanh for a bounded signal\n    norm_delta_cost = torch.tanh(delta_cost / norm_scale)\n\n    # 3. (New Coupling 1) Compute a symmetric adaptive margin directly from tanh output\n    margin = max_margin * norm_delta_cost\n\n    # 4. Compute the core preference loss (Bradley-Terry style with margin)\n    core_loss = -F.logsigmoid(delta_logp - margin)\n    \n    # 5. (New Coupling 2) Compute a rank-based modulation factor\n    # This focuses loss weight on pairs the model currently ranks poorly.\n    with torch.no_grad(): # Ranking should not contribute to gradients\n        # rank_gap normalizes ranks of delta_logp to [-1, 1]\n        # Low delta_logp -> rank_gap near -1. High delta_logp -> rank_gap near 1.\n        r_gap = rank_gap(delta_logp)\n        # We want to up-weight samples with low delta_logp (model is wrong).\n        # 1 - r_gap maps [-1, 1] to [2, 0].\n        rank_modulation = 1.0 - r_gap\n\n    # 6. (Inherited) Compute a cost-based scaling factor, now modulated by rank\n    weight_scale = F.softplus(beta * rank_modulation * delta_cost)\n\n    # 7. Combine core loss and scaling factor\n    loss_per_sample = weight_scale * core_loss\n\n    # 8. Apply optional sample weights if provided\n    sample_weights = batch.get('weight', None)\n    if sample_weights is not None:\n        loss_per_sample = loss_per_sample * sample_weights\n\n    # 9. Return the mean loss over the batch\n    return loss_per_sample.mean()"}, "static_ok": true, "static_reason": "", "dynamic_ok": false, "dynamic_reason": "forward_error: name 'rank_gap' is not defined", "loss_value": null, "grad_norm": null}
{"generation": 9, "index": 2, "ir": {"name": "Rank-Modulated Tanh-Margin Loss", "intuition": "This loss function creates a robust preference objective by combining an adaptive margin with a novel rank-based normalization scheme. It inherits the core structure of a logistic loss (`-logsigmoid`) with an adaptive margin from both parents ('Clipped Tanh-Normalized Adaptive Margin Loss' and 'Adaptive Sigmoid Margin Loss with Tanh Normalization'). The margin itself is a function of the cost difference, an idea also present in both parents, ensuring that pairs with larger cost gaps are required to have a larger log-probability separation.\n\nFrom 'Adaptive Sigmoid Margin Loss with Tanh Normalization' (Parent 1), it specifically inherits the use of `tanh` to create a bounded and smooth margin function, `max_margin * tanh(norm_delta_cost)`. This provides a stable, batch-independent margin that saturates for very large cost differences. From 'Clipped Tanh-Normalized Adaptive Margin Loss' (Parent 0), it inherits the idea of applying a cost-based weighting to the final loss using `softplus(beta * delta_cost)`, which focuses training on pairs with more significant cost differences.\n\nAs a new coupling idea, this child loss introduces a batch-level rank-based normalization of the log-probability differences. Instead of using the raw `delta_logp`, it uses `rank_gap(logp_a, logp_b)`, which computes the difference between the rank-percentile of `logp_a` and `logp_b` within the batch. This normalization makes the loss less sensitive to the absolute scale of log-probabilities and more focused on the relative ordering of preferences within the batch, potentially improving stability and preventing outlier log-probabilities from dominating the gradients. A second new idea is the use of `tanh` directly to shape the margin, which differs from the parents' `sigmoid(tanh(...))` structure, providing a symmetric margin around zero before being scaled.", "pseudocode": "1. Calculate the raw cost difference: delta_cost = cost(b) - cost(a).\n2. Calculate the raw log-probability difference: delta_logp = logp(a) - logp(b).\n3. (New Coupling) Normalize the log-probability difference using batch-level rank percentiles: norm_delta_logp = rank_gap(logp_a, logp_b). This makes the loss robust to the scale of log-probabilities.\n4. (Inherited from Parent 1) Compute an adaptive margin using a scaled tanh function on the cost difference: margin = max_margin * tanh(delta_cost / norm_scale). This creates a bounded, smooth margin.\n5. Compute the core preference loss using the rank-normalized log-probabilities: core_loss = -logsigmoid(norm_delta_logp - margin). This is the core logistic objective.\n6. (Inherited from Parent 0) Calculate a cost-based weight: weight_scale = softplus(beta * delta_cost). This amplifies the loss for pairs with larger cost differences.\n7. Combine them: final_loss = weight_scale * core_loss.\n8. Return the mean of final_loss over the batch.", "hyperparams": {"beta": 1.0, "max_margin": 0.5, "norm_scale": 10.0}, "operators_used": ["logsigmoid", "softplus", "tanh", "rank_gap"], "implementation_hint": {"expects": ["cost_a", "cost_b", "logp_a", "logp_b"], "returns": "scalar"}, "code": "import torch\nimport torch.nn.functional as F\n\n# Helper function for rank_gap operator\ndef _rank_gap(logp_a, logp_b):\n    \"\"\"Computes the difference in rank percentiles for logp_a and logp_b.\"\"\"\n    # Concatenate for batch-wide ranking\n    combined_logp = torch.cat([logp_a, logp_b])\n    # Get ranks (indices of sorted values)\n    ranks = torch.argsort(torch.argsort(combined_logp))\n    # Split back into a and b ranks\n    rank_a, rank_b = ranks.chunk(2)\n    # Normalize ranks to percentiles [0, 1]\n    # Add 1 to max rank to make the max percentile slightly less than 1.0\n    max_rank = combined_logp.numel() - 1\n    if max_rank == 0:\n        return torch.zeros_like(logp_a) # Avoid division by zero for batch size 1\n    norm_rank_a = rank_a.float() / max_rank\n    norm_rank_b = rank_b.float() / max_rank\n    # The gap is the difference in rank percentiles\n    return norm_rank_a - norm_rank_b\n\ndef generated_loss(batch, model_output, extra):\n    \"\"\"\n    Rank-Modulated Tanh-Margin Loss.\n\n    Args:\n        batch (dict): A dictionary containing tensors from the data loader.\n                      Expected keys: 'cost_a', 'cost_b', 'log_prob_w', 'log_prob_l'.\n                      'cost_a' and 'log_prob_w' correspond to the preferred solution (winner).\n                      'cost_b' and 'log_prob_l' correspond to the non-preferred solution (loser).\n                      An optional 'weight' tensor can be used for sample weighting.\n        model_output (dict): Output from the model (not used in this loss structure).\n        extra (dict): A dictionary for hyperparameters, e.g., {'beta': 1.0, 'max_margin': 0.5, 'norm_scale': 10.0}.\n\n    Returns:\n        torch.Tensor: A scalar tensor representing the mean loss.\n    \"\"\"\n    # Read hyperparameters\n    beta = extra.get('beta', 1.0)\n    max_margin = extra.get('max_margin', 0.5)\n    norm_scale = extra.get('norm_scale', 10.0)\n\n    # In our setting, 'a' is the winner (w) and 'b' is the loser (l).\n    # cost_a < cost_b\n    cost_a = batch['cost_a']\n    cost_b = batch['cost_b']\n    logp_a = batch['log_prob_w']\n    logp_b = batch['log_prob_l']\n\n    # 1. Calculate raw cost difference\n    delta_cost = cost_b - cost_a\n\n    # 2. (New Coupling) Normalize log-prob difference using rank_gap\n    # This focuses on relative ordering within the batch, not absolute logp scale.\n    norm_delta_logp = _rank_gap(logp_a, logp_b)\n\n    # 3. (Inherited from Parent 1) Compute adaptive margin with tanh\n    # This creates a bounded margin [-max_margin, max_margin] based on cost difference.\n    margin = max_margin * torch.tanh(delta_cost / norm_scale)\n\n    # 4. Compute the core preference loss with rank-normalized log-probs\n    # We want norm_delta_logp > margin.\n    core_loss = -F.logsigmoid(norm_delta_logp - margin)\n\n    # 5. (Inherited from Parent 0) Compute a cost-based scaling factor\n    # This amplifies the loss for pairs with larger raw cost differences.\n    weight_scale = F.softplus(beta * delta_cost)\n\n    # 6. Combine core loss and scaling factor\n    loss_per_sample = weight_scale * core_loss\n\n    # 7. Apply optional sample weights if provided\n    sample_weights = batch.get('weight', None)\n    if sample_weights is not None:\n        loss_per_sample = loss_per_sample * sample_weights\n\n    # 8. Return the mean loss over the batch\n    return loss_per_sample.mean()"}, "static_ok": true, "static_reason": "", "dynamic_ok": false, "dynamic_reason": "forward_error: name '_rank_gap' is not defined", "loss_value": null, "grad_norm": null}
{"generation": 9, "index": 3, "ir": {"name": "Logit-Scaled Adaptive Margin Loss", "intuition": "This loss function creates a dynamic, cost-sensitive margin inspired by its parents, but with a novel scaling mechanism. \n\nInherited Ideas:\n1.  From both parents, it inherits the core idea of using a logistic loss (`-logsigmoid`) where the model's log-probability difference is pushed to exceed an adaptive margin. The margin's size is a function of the cost difference between the preferred and non-preferred solutions (`delta_cost = cost_b - cost_a`).\n2.  From both parents, it also inherits the idea of scaling the entire loss term by a function of the cost difference to focus training on more significant preference pairs. Here, we use `softplus(beta * delta_cost)` for this purpose.\n\nNew Coupling Ideas:\n1.  **Logit-based Margin Scaling**: Instead of normalizing `delta_cost` with `tanh` or `zscore`, this child loss introduces a new margin mechanism. It first computes a normalized preference strength `p = sigmoid(delta_cost / temp)`. This maps the cost difference to a probability-like value in (0, 1). The margin is then derived from the logit of this value, `log(p / (1-p))`, which is simply `delta_cost / temp`. This creates a margin that is linearly proportional to the cost difference, which is simpler and more direct than the sigmoid-of-tanh approach of the parents. The `temp` parameter controls the steepness.\n2.  **Reward Difference Clipping**: To enhance stability, especially when `delta_logp` is very large or very small, the loss applies `tanh` to the core learning signal `(delta_logp - margin)`. This bounds the argument to `logsigmoid` within a stable range, preventing extreme gradients from destabilizing training. The `tanh` acts as a soft clamp on the reward difference signal, ensuring it doesn't grow uncontrollably while preserving the sign of the gradient.", "pseudocode": "1. Calculate the cost difference: delta_cost = cost(b) - cost(a).\n2. Calculate the log-probability difference: delta_logp = logp(a) - logp(b).\n3. (Inherited) Use a logistic loss framework based on the difference between `delta_logp` and a margin.\n4. (New Coupling 1) Compute an adaptive margin that is linearly proportional to the cost difference, scaled by a temperature: margin = delta_cost / temp. This is mathematically equivalent to taking the logit of a sigmoid-transformed cost difference.\n5. (New Coupling 2) Compute the core learning signal and stabilize it by applying a `tanh` function: stable_signal = tanh(delta_logp - margin). This prevents extreme values from causing numerical instability.\n6. Compute the core loss using the stabilized signal: core_loss = -logsigmoid(stable_signal).\n7. (Inherited) Calculate a cost-based weight to amplify the loss for more significant preferences: weight_scale = softplus(beta * delta_cost).\n8. Combine the weight and core loss: final_loss = weight_scale * core_loss.\n9. Return the mean of final_loss over the batch.", "hyperparams": {"beta": 1.0, "temp": 5.0}, "operators_used": ["logsigmoid", "softplus", "tanh"], "implementation_hint": {"expects": ["cost_a", "cost_b", "logp_a", "logp_b"], "returns": "scalar"}, "code": "import torch\nimport torch.nn.functional as F\n\ndef generated_loss(batch, model_output, extra):\n    \"\"\"\n    Logit-Scaled Adaptive Margin Loss.\n\n    Args:\n        batch (dict): A dictionary containing tensors from the data loader.\n                      Expected keys: 'cost_a', 'cost_b', 'log_prob_w', 'log_prob_l'.\n                      'cost_a' and 'log_prob_w' correspond to the preferred solution (winner).\n                      'cost_b' and 'log_prob_l' correspond to the non-preferred solution (loser).\n                      An optional 'weight' tensor can be used for sample weighting.\n        model_output (dict): Output from the model (not used in this loss structure).\n        extra (dict): A dictionary for hyperparameters, e.g., {'beta': 1.0, 'temp': 5.0}.\n\n    Returns:\n        torch.Tensor: A scalar tensor representing the mean loss.\n    \"\"\"\n    # Read hyperparameters\n    beta = extra.get('beta', 1.0)\n    temp = extra.get('temp', 5.0)\n\n    # In our setting, 'a' is the winner (w) and 'b' is the loser (l).\n    # cost_a < cost_b\n    cost_a = batch['cost_a']\n    cost_b = batch['cost_b']\n    logp_a = batch['log_prob_w']\n    logp_b = batch['log_prob_l']\n\n    # 1. Calculate cost and log-probability differences\n    # delta_cost should be positive as cost_b > cost_a\n    delta_cost = cost_b - cost_a\n    delta_logp = logp_a - logp_b\n\n    # 2. (New Coupling 1) Compute an adaptive margin linearly proportional to delta_cost\n    # This is equivalent to logit(sigmoid(delta_cost/temp)), providing a direct scaling.\n    # Add a small epsilon to temp to prevent division by zero.\n    margin = delta_cost / (temp + 1e-8)\n\n    # 3. (New Coupling 2) Stabilize the core learning signal using tanh\n    # This bounds the input to logsigmoid, preventing extreme gradients.\n    stable_signal = torch.tanh(delta_logp - margin)\n\n    # 4. (Inherited) Compute the core preference loss (Bradley-Terry style)\n    # We want delta_logp > margin, so (delta_logp - margin) should be positive.\n    core_loss = -F.logsigmoid(stable_signal)\n\n    # 5. (Inherited) Compute a cost-based scaling factor using softplus for stability\n    # This amplifies the loss for pairs with larger raw cost differences.\n    weight_scale = F.softplus(beta * delta_cost)\n\n    # 6. Combine core loss and scaling factor\n    loss_per_sample = weight_scale * core_loss\n\n    # 7. Apply optional sample weights if provided\n    sample_weights = batch.get('weight', None)\n    if sample_weights is not None:\n        loss_per_sample = loss_per_sample * sample_weights\n\n    # 8. Return the mean loss over the batch\n    return loss_per_sample.mean()"}, "static_ok": true, "static_reason": "", "dynamic_ok": true, "dynamic_reason": "ok", "loss_value": 1.0462709665298462, "grad_norm": 0.0}
{"generation": 9, "index": 4, "ir": {"name": "Rank-Modulated Tanh-Normalized Margin Loss", "intuition": "This loss function synthesizes an adaptive margin approach with a novel rank-based modulation scheme. It inherits the core concept of an adaptive margin from both parents, where the required separation in log-probabilities (`delta_logp`) depends on the magnitude of the cost difference (`delta_cost`). Specifically, it inherits the use of a scaled `tanh` function from Parent 1 to create a stable, batch-independent, and bounded normalization of the cost difference. It also inherits the `softplus` weighting from both parents, which scales the entire loss term by the magnitude of the cost difference, focusing training on more significant preference pairs.\n\nAs a new coupling idea, this child loss introduces a 'rank-modulated' margin. Instead of the margin being a direct `sigmoid` function of the normalized cost difference, it is now modulated by the *rank* of the cost difference within the batch. This is achieved using the `rank_gap` operator. The `rank_gap`, which is a value between 0 and 1 representing the percentile rank of `delta_cost` in the batch, is used to scale the `max_margin`. This means that pairs with a relatively small cost difference *compared to others in the same batch* will have a smaller margin, while pairs with a large relative cost difference will have a larger margin. This couples the margin's magnitude to the batch-wide distribution of preference strengths, allowing the model to dynamically focus on the most discriminative pairs in each training step. The `tanh`-normalized cost difference is still used inside the `logsigmoid` term to provide a stable offset, but the margin's *scale* is now determined by the cost's relative rank.", "pseudocode": "1. Calculate the cost difference: delta_cost = cost(b) - cost(a).\n2. Calculate the log-probability difference: delta_logp = logp(a) - logp(b).\n3. (Inherited from Parent 1) Normalize the cost difference using a scaled tanh function for stability: norm_delta_cost = tanh(delta_cost / norm_scale).\n4. (New Coupling) Compute the rank-based gap for the cost difference within the batch: cost_rank_gap = rank_gap(delta_cost). This returns a value in [0, 1] indicating the relative magnitude of the cost difference in the current batch.\n5. (New Coupling) Compute a rank-modulated margin: margin = max_margin * cost_rank_gap. The margin size is now determined by the relative importance of the preference pair in the batch.\n6. Compute the core preference loss. The argument to logsigmoid is the log-probability difference, penalized by the rank-modulated margin, and offset by the stable tanh-normalized cost difference: core_loss = -logsigmoid(delta_logp - margin + norm_delta_cost).\n7. (Inherited from both parents) Calculate a cost-based weight: weight_scale = softplus(beta * delta_cost). This amplifies the loss for pairs with larger absolute cost differences.\n8. Combine them: final_loss = weight_scale * core_loss.\n9. Return the mean of final_loss over the batch.", "hyperparams": {"beta": 1.0, "max_margin": 1.0, "norm_scale": 10.0}, "operators_used": ["logsigmoid", "softplus", "tanh", "rank_gap"], "implementation_hint": {"expects": ["cost_a", "cost_b", "logp_a", "logp_b"], "returns": "scalar"}, "code": "import torch\nimport torch.nn.functional as F\n\ndef rank_gap(x):\n    \"\"\"Computes the rank-based gap (percentile) for each element in a 1D tensor.\"\"\"\n    if x.numel() <= 1:\n        return torch.zeros_like(x)\n    # Sort the tensor and get the indices\n    _, sorted_indices = torch.sort(x)\n    # Create a tensor of ranks (0, 1, 2, ...)\n    ranks = torch.empty_like(sorted_indices, dtype=x.dtype)\n    ranks[sorted_indices] = torch.arange(x.numel(), device=x.device, dtype=x.dtype)\n    # Normalize ranks to [0, 1]\n    normalized_ranks = ranks / (x.numel() - 1)\n    return normalized_ranks\n\ndef generated_loss(batch, model_output, extra):\n    \"\"\"\n    Rank-Modulated Tanh-Normalized Margin Loss.\n\n    Args:\n        batch (dict): A dictionary containing tensors from the data loader.\n                      Expected keys: 'cost_a', 'cost_b', 'log_prob_w', 'log_prob_l'.\n                      'cost_a' and 'log_prob_w' correspond to the preferred solution (winner).\n                      'cost_b' and 'log_prob_l' correspond to the non-preferred solution (loser).\n                      An optional 'weight' tensor can be used for sample weighting.\n        model_output (dict): Output from the model (not used in this loss structure).\n        extra (dict): A dictionary for hyperparameters, e.g., {'beta': 1.0, 'max_margin': 1.0, 'norm_scale': 10.0}.\n\n    Returns:\n        torch.Tensor: A scalar tensor representing the mean loss.\n    \"\"\"\n    # Read hyperparameters\n    beta = extra.get('beta', 1.0)\n    max_margin = extra.get('max_margin', 1.0)\n    norm_scale = extra.get('norm_scale', 10.0)\n\n    # In our setting, 'a' is the winner (w) and 'b' is the loser (l).\n    # cost_a < cost_b\n    cost_a = batch['cost_a']\n    cost_b = batch['cost_b']\n    logp_a = batch['log_prob_w']\n    logp_b = batch['log_prob_l']\n\n    # 1. Calculate cost and log-probability differences\n    delta_cost = cost_b - cost_a\n    delta_logp = logp_a - logp_b\n\n    # 2. (Inherited) Normalize cost difference with tanh for a stable, bounded offset\n    norm_delta_cost = torch.tanh(delta_cost / norm_scale)\n\n    # 3. (New Coupling) Compute the rank-based gap of the cost difference in the batch\n    cost_rank_gap = rank_gap(delta_cost)\n\n    # 4. (New Coupling) Compute a rank-modulated margin\n    margin = max_margin * cost_rank_gap\n\n    # 5. Compute the core preference loss, combining the rank-modulated margin and the tanh offset\n    # The goal is to make delta_logp > (margin - norm_delta_cost)\n    # The norm_delta_cost term acts as a helpful offset, reducing the effective margin for large cost gaps.\n    core_loss = -F.logsigmoid(delta_logp - margin + norm_delta_cost)\n\n    # 6. (Inherited) Compute a cost-based scaling factor using softplus\n    weight_scale = F.softplus(beta * delta_cost)\n\n    # 7. Combine core loss and scaling factor\n    loss_per_sample = weight_scale * core_loss\n\n    # 8. Apply optional sample weights if provided\n    sample_weights = batch.get('weight', None)\n    if sample_weights is not None:\n        loss_per_sample = loss_per_sample * sample_weights\n\n    # 9. Return the mean loss over the batch\n    return loss_per_sample.mean()"}, "static_ok": true, "static_reason": "", "dynamic_ok": false, "dynamic_reason": "forward_error: name 'rank_gap' is not defined", "loss_value": null, "grad_norm": null}
{"generation": 9, "index": 5, "ir": {"name": "Rank-Modulated Tanh-Normalized Margin Loss", "intuition": "This loss function synthesizes the stable, adaptive margin concept from its parents with a novel rank-based modulation scheme. It inherits the core idea of using a `tanh`-normalized cost difference to create a dynamic margin, which is a common strength of both Parent 0 and Parent 1. This ensures the margin is sensitive to the magnitude of the cost gap in a bounded, batch-independent manner. It also inherits the `softplus(beta * delta_cost)` weighting scheme, which amplifies the loss for pairs with larger cost differences, focusing training on more significant preferences.\n\nAs a new coupling idea, this child loss introduces a 'rank-modulated' term that adjusts the core logistic loss. Instead of applying the `logsigmoid` directly to `delta_logp - margin`, it is applied to `delta_logp - margin + rank_term`. This `rank_term` is calculated using a new `rank_gap` operator, which computes the difference in the rank-percentile of `cost_a` and `cost_b` within the batch. The `rank_term` is then `rank_scale * (1 - rank_gap)`, where `rank_gap` is the percentile difference. This modification introduces batch-context awareness: for a given cost difference, a preference pair that represents a larger 'jump' in the cost ranking (e.g., comparing the best item to the worst) will have a `rank_gap` close to 1, making `rank_term` small. Conversely, a pair with a similar cost difference but adjacent ranks will have a small `rank_gap`, increasing the `rank_term`. This effectively makes the target margin harder to meet for pairs that are close in the overall quality ranking of the batch, encouraging the model to be more discriminative on fine-grained distinctions. The `tanh` function is applied to the final `rank_term` to ensure its contribution is bounded and stable.", "pseudocode": "1. Calculate the cost difference: delta_cost = cost(b) - cost(a).\n2. (Inherited from Parents) Normalize the cost difference using a scaled tanh function for a bounded signal: norm_delta_cost = tanh(delta_cost / norm_scale).\n3. (Inherited from Parents) Compute an adaptive margin based on the normalized cost: margin = max_margin * sigmoid(norm_delta_cost).\n4. Calculate the log-probability difference: delta_logp = logp(a) - logp(b).\n5. (New Coupling) Compute the rank gap within the batch: rank_gap = rank_gap(cost_a, cost_b). This operator returns the difference in percentile ranks of the costs within the current batch.\n6. (New Coupling) Compute a rank-based modulation term. This term is larger for pairs with smaller rank gaps, making the loss more stringent for closely-ranked items: rank_term = rank_scale * (1 - rank_gap).\n7. (New Coupling) Apply a tanh function to the rank term to bound its influence and ensure stability: bounded_rank_term = tanh(rank_term).\n8. Compute the core preference loss, incorporating the rank term: core_loss = -logsigmoid(delta_logp - margin + bounded_rank_term).\n9. (Inherited from Parents) Calculate a cost-based weight: weight_scale = softplus(beta * delta_cost).\n10. Combine them: final_loss = weight_scale * core_loss.\n11. Return the mean of final_loss over the batch.", "hyperparams": {"beta": 1.0, "max_margin": 2.0, "norm_scale": 10.0, "rank_scale": 0.5}, "operators_used": ["logsigmoid", "softplus", "sigmoid", "tanh", "rank_gap"], "implementation_hint": {"expects": ["cost_a", "cost_b", "logp_a", "logp_b"], "returns": "scalar"}, "code": "import torch\nimport torch.nn.functional as F\n\ndef rank_gap(cost_a, cost_b):\n    \"\"\"Calculates the percentile rank difference between cost_a and cost_b in the batch.\"\"\"\n    # Combine all costs to establish a batch-wide ranking\n    all_costs = torch.cat([cost_a, cost_b])\n    # Calculate ranks (indices of sorted values)\n    ranks = torch.argsort(torch.argsort(all_costs))\n    # Split ranks back into a and b components\n    rank_a, rank_b = ranks.chunk(2)\n    # Normalize ranks to percentiles [0, 1]\n    # Add a small epsilon to avoid division by zero if batch size is 1\n    num_items = all_costs.shape[0]\n    if num_items <= 1:\n        return torch.zeros_like(cost_a)\n    percentile_a = rank_a.float() / (num_items - 1)\n    percentile_b = rank_b.float() / (num_items - 1)\n    # Since cost_a < cost_b, rank_a should be < rank_b, so percentile_b - percentile_a is positive\n    return percentile_b - percentile_a\n\ndef generated_loss(batch, model_output, extra):\n    \"\"\"\n    Rank-Modulated Tanh-Normalized Margin Loss.\n\n    Args:\n        batch (dict): A dictionary containing tensors from the data loader.\n                      Expected keys: 'cost_a', 'cost_b', 'log_prob_w', 'log_prob_l'.\n                      'cost_a' and 'log_prob_w' correspond to the preferred solution (winner).\n                      'cost_b' and 'log_prob_l' correspond to the non-preferred solution (loser).\n                      An optional 'weight' tensor can be used for sample weighting.\n        model_output (dict): Output from the model (not used in this loss structure).\n        extra (dict): A dictionary for hyperparameters, e.g., {'beta': 1.0, 'max_margin': 2.0, 'norm_scale': 10.0, 'rank_scale': 0.5}.\n\n    Returns:\n        torch.Tensor: A scalar tensor representing the mean loss.\n    \"\"\"\n    # Read hyperparameters\n    beta = extra.get('beta', 1.0)\n    max_margin = extra.get('max_margin', 2.0)\n    norm_scale = extra.get('norm_scale', 10.0)\n    rank_scale = extra.get('rank_scale', 0.5)\n\n    # In our setting, 'a' is the winner (w) and 'b' is the loser (l).\n    cost_a = batch['cost_a']\n    cost_b = batch['cost_b']\n    logp_a = batch['log_prob_w']\n    logp_b = batch['log_prob_l']\n\n    # 1. Calculate cost and log-probability differences\n    delta_cost = cost_b - cost_a\n    delta_logp = logp_a - logp_b\n\n    # 2. (Inherited) Normalize cost difference with tanh\n    norm_delta_cost = torch.tanh(delta_cost / norm_scale)\n\n    # 3. (Inherited) Compute the adaptive sigmoid margin\n    margin = max_margin * torch.sigmoid(norm_delta_cost)\n\n    # 4. (New Coupling) Compute the rank gap and the rank-based modulation term\n    # rank_gap is in [0, 1], where 1 means max possible rank difference\n    rg = rank_gap(cost_a, cost_b)\n    # rank_term is large when rank_gap is small (items are close in rank)\n    rank_term = rank_scale * (1.0 - rg)\n    # Bound the term's influence for stability\n    bounded_rank_term = torch.tanh(rank_term)\n\n    # 5. Compute the core preference loss, modulated by the rank term\n    # A positive rank_term makes the loss more stringent (effectively a higher margin)\n    core_loss = -F.logsigmoid(delta_logp - margin + bounded_rank_term)\n\n    # 6. (Inherited) Compute a cost-based scaling factor using softplus\n    weight_scale = F.softplus(beta * delta_cost)\n\n    # 7. Combine core loss and scaling factor\n    loss_per_sample = weight_scale * core_loss\n\n    # 8. Apply optional sample weights if provided\n    sample_weights = batch.get('weight', None)\n    if sample_weights is not None:\n        loss_per_sample = loss_per_sample * sample_weights\n\n    # 9. Return the mean loss over the batch\n    return loss_per_sample.mean()"}, "static_ok": true, "static_reason": "", "dynamic_ok": false, "dynamic_reason": "forward_error: name 'rank_gap' is not defined", "loss_value": null, "grad_norm": null}
{"generation": 9, "index": 6, "ir": {"name": "Adaptive Margin Loss with Decaying Rank-Gap Normalization", "intuition": "This loss function synthesizes an adaptive margin with a novel normalization scheme and a dynamic loss scaling mechanism. \n\nIt inherits the core concept of an adaptive margin, where the required separation in log-probabilities (`delta_logp`) depends on the cost difference (`delta_cost`), from both parents ('Clipped Tanh-Normalized Adaptive Margin Loss' and 'Adaptive Sigmoid Margin Loss with Tanh Normalization'). The margin is computed via `max_margin * sigmoid(...)`, ensuring it is small for similar costs and grows for disparate costs.\n\nIt also inherits the `softplus(beta * delta_cost)` weighting scheme from Parent 1 ('Adaptive Sigmoid Margin Loss with Tanh Normalization'), which up-weights training examples where the cost difference is more significant, focusing the model on learning from clear preferences.\n\nAs a new coupling idea, this child loss replaces the `tanh` or `zscore` normalization with a 'Decaying Rank-Gap Normalization'. Instead of normalizing the cost difference directly, it normalizes the log-probability difference (`delta_logp`) by a factor derived from the cost difference. This normalization factor is `1 + exp(-alpha * delta_cost)`. When `delta_cost` is very small, this factor is close to 2, effectively shrinking `delta_logp` and making it harder to satisfy the loss, thus pushing the model to create a larger probability gap even for small cost differences. As `delta_cost` grows, the factor decays towards 1, allowing the raw `delta_logp` to be used. This creates a dynamic where the model is forced to be decisive on small gaps but is trusted more on large gaps. This normalization is directly applied inside the main `logsigmoid` term.\n\nA second new coupling idea is the introduction of a `rank_gap` term. This adds a constant penalty `gamma` to the loss if the model ranks the pair incorrectly (`delta_logp < 0`). This is implemented via `gamma * relu(-delta_logp)`, providing a direct, linear penalty for egregious misrankings, which acts as a stability floor and a strong corrective signal early in training, complementing the main logarithmic loss.", "pseudocode": "1. Calculate the cost difference: delta_cost = cost(b) - cost(a).\n2. Calculate the log-probability difference: delta_logp = logp(a) - logp(b).\n3. (Inherited from both parents) Compute an adaptive margin based on the cost difference: margin = max_margin * sigmoid(delta_cost / norm_scale).\n4. (New Coupling 1) Compute a decaying normalization factor based on the cost difference: norm_factor = 1.0 + exp(-alpha * delta_cost). This factor is ~2 for small delta_cost and decays to 1 for large delta_cost.\n5. (New Coupling 1) Apply the decaying normalization to the log-probability difference: normalized_delta_logp = delta_logp / norm_factor.\n6. Compute the primary logistic loss component using the normalized logp difference and the adaptive margin: primary_loss = -logsigmoid(normalized_delta_logp - margin).\n7. (Inherited from Parent 1) Calculate a cost-based weight: weight_scale = softplus(beta * delta_cost). This amplifies the loss for pairs with larger cost differences.\n8. (New Coupling 2) Compute a rank-gap penalty: rank_gap_penalty = gamma * rank_gap(delta_logp), where rank_gap(x) is relu(-x). This adds a linear penalty if logp(a) < logp(b).\n9. Combine the components: final_loss = weight_scale * primary_loss + rank_gap_penalty.\n10. Return the mean of final_loss over the batch.", "hyperparams": {"beta": 0.5, "max_margin": 1.5, "norm_scale": 10.0, "alpha": 0.1, "gamma": 0.1}, "operators_used": ["logsigmoid", "softplus", "sigmoid", "exp", "rank_gap", "relu"], "implementation_hint": {"expects": ["cost_a", "cost_b", "logp_a", "logp_b"], "returns": "scalar"}, "code": "import torch\nimport torch.nn.functional as F\n\n# rank_gap is not a standard torch function, but can be implemented with relu.\n# We define it here for clarity, matching the operator list.\ndef rank_gap(x):\n    return F.relu(-x)\n\ndef generated_loss(batch, model_output, extra):\n    \"\"\"\n    Adaptive Margin Loss with Decaying Rank-Gap Normalization.\n\n    Args:\n        batch (dict): A dictionary containing tensors from the data loader.\n                      Expected keys: 'cost_a', 'cost_b', 'log_prob_w', 'log_prob_l'.\n                      'cost_a' and 'log_prob_w' correspond to the preferred solution (winner).\n                      'cost_b' and 'log_prob_l' correspond to the non-preferred solution (loser).\n                      An optional 'weight' tensor can be used for sample weighting.\n        model_output (dict): Output from the model (not used in this loss structure).\n        extra (dict): A dictionary for hyperparameters.\n\n    Returns:\n        torch.Tensor: A scalar tensor representing the mean loss.\n    \"\"\"\n    # Read hyperparameters\n    beta = extra.get('beta', 0.5)\n    max_margin = extra.get('max_margin', 1.5)\n    norm_scale = extra.get('norm_scale', 10.0)\n    alpha = extra.get('alpha', 0.1)\n    gamma = extra.get('gamma', 0.1)\n\n    # In our setting, 'a' is the winner (w) and 'b' is the loser (l).\n    # cost_a < cost_b\n    cost_a = batch['cost_a']\n    cost_b = batch['cost_b']\n    logp_a = batch['log_prob_w']\n    logp_b = batch['log_prob_l']\n\n    # 1. Calculate cost and log-probability differences\n    delta_cost = cost_b - cost_a\n    delta_logp = logp_a - logp_b\n\n    # 2. (Inherited) Compute an adaptive margin based on the cost difference.\n    # Using sigmoid on scaled delta_cost directly, similar to parents but without tanh pre-normalization.\n    margin = max_margin * torch.sigmoid(delta_cost / norm_scale)\n\n    # 3. (New Coupling 1) Compute a decaying normalization factor for delta_logp.\n    # As delta_cost -> 0, factor -> 2. As delta_cost -> inf, factor -> 1.\n    # This pushes for a larger logp gap on pairs with small cost differences.\n    norm_factor = 1.0 + torch.exp(-alpha * delta_cost)\n    normalized_delta_logp = delta_logp / norm_factor\n\n    # 4. Compute the primary logistic loss component\n    primary_loss = -F.logsigmoid(normalized_delta_logp - margin)\n\n    # 5. (Inherited) Calculate a cost-based weight to amplify loss on significant pairs.\n    weight_scale = F.softplus(beta * delta_cost)\n\n    # 6. (New Coupling 2) Add a rank_gap penalty for incorrect rankings.\n    # This provides a stable, linear penalty when delta_logp is negative.\n    rank_gap_penalty = gamma * rank_gap(delta_logp)\n\n    # 7. Combine the components\n    loss_per_sample = weight_scale * primary_loss + rank_gap_penalty\n\n    # 8. Apply optional sample weights if provided\n    sample_weights = batch.get('weight', None)\n    if sample_weights is not None:\n        loss_per_sample = loss_per_sample * sample_weights\n\n    # 9. Return the mean loss over the batch\n    return loss_per_sample.mean()"}, "static_ok": true, "static_reason": "", "dynamic_ok": false, "dynamic_reason": "forward_error: name 'rank_gap' is not defined", "loss_value": null, "grad_norm": null}
{"generation": 9, "index": 7, "ir": {"name": "Rank-Gapped Adaptive Margin Loss", "intuition": "This loss function synthesizes the adaptive margin concept from both parents with a novel rank-based normalization scheme. It inherits the core logistic loss structure `-logsigmoid(delta_logp - margin)` and the `softplus(beta * delta_cost)` weighting mechanism, which are common to both `Clipped Tanh-Normalized Adaptive Margin Loss` (Parent 0) and `Adaptive Sigmoid Margin Loss with Tanh Normalization` (Parent 1). This ensures that the model is pushed to separate preferred from non-preferred solutions and that pairs with larger cost differences receive more weight.\n\nInstead of the `tanh` normalization from the parents, this child introduces a new coupling idea: `rank_gap` normalization. The `rank_gap` operator normalizes the cost difference by its rank within the batch, transforming it into a uniform distribution from -1 to 1. This makes the normalization robust to outliers and independent of the absolute scale of the cost values, focusing instead on the relative importance of each preference pair within the current batch. This rank-normalized value is then used to compute the adaptive margin, `margin = max_margin * sigmoid(rank_gapped_cost)`. \n\nAs a second new coupling, the loss incorporates an 'optimism' parameter (`optimism_factor`). This factor adds a small, positive offset to the `delta_logp` term, `delta_logp + optimism_factor`. This encourages the model to be slightly more confident in its preference predictions, acting as a gentle regularizer that pushes `logp(a)` slightly higher than `logp(b)` even when the margin is zero, which can help prevent model indifference on pairs with very small cost differences.", "pseudocode": "1. Calculate the cost difference: delta_cost = cost(b) - cost(a).\n2. (New Coupling) Normalize the cost difference using the `rank_gap` operator: rank_gapped_cost = rank_gap(delta_cost). This maps the cost differences to a range of [-1, 1] based on their rank in the batch, making the normalization robust to outliers.\n3. Compute an adaptive margin based on the rank-normalized value. This is inherited from both parents, but uses the new normalization: margin = max_margin * sigmoid(rank_gapped_cost).\n4. Calculate the log-probability difference: delta_logp = logp(a) - logp(b).\n5. (New Coupling) Introduce an optimism factor to the log-probability difference: optimistic_delta_logp = delta_logp + optimism_factor. This encourages a baseline level of confidence.\n6. Compute the core preference loss using the optimistic delta_logp: core_loss = -logsigmoid(optimistic_delta_logp - margin). This is inherited from both parents.\n7. Calculate a cost-based weight: weight_scale = softplus(beta * delta_cost). This is also inherited from both parents.\n8. Combine them: final_loss = weight_scale * core_loss.\n9. Return the mean of final_loss over the batch.", "hyperparams": {"beta": 1.0, "max_margin": 1.5, "optimism_factor": 0.1}, "operators_used": ["logsigmoid", "softplus", "sigmoid", "rank_gap"], "implementation_hint": {"expects": ["cost_a", "cost_b", "log_prob_w", "log_prob_l"], "returns": "scalar"}, "code": "import torch\nimport torch.nn.functional as F\n\ndef rank_gap(x):\n    \"\"\"Normalizes a tensor by its rank to the range [-1, 1].\"\"\"\n    if x.numel() <= 1:\n        return torch.zeros_like(x)\n    # Compute ranks (handle ties by averaging)\n    ranks = x.argsort().argsort().float()\n    # Scale ranks to [0, 1]\n    scaled_ranks = ranks / (x.numel() - 1)\n    # Shift to [-1, 1]\n    return 2.0 * scaled_ranks - 1.0\n\ndef generated_loss(batch, model_output, extra):\n    \"\"\"\n    Rank-Gapped Adaptive Margin Loss.\n\n    Args:\n        batch (dict): A dictionary containing tensors from the data loader.\n                      Expected keys: 'cost_a', 'cost_b', 'log_prob_w', 'log_prob_l'.\n                      'cost_a' and 'log_prob_w' correspond to the preferred solution (winner).\n                      'cost_b' and 'log_prob_l' correspond to the non-preferred solution (loser).\n                      An optional 'weight' tensor can be used for sample weighting.\n        model_output (dict): Output from the model (not used in this loss structure).\n        extra (dict): A dictionary for hyperparameters, e.g., {'beta': 1.0, 'max_margin': 1.5, 'optimism_factor': 0.1}.\n\n    Returns:\n        torch.Tensor: A scalar tensor representing the mean loss.\n    \"\"\"\n    # Read hyperparameters\n    beta = extra.get('beta', 1.0)\n    max_margin = extra.get('max_margin', 1.5)\n    optimism_factor = extra.get('optimism_factor', 0.1)\n\n    # In our setting, 'a' is the winner (w) and 'b' is the loser (l).\n    # cost_a < cost_b\n    cost_a = batch['cost_a']\n    cost_b = batch['cost_b']\n    logp_a = batch['log_prob_w']\n    logp_b = batch['log_prob_l']\n\n    # Calculate cost and log-probability differences\n    delta_cost = cost_b - cost_a\n    delta_logp = logp_a - logp_b\n\n    # (New Coupling 1) Normalize cost difference using rank_gap for outlier robustness\n    with torch.no_grad(): # no_grad to prevent gradients through the ranking process\n      rank_gapped_cost = rank_gap(delta_cost)\n\n    # Compute the adaptive margin (idea inherited from parents)\n    margin = max_margin * torch.sigmoid(rank_gapped_cost)\n\n    # (New Coupling 2) Introduce an optimism factor to the log-probability difference\n    optimistic_delta_logp = delta_logp + optimism_factor\n\n    # Compute the core preference loss (inherited from parents)\n    core_loss = -F.logsigmoid(optimistic_delta_logp - margin)\n\n    # Compute cost-based scaling factor (inherited from parents)\n    weight_scale = F.softplus(beta * delta_cost)\n\n    # Combine core loss and scaling factor\n    loss_per_sample = weight_scale * core_loss\n\n    # Apply optional sample weights if provided\n    sample_weights = batch.get('weight', None)\n    if sample_weights is not None:\n        loss_per_sample = loss_per_sample * sample_weights\n\n    # Return the mean loss over the batch\n    return loss_per_sample.mean()"}, "static_ok": true, "static_reason": "", "dynamic_ok": false, "dynamic_reason": "forward_error: name 'rank_gap' is not defined", "loss_value": null, "grad_norm": null}
