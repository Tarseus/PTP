{"generation": 1, "index": 4, "ir": {"name": "Adaptive Sigmoid Margin Loss", "intuition": "This loss function uses a dynamic, cost-sensitive margin to separate preferred and non-preferred solutions. The margin is not fixed but is a sigmoid function of the normalized cost difference. When the cost difference is small, the margin is also small, allowing the model to be uncertain. As the cost difference grows, the margin increases towards a maximum value, demanding a stronger preference signal from the model. This adaptive margin is combined with a standard logistic loss (logsigmoid) on the log-probability difference. The entire term is then scaled by a softplus function of the cost difference, which amplifies the loss for pairs with larger cost gaps, focusing the training on more significant preference signals while ensuring numerical stability and bounded gradients.", "pseudocode": "1. Calculate the cost difference: delta_cost = cost(b) - cost(a).\n2. Normalize the cost difference across the batch (e.g., z-score) to get norm_delta_cost. This makes the loss invariant to the scale of the costs.\n3. Compute an adaptive margin: margin = max_margin * sigmoid(norm_delta_cost). The margin is small for small cost gaps and approaches max_margin for large gaps.\n4. Calculate the log-probability difference: delta_logp = logp(a) - logp(b).\n5. Compute the core preference loss: core_loss = -logsigmoid(delta_logp - margin). This is a logistic loss that pushes delta_logp to be greater than the adaptive margin.\n6. Calculate a cost-based weight: weight_scale = softplus(beta * delta_cost). This amplifies the loss for pairs with larger cost differences, making the training focus on more impactful preferences.\n7. Combine them: final_loss = weight_scale * core_loss.\n8. Return the mean of final_loss over the batch.", "hyperparams": {"beta": 1.0, "max_margin": 2.0, "eps": 1e-08}, "operators_used": ["logsigmoid", "softplus", "sigmoid", "zscore"], "implementation_hint": {"expects": ["cost_a", "cost_b", "logp_a", "logp_b"], "returns": "scalar"}, "code": "import torch\nimport torch.nn.functional as F\n\ndef generated_loss(batch, model_output, extra):\n    \"\"\"\n    Adaptive Sigmoid Margin Loss.\n\n    Args:\n        batch (dict): A dictionary containing tensors from the data loader.\n                      Expected keys: 'cost_a', 'cost_b', 'log_prob_w', 'log_prob_l'.\n                      'cost_a' and 'log_prob_w' correspond to the preferred solution (winner).\n                      'cost_b' and 'log_prob_l' correspond to the non-preferred solution (loser).\n                      An optional 'weight' tensor can be used for sample weighting.\n        model_output (dict): Output from the model (not used in this loss structure).\n        extra (dict): A dictionary for hyperparameters, e.g., {'beta': 1.0, 'max_margin': 2.0, 'eps': 1e-8}.\n\n    Returns:\n        torch.Tensor: A scalar tensor representing the mean loss.\n    \"\"\"\n    # Read hyperparameters\n    beta = extra.get('beta', 1.0)\n    max_margin = extra.get('max_margin', 2.0)\n    eps = extra.get('eps', 1e-8)\n\n    # In our setting, 'a' is the winner (w) and 'b' is the loser (l).\n    # cost_a < cost_b\n    cost_a = batch['cost_a']\n    cost_b = batch['cost_b']\n    logp_a = batch['log_prob_w']\n    logp_b = batch['log_prob_l']\n\n    # 1. Calculate cost and log-probability differences\n    # delta_cost should be positive since cost_b > cost_a\n    delta_cost = cost_b - cost_a\n    delta_logp = logp_a - logp_b\n\n    # 2. Normalize the cost difference (z-score for batch-level adaptation)\n    # Ensure we only normalize if there's more than one element and some variance\n    if delta_cost.numel() > 1 and delta_cost.std() > eps:\n        mean = delta_cost.mean()\n        std = delta_cost.std()\n        norm_delta_cost = (delta_cost - mean) / (std + eps)\n    else:\n        norm_delta_cost = torch.zeros_like(delta_cost)\n\n    # 3. Compute the adaptive sigmoid margin\n    # The margin is a function of the normalized cost difference.\n    margin = max_margin * torch.sigmoid(norm_delta_cost)\n\n    # 4. Compute the core preference loss (Bradley-Terry style with margin)\n    # We want logp_a > logp_b + margin, so (logp_a - logp_b - margin) should be positive.\n    # The logsigmoid of a large positive number is close to 0.\n    core_loss = -F.logsigmoid(delta_logp - margin)\n\n    # 5. Compute a cost-based scaling factor using softplus for stability\n    # This amplifies the loss for pairs with larger raw cost differences.\n    weight_scale = F.softplus(beta * delta_cost)\n\n    # 6. Combine core loss and scaling factor\n    loss_per_sample = weight_scale * core_loss\n\n    # 7. Apply optional sample weights if provided\n    sample_weights = batch.get('weight', None)\n    if sample_weights is not None:\n        loss_per_sample = loss_per_sample * sample_weights\n\n    # 8. Return the mean loss over the batch\n    return loss_per_sample.mean()"}, "fitness": {"hf_like_score": 3.846902370452881, "validation_objective": 3.846902370452881, "generalization_penalty": 0.0, "generalization_objectives": {"20": 3.842053771018982}, "train_score_mean": 3.9600448080918307, "train_loss_mean": 0.35803062787001816, "pair_count": 187116016, "config": {"hf": {"problem": "tsp", "hf_steps": 0, "hf_epochs": 10, "hf_instances_per_epoch": 100000, "train_problem_size": 20, "valid_problem_sizes": [20], "train_batch_size": 64, "pomo_size": 20, "learning_rate": 0.0003, "weight_decay": 1e-06, "alpha": 0.05, "device": "cuda:0", "seed": 1234, "num_validation_episodes": 128, "validation_batch_size": 64, "generalization_penalty_weight": 1.0, "pool_version": "v0"}, "free_loss": {"f1_steps": 0, "total_train_steps": 15630, "f2_steps": 100, "f3_enabled": false}}, "loss_ir": {"name": "Adaptive Sigmoid Margin Loss", "intuition": "This loss function uses a dynamic, cost-sensitive margin to separate preferred and non-preferred solutions. The margin is not fixed but is a sigmoid function of the normalized cost difference. When the cost difference is small, the margin is also small, allowing the model to be uncertain. As the cost difference grows, the margin increases towards a maximum value, demanding a stronger preference signal from the model. This adaptive margin is combined with a standard logistic loss (logsigmoid) on the log-probability difference. The entire term is then scaled by a softplus function of the cost difference, which amplifies the loss for pairs with larger cost gaps, focusing the training on more significant preference signals while ensuring numerical stability and bounded gradients.", "hyperparams": {"beta": 1.0, "max_margin": 2.0, "eps": 1e-08}, "operators_used": ["logsigmoid", "softplus", "sigmoid", "zscore"]}}, "better_than_baseline": true}
{"generation": 2, "index": 0, "ir": {"name": "Adaptive Sigmoid Margin Loss with Tanh Scaling", "intuition": "This loss function modifies the parent by replacing the unbounded `softplus` scaling with a bounded `tanh` scaling. The core idea remains the same: use a dynamic, cost-sensitive margin based on a sigmoid function of the normalized cost difference. This adaptive margin is used in a standard logistic loss. However, instead of amplifying the loss indefinitely with `softplus(beta * delta_cost)`, this version uses `tanh(beta * delta_cost)` to scale the loss. This bounds the influence of any single data point, especially outliers with extremely large cost differences, which can prevent gradient explosion and lead to more stable training. The loss is still amplified for larger cost gaps, but this amplification saturates, making the training process more robust.", "pseudocode": "1. Calculate the cost difference: delta_cost = cost(b) - cost(a).\n2. Normalize the cost difference across the batch (e.g., z-score) to get norm_delta_cost. This makes the loss invariant to the scale of the costs.\n3. Compute an adaptive margin: margin = max_margin * sigmoid(norm_delta_cost). The margin is small for small cost gaps and approaches max_margin for large gaps.\n4. Calculate the log-probability difference: delta_logp = logp(a) - logp(b).\n5. Compute the core preference loss: core_loss = -logsigmoid(delta_logp - margin). This is a logistic loss that pushes delta_logp to be greater than the adaptive margin.\n6. Calculate a cost-based weight: weight_scale = tanh(beta * delta_cost). This amplifies the loss for pairs with larger cost differences but bounds the scaling factor between 0 and 1, preventing instability from outliers.\n7. Combine them: final_loss = weight_scale * core_loss.\n8. Return the mean of final_loss over the batch.", "hyperparams": {"beta": 1.0, "max_margin": 2.0, "eps": 1e-08}, "operators_used": ["logsigmoid", "tanh", "sigmoid", "zscore"], "implementation_hint": {"expects": ["cost_a", "cost_b", "logp_a", "logp_b"], "returns": "scalar"}, "code": "import torch\nimport torch.nn.functional as F\n\ndef generated_loss(batch, model_output, extra):\n    \"\"\"\n    Adaptive Sigmoid Margin Loss with Tanh Scaling.\n\n    Args:\n        batch (dict): A dictionary containing tensors from the data loader.\n                      Expected keys: 'cost_a', 'cost_b', 'log_prob_w', 'log_prob_l'.\n                      'cost_a' and 'log_prob_w' correspond to the preferred solution (winner).\n                      'cost_b' and 'log_prob_l' correspond to the non-preferred solution (loser).\n                      An optional 'weight' tensor can be used for sample weighting.\n        model_output (dict): Output from the model (not used in this loss structure).\n        extra (dict): A dictionary for hyperparameters, e.g., {'beta': 1.0, 'max_margin': 2.0, 'eps': 1e-8}.\n\n    Returns:\n        torch.Tensor: A scalar tensor representing the mean loss.\n    \"\"\"\n    # Read hyperparameters\n    beta = extra.get('beta', 1.0)\n    max_margin = extra.get('max_margin', 2.0)\n    eps = extra.get('eps', 1e-8)\n\n    # In our setting, 'a' is the winner (w) and 'b' is the loser (l).\n    # cost_a < cost_b\n    cost_a = batch['cost_a']\n    cost_b = batch['cost_b']\n    logp_a = batch['log_prob_w']\n    logp_b = batch['log_prob_l']\n\n    # 1. Calculate cost and log-probability differences\n    # delta_cost should be positive since cost_b > cost_a\n    delta_cost = cost_b - cost_a\n    delta_logp = logp_a - logp_b\n\n    # 2. Normalize the cost difference (z-score for batch-level adaptation)\n    # Ensure we only normalize if there's more than one element and some variance\n    if delta_cost.numel() > 1 and delta_cost.std() > eps:\n        mean = delta_cost.mean()\n        std = delta_cost.std()\n        norm_delta_cost = (delta_cost - mean) / (std + eps)\n    else:\n        norm_delta_cost = torch.zeros_like(delta_cost)\n\n    # 3. Compute the adaptive sigmoid margin\n    # The margin is a function of the normalized cost difference.\n    margin = max_margin * torch.sigmoid(norm_delta_cost)\n\n    # 4. Compute the core preference loss (Bradley-Terry style with margin)\n    # We want logp_a > logp_b + margin, so (logp_a - logp_b - margin) should be positive.\n    # The logsigmoid of a large positive number is close to 0.\n    core_loss = -F.logsigmoid(delta_logp - margin)\n\n    # 5. Compute a cost-based scaling factor using tanh for bounded scaling\n    # This amplifies the loss for pairs with larger raw cost differences but bounds the effect.\n    weight_scale = torch.tanh(beta * delta_cost)\n\n    # 6. Combine core loss and scaling factor\n    loss_per_sample = weight_scale * core_loss\n\n    # 7. Apply optional sample weights if provided\n    sample_weights = batch.get('weight', None)\n    if sample_weights is not None:\n        loss_per_sample = loss_per_sample * sample_weights\n\n    # 8. Return the mean loss over the batch\n    return loss_per_sample.mean()"}, "fitness": {"hf_like_score": 3.8457462787628174, "validation_objective": 3.8457462787628174, "generalization_penalty": 0.0, "generalization_objectives": {"20": 3.840752124786377}, "train_score_mean": 3.8448974433344905, "train_loss_mean": 0.06649178131599687, "pair_count": 178946359, "config": {"hf": {"problem": "tsp", "hf_steps": 0, "hf_epochs": 10, "hf_instances_per_epoch": 100000, "train_problem_size": 20, "valid_problem_sizes": [20], "train_batch_size": 64, "pomo_size": 20, "learning_rate": 0.0003, "weight_decay": 1e-06, "alpha": 0.05, "device": "cuda:0", "seed": 1234, "num_validation_episodes": 128, "validation_batch_size": 64, "generalization_penalty_weight": 1.0, "pool_version": "v0"}, "free_loss": {"f1_steps": 0, "total_train_steps": 15630, "f2_steps": 100, "f3_enabled": false}}, "loss_ir": {"name": "Adaptive Sigmoid Margin Loss with Tanh Scaling", "intuition": "This loss function modifies the parent by replacing the unbounded `softplus` scaling with a bounded `tanh` scaling. The core idea remains the same: use a dynamic, cost-sensitive margin based on a sigmoid function of the normalized cost difference. This adaptive margin is used in a standard logistic loss. However, instead of amplifying the loss indefinitely with `softplus(beta * delta_cost)`, this version uses `tanh(beta * delta_cost)` to scale the loss. This bounds the influence of any single data point, especially outliers with extremely large cost differences, which can prevent gradient explosion and lead to more stable training. The loss is still amplified for larger cost gaps, but this amplification saturates, making the training process more robust.", "hyperparams": {"beta": 1.0, "max_margin": 2.0, "eps": 1e-08}, "operators_used": ["logsigmoid", "tanh", "sigmoid", "zscore"]}}, "better_than_baseline": true}
{"generation": 2, "index": 2, "ir": {"name": "Adaptive Tanh Margin Loss", "intuition": "This loss function refines the adaptive margin concept by using the `tanh` function instead of `sigmoid`. The margin, which separates preferred and non-preferred solutions, is now proportional to `tanh` of the normalized cost difference. The `tanh` function is symmetric around zero and has a steeper slope, which might make the margin more responsive to small cost differences around the mean. The loss still uses a standard logistic objective (`logsigmoid`) and is scaled by a `softplus` function of the cost difference, preserving the parent's core structure of focusing on more significant preferences while ensuring stability. The switch to `tanh` offers a different margin dynamic, potentially improving learning for pairs with small to medium cost gaps.", "pseudocode": "1. Calculate the cost difference: delta_cost = cost(b) - cost(a).\n2. Normalize the cost difference across the batch (e.g., z-score) to get norm_delta_cost. This makes the loss invariant to the scale of the costs.\n3. Compute an adaptive margin using tanh: margin = max_margin * tanh(norm_delta_cost). The margin is small for small cost gaps and approaches max_margin for large gaps, with a steeper response around the mean compared to sigmoid.\n4. Calculate the log-probability difference: delta_logp = logp(a) - logp(b).\n5. Compute the core preference loss: core_loss = -logsigmoid(delta_logp - margin). This is a logistic loss that pushes delta_logp to be greater than the adaptive margin.\n6. Calculate a cost-based weight: weight_scale = softplus(beta * delta_cost). This amplifies the loss for pairs with larger cost differences, focusing training on more impactful preferences.\n7. Combine them: final_loss = weight_scale * core_loss.\n8. Return the mean of final_loss over the batch.", "hyperparams": {"beta": 1.0, "max_margin": 1.0, "eps": 1e-08}, "operators_used": ["logsigmoid", "softplus", "tanh", "zscore"], "implementation_hint": {"expects": ["cost_a", "cost_b", "logp_a", "logp_b"], "returns": "scalar"}, "code": "import torch\nimport torch.nn.functional as F\n\ndef generated_loss(batch, model_output, extra):\n    \"\"\"\n    Adaptive Tanh Margin Loss.\n\n    Args:\n        batch (dict): A dictionary containing tensors from the data loader.\n                      Expected keys: 'cost_a', 'cost_b', 'log_prob_w', 'log_prob_l'.\n                      'cost_a' and 'log_prob_w' correspond to the preferred solution (winner).\n                      'cost_b' and 'log_prob_l' correspond to the non-preferred solution (loser).\n                      An optional 'weight' tensor can be used for sample weighting.\n        model_output (dict): Output from the model (not used in this loss structure).\n        extra (dict): A dictionary for hyperparameters, e.g., {'beta': 1.0, 'max_margin': 1.0, 'eps': 1e-8}.\n\n    Returns:\n        torch.Tensor: A scalar tensor representing the mean loss.\n    \"\"\"\n    # Read hyperparameters\n    beta = extra.get('beta', 1.0)\n    max_margin = extra.get('max_margin', 1.0)\n    eps = extra.get('eps', 1e-8)\n\n    # In our setting, 'a' is the winner (w) and 'b' is the loser (l).\n    # cost_a < cost_b\n    cost_a = batch['cost_a']\n    cost_b = batch['cost_b']\n    logp_a = batch['log_prob_w']\n    logp_b = batch['log_prob_l']\n\n    # 1. Calculate cost and log-probability differences\n    # delta_cost should be positive since cost_b > cost_a\n    delta_cost = cost_b - cost_a\n    delta_logp = logp_a - logp_b\n\n    # 2. Normalize the cost difference (z-score for batch-level adaptation)\n    # Ensure we only normalize if there's more than one element and some variance\n    if delta_cost.numel() > 1 and delta_cost.std() > eps:\n        mean = delta_cost.mean()\n        std = delta_cost.std()\n        norm_delta_cost = (delta_cost - mean) / (std + eps)\n    else:\n        norm_delta_cost = torch.zeros_like(delta_cost)\n\n    # 3. Compute the adaptive tanh margin\n    # The margin is a function of the normalized cost difference using tanh.\n    margin = max_margin * torch.tanh(norm_delta_cost)\n\n    # 4. Compute the core preference loss (Bradley-Terry style with margin)\n    # We want logp_a > logp_b + margin, so (logp_a - logp_b - margin) should be positive.\n    # The logsigmoid of a large positive number is close to 0.\n    core_loss = -F.logsigmoid(delta_logp - margin)\n\n    # 5. Compute a cost-based scaling factor using softplus for stability\n    # This amplifies the loss for pairs with larger raw cost differences.\n    weight_scale = F.softplus(beta * delta_cost)\n\n    # 6. Combine core loss and scaling factor\n    loss_per_sample = weight_scale * core_loss\n\n    # 7. Apply optional sample weights if provided\n    sample_weights = batch.get('weight', None)\n    if sample_weights is not None:\n        loss_per_sample = loss_per_sample * sample_weights\n\n    # 8. Return the mean loss over the batch\n    return loss_per_sample.mean()"}, "fitness": {"hf_like_score": 3.8458210229873657, "validation_objective": 3.8458210229873657, "generalization_penalty": 0.0, "generalization_objectives": {"20": 3.841160774230957}, "train_score_mean": 4.519176265877634, "train_loss_mean": 0.15401005518270547, "pair_count": 190060687, "config": {"hf": {"problem": "tsp", "hf_steps": 0, "hf_epochs": 10, "hf_instances_per_epoch": 100000, "train_problem_size": 20, "valid_problem_sizes": [20], "train_batch_size": 64, "pomo_size": 20, "learning_rate": 0.0003, "weight_decay": 1e-06, "alpha": 0.05, "device": "cuda:1", "seed": 1234, "num_validation_episodes": 128, "validation_batch_size": 64, "generalization_penalty_weight": 1.0, "pool_version": "v0"}, "free_loss": {"f1_steps": 0, "total_train_steps": 15630, "f2_steps": 100, "f3_enabled": false}}, "loss_ir": {"name": "Adaptive Tanh Margin Loss", "intuition": "This loss function refines the adaptive margin concept by using the `tanh` function instead of `sigmoid`. The margin, which separates preferred and non-preferred solutions, is now proportional to `tanh` of the normalized cost difference. The `tanh` function is symmetric around zero and has a steeper slope, which might make the margin more responsive to small cost differences around the mean. The loss still uses a standard logistic objective (`logsigmoid`) and is scaled by a `softplus` function of the cost difference, preserving the parent's core structure of focusing on more significant preferences while ensuring stability. The switch to `tanh` offers a different margin dynamic, potentially improving learning for pairs with small to medium cost gaps.", "hyperparams": {"beta": 1.0, "max_margin": 1.0, "eps": 1e-08}, "operators_used": ["logsigmoid", "softplus", "tanh", "zscore"]}}, "better_than_baseline": true}
{"generation": 2, "index": 4, "ir": {"name": "Adaptive Sigmoid Margin Loss with Tanh", "intuition": "This loss function uses a dynamic, cost-sensitive margin to separate preferred and non-preferred solutions. The margin is not fixed but is a tanh function of the normalized cost difference. When the cost difference is small, the margin is also small, allowing the model to be uncertain. As the cost difference grows, the margin increases towards a maximum value, demanding a stronger preference signal from the model. Using tanh instead of sigmoid provides a symmetric margin around zero for the normalized cost difference. This adaptive margin is combined with a standard logistic loss (logsigmoid) on the log-probability difference. The entire term is then scaled by a softplus function of the cost difference, which amplifies the loss for pairs with larger cost gaps, focusing the training on more significant preference signals while ensuring numerical stability and bounded gradients.", "pseudocode": "1. Calculate the cost difference: delta_cost = cost(b) - cost(a).\n2. Normalize the cost difference across the batch (e.g., z-score) to get norm_delta_cost. This makes the loss invariant to the scale of the costs.\n3. Compute an adaptive margin: margin = max_margin * tanh(norm_delta_cost). The margin is small for small cost gaps and approaches max_margin for large positive gaps.\n4. Calculate the log-probability difference: delta_logp = logp(a) - logp(b).\n5. Compute the core preference loss: core_loss = -logsigmoid(delta_logp - margin). This is a logistic loss that pushes delta_logp to be greater than the adaptive margin.\n6. Calculate a cost-based weight: weight_scale = softplus(beta * delta_cost). This amplifies the loss for pairs with larger cost differences, making the training focus on more impactful preferences.\n7. Combine them: final_loss = weight_scale * core_loss.\n8. Return the mean of final_loss over the batch.", "hyperparams": {"beta": 1.0, "max_margin": 1.5, "eps": 1e-08}, "operators_used": ["logsigmoid", "softplus", "tanh", "zscore"], "implementation_hint": {"expects": ["cost_a", "cost_b", "logp_a", "logp_b"], "returns": "scalar"}, "code": "import torch\nimport torch.nn.functional as F\n\ndef generated_loss(batch, model_output, extra):\n    \"\"\"\n    Adaptive Sigmoid Margin Loss with Tanh.\n\n    Args:\n        batch (dict): A dictionary containing tensors from the data loader.\n                      Expected keys: 'cost_a', 'cost_b', 'log_prob_w', 'log_prob_l'.\n                      'cost_a' and 'log_prob_w' correspond to the preferred solution (winner).\n                      'cost_b' and 'log_prob_l' correspond to the non-preferred solution (loser).\n                      An optional 'weight' tensor can be used for sample weighting.\n        model_output (dict): Output from the model (not used in this loss structure).\n        extra (dict): A dictionary for hyperparameters, e.g., {'beta': 1.0, 'max_margin': 1.5, 'eps': 1e-8}.\n\n    Returns:\n        torch.Tensor: A scalar tensor representing the mean loss.\n    \"\"\"\n    # Read hyperparameters\n    beta = extra.get('beta', 1.0)\n    max_margin = extra.get('max_margin', 1.5)\n    eps = extra.get('eps', 1e-8)\n\n    # In our setting, 'a' is the winner (w) and 'b' is the loser (l).\n    # cost_a < cost_b\n    cost_a = batch['cost_a']\n    cost_b = batch['cost_b']\n    logp_a = batch['log_prob_w']\n    logp_b = batch['log_prob_l']\n\n    # 1. Calculate cost and log-probability differences\n    # delta_cost should be positive since cost_b > cost_a\n    delta_cost = cost_b - cost_a\n    delta_logp = logp_a - logp_b\n\n    # 2. Normalize the cost difference (z-score for batch-level adaptation)\n    # Ensure we only normalize if there's more than one element and some variance\n    if delta_cost.numel() > 1 and delta_cost.std() > eps:\n        mean = delta_cost.mean()\n        std = delta_cost.std()\n        norm_delta_cost = (delta_cost - mean) / (std + eps)\n    else:\n        norm_delta_cost = torch.zeros_like(delta_cost)\n\n    # 3. Compute the adaptive tanh margin\n    # The margin is a function of the normalized cost difference.\n    margin = max_margin * torch.tanh(norm_delta_cost)\n\n    # 4. Compute the core preference loss (Bradley-Terry style with margin)\n    # We want logp_a > logp_b + margin, so (logp_a - logp_b - margin) should be positive.\n    # The logsigmoid of a large positive number is close to 0.\n    core_loss = -F.logsigmoid(delta_logp - margin)\n\n    # 5. Compute a cost-based scaling factor using softplus for stability\n    # This amplifies the loss for pairs with larger raw cost differences.\n    weight_scale = F.softplus(beta * delta_cost)\n\n    # 6. Combine core loss and scaling factor\n    loss_per_sample = weight_scale * core_loss\n\n    # 7. Apply optional sample weights if provided\n    sample_weights = batch.get('weight', None)\n    if sample_weights is not None:\n        loss_per_sample = loss_per_sample * sample_weights\n\n    # 8. Return the mean loss over the batch\n    return loss_per_sample.mean()"}, "fitness": {"hf_like_score": 3.8456132411956787, "validation_objective": 3.8456132411956787, "generalization_penalty": 0.0, "generalization_objectives": {"20": 3.8415075540542603}, "train_score_mean": 4.385796546463164, "train_loss_mean": 0.10828321632200415, "pair_count": 190060659, "config": {"hf": {"problem": "tsp", "hf_steps": 0, "hf_epochs": 10, "hf_instances_per_epoch": 100000, "train_problem_size": 20, "valid_problem_sizes": [20], "train_batch_size": 64, "pomo_size": 20, "learning_rate": 0.0003, "weight_decay": 1e-06, "alpha": 0.05, "device": "cuda:2", "seed": 1234, "num_validation_episodes": 128, "validation_batch_size": 64, "generalization_penalty_weight": 1.0, "pool_version": "v0"}, "free_loss": {"f1_steps": 0, "total_train_steps": 15630, "f2_steps": 100, "f3_enabled": false}}, "loss_ir": {"name": "Adaptive Sigmoid Margin Loss with Tanh", "intuition": "This loss function uses a dynamic, cost-sensitive margin to separate preferred and non-preferred solutions. The margin is not fixed but is a tanh function of the normalized cost difference. When the cost difference is small, the margin is also small, allowing the model to be uncertain. As the cost difference grows, the margin increases towards a maximum value, demanding a stronger preference signal from the model. Using tanh instead of sigmoid provides a symmetric margin around zero for the normalized cost difference. This adaptive margin is combined with a standard logistic loss (logsigmoid) on the log-probability difference. The entire term is then scaled by a softplus function of the cost difference, which amplifies the loss for pairs with larger cost gaps, focusing the training on more significant preference signals while ensuring numerical stability and bounded gradients.", "hyperparams": {"beta": 1.0, "max_margin": 1.5, "eps": 1e-08}, "operators_used": ["logsigmoid", "softplus", "tanh", "zscore"]}}, "better_than_baseline": true}
{"generation": 2, "index": 6, "ir": {"name": "Adaptive Sigmoid Margin Loss with Tanh Normalization", "intuition": "This loss function uses a dynamic, cost-sensitive margin to separate preferred and non-preferred solutions. The margin is not fixed but is a sigmoid function of the normalized cost difference. When the cost difference is small, the margin is also small, allowing the model to be uncertain. As the cost difference grows, the margin increases towards a maximum value, demanding a stronger preference signal from the model. Instead of z-score normalization which is batch-dependent, this version uses a scaled tanh function on the cost difference, which provides a bounded, batch-independent normalization. This adaptive margin is combined with a standard logistic loss (logsigmoid) on the log-probability difference. The entire term is then scaled by a softplus function of the cost difference, which amplifies the loss for pairs with larger cost gaps, focusing the training on more significant preference signals.", "pseudocode": "1. Calculate the cost difference: delta_cost = cost(b) - cost(a).\n2. Normalize the cost difference using a scaled tanh function: norm_delta_cost = tanh(delta_cost / scale). This provides a bounded, batch-independent normalization of the cost gap.\n3. Compute an adaptive margin: margin = max_margin * sigmoid(norm_delta_cost). The margin is small for small cost gaps and approaches max_margin for large gaps.\n4. Calculate the log-probability difference: delta_logp = logp(a) - logp(b).\n5. Compute the core preference loss: core_loss = -logsigmoid(delta_logp - margin). This is a logistic loss that pushes delta_logp to be greater than the adaptive margin.\n6. Calculate a cost-based weight: weight_scale = softplus(beta * delta_cost). This amplifies the loss for pairs with larger cost differences, making the training focus on more impactful preferences.\n7. Combine them: final_loss = weight_scale * core_loss.\n8. Return the mean of final_loss over the batch.", "hyperparams": {"beta": 1.0, "max_margin": 2.0, "norm_scale": 10.0}, "operators_used": ["logsigmoid", "softplus", "sigmoid", "tanh"], "implementation_hint": {"expects": ["cost_a", "cost_b", "log_prob_w", "log_prob_l"], "returns": "scalar"}, "code": "import torch\nimport torch.nn.functional as F\n\ndef generated_loss(batch, model_output, extra):\n    \"\"\"\n    Adaptive Sigmoid Margin Loss with Tanh Normalization.\n\n    Args:\n        batch (dict): A dictionary containing tensors from the data loader.\n                      Expected keys: 'cost_a', 'cost_b', 'log_prob_w', 'log_prob_l'.\n                      'cost_a' and 'log_prob_w' correspond to the preferred solution (winner).\n                      'cost_b' and 'log_prob_l' correspond to the non-preferred solution (loser).\n                      An optional 'weight' tensor can be used for sample weighting.\n        model_output (dict): Output from the model (not used in this loss structure).\n        extra (dict): A dictionary for hyperparameters, e.g., {'beta': 1.0, 'max_margin': 2.0, 'norm_scale': 10.0}.\n\n    Returns:\n        torch.Tensor: A scalar tensor representing the mean loss.\n    \"\"\"\n    # Read hyperparameters\n    beta = extra.get('beta', 1.0)\n    max_margin = extra.get('max_margin', 2.0)\n    norm_scale = extra.get('norm_scale', 10.0)\n\n    # In our setting, 'a' is the winner (w) and 'b' is the loser (l).\n    # cost_a < cost_b\n    cost_a = batch['cost_a']\n    cost_b = batch['cost_b']\n    logp_a = batch['log_prob_w']\n    logp_b = batch['log_prob_l']\n\n    # 1. Calculate cost and log-probability differences\n    # delta_cost should be positive since cost_b > cost_a\n    delta_cost = cost_b - cost_a\n    delta_logp = logp_a - logp_b\n\n    # 2. Normalize the cost difference using a scaled tanh function\n    # This provides a bounded (-1, 1) and batch-independent normalization\n    norm_delta_cost = torch.tanh(delta_cost / norm_scale)\n\n    # 3. Compute the adaptive sigmoid margin\n    # The margin is a function of the normalized cost difference.\n    margin = max_margin * torch.sigmoid(norm_delta_cost)\n\n    # 4. Compute the core preference loss (Bradley-Terry style with margin)\n    # We want logp_a > logp_b + margin, so (logp_a - logp_b - margin) should be positive.\n    # The logsigmoid of a large positive number is close to 0.\n    core_loss = -F.logsigmoid(delta_logp - margin)\n\n    # 5. Compute a cost-based scaling factor using softplus for stability\n    # This amplifies the loss for pairs with larger raw cost differences.\n    weight_scale = F.softplus(beta * delta_cost)\n\n    # 6. Combine core loss and scaling factor\n    loss_per_sample = weight_scale * core_loss\n\n    # 7. Apply optional sample weights if provided\n    sample_weights = batch.get('weight', None)\n    if sample_weights is not None:\n        loss_per_sample = loss_per_sample * sample_weights\n\n    # 8. Return the mean loss over the batch\n    return loss_per_sample.mean()"}, "fitness": {"hf_like_score": 3.8444610834121704, "validation_objective": 3.8444610834121704, "generalization_penalty": 0.0, "generalization_objectives": {"20": 3.8416337966918945}, "train_score_mean": 3.9618898826307465, "train_loss_mean": 0.42984731498049833, "pair_count": 186409811, "config": {"hf": {"problem": "tsp", "hf_steps": 0, "hf_epochs": 10, "hf_instances_per_epoch": 100000, "train_problem_size": 20, "valid_problem_sizes": [20], "train_batch_size": 64, "pomo_size": 20, "learning_rate": 0.0003, "weight_decay": 1e-06, "alpha": 0.05, "device": "cuda:3", "seed": 1234, "num_validation_episodes": 128, "validation_batch_size": 64, "generalization_penalty_weight": 1.0, "pool_version": "v0"}, "free_loss": {"f1_steps": 0, "total_train_steps": 15630, "f2_steps": 100, "f3_enabled": false}}, "loss_ir": {"name": "Adaptive Sigmoid Margin Loss with Tanh Normalization", "intuition": "This loss function uses a dynamic, cost-sensitive margin to separate preferred and non-preferred solutions. The margin is not fixed but is a sigmoid function of the normalized cost difference. When the cost difference is small, the margin is also small, allowing the model to be uncertain. As the cost difference grows, the margin increases towards a maximum value, demanding a stronger preference signal from the model. Instead of z-score normalization which is batch-dependent, this version uses a scaled tanh function on the cost difference, which provides a bounded, batch-independent normalization. This adaptive margin is combined with a standard logistic loss (logsigmoid) on the log-probability difference. The entire term is then scaled by a softplus function of the cost difference, which amplifies the loss for pairs with larger cost gaps, focusing the training on more significant preference signals.", "hyperparams": {"beta": 1.0, "max_margin": 2.0, "norm_scale": 10.0}, "operators_used": ["logsigmoid", "softplus", "sigmoid", "tanh"]}}, "better_than_baseline": true}
{"generation": 2, "index": 7, "ir": {"name": "Clipped Adaptive Sigmoid Margin Loss", "intuition": "This loss function refines the adaptive margin concept by introducing a clipping mechanism for numerical stability and to prevent extreme values from dominating the batch. The margin is a sigmoid function of the normalized cost difference, making it sensitive to the relative cost improvement within the batch. By clamping the normalized cost difference before applying the sigmoid function, we ensure the margin signal remains within a controlled range, preventing outliers from creating excessively large or small margins. This makes the training process more stable. The core loss remains a logistic objective, scaled by a softplus function of the raw cost difference to emphasize pairs with more significant cost gaps.", "pseudocode": "1. Calculate the cost difference: delta_cost = cost(b) - cost(a).\n2. Normalize the cost difference across the batch using z-score: norm_delta_cost.\n3. Clip the normalized cost difference to a range [-clip_value, clip_value] to prevent extreme values from having an outsized effect.\n4. Compute an adaptive margin based on the clipped, normalized cost difference: margin = max_margin * sigmoid(clipped_norm_delta_cost).\n5. Calculate the log-probability difference: delta_logp = logp(a) - logp(b).\n6. Compute the core preference loss: core_loss = -logsigmoid(delta_logp - margin). This pushes delta_logp to be greater than the adaptive margin.\n7. Calculate a cost-based weight: weight_scale = softplus(beta * delta_cost). This amplifies the loss for pairs with larger cost differences.\n8. Combine them: final_loss = weight_scale * core_loss.\n9. Return the mean of final_loss over the batch.", "hyperparams": {"beta": 1.0, "max_margin": 2.0, "clip_value": 3.0, "eps": 1e-08}, "operators_used": ["logsigmoid", "softplus", "sigmoid", "zscore", "clamp"], "implementation_hint": {"expects": ["cost_a", "cost_b", "logp_a", "logp_b"], "returns": "scalar"}, "code": "import torch\nimport torch.nn.functional as F\n\ndef generated_loss(batch, model_output, extra):\n    \"\"\"\n    Clipped Adaptive Sigmoid Margin Loss.\n\n    Args:\n        batch (dict): A dictionary containing tensors from the data loader.\n                      Expected keys: 'cost_a', 'cost_b', 'log_prob_w', 'log_prob_l'.\n                      'cost_a' and 'log_prob_w' correspond to the preferred solution (winner).\n                      'cost_b' and 'log_prob_l' correspond to the non-preferred solution (loser).\n                      An optional 'weight' tensor can be used for sample weighting.\n        model_output (dict): Output from the model (not used in this loss structure).\n        extra (dict): A dictionary for hyperparameters, e.g., {'beta': 1.0, 'max_margin': 2.0, 'clip_value': 3.0, 'eps': 1e-8}.\n\n    Returns:\n        torch.Tensor: A scalar tensor representing the mean loss.\n    \"\"\"\n    # Read hyperparameters\n    beta = extra.get('beta', 1.0)\n    max_margin = extra.get('max_margin', 2.0)\n    clip_value = extra.get('clip_value', 3.0)\n    eps = extra.get('eps', 1e-8)\n\n    # In our setting, 'a' is the winner (w) and 'b' is the loser (l).\n    # cost_a < cost_b\n    cost_a = batch['cost_a']\n    cost_b = batch['cost_b']\n    logp_a = batch['log_prob_w']\n    logp_b = batch['log_prob_l']\n\n    # 1. Calculate cost and log-probability differences\n    # delta_cost should be positive since cost_b > cost_a\n    delta_cost = cost_b - cost_a\n    delta_logp = logp_a - logp_b\n\n    # 2. Normalize the cost difference (z-score for batch-level adaptation)\n    if delta_cost.numel() > 1 and delta_cost.std() > eps:\n        mean = delta_cost.mean()\n        std = delta_cost.std()\n        norm_delta_cost = (delta_cost - mean) / (std + eps)\n    else:\n        norm_delta_cost = torch.zeros_like(delta_cost)\n\n    # 3. Clip the normalized cost difference for stability\n    clipped_norm_delta_cost = torch.clamp(norm_delta_cost, -clip_value, clip_value)\n\n    # 4. Compute the adaptive sigmoid margin from the clipped value\n    margin = max_margin * torch.sigmoid(clipped_norm_delta_cost)\n\n    # 5. Compute the core preference loss (Bradley-Terry style with margin)\n    core_loss = -F.logsigmoid(delta_logp - margin)\n\n    # 6. Compute a cost-based scaling factor using softplus for stability\n    weight_scale = F.softplus(beta * delta_cost)\n\n    # 7. Combine core loss and scaling factor\n    loss_per_sample = weight_scale * core_loss\n\n    # 8. Apply optional sample weights if provided\n    sample_weights = batch.get('weight', None)\n    if sample_weights is not None:\n        loss_per_sample = loss_per_sample * sample_weights\n\n    # 9. Return the mean loss over the batch\n    return loss_per_sample.mean()"}, "fitness": {"hf_like_score": 3.844765067100525, "validation_objective": 3.844765067100525, "generalization_penalty": 0.0, "generalization_objectives": {"20": 3.840234398841858}, "train_score_mean": 3.9615845243669976, "train_loss_mean": 0.3577503520778487, "pair_count": 187250221, "config": {"hf": {"problem": "tsp", "hf_steps": 0, "hf_epochs": 10, "hf_instances_per_epoch": 100000, "train_problem_size": 20, "valid_problem_sizes": [20], "train_batch_size": 64, "pomo_size": 20, "learning_rate": 0.0003, "weight_decay": 1e-06, "alpha": 0.05, "device": "cuda:0", "seed": 1234, "num_validation_episodes": 128, "validation_batch_size": 64, "generalization_penalty_weight": 1.0, "pool_version": "v0"}, "free_loss": {"f1_steps": 0, "total_train_steps": 15630, "f2_steps": 100, "f3_enabled": false}}, "loss_ir": {"name": "Clipped Adaptive Sigmoid Margin Loss", "intuition": "This loss function refines the adaptive margin concept by introducing a clipping mechanism for numerical stability and to prevent extreme values from dominating the batch. The margin is a sigmoid function of the normalized cost difference, making it sensitive to the relative cost improvement within the batch. By clamping the normalized cost difference before applying the sigmoid function, we ensure the margin signal remains within a controlled range, preventing outliers from creating excessively large or small margins. This makes the training process more stable. The core loss remains a logistic objective, scaled by a softplus function of the raw cost difference to emphasize pairs with more significant cost gaps.", "hyperparams": {"beta": 1.0, "max_margin": 2.0, "clip_value": 3.0, "eps": 1e-08}, "operators_used": ["logsigmoid", "softplus", "sigmoid", "zscore", "clamp"]}}, "better_than_baseline": true}
{"generation": 3, "index": 0, "ir": {"name": "Clipped Tanh Margin Loss with Cost-Gap Gating", "intuition": "This loss function synthesizes the stable, batch-independent normalization from Parent 0 with the outlier-resistant clipping from Parent 1, and introduces a novel cost-gap gating mechanism. \n\nInherited ideas:\n- **Tanh Normalization (from Parent 0):** It uses a scaled `tanh` function to normalize the cost difference (`cost_b - cost_a`). This provides a bounded, batch-independent normalization, making the loss less sensitive to the specific composition of a training batch compared to z-scoring.\n- **Clipping (from Parent 1):** Before computing the margin, the normalized cost difference is clipped. This idea is inherited from Parent 1, but applied to the output of `tanh` rather than z-score. It prevents extreme values from creating excessively large margins, enhancing numerical stability.\n\nNew coupling ideas:\n1. **Cost-Gap Gating:** The core innovation is a gating mechanism that modulates the influence of the log-probability difference (`delta_logp`). This gate is a `sigmoid` function of the raw cost difference. For pairs with a very small cost difference, the gate's output is close to 0.5, effectively shrinking `delta_logp` and reducing the learning signal. This allows the model to be uncertain when the preference is weak. As the cost difference grows, the gate approaches 1, allowing the full `delta_logp` to contribute to the loss. This creates a smoother, more proportional response to the cost gap than the all-or-nothing scaling seen in the parents.\n2. **Unified Scaling:** Instead of using a separate `softplus` weight for the entire loss term, the cost-gap information is integrated directly into the core `logsigmoid` argument via the gating mechanism. This provides a more direct and potentially more stable way to scale the learning signal based on the magnitude of the preference.", "pseudocode": "1. Calculate the cost difference: delta_cost = cost(b) - cost(a).\n2. Calculate the log-probability difference: delta_logp = logp(a) - logp(b).\n3. Normalize the cost difference using a scaled tanh function (inherited from Parent 0): norm_delta_cost = tanh(delta_cost / norm_scale).\n4. Clip the normalized cost difference to prevent extreme values (inspired by Parent 1): clipped_norm_cost = clamp(norm_delta_cost, -clip_value, clip_value).\n5. Compute an adaptive margin based on the clipped, normalized cost: margin = max_margin * sigmoid(clipped_norm_cost).\n6. (New Coupling 1) Create a cost-gap gate: gate = sigmoid(gate_beta * delta_cost). This gate value is between 0.5 and 1, scaling up as the cost difference increases.\n7. (New Coupling 2) Apply the gate to the log-probability difference, modulating its effect: gated_delta_logp = gate * delta_logp.\n8. Compute the final loss using a single logsigmoid term that combines the gated probability difference and the adaptive margin: loss = -logsigmoid(gated_delta_logp - margin).\n9. Return the mean loss over the batch.", "hyperparams": {"max_margin": 1.5, "norm_scale": 10.0, "clip_value": 0.95, "gate_beta": 0.5}, "operators_used": ["logsigmoid", "sigmoid", "tanh", "clamp"], "implementation_hint": {"expects": ["cost_a", "cost_b", "logp_a", "logp_b", "weight"], "returns": "scalar"}, "code": "import torch\nimport torch.nn.functional as F\n\ndef generated_loss(batch, model_output, extra):\n    \"\"\"\n    Clipped Tanh Margin Loss with Cost-Gap Gating.\n\n    Args:\n        batch (dict): A dictionary containing tensors from the data loader.\n                      Expected keys: 'cost_a', 'cost_b', 'log_prob_w', 'log_prob_l'.\n                      'cost_a' and 'log_prob_w' correspond to the preferred solution (winner).\n                      'cost_b' and 'log_prob_l' correspond to the non-preferred solution (loser).\n                      An optional 'weight' tensor can be used for sample weighting.\n        model_output (dict): Output from the model (not used in this loss structure).\n        extra (dict): A dictionary for hyperparameters, e.g., {'max_margin': 1.5, 'norm_scale': 10.0, 'clip_value': 0.95, 'gate_beta': 0.5}.\n\n    Returns:\n        torch.Tensor: A scalar tensor representing the mean loss.\n    \"\"\"\n    # Read hyperparameters\n    max_margin = extra.get('max_margin', 1.5)\n    norm_scale = extra.get('norm_scale', 10.0)\n    clip_value = extra.get('clip_value', 0.95) # Clip the tanh output, so should be <= 1.0\n    gate_beta = extra.get('gate_beta', 0.5)\n\n    # In our setting, 'a' is the winner (w) and 'b' is the loser (l).\n    # cost_a < cost_b\n    cost_a = batch['cost_a']\n    cost_b = batch['cost_b']\n    logp_a = batch['log_prob_w']\n    logp_b = batch['log_prob_l']\n\n    # 1. Calculate cost and log-probability differences\n    delta_cost = cost_b - cost_a\n    delta_logp = logp_a - logp_b\n\n    # 2. Normalize cost difference with tanh (from Parent 0)\n    norm_delta_cost = torch.tanh(delta_cost / norm_scale)\n\n    # 3. Clip the normalized cost for stability (inspired by Parent 1)\n    # Since tanh is bounded by [-1, 1], clip_value should be <= 1.\n    clipped_norm_cost = torch.clamp(norm_delta_cost, -clip_value, clip_value)\n\n    # 4. Compute the adaptive margin from the clipped, normalized cost\n    margin = max_margin * torch.sigmoid(clipped_norm_cost)\n\n    # 5. (New Coupling) Create a cost-gap gate to modulate the logp difference\n    # For delta_cost -> 0, gate -> 0.5. For delta_cost -> inf, gate -> 1.\n    # This smoothly scales the importance of delta_logp based on the cost gap.\n    gate = torch.sigmoid(gate_beta * delta_cost) \n    gated_delta_logp = gate * delta_logp\n\n    # 6. Compute the final loss\n    # The objective is to make (gated_delta_logp - margin) large and positive.\n    loss_per_sample = -F.logsigmoid(gated_delta_logp - margin)\n\n    # 7. Apply optional sample weights if provided\n    sample_weights = batch.get('weight', None)\n    if sample_weights is not None:\n        loss_per_sample = loss_per_sample * sample_weights\n\n    # 8. Return the mean loss over the batch\n    return loss_per_sample.mean()"}, "fitness": {"hf_like_score": 3.848331570625305, "validation_objective": 3.848331570625305, "generalization_penalty": 0.0, "generalization_objectives": {"20": 3.843472719192505}, "train_score_mean": 3.841585176630197, "train_loss_mean": 0.9658640917607476, "pair_count": 163681948, "config": {"hf": {"problem": "tsp", "hf_steps": 0, "hf_epochs": 10, "hf_instances_per_epoch": 100000, "train_problem_size": 20, "valid_problem_sizes": [20], "train_batch_size": 64, "pomo_size": 20, "learning_rate": 0.0003, "weight_decay": 1e-06, "alpha": 0.05, "device": "cuda:0", "seed": 1234, "num_validation_episodes": 128, "validation_batch_size": 64, "generalization_penalty_weight": 1.0, "pool_version": "v0"}, "free_loss": {"f1_steps": 0, "total_train_steps": 15630, "f2_steps": 100, "f3_enabled": false}}, "loss_ir": {"name": "Clipped Tanh Margin Loss with Cost-Gap Gating", "intuition": "This loss function synthesizes the stable, batch-independent normalization from Parent 0 with the outlier-resistant clipping from Parent 1, and introduces a novel cost-gap gating mechanism. \n\nInherited ideas:\n- **Tanh Normalization (from Parent 0):** It uses a scaled `tanh` function to normalize the cost difference (`cost_b - cost_a`). This provides a bounded, batch-independent normalization, making the loss less sensitive to the specific composition of a training batch compared to z-scoring.\n- **Clipping (from Parent 1):** Before computing the margin, the normalized cost difference is clipped. This idea is inherited from Parent 1, but applied to the output of `tanh` rather than z-score. It prevents extreme values from creating excessively large margins, enhancing numerical stability.\n\nNew coupling ideas:\n1. **Cost-Gap Gating:** The core innovation is a gating mechanism that modulates the influence of the log-probability difference (`delta_logp`). This gate is a `sigmoid` function of the raw cost difference. For pairs with a very small cost difference, the gate's output is close to 0.5, effectively shrinking `delta_logp` and reducing the learning signal. This allows the model to be uncertain when the preference is weak. As the cost difference grows, the gate approaches 1, allowing the full `delta_logp` to contribute to the loss. This creates a smoother, more proportional response to the cost gap than the all-or-nothing scaling seen in the parents.\n2. **Unified Scaling:** Instead of using a separate `softplus` weight for the entire loss term, the cost-gap information is integrated directly into the core `logsigmoid` argument via the gating mechanism. This provides a more direct and potentially more stable way to scale the learning signal based on the magnitude of the preference.", "hyperparams": {"max_margin": 1.5, "norm_scale": 10.0, "clip_value": 0.95, "gate_beta": 0.5}, "operators_used": ["logsigmoid", "sigmoid", "tanh", "clamp"]}}, "better_than_baseline": true}
{"generation": 3, "index": 2, "ir": {"name": "Clipped Tanh-Normalized Adaptive Margin Loss", "intuition": "This loss function combines the stability of tanh normalization with a clipping mechanism to create a robust, adaptive margin for preference learning. The core objective is a logistic loss encouraging the model's log-probability difference to exceed a dynamic margin.\n\nInherited ideas:\n- From 'Adaptive Sigmoid Margin Loss with Tanh Normalization' (Parent 0), it inherits the use of a scaled `tanh` function to normalize the cost difference. This provides a bounded, batch-independent measure of the cost gap, which is inherently more stable than batch-dependent z-scoring.\n- From 'Clipped Adaptive Sigmoid Margin Loss' (Parent 1), it inherits the idea of `clamping` the normalized value before it is used to compute the margin. This prevents extreme cost differences from creating overly aggressive margin targets, thereby stabilizing training.\n- Both parents contribute the general structure: a logistic loss (`-logsigmoid`) with a cost-sensitive margin, and a `softplus` scaling of the final loss to emphasize pairs with larger raw cost differences.\n\nNew coupling ideas:\n1.  **Dual-Stage Normalization (Tanh + Clamp):** The child loss couples the `tanh` normalization from Parent 0 with the `clamp` operator from Parent 1. While `tanh` already bounds its output to (-1, 1), applying a clamp (e.g., `clamp(-0.9, 0.9)`) before the sigmoid function provides a finer-grained control over the margin's sensitivity. It ensures that the effective margin never quite reaches its theoretical maximum or minimum, preventing the sigmoid from operating in its saturated regions and thus maintaining a healthier gradient flow.\n2.  **Adaptive Beta Scaling:** The `beta` hyperparameter, which scales the final loss weighting, is made adaptive. It is scaled by the sigmoid of the clipped, normalized cost difference. This means that for pairs with a very small normalized cost gap, the `beta` scaling is attenuated, reducing the loss contribution from ambiguous or noisy preference pairs. Conversely, for pairs with a clear and significant normalized cost gap, the `beta` scaling is stronger, focusing the model's attention on confident preference signals.", "pseudocode": "1. Calculate the cost difference: delta_cost = cost(b) - cost(a).\n2. Normalize the cost difference using a scaled tanh function for bounded, batch-independent normalization: tanh_norm_delta_cost = tanh(delta_cost / norm_scale).\n3. Clip the tanh-normalized cost difference to a range [-clip_value, clip_value] to control the margin's sensitivity and prevent saturation: clipped_norm_cost = clamp(tanh_norm_delta_cost, -clip_value, clip_value).\n4. Compute an adaptive margin based on the clipped, normalized cost: margin = max_margin * sigmoid(clipped_norm_cost).\n5. Calculate the log-probability difference: delta_logp = logp(a) - logp(b).\n6. Compute the core preference loss: core_loss = -logsigmoid(delta_logp - margin).\n7. Compute an adaptive beta scaling factor: adaptive_beta = beta * sigmoid(clipped_norm_cost). This attenuates the loss weight for pairs with small normalized cost gaps.\n8. Calculate the final cost-based weight: weight_scale = softplus(adaptive_beta * delta_cost).\n9. Combine them: final_loss = weight_scale * core_loss.\n10. Return the mean of final_loss over the batch.", "hyperparams": {"beta": 1.0, "max_margin": 2.0, "norm_scale": 10.0, "clip_value": 0.9}, "operators_used": ["logsigmoid", "softplus", "sigmoid", "tanh", "clamp"], "implementation_hint": {"expects": ["cost_a", "cost_b", "logp_a", "logp_b"], "returns": "scalar"}, "code": "import torch\nimport torch.nn.functional as F\n\ndef generated_loss(batch, model_output, extra):\n    \"\"\"\n    Clipped Tanh-Normalized Adaptive Margin Loss.\n\n    Args:\n        batch (dict): A dictionary containing tensors from the data loader.\n                      Expected keys: 'cost_a', 'cost_b', 'log_prob_w', 'log_prob_l'.\n                      'cost_a' and 'log_prob_w' correspond to the preferred solution (winner).\n                      'cost_b' and 'log_prob_l' correspond to the non-preferred solution (loser).\n                      An optional 'weight' tensor can be used for sample weighting.\n        model_output (dict): Output from the model (not used in this loss structure).\n        extra (dict): A dictionary for hyperparameters, e.g., {'beta': 1.0, 'max_margin': 2.0, 'norm_scale': 10.0, 'clip_value': 0.9}.\n\n    Returns:\n        torch.Tensor: A scalar tensor representing the mean loss.\n    \"\"\"\n    # Read hyperparameters\n    beta = extra.get('beta', 1.0)\n    max_margin = extra.get('max_margin', 2.0)\n    norm_scale = extra.get('norm_scale', 10.0)\n    clip_value = extra.get('clip_value', 0.9)\n\n    # In our setting, 'a' is the winner (w) and 'b' is the loser (l).\n    # cost_a < cost_b\n    cost_a = batch['cost_a']\n    cost_b = batch['cost_b']\n    logp_a = batch['log_prob_w']\n    logp_b = batch['log_prob_l']\n\n    # 1. Calculate cost and log-probability differences\n    # delta_cost should be positive since cost_b > cost_a\n    delta_cost = cost_b - cost_a\n    delta_logp = logp_a - logp_b\n\n    # 2. Inherited Idea (Parent 0): Normalize cost difference using tanh for stability.\n    tanh_norm_delta_cost = torch.tanh(delta_cost / norm_scale)\n\n    # 3. New Coupling Idea (1): Couple tanh normalization with clamping (from Parent 1) to control margin sensitivity.\n    # Ensures the input to sigmoid is not in saturated regions.\n    clipped_norm_cost = torch.clamp(tanh_norm_delta_cost, -clip_value, clip_value)\n\n    # 4. Compute the adaptive sigmoid margin from the clipped, normalized value.\n    margin = max_margin * torch.sigmoid(clipped_norm_cost)\n\n    # 5. Compute the core preference loss (Bradley-Terry style with margin).\n    core_loss = -F.logsigmoid(delta_logp - margin)\n\n    # 6. New Coupling Idea (2): Create an adaptive beta scaling factor.\n    # This reduces the loss weight for pairs with small normalized cost gaps.\n    adaptive_beta = beta * torch.sigmoid(clipped_norm_cost)\n\n    # 7. Compute a cost-based scaling factor using softplus for stability.\n    # This now uses the adaptive beta.\n    weight_scale = F.softplus(adaptive_beta * delta_cost)\n\n    # 8. Combine core loss and scaling factor.\n    loss_per_sample = weight_scale * core_loss\n\n    # 9. Apply optional sample weights if provided.\n    sample_weights = batch.get('weight', None)\n    if sample_weights is not None:\n        loss_per_sample = loss_per_sample * sample_weights\n\n    # 10. Return the mean loss over the batch.\n    return loss_per_sample.mean()"}, "fitness": {"hf_like_score": 3.8459575176239014, "validation_objective": 3.8459575176239014, "generalization_penalty": 0.0, "generalization_objectives": {"20": 3.841825246810913}, "train_score_mean": 3.9975821547987214, "train_loss_mean": 0.41271566195855597, "pair_count": 186447271, "config": {"hf": {"problem": "tsp", "hf_steps": 0, "hf_epochs": 10, "hf_instances_per_epoch": 100000, "train_problem_size": 20, "valid_problem_sizes": [20], "train_batch_size": 64, "pomo_size": 20, "learning_rate": 0.0003, "weight_decay": 1e-06, "alpha": 0.05, "device": "cuda:1", "seed": 1234, "num_validation_episodes": 128, "validation_batch_size": 64, "generalization_penalty_weight": 1.0, "pool_version": "v0"}, "free_loss": {"f1_steps": 0, "total_train_steps": 15630, "f2_steps": 100, "f3_enabled": false}}, "loss_ir": {"name": "Clipped Tanh-Normalized Adaptive Margin Loss", "intuition": "This loss function combines the stability of tanh normalization with a clipping mechanism to create a robust, adaptive margin for preference learning. The core objective is a logistic loss encouraging the model's log-probability difference to exceed a dynamic margin.\n\nInherited ideas:\n- From 'Adaptive Sigmoid Margin Loss with Tanh Normalization' (Parent 0), it inherits the use of a scaled `tanh` function to normalize the cost difference. This provides a bounded, batch-independent measure of the cost gap, which is inherently more stable than batch-dependent z-scoring.\n- From 'Clipped Adaptive Sigmoid Margin Loss' (Parent 1), it inherits the idea of `clamping` the normalized value before it is used to compute the margin. This prevents extreme cost differences from creating overly aggressive margin targets, thereby stabilizing training.\n- Both parents contribute the general structure: a logistic loss (`-logsigmoid`) with a cost-sensitive margin, and a `softplus` scaling of the final loss to emphasize pairs with larger raw cost differences.\n\nNew coupling ideas:\n1.  **Dual-Stage Normalization (Tanh + Clamp):** The child loss couples the `tanh` normalization from Parent 0 with the `clamp` operator from Parent 1. While `tanh` already bounds its output to (-1, 1), applying a clamp (e.g., `clamp(-0.9, 0.9)`) before the sigmoid function provides a finer-grained control over the margin's sensitivity. It ensures that the effective margin never quite reaches its theoretical maximum or minimum, preventing the sigmoid from operating in its saturated regions and thus maintaining a healthier gradient flow.\n2.  **Adaptive Beta Scaling:** The `beta` hyperparameter, which scales the final loss weighting, is made adaptive. It is scaled by the sigmoid of the clipped, normalized cost difference. This means that for pairs with a very small normalized cost gap, the `beta` scaling is attenuated, reducing the loss contribution from ambiguous or noisy preference pairs. Conversely, for pairs with a clear and significant normalized cost gap, the `beta` scaling is stronger, focusing the model's attention on confident preference signals.", "hyperparams": {"beta": 1.0, "max_margin": 2.0, "norm_scale": 10.0, "clip_value": 0.9}, "operators_used": ["logsigmoid", "softplus", "sigmoid", "tanh", "clamp"]}}, "better_than_baseline": true}
{"generation": 3, "index": 5, "ir": {"name": "Clipped Tanh-Normalized Adaptive Margin Loss", "intuition": "This loss function creates a robust, cost-sensitive preference objective by merging ideas from its parents and introducing a new coupling mechanism. \n\nInherited Ideas:\n- From Parent 0, it inherits the use of a scaled `tanh` function for batch-independent normalization of the cost difference. This provides a bounded and stable signal of the cost gap's magnitude.\n- From Parent 1, it inherits the idea of clipping the normalized signal (`clamp`) before it's used to compute the margin. This prevents extreme cost differences from creating overly dominant margin values, enhancing training stability.\n- Both parents contribute the core structure: a logistic loss (`-logsigmoid`) on the log-probability difference, an adaptive margin that is a `sigmoid` function of the (now clipped and tanh-normalized) cost difference, and a `softplus` weighting based on the raw cost difference to focus on more significant preference pairs.\n\nNew Coupling Idea:\n- The new idea is to couple the clipping range with the `tanh` normalization scale. The `clamp` bounds are set to `[-1.0, 1.0]`, which is the natural output range of the `tanh` function. This creates a more principled interaction: the `tanh` function smoothly squashes the cost difference into this range, and the `clamp` acts as a hard guard at the boundaries, ensuring perfect numerical stability and preventing any values from exceeding the theoretical `tanh` limits due to floating-point inaccuracies. This tight coupling makes the `clip_value` hyperparameter unnecessary, simplifying the loss design.", "pseudocode": "1. Calculate the cost difference: delta_cost = cost(b) - cost(a).\n2. Normalize the cost difference using a scaled tanh function: norm_delta_cost = tanh(delta_cost / norm_scale). This is inherited from Parent 0 for batch-independent normalization.\n3. Clip the normalized cost difference to the range [-1.0, 1.0]. This is inherited from Parent 1, but the range is now coupled with the output range of the tanh function, which is a new modification.\n4. Compute an adaptive margin: margin = max_margin * sigmoid(clipped_norm_delta_cost). This combines the sigmoid margin from both parents with the new clipped signal.\n5. Calculate the log-probability difference: delta_logp = logp(a) - logp(b).\n6. Compute the core preference loss: core_loss = -logsigmoid(delta_logp - margin). This is the standard logistic loss structure from both parents.\n7. Calculate a cost-based weight: weight_scale = softplus(beta * delta_cost). This weighting scheme is also inherited from both parents.\n8. Combine them: final_loss = weight_scale * core_loss.\n9. Return the mean of final_loss over the batch.", "hyperparams": {"beta": 1.0, "max_margin": 2.0, "norm_scale": 10.0}, "operators_used": ["logsigmoid", "softplus", "sigmoid", "tanh", "clamp"], "implementation_hint": {"expects": ["cost_a", "cost_b", "logp_a", "logp_b"], "returns": "scalar"}, "code": "import torch\nimport torch.nn.functional as F\n\ndef generated_loss(batch, model_output, extra):\n    \"\"\"\n    Clipped Tanh-Normalized Adaptive Margin Loss.\n\n    Args:\n        batch (dict): A dictionary containing tensors from the data loader.\n                      Expected keys: 'cost_a', 'cost_b', 'log_prob_w', 'log_prob_l'.\n                      'cost_a' and 'log_prob_w' correspond to the preferred solution (winner).\n                      'cost_b' and 'log_prob_l' correspond to the non-preferred solution (loser).\n                      An optional 'weight' tensor can be used for sample weighting.\n        model_output (dict): Output from the model (not used in this loss structure).\n        extra (dict): A dictionary for hyperparameters, e.g., {'beta': 1.0, 'max_margin': 2.0, 'norm_scale': 10.0}.\n\n    Returns:\n        torch.Tensor: A scalar tensor representing the mean loss.\n    \"\"\"\n    # Read hyperparameters\n    beta = extra.get('beta', 1.0)\n    max_margin = extra.get('max_margin', 2.0)\n    norm_scale = extra.get('norm_scale', 10.0)\n\n    # In our setting, 'a' is the winner (w) and 'b' is the loser (l).\n    # cost_a < cost_b\n    cost_a = batch['cost_a']\n    cost_b = batch['cost_b']\n    logp_a = batch['log_prob_w']\n    logp_b = batch['log_prob_l']\n\n    # 1. Calculate cost and log-probability differences\n    # delta_cost should be positive since cost_b > cost_a\n    delta_cost = cost_b - cost_a\n    delta_logp = logp_a - logp_b\n\n    # 2. Normalize the cost difference using a scaled tanh function (from Parent 0)\n    # This provides a bounded (-1, 1) and batch-independent normalization\n    norm_delta_cost = torch.tanh(delta_cost / norm_scale)\n\n    # 3. Clip the normalized cost difference for stability (from Parent 1)\n    # NEW COUPLING: The clip range is coupled with the tanh output range.\n    clipped_norm_delta_cost = torch.clamp(norm_delta_cost, -1.0, 1.0)\n\n    # 4. Compute the adaptive sigmoid margin from the clipped, normalized value (from both parents)\n    margin = max_margin * torch.sigmoid(clipped_norm_delta_cost)\n\n    # 5. Compute the core preference loss (from both parents)\n    core_loss = -F.logsigmoid(delta_logp - margin)\n\n    # 6. Compute a cost-based scaling factor using softplus (from both parents)\n    weight_scale = F.softplus(beta * delta_cost)\n\n    # 7. Combine core loss and scaling factor\n    loss_per_sample = weight_scale * core_loss\n\n    # 8. Apply optional sample weights if provided\n    sample_weights = batch.get('weight', None)\n    if sample_weights is not None:\n        loss_per_sample = loss_per_sample * sample_weights\n\n    # 9. Return the mean loss over the batch\n    return loss_per_sample.mean()"}, "fitness": {"hf_like_score": 3.8464789390563965, "validation_objective": 3.8464789390563965, "generalization_penalty": 0.0, "generalization_objectives": {"20": 3.843708038330078}, "train_score_mean": 3.9629660384592458, "train_loss_mean": 0.42892209017078464, "pair_count": 186400651, "config": {"hf": {"problem": "tsp", "hf_steps": 0, "hf_epochs": 10, "hf_instances_per_epoch": 100000, "train_problem_size": 20, "valid_problem_sizes": [20], "train_batch_size": 64, "pomo_size": 20, "learning_rate": 0.0003, "weight_decay": 1e-06, "alpha": 0.05, "device": "cuda:2", "seed": 1234, "num_validation_episodes": 128, "validation_batch_size": 64, "generalization_penalty_weight": 1.0, "pool_version": "v0"}, "free_loss": {"f1_steps": 0, "total_train_steps": 15630, "f2_steps": 100, "f3_enabled": false}}, "loss_ir": {"name": "Clipped Tanh-Normalized Adaptive Margin Loss", "intuition": "This loss function creates a robust, cost-sensitive preference objective by merging ideas from its parents and introducing a new coupling mechanism. \n\nInherited Ideas:\n- From Parent 0, it inherits the use of a scaled `tanh` function for batch-independent normalization of the cost difference. This provides a bounded and stable signal of the cost gap's magnitude.\n- From Parent 1, it inherits the idea of clipping the normalized signal (`clamp`) before it's used to compute the margin. This prevents extreme cost differences from creating overly dominant margin values, enhancing training stability.\n- Both parents contribute the core structure: a logistic loss (`-logsigmoid`) on the log-probability difference, an adaptive margin that is a `sigmoid` function of the (now clipped and tanh-normalized) cost difference, and a `softplus` weighting based on the raw cost difference to focus on more significant preference pairs.\n\nNew Coupling Idea:\n- The new idea is to couple the clipping range with the `tanh` normalization scale. The `clamp` bounds are set to `[-1.0, 1.0]`, which is the natural output range of the `tanh` function. This creates a more principled interaction: the `tanh` function smoothly squashes the cost difference into this range, and the `clamp` acts as a hard guard at the boundaries, ensuring perfect numerical stability and preventing any values from exceeding the theoretical `tanh` limits due to floating-point inaccuracies. This tight coupling makes the `clip_value` hyperparameter unnecessary, simplifying the loss design.", "hyperparams": {"beta": 1.0, "max_margin": 2.0, "norm_scale": 10.0}, "operators_used": ["logsigmoid", "softplus", "sigmoid", "tanh", "clamp"]}}, "better_than_baseline": true}
{"generation": 3, "index": 6, "ir": {"name": "Clipped Tanh-Normalized Adaptive Margin Loss", "intuition": "This loss function combines the stable, batch-independent normalization from the 'Adaptive Sigmoid Margin Loss with Tanh Normalization' (Parent 0) with the robustness-enhancing clipping mechanism from the 'Clipped Adaptive Sigmoid Margin Loss' (Parent 1). Specifically, it inherits the use of a scaled `tanh` function to normalize the cost difference, which provides a bounded and predictable normalization that is not dependent on batch statistics. From Parent 1, it inherits the `clamp` operator, which is applied to the output of the tanh function before it is used to compute the margin. This acts as a secondary stability control, ensuring the input to the sigmoid function is well-behaved, even if the tanh scaling is miscalibrated. The core loss remains a logistic objective (`-logsigmoid`) where the model's log-probability difference is pushed to exceed this adaptive margin, and the entire loss term is scaled by a `softplus` of the raw cost difference, an idea common to both parents.\n\nAs a new coupling idea, this child loss introduces a dynamic `beta` schedule based on the tanh-normalized cost difference. The `beta` parameter, which scales the importance of the loss based on the raw cost gap, is now modulated by `tanh(delta_cost)`. This means that for pairs with very small cost differences, the `beta` scaling is reduced, preventing the model from being over-penalized on noisy or insignificant preferences. As the cost difference grows, `beta`'s effect ramps up towards its maximum configured value, smoothly focusing the training on more meaningful preference pairs. This creates a more nuanced weighting scheme than the fixed `beta` used by the parents.", "pseudocode": "1. Calculate the cost difference: delta_cost = cost(b) - cost(a).\n2. Normalize the cost difference using a scaled tanh function: norm_delta_cost = tanh(delta_cost / norm_scale). This is inherited from Parent 0.\n3. Clip the normalized cost difference to a range [-clip_value, clip_value] for added stability. This idea is inherited from Parent 1.\n4. Compute an adaptive margin based on the clipped, normalized value: margin = max_margin * sigmoid(clipped_norm_delta_cost).\n5. Calculate the log-probability difference: delta_logp = logp(a) - logp(b).\n6. Compute the core preference loss: core_loss = -logsigmoid(delta_logp - margin).\n7. (New Coupling) Compute a dynamic beta schedule: dynamic_beta = base_beta * norm_delta_cost. This reduces the weight for pairs with small cost differences.\n8. Calculate a cost-based weight using the dynamic beta: weight_scale = softplus(dynamic_beta * delta_cost). This is a modification of the weighting scheme from both parents.\n9. Combine them: final_loss = weight_scale * core_loss.\n10. Return the mean of final_loss over the batch.", "hyperparams": {"base_beta": 1.0, "max_margin": 2.0, "norm_scale": 10.0, "clip_value": 0.99}, "operators_used": ["logsigmoid", "softplus", "sigmoid", "tanh", "clamp"], "implementation_hint": {"expects": ["cost_a", "cost_b", "log_prob_w", "log_prob_l", "weight"], "returns": "scalar"}, "code": "import torch\nimport torch.nn.functional as F\n\ndef generated_loss(batch, model_output, extra):\n    \"\"\"\n    Clipped Tanh-Normalized Adaptive Margin Loss.\n\n    Args:\n        batch (dict): A dictionary containing tensors from the data loader.\n                      Expected keys: 'cost_a', 'cost_b', 'log_prob_w', 'log_prob_l'.\n                      'cost_a' and 'log_prob_w' correspond to the preferred solution (winner).\n                      'cost_b' and 'log_prob_l' correspond to the non-preferred solution (loser).\n                      An optional 'weight' tensor can be used for sample weighting.\n        model_output (dict): Output from the model (not used in this loss structure).\n        extra (dict): A dictionary for hyperparameters, e.g., {'base_beta': 1.0, 'max_margin': 2.0, 'norm_scale': 10.0, 'clip_value': 0.99}.\n\n    Returns:\n        torch.Tensor: A scalar tensor representing the mean loss.\n    \"\"\"\n    # Read hyperparameters\n    base_beta = extra.get('base_beta', 1.0)\n    max_margin = extra.get('max_margin', 2.0)\n    norm_scale = extra.get('norm_scale', 10.0)\n    clip_value = extra.get('clip_value', 0.99) # Clip inside tanh's range\n\n    # In our setting, 'a' is the winner (w) and 'b' is the loser (l).\n    # cost_a < cost_b\n    cost_a = batch['cost_a']\n    cost_b = batch['cost_b']\n    logp_a = batch['log_prob_w']\n    logp_b = batch['log_prob_l']\n\n    # 1. Calculate cost and log-probability differences\n    delta_cost = cost_b - cost_a\n    delta_logp = logp_a - logp_b\n\n    # 2. Normalize cost difference with tanh (inherited from Parent 0)\n    norm_delta_cost = torch.tanh(delta_cost / norm_scale)\n\n    # 3. Clip the normalized cost for stability (inherited from Parent 1)\n    # The clip_value should be <= 1 to be effective with tanh output.\n    clipped_norm_delta_cost = torch.clamp(norm_delta_cost, -clip_value, clip_value)\n\n    # 4. Compute the adaptive sigmoid margin\n    margin = max_margin * torch.sigmoid(clipped_norm_delta_cost)\n\n    # 5. Compute the core preference loss (Bradley-Terry style with margin)\n    core_loss = -F.logsigmoid(delta_logp - margin)\n\n    # 6. (New Coupling) Compute a dynamic beta, modulated by the normalized cost difference.\n    # This reduces the weight for pairs with very small cost differences, making beta's effect smoother.\n    dynamic_beta = base_beta * norm_delta_cost.abs() # Use abs to ensure beta is positive\n\n    # 7. Compute a cost-based scaling factor using dynamic beta and softplus\n    weight_scale = F.softplus(dynamic_beta * delta_cost)\n\n    # 8. Combine core loss and scaling factor\n    loss_per_sample = weight_scale * core_loss\n\n    # 9. Apply optional sample weights if provided\n    sample_weights = batch.get('weight', None)\n    if sample_weights is not None:\n        loss_per_sample = loss_per_sample * sample_weights\n\n    # 10. Return the mean loss over the batch\n    return loss_per_sample.mean()"}, "fitness": {"hf_like_score": 3.8443875312805176, "validation_objective": 3.8443875312805176, "generalization_penalty": 0.0, "generalization_objectives": {"20": 3.8414416313171387}, "train_score_mean": 4.079995856129505, "train_loss_mean": 0.3759007084878759, "pair_count": 188522405, "config": {"hf": {"problem": "tsp", "hf_steps": 0, "hf_epochs": 10, "hf_instances_per_epoch": 100000, "train_problem_size": 20, "valid_problem_sizes": [20], "train_batch_size": 64, "pomo_size": 20, "learning_rate": 0.0003, "weight_decay": 1e-06, "alpha": 0.05, "device": "cuda:3", "seed": 1234, "num_validation_episodes": 128, "validation_batch_size": 64, "generalization_penalty_weight": 1.0, "pool_version": "v0"}, "free_loss": {"f1_steps": 0, "total_train_steps": 15630, "f2_steps": 100, "f3_enabled": false}}, "loss_ir": {"name": "Clipped Tanh-Normalized Adaptive Margin Loss", "intuition": "This loss function combines the stable, batch-independent normalization from the 'Adaptive Sigmoid Margin Loss with Tanh Normalization' (Parent 0) with the robustness-enhancing clipping mechanism from the 'Clipped Adaptive Sigmoid Margin Loss' (Parent 1). Specifically, it inherits the use of a scaled `tanh` function to normalize the cost difference, which provides a bounded and predictable normalization that is not dependent on batch statistics. From Parent 1, it inherits the `clamp` operator, which is applied to the output of the tanh function before it is used to compute the margin. This acts as a secondary stability control, ensuring the input to the sigmoid function is well-behaved, even if the tanh scaling is miscalibrated. The core loss remains a logistic objective (`-logsigmoid`) where the model's log-probability difference is pushed to exceed this adaptive margin, and the entire loss term is scaled by a `softplus` of the raw cost difference, an idea common to both parents.\n\nAs a new coupling idea, this child loss introduces a dynamic `beta` schedule based on the tanh-normalized cost difference. The `beta` parameter, which scales the importance of the loss based on the raw cost gap, is now modulated by `tanh(delta_cost)`. This means that for pairs with very small cost differences, the `beta` scaling is reduced, preventing the model from being over-penalized on noisy or insignificant preferences. As the cost difference grows, `beta`'s effect ramps up towards its maximum configured value, smoothly focusing the training on more meaningful preference pairs. This creates a more nuanced weighting scheme than the fixed `beta` used by the parents.", "hyperparams": {"base_beta": 1.0, "max_margin": 2.0, "norm_scale": 10.0, "clip_value": 0.99}, "operators_used": ["logsigmoid", "softplus", "sigmoid", "tanh", "clamp"]}}, "better_than_baseline": true}
{"generation": 5, "index": 1, "ir": {"name": "Adaptive Logit-Scaled Margin Loss with Tanh Normalization", "intuition": "This loss function aims to create a more direct and interpretable link between the cost difference and the learning signal. It inherits two core ideas from its parents: 1) The use of a scaled `tanh` function to normalize the cost difference, providing a stable, batch-independent measure of preference strength (from Parent 1, 'Adaptive Sigmoid Margin Loss with Tanh Normalization'). 2) The overall structure of a cost-weighted logistic loss, where `softplus(beta * delta_cost)` scales the core loss term, focusing training on pairs with larger cost gaps (common to both parents).\n\nThe first new coupling idea is the margin calculation. Instead of using a sigmoid or another bounded function, the margin is computed by applying an inverse sigmoid (logit) transformation to the tanh-normalized cost difference. Specifically, `margin = log(norm_delta_cost / (1 - norm_delta_cost))`. This creates a margin that grows much more rapidly and becomes unbounded as the normalized cost difference approaches 1. The intuition is that for very confident preferences (large cost gap), the model should be pushed to create an extremely large log-probability separation, rather than just exceeding a bounded margin. A `clamp` is used on the input to the `log` to ensure numerical stability when `norm_delta_cost` is close to 0 or 1.\n\nThe second new coupling idea is a dynamic `beta` schedule that is inversely related to the log-probability difference. We calculate `dynamic_beta = base_beta * exp(-gamma * |delta_logp|)`. When the model is already very confident about a preference pair (large `|delta_logp|`), `dynamic_beta` is down-weighted. Conversely, when the model is uncertain (small `|delta_logp|`), `dynamic_beta` is larger. This focuses the `softplus` weighting mechanism on the examples where the model's predictions are still weak or incorrect, preventing the loss from being dominated by already well-learned examples that have large cost differences.", "pseudocode": "1. Calculate the cost difference: delta_cost = cost(b) - cost(a).\n2. Calculate the log-probability difference: delta_logp = logp(a) - logp(b).\n3. (Inherited from Parent 1) Normalize the cost difference using a scaled tanh function: norm_delta_cost = tanh(delta_cost / norm_scale).\n4. (New Coupling 1) Compute an adaptive margin using an inverse sigmoid (logit-like) transformation. First, clamp the normalized cost to avoid log(0) or division by zero: clamped_norm = clamp(norm_delta_cost, eps, 1-eps). Then, compute margin = log(clamped_norm / (1 - clamped_norm)). This creates a margin that grows rapidly as the cost gap increases.\n5. Compute the core preference loss: core_loss = -logsigmoid(delta_logp - margin).\n6. (New Coupling 2) Compute a dynamic beta that decreases as the model's confidence (`|delta_logp|`) increases: dynamic_beta = base_beta * exp(-gamma * abs(delta_logp)).\n7. (Inherited from both parents) Calculate a cost-based weight using the dynamic beta: weight_scale = softplus(dynamic_beta * delta_cost).\n8. Combine them: final_loss = weight_scale * core_loss.\n9. Return the mean of final_loss over the batch.", "hyperparams": {"base_beta": 1.0, "norm_scale": 10.0, "gamma": 0.5, "eps": 1e-08}, "operators_used": ["logsigmoid", "softplus", "tanh", "clamp", "log", "exp"], "implementation_hint": {"expects": ["cost_a", "cost_b", "logp_a", "logp_b"], "returns": "scalar"}, "code": "import torch\nimport torch.nn.functional as F\n\ndef generated_loss(batch, model_output, extra):\n    \"\"\"\n    Adaptive Logit-Scaled Margin Loss with Tanh Normalization.\n\n    Args:\n        batch (dict): A dictionary containing tensors from the data loader.\n                      Expected keys: 'cost_a', 'cost_b', 'log_prob_w', 'log_prob_l'.\n                      'cost_a' and 'log_prob_w' correspond to the preferred solution (winner).\n                      'cost_b' and 'log_prob_l' correspond to the non-preferred solution (loser).\n                      An optional 'weight' tensor can be used for sample weighting.\n        model_output (dict): Output from the model (not used in this loss structure).\n        extra (dict): A dictionary for hyperparameters, e.g., {'base_beta': 1.0, 'norm_scale': 10.0, 'gamma': 0.5, 'eps': 1e-8}.\n\n    Returns:\n        torch.Tensor: A scalar tensor representing the mean loss.\n    \"\"\"\n    # Read hyperparameters\n    base_beta = extra.get('base_beta', 1.0)\n    norm_scale = extra.get('norm_scale', 10.0)\n    gamma = extra.get('gamma', 0.5)\n    eps = extra.get('eps', 1e-8)\n\n    # In our setting, 'a' is the winner (w) and 'b' is the loser (l).\n    # cost_a < cost_b\n    cost_a = batch['cost_a']\n    cost_b = batch['cost_b']\n    logp_a = batch['log_prob_w']\n    logp_b = batch['log_prob_l']\n\n    # 1. Calculate cost and log-probability differences\n    delta_cost = cost_b - cost_a\n    delta_logp = logp_a - logp_b\n\n    # 2. Normalize cost difference with tanh (inherited from Parent 1)\n    # This maps the cost difference to (0, 1) since delta_cost > 0\n    norm_delta_cost = torch.tanh(delta_cost / norm_scale)\n\n    # 3. (New Coupling 1) Compute an adaptive margin using a logit-like transformation.\n    # Clamp is crucial for numerical stability near 0 and 1.\n    clamped_norm = torch.clamp(norm_delta_cost, min=eps, max=1.0 - eps)\n    margin = torch.log(clamped_norm / (1.0 - clamped_norm))\n\n    # 4. Compute the core preference loss (Bradley-Terry style with margin)\n    core_loss = -F.logsigmoid(delta_logp - margin)\n\n    # 5. (New Coupling 2) Compute a dynamic beta based on model confidence.\n    # This down-weights examples where the model is already confident.\n    dynamic_beta = base_beta * torch.exp(-gamma * torch.abs(delta_logp.detach()))\n\n    # 6. Compute a cost-based scaling factor using dynamic beta and softplus (inherited from both parents)\n    weight_scale = F.softplus(dynamic_beta * delta_cost)\n\n    # 7. Combine core loss and scaling factor\n    loss_per_sample = weight_scale * core_loss\n\n    # 8. Apply optional sample weights if provided\n    sample_weights = batch.get('weight', None)\n    if sample_weights is not None:\n        loss_per_sample = loss_per_sample * sample_weights\n\n    # 9. Return the mean loss over the batch\n    return loss_per_sample.mean()"}, "fitness": {"hf_like_score": 7.840851545333862, "validation_objective": 7.840851545333862, "generalization_penalty": 0.0, "generalization_objectives": {"20": 7.735002279281616}, "train_score_mean": 7.239890946109403, "train_loss_mean": 0.16001713609397983, "pair_count": 190060733, "config": {"hf": {"problem": "tsp", "hf_steps": 0, "hf_epochs": 10, "hf_instances_per_epoch": 100000, "train_problem_size": 20, "valid_problem_sizes": [20], "train_batch_size": 64, "pomo_size": 20, "learning_rate": 0.0003, "weight_decay": 1e-06, "alpha": 0.05, "device": "cuda:0", "seed": 1234, "num_validation_episodes": 128, "validation_batch_size": 64, "generalization_penalty_weight": 1.0, "pool_version": "v0"}, "free_loss": {"f1_steps": 0, "total_train_steps": 15630, "f2_steps": 100, "f3_enabled": false}}, "loss_ir": {"name": "Adaptive Logit-Scaled Margin Loss with Tanh Normalization", "intuition": "This loss function aims to create a more direct and interpretable link between the cost difference and the learning signal. It inherits two core ideas from its parents: 1) The use of a scaled `tanh` function to normalize the cost difference, providing a stable, batch-independent measure of preference strength (from Parent 1, 'Adaptive Sigmoid Margin Loss with Tanh Normalization'). 2) The overall structure of a cost-weighted logistic loss, where `softplus(beta * delta_cost)` scales the core loss term, focusing training on pairs with larger cost gaps (common to both parents).\n\nThe first new coupling idea is the margin calculation. Instead of using a sigmoid or another bounded function, the margin is computed by applying an inverse sigmoid (logit) transformation to the tanh-normalized cost difference. Specifically, `margin = log(norm_delta_cost / (1 - norm_delta_cost))`. This creates a margin that grows much more rapidly and becomes unbounded as the normalized cost difference approaches 1. The intuition is that for very confident preferences (large cost gap), the model should be pushed to create an extremely large log-probability separation, rather than just exceeding a bounded margin. A `clamp` is used on the input to the `log` to ensure numerical stability when `norm_delta_cost` is close to 0 or 1.\n\nThe second new coupling idea is a dynamic `beta` schedule that is inversely related to the log-probability difference. We calculate `dynamic_beta = base_beta * exp(-gamma * |delta_logp|)`. When the model is already very confident about a preference pair (large `|delta_logp|`), `dynamic_beta` is down-weighted. Conversely, when the model is uncertain (small `|delta_logp|`), `dynamic_beta` is larger. This focuses the `softplus` weighting mechanism on the examples where the model's predictions are still weak or incorrect, preventing the loss from being dominated by already well-learned examples that have large cost differences.", "hyperparams": {"base_beta": 1.0, "norm_scale": 10.0, "gamma": 0.5, "eps": 1e-08}, "operators_used": ["logsigmoid", "softplus", "tanh", "clamp", "log", "exp"]}}, "better_than_baseline": false}
{"generation": 5, "index": 2, "ir": {"name": "Adaptive Margin Loss with Tanh-Softplus Coupling", "intuition": "This loss function synthesizes the core concepts of its parents while introducing a novel coupling mechanism for improved stability and training dynamics. It inherits the foundational Bradley-Terry structure where the log-probability difference is encouraged to exceed a margin, specifically using `-logsigmoid(delta_logp - margin)` as seen in both parents. It also adopts the idea of an adaptive margin that is a function of the cost difference, and the `softplus(beta * delta_cost)` weighting scheme, both common to its parents. The use of `tanh` for cost normalization is also inherited from Parent 1 (`Adaptive Sigmoid Margin Loss with Tanh Normalization`).\n\nThe first new coupling idea is in the margin calculation. Instead of `sigmoid(tanh(delta_cost))`, this child uses `softplus(tanh(delta_cost))`. This creates a margin that is non-saturating for positive cost differences, allowing the required log-probability gap to grow more naturally with the cost gap, while still being bounded at zero for negative or zero cost gaps. This prevents the model from being forced to learn a large, potentially noisy, separation for pairs with extremely large cost differences, which `sigmoid` would enforce by saturating near `max_margin`.\n\nAs a second new coupling, the loss introduces a `log_prob_gap` term. This term, `log(1 + exp(delta_logp))`, is subtracted from the final loss. It acts as a gentle regularizer that penalizes the model for making the log-probability difference excessively large, especially when the preference is already correctly satisfied. This encourages the model to allocate its probability mass more efficiently, preventing it from becoming overconfident on easy examples and improving generalization. This term is inspired by the idea of preventing overfitting but is implemented in a novel, smooth, and differentiable way.", "pseudocode": "1. Calculate the cost difference: delta_cost = cost(b) - cost(a).\n2. Calculate the log-probability difference: delta_logp = logp(a) - logp(b).\n3. (Inherited from Parent 1) Normalize the cost difference using a scaled tanh function: norm_delta_cost = tanh(delta_cost / norm_scale).\n4. (New Coupling 1) Compute an adaptive margin using a softplus activation on the normalized cost: margin = max_margin * softplus(norm_delta_cost). This creates a non-saturating margin for positive cost gaps.\n5. (Inherited from both) Compute the core preference loss: core_loss = -logsigmoid(delta_logp - margin).\n6. (Inherited from both) Calculate a cost-based weight: weight_scale = softplus(beta * delta_cost).\n7. Compute the scaled loss: scaled_loss = weight_scale * core_loss.\n8. (New Coupling 2) Compute a log-probability gap regularizer: log_prob_gap = log(1 + exp(delta_logp)). This term is approximately delta_logp for large positive delta_logp, and near zero otherwise.\n9. Combine the scaled loss and the regularizer: final_loss = scaled_loss - regularizer_strength * log_prob_gap.\n10. Return the mean of final_loss over the batch.", "hyperparams": {"beta": 1.0, "max_margin": 1.0, "norm_scale": 10.0, "regularizer_strength": 0.01}, "operators_used": ["logsigmoid", "softplus", "tanh", "log", "exp"], "implementation_hint": {"expects": ["cost_a", "cost_b", "logp_a", "logp_b"], "returns": "scalar"}, "code": "import torch\nimport torch.nn.functional as F\n\ndef generated_loss(batch, model_output, extra):\n    \"\"\"\n    Adaptive Margin Loss with Tanh-Softplus Coupling and a Log-Prob Gap Regularizer.\n\n    Args:\n        batch (dict): A dictionary containing tensors from the data loader.\n                      Expected keys: 'cost_a', 'cost_b', 'log_prob_w', 'log_prob_l'.\n                      'cost_a' and 'log_prob_w' correspond to the preferred solution (winner).\n                      'cost_b' and 'log_prob_l' correspond to the non-preferred solution (loser).\n                      An optional 'weight' tensor can be used for sample weighting.\n        model_output (dict): Output from the model (not used in this loss structure).\n        extra (dict): A dictionary for hyperparameters, e.g., {'beta': 1.0, 'max_margin': 1.0, 'norm_scale': 10.0, 'regularizer_strength': 0.01}.\n\n    Returns:\n        torch.Tensor: A scalar tensor representing the mean loss.\n    \"\"\"\n    # Read hyperparameters\n    beta = extra.get('beta', 1.0)\n    max_margin = extra.get('max_margin', 1.0)\n    norm_scale = extra.get('norm_scale', 10.0)\n    regularizer_strength = extra.get('regularizer_strength', 0.01)\n\n    # In our setting, 'a' is the winner (w) and 'b' is the loser (l).\n    # cost_a < cost_b\n    cost_a = batch['cost_a']\n    cost_b = batch['cost_b']\n    logp_a = batch['log_prob_w']\n    logp_b = batch['log_prob_l']\n\n    # 1. Calculate cost and log-probability differences\n    delta_cost = cost_b - cost_a\n    delta_logp = logp_a - logp_b\n\n    # 2. Normalize cost difference with tanh (inherited from Parent 1)\n    norm_delta_cost = torch.tanh(delta_cost / norm_scale)\n\n    # 3. (New Coupling 1) Compute adaptive margin using softplus on the tanh-normalized cost.\n    # This creates a non-saturating margin for positive delta_cost, unlike the sigmoid margin in parents.\n    margin = max_margin * F.softplus(norm_delta_cost)\n\n    # 4. Compute the core preference loss (inherited from both parents)\n    core_loss = -F.logsigmoid(delta_logp - margin)\n\n    # 5. Compute a cost-based scaling factor (inherited from both parents)\n    weight_scale = F.softplus(beta * delta_cost)\n\n    # 6. Combine core loss and scaling factor\n    scaled_loss = weight_scale * core_loss\n\n    # 7. (New Coupling 2) Compute a log-prob gap regularizer to prevent overconfidence.\n    # log(1 + exp(x)) is a smooth approximation of relu(x). It penalizes large positive delta_logp.\n    # It can be computed more stably as F.softplus(delta_logp).\n    log_prob_gap_regularizer = F.softplus(delta_logp)\n\n    # 8. Combine the scaled loss with the regularizer.\n    # We subtract the regularizer as we want to penalize (increase loss for) large delta_logp.\n    # The main loss term is negative, so adding a positive penalty is equivalent to subtracting it from the -logsigmoid term.\n    # To keep the logic clear, we subtract a positive term from the positive loss.\n    loss_per_sample = scaled_loss + regularizer_strength * log_prob_gap_regularizer\n\n    # 9. Apply optional sample weights if provided\n    sample_weights = batch.get('weight', None)\n    if sample_weights is not None:\n        loss_per_sample = loss_per_sample * sample_weights\n\n    # 10. Return the mean loss over the batch\n    return loss_per_sample.mean()"}, "fitness": {"hf_like_score": 3.84498393535614, "validation_objective": 3.84498393535614, "generalization_penalty": 0.0, "generalization_objectives": {"20": 3.8407715559005737}, "train_score_mean": 4.367818964725111, "train_loss_mean": 0.33691483930525534, "pair_count": 190060581, "config": {"hf": {"problem": "tsp", "hf_steps": 0, "hf_epochs": 10, "hf_instances_per_epoch": 100000, "train_problem_size": 20, "valid_problem_sizes": [20], "train_batch_size": 64, "pomo_size": 20, "learning_rate": 0.0003, "weight_decay": 1e-06, "alpha": 0.05, "device": "cuda:1", "seed": 1234, "num_validation_episodes": 128, "validation_batch_size": 64, "generalization_penalty_weight": 1.0, "pool_version": "v0"}, "free_loss": {"f1_steps": 0, "total_train_steps": 15630, "f2_steps": 100, "f3_enabled": false}}, "loss_ir": {"name": "Adaptive Margin Loss with Tanh-Softplus Coupling", "intuition": "This loss function synthesizes the core concepts of its parents while introducing a novel coupling mechanism for improved stability and training dynamics. It inherits the foundational Bradley-Terry structure where the log-probability difference is encouraged to exceed a margin, specifically using `-logsigmoid(delta_logp - margin)` as seen in both parents. It also adopts the idea of an adaptive margin that is a function of the cost difference, and the `softplus(beta * delta_cost)` weighting scheme, both common to its parents. The use of `tanh` for cost normalization is also inherited from Parent 1 (`Adaptive Sigmoid Margin Loss with Tanh Normalization`).\n\nThe first new coupling idea is in the margin calculation. Instead of `sigmoid(tanh(delta_cost))`, this child uses `softplus(tanh(delta_cost))`. This creates a margin that is non-saturating for positive cost differences, allowing the required log-probability gap to grow more naturally with the cost gap, while still being bounded at zero for negative or zero cost gaps. This prevents the model from being forced to learn a large, potentially noisy, separation for pairs with extremely large cost differences, which `sigmoid` would enforce by saturating near `max_margin`.\n\nAs a second new coupling, the loss introduces a `log_prob_gap` term. This term, `log(1 + exp(delta_logp))`, is subtracted from the final loss. It acts as a gentle regularizer that penalizes the model for making the log-probability difference excessively large, especially when the preference is already correctly satisfied. This encourages the model to allocate its probability mass more efficiently, preventing it from becoming overconfident on easy examples and improving generalization. This term is inspired by the idea of preventing overfitting but is implemented in a novel, smooth, and differentiable way.", "hyperparams": {"beta": 1.0, "max_margin": 1.0, "norm_scale": 10.0, "regularizer_strength": 0.01}, "operators_used": ["logsigmoid", "softplus", "tanh", "log", "exp"]}}, "better_than_baseline": true}
{"generation": 6, "index": 1, "ir": {"name": "Soft-Clipped Exponential Margin Loss", "intuition": "This loss function creates a novel preference objective by blending ideas from its parents and introducing new coupling mechanisms for stability and focus. \n\nInherited Ideas:\n- From both parents, it inherits the core concept of an adaptive margin that depends on the cost difference (`delta_cost = cost_b - cost_a`). This ensures that pairs with a larger cost gap demand a larger separation in log-probabilities.\n- It also inherits the `softplus(beta * delta_cost)` weighting scheme, which up-weights pairs that are more distinct in cost, focusing the training on clear-cut preferences.\n\nNew Couplings & Modifications:\n1. **Tanh-based Soft Clipping of Log-Probabilities**: Instead of clipping the normalized cost to compute the margin, we apply a `tanh` function directly to the log-probability difference (`delta_logp`). `tanh(delta_logp / tau)` acts as a soft clipping mechanism, preventing extremely large `delta_logp` values from causing instability or gradient explosion. The `tau` hyperparameter controls the sensitivity of this clipping. This is a stability trick that regularizes the model's output space.\n2. **Exponential Margin**: The margin is now an exponential function of the cost difference, `exp(alpha * delta_cost) - 1`. This creates a more aggressive margin that grows non-linearly with the cost gap, strongly pushing the model to differentiate pairs with high cost differences. The `-1` ensures the margin is zero when the cost gap is zero.\n\nOverall, the loss becomes `softplus(beta * delta_cost) * -logsigmoid(tanh(delta_logp / tau) - (exp(alpha * delta_cost) - 1))`. This design combines an aggressive exponential margin with a stabilizing soft-clip on the model's log-probability difference, all weighted by the significance of the cost gap.", "pseudocode": "1. Calculate the cost difference: delta_cost = cost(b) - cost(a).\n2. Calculate the log-probability difference: delta_logp = logp(a) - logp(b).\n3. (New Coupling) Compute an exponential margin: margin = exp(alpha * delta_cost) - 1. This margin grows aggressively with the cost gap.\n4. (New Coupling) Apply a soft-clipping function to the log-probability difference for stability: soft_clipped_delta_logp = tanh(delta_logp / tau).\n5. Compute the core preference loss using the soft-clipped log-probability difference and the exponential margin: core_loss = -logsigmoid(soft_clipped_delta_logp - margin).\n6. (Inherited) Calculate a cost-based weight to emphasize significant pairs: weight_scale = softplus(beta * delta_cost).\n7. Combine the weight and core loss: final_loss = weight_scale * core_loss.\n8. Return the mean of final_loss over the batch.", "hyperparams": {"beta": 0.5, "alpha": 0.1, "tau": 4.0}, "operators_used": ["logsigmoid", "softplus", "tanh", "exp"], "implementation_hint": {"expects": ["cost_a", "cost_b", "logp_a", "logp_b"], "returns": "scalar"}, "code": "import torch\nimport torch.nn.functional as F\n\ndef generated_loss(batch, model_output, extra):\n    \"\"\"\n    Soft-Clipped Exponential Margin Loss.\n\n    Args:\n        batch (dict): A dictionary containing tensors from the data loader.\n                      Expected keys: 'cost_a', 'cost_b', 'log_prob_w', 'log_prob_l'.\n                      'cost_a' and 'log_prob_w' correspond to the preferred solution (winner).\n                      'cost_b' and 'log_prob_l' correspond to the non-preferred solution (loser).\n                      An optional 'weight' tensor can be used for sample weighting.\n        model_output (dict): Output from the model (not used in this loss structure).\n        extra (dict): A dictionary for hyperparameters, e.g., {'beta': 0.5, 'alpha': 0.1, 'tau': 4.0}.\n\n    Returns:\n        torch.Tensor: A scalar tensor representing the mean loss.\n    \"\"\"\n    # Read hyperparameters\n    beta = extra.get('beta', 0.5)\n    alpha = extra.get('alpha', 0.1)\n    tau = extra.get('tau', 4.0)\n\n    # In our setting, 'a' is the winner (w) and 'b' is the loser (l).\n    # cost_a < cost_b\n    cost_a = batch['cost_a']\n    cost_b = batch['cost_b']\n    logp_a = batch['log_prob_w']\n    logp_b = batch['log_prob_l']\n\n    # 1. Calculate cost and log-probability differences\n    delta_cost = cost_b - cost_a\n    delta_logp = logp_a - logp_b\n\n    # 2. (New Coupling) Compute an exponential margin based on the cost difference.\n    # This margin grows non-linearly, demanding stronger separation for high-cost-gap pairs.\n    # The -1 ensures margin is 0 when delta_cost is 0.\n    margin = torch.exp(alpha * delta_cost) - 1.0\n\n    # 3. (New Coupling) Apply a soft-clipping function (tanh) to the log-probability difference.\n    # This acts as a stabilizer, preventing extreme delta_logp values from causing gradient issues.\n    # The tau parameter controls the saturation point.\n    soft_clipped_delta_logp = torch.tanh(delta_logp / tau)\n\n    # 4. Compute the core preference loss using the soft-clipped logp and exponential margin.\n    core_loss = -F.logsigmoid(soft_clipped_delta_logp - margin)\n\n    # 5. (Inherited) Compute a cost-based scaling factor to up-weight significant preferences.\n    weight_scale = F.softplus(beta * delta_cost)\n\n    # 6. Combine core loss and scaling factor\n    loss_per_sample = weight_scale * core_loss\n\n    # 7. Apply optional sample weights if provided\n    sample_weights = batch.get('weight', None)\n    if sample_weights is not None:\n        loss_per_sample = loss_per_sample * sample_weights\n\n    # 8. Return the mean loss over the batch\n    return loss_per_sample.mean()"}, "fitness": {"hf_like_score": 3.8481924533843994, "validation_objective": 3.8481924533843994, "generalization_penalty": 0.0, "generalization_objectives": {"20": 3.844746470451355}, "train_score_mean": 3.8421654226073683, "train_loss_mean": 0.46796078262463336, "pair_count": 155380531, "config": {"hf": {"problem": "tsp", "hf_steps": 0, "hf_epochs": 10, "hf_instances_per_epoch": 100000, "train_problem_size": 20, "valid_problem_sizes": [20], "train_batch_size": 64, "pomo_size": 20, "learning_rate": 0.0003, "weight_decay": 1e-06, "alpha": 0.05, "device": "cuda:0", "seed": 1234, "num_validation_episodes": 128, "validation_batch_size": 64, "generalization_penalty_weight": 1.0, "pool_version": "v0"}, "free_loss": {"f1_steps": 0, "total_train_steps": 15630, "f2_steps": 100, "f3_enabled": false}}, "loss_ir": {"name": "Soft-Clipped Exponential Margin Loss", "intuition": "This loss function creates a novel preference objective by blending ideas from its parents and introducing new coupling mechanisms for stability and focus. \n\nInherited Ideas:\n- From both parents, it inherits the core concept of an adaptive margin that depends on the cost difference (`delta_cost = cost_b - cost_a`). This ensures that pairs with a larger cost gap demand a larger separation in log-probabilities.\n- It also inherits the `softplus(beta * delta_cost)` weighting scheme, which up-weights pairs that are more distinct in cost, focusing the training on clear-cut preferences.\n\nNew Couplings & Modifications:\n1. **Tanh-based Soft Clipping of Log-Probabilities**: Instead of clipping the normalized cost to compute the margin, we apply a `tanh` function directly to the log-probability difference (`delta_logp`). `tanh(delta_logp / tau)` acts as a soft clipping mechanism, preventing extremely large `delta_logp` values from causing instability or gradient explosion. The `tau` hyperparameter controls the sensitivity of this clipping. This is a stability trick that regularizes the model's output space.\n2. **Exponential Margin**: The margin is now an exponential function of the cost difference, `exp(alpha * delta_cost) - 1`. This creates a more aggressive margin that grows non-linearly with the cost gap, strongly pushing the model to differentiate pairs with high cost differences. The `-1` ensures the margin is zero when the cost gap is zero.\n\nOverall, the loss becomes `softplus(beta * delta_cost) * -logsigmoid(tanh(delta_logp / tau) - (exp(alpha * delta_cost) - 1))`. This design combines an aggressive exponential margin with a stabilizing soft-clip on the model's log-probability difference, all weighted by the significance of the cost gap.", "hyperparams": {"beta": 0.5, "alpha": 0.1, "tau": 4.0}, "operators_used": ["logsigmoid", "softplus", "tanh", "exp"]}}, "better_than_baseline": true}
{"generation": 6, "index": 4, "ir": {"name": "Adaptive Logit-Margin Loss with Exponential Weighting", "intuition": "This loss function synthesizes and extends ideas from its parents to create a robust and adaptive preference learning objective. It inherits the core logistic loss structure (`-logsigmoid`) and the use of an adaptive, cost-sensitive margin from both parents. The margin's adaptivity is achieved by making it a function of the cost difference between the preferred and non-preferred solutions. From both parents, it also inherits the idea of scaling the loss based on the magnitude of the cost difference, focusing training on more significant preference pairs.\n\nThe first new coupling idea is a different method for calculating the margin. Instead of a sigmoid function, it uses a 'logit-margin' approach: `margin = max_margin * log(1 + exp(delta_cost / norm_scale)) / (1 + log(1 + exp(delta_cost / norm_scale)))`. This creates a margin that grows from 0 towards `max_margin` but has a gentler slope for small cost differences compared to sigmoid, while still being bounded. This can make the model less sensitive to noisy preferences near zero cost difference.\n\nThe second new coupling idea is a novel weighting scheme. Instead of using `softplus` on the cost difference, which can grow linearly, this loss uses an exponential function `exp(beta * tanh(delta_cost / norm_scale))`. The `tanh` normalization inside the exponential ensures the weight is bounded (between 1 and `exp(beta)`), preventing extremely large cost differences from creating unstable, exploding loss values. This provides a stable, bounded amplification of the loss for more significant preferences, combining the stability of `tanh` normalization (from Parent 1) with a powerful exponential weighting.", "pseudocode": "1. Calculate the cost difference: delta_cost = cost(b) - cost(a).\n2. Calculate the log-probability difference: delta_logp = logp(a) - logp(b).\n3. (Inherited) Define the base logistic loss structure: loss = -(logp(a) - logp(b) - margin).\n4. (New Coupling 1) Compute an adaptive 'logit-margin'. First, calculate a softplus-like term: term = log(1 + exp(delta_cost / norm_scale)). Then compute the margin: margin = max_margin * term / (1 + term). This creates a margin that smoothly grows from 0 to max_margin.\n5. (New Coupling 2) Compute a bounded exponential weight. First, normalize the cost difference with tanh: norm_delta_cost = tanh(delta_cost / norm_scale). Then compute the weight: weight_scale = exp(beta * norm_delta_cost). This weight is bounded between exp(0)=1 and exp(beta).\n6. Combine the components into the final loss expression: final_loss = weight_scale * logsigmoid(margin - delta_logp). This is equivalent to weight_scale * -logsigmoid(delta_logp - margin) but can be more stable.\n7. Return the mean of final_loss over the batch.", "hyperparams": {"beta": 1.0, "max_margin": 2.0, "norm_scale": 10.0}, "operators_used": ["logsigmoid", "exp", "log", "tanh"], "implementation_hint": {"expects": ["cost_a", "cost_b", "logp_a", "logp_b"], "returns": "scalar"}, "code": "import torch\nimport torch.nn.functional as F\n\ndef generated_loss(batch, model_output, extra):\n    \"\"\"\n    Adaptive Logit-Margin Loss with Exponential Weighting.\n\n    Args:\n        batch (dict): A dictionary containing tensors from the data loader.\n                      Expected keys: 'cost_a', 'cost_b', 'log_prob_w', 'log_prob_l'.\n                      'cost_a' and 'log_prob_w' correspond to the preferred solution (winner).\n                      'cost_b' and 'log_prob_l' correspond to the non-preferred solution (loser).\n                      An optional 'weight' tensor can be used for sample weighting.\n        model_output (dict): Output from the model (not used in this loss structure).\n        extra (dict): A dictionary for hyperparameters, e.g., {'beta': 1.0, 'max_margin': 2.0, 'norm_scale': 10.0}.\n\n    Returns:\n        torch.Tensor: A scalar tensor representing the mean loss.\n    \"\"\"\n    # Read hyperparameters\n    beta = extra.get('beta', 1.0)\n    max_margin = extra.get('max_margin', 2.0)\n    norm_scale = extra.get('norm_scale', 10.0)\n\n    # In our setting, 'a' is the winner (w) and 'b' is the loser (l).\n    # cost_a < cost_b\n    cost_a = batch['cost_a']\n    cost_b = batch['cost_b']\n    logp_a = batch['log_prob_w']\n    logp_b = batch['log_prob_l']\n\n    # 1. Calculate cost and log-probability differences\n    # delta_cost should be positive since cost_b > cost_a\n    delta_cost = cost_b - cost_a\n    delta_logp = logp_a - logp_b\n\n    # 2. (New Coupling 1) Compute the adaptive 'logit-margin'\n    # This margin function grows smoothly from 0 to max_margin.\n    # It is a variation of softplus, normalized to be bounded.\n    # term = log(1 + exp(delta_cost / norm_scale)) is equivalent to softplus(delta_cost / norm_scale)\n    # We use the log-sum-exp trick for stability: log(exp(0) + exp(x)) = max(0,x) + log(1+exp(-|x|))\n    x = delta_cost / norm_scale\n    term = torch.max(torch.zeros_like(x), x) + torch.log1p(torch.exp(-torch.abs(x)))\n    margin = max_margin * term / (1.0 + term)\n\n    # 3. (New Coupling 2) Compute a bounded exponential weight\n    # The tanh normalization ensures the weight is bounded between 1 and exp(beta),\n    # preventing exploding loss values from very large cost differences.\n    norm_delta_cost = torch.tanh(delta_cost / norm_scale)\n    weight_scale = torch.exp(beta * norm_delta_cost)\n\n    # 4. Compute the core preference loss (Bradley-Terry style with margin)\n    # We want logp_a > logp_b + margin, so (delta_logp - margin) should be positive.\n    # The loss is -logsigmoid(delta_logp - margin).\n    # This is equivalent to logsigmoid(-(delta_logp - margin)) = logsigmoid(margin - delta_logp).\n    core_loss = F.logsigmoid(margin - delta_logp)\n\n    # 5. Combine weight and loss. The negative sign is included here.\n    loss_per_sample = -weight_scale * core_loss\n\n    # 6. Apply optional sample weights if provided\n    sample_weights = batch.get('weight', None)\n    if sample_weights is not None:\n        loss_per_sample = loss_per_sample * sample_weights\n\n    # 7. Return the mean loss over the batch\n    return loss_per_sample.mean()"}, "fitness": {"hf_like_score": 11.345383167266846, "validation_objective": 11.345383167266846, "generalization_penalty": 0.0, "generalization_objectives": {"20": 11.176851272583008}, "train_score_mean": 10.528787413637987, "train_loss_mean": 0.40108912869744473, "pair_count": 190060706, "config": {"hf": {"problem": "tsp", "hf_steps": 0, "hf_epochs": 10, "hf_instances_per_epoch": 100000, "train_problem_size": 20, "valid_problem_sizes": [20], "train_batch_size": 64, "pomo_size": 20, "learning_rate": 0.0003, "weight_decay": 1e-06, "alpha": 0.05, "device": "cuda:1", "seed": 1234, "num_validation_episodes": 128, "validation_batch_size": 64, "generalization_penalty_weight": 1.0, "pool_version": "v0"}, "free_loss": {"f1_steps": 0, "total_train_steps": 15630, "f2_steps": 100, "f3_enabled": false}}, "loss_ir": {"name": "Adaptive Logit-Margin Loss with Exponential Weighting", "intuition": "This loss function synthesizes and extends ideas from its parents to create a robust and adaptive preference learning objective. It inherits the core logistic loss structure (`-logsigmoid`) and the use of an adaptive, cost-sensitive margin from both parents. The margin's adaptivity is achieved by making it a function of the cost difference between the preferred and non-preferred solutions. From both parents, it also inherits the idea of scaling the loss based on the magnitude of the cost difference, focusing training on more significant preference pairs.\n\nThe first new coupling idea is a different method for calculating the margin. Instead of a sigmoid function, it uses a 'logit-margin' approach: `margin = max_margin * log(1 + exp(delta_cost / norm_scale)) / (1 + log(1 + exp(delta_cost / norm_scale)))`. This creates a margin that grows from 0 towards `max_margin` but has a gentler slope for small cost differences compared to sigmoid, while still being bounded. This can make the model less sensitive to noisy preferences near zero cost difference.\n\nThe second new coupling idea is a novel weighting scheme. Instead of using `softplus` on the cost difference, which can grow linearly, this loss uses an exponential function `exp(beta * tanh(delta_cost / norm_scale))`. The `tanh` normalization inside the exponential ensures the weight is bounded (between 1 and `exp(beta)`), preventing extremely large cost differences from creating unstable, exploding loss values. This provides a stable, bounded amplification of the loss for more significant preferences, combining the stability of `tanh` normalization (from Parent 1) with a powerful exponential weighting.", "hyperparams": {"beta": 1.0, "max_margin": 2.0, "norm_scale": 10.0}, "operators_used": ["logsigmoid", "exp", "log", "tanh"]}}, "better_than_baseline": false}
{"generation": 6, "index": 7, "ir": {"name": "Softplus-Normalized Sigmoid Margin Loss with Exponential Weighting", "intuition": "This loss function synthesizes ideas from its parents to create a robust, cost-sensitive preference objective. It inherits the core structure of an adaptive margin loss from both parents, where the margin that the model's log-probability difference must overcome is a function of the cost difference between the preferred and non-preferred solutions. From both parents, it also inherits the use of a logistic loss (`-logsigmoid`) as the fundamental preference objective.\n\nThe first new coupling idea is a different normalization scheme. Instead of `tanh` (Parent 1) or clipping (Parent 0), this child uses a scaled `softplus` function to normalize the cost difference. This creates a non-negative, unbounded normalization that grows smoothly and non-linearly with the cost gap, offering a different inductive bias compared to the bounded `tanh` normalization. This `softplus` normalized value is then used within a `sigmoid` function to compute the adaptive margin, similar to both parents.\n\nThe second new coupling idea is a novel weighting mechanism. Instead of scaling the loss with `softplus(beta * delta_cost)` as both parents do, this child uses an exponential weighting `exp(beta * norm_delta_cost)`. This couples the loss weighting directly to the *normalized* cost difference. Using `exp` creates a sharper penalty gradient for pairs with larger normalized cost gaps, aggressively focusing the model on learning from the most significant preference signals as defined by the `softplus` normalization. This avoids potential numerical instability from `exp(raw_delta_cost)` while still providing strong, exponential emphasis.", "pseudocode": "1. Calculate the cost difference: delta_cost = cost(b) - cost(a).\n2. (New Coupling 1) Normalize the cost difference using a scaled softplus function: norm_delta_cost = softplus(delta_cost / norm_scale). This provides a smooth, non-negative, and unbounded normalization.\n3. (Inherited) Compute an adaptive margin based on the normalized cost difference: margin = max_margin * sigmoid(norm_delta_cost). The margin is small for small cost gaps and grows towards max_margin.\n4. (Inherited) Calculate the log-probability difference: delta_logp = logp(a) - logp(b).\n5. (Inherited) Compute the core preference loss: core_loss = -logsigmoid(delta_logp - margin).\n6. (New Coupling 2) Calculate an exponential weight based on the normalized cost difference: weight_scale = exp(beta * norm_delta_cost). This sharply increases the loss weight for pairs with larger normalized cost gaps.\n7. Combine them: final_loss = weight_scale * core_loss.\n8. Return the mean of final_loss over the batch.", "hyperparams": {"beta": 0.5, "max_margin": 2.0, "norm_scale": 10.0}, "operators_used": ["logsigmoid", "softplus", "sigmoid", "exp"], "implementation_hint": {"expects": ["cost_a", "cost_b", "logp_a", "logp_b", "weight"], "returns": "scalar"}, "code": "import torch\nimport torch.nn.functional as F\n\ndef generated_loss(batch, model_output, extra):\n    \"\"\"\n    Softplus-Normalized Sigmoid Margin Loss with Exponential Weighting.\n\n    Args:\n        batch (dict): A dictionary containing tensors from the data loader.\n                      Expected keys: 'cost_a', 'cost_b', 'log_prob_w', 'log_prob_l'.\n                      'cost_a' and 'log_prob_w' correspond to the preferred solution (winner).\n                      'cost_b' and 'log_prob_l' correspond to the non-preferred solution (loser).\n                      An optional 'weight' tensor can be used for sample weighting.\n        model_output (dict): Output from the model (not used in this loss structure).\n        extra (dict): A dictionary for hyperparameters, e.g., {'beta': 0.5, 'max_margin': 2.0, 'norm_scale': 10.0}.\n\n    Returns:\n        torch.Tensor: A scalar tensor representing the mean loss.\n    \"\"\"\n    # Read hyperparameters\n    beta = extra.get('beta', 0.5)\n    max_margin = extra.get('max_margin', 2.0)\n    norm_scale = extra.get('norm_scale', 10.0)\n\n    # In our setting, 'a' is the winner (w) and 'b' is the loser (l).\n    # cost_a < cost_b\n    cost_a = batch['cost_a']\n    cost_b = batch['cost_b']\n    logp_a = batch['log_prob_w']\n    logp_b = batch['log_prob_l']\n\n    # 1. Calculate cost and log-probability differences\n    # delta_cost should be positive since cost_b > cost_a\n    delta_cost = cost_b - cost_a\n    delta_logp = logp_a - logp_b\n\n    # 2. (New Coupling 1) Normalize the cost difference using a scaled softplus function.\n    # This provides a smooth, non-negative, and unbounded normalization.\n    norm_delta_cost = F.softplus(delta_cost / norm_scale)\n\n    # 3. (Inherited) Compute the adaptive sigmoid margin.\n    # The margin is a function of the normalized cost difference.\n    margin = max_margin * torch.sigmoid(norm_delta_cost)\n\n    # 4. (Inherited) Compute the core preference loss (Bradley-Terry style with margin).\n    core_loss = -F.logsigmoid(delta_logp - margin)\n\n    # 5. (New Coupling 2) Compute an exponential scaling factor based on the *normalized* cost difference.\n    # This sharply amplifies the loss for pairs with larger normalized cost differences.\n    weight_scale = torch.exp(beta * norm_delta_cost)\n\n    # 6. Combine core loss and scaling factor\n    loss_per_sample = weight_scale * core_loss\n\n    # 7. Apply optional sample weights if provided\n    sample_weights = batch.get('weight', None)\n    if sample_weights is not None:\n        loss_per_sample = loss_per_sample * sample_weights\n\n    # 8. Return the mean loss over the batch\n    return loss_per_sample.mean()"}, "fitness": {"hf_like_score": 3.8472224473953247, "validation_objective": 3.8472224473953247, "generalization_penalty": 0.0, "generalization_objectives": {"20": 3.841059684753418}, "train_score_mean": 3.9502386841191286, "train_loss_mean": 1.0992988111342823, "pair_count": 181847477, "config": {"hf": {"problem": "tsp", "hf_steps": 0, "hf_epochs": 10, "hf_instances_per_epoch": 100000, "train_problem_size": 20, "valid_problem_sizes": [20], "train_batch_size": 64, "pomo_size": 20, "learning_rate": 0.0003, "weight_decay": 1e-06, "alpha": 0.05, "device": "cuda:2", "seed": 1234, "num_validation_episodes": 128, "validation_batch_size": 64, "generalization_penalty_weight": 1.0, "pool_version": "v0"}, "free_loss": {"f1_steps": 0, "total_train_steps": 15630, "f2_steps": 100, "f3_enabled": false}}, "loss_ir": {"name": "Softplus-Normalized Sigmoid Margin Loss with Exponential Weighting", "intuition": "This loss function synthesizes ideas from its parents to create a robust, cost-sensitive preference objective. It inherits the core structure of an adaptive margin loss from both parents, where the margin that the model's log-probability difference must overcome is a function of the cost difference between the preferred and non-preferred solutions. From both parents, it also inherits the use of a logistic loss (`-logsigmoid`) as the fundamental preference objective.\n\nThe first new coupling idea is a different normalization scheme. Instead of `tanh` (Parent 1) or clipping (Parent 0), this child uses a scaled `softplus` function to normalize the cost difference. This creates a non-negative, unbounded normalization that grows smoothly and non-linearly with the cost gap, offering a different inductive bias compared to the bounded `tanh` normalization. This `softplus` normalized value is then used within a `sigmoid` function to compute the adaptive margin, similar to both parents.\n\nThe second new coupling idea is a novel weighting mechanism. Instead of scaling the loss with `softplus(beta * delta_cost)` as both parents do, this child uses an exponential weighting `exp(beta * norm_delta_cost)`. This couples the loss weighting directly to the *normalized* cost difference. Using `exp` creates a sharper penalty gradient for pairs with larger normalized cost gaps, aggressively focusing the model on learning from the most significant preference signals as defined by the `softplus` normalization. This avoids potential numerical instability from `exp(raw_delta_cost)` while still providing strong, exponential emphasis.", "hyperparams": {"beta": 0.5, "max_margin": 2.0, "norm_scale": 10.0}, "operators_used": ["logsigmoid", "softplus", "sigmoid", "exp"]}}, "better_than_baseline": true}
{"generation": 7, "index": 0, "ir": {"name": "Adaptive Logit-Scaled Margin Loss with Tanh-Softplus Weighting", "intuition": "This loss function creates a dynamic, cost-sensitive margin that is directly proportional to the log-odds (logit) of a sigmoid-transformed cost difference. This inherits the core idea of an adaptive margin from both parents, but reframes its calculation. Instead of using the sigmoid output directly, we use `log(sigmoid(x) / (1 - sigmoid(x)))`, which is simply `x`. This means the margin is `max_margin * tanh(delta_cost / norm_scale)`, creating a bounded, linear-in-the-middle relationship between the normalized cost gap and the required log-probability separation.\n\nFrom both parents, it inherits the use of a scaled `tanh` function to provide a batch-independent normalization of the cost difference (`delta_cost`). It also inherits the overall structure of a Bradley-Terry style loss, `logsigmoid(delta_logp - margin)`, and the concept of weighting the loss by the magnitude of the cost difference.\n\nAs a new coupling idea, this child loss introduces a novel weighting mechanism. Instead of scaling the loss by `softplus(beta * delta_cost)` as the parents do, it scales the loss by `softplus(tanh(delta_cost / norm_scale))`. This has two benefits: 1) It makes the weighting factor less sensitive to extreme outliers in `delta_cost`, as the `tanh` function bounds its input. 2) It couples the weighting scale directly to the same normalized cost representation used for the margin, creating a more consistent and stable response to the cost gap. A second new idea is the introduction of a `margin_offset` hyperparameter, which adds a small, constant baseline to the margin. This ensures that even for pairs with a near-zero cost difference, the model is still encouraged to maintain a minimal separation, preventing complete indifference.", "pseudocode": "1. Calculate the cost difference: delta_cost = cost(b) - cost(a).\n2. Calculate the log-probability difference: delta_logp = logp(a) - logp(b).\n3. (Inherited) Normalize the cost difference using a scaled tanh function: norm_delta_cost = tanh(delta_cost / norm_scale).\n4. (New Coupling 1) Compute the adaptive margin. Instead of a sigmoid, the margin is now directly proportional to the tanh-normalized cost, plus a small offset: margin = margin_offset + max_margin * norm_delta_cost. This simplifies the margin calculation while retaining the desired adaptive behavior.\n5. (New Coupling 2) Compute a bounded, cost-based weight. The weight is a softplus of the tanh-normalized cost difference, making it robust to cost outliers: weight_scale = softplus(beta * norm_delta_cost).\n6. Compute the core preference loss using the adaptive margin: core_loss = -logsigmoid(delta_logp - margin).\n7. Combine the core loss and the new weight: final_loss = weight_scale * core_loss.\n8. Return the mean of final_loss over the batch.", "hyperparams": {"beta": 1.0, "max_margin": 2.0, "norm_scale": 10.0, "margin_offset": 0.05}, "operators_used": ["logsigmoid", "softplus", "tanh"], "implementation_hint": {"expects": ["cost_a", "cost_b", "log_prob_w", "log_prob_l", "weight"], "returns": "scalar"}, "code": "import torch\nimport torch.nn.functional as F\n\ndef generated_loss(batch, model_output, extra):\n    \"\"\"\n    Adaptive Logit-Scaled Margin Loss with Tanh-Softplus Weighting.\n\n    Args:\n        batch (dict): A dictionary containing tensors from the data loader.\n                      Expected keys: 'cost_a', 'cost_b', 'log_prob_w', 'log_prob_l'.\n                      'cost_a' and 'log_prob_w' correspond to the preferred solution (winner).\n                      'cost_b' and 'log_prob_l' correspond to the non-preferred solution (loser).\n                      An optional 'weight' tensor can be used for sample weighting.\n        model_output (dict): Output from the model (not used in this loss structure).\n        extra (dict): A dictionary for hyperparameters, e.g., {'beta': 1.0, 'max_margin': 2.0, 'norm_scale': 10.0, 'margin_offset': 0.05}.\n\n    Returns:\n        torch.Tensor: A scalar tensor representing the mean loss.\n    \"\"\"\n    # Read hyperparameters from the extra dictionary\n    beta = extra.get('beta', 1.0)\n    max_margin = extra.get('max_margin', 2.0)\n    norm_scale = extra.get('norm_scale', 10.0)\n    margin_offset = extra.get('margin_offset', 0.05)\n\n    # In our setting, 'a' is the winner (w) and 'b' is the loser (l).\n    # Therefore, we expect cost_a < cost_b.\n    cost_a = batch['cost_a']\n    cost_b = batch['cost_b']\n    logp_a = batch['log_prob_w']\n    logp_b = batch['log_prob_l']\n\n    # Calculate cost and log-probability differences\n    # delta_cost should be positive since cost_b > cost_a\n    delta_cost = cost_b - cost_a\n    delta_logp = logp_a - logp_b\n\n    # Inherited Idea: Normalize the cost difference using a scaled tanh function\n    # This provides a bounded (-1, 1) and batch-independent normalization\n    norm_delta_cost = torch.tanh(delta_cost / norm_scale)\n\n    # New Coupling 1: Compute the adaptive margin directly from the tanh-normalized cost\n    # This is equivalent to a logit-scaling of a sigmoid, but simpler.\n    # A small offset ensures a non-zero margin even for tiny cost differences.\n    margin = margin_offset + max_margin * norm_delta_cost\n\n    # The core preference loss (Bradley-Terry style with an adaptive margin)\n    core_loss = -F.logsigmoid(delta_logp - margin)\n\n    # New Coupling 2: Compute a bounded weight scale using softplus on the normalized cost\n    # This makes the weighting robust to extreme delta_cost values, unlike softplus(beta * delta_cost).\n    weight_scale = F.softplus(beta * norm_delta_cost)\n\n    # Combine core loss and the new bounded scaling factor\n    loss_per_sample = weight_scale * core_loss\n\n    # Apply optional sample weights if provided\n    sample_weights = batch.get('weight', None)\n    if sample_weights is not None:\n        loss_per_sample = loss_per_sample * sample_weights\n\n    # Return the mean loss over the batch\n    return loss_per_sample.mean()"}, "fitness": {"hf_like_score": 9.597955703735352, "validation_objective": 9.597955703735352, "generalization_penalty": 0.0, "generalization_objectives": {"20": 9.421101093292236}, "train_score_mean": 8.521310771373473, "train_loss_mean": 0.6298345110237942, "pair_count": 190060740, "config": {"hf": {"problem": "tsp", "hf_steps": 0, "hf_epochs": 10, "hf_instances_per_epoch": 100000, "train_problem_size": 20, "valid_problem_sizes": [20], "train_batch_size": 64, "pomo_size": 20, "learning_rate": 0.0003, "weight_decay": 1e-06, "alpha": 0.05, "device": "cuda:0", "seed": 1234, "num_validation_episodes": 128, "validation_batch_size": 64, "generalization_penalty_weight": 1.0, "pool_version": "v0"}, "free_loss": {"f1_steps": 0, "total_train_steps": 15630, "f2_steps": 100, "f3_enabled": false}}, "loss_ir": {"name": "Adaptive Logit-Scaled Margin Loss with Tanh-Softplus Weighting", "intuition": "This loss function creates a dynamic, cost-sensitive margin that is directly proportional to the log-odds (logit) of a sigmoid-transformed cost difference. This inherits the core idea of an adaptive margin from both parents, but reframes its calculation. Instead of using the sigmoid output directly, we use `log(sigmoid(x) / (1 - sigmoid(x)))`, which is simply `x`. This means the margin is `max_margin * tanh(delta_cost / norm_scale)`, creating a bounded, linear-in-the-middle relationship between the normalized cost gap and the required log-probability separation.\n\nFrom both parents, it inherits the use of a scaled `tanh` function to provide a batch-independent normalization of the cost difference (`delta_cost`). It also inherits the overall structure of a Bradley-Terry style loss, `logsigmoid(delta_logp - margin)`, and the concept of weighting the loss by the magnitude of the cost difference.\n\nAs a new coupling idea, this child loss introduces a novel weighting mechanism. Instead of scaling the loss by `softplus(beta * delta_cost)` as the parents do, it scales the loss by `softplus(tanh(delta_cost / norm_scale))`. This has two benefits: 1) It makes the weighting factor less sensitive to extreme outliers in `delta_cost`, as the `tanh` function bounds its input. 2) It couples the weighting scale directly to the same normalized cost representation used for the margin, creating a more consistent and stable response to the cost gap. A second new idea is the introduction of a `margin_offset` hyperparameter, which adds a small, constant baseline to the margin. This ensures that even for pairs with a near-zero cost difference, the model is still encouraged to maintain a minimal separation, preventing complete indifference.", "hyperparams": {"beta": 1.0, "max_margin": 2.0, "norm_scale": 10.0, "margin_offset": 0.05}, "operators_used": ["logsigmoid", "softplus", "tanh"]}}, "better_than_baseline": false}
{"generation": 7, "index": 2, "ir": {"name": "Soft-Clipped Exponential Margin Loss with Dynamic Beta", "intuition": "This loss function synthesizes ideas from its parents to create a robust, cost-sensitive preference objective, while introducing novel coupling mechanisms for improved stability and learning dynamics.\n\nInherited Ideas:\n- From Parent 1 ('Adaptive Sigmoid Margin Loss with Tanh Normalization'), it inherits the concept of an adaptive margin that grows with the cost difference. However, instead of a sigmoid, this child uses an exponential function (`exp`) to create the margin, which can provide a stronger separation signal for pairs with large cost differences.\n- From Parent 0 ('Clipped Tanh-Normalized Adaptive Margin Loss'), it inherits the use of a `softplus` function to weight the overall loss. This `softplus(beta * delta_cost)` term smoothly amplifies the loss for pairs with larger cost differences, focusing training on more significant preference signals.\n\nNew Coupling Ideas:\n1.  **Soft-Clipping with `tanh`:** The exponentially growing margin can become numerically unstable. To counteract this, the child introduces a 'soft-clipping' mechanism. The output of the `exp` function is passed through a `tanh` function. This allows the margin to grow exponentially for small-to-moderate cost differences but smoothly caps it at a maximum value (`max_margin`), preventing instability while retaining the steep initial growth.\n2.  **Dynamic Beta from Log-Probability Gap:** Instead of a fixed `beta` or one based on cost, this child introduces a `beta` that is dynamically adjusted based on the model's current log-probability gap (`delta_logp`). Specifically, `dynamic_beta = base_beta * exp(-delta_logp.detach())`. When the model is already confident (large positive `delta_logp`), `beta` is down-weighted, reducing the gradient and preventing over-confidence. When the model is wrong or uncertain (negative or small `delta_logp`), `beta` is increased, amplifying the learning signal. Using `.detach()` ensures this dynamic scaling doesn't introduce unwanted gradients through the `beta` term itself, focusing the gradient purely on the primary loss objective.", "pseudocode": "1. Calculate the cost difference: delta_cost = cost(b) - cost(a).\n2. Calculate the log-probability difference: delta_logp = logp(a) - logp(b).\n3. (Inherited from Parent 1, modified) Compute a base margin using an exponential function of the cost difference: base_margin = exp(delta_cost / norm_scale) - 1. This provides a rapidly increasing margin.\n4. (New Coupling 1) Apply 'soft-clipping' to the margin using tanh for stability: margin = max_margin * tanh(base_margin). This caps the margin smoothly.\n5. (New Coupling 2) Compute a dynamic beta based on the current log-probability gap to modulate loss weight: dynamic_beta = base_beta * exp(-delta_logp.detach()). This increases the weight for misclassified or uncertain pairs.\n6. (Inherited from Parent 0) Calculate a cost-based weight using the dynamic beta: weight_scale = softplus(dynamic_beta * delta_cost).\n7. Compute the core preference loss: core_loss = -logsigmoid(delta_logp - margin).\n8. Combine them: final_loss = weight_scale * core_loss.\n9. Return the mean of final_loss over the batch.", "hyperparams": {"base_beta": 0.5, "max_margin": 4.0, "norm_scale": 5.0}, "operators_used": ["logsigmoid", "softplus", "exp", "tanh"], "implementation_hint": {"expects": ["cost_a", "cost_b", "logp_a", "logp_b"], "returns": "scalar"}, "code": "import torch\nimport torch.nn.functional as F\n\ndef generated_loss(batch, model_output, extra):\n    \"\"\"\n    Soft-Clipped Exponential Margin Loss with Dynamic Beta.\n\n    Args:\n        batch (dict): A dictionary containing tensors from the data loader.\n                      Expected keys: 'cost_a', 'cost_b', 'log_prob_w', 'log_prob_l'.\n                      'cost_a' and 'log_prob_w' correspond to the preferred solution (winner).\n                      'cost_b' and 'log_prob_l' correspond to the non-preferred solution (loser).\n                      An optional 'weight' tensor can be used for sample weighting.\n        model_output (dict): Output from the model (not used in this loss structure).\n        extra (dict): A dictionary for hyperparameters, e.g., {'base_beta': 0.5, 'max_margin': 4.0, 'norm_scale': 5.0}.\n\n    Returns:\n        torch.Tensor: A scalar tensor representing the mean loss.\n    \"\"\"\n    # Read hyperparameters\n    base_beta = extra.get('base_beta', 0.5)\n    max_margin = extra.get('max_margin', 4.0)\n    norm_scale = extra.get('norm_scale', 5.0)\n\n    # In our setting, 'a' is the winner (w) and 'b' is the loser (l).\n    # cost_a < cost_b\n    cost_a = batch['cost_a']\n    cost_b = batch['cost_b']\n    logp_a = batch['log_prob_w']\n    logp_b = batch['log_prob_l']\n\n    # 1. Calculate cost and log-probability differences\n    delta_cost = cost_b - cost_a\n    delta_logp = logp_a - logp_b\n\n    # 2. (Inherited/Modified) Compute an exponential margin base\n    # The `-1` ensures the margin is zero when delta_cost is zero.\n    base_margin = torch.exp(delta_cost / norm_scale) - 1.0\n\n    # 3. (New Coupling 1) Soft-clip the margin using tanh for stability\n    # This allows exponential growth but smoothly caps it at max_margin.\n    margin = max_margin * torch.tanh(base_margin)\n\n    # 4. (New Coupling 2) Dynamic beta based on log-prob gap\n    # When model is wrong (delta_logp < 0), exp term is large, increasing beta.\n    # When model is correct (delta_logp > 0), exp term is small, decreasing beta.\n    # .detach() is used to prevent this from affecting the gradient direction of delta_logp.\n    with torch.no_grad():\n        dynamic_beta = base_beta * torch.exp(-delta_logp)\n\n    # 5. (Inherited) Compute cost-based weight using softplus and dynamic beta\n    weight_scale = F.softplus(dynamic_beta * delta_cost)\n\n    # 6. Compute the core preference loss\n    core_loss = -F.logsigmoid(delta_logp - margin)\n\n    # 7. Combine core loss and scaling factor\n    loss_per_sample = weight_scale * core_loss\n\n    # 8. Apply optional sample weights if provided\n    sample_weights = batch.get('weight', None)\n    if sample_weights is not None:\n        loss_per_sample = loss_per_sample * sample_weights\n\n    # 9. Return the mean loss over the batch\n    return loss_per_sample.mean()"}, "fitness": {"hf_like_score": 7.817354917526245, "validation_objective": 7.817354917526245, "generalization_penalty": 0.0, "generalization_objectives": {"20": 7.732081174850464}, "train_score_mean": 7.744974297311774, "train_loss_mean": 361.66827907476835, "pair_count": 190060726, "config": {"hf": {"problem": "tsp", "hf_steps": 0, "hf_epochs": 10, "hf_instances_per_epoch": 100000, "train_problem_size": 20, "valid_problem_sizes": [20], "train_batch_size": 64, "pomo_size": 20, "learning_rate": 0.0003, "weight_decay": 1e-06, "alpha": 0.05, "device": "cuda:1", "seed": 1234, "num_validation_episodes": 128, "validation_batch_size": 64, "generalization_penalty_weight": 1.0, "pool_version": "v0"}, "free_loss": {"f1_steps": 0, "total_train_steps": 15630, "f2_steps": 100, "f3_enabled": false}}, "loss_ir": {"name": "Soft-Clipped Exponential Margin Loss with Dynamic Beta", "intuition": "This loss function synthesizes ideas from its parents to create a robust, cost-sensitive preference objective, while introducing novel coupling mechanisms for improved stability and learning dynamics.\n\nInherited Ideas:\n- From Parent 1 ('Adaptive Sigmoid Margin Loss with Tanh Normalization'), it inherits the concept of an adaptive margin that grows with the cost difference. However, instead of a sigmoid, this child uses an exponential function (`exp`) to create the margin, which can provide a stronger separation signal for pairs with large cost differences.\n- From Parent 0 ('Clipped Tanh-Normalized Adaptive Margin Loss'), it inherits the use of a `softplus` function to weight the overall loss. This `softplus(beta * delta_cost)` term smoothly amplifies the loss for pairs with larger cost differences, focusing training on more significant preference signals.\n\nNew Coupling Ideas:\n1.  **Soft-Clipping with `tanh`:** The exponentially growing margin can become numerically unstable. To counteract this, the child introduces a 'soft-clipping' mechanism. The output of the `exp` function is passed through a `tanh` function. This allows the margin to grow exponentially for small-to-moderate cost differences but smoothly caps it at a maximum value (`max_margin`), preventing instability while retaining the steep initial growth.\n2.  **Dynamic Beta from Log-Probability Gap:** Instead of a fixed `beta` or one based on cost, this child introduces a `beta` that is dynamically adjusted based on the model's current log-probability gap (`delta_logp`). Specifically, `dynamic_beta = base_beta * exp(-delta_logp.detach())`. When the model is already confident (large positive `delta_logp`), `beta` is down-weighted, reducing the gradient and preventing over-confidence. When the model is wrong or uncertain (negative or small `delta_logp`), `beta` is increased, amplifying the learning signal. Using `.detach()` ensures this dynamic scaling doesn't introduce unwanted gradients through the `beta` term itself, focusing the gradient purely on the primary loss objective.", "hyperparams": {"base_beta": 0.5, "max_margin": 4.0, "norm_scale": 5.0}, "operators_used": ["logsigmoid", "softplus", "exp", "tanh"]}}, "better_than_baseline": false}
{"generation": 8, "index": 7, "ir": {"name": "Adaptive Margin Loss with Tanh-Normalized Log-Probabilities", "intuition": "This loss function creates a robust preference learning objective by combining an adaptive margin with a novel normalization scheme for the model's log-probabilities. \n\nIt inherits two key ideas from its parents:\n1.  From 'Adaptive Sigmoid Margin Loss with Tanh Normalization' (Parent 1), it inherits the concept of an **adaptive margin** that is a sigmoid function of the normalized cost difference (`margin = max_margin * sigmoid(...)`). This makes the required separation between winner and loser log-probabilities dependent on the magnitude of their cost difference.\n2.  From both parents, it inherits the use of a **cost-based weighting scheme** (`softplus(beta * delta_cost)`), which scales the loss to focus training on pairs with more significant cost gaps.\n\nThis child loss introduces two new coupling ideas:\n1.  **Log-Probability Normalization:** Instead of directly using the raw log-probability difference (`logp_a - logp_b`), it first normalizes this difference using a scaled `tanh` function: `norm_delta_logp = tanh((logp_a - logp_b) / logp_norm_scale)`. This bounds the influence of the model's output, preventing extremely confident (and potentially erroneous) predictions from generating huge gradients, thus improving training stability. The core loss then becomes a comparison between this normalized log-prob difference and the adaptive margin.\n2.  **Margin Clipping:** To further enhance stability, the adaptive margin is clipped to a maximum value using `clamp`. This prevents the `tanh`-normalized log-prob difference from being pushed towards an unnecessarily large target value, especially if the `max_margin` hyperparameter is set aggressively. This acts as a safeguard, ensuring the target for `norm_delta_logp` remains within a sensible range.", "pseudocode": "1. Calculate the cost difference: delta_cost = cost(b) - cost(a).\n2. Calculate the log-probability difference: delta_logp = logp(a) - logp(b).\n3. (Inherited from Parent 1) Normalize the cost difference using a scaled tanh function: norm_delta_cost = tanh(delta_cost / cost_norm_scale).\n4. (Inherited from Parent 1) Compute an adaptive margin based on the normalized cost difference: adaptive_margin = max_margin * sigmoid(norm_delta_cost).\n5. (New Coupling) Clip the adaptive margin to a maximum value for stability: margin = clamp(adaptive_margin, min=0, max=margin_clip).\n6. (New Coupling) Normalize the log-probability difference using a scaled tanh function: norm_delta_logp = tanh(delta_logp / logp_norm_scale). This bounds the model's output contribution to the loss.\n7. Compute the core preference loss by comparing the normalized log-prob difference to the clipped, adaptive margin: core_loss = -logsigmoid(norm_delta_logp - margin).\n8. (Inherited from both parents) Calculate a cost-based weight: weight_scale = softplus(beta * delta_cost).\n9. Combine them: final_loss = weight_scale * core_loss.\n10. Return the mean of final_loss over the batch.", "hyperparams": {"beta": 1.0, "max_margin": 0.8, "margin_clip": 0.95, "cost_norm_scale": 10.0, "logp_norm_scale": 5.0}, "operators_used": ["logsigmoid", "softplus", "sigmoid", "tanh", "clamp"], "implementation_hint": {"expects": ["cost_a", "cost_b", "logp_a", "logp_b"], "returns": "scalar"}, "code": "import torch\nimport torch.nn.functional as F\n\ndef generated_loss(batch, model_output, extra):\n    \"\"\"\n    Adaptive Margin Loss with Tanh-Normalized Log-Probabilities.\n\n    Args:\n        batch (dict): A dictionary containing tensors from the data loader.\n                      Expected keys: 'cost_a', 'cost_b', 'log_prob_w', 'log_prob_l'.\n                      'cost_a' and 'log_prob_w' correspond to the preferred solution (winner).\n                      'cost_b' and 'log_prob_l' correspond to the non-preferred solution (loser).\n                      An optional 'weight' tensor can be used for sample weighting.\n        model_output (dict): Output from the model (not used in this loss structure).\n        extra (dict): A dictionary for hyperparameters.\n\n    Returns:\n        torch.Tensor: A scalar tensor representing the mean loss.\n    \"\"\"\n    # Read hyperparameters\n    beta = extra.get('beta', 1.0)\n    max_margin = extra.get('max_margin', 0.8)\n    margin_clip = extra.get('margin_clip', 0.95)\n    cost_norm_scale = extra.get('cost_norm_scale', 10.0)\n    logp_norm_scale = extra.get('logp_norm_scale', 5.0)\n\n    # In our setting, 'a' is the winner (w) and 'b' is the loser (l).\n    cost_a = batch['cost_a']\n    cost_b = batch['cost_b']\n    logp_a = batch['log_prob_w']\n    logp_b = batch['log_prob_l']\n\n    # 1. Calculate cost and log-probability differences\n    delta_cost = cost_b - cost_a\n    delta_logp = logp_a - logp_b\n\n    # 2. (Inherited) Normalize cost difference with tanh for the margin calculation\n    norm_delta_cost = torch.tanh(delta_cost / cost_norm_scale)\n\n    # 3. (Inherited) Compute the adaptive sigmoid margin\n    adaptive_margin = max_margin * torch.sigmoid(norm_delta_cost)\n\n    # 4. (New Coupling) Clip the margin to a max value for stability. \n    # The target for tanh(delta_logp/scale) should not exceed 1.\n    margin = torch.clamp(adaptive_margin, min=0, max=margin_clip)\n    \n    # 5. (New Coupling) Normalize the log-probability difference with tanh\n    # This bounds the model's contribution to the loss, enhancing stability.\n    norm_delta_logp = torch.tanh(delta_logp / logp_norm_scale)\n\n    # 6. Compute the core preference loss using normalized values\n    # We push the normalized logp difference to exceed the margin.\n    core_loss = -F.logsigmoid(norm_delta_logp - margin)\n\n    # 7. (Inherited) Compute a cost-based scaling factor using softplus\n    weight_scale = F.softplus(beta * delta_cost)\n\n    # 8. Combine core loss and scaling factor\n    loss_per_sample = weight_scale * core_loss\n\n    # 9. Apply optional sample weights if provided\n    sample_weights = batch.get('weight', None)\n    if sample_weights is not None:\n        loss_per_sample = loss_per_sample * sample_weights\n\n    # 10. Return the mean loss over the batch\n    return loss_per_sample.mean()"}, "fitness": {"hf_like_score": 3.850210189819336, "validation_objective": 3.850210189819336, "generalization_penalty": 0.0, "generalization_objectives": {"20": 3.845182418823242}, "train_score_mean": 3.8434149827090733, "train_loss_mean": 0.6416661844982677, "pair_count": 152167906, "config": {"hf": {"problem": "tsp", "hf_steps": 0, "hf_epochs": 10, "hf_instances_per_epoch": 100000, "train_problem_size": 20, "valid_problem_sizes": [20], "train_batch_size": 64, "pomo_size": 20, "learning_rate": 0.0003, "weight_decay": 1e-06, "alpha": 0.05, "device": "cuda:0", "seed": 1234, "num_validation_episodes": 128, "validation_batch_size": 64, "generalization_penalty_weight": 1.0, "pool_version": "v0"}, "free_loss": {"f1_steps": 0, "total_train_steps": 15630, "f2_steps": 100, "f3_enabled": false}}, "loss_ir": {"name": "Adaptive Margin Loss with Tanh-Normalized Log-Probabilities", "intuition": "This loss function creates a robust preference learning objective by combining an adaptive margin with a novel normalization scheme for the model's log-probabilities. \n\nIt inherits two key ideas from its parents:\n1.  From 'Adaptive Sigmoid Margin Loss with Tanh Normalization' (Parent 1), it inherits the concept of an **adaptive margin** that is a sigmoid function of the normalized cost difference (`margin = max_margin * sigmoid(...)`). This makes the required separation between winner and loser log-probabilities dependent on the magnitude of their cost difference.\n2.  From both parents, it inherits the use of a **cost-based weighting scheme** (`softplus(beta * delta_cost)`), which scales the loss to focus training on pairs with more significant cost gaps.\n\nThis child loss introduces two new coupling ideas:\n1.  **Log-Probability Normalization:** Instead of directly using the raw log-probability difference (`logp_a - logp_b`), it first normalizes this difference using a scaled `tanh` function: `norm_delta_logp = tanh((logp_a - logp_b) / logp_norm_scale)`. This bounds the influence of the model's output, preventing extremely confident (and potentially erroneous) predictions from generating huge gradients, thus improving training stability. The core loss then becomes a comparison between this normalized log-prob difference and the adaptive margin.\n2.  **Margin Clipping:** To further enhance stability, the adaptive margin is clipped to a maximum value using `clamp`. This prevents the `tanh`-normalized log-prob difference from being pushed towards an unnecessarily large target value, especially if the `max_margin` hyperparameter is set aggressively. This acts as a safeguard, ensuring the target for `norm_delta_logp` remains within a sensible range.", "hyperparams": {"beta": 1.0, "max_margin": 0.8, "margin_clip": 0.95, "cost_norm_scale": 10.0, "logp_norm_scale": 5.0}, "operators_used": ["logsigmoid", "softplus", "sigmoid", "tanh", "clamp"]}}, "better_than_baseline": true}
{"generation": 9, "index": 3, "ir": {"name": "Logit-Scaled Adaptive Margin Loss", "intuition": "This loss function creates a dynamic, cost-sensitive margin inspired by its parents, but with a novel scaling mechanism. \n\nInherited Ideas:\n1.  From both parents, it inherits the core idea of using a logistic loss (`-logsigmoid`) where the model's log-probability difference is pushed to exceed an adaptive margin. The margin's size is a function of the cost difference between the preferred and non-preferred solutions (`delta_cost = cost_b - cost_a`).\n2.  From both parents, it also inherits the idea of scaling the entire loss term by a function of the cost difference to focus training on more significant preference pairs. Here, we use `softplus(beta * delta_cost)` for this purpose.\n\nNew Coupling Ideas:\n1.  **Logit-based Margin Scaling**: Instead of normalizing `delta_cost` with `tanh` or `zscore`, this child loss introduces a new margin mechanism. It first computes a normalized preference strength `p = sigmoid(delta_cost / temp)`. This maps the cost difference to a probability-like value in (0, 1). The margin is then derived from the logit of this value, `log(p / (1-p))`, which is simply `delta_cost / temp`. This creates a margin that is linearly proportional to the cost difference, which is simpler and more direct than the sigmoid-of-tanh approach of the parents. The `temp` parameter controls the steepness.\n2.  **Reward Difference Clipping**: To enhance stability, especially when `delta_logp` is very large or very small, the loss applies `tanh` to the core learning signal `(delta_logp - margin)`. This bounds the argument to `logsigmoid` within a stable range, preventing extreme gradients from destabilizing training. The `tanh` acts as a soft clamp on the reward difference signal, ensuring it doesn't grow uncontrollably while preserving the sign of the gradient.", "pseudocode": "1. Calculate the cost difference: delta_cost = cost(b) - cost(a).\n2. Calculate the log-probability difference: delta_logp = logp(a) - logp(b).\n3. (Inherited) Use a logistic loss framework based on the difference between `delta_logp` and a margin.\n4. (New Coupling 1) Compute an adaptive margin that is linearly proportional to the cost difference, scaled by a temperature: margin = delta_cost / temp. This is mathematically equivalent to taking the logit of a sigmoid-transformed cost difference.\n5. (New Coupling 2) Compute the core learning signal and stabilize it by applying a `tanh` function: stable_signal = tanh(delta_logp - margin). This prevents extreme values from causing numerical instability.\n6. Compute the core loss using the stabilized signal: core_loss = -logsigmoid(stable_signal).\n7. (Inherited) Calculate a cost-based weight to amplify the loss for more significant preferences: weight_scale = softplus(beta * delta_cost).\n8. Combine the weight and core loss: final_loss = weight_scale * core_loss.\n9. Return the mean of final_loss over the batch.", "hyperparams": {"beta": 1.0, "temp": 5.0}, "operators_used": ["logsigmoid", "softplus", "tanh"], "implementation_hint": {"expects": ["cost_a", "cost_b", "logp_a", "logp_b"], "returns": "scalar"}, "code": "import torch\nimport torch.nn.functional as F\n\ndef generated_loss(batch, model_output, extra):\n    \"\"\"\n    Logit-Scaled Adaptive Margin Loss.\n\n    Args:\n        batch (dict): A dictionary containing tensors from the data loader.\n                      Expected keys: 'cost_a', 'cost_b', 'log_prob_w', 'log_prob_l'.\n                      'cost_a' and 'log_prob_w' correspond to the preferred solution (winner).\n                      'cost_b' and 'log_prob_l' correspond to the non-preferred solution (loser).\n                      An optional 'weight' tensor can be used for sample weighting.\n        model_output (dict): Output from the model (not used in this loss structure).\n        extra (dict): A dictionary for hyperparameters, e.g., {'beta': 1.0, 'temp': 5.0}.\n\n    Returns:\n        torch.Tensor: A scalar tensor representing the mean loss.\n    \"\"\"\n    # Read hyperparameters\n    beta = extra.get('beta', 1.0)\n    temp = extra.get('temp', 5.0)\n\n    # In our setting, 'a' is the winner (w) and 'b' is the loser (l).\n    # cost_a < cost_b\n    cost_a = batch['cost_a']\n    cost_b = batch['cost_b']\n    logp_a = batch['log_prob_w']\n    logp_b = batch['log_prob_l']\n\n    # 1. Calculate cost and log-probability differences\n    # delta_cost should be positive as cost_b > cost_a\n    delta_cost = cost_b - cost_a\n    delta_logp = logp_a - logp_b\n\n    # 2. (New Coupling 1) Compute an adaptive margin linearly proportional to delta_cost\n    # This is equivalent to logit(sigmoid(delta_cost/temp)), providing a direct scaling.\n    # Add a small epsilon to temp to prevent division by zero.\n    margin = delta_cost / (temp + 1e-8)\n\n    # 3. (New Coupling 2) Stabilize the core learning signal using tanh\n    # This bounds the input to logsigmoid, preventing extreme gradients.\n    stable_signal = torch.tanh(delta_logp - margin)\n\n    # 4. (Inherited) Compute the core preference loss (Bradley-Terry style)\n    # We want delta_logp > margin, so (delta_logp - margin) should be positive.\n    core_loss = -F.logsigmoid(stable_signal)\n\n    # 5. (Inherited) Compute a cost-based scaling factor using softplus for stability\n    # This amplifies the loss for pairs with larger raw cost differences.\n    weight_scale = F.softplus(beta * delta_cost)\n\n    # 6. Combine core loss and scaling factor\n    loss_per_sample = weight_scale * core_loss\n\n    # 7. Apply optional sample weights if provided\n    sample_weights = batch.get('weight', None)\n    if sample_weights is not None:\n        loss_per_sample = loss_per_sample * sample_weights\n\n    # 8. Return the mean loss over the batch\n    return loss_per_sample.mean()"}, "fitness": {"hf_like_score": 3.844866394996643, "validation_objective": 3.844866394996643, "generalization_penalty": 0.0, "generalization_objectives": {"20": 3.840610980987549}, "train_score_mean": 4.18932978688946, "train_loss_mean": 0.39699264636462267, "pair_count": 189654054, "config": {"hf": {"problem": "tsp", "hf_steps": 0, "hf_epochs": 10, "hf_instances_per_epoch": 100000, "train_problem_size": 20, "valid_problem_sizes": [20], "train_batch_size": 64, "pomo_size": 20, "learning_rate": 0.0003, "weight_decay": 1e-06, "alpha": 0.05, "device": "cuda:0", "seed": 1234, "num_validation_episodes": 128, "validation_batch_size": 64, "generalization_penalty_weight": 1.0, "pool_version": "v0"}, "free_loss": {"f1_steps": 0, "total_train_steps": 15630, "f2_steps": 100, "f3_enabled": false}}, "loss_ir": {"name": "Logit-Scaled Adaptive Margin Loss", "intuition": "This loss function creates a dynamic, cost-sensitive margin inspired by its parents, but with a novel scaling mechanism. \n\nInherited Ideas:\n1.  From both parents, it inherits the core idea of using a logistic loss (`-logsigmoid`) where the model's log-probability difference is pushed to exceed an adaptive margin. The margin's size is a function of the cost difference between the preferred and non-preferred solutions (`delta_cost = cost_b - cost_a`).\n2.  From both parents, it also inherits the idea of scaling the entire loss term by a function of the cost difference to focus training on more significant preference pairs. Here, we use `softplus(beta * delta_cost)` for this purpose.\n\nNew Coupling Ideas:\n1.  **Logit-based Margin Scaling**: Instead of normalizing `delta_cost` with `tanh` or `zscore`, this child loss introduces a new margin mechanism. It first computes a normalized preference strength `p = sigmoid(delta_cost / temp)`. This maps the cost difference to a probability-like value in (0, 1). The margin is then derived from the logit of this value, `log(p / (1-p))`, which is simply `delta_cost / temp`. This creates a margin that is linearly proportional to the cost difference, which is simpler and more direct than the sigmoid-of-tanh approach of the parents. The `temp` parameter controls the steepness.\n2.  **Reward Difference Clipping**: To enhance stability, especially when `delta_logp` is very large or very small, the loss applies `tanh` to the core learning signal `(delta_logp - margin)`. This bounds the argument to `logsigmoid` within a stable range, preventing extreme gradients from destabilizing training. The `tanh` acts as a soft clamp on the reward difference signal, ensuring it doesn't grow uncontrollably while preserving the sign of the gradient.", "hyperparams": {"beta": 1.0, "temp": 5.0}, "operators_used": ["logsigmoid", "softplus", "tanh"]}}, "better_than_baseline": true}
