{"generation": 0, "index": 0, "ir": {"name": "SigmoidAttenuatedMarginLoss", "intuition": "This loss function, named Sigmoid Attenuated Margin Loss (SAML), introduces a dynamic, cost-difference-aware margin into a logistic-style preference loss. The core idea is that the 'correctness' of the model's preference (logp(a) > logp(b)) should be judged more strictly when the true cost difference between the preferred solution 'a' and the other solution 'b' is large. It uses a sigmoid function to transform the normalized cost difference into a dynamic margin between 0 and a maximum value `m`. When cost(a) is much better than cost(b), the margin approaches `m`, demanding a large log probability difference from the model. When the costs are very close, the margin approaches 0, relaxing the requirement and preventing the model from over-optimizing on trivial differences. The entire expression is wrapped in `softplus` for smoothness and non-negativity, ensuring stability and preventing gradient explosion from extreme logit differences.", "pseudocode": "1. For a batch of solution pairs (a, b), compute cost differences: \u0394cost = cost(a) - cost(b).\n2. Normalize the cost differences across the batch using z-score to get \u0394cost_norm. This makes the scaling robust to the absolute magnitude of costs.\n3. Compute a dynamic margin `margin_dyn` for each pair. This is done by passing `-\u0394cost_norm` through a sigmoid function and scaling it by a hyperparameter `m`. The sigmoid ensures the margin is bounded between 0 and `m`. We use `-\u0394cost_norm` so that a larger cost improvement (more negative \u0394cost) results in a larger positive margin.\n4. Compute the log probability difference: \u0394logp = logp(a) - logp(b).\n5. The core of the loss is `softplus(margin_dyn - \u0394logp)`. This structure is inspired by margin-based losses and the logistic loss. It penalizes the model if `\u0394logp` is not greater than the dynamic margin `margin_dyn`.\n6. The final loss is the mean of this value over the batch.", "hyperparams": {"m": {"value": 2.0, "description": "The maximum margin. Controls the upper bound of the required log probability separation for pairs with large cost differences."}}, "operators_used": ["zscore", "sigmoid", "softplus"], "implementation_hint": {"expects": ["{'costs_a': \"A 1D tensor of costs for solution 'a' in each pair of a batch. Shape: (batch_size,).\", 'costs_b': \"A 1D tensor of costs for solution 'b' in each pair of a batch. Shape: (batch_size,).\", 'logps_a': \"A 1D tensor of log probabilities for solution 'a'. Shape: (batch_size,).\", 'logps_b': \"A 1D tensor of log probabilities for solution 'b'. Shape: (batch_size,).\"}"], "returns": "A single scalar value representing the mean loss for the batch."}}, "static_ok": false, "static_reason": "implementation_hint.returns must be 'scalar'."}
{"generation": 0, "index": 1, "ir": {"name": "AdaptiveMarginHingeLoss", "intuition": "This loss function, named Adaptive Margin Hinge Loss, is designed to dynamically adjust the learning pressure based on the magnitude of the cost difference between two solutions. It combines the stability of a hinge-loss-like structure (using softplus) with a margin that adapts to the normalized cost gap. The core idea is that when one solution is significantly better than another (large cost gap), the model should be penalized more heavily for preferring the worse one. Conversely, for solutions with very similar costs, the loss should be small, preventing the model from overfitting to noisy or insignificant preferences. The use of `tanh` on the logit difference and `rank_gap` on costs ensures that extreme input values are squashed into a bounded range, guaranteeing numerical stability.", "pseudocode": "1. For a batch of solution pairs (a, b), compute the cost gap for each pair: \u0394cost = cost(b) - cost(a).\n2. Normalize these cost gaps across the batch using z-score to get `normalized_cost_gap`. This makes the scale independent of the absolute cost values.\n3. Compute an adaptive margin for each pair: `margin = margin_base + margin_scale * relu(normalized_cost_gap)`. The margin is larger for pairs where 'a' is significantly better than 'b'.\n4. Compute the log probability difference for each pair: `logit_diff = logp(a) - logp(b)`.\n5. To ensure stability, squash the logit difference using tanh: `squashed_logit_diff = tanh(logit_diff / temp)`. The temperature `temp` controls the steepness.\n6. The core of the loss is a softplus-based hinge loss. The term inside the softplus is `margin - squashed_logit_diff`. This term is large when the model incorrectly prefers 'b' (logit_diff is negative) or doesn't prefer 'a' enough (logit_diff is smaller than the margin).\n7. The final loss for a pair is `softplus(margin - squashed_logit_diff)`. The total loss is the mean over the batch.", "hyperparams": {"margin_base": 0.1, "margin_scale": 0.5, "temp": 2.0}, "operators_used": ["zscore", "relu", "tanh", "softplus"], "implementation_hint": {"expects": ["{'pair_costs_a': \"A 1D tensor of costs for solution 'a' for each pair in a batch.\", 'pair_costs_b': \"A 1D tensor of costs for solution 'b' for each pair in a batch.\", 'pair_logp_a': \"A 1D tensor of log probabilities for solution 'a' for each pair.\", 'pair_logp_b': \"A 1D tensor of log probabilities for solution 'b' for each pair.\"}"], "returns": "A single scalar value representing the mean loss for the batch."}}, "static_ok": false, "static_reason": "implementation_hint.returns must be 'scalar'."}
{"generation": 0, "index": 2, "ir": {"name": "AdaptiveMarginHingeLoss", "intuition": "This loss function, named Adaptive Margin Hinge Loss, is designed for stable and robust preference learning in combinatorial optimization. Its core idea is to combine a classic hinge loss structure with an adaptive margin that is sensitive to the magnitude of the cost difference. When the cost difference is small, the margin is also small, allowing the model to focus on clear-cut cases. When the cost difference is large, the margin increases, creating a stronger push for the model to align its preferences with the ground truth. The `tanh` function is used to scale both the log probability difference and the cost difference. This squashes extreme values into a bounded range (e.g., [-1, 1]), preventing the loss from exploding due to outliers and ensuring numerical stability. The `softplus` function ensures the final loss is non-negative and smooth, providing a well-behaved gradient for backpropagation.", "pseudocode": "def AdaptiveMarginHingeLoss(cost_a, cost_b, logp_a, logp_b, alpha, beta, tau):\n    # 1. Calculate the difference in log probabilities.\n    logp_diff = logp_a - logp_b\n\n    # 2. Normalize and bound the log probability difference using tanh for stability.\n    # alpha controls the steepness of the tanh curve for logp_diff.\n    scaled_logp_diff = tanh(alpha * logp_diff)\n\n    # 3. Calculate the rank-based cost difference.\n    # rank_gap(a, b) returns 1 if a < b, -1 if a > b, 0 if a == b.\n    cost_rank_gap = rank_gap(cost_a, cost_b)\n\n    # 4. Calculate the magnitude of the cost difference and scale it.\n    # This will serve as the basis for our adaptive margin.\n    # beta controls the sensitivity to the cost difference magnitude.\n    cost_magnitude = abs(cost_a - cost_b)\n    adaptive_margin = tanh(beta * cost_magnitude)\n\n    # 5. Combine the components into a hinge-like structure.\n    # The target is to have (scaled_logp_diff * cost_rank_gap) be positive.\n    # We want scaled_logp_diff to be greater than -adaptive_margin when cost_rank_gap is -1 (i.e., a is worse),\n    # and less than adaptive_margin when cost_rank_gap is 1 (i.e., a is better).\n    # This can be formulated as making `cost_rank_gap * scaled_logp_diff` large.\n    # The hinge loss term is `margin - y*f(x)`. Here, y is cost_rank_gap, f(x) is scaled_logp_diff.\n    hinge_term = adaptive_margin - cost_rank_gap * scaled_logp_diff\n\n    # 6. Apply softplus for a smooth, non-negative loss. The temperature `tau` controls smoothness.\n    # softplus(x) = log(1 + exp(x)). It's a smooth approximation of relu(x).\n    loss = softplus(hinge_term / tau)\n\n    return loss", "hyperparams": {"alpha": 1.0, "beta": 0.1, "tau": 0.5}, "operators_used": ["tanh", "softplus", "rank_gap"], "implementation_hint": {"expects": ["{'cost_a': \"Tensor of costs for solution 'a' in each pair.\", 'cost_b': \"Tensor of costs for solution 'b' in each pair.\", 'logp_a': \"Tensor of model-assigned log probabilities for solution 'a'.\", 'logp_b': \"Tensor of model-assigned log probabilities for solution 'b'.\"}"], "returns": "A scalar loss value, typically the mean of the loss over the batch."}}, "static_ok": false, "static_reason": "implementation_hint.returns must be 'scalar'."}
{"generation": 0, "index": 3, "ir": {"name": "AdaptiveMarginHingeLoss", "intuition": "This loss function, named 'Adaptive Margin Hinge Loss', is designed for robust preference learning in combinatorial optimization. It improves upon the standard hinge loss by incorporating an adaptive margin mechanism. The margin is dynamically scaled by the normalized cost difference between two solutions. When the cost difference is large, the margin is also large, enforcing a stronger separation between the log probabilities of the good and bad solutions. Conversely, when the cost difference is small (i.e., the solutions are of similar quality), the margin shrinks, allowing the model more flexibility and preventing it from being overly penalized for minor preference misalignments. The use of `tanh` on the log probability difference and `clamp` on the cost gap ensures that the loss remains bounded and numerically stable, even with extreme input values. This approach makes the training more stable and focuses the learning signal on meaningful preferences.", "pseudocode": "def AdaptiveMarginHingeLoss(cost_a, cost_b, logp_a, logp_b, beta, tau):\n  # Assume cost_a is preferred over cost_b (cost_a < cost_b)\n  cost_gap = cost_b - cost_a\n  \n  # 1. Normalize and clamp the cost difference to create a stable scaling factor between 0 and 1\n  #    This represents the 'confidence' in the preference.\n  #    Using clamp makes it robust against extremely large cost gaps.\n  adaptive_margin_scale = clamp(cost_gap / tau, 0.0, 1.0)\n  \n  # 2. Calculate the difference in log probabilities\n  logp_diff = logp_a - logp_b\n  \n  # 3. Use tanh to bound the log probability difference between -1 and 1.\n  #    This prevents extreme logit differences from causing the loss to explode.\n  bounded_logp_diff = tanh(logp_diff / beta)\n  \n  # 4. Construct the hinge loss. The loss is incurred only if the bounded log probability difference\n  #    is smaller than the adaptive margin. The model is penalized for not preferring the better solution 'enough'.\n  loss = relu(adaptive_margin_scale - bounded_logp_diff)\n  \n  return loss", "hyperparams": {"beta": 2.0, "tau": 10.0}, "operators_used": ["clamp", "tanh", "relu"], "implementation_hint": {"expects": ["{'preferred_cost': 'A tensor of costs for the preferred solutions in a batch.', 'dispreferred_cost': 'A tensor of costs for the dispreferred solutions in a batch.', 'preferred_logp': 'A tensor of log probabilities for the preferred solutions.', 'dispreferred_logp': 'A tensor of log probabilities for the dispreferred solutions.'}"], "returns": "A scalar representing the mean loss over the batch."}}, "static_ok": false, "static_reason": "implementation_hint.returns must be 'scalar'."}
{"generation": 1, "index": 0, "ir": {"name": "AdaptiveMarginHingeLoss", "intuition": "The core idea is to create a hinge-like loss where the 'margin' is not fixed, but dynamically adapts to the magnitude of the cost difference between two solutions. When the cost difference is large, the model is strongly penalized for disagreeing with this clear preference. When the cost difference is small, the margin is also small, allowing the model more flexibility and preventing it from over-optimizing on noisy or insignificant preferences. The loss is asymmetric: it heavily penalizes the model for preferring the worse solution but gives zero penalty if the model's preference for the better solution exceeds the adaptive margin. The use of `tanh` on the logit difference and cost difference ensures that extreme input values are squashed into a bounded range, guaranteeing numerical stability.", "pseudocode": "1. Calculate the log probability difference: `logit_diff = logp_a - logp_b`.\n2. Calculate the cost difference: `cost_diff = cost_b - cost_a`.\n3. Normalize the cost difference into a stable range, e.g., `[-1, 1]`, using `tanh`. This represents the strength of the ground-truth preference: `pref_strength = tanh(cost_diff / temp)`. `temp` is a temperature hyperparameter to control sensitivity.\n4. Define an adaptive margin based on this preference strength. The margin is scaled by `margin_scale` and is larger for clearer preferences: `adaptive_margin = margin_scale * pref_strength`.\n5. Calculate the 'preference violation' as the difference between the adaptive margin and the model's preference (also squashed by `tanh` for stability): `violation = adaptive_margin - tanh(logit_diff)`.\n6. Apply a ReLU-like activation (`softplus` or `relu`) to only penalize violations, i.e., when `logit_diff` is smaller than the `adaptive_margin`. The final loss is `softplus(violation)`.", "hyperparams": {"temp": 10.0, "margin_scale": 1.0}, "operators_used": ["tanh", "softplus"], "implementation_hint": {"expects": ["{'logp_a': 'Scalar log probability of solution a.', 'logp_b': 'Scalar log probability of solution b.', 'cost_a': 'Scalar cost of solution a (lower is better).', 'cost_b': 'Scalar cost of solution b (lower is better).'}"], "returns": "scalar_loss"}}, "static_ok": false, "static_reason": "implementation_hint.returns must be 'scalar'."}
{"generation": 1, "index": 1, "ir": {"name": "AdaptiveMarginHingeLoss", "intuition": "This loss function, named 'Adaptive Margin Hinge Loss', is designed for stable preference learning in combinatorial optimization. Its core idea is to create a dynamic margin that adapts to the magnitude of the cost difference between two solutions. When the cost difference is large, the model is strongly penalized for not preferring the better solution. When the cost difference is small (i.e., the solutions are of similar quality), the margin shrinks, providing a 'tolerance zone' where the model is not heavily penalized for minor preference disagreements. This is achieved by combining a hinge-like structure (using `relu`) with a margin term modulated by the tanh-scaled cost difference. The `tanh` function ensures that the margin is bounded, preventing extreme cost differences from causing exploding gradients. The `logsigmoid` function is applied to the logit difference to maintain a consistent preference direction, similar to DPO, but the core innovation lies in how the cost difference (`\u0394cost`) modulates the penalty's magnitude rather than just scaling the log-probability term.", "pseudocode": "def adaptive_margin_hinge_loss(cost_a, cost_b, logp_a, logp_b, beta, margin_scale):\n  # Ensure cost_a is the better solution (lower cost)\n  if cost_a > cost_b:\n    cost_a, cost_b = cost_b, cost_a\n    logp_a, logp_b = logp_b, logp_a\n\n  # Calculate cost and log probability differences\n  delta_cost = cost_b - cost_a  # This is always >= 0\n  logp_diff = logp_a - logp_b\n\n  # Create an adaptive margin based on the normalized cost difference.\n  # tanh ensures the margin is bounded between 0 and margin_scale.\n  adaptive_margin = margin_scale * tanh(delta_cost)\n\n  # The core loss structure.\n  # We want logp_diff to be positive. The loss is activated if logp_diff is smaller than the adaptive_margin.\n  # The term inside relu is `margin - logp_diff`, which is positive (and thus incurs a loss) \n  # only when the model's preference `logp_diff` is not strong enough to overcome the `adaptive_margin`.\n  hinge_term = relu(adaptive_margin - logp_diff)\n\n  # Use logsigmoid on the logit difference to add a base preference signal, scaled by beta.\n  # This term always encourages logp_a > logp_b, similar to DPO.\n  preference_term = -logsigmoid(beta * logp_diff)\n\n  # Combine the hinge term and the base preference term.\n  loss = hinge_term + preference_term\n  return loss", "hyperparams": {"beta": {"value": 1.0, "description": "Scaling factor for the log-probability difference, controlling the strength of the base preference signal (similar to DPO's beta)."}, "margin_scale": {"value": 5.0, "description": "The maximum possible margin value. This caps the influence of the cost difference, ensuring numerical stability."}}, "operators_used": ["tanh", "relu", "logsigmoid"], "implementation_hint": {"expects": ["{'cost_w': 'Scalar cost of the winning/preferred solution in a pair.', 'cost_l': 'Scalar cost of the losing/dispreferred solution in a pair.', 'logp_w': 'Log probability of the winning solution.', 'logp_l': 'Log probability of the losing solution.'}"], "returns": "A single scalar loss value for the pair."}}, "static_ok": false, "static_reason": "implementation_hint.returns must be 'scalar'."}
{"generation": 1, "index": 2, "ir": {"name": "Rank-Aware Logarithmic Hinge Loss", "intuition": "The core idea is to create a soft 'hinge-like' loss that penalizes the model when its preference (logit difference) contradicts the ground truth cost difference. The penalty is scaled by how 'wrong' it is, but this scaling is done in a controlled, non-explosive way. Specifically, the cost difference is transformed into a 'rank gap' and then normalized, making the loss robust to the absolute scale of costs. The logit difference is also passed through a tanh function to bound its influence. The final loss is constructed using `softplus`, which acts like a smoothed ReLU (hinge), ensuring the loss is non-negative and only applies a penalty when the model's preference is misaligned with the cost-based preference. This design avoids issues with extreme cost/logit differences and provides a smooth, stable gradient for backpropagation.", "pseudocode": "def rank_aware_log_hinge_loss(cost_a, cost_b, logp_a, logp_b, alpha, margin):\n  # 1. Calculate cost difference using a rank-aware, normalized metric.\n  # rank_gap(a, b) = sign(a - b). This captures only the preference direction.\n  cost_direction = rank_gap(cost_a, cost_b)  # Returns -1 if a<b, 1 if b<a, 0 if equal\n\n  # 2. Calculate the log probability difference and bound it for stability.\n  logit_diff = logp_a - logp_b\n  bounded_logit_diff = tanh(logit_diff) # Bound between -1 and 1\n\n  # 3. Combine cost and logit differences. The sign should be negative when they disagree.\n  # If cost_a < cost_b (direction=-1) and logp_a < logp_b (logit_diff<0), we want the argument to be negative.\n  # Argument = -cost_direction * logit_diff. \n  # Example: a is better (cost_direction=-1), but model prefers b (logit_diff<0). Argument = -(-1)*(-ve) = -ve. This is wrong, should be penalized.\n  # Let's adjust: Argument = cost_direction * logit_diff.\n  # Example: a is better (cost_direction=-1), model prefers b (logit_diff<0). Argument = (-1)*(-ve) = +ve. This is a mis-prediction, softplus will be > 0.\n  # Example: a is better (cost_direction=-1), model prefers a (logit_diff>0). Argument = (-1)*(+ve) = -ve. This is correct, softplus will be close to 0.\n  argument = cost_direction * logit_diff\n\n  # 4. Apply a scaled hinge-like loss using softplus.\n  # softplus(x) = log(1 + exp(x)). It's a smooth version of max(0, x).\n  # We want to penalize mis-predictions, which correspond to a positive `argument`.\n  # We use softplus(-argument + margin) to create a margin-based hinge loss.\n  # The loss is high if argument is very negative (strong disagreement).\n  # Let's re-verify. We want loss when cost_direction and logit_diff have *opposite* signs. This means their product is negative. \n  # So we want to penalize `cost_direction * logit_diff < 0`. This is equivalent to `-(cost_direction * logit_diff) > 0`.\n  # Therefore, the argument to softplus should be `alpha * (-cost_direction * logit_diff + margin)`.\n  # Let's use the bounded version for stability.\n  argument_for_loss = -cost_direction * bounded_logit_diff\n  loss = softplus(alpha * (argument_for_loss + margin))\n\n  return loss", "hyperparams": {"alpha": 10.0, "margin": 0.1}, "operators_used": ["rank_gap", "tanh", "softplus"], "implementation_hint": {"expects": ["pair_cost_a", "pair_cost_b", "pair_logp_a", "pair_logp_b"], "returns": "scalar_loss"}}, "static_ok": false, "static_reason": "implementation_hint.returns must be 'scalar'."}
{"generation": 1, "index": 3, "ir": {"name": "AdaptiveMarginHingeLoss", "intuition": "This loss function, named Adaptive Margin Hinge Loss, combines the principles of hinge loss with an adaptive margin mechanism. The core idea is that the required 'confidence' of the model's preference (logit difference) should scale with the 'obviousness' of the ground-truth preference (cost difference). When the cost difference between two solutions is large, the model should be penalized more heavily for getting the preference wrong. The margin is made 'adaptive' by normalizing the cost difference, preventing it from becoming excessively large and causing numerical instability. We use `tanh` to smoothly bound the normalized cost difference, creating a margin that adapts within a [-1, 1] range. The `softplus` function is used instead of a hard `relu` (max(0, x)) to create a smoothed hinge loss, which has better gradient properties around zero. This design ensures the loss is stable, differentiable, and correctly encourages the model to align its probability distribution with the cost-based preferences.", "pseudocode": "def AdaptiveMarginHingeLoss(cost_a, cost_b, logp_a, logp_b, alpha, beta):\n    # 1. Calculate cost and log probability differences\n    cost_diff = cost_a - cost_b  # negative if a is better\n    logp_diff = logp_a - logp_b\n\n    # 2. Compute a normalized and bounded 'adaptive margin' from the cost difference.\n    # rank_gap normalizes the difference to a [-1, 1] range based on their relative ordering and gap.\n    # This is more robust than simple subtraction for varying cost scales.\n    adaptive_margin = tanh(beta * rank_gap(cost_a, cost_b))\n    # 'adaptive_margin' is negative if cost_a < cost_b, positive otherwise.\n    # It approaches -1 when a is much better than b, and +1 when b is much better than a.\n\n    # 3. Formulate the core hinge loss term.\n    # The target is to have logp_diff's sign match the margin's sign.\n    # We want logp_diff to be > adaptive_margin if a is better, but since the margin is negative,\n    # it's cleaner to formulate it as: we want (logp_diff - adaptive_margin) to be positive.\n    # The loss is incurred when this is not the case.\n    # Let's flip the sign to match the standard preference loss form where we minimize -log_sigmoid(pi_w - pi_r).\n    # We want to push `logp_diff` towards the `adaptive_margin`.\n    # Let's define the argument for the loss: `x = -logp_diff + adaptive_margin`.\n    # If `a` is preferred (cost_a < cost_b), `adaptive_margin` is negative.\n    # If the model agrees (logp_a > logp_b -> logp_diff > 0), then `x` is negative, and loss is small.\n    # If the model disagrees (logp_a < logp_b -> logp_diff < 0), then `x` is positive, and loss is large.\n    # This is incorrect. Let's rethink the hinge logic.\n    # Hinge loss is max(0, margin - y*f(x)). Here, y is sign(cost_b - cost_a), f(x) is logp_diff.\n    # Let's use a simpler, more direct formulation with softplus, which is smooth relu.\n    # We want to penalize `-(logp_diff - adaptive_margin)` when `a` is better.\n    # Let's define the argument `x` such that it's positive when there's a violation.\n    # If cost_a < cost_b, we want logp_a > logp_b. A violation occurs if `logp_b > logp_a`.\n    # The margin is `adaptive_margin`, which is negative.\n    # The violation term is `adaptive_margin - logp_diff`.\n    # If logp_diff is large and positive (correct), this term is very negative -> softplus(negative) is near 0.\n    # If logp_diff is negative (incorrect), this term is `(negative) - (negative)`. If `|logp_diff|` is large, this can be positive, incurring loss.\n    argument = adaptive_margin - logp_diff\n\n    # 4. Apply a scaled softplus for a smooth, non-negative loss.\n    # softplus(x) = log(1 + exp(x)). It's a smooth version of relu(x).\n    loss = softplus(alpha * argument)\n\n    return loss", "hyperparams": {"alpha": 10.0, "beta": 1.0}, "operators_used": ["rank_gap", "tanh", "softplus"], "implementation_hint": {"expects": ["pair_cost_a", "pair_cost_b", "pair_logp_a", "pair_logp_b"], "returns": "scalar_loss"}}, "static_ok": false, "static_reason": "implementation_hint.returns must be 'scalar'."}
{"generation": 1, "index": 4, "ir": {"name": "SigmoidAttenuatedMarginLoss", "intuition": "This loss function aims to create a dynamic, cost-sensitive margin. The core idea is that the required log-probability gap between a better solution (a) and a worse one (b) should be proportional to their cost difference. We use a sigmoid function on the normalized cost difference to create a 'target margin' that is bounded between 0 and a hyperparameter `beta`. This prevents extremely large cost differences from demanding an infinitely large log-probability gap, ensuring stability. The loss is then a softplus of this dynamic margin minus the actual log-probability difference. This structure robustly encourages the model's preference to align with the ground-truth cost ordering while gracefully handling both small and large cost/logit differences.", "pseudocode": "['// Given costs for a pair of solutions (cost_a, cost_b) and their log probabilities (logp_a, logp_b)', '// Assume cost_a < cost_b without loss of generality (preferred > unpreferred)', 'logit_diff = logp_a - logp_b', '', '// 1. Calculate the rank-based gap. This is robust to cost scaling.', 'cost_gap = rank_gap(cost_a, cost_b) // This will be positive since cost_a < cost_b', '', '// 2. Normalize the cost gap using tanh. This maps any gap to (-1, 1).', '// We scale it by `alpha` to control the steepness of the margin curve.', 'normalized_gap = tanh(alpha * cost_gap)', '', '// 3. Create a dynamic, bounded margin using the normalized gap.', \"// The margin `m` will be in [0, beta]. It's small for small cost gaps and saturates at `beta` for large gaps.\", \"// This is the 'target' log probability difference the model should achieve.\", 'dynamic_margin = beta * normalized_gap', '', '// 4. Calculate the final loss using softplus for smoothness and stability.', '// The loss is incurred if the actual logit_diff is less than the dynamic_margin.', '// softplus(x) = log(1 + exp(x))', 'loss = softplus(dynamic_margin - logit_diff)']", "hyperparams": {"alpha": 0.1, "beta": 5.0}, "operators_used": ["rank_gap", "tanh", "softplus"], "implementation_hint": {"expects": ["{'cost_w': 'Scalar cost of the winning/preferred solution (e.g., cost_a).', 'cost_l': 'Scalar cost of the losing/unpreferred solution (e.g., cost_b).', 'logp_w': 'Scalar log probability of the winning solution.', 'logp_l': 'Scalar log probability of the losing solution.'}"], "returns": "scalar_loss"}}, "static_ok": false, "static_reason": "implementation_hint.returns must be 'scalar'."}
{"generation": 1, "index": 5, "ir": {"name": "SigmoidScaledMarginRankLoss", "intuition": "\u8be5\u635f\u5931\u51fd\u6570\u7684\u6838\u5fc3\u601d\u60f3\u662f\u5c06\u6210\u672c\u5dee\u5f02\uff08cost difference\uff09\u6620\u5c04\u5230\u4e00\u4e2a\u6709\u754c\u7684\u3001\u975e\u7ebf\u6027\u7684\u7f29\u653e\u56e0\u5b50\uff0c\u7528\u4e8e\u8c03\u6574\u504f\u597d\u5b66\u4e60\u7684\u5f3a\u5ea6\u3002\u5177\u4f53\u800c\u8a00\uff0c\u5b83\u4f7f\u7528 sigmoid \u51fd\u6570\u5c06\u5f52\u4e00\u5316\u540e\u7684\u6210\u672c\u5dee\u5f02 `\u0394cost` \u8f6c\u6362\u5230\u4e00\u4e2a (0, 1) \u533a\u95f4\u7684\u7f29\u653e\u56e0\u5b50 `s`\u3002\u8fd9\u4e2a\u56e0\u5b50 `s` \u968f\u540e\u4e58\u4ee5\u5bf9\u6570\u6982\u7387\u5dee `\u0394logp`\uff0c\u5e76\u4e0e\u4e00\u4e2a\u56fa\u5b9a\u7684\u95f4\u9694\uff08margin\uff09\u76f8\u7ed3\u5408\u3002\u8fd9\u79cd\u8bbe\u8ba1\u7684\u4f18\u70b9\u662f\uff1a1) \u5f53\u6210\u672c\u5dee\u5f02\u975e\u5e38\u5927\u65f6\uff0c\u7f29\u653e\u56e0\u5b50 `s` \u8d8b\u8fd1\u4e8e1\uff0c\u635f\u5931\u63a5\u8fd1\u4e8e\u4e00\u4e2a\u6807\u51c6\u7684\u95f4\u9694\u6392\u540d\u635f\u5931\uff08hinge loss\uff09\uff0c\u63d0\u4f9b\u4e86\u5f3a\u70c8\u7684\u76d1\u7763\u4fe1\u53f7\uff1b2) \u5f53\u6210\u672c\u5dee\u5f02\u5f88\u5c0f\u65f6\uff0c`s` \u8d8b\u8fd1\u4e8e0\uff0c\u6709\u6548\u51cf\u5f31\u4e86\u5bf9\u566a\u58f0\u6216\u5fae\u5c0f\u6539\u8fdb\u7684\u8fc7\u62df\u5408\uff0c\u4f7f\u6a21\u578b\u66f4\u5173\u6ce8\u663e\u8457\u7684\u6539\u8fdb\uff1b3) \u4f7f\u7528 softplus \u51fd\u6570\u786e\u4fdd\u635f\u5931\u59cb\u7ec8\u4e3a\u975e\u8d1f\uff0c\u5e76\u4e14\u5728\u6ee1\u8db3\u504f\u597d\u65f6\uff08\u5373 `\u0394logp` \u8db3\u591f\u5927\uff09\u635f\u5931\u8d8b\u8fd1\u4e8e\u96f6\uff0c\u540c\u65f6\u51fd\u6570\u672c\u8eab\u662f\u5e73\u6ed1\u7684\uff0c\u6709\u5229\u4e8e\u4f18\u5316\u3002\u6574\u4e2a\u7ed3\u6784\u901a\u8fc7 clamp \u548c softplus \u4fdd\u8bc1\u4e86\u6570\u503c\u7a33\u5b9a\u6027\u3002", "pseudocode": "def loss(cost_a, cost_b, logp_a, logp_b, beta, margin):\n    # 1. \u8ba1\u7b97\u6210\u672c\u5dee\u5f02\uff0c\u5047\u8bbe a \u662f\u66f4\u4f18\u89e3 (cost_a < cost_b)\n    # \u5982\u679c cost_a > cost_b\uff0c\u5219\u4ea4\u6362 a, b\uff0c\u5e76\u53cd\u8f6c logp \u5dee\n    if cost_a > cost_b:\n        cost_a, cost_b = cost_b, cost_a\n        logp_a, logp_b = logp_b, logp_a\n\n    # 2. \u8ba1\u7b97\u5f52\u4e00\u5316\u7684\u6210\u672c\u5dee\u5f02\uff0c\u5e76\u7528 clamp \u9650\u5236\u8303\u56f4\u9632\u6b62\u6781\u7aef\u503c\n    # rank_gap(a, b) = (cost_b - cost_a) / (abs(cost_a) + abs(cost_b) + 1e-9)\n    delta_cost_norm = rank_gap(cost_a, cost_b)\n    clamped_delta_cost = clamp(delta_cost_norm, 0, 1.0) # rank_gap \u7ed3\u679c\u901a\u5e38\u5728 [0,1]\n\n    # 3. \u4f7f\u7528 sigmoid \u5c06\u6210\u672c\u5dee\u5f02\u6620\u5c04\u4e3a\u52a8\u6001\u7f29\u653e\u56e0\u5b50 scale_factor\n    # beta \u63a7\u5236 sigmoid \u7684\u9661\u5ced\u7a0b\u5ea6\uff0c\u5373\u6210\u672c\u5dee\u5f02\u5bf9 scale_factor \u7684\u654f\u611f\u5ea6\n    scale_factor = sigmoid(beta * clamped_delta_cost)\n\n    # 4. \u8ba1\u7b97\u5bf9\u6570\u6982\u7387\u5dee (\u6211\u4eec\u5e0c\u671b logp_a > logp_b)\n    delta_logp = logp_a - logp_b\n\n    # 5. \u6838\u5fc3\u635f\u5931\u7ed3\u6784\uff1a\u5c06\u52a8\u6001\u7f29\u653e\u56e0\u5b50\u5e94\u7528\u4e8e logp \u5dee\uff0c\u5e76\u5f15\u5165 margin\n    # \u76ee\u6807\u662f\u8ba9 delta_logp > margin\u3002\u635f\u5931\u60e9\u7f5a delta_logp \u8fc7\u5c0f\u7684\u60c5\u51b5\u3002\n    # scale_factor \u8c03\u8282\u4e86\u60e9\u7f5a\u7684\u5f3a\u5ea6\u3002\n    # softplus(x) = log(1 + exp(x))\uff0c\u7528\u4e8e\u5b9e\u73b0\u5e73\u6ed1\u7684 hinge loss\n    # \u6211\u4eec\u8981\u6700\u5c0f\u5316 softplus(margin - scale_factor * delta_logp)\n    loss_value = softplus(margin - scale_factor * delta_logp)\n\n    return loss_value", "hyperparams": {"beta": 10.0, "margin": 0.5}, "operators_used": ["rank_gap", "clamp", "sigmoid", "softplus"], "implementation_hint": {"expects": ["{'cost_w': 'Scalar, cost of the winning solution in a pair.', 'cost_l': 'Scalar, cost of the losing solution in a pair.', 'logp_w': \"Scalar, model's log probability for the winning solution.\", 'logp_l': \"Scalar, model's log probability for the losing solution.\"}"], "returns": "A single non-negative scalar representing the loss for the pair."}}, "static_ok": false, "static_reason": "implementation_hint.returns must be 'scalar'."}
{"generation": 1, "index": 6, "ir": {"name": "AdaptiveMarginHingeLoss", "intuition": "The core idea is to create a hinge-like loss where the 'margin' is not fixed, but dynamically adapts to the normalized difference in the costs of the two solutions. This ensures that when one solution is significantly better than another (large cost difference), the model is penalized more heavily for preferring the worse one. We use `tanh` to squash the cost difference into a bounded range [-1, 1], which acts as an adaptive margin. The `softplus` function is used as a smooth version of the hinge loss (`relu`), ensuring the loss is always non-negative and has a smooth gradient. This design prevents extreme cost differences from causing exploding gradients while still providing a strong learning signal proportional to the magnitude of the preference.", "pseudocode": "def AdaptiveMarginHingeLoss(cost_a, cost_b, logp_a, logp_b, beta, tau):\n    # 1. Calculate the log probability difference.\n    log_prob_diff = logp_a - logp_b\n\n    # 2. Calculate the normalized cost difference using tanh. \n    # The sign is flipped (b-a) because a lower cost is better.\n    # tau scales the cost difference before tanh to control sensitivity.\n    normalized_cost_gap = tanh(tau * (cost_b - cost_a))\n\n    # 3. Construct the loss argument.\n    # We want log_prob_diff to be large when cost_a < cost_b (i.e., cost_b - cost_a > 0).\n    # The target is for `beta * (normalized_cost_gap - log_prob_diff)` to be small or negative.\n    # The loss is incurred when log_prob_diff is smaller than the desired margin (normalized_cost_gap).\n    loss_argument = beta * (normalized_cost_gap - log_prob_diff)\n\n    # 4. Apply softplus for a smooth, non-negative hinge-like loss.\n    # softplus(x) = log(1 + exp(x)). It's a smooth approximation of relu(x).\n    loss = softplus(loss_argument)\n\n    return loss", "hyperparams": {"beta": {"value": 10.0, "description": "Inverse temperature parameter. Controls the steepness of the loss. Higher beta means a stronger penalty for incorrect preferences."}, "tau": {"value": 0.05, "description": "Cost scaling factor. Controls how sensitive the adaptive margin is to the raw cost difference. Should be tuned based on the typical range of cost values."}}, "operators_used": ["tanh", "softplus"], "implementation_hint": {"expects": ["{'cost_a': \"Tensor of costs for solution 'a' in each pair.\", 'cost_b': \"Tensor of costs for solution 'b' in each pair.\", 'logp_a': \"Tensor of log probabilities for solution 'a' in each pair.\", 'logp_b': \"Tensor of log probabilities for solution 'b' in each pair.\"}"], "returns": "A scalar tensor representing the mean loss over the batch."}}, "static_ok": false, "static_reason": "implementation_hint.returns must be 'scalar'."}
{"generation": 1, "index": 7, "ir": {"name": "AdaptiveMarginHingeLoss", "intuition": "This loss function, inspired by hinge loss, introduces an adaptive margin that is proportional to the normalized cost difference between two solutions. The core idea is that when the cost difference is large, the model should have a correspondingly larger log probability difference to reflect this strong preference. We use `tanh` to squash the cost difference into a bounded [-1, 1] range, preventing extreme cost gaps from creating excessively large margins and ensuring numerical stability. The loss is zero if the model's preference (logit difference) already respects the cost-based preference by a sufficient, adaptively-scaled margin. Otherwise, the loss penalizes the model linearly for failing to meet this margin, encouraging it to increase the probability of the better solution.", "pseudocode": "def adaptive_margin_hinge_loss(cost_a, cost_b, logp_a, logp_b, beta, margin_scale):\n  # 1. Calculate the log probability difference.\n  logit_diff = logp_a - logp_b\n\n  # 2. Calculate the normalized cost difference and squash it into [-1, 1].\n  # This represents the strength and direction of the ground-truth preference.\n  # The sign is important: sign > 0 if a is worse than b.\n  cost_diff = cost_a - cost_b\n  preference_strength = tanh(beta * cost_diff)\n\n  # 3. Define the target: the model's logit_diff should align with the preference.\n  # If cost_a < cost_b, preference_strength is negative. We want logit_diff to be positive.\n  # So, the target is -logit_diff * preference_strength.\n  target = -logit_diff * preference_strength\n\n  # 4. Define the adaptive margin based on the preference strength.\n  # The margin is larger when the cost difference is more significant.\n  adaptive_margin = margin_scale * abs(preference_strength)\n\n  # 5. Apply the hinge loss formula: max(0, margin - target).\n  # The loss is positive only when target < adaptive_margin, which means the model's\n  # preference is not strong enough in the correct direction.\n  loss = relu(adaptive_margin - target)\n\n  return loss", "hyperparams": {"beta": 0.1, "margin_scale": 1.0}, "operators_used": ["tanh", "relu"], "implementation_hint": {"expects": ["{'cost_a': \"Tensor of costs for solution 'a' in each pair.\", 'cost_b': \"Tensor of costs for solution 'b' in each pair.\", 'logp_a': \"Tensor of log probabilities for solution 'a' in each pair.\", 'logp_b': \"Tensor of log probabilities for solution 'b' in each pair.\"}"], "returns": "A scalar tensor representing the mean loss over the batch."}}, "static_ok": false, "static_reason": "implementation_hint.returns must be 'scalar'."}
{"generation": 2, "index": 0, "ir": {"name": "SigmoidScaledMarginRankLoss", "intuition": "This loss function is designed for robust preference learning in combinatorial optimization by adaptively scaling the margin based on the magnitude of the cost difference. The core idea is to use a sigmoid function to transform the normalized cost difference into a dynamic margin between 0 and 1. When the cost difference is large, the margin approaches 1, creating a strong preference signal. When the difference is small, the margin shrinks towards 0, effectively treating the solutions as nearly equivalent and preventing the model from overfitting to noisy or insignificant cost variations. The loss is then calculated as the softplus of the log-probability difference minus this adaptive margin, ensuring it is always non-negative, smooth, and numerically stable.", "pseudocode": "def SigmoidScaledMarginRankLoss(cost_a, cost_b, logp_a, logp_b, beta, tau):\n    # Ensure cost_w is the better solution (lower cost)\n    if cost_a < cost_b:\n        cost_w, cost_l = cost_a, cost_b\n        logp_w, logp_l = logp_a, logp_b\n    else:\n        cost_w, cost_l = cost_b, cost_a\n        logp_w, logp_l = logp_b, logp_a\n\n    # Calculate log probability difference\n    log_prob_diff = logp_w - logp_l\n\n    # Calculate normalized cost difference (smaller is better)\n    cost_diff = cost_w - cost_l\n\n    # Use a sigmoid function on the scaled cost difference to create an adaptive margin\n    # The margin will be between 0 and 1.\n    # When cost_diff is very negative (large preference), margin -> 1.\n    # When cost_diff is close to 0 (small preference), margin -> 0.5 (or less, depending on tau).\n    adaptive_margin = sigmoid(beta * cost_diff / tau)\n\n    # The loss encourages log_prob_diff to be greater than the adaptive_margin.\n    # We use softplus for a smooth, non-negative loss.\n    # softplus(x) = log(1 + exp(x)). It's a smooth approximation of relu(x).\n    # loss = softplus(-(log_prob_diff - adaptive_margin))\n    loss = softplus(adaptive_margin - log_prob_diff)\n\n    return loss", "hyperparams": {"beta": {"value": 5.0, "description": "A scaling factor that controls the steepness of the sigmoid function, determining how sensitively the margin reacts to cost differences. Higher beta means a sharper transition."}, "tau": {"value": 1.0, "description": "A temperature or normalization factor for the cost difference. It should be set to the expected scale or standard deviation of cost differences within a batch to ensure stable learning."}}, "operators_used": ["sigmoid", "softplus"], "implementation_hint": {"expects": ["{'cost_w': 'Tensor of costs for the winning (preferred) solutions in a batch.', 'cost_l': 'Tensor of costs for the losing (less preferred) solutions in a batch.', 'logp_w': 'Tensor of log probabilities for the winning solutions.', 'logp_l': 'Tensor of log probabilities for the losing solutions.'}"], "returns": "A scalar loss value, typically the mean of the batch."}}, "static_ok": false, "static_reason": "implementation_hint.returns must be 'scalar'."}
{"generation": 2, "index": 1, "ir": {"name": "AdaptiveMarginHingeLoss", "intuition": "This loss function, inspired by hinge loss, introduces an 'adaptive margin' that is dynamically determined by the normalized cost difference between two solutions. The core idea is that the required 'confidence' of the model (logit difference) should scale with the magnitude of the quality difference (cost difference). A small cost difference only requires a small logit difference to be considered 'correct', while a large cost difference demands a much larger logit difference. This prevents the model from over-penalizing on nearly-tied solutions and forces it to focus on learning from significant quality gaps. The use of `tanh` on the logit difference and cost difference ensures that extreme values are squashed into a bounded range [-1, 1], guaranteeing numerical stability. The final `softplus` ensures the loss is always non-negative and smooth.", "pseudocode": "def AdaptiveMarginHingeLoss(cost_a, cost_b, logp_a, logp_b, alpha, beta):\n    # 1. Calculate the log probability difference.\n    logit_diff = logp_a - logp_b\n\n    # 2. Calculate the rank-based cost difference. This is more robust to cost distribution.\n    cost_gap = rank_gap(cost_a, cost_b) # Returns a signed value, e.g., -1 if a is better, 1 if b is better.\n\n    # 3. Squash both differences into a bounded range [-1, 1] for stability.\n    # tanh(x) is approximately x for small x, and saturates for large |x|.\n    # alpha scales the sensitivity to logit differences.\n    # beta scales the sensitivity to cost differences.\n    squashed_logit_diff = tanh(alpha * logit_diff)\n    squashed_cost_gap = tanh(beta * cost_gap)\n\n    # 4. Define the adaptive margin. The margin is the squashed cost gap.\n    # The target is to have the squashed_logit_diff be greater than the squashed_cost_gap.\n    # If cost_a < cost_b, cost_gap is positive, we want logit_diff to be positive.\n    # The expression (squashed_cost_gap - squashed_logit_diff) is the 'violation'.\n    # It's positive if the model's preference is 'wrong' or 'insufficient'.\n    violation = squashed_cost_gap - squashed_logit_diff\n\n    # 5. Apply softplus as a smooth version of ReLU (hinge loss).\n    # softplus(x) = log(1 + exp(x)). It's 0 for large negative x, and x for large positive x.\n    # The loss is incurred only when there is a violation.\n    loss = softplus(violation)\n\n    return loss", "hyperparams": {"alpha": 1.0, "beta": 5.0}, "operators_used": ["rank_gap", "tanh", "softplus"], "implementation_hint": {"expects": ["pair_cost_a", "pair_cost_b", "pair_logp_a", "pair_logp_b"], "returns": "scalar_loss"}}, "static_ok": false, "static_reason": "implementation_hint.returns must be 'scalar'."}
{"generation": 2, "index": 2, "ir": {"name": "Rank-Discretized Sigmoid Loss (RDSL)", "intuition": "This loss function discretizes the continuous cost difference into a few ranked 'buckets'. The idea is that for preference learning, the exact magnitude of the cost difference is less important than its relative order of magnitude (e.g., 'slightly better', 'moderately better', 'significantly better'). Each bucket is assigned a target log-probability difference (margin). The loss is then a sigmoid-based penalty that pushes the model's log-probability difference towards the target margin corresponding to the cost difference's bucket. Using `tanh` to scale the logit difference and `softplus` for the final loss calculation ensures smoothness and numerical stability across extreme input ranges.", "pseudocode": "['// Given costs for a pair of solutions (a, b) and their log probabilities', '// 1. Calculate the rank-based gap between costs. This normalizes the cost difference.', 'cost_gap = rank_gap(cost_a, cost_b) // e.g., returns 1 if a is better, -1 if b is better, 0 if equal', '', '// 2. Define discrete target margins based on the cost gap.', '//    If a is better (cost_gap > 0), target is a positive margin.', '//    If b is better (cost_gap < 0), target is a negative margin.', \"//    We use multiple levels for 'significant' vs 'marginal' wins.\", 'target_margin = 0', 'if cost_gap > 0.5: // a is significantly better', '  target_margin = margin_strong', 'elif cost_gap > 0: // a is marginally better', '  target_margin = margin_weak', 'elif cost_gap < -0.5: // b is significantly better', '  target_margin = -margin_strong', 'elif cost_gap < 0: // b is marginally better', '  target_margin = -margin_weak', '', '// 3. Calculate the difference in model log probabilities.', 'logit_diff = logp_a - logp_b', '', '// 4. Scale and bound the logit difference for stability using tanh.', '//    This prevents extreme logit differences from causing instability.', \"//    The scaling factor `alpha` controls how much we trust the model's current logits.\", 'bounded_logit_diff = alpha * tanh(logit_diff / alpha)', '', '// 5. Calculate the final loss using softplus (a smooth version of ReLU).', '//    The loss is the discrepancy between the bounded logit difference and the target margin.', '//    The argument to softplus is structured like a logistic loss: `-(y * x)` where y is the preference label (+1/-1) and x is the logit diff.', '//    Here, we generalize it to a target margin.', 'loss = softplus(beta * (target_margin - bounded_logit_diff))', '', 'return loss']", "hyperparams": {"alpha": {"value": 10.0, "description": "Controls the saturation of the logit difference via tanh. A larger alpha allows a wider range of logit differences to have a linear effect, effectively trusting the model's logit scale more. A smaller alpha squashes the logits into a smaller range, making the loss less sensitive to extreme logit differences."}, "beta": {"value": 1.0, "description": "The temperature or scaling factor for the final softplus loss. Higher beta leads to a steeper loss curve, penalizing deviations from the target margin more aggressively."}, "margin_weak": {"value": 0.5, "description": "The target log-probability difference when one solution is marginally better than the other."}, "margin_strong": {"value": 2.0, "description": "The target log-probability difference when one solution is significantly better than the other. Should be greater than margin_weak."}}, "operators_used": ["rank_gap", "tanh", "softplus"], "implementation_hint": {"expects": ["{'name': 'pair_cost_a', 'type': 'Tensor', 'description': \"A batch of costs for solution 'a' in each pair.\"}", "{'name': 'pair_cost_b', 'type': 'Tensor', 'description': \"A batch of costs for solution 'b' in each pair.\"}", "{'name': 'pair_logp_a', 'type': 'Tensor', 'description': \"A batch of model log probabilities for solution 'a'.\"}", "{'name': 'pair_logp_b', 'type': 'Tensor', 'description': \"A batch of model log probabilities for solution 'b'.\"}"], "returns": "{'name': 'loss', 'type': 'Scalar', 'description': 'The mean loss value over the batch.'}"}}, "static_ok": false, "static_reason": "implementation_hint.returns must be 'scalar'."}
{"generation": 2, "index": 3, "ir": {"name": "AdaptiveMarginHingeLoss", "intuition": "This loss function, named 'Adaptive Margin Hinge Loss', is designed to be a robust and adaptive alternative to traditional preference losses like DPO. Its core idea is to treat preference learning as a margin-based classification problem, but with a crucial twist: the margin is not fixed, but dynamically adapts to the magnitude of the cost difference between two solutions. When the cost difference is large, the model is pushed more forcefully to respect this preference. When the difference is small, the penalty is softer, preventing the model from over-confidently distinguishing between nearly-equal solutions. This is achieved by combining a hinge-like structure (via `softplus`) with a cost-difference term that is normalized and scaled. The use of `tanh` on the log-probability difference ensures that the loss is bounded even with extreme logit gaps, preventing numerical instability and gradient explosion.", "pseudocode": "1. Calculate the cost difference \u0394_cost = cost_b - cost_a. Note: A positive \u0394_cost means 'a' is the preferred solution.\n2. Calculate the log-probability difference \u0394_logp = logp(a) - logp(b).\n3. Normalize the cost difference within a batch to get a standardized measure of preference strength. This can be done using z-score normalization: z_cost = zscore(\u0394_cost).\n4. Create an adaptive margin term, `margin`, which is proportional to the normalized cost difference. We use `softplus` to ensure the margin is non-negative and smooth, scaled by a hyperparameter `alpha`: margin = softplus(alpha * z_cost).\n5. Apply the `tanh` function to the log-probability difference to bound it between -1 and 1, preventing extreme values from causing instability: bounded_\u0394_logp = tanh(\u0394_logp).\n6. The core of the loss is a hinge-like term: `margin - bounded_\u0394_logp`. This term is large if the model's preference (`bounded_\u0394_logp`) does not align with the ground-truth preference (`margin`). For instance, if 'a' is much better (large positive margin) but the model prefers 'b' (negative `bounded_\u0394_logp`), the loss will be high.\n7. Finally, apply `softplus` to the entire expression to create a smooth, non-negative loss. This acts like a 'soft' ReLU, penalizing violations (when `bounded_\u0394_logp < margin`) while giving zero loss for correctly classified pairs with sufficient confidence: loss = beta * softplus(margin - bounded_\u0394_logp). `beta` is a scaling factor.", "hyperparams": {"alpha": 2.0, "beta": 1.0}, "operators_used": ["zscore", "softplus", "tanh"], "implementation_hint": {"expects": ["{'pair_costs_a': \"A 1D tensor of costs for solution 'a' for each pair in the batch. Shape: (batch_size,).\", 'pair_costs_b': \"A 1D tensor of costs for solution 'b' for each pair in the batch. Shape: (batch_size,).\", 'pair_logp_a': \"A 1D tensor of log probabilities for solution 'a'. Shape: (batch_size,).\", 'pair_logp_b': \"A 1D tensor of log probabilities for solution 'b'. Shape: (batch_size,).\"}"], "returns": "A scalar loss value, typically the mean of the loss over the batch."}}, "static_ok": false, "static_reason": "implementation_hint.returns must be 'scalar'."}
{"generation": 2, "index": 4, "ir": {"name": "SigmoidAttenuatedRankGapLoss", "intuition": "The core idea is to create a hinge-like loss where the 'margin violation' is determined by both the cost difference and the model's current preference. We use the rank_gap of costs to capture the ordinal preference magnitude, which is more robust to outliers than the raw difference. This rank gap is then attenuated (scaled down) by a sigmoid function of the model's log probability difference. If the model already strongly prefers the better solution (large positive logit_diff), the sigmoid approaches 1, applying the full rank_gap pressure. If the model wrongly prefers the worse solution (large negative logit_diff), the sigmoid approaches 0, significantly reducing the penalty to prevent overly aggressive updates from potentially noisy or misleading large cost gaps, thus enhancing stability. The softplus function ensures the final loss is non-negative and smooth.", "pseudocode": "def SigmoidAttenuatedRankGapLoss(cost_a, cost_b, logp_a, logp_b, beta, margin):\n  # Assume cost_a is the preferred (lower) cost\n  # Ensure cost_a < cost_b for this formulation\n  \n  # 1. Calculate the log probability difference\n  logit_diff = logp_a - logp_b\n  \n  # 2. Calculate the rank gap, which is robust to cost scale\n  # rank_gap(x, y) = rank(x) - rank(y) over a batch of pairs\n  # This is a conceptual step; in implementation, it uses normalized ranks.\n  cost_rank_gap = rank_gap(cost_a, cost_b) # This will be negative since cost_a is better\n  \n  # 3. Use a sigmoid function to create an adaptive scaling factor based on model's belief\n  # When logit_diff is very positive (correct belief), attenuation_factor -> 1\n  # When logit_diff is very negative (wrong belief), attenuation_factor -> 0\n  attenuation_factor = sigmoid(beta * logit_diff)\n  \n  # 4. Combine the rank gap and attenuation factor.\n  # The term inside softplus represents the 'error'.\n  # We want logit_diff to be positive. The loss is driven by the negative cost_rank_gap.\n  # The structure is `margin - (positive_term * scaling_factor)`.\n  # Since cost_rank_gap is negative, we use `-cost_rank_gap`.\n  error_signal = margin - attenuation_factor * (-cost_rank_gap)\n  \n  # 5. Apply softplus to create a smooth, non-negative loss.\n  # Loss is high only when attenuation_factor * (-cost_rank_gap) < margin.\n  # This happens if the model is not confident enough (small logit_diff) or wrong (negative logit_diff).\n  loss = softplus(error_signal)\n  \n  return loss", "hyperparams": {"beta": 1.0, "margin": 0.5}, "operators_used": ["rank_gap", "sigmoid", "softplus"], "implementation_hint": {"expects": ["{'preferred_costs': 'A tensor of costs for the winning solutions in a batch of pairs.', 'losing_costs': 'A tensor of costs for the losing solutions in a batch of pairs.', 'preferred_logp': 'A tensor of model log probabilities for the winning solutions.', 'losing_logp': 'A tensor of model log probabilities for the losing solutions.'}"], "returns": "A scalar loss value, averaged over the batch."}}, "static_ok": false, "static_reason": "implementation_hint.returns must be 'scalar'."}
{"generation": 2, "index": 5, "ir": {"name": "AdaptiveMarginHingeLoss", "intuition": "This loss function, named 'Adaptive Margin Hinge Loss', is designed for preference learning in combinatorial optimization. Its core idea is to create a dynamic, cost-difference-aware margin. Unlike a fixed-margin hinge loss, this loss demands that the log-probability difference between a better solution (a) and a worse solution (b) not only be positive but also exceed a margin proportional to their cost difference. The `tanh` function is used to scale the cost difference into a bounded range [-1, 1], preventing extreme cost gaps from creating excessively large, unstable margins. This 'adaptive margin' ensures that the model is pushed harder to separate solutions with large quality differences, while being more lenient with nearly-equivalent solutions. The `softplus` function, a smooth approximation of ReLU, is used as the hinge mechanism. This provides a continuously differentiable loss (avoiding the non-differentiable point of ReLU at zero) that penalizes incorrect preferences (logp(a) < logp(b)) and preferences that do not meet the adaptive margin, while providing zero loss for correctly and confidently ranked pairs. The overall structure is numerically stable due to the bounded nature of `tanh` and the well-behaved properties of `softplus`.", "pseudocode": "def AdaptiveMarginHingeLoss(cost_a, cost_b, logp_a, logp_b, alpha, beta):\n  # Calculate the difference in log probabilities\n  logp_diff = logp_a - logp_b\n\n  # Calculate the normalized and scaled cost difference using rank_gap and tanh.\n  # rank_gap(cost_a, cost_b) returns a signed value indicating preference (e.g., -1 if a is better, 1 if b is better).\n  # We use cost_b - cost_a so that if cost_a < cost_b, the difference is positive.\n  # Tanh squashes the difference to a bounded range [-1, 1], ensuring stability.\n  cost_delta_normalized = tanh(alpha * (cost_b - cost_a))\n\n  # Create an adaptive margin. The margin is beta when the cost difference is large and positive, \n  # and near zero when costs are similar.\n  adaptive_margin = beta * cost_delta_normalized\n\n  # The core of the loss. We want logp_diff to be greater than the adaptive_margin.\n  # The target is to make (adaptive_margin - logp_diff) negative.\n  # softplus(x) = log(1 + exp(x)). It's a smooth version of max(0, x).\n  # If (adaptive_margin - logp_diff) is large and positive (bad case), loss is high.\n  # If (adaptive_margin - logp_diff) is large and negative (good case), loss is near zero.\n  loss = softplus(adaptive_margin - logp_diff)\n\n  return loss", "hyperparams": {"alpha": {"description": "Scales the cost difference before applying tanh. A higher alpha makes the margin more sensitive to small cost differences.", "default_value": 0.1}, "beta": {"description": "The maximum margin size. It scales the output of the tanh function.", "default_value": 5.0}}, "operators_used": ["tanh", "softplus"], "implementation_hint": {"expects": ["{'name': 'cost_a', 'description': \"Scalar cost of solution 'a' in a pair. Lower is better.\"}", "{'name': 'cost_b', 'description': \"Scalar cost of solution 'b' in a pair.\"}", "{'name': 'logp_a', 'description': \"Log probability of generating solution 'a', produced by the model.\"}", "{'name': 'logp_b', 'description': \"Log probability of generating solution 'b', produced by the model.\"}"], "returns": "A single scalar loss value for the pair, which is non-negative."}}, "static_ok": false, "static_reason": "implementation_hint.returns must be 'scalar'."}
{"generation": 2, "index": 6, "ir": {"name": "SigmoidAttenuatedMarginLoss", "intuition": "This loss function is designed to be a robust alternative to standard preference losses like DPO. The core idea is to create a 'target' preference margin based on the normalized cost difference between two solutions, and then penalize the model's log-probability difference if it doesn't align with this target. The cost difference is transformed by a sigmoid function, which naturally maps any cost gap into a (0, 1) range, acting as an 'attention' or 'weight' for the log-probability difference. This prevents extreme cost differences from causing exploding gradients. A softplus function is used as the final loss activation, ensuring the loss is always non-negative and smooth. The `margin` hyperparameter introduces a tolerance, encouraging the model to not just match the preference, but to prefer the better solution by a confident margin, especially when the cost difference is significant.", "pseudocode": "def SigmoidAttenuatedMarginLoss(cost_a, cost_b, logp_a, logp_b, beta, margin):\n  # 1. Calculate the difference in costs and log probabilities.\n  # The sign is chosen so that a positive diff_cost means 'a' is better.\n  diff_cost = cost_b - cost_a\n  diff_logp = logp_a - logp_b\n\n  # 2. Normalize the cost difference using a sigmoid function.\n  # This maps any cost difference to a (0, 1) range, acting as a weight.\n  # beta controls the steepness of this mapping.\n  cost_weight = sigmoid(beta * diff_cost)\n\n  # 3. Construct the core term of the loss.\n  # We want diff_logp to be positive when cost_a < cost_b (i.e., diff_cost > 0).\n  # The term (1 - 2 * cost_weight) smoothly maps cost_weight from (0, 1) to (-1, 1).\n  # When cost_a << cost_b, cost_weight -> 1, term -> -1. Loss encourages large positive diff_logp.\n  # When cost_a >> cost_b, cost_weight -> 0, term -> +1. Loss encourages large negative diff_logp.\n  # When cost_a == cost_b, cost_weight -> 0.5, term -> 0. Loss is driven only by the margin.\n  core_term = (1 - 2 * cost_weight) * diff_logp\n\n  # 4. Apply a margin and the final smooth, non-negative loss function (softplus).\n  # The loss penalizes the model if the core term is not greater than the margin.\n  # softplus(x) = log(1 + exp(x)). It's a smooth version of ReLU.\n  loss = softplus(margin - core_term)\n\n  return loss", "hyperparams": {"beta": {"default": 1.0, "description": "A temperature parameter that scales the cost difference before the sigmoid. A higher beta makes the loss more sensitive to small cost differences, while a lower beta makes it smoother."}, "margin": {"default": 0.1, "description": "A positive value that sets a desired separation margin for the log-probabilities, encouraging the model to be more confident in its preference."}}, "operators_used": ["sigmoid", "softplus"], "implementation_hint": {"expects": ["{'cost_a': \"Tensor of costs for solution 'a' in each pair.\", 'cost_b': \"Tensor of costs for solution 'b' in each pair.\", 'logp_a': \"Tensor of model's log-probabilities for solution 'a'.\", 'logp_b': \"Tensor of model's log-probabilities for solution 'b'.\"}"], "returns": "A scalar loss value, typically the mean of the loss over the batch."}}, "static_ok": false, "static_reason": "implementation_hint.returns must be 'scalar'."}
{"generation": 2, "index": 7, "ir": {"name": "AdaptiveMarginHingeLoss", "intuition": "This loss function, named 'Adaptive Margin Hinge Loss', is designed to dynamically adjust the learning signal based on the magnitude of the cost difference between two solutions. It combines a classic hinge loss structure with an adaptive margin, which is a sigmoidal function of the normalized cost difference. When the cost difference is small, the margin is also small, allowing the model to focus on clear-cut cases and not be overly penalized for ambiguity. As the cost difference grows, the margin increases and saturates, providing a strong and stable signal to correct significant preference errors. The use of `tanh` for the log probability difference and `softplus` for the hinge mechanism ensures the final loss is smooth, bounded, and numerically stable, preventing issues with extreme input values.", "pseudocode": "def adaptive_margin_hinge_loss(cost_a, cost_b, logp_a, logp_b, alpha, beta, tau):\n    # 1. Normalize the cost difference to a stable range, e.g., using a rank-based gap.\n    #    The sign indicates which solution is better.\n    cost_gap = rank_gap(cost_a, cost_b) # Returns a value in [-1, 1]\n\n    # 2. Create an adaptive margin that grows with the absolute cost difference.\n    #    The sigmoid function maps the absolute gap to a margin in [0, beta].\n    adaptive_margin = beta * sigmoid(alpha * abs(cost_gap))\n\n    # 3. Calculate the log probability difference and stabilize it using tanh.\n    #    This bounds the logit difference, preventing extreme values from dominating the loss.\n    logit_diff = logp_a - logp_b\n    stable_logit_diff = tau * tanh(logit_diff / tau)\n\n    # 4. Combine into a hinge-like structure using softplus (a smooth ReLU).\n    #    The loss is incurred only when the model's preference (logit_diff) doesn't align\n    #    with the ground truth preference (cost_gap) by a sufficient margin.\n    #    The term `cost_gap * stable_logit_diff` is positive if preferences are misaligned.\n    loss = softplus(adaptive_margin - cost_gap * stable_logit_diff)\n\n    return loss", "hyperparams": {"alpha": 5.0, "beta": 1.0, "tau": 10.0}, "operators_used": ["rank_gap", "sigmoid", "tanh", "softplus"], "implementation_hint": {"expects": ["{'cost_a': \"Tensor of costs for solution 'a' in each pair (shape: [batch_size])\", 'cost_b': \"Tensor of costs for solution 'b' in each pair (shape: [batch_size])\", 'logp_a': \"Tensor of log probabilities for solution 'a' (shape: [batch_size])\", 'logp_b': \"Tensor of log probabilities for solution 'b' (shape: [batch_size])\"}"], "returns": "A scalar tensor representing the mean loss over the batch."}}, "static_ok": false, "static_reason": "implementation_hint.returns must be 'scalar'."}
