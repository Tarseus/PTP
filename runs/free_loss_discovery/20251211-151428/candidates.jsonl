{"generation": 0, "index": 0, "ir": {"name": "Normalized Rank-Gap Hinge Loss", "intuition": "This loss function uses the rank-gap to create a standardized, signed indicator of which solution is better. This rank-gap is then normalized using a tanh function to prevent extreme cost differences from dominating the loss signal. The core idea is a hinge-like mechanism: the loss is zero if the model's preference (log-probability difference) already exceeds the normalized cost difference by a certain margin. If the model prefers the worse solution, or prefers the better solution by an insufficient amount, a linear penalty is applied. The use of tanh for normalization and a ReLU-based hinge structure ensures the final loss is bounded, numerically stable, and focuses the learning signal on 'difficult' pairs where the model's preference is incorrect or weak.", "pseudocode": "1. Calculate the log-probability difference: logp_diff = logp_a - logp_b.\n2. Calculate the signed rank gap of the costs: cost_rg = rank_gap(cost_a, cost_b). This will be -1 if a is better, +1 if b is better.\n3. Normalize the cost difference using tanh to create a bounded target: target_diff = tanh( (cost_b - cost_a) / temperature ).\n4. The loss is computed as a hinge loss. We want the log-probability difference to align with the target difference. Specifically, we want logp_diff to be greater than target_diff plus a margin. The loss is the amount by which this condition is violated: loss = relu( margin - logp_diff * cost_rg ). An alternative formulation is loss = relu( margin + target_diff - logp_diff ). Let's use the second one as it's more direct. No, wait, the first one is better as it directly uses the signed rank_gap. Let's refine: loss = relu( margin - logp_diff * rank_gap(cost_a, cost_b) ). This is simple but doesn't use the magnitude. A better version: loss = relu( margin - (logp_a - logp_b) * tanh( (cost_b - cost_a) / temperature ) ). No, this product is not what we want. Let's try again. We want logp_a to be higher than logp_b when cost_a is lower than cost_b. The 'target' for (logp_a - logp_b) is a positive value whose magnitude depends on (cost_b - cost_a). So, let target = tanh((cost_b - cost_a) / temperature). The error is how far logp_diff is from this target. A hinge loss on the error: loss = relu(margin + target - (logp_a - logp_b)). This penalizes when logp_a - logp_b < target + margin. This seems correct and stable.", "hyperparams": {"margin": 0.5, "temperature": 1.0}, "operators_used": ["tanh", "relu"], "implementation_hint": {"expects": ["cost_a", "cost_b", "logp_a", "logp_b"], "returns": "scalar"}}, "fitness": {"hf_like_score": 8.464062690734863, "validation_objective": 8.464062690734863, "generalization_penalty": 0.0, "generalization_objectives": {"20": 3.9757773876190186}, "train_score_mean": 10.425789546966552, "train_loss_mean": 0.6131650882959366, "pair_count": 12902387, "config": {"hf": {"problem": "tsp", "hf_steps": 100, "train_problem_size": 100, "valid_problem_sizes": [20], "train_batch_size": 64, "pomo_size": 64, "learning_rate": 0.0003, "weight_decay": 1e-06, "alpha": 0.05, "device": "cuda", "seed": 1234, "num_validation_episodes": 128, "validation_batch_size": 64, "generalization_penalty_weight": 1.0, "pool_version": "v0"}, "free_loss": {"f1_steps": 100, "f2_steps": 100, "f3_enabled": false}}, "loss_ir": {"name": "Normalized Rank-Gap Hinge Loss", "intuition": "This loss function uses the rank-gap to create a standardized, signed indicator of which solution is better. This rank-gap is then normalized using a tanh function to prevent extreme cost differences from dominating the loss signal. The core idea is a hinge-like mechanism: the loss is zero if the model's preference (log-probability difference) already exceeds the normalized cost difference by a certain margin. If the model prefers the worse solution, or prefers the better solution by an insufficient amount, a linear penalty is applied. The use of tanh for normalization and a ReLU-based hinge structure ensures the final loss is bounded, numerically stable, and focuses the learning signal on 'difficult' pairs where the model's preference is incorrect or weak.", "hyperparams": {"margin": 0.5, "temperature": 1.0}, "operators_used": ["tanh", "relu"]}}, "better_than_baseline": false}
{"generation": 0, "index": 1, "ir": {"name": "Adaptive Margin Softplus Loss", "intuition": "This loss function uses the `softplus` function to create a smooth, non-negative penalty, similar to a hinge loss. The core idea is to make the required margin between `logp(a)` and `logp(b)` adaptive, based on the normalized quality difference between solutions `a` and `b`. The cost difference is transformed into a stable `[0, 1]` range using `tanh`, which acts as a dynamic margin. When costs are very different, the loss strongly enforces a large log-probability gap. When costs are similar, it allows the model more flexibility. A temperature parameter `tau` controls the sensitivity to cost differences, and a scaling factor `beta` adjusts the overall magnitude of the loss gradient. The design is inherently stable due to the properties of `softplus` and `tanh`.", "pseudocode": "1. Compute cost difference: delta_cost = cost_b - cost_a.\n2. Normalize the cost difference into a stable, signed target margin using tanh: target_margin = tanh(delta_cost / tau).\n3. Compute the log-probability difference: delta_logp = logp_a - logp_b.\n4. Calculate the loss as a scaled softplus of the difference between the target margin and the log-probability difference. This penalizes cases where delta_logp is smaller than the target_margin, effectively pushing logp_a to be higher than logp_b by an amount proportional to the cost improvement.\n5. Final loss: loss = beta * softplus(target_margin - delta_logp).", "hyperparams": {"beta": {"value": 1.0, "description": "A scaling factor for the loss magnitude, controlling the overall strength of the gradient."}, "tau": {"value": 1.0, "description": "A temperature parameter to control the sensitivity of the adaptive margin to cost differences. Smaller tau makes the margin more sensitive to small cost gaps."}}, "operators_used": ["tanh", "softplus"], "implementation_hint": {"expects": ["cost_a", "cost_b", "logp_a", "logp_b"], "returns": "scalar"}}, "fitness": {"hf_like_score": 8.484230041503906, "validation_objective": 8.484230041503906, "generalization_penalty": 0.0, "generalization_objectives": {"20": 4.022554159164429}, "train_score_mean": 10.515706558227539, "train_loss_mean": 0.6007438188791275, "pair_count": 12902384, "config": {"hf": {"problem": "tsp", "hf_steps": 100, "train_problem_size": 100, "valid_problem_sizes": [20], "train_batch_size": 64, "pomo_size": 64, "learning_rate": 0.0003, "weight_decay": 1e-06, "alpha": 0.05, "device": "cuda", "seed": 1234, "num_validation_episodes": 128, "validation_batch_size": 64, "generalization_penalty_weight": 1.0, "pool_version": "v0"}, "free_loss": {"f1_steps": 100, "f2_steps": 100, "f3_enabled": false}}, "loss_ir": {"name": "Adaptive Margin Softplus Loss", "intuition": "This loss function uses the `softplus` function to create a smooth, non-negative penalty, similar to a hinge loss. The core idea is to make the required margin between `logp(a)` and `logp(b)` adaptive, based on the normalized quality difference between solutions `a` and `b`. The cost difference is transformed into a stable `[0, 1]` range using `tanh`, which acts as a dynamic margin. When costs are very different, the loss strongly enforces a large log-probability gap. When costs are similar, it allows the model more flexibility. A temperature parameter `tau` controls the sensitivity to cost differences, and a scaling factor `beta` adjusts the overall magnitude of the loss gradient. The design is inherently stable due to the properties of `softplus` and `tanh`.", "hyperparams": {"beta": {"value": 1.0, "description": "A scaling factor for the loss magnitude, controlling the overall strength of the gradient."}, "tau": {"value": 1.0, "description": "A temperature parameter to control the sensitivity of the adaptive margin to cost differences. Smaller tau makes the margin more sensitive to small cost gaps."}}, "operators_used": ["tanh", "softplus"]}}, "better_than_baseline": false}
{"generation": 0, "index": 2, "ir": {"name": "Normalized Rank-Gap Hinge Loss", "intuition": "This loss function uses a hinge-like structure, but instead of a fixed margin, it employs a dynamic margin derived from the normalized rank gap of the costs. The core idea is to scale the required log-probability difference based on how different the solution costs are. A large cost difference demands a proportionally large log-probability gap, while a small cost difference requires only a small gap. The `tanh` function is applied to the normalized cost gap to keep this dynamic margin bounded between [-1, 1], preventing extreme cost differences from causing instability. The `softplus` function creates a smooth, non-negative loss that is zero when the model's preference alignment exceeds the dynamic margin, and positive otherwise, ensuring stability and differentiability.", "pseudocode": "1. Compute the log-probability difference: logp_diff = logp_a - logp_b.\n2. Compute the signed, normalized cost gap: cost_gap_signed = rank_gap(cost_a, cost_b).\n3. Standardize the cost gap across a batch: cost_gap_zscored = zscore(cost_gap_signed).\n4. Create a bounded dynamic margin from the standardized cost gap: dynamic_margin = beta * tanh(cost_gap_zscored).\n5. Compute the hinge-like term: The loss is incurred if the log-probability difference does not align with the dynamic margin. We want logp_diff to be greater than the margin. So, the argument to softplus is margin - logp_diff.\n6. Calculate the final loss using softplus for smoothness and non-negativity: loss = softplus(dynamic_margin - logp_diff).", "hyperparams": {"beta": 1.0}, "operators_used": ["rank_gap", "zscore", "tanh", "softplus"], "implementation_hint": {"expects": ["cost_a", "cost_b", "logp_a", "logp_b"], "returns": "scalar"}}, "fitness": {"hf_like_score": 8.428038597106934, "validation_objective": 8.428038597106934, "generalization_penalty": 0.0, "generalization_objectives": {"20": 4.004412531852722}, "train_score_mean": 10.480581436157227, "train_loss_mean": 0.6015512990951538, "pair_count": 12902386, "config": {"hf": {"problem": "tsp", "hf_steps": 100, "train_problem_size": 100, "valid_problem_sizes": [20], "train_batch_size": 64, "pomo_size": 64, "learning_rate": 0.0003, "weight_decay": 1e-06, "alpha": 0.05, "device": "cuda", "seed": 1234, "num_validation_episodes": 128, "validation_batch_size": 64, "generalization_penalty_weight": 1.0, "pool_version": "v0"}, "free_loss": {"f1_steps": 100, "f2_steps": 100, "f3_enabled": false}}, "loss_ir": {"name": "Normalized Rank-Gap Hinge Loss", "intuition": "This loss function uses a hinge-like structure, but instead of a fixed margin, it employs a dynamic margin derived from the normalized rank gap of the costs. The core idea is to scale the required log-probability difference based on how different the solution costs are. A large cost difference demands a proportionally large log-probability gap, while a small cost difference requires only a small gap. The `tanh` function is applied to the normalized cost gap to keep this dynamic margin bounded between [-1, 1], preventing extreme cost differences from causing instability. The `softplus` function creates a smooth, non-negative loss that is zero when the model's preference alignment exceeds the dynamic margin, and positive otherwise, ensuring stability and differentiability.", "hyperparams": {"beta": 1.0}, "operators_used": ["rank_gap", "zscore", "tanh", "softplus"]}}, "better_than_baseline": true}
{"generation": 0, "index": 3, "ir": {"name": "Adaptive Margin Sigmoid Loss", "intuition": "This loss uses a sigmoid-like structure to compare model preference against ground truth preference, similar to logistic loss. However, it introduces an adaptive margin that is a function of the normalized cost difference. When the cost difference is large, the margin is also large, demanding a stronger preference from the model. When the cost difference is small, the margin shrinks, tolerating ambiguity. The cost difference is normalized using a tanh function, which smoothly maps it to a bounded [-1, 1] range, preventing extreme cost gaps from dominating the loss and ensuring numerical stability. The entire term is then passed through a softplus function, which acts like a smooth ReLU, ensuring the loss is non-negative and penalizing incorrect preferences.", "pseudocode": "1. Compute the cost difference: delta_cost = cost_a - cost_b.\n2. Compute the log-probability difference: delta_logp = logp_a - logp_b.\n3. Normalize the cost difference into a bounded range using tanh: normalized_cost_gap = tanh(delta_cost / temp).\n4. Create an adaptive margin based on the normalized cost gap: margin = margin_scale * normalized_cost_gap.\n5. The loss is computed by comparing the log-probability difference against the adaptive margin. The sign of delta_cost determines the direction of preference. We want delta_logp to be large and negative if delta_cost is large and positive (b is better), and vice-versa. The final loss is softplus applied to the signed difference between the margin and the log-probability difference: loss = softplus(sign(delta_cost) * (margin - delta_logp)).", "hyperparams": {"margin_scale": 1.0, "temp": 10.0}, "operators_used": ["tanh", "softplus"], "implementation_hint": {"expects": ["cost_a", "cost_b", "logp_a", "logp_b"], "returns": "scalar"}}, "fitness": {"hf_like_score": 8.443606853485107, "validation_objective": 8.443606853485107, "generalization_penalty": 0.0, "generalization_objectives": {"20": 4.01371705532074}, "train_score_mean": 10.497917833328247, "train_loss_mean": 0.600828481912613, "pair_count": 12902393, "config": {"hf": {"problem": "tsp", "hf_steps": 100, "train_problem_size": 100, "valid_problem_sizes": [20], "train_batch_size": 64, "pomo_size": 64, "learning_rate": 0.0003, "weight_decay": 1e-06, "alpha": 0.05, "device": "cuda", "seed": 1234, "num_validation_episodes": 128, "validation_batch_size": 64, "generalization_penalty_weight": 1.0, "pool_version": "v0"}, "free_loss": {"f1_steps": 100, "f2_steps": 100, "f3_enabled": false}}, "loss_ir": {"name": "Adaptive Margin Sigmoid Loss", "intuition": "This loss uses a sigmoid-like structure to compare model preference against ground truth preference, similar to logistic loss. However, it introduces an adaptive margin that is a function of the normalized cost difference. When the cost difference is large, the margin is also large, demanding a stronger preference from the model. When the cost difference is small, the margin shrinks, tolerating ambiguity. The cost difference is normalized using a tanh function, which smoothly maps it to a bounded [-1, 1] range, preventing extreme cost gaps from dominating the loss and ensuring numerical stability. The entire term is then passed through a softplus function, which acts like a smooth ReLU, ensuring the loss is non-negative and penalizing incorrect preferences.", "hyperparams": {"margin_scale": 1.0, "temp": 10.0}, "operators_used": ["tanh", "softplus"]}}, "better_than_baseline": true}
{"generation": 0, "index": 4, "ir": {"name": "Sigmoid-Gated Adaptive Margin Loss", "intuition": "This loss dynamically adjusts the preference margin based on the magnitude of the cost difference. For small cost differences, it enforces a soft preference, preventing the model from overfitting to noisy or insignificant cost variations. For large, clear cost differences, it uses a stronger, saturated margin to decisively guide the model. The entire expression is gated by a sigmoid function of the cost difference, which smoothly transitions the loss from being active (for true preferences) to near-zero (for ties), ensuring stability and preventing updates on ambiguous pairs. The final softplus ensures the loss is non-negative and avoids issues with log(0).", "pseudocode": "1. Compute the cost difference: cost_diff = cost_b - cost_a.\n2. Compute the log-probability difference: logp_diff = logp_a - logp_b.\n3. Create an adaptive margin that saturates for large cost differences: adaptive_margin = tanh(beta * abs(cost_diff)).\n4. Form the core preference term, which is the log-probability difference shifted by the signed adaptive margin: term = logp_diff - adaptive_margin * sign(cost_diff).\n5. Create a gating factor that is close to 1 for significant cost differences and close to 0 for ties: gate = sigmoid(alpha * abs(cost_diff)).\n6. Apply the gate to the negative of the core term. The negative sign ensures the loss decreases when the model's preference aligns with the cost preference: gated_term = -gate * term.\n7. Pass the result through a softplus function to ensure the final loss is non-negative, stable, and differentiable: loss = softplus(gated_term).", "hyperparams": {"alpha": 10.0, "beta": 1.0}, "operators_used": ["tanh", "sigmoid", "softplus"], "implementation_hint": {"expects": ["cost_a", "cost_b", "logp_a", "logp_b"], "returns": "scalar"}}, "fitness": {"hf_like_score": 23.771821975708008, "validation_objective": 23.771821975708008, "generalization_penalty": 0.0, "generalization_objectives": {"20": 5.290795087814331}, "train_score_mean": 25.81061721801758, "train_loss_mean": 13.525993547439576, "pair_count": 12902385, "config": {"hf": {"problem": "tsp", "hf_steps": 100, "train_problem_size": 100, "valid_problem_sizes": [20], "train_batch_size": 64, "pomo_size": 64, "learning_rate": 0.0003, "weight_decay": 1e-06, "alpha": 0.05, "device": "cuda", "seed": 1234, "num_validation_episodes": 128, "validation_batch_size": 64, "generalization_penalty_weight": 1.0, "pool_version": "v0"}, "free_loss": {"f1_steps": 100, "f2_steps": 100, "f3_enabled": false}}, "loss_ir": {"name": "Sigmoid-Gated Adaptive Margin Loss", "intuition": "This loss dynamically adjusts the preference margin based on the magnitude of the cost difference. For small cost differences, it enforces a soft preference, preventing the model from overfitting to noisy or insignificant cost variations. For large, clear cost differences, it uses a stronger, saturated margin to decisively guide the model. The entire expression is gated by a sigmoid function of the cost difference, which smoothly transitions the loss from being active (for true preferences) to near-zero (for ties), ensuring stability and preventing updates on ambiguous pairs. The final softplus ensures the loss is non-negative and avoids issues with log(0).", "hyperparams": {"alpha": 10.0, "beta": 1.0}, "operators_used": ["tanh", "sigmoid", "softplus"]}}, "better_than_baseline": false}
{"generation": 0, "index": 5, "ir": {"name": "Adaptive Margin Sigmoid Loss", "intuition": "This loss function adapts the learning signal based on the magnitude of the cost difference between two solutions. It computes a 'target preference strength' from the z-scored cost difference, which is then used as a dynamic margin. The model's preference (logit difference) is compared against this target. A logsigmoid function ensures the final loss is a smooth, bounded, and stable probability-like value, preventing issues with extreme cost or logit differences. The core idea is that pairs with larger cost differences should provide a stronger, more confident training signal than pairs with nearly identical costs.", "pseudocode": "1. Compute cost difference: cost_diff = cost_b - cost_a.\n2. Normalize the cost difference across the batch: cost_gap_normalized = zscore(cost_diff).\n3. Compute a target preference strength using tanh to keep it bounded: target_strength = beta * tanh(cost_gap_normalized).\n4. Compute the model's preference difference: logp_diff = logp_a - logp_b.\n5. Calculate the final loss argument by comparing the model's preference to the target strength: loss_arg = logp_diff - target_strength.\n6. Apply a scaled negative logsigmoid for a stable, bounded loss: loss = -logsigmoid(alpha * loss_arg).", "hyperparams": {"alpha": 5.0, "beta": 1.0}, "operators_used": ["zscore", "tanh", "logsigmoid"], "implementation_hint": {"expects": ["cost_a", "cost_b", "logp_a", "logp_b"], "returns": "scalar"}}, "fitness": {"hf_like_score": 23.244123458862305, "validation_objective": 23.244123458862305, "generalization_penalty": 0.0, "generalization_objectives": {"20": 5.278779983520508}, "train_score_mean": 25.850948123931886, "train_loss_mean": 6.7977281761169435, "pair_count": 12902388, "config": {"hf": {"problem": "tsp", "hf_steps": 100, "train_problem_size": 100, "valid_problem_sizes": [20], "train_batch_size": 64, "pomo_size": 64, "learning_rate": 0.0003, "weight_decay": 1e-06, "alpha": 0.05, "device": "cuda", "seed": 1234, "num_validation_episodes": 128, "validation_batch_size": 64, "generalization_penalty_weight": 1.0, "pool_version": "v0"}, "free_loss": {"f1_steps": 100, "f2_steps": 100, "f3_enabled": false}}, "loss_ir": {"name": "Adaptive Margin Sigmoid Loss", "intuition": "This loss function adapts the learning signal based on the magnitude of the cost difference between two solutions. It computes a 'target preference strength' from the z-scored cost difference, which is then used as a dynamic margin. The model's preference (logit difference) is compared against this target. A logsigmoid function ensures the final loss is a smooth, bounded, and stable probability-like value, preventing issues with extreme cost or logit differences. The core idea is that pairs with larger cost differences should provide a stronger, more confident training signal than pairs with nearly identical costs.", "hyperparams": {"alpha": 5.0, "beta": 1.0}, "operators_used": ["zscore", "tanh", "logsigmoid"]}}, "better_than_baseline": false}
{"generation": 0, "index": 6, "ir": {"name": "Adaptive Margin Sigmoid Loss", "intuition": "This loss uses the sigmoid function to create a bounded, smooth preference signal, similar to logistic loss. However, it introduces an adaptive margin based on the normalized rank gap of the costs. When the cost difference is large, the margin increases, demanding a more confident prediction from the model. When the cost difference is small, the margin shrinks, tolerating more uncertainty. This prevents the model from over-penalizing small, noisy cost differences while still enforcing strong preferences for clearly distinct solutions. The `tanh` function is used to scale the log-probability difference into a bounded range [-beta, beta], preventing extreme log-probability values from causing instability and ensuring the loss remains finite.", "pseudocode": "1. Calculate the log-probability difference: logp_diff = logp_a - logp_b.\n2. Scale and bound the log-probability difference: scaled_logp_diff = beta * tanh(logp_diff / beta).\n3. Calculate the signed rank gap of the costs: cost_gap = rank_gap(cost_a, cost_b).\n4. Compute an adaptive margin based on the cost gap: margin = M * sigmoid(S * cost_gap).\n5. The loss is the softplus of the margin minus the scaled log-probability difference. This is equivalent to log(1 + exp(margin - scaled_logp_diff)), providing a smooth, non-negative, and stable loss value.", "hyperparams": {"beta": 10.0, "M": 5.0, "S": 1.0}, "operators_used": ["rank_gap", "tanh", "sigmoid", "softplus"], "implementation_hint": {"expects": ["cost_a", "cost_b", "logp_a", "logp_b"], "returns": "scalar"}}, "fitness": {"hf_like_score": 8.47013807296753, "validation_objective": 8.47013807296753, "generalization_penalty": 0.0, "generalization_objectives": {"20": 3.990532875061035}, "train_score_mean": 10.496176652908325, "train_loss_mean": 0.5996677696704864, "pair_count": 12902387, "config": {"hf": {"problem": "tsp", "hf_steps": 100, "train_problem_size": 100, "valid_problem_sizes": [20], "train_batch_size": 64, "pomo_size": 64, "learning_rate": 0.0003, "weight_decay": 1e-06, "alpha": 0.05, "device": "cuda", "seed": 1234, "num_validation_episodes": 128, "validation_batch_size": 64, "generalization_penalty_weight": 1.0, "pool_version": "v0"}, "free_loss": {"f1_steps": 100, "f2_steps": 100, "f3_enabled": false}}, "loss_ir": {"name": "Adaptive Margin Sigmoid Loss", "intuition": "This loss uses the sigmoid function to create a bounded, smooth preference signal, similar to logistic loss. However, it introduces an adaptive margin based on the normalized rank gap of the costs. When the cost difference is large, the margin increases, demanding a more confident prediction from the model. When the cost difference is small, the margin shrinks, tolerating more uncertainty. This prevents the model from over-penalizing small, noisy cost differences while still enforcing strong preferences for clearly distinct solutions. The `tanh` function is used to scale the log-probability difference into a bounded range [-beta, beta], preventing extreme log-probability values from causing instability and ensuring the loss remains finite.", "hyperparams": {"beta": 10.0, "M": 5.0, "S": 1.0}, "operators_used": ["rank_gap", "tanh", "sigmoid", "softplus"]}}, "better_than_baseline": false}
{"generation": 0, "index": 7, "ir": {"name": "Sigmoid-Scaled Rank-Gap Loss", "intuition": "This loss function uses the rank-based gap between costs to create a stable, signed target signal. This signal is then scaled by a sigmoid function of the log-probability difference, which acts as a dynamic learning rate. When the model is confident and correct (large positive logp difference), the scaling factor is large, reinforcing the correct gradient. When the model is confident but wrong (large negative logp difference), the scaling factor is also large, providing a strong corrective signal. When the model is uncertain (logp difference near zero), the scaling factor is moderate, preventing overly aggressive updates on ambiguous pairs. The final loss is a softplus of the scaled, signed disagreement, ensuring it is always non-negative and smooth.", "pseudocode": "1. Compute the log-probability difference: logp_diff = logp_a - logp_b.\n2. Compute the signed, normalized cost gap: cost_gap = rank_gap(cost_a, cost_b).\n3. Calculate a dynamic scaling factor based on the model's current preference: scale_factor = sigmoid(abs(logp_diff) * alpha).\n4. The core loss term is the disagreement between the cost gap and the logp difference: disagreement = -cost_gap * logp_diff.\n5. Apply the dynamic scaling to the disagreement: scaled_disagreement = disagreement * scale_factor.\n6. The final loss is a softplus transformation of the scaled disagreement, ensuring it is non-negative and stable: loss = softplus(scaled_disagreement).", "hyperparams": {"alpha": 2.0}, "operators_used": ["rank_gap", "sigmoid", "softplus"], "implementation_hint": {"expects": ["cost_a", "cost_b", "logp_a", "logp_b"], "returns": "scalar"}}, "fitness": {"hf_like_score": 23.21147632598877, "validation_objective": 23.21147632598877, "generalization_penalty": 0.0, "generalization_objectives": {"20": 5.246752023696899}, "train_score_mean": 25.57807936668396, "train_loss_mean": 2.7994004702568054, "pair_count": 12902387, "config": {"hf": {"problem": "tsp", "hf_steps": 100, "train_problem_size": 100, "valid_problem_sizes": [20], "train_batch_size": 64, "pomo_size": 64, "learning_rate": 0.0003, "weight_decay": 1e-06, "alpha": 0.05, "device": "cuda", "seed": 1234, "num_validation_episodes": 128, "validation_batch_size": 64, "generalization_penalty_weight": 1.0, "pool_version": "v0"}, "free_loss": {"f1_steps": 100, "f2_steps": 100, "f3_enabled": false}}, "loss_ir": {"name": "Sigmoid-Scaled Rank-Gap Loss", "intuition": "This loss function uses the rank-based gap between costs to create a stable, signed target signal. This signal is then scaled by a sigmoid function of the log-probability difference, which acts as a dynamic learning rate. When the model is confident and correct (large positive logp difference), the scaling factor is large, reinforcing the correct gradient. When the model is confident but wrong (large negative logp difference), the scaling factor is also large, providing a strong corrective signal. When the model is uncertain (logp difference near zero), the scaling factor is moderate, preventing overly aggressive updates on ambiguous pairs. The final loss is a softplus of the scaled, signed disagreement, ensuring it is always non-negative and smooth.", "hyperparams": {"alpha": 2.0}, "operators_used": ["rank_gap", "sigmoid", "softplus"]}}, "better_than_baseline": false}
{"generation": 0, "index": 8, "ir": {"name": "Adaptive Margin Sigmoid Loss", "intuition": "This loss function uses the relative cost difference to dynamically set a target margin for the log-probability difference. A larger cost gap demands a larger log-probability gap, encouraging the model to be more confident when the quality difference is significant. The `tanh` function is used to squash the normalized cost difference into a bounded, adaptive margin, preventing extreme cost gaps from creating excessively large or unstable targets. The final loss is a softplus of the difference between the target margin and the actual log-probability difference, which behaves like a hinge loss but is smooth and always non-negative, providing a stable gradient signal.", "pseudocode": "1. Compute the cost difference: cost_diff = cost_b - cost_a.\n2. Normalize the cost difference using a z-score transformation across a batch to make it scale-invariant: normalized_cost_diff = zscore(cost_diff).\n3. Compute an adaptive margin by scaling the normalized cost difference with a hyperparameter `beta` and passing it through a tanh function. This keeps the target margin bounded: adaptive_margin = beta * tanh(normalized_cost_diff).\n4. Compute the log-probability difference: logp_diff = logp_a - logp_b.\n5. The loss is the softplus of the difference between the adaptive margin and the log-probability difference. This penalizes the model if logp_diff is less than the desired adaptive_margin: loss = softplus(adaptive_margin - logp_diff).", "hyperparams": {"beta": 5.0}, "operators_used": ["zscore", "tanh", "softplus"], "implementation_hint": {"expects": ["cost_a", "cost_b", "logp_a", "logp_b"], "returns": "scalar"}}, "fitness": {"hf_like_score": 8.486920833587646, "validation_objective": 8.486920833587646, "generalization_penalty": 0.0, "generalization_objectives": {"20": 4.005715608596802}, "train_score_mean": 10.523669538497925, "train_loss_mean": 0.6008408111333847, "pair_count": 12902382, "config": {"hf": {"problem": "tsp", "hf_steps": 100, "train_problem_size": 100, "valid_problem_sizes": [20], "train_batch_size": 64, "pomo_size": 64, "learning_rate": 0.0003, "weight_decay": 1e-06, "alpha": 0.05, "device": "cuda", "seed": 1234, "num_validation_episodes": 128, "validation_batch_size": 64, "generalization_penalty_weight": 1.0, "pool_version": "v0"}, "free_loss": {"f1_steps": 100, "f2_steps": 100, "f3_enabled": false}}, "loss_ir": {"name": "Adaptive Margin Sigmoid Loss", "intuition": "This loss function uses the relative cost difference to dynamically set a target margin for the log-probability difference. A larger cost gap demands a larger log-probability gap, encouraging the model to be more confident when the quality difference is significant. The `tanh` function is used to squash the normalized cost difference into a bounded, adaptive margin, preventing extreme cost gaps from creating excessively large or unstable targets. The final loss is a softplus of the difference between the target margin and the actual log-probability difference, which behaves like a hinge loss but is smooth and always non-negative, providing a stable gradient signal.", "hyperparams": {"beta": 5.0}, "operators_used": ["zscore", "tanh", "softplus"]}}, "better_than_baseline": false}
{"generation": 0, "index": 9, "ir": {"name": "Adaptive Margin Sigmoid Loss", "intuition": "This loss uses a sigmoid function to create a smooth, bounded loss landscape, preventing instability from extreme values. It introduces an adaptive margin based on the normalized cost difference. When the cost difference is large, the margin increases, demanding a stronger preference signal from the model. When the cost difference is small, the margin shrinks, allowing the model to be less certain. The cost difference is normalized using a robust z-score over a batch to handle varying problem scales. The temperature parameter `beta` controls the steepness of the loss curve, balancing between penalizing small and large preference violations.", "pseudocode": "1. Compute the cost difference: delta_cost = cost_a - cost_b.\n2. Compute the log probability difference: delta_logp = logp_a - logp_b.\n3. Normalize the cost difference across a batch of pairs to get a z-scored gap: cost_gap_norm = zscore(delta_cost).\n4. Create an adaptive margin: margin = margin_strength * tanh(cost_gap_norm). The tanh function bounds the margin, preventing it from becoming excessively large.\n5. The core argument for the loss is a scaled and margin-adjusted preference misalignment. We expect delta_logp to be positive if delta_cost is negative. So, we compute: arg = -beta * (delta_logp - margin).\n6. The final loss is the softplus of this argument, which is a smooth and non-negative approximation of relu(arg). This ensures the loss is always non-negative and penalizes misalignment (arg > 0).", "hyperparams": {"beta": 10.0, "margin_strength": 0.5}, "operators_used": ["zscore", "tanh", "softplus"], "implementation_hint": {"expects": ["cost_a", "cost_b", "logp_a", "logp_b"], "returns": "scalar"}}, "fitness": {"hf_like_score": 8.492331981658936, "validation_objective": 8.492331981658936, "generalization_penalty": 0.0, "generalization_objectives": {"20": 3.9859015941619873}, "train_score_mean": 10.472302541732788, "train_loss_mean": 0.598767985701561, "pair_count": 12902388, "config": {"hf": {"problem": "tsp", "hf_steps": 100, "train_problem_size": 100, "valid_problem_sizes": [20], "train_batch_size": 64, "pomo_size": 64, "learning_rate": 0.0003, "weight_decay": 1e-06, "alpha": 0.05, "device": "cuda", "seed": 1234, "num_validation_episodes": 128, "validation_batch_size": 64, "generalization_penalty_weight": 1.0, "pool_version": "v0"}, "free_loss": {"f1_steps": 100, "f2_steps": 100, "f3_enabled": false}}, "loss_ir": {"name": "Adaptive Margin Sigmoid Loss", "intuition": "This loss uses a sigmoid function to create a smooth, bounded loss landscape, preventing instability from extreme values. It introduces an adaptive margin based on the normalized cost difference. When the cost difference is large, the margin increases, demanding a stronger preference signal from the model. When the cost difference is small, the margin shrinks, allowing the model to be less certain. The cost difference is normalized using a robust z-score over a batch to handle varying problem scales. The temperature parameter `beta` controls the steepness of the loss curve, balancing between penalizing small and large preference violations.", "hyperparams": {"beta": 10.0, "margin_strength": 0.5}, "operators_used": ["zscore", "tanh", "softplus"]}}, "better_than_baseline": false}
{"generation": 1, "index": 0, "ir": {"name": "Adaptive Rank-Margin Sigmoid Loss", "intuition": "This loss function synthesizes ideas from both parents to create a robust and adaptive preference learning objective. From 'Normalized Rank-Gap Hinge Loss', we inherit the use of a rank-based signal (`rank_gap`) and a batch-level standardization (`zscore`) to create a context-aware measure of cost difference. This makes the loss less sensitive to the absolute scale of costs and more attuned to their relative ordering within a batch. From 'Adaptive Margin Sigmoid Loss', we inherit the core sigmoid loss structure (`logsigmoid`) which frames the problem as a binary classification task of predicting the correct preference. We also adopt its idea of scaling the log-probability difference. \n\nThe primary new coupling idea is a 'rank-scaled log-probability difference'. Instead of directly using `logp_a - logp_b`, we multiply it by a term derived from the standardized rank gap: `beta * softplus(rank_gap_zscored)`. This dynamically amplifies the model's preference signal (`logp_diff`) when the rank difference between solutions `a` and `b` is significant, and dampens it when they are close in rank. The `softplus` ensures this scaling factor is always positive and smooth. This coupling forces the model to learn not just the direction of preference but also to modulate the strength of its conviction based on the relative quality of the solutions. \n\nWhen `cost(a) < cost(b)`, the `rank_gap` is negative, making `rank_gap_zscored` likely negative. The target for `logp_a - logp_b` is positive. The loss term `logsigmoid(scaled_logp_diff)` encourages `scaled_logp_diff` to be large and positive, aligning the model's preference with the ground truth.", "pseudocode": "1. For each pair (a, b) in the batch, compute the log-probability difference: logp_diff = logp_a - logp_b.\n2. Compute the signed rank gap based on costs: cost_gap_signed = rank_gap(cost_a, cost_b). This will be negative if a is better, positive if b is better.\n3. Standardize the signed rank gap across the entire batch: rank_gap_zscored = zscore(cost_gap_signed).\n4. Create a dynamic scaling factor from the standardized rank gap. The softplus ensures it's a smooth, non-negative multiplier: rank_scale_factor = softplus(rank_gap_zscored).\n5. Compute the scaled log-probability difference. Note the negative sign: we want to maximize this term when cost_a < cost_b (i.e., rank_gap is negative). scaled_logp_diff = -logp_diff * (beta * rank_scale_factor).\n6. Calculate the final loss using logsigmoid. A large positive argument to logsigmoid results in a loss near zero. We negate the result to formulate it as a loss to be minimized: loss = -logsigmoid(scaled_logp_diff).", "hyperparams": {"beta": 2.0}, "operators_used": ["rank_gap", "zscore", "softplus", "logsigmoid"], "implementation_hint": {"expects": ["cost_a", "cost_b", "logp_a", "logp_b"], "returns": "scalar"}}, "fitness": {"hf_like_score": 8.448190689086914, "validation_objective": 8.448190689086914, "generalization_penalty": 0.0, "generalization_objectives": {"20": 4.030037879943848}, "train_score_mean": 10.464766721725464, "train_loss_mean": 0.5994095993041992, "pair_count": 12902390, "config": {"hf": {"problem": "tsp", "hf_steps": 100, "train_problem_size": 100, "valid_problem_sizes": [20], "train_batch_size": 64, "pomo_size": 64, "learning_rate": 0.0003, "weight_decay": 1e-06, "alpha": 0.05, "device": "cuda", "seed": 1234, "num_validation_episodes": 128, "validation_batch_size": 64, "generalization_penalty_weight": 1.0, "pool_version": "v0"}, "free_loss": {"f1_steps": 100, "f2_steps": 100, "f3_enabled": false}}, "loss_ir": {"name": "Adaptive Rank-Margin Sigmoid Loss", "intuition": "This loss function synthesizes ideas from both parents to create a robust and adaptive preference learning objective. From 'Normalized Rank-Gap Hinge Loss', we inherit the use of a rank-based signal (`rank_gap`) and a batch-level standardization (`zscore`) to create a context-aware measure of cost difference. This makes the loss less sensitive to the absolute scale of costs and more attuned to their relative ordering within a batch. From 'Adaptive Margin Sigmoid Loss', we inherit the core sigmoid loss structure (`logsigmoid`) which frames the problem as a binary classification task of predicting the correct preference. We also adopt its idea of scaling the log-probability difference. \n\nThe primary new coupling idea is a 'rank-scaled log-probability difference'. Instead of directly using `logp_a - logp_b`, we multiply it by a term derived from the standardized rank gap: `beta * softplus(rank_gap_zscored)`. This dynamically amplifies the model's preference signal (`logp_diff`) when the rank difference between solutions `a` and `b` is significant, and dampens it when they are close in rank. The `softplus` ensures this scaling factor is always positive and smooth. This coupling forces the model to learn not just the direction of preference but also to modulate the strength of its conviction based on the relative quality of the solutions. \n\nWhen `cost(a) < cost(b)`, the `rank_gap` is negative, making `rank_gap_zscored` likely negative. The target for `logp_a - logp_b` is positive. The loss term `logsigmoid(scaled_logp_diff)` encourages `scaled_logp_diff` to be large and positive, aligning the model's preference with the ground truth.", "hyperparams": {"beta": 2.0}, "operators_used": ["rank_gap", "zscore", "softplus", "logsigmoid"]}}, "better_than_baseline": false}
{"generation": 1, "index": 1, "ir": {"name": "Rank-Aware Sigmoid Hinge Loss", "intuition": "This loss function synthesizes a hinge-like structure with a sigmoid-based preference signal. From Parent A (Normalized Rank-Gap Hinge Loss), we inherit the use of a dynamic margin based on the rank gap of costs, which scales the learning signal based on the relative quality of solutions within a batch. From Parent B (Adaptive Margin Sigmoid Loss), we inherit the core idea of using a sigmoid-like structure to frame the preference learning problem, specifically using `logsigmoid` to model the probability of preferring one solution over another. The child introduces two new coupling ideas: 1. The hinge loss is not applied directly to the log-probability difference, but rather to the output of a `logsigmoid` function, which bounds the model's preference signal between (-inf, 0] and makes the loss more robust to outlier log-probabilities. 2. A new `tau` hyperparameter is introduced to scale the raw log-probability difference before it enters the sigmoid, controlling the steepness of the preference curve and adding a temperature-like control. The final loss is `softplus(margin - logsigmoid(tau * logp_diff))`. When `cost(a) < cost(b)`, the `margin` becomes positive, pushing `logsigmoid(tau * logp_diff)` to be larger (i.e., closer to 0), which requires `logp_a > logp_b`.", "pseudocode": "1. Compute the log-probability difference: logp_diff = logp_a - logp_b.\n2. Compute the signed, normalized cost gap based on ranks within the batch: cost_gap_signed = rank_gap(cost_a, cost_b).\n3. Standardize the cost gap across the batch to create a stable signal: cost_gap_zscored = zscore(cost_gap_signed).\n4. Create a bounded, dynamic margin from the standardized rank gap (inherited from Parent A): dynamic_margin = beta * tanh(cost_gap_zscored).\n5. Compute a scaled and bounded model preference signal using logsigmoid (inspired by Parent B): preference_signal = logsigmoid(tau * logp_diff).\n6. Calculate the final loss using a smooth hinge structure (softplus) that compares the preference signal against the dynamic margin: loss = softplus(dynamic_margin - preference_signal).", "hyperparams": {"beta": 1.0, "tau": 1.0}, "operators_used": ["rank_gap", "zscore", "tanh", "logsigmoid", "softplus"], "implementation_hint": {"expects": ["cost_a", "cost_b", "logp_a", "logp_b"], "returns": "scalar"}}, "fitness": {"hf_like_score": 8.477613925933838, "validation_objective": 8.477613925933838, "generalization_penalty": 0.0, "generalization_objectives": {"20": 3.9906249046325684}, "train_score_mean": 10.4861811542511, "train_loss_mean": 0.5990426772832871, "pair_count": 12902388, "config": {"hf": {"problem": "tsp", "hf_steps": 100, "train_problem_size": 100, "valid_problem_sizes": [20], "train_batch_size": 64, "pomo_size": 64, "learning_rate": 0.0003, "weight_decay": 1e-06, "alpha": 0.05, "device": "cuda", "seed": 1234, "num_validation_episodes": 128, "validation_batch_size": 64, "generalization_penalty_weight": 1.0, "pool_version": "v0"}, "free_loss": {"f1_steps": 100, "f2_steps": 100, "f3_enabled": false}}, "loss_ir": {"name": "Rank-Aware Sigmoid Hinge Loss", "intuition": "This loss function synthesizes a hinge-like structure with a sigmoid-based preference signal. From Parent A (Normalized Rank-Gap Hinge Loss), we inherit the use of a dynamic margin based on the rank gap of costs, which scales the learning signal based on the relative quality of solutions within a batch. From Parent B (Adaptive Margin Sigmoid Loss), we inherit the core idea of using a sigmoid-like structure to frame the preference learning problem, specifically using `logsigmoid` to model the probability of preferring one solution over another. The child introduces two new coupling ideas: 1. The hinge loss is not applied directly to the log-probability difference, but rather to the output of a `logsigmoid` function, which bounds the model's preference signal between (-inf, 0] and makes the loss more robust to outlier log-probabilities. 2. A new `tau` hyperparameter is introduced to scale the raw log-probability difference before it enters the sigmoid, controlling the steepness of the preference curve and adding a temperature-like control. The final loss is `softplus(margin - logsigmoid(tau * logp_diff))`. When `cost(a) < cost(b)`, the `margin` becomes positive, pushing `logsigmoid(tau * logp_diff)` to be larger (i.e., closer to 0), which requires `logp_a > logp_b`.", "hyperparams": {"beta": 1.0, "tau": 1.0}, "operators_used": ["rank_gap", "zscore", "tanh", "logsigmoid", "softplus"]}}, "better_than_baseline": false}
{"generation": 1, "index": 2, "ir": {"name": "Rank-Adaptive Sigmoid Hinge Loss", "intuition": "This loss function synthesizes a dynamic margin from rank-based statistics with a sigmoid-based preference term. From 'Normalized Rank-Gap Hinge Loss' (Parent 1), we inherit the use of a z-scored rank gap (`rank_gap` and `zscore`) to create a batch-adaptive signal representing relative cost improvement. This provides a robust, outlier-resistant measure of how much better one solution is compared to others in the batch. From 'Adaptive Margin Sigmoid Loss' (Parent 2), we inherit the core sigmoid loss structure, which frames the problem as correctly classifying the preference direction, and the use of `tanh` to create a bounded margin from a cost-related signal. \n\nThe primary new coupling idea is to use the rank-based signal not just as a margin, but as a scaling factor inside the sigmoid function itself. Specifically, we compute a `rank_margin` using `tanh` on the z-scored rank gap, which is then multiplied by a fixed `beta` and the log-probability difference (`logp_a - logp_b`). This product `rank_margin * (logp_a - logp_b)` becomes the argument to a `logsigmoid` function. This novel coupling makes the loss curvature itself adaptive: when the rank gap is large (one solution is clearly better), the margin is large, and the sigmoid becomes steeper, demanding a confident and correct preference from the model. When the rank gap is small, the margin shrinks, and the sigmoid becomes flatter, tolerating more ambiguity. A second new idea is to add a small, fixed hinge term using `softplus` on top of the sigmoid loss. This ensures that even for correctly classified pairs, there is a small penalty if the magnitude of the model's preference (`logp_a - logp_b`) is not sufficiently large, preventing the model from becoming overly complacent. When `cost_a < cost_b`, `rank_gap` is positive, making `rank_margin` positive. The loss encourages `logp_a - logp_b` to be positive, thus preferring `a` over `b`.", "pseudocode": "1. Compute the log-probability difference: logp_diff = logp_a - logp_b.\n2. Compute the signed rank gap based on costs: cost_rank_gap = rank_gap(cost_a, cost_b).\n3. Standardize the rank gap across the batch: cost_rank_zscored = zscore(cost_rank_gap).\n4. Create a bounded, rank-based adaptive margin: rank_margin = tanh(cost_rank_zscored).\n5. Compute the primary sigmoid loss term. The loss is the negative log-likelihood of the correctly classified preference, scaled by the adaptive rank margin: sigmoid_loss = -logsigmoid(beta * rank_margin * logp_diff).\n6. Compute a secondary, small hinge loss term to enforce a minimum preference magnitude. This penalizes the model if its preference is not strong enough in the correct direction: hinge_loss = softplus(hinge_margin - rank_margin * logp_diff).\n7. Combine the two terms to get the final loss: loss = sigmoid_loss + hinge_scale * hinge_loss.", "hyperparams": {"beta": 2.0, "hinge_margin": 0.1, "hinge_scale": 0.05}, "operators_used": ["rank_gap", "zscore", "tanh", "logsigmoid", "softplus"], "implementation_hint": {"expects": ["cost_a", "cost_b", "logp_a", "logp_b"], "returns": "scalar"}}, "fitness": {"hf_like_score": 8.50074291229248, "validation_objective": 8.50074291229248, "generalization_penalty": 0.0, "generalization_objectives": {"20": 4.0464136600494385}, "train_score_mean": 10.474278478622436, "train_loss_mean": 0.5995902049541474, "pair_count": 12902385, "config": {"hf": {"problem": "tsp", "hf_steps": 100, "train_problem_size": 100, "valid_problem_sizes": [20], "train_batch_size": 64, "pomo_size": 64, "learning_rate": 0.0003, "weight_decay": 1e-06, "alpha": 0.05, "device": "cuda", "seed": 1234, "num_validation_episodes": 128, "validation_batch_size": 64, "generalization_penalty_weight": 1.0, "pool_version": "v0"}, "free_loss": {"f1_steps": 100, "f2_steps": 100, "f3_enabled": false}}, "loss_ir": {"name": "Rank-Adaptive Sigmoid Hinge Loss", "intuition": "This loss function synthesizes a dynamic margin from rank-based statistics with a sigmoid-based preference term. From 'Normalized Rank-Gap Hinge Loss' (Parent 1), we inherit the use of a z-scored rank gap (`rank_gap` and `zscore`) to create a batch-adaptive signal representing relative cost improvement. This provides a robust, outlier-resistant measure of how much better one solution is compared to others in the batch. From 'Adaptive Margin Sigmoid Loss' (Parent 2), we inherit the core sigmoid loss structure, which frames the problem as correctly classifying the preference direction, and the use of `tanh` to create a bounded margin from a cost-related signal. \n\nThe primary new coupling idea is to use the rank-based signal not just as a margin, but as a scaling factor inside the sigmoid function itself. Specifically, we compute a `rank_margin` using `tanh` on the z-scored rank gap, which is then multiplied by a fixed `beta` and the log-probability difference (`logp_a - logp_b`). This product `rank_margin * (logp_a - logp_b)` becomes the argument to a `logsigmoid` function. This novel coupling makes the loss curvature itself adaptive: when the rank gap is large (one solution is clearly better), the margin is large, and the sigmoid becomes steeper, demanding a confident and correct preference from the model. When the rank gap is small, the margin shrinks, and the sigmoid becomes flatter, tolerating more ambiguity. A second new idea is to add a small, fixed hinge term using `softplus` on top of the sigmoid loss. This ensures that even for correctly classified pairs, there is a small penalty if the magnitude of the model's preference (`logp_a - logp_b`) is not sufficiently large, preventing the model from becoming overly complacent. When `cost_a < cost_b`, `rank_gap` is positive, making `rank_margin` positive. The loss encourages `logp_a - logp_b` to be positive, thus preferring `a` over `b`.", "hyperparams": {"beta": 2.0, "hinge_margin": 0.1, "hinge_scale": 0.05}, "operators_used": ["rank_gap", "zscore", "tanh", "logsigmoid", "softplus"]}}, "better_than_baseline": false}
{"generation": 1, "index": 3, "ir": {"name": "Rank-Aware Adaptive Sigmoid Loss", "intuition": "This loss function combines the rank-based margin from the Normalized Rank-Gap Hinge Loss (Parent 0) with the sigmoid-based preference structure from the Adaptive Margin Sigmoid Loss (Parent 1). From Parent 0, we inherit the use of `rank_gap` to create a discrete, batch-aware signal that is robust to the absolute scale of costs. From Parent 1, we inherit the core sigmoid loss structure, which frames the problem as learning the probability of preferring one solution over another. The new coupling idea is to use the rank-based signal to directly define the target probability for the sigmoid loss. Instead of a fixed target (like 1.0 for the better solution), the target is dynamically set using a scaled and shifted `tanh` of the rank gap. This makes the target more aggressive (closer to 0 or 1) for widely separated ranks and more lenient (closer to 0.5) for adjacent ranks, allowing the model to be less certain about pairs with similar quality. A second coupling mechanism is a beta-scaled log-probability difference, which controls the steepness of the sigmoid curve and modulates the model's confidence.", "pseudocode": "1. Compute the log-probability difference: logp_diff = logp_a - logp_b.\n2. Compute the signed, normalized cost gap based on ranks within the batch: cost_gap_signed = rank_gap(cost_a, cost_b).\n3. Create a dynamic target probability from the rank gap. We use tanh to bound the gap, then scale and shift it to the [0, 1] range. A larger positive rank gap (a is much better than b) pushes the target towards 1.0. A larger negative rank gap pushes it towards 0.0: target_prob = 0.5 * (1 + tanh(cost_gap_signed)).\n4. Compute the model's predicted probability of preferring 'a' over 'b', scaled by beta: model_prob = sigmoid(beta * logp_diff).\n5. Calculate the loss as the negative log-likelihood (cross-entropy) between the model's probability and the dynamic target probability. We use log(sigmoid(x)) = -softplus(-x) for numerical stability: loss = -(target_prob * -softplus(-beta * logp_diff) + (1 - target_prob) * -softplus(beta * logp_diff)).", "hyperparams": {"beta": 1.0}, "operators_used": ["rank_gap", "tanh", "sigmoid", "softplus"], "implementation_hint": {"expects": ["cost_a", "cost_b", "logp_a", "logp_b"], "returns": "scalar"}}, "fitness": {"hf_like_score": 8.487631797790527, "validation_objective": 8.487631797790527, "generalization_penalty": 0.0, "generalization_objectives": {"20": 3.9808475971221924}, "train_score_mean": 10.510965690612792, "train_loss_mean": 0.6009347677230835, "pair_count": 12902390, "config": {"hf": {"problem": "tsp", "hf_steps": 100, "train_problem_size": 100, "valid_problem_sizes": [20], "train_batch_size": 64, "pomo_size": 64, "learning_rate": 0.0003, "weight_decay": 1e-06, "alpha": 0.05, "device": "cuda", "seed": 1234, "num_validation_episodes": 128, "validation_batch_size": 64, "generalization_penalty_weight": 1.0, "pool_version": "v0"}, "free_loss": {"f1_steps": 100, "f2_steps": 100, "f3_enabled": false}}, "loss_ir": {"name": "Rank-Aware Adaptive Sigmoid Loss", "intuition": "This loss function combines the rank-based margin from the Normalized Rank-Gap Hinge Loss (Parent 0) with the sigmoid-based preference structure from the Adaptive Margin Sigmoid Loss (Parent 1). From Parent 0, we inherit the use of `rank_gap` to create a discrete, batch-aware signal that is robust to the absolute scale of costs. From Parent 1, we inherit the core sigmoid loss structure, which frames the problem as learning the probability of preferring one solution over another. The new coupling idea is to use the rank-based signal to directly define the target probability for the sigmoid loss. Instead of a fixed target (like 1.0 for the better solution), the target is dynamically set using a scaled and shifted `tanh` of the rank gap. This makes the target more aggressive (closer to 0 or 1) for widely separated ranks and more lenient (closer to 0.5) for adjacent ranks, allowing the model to be less certain about pairs with similar quality. A second coupling mechanism is a beta-scaled log-probability difference, which controls the steepness of the sigmoid curve and modulates the model's confidence.", "hyperparams": {"beta": 1.0}, "operators_used": ["rank_gap", "tanh", "sigmoid", "softplus"]}}, "better_than_baseline": false}
{"generation": 1, "index": 4, "ir": {"name": "Adaptive Rank-Margin Sigmoid Loss", "intuition": "This loss function synthesizes ideas from its parents to create a stable and adaptive preference learning objective. From 'Normalized Rank-Gap Hinge Loss' (Parent A), it inherits the use of a signed, rank-based cost gap (`rank_gap`) and the `zscore` normalization across a batch. This makes the loss robust to the absolute scale of costs and focuses on relative rankings. From 'Adaptive Margin Sigmoid Loss' (Parent B), it inherits the core sigmoid-based structure (`logsigmoid`) for comparing model preference (`logp_a - logp_b`) against a target preference. The new coupling idea is how these inherited concepts are integrated. We create an adaptive margin, `dynamic_margin`, by scaling the z-scored rank gap. This margin is then used to modulate the argument of a `logsigmoid` function. Specifically, we want the model's log-probability difference (`logp_diff`) to be greater than this dynamic margin when `a` is better than `b`. The `logsigmoid` function smoothly penalizes deviations from this target. A second new idea is the use of `clamp` on the z-scored rank gap before scaling, which adds a stability trick to prevent outlier batches from creating excessively large margins, ensuring the loss remains well-behaved. When `cost(a) < cost(b)`, the `dynamic_margin` becomes positive, and the loss encourages `logp_a > logp_b` to make the argument to `logsigmoid` positive, minimizing the loss.", "pseudocode": "1. Compute the log-probability difference: logp_diff = logp_a - logp_b.\n2. Compute the signed rank gap of the costs: cost_gap_signed = rank_gap(cost_a, cost_b).\n3. Standardize the signed rank gap across the batch: cost_gap_zscored = zscore(cost_gap_signed).\n4. Clamp the standardized gap to prevent extreme values and enhance stability: clamped_gap = clamp(cost_gap_zscored, -clamp_range, clamp_range).\n5. Create an adaptive dynamic margin by scaling the clamped, standardized rank gap: dynamic_margin = beta * clamped_gap.\n6. The loss is calculated based on how well the log-probability difference meets the dynamic margin. The argument to the logsigmoid function is the difference between the model's preference and the target margin: loss_arg = logp_diff - dynamic_margin.\n7. Calculate the final loss using logsigmoid, which penalizes cases where the loss argument is negative (i.e., when logp_diff < dynamic_margin): loss = -logsigmoid(loss_arg).", "hyperparams": {"beta": 1.0, "clamp_range": 3.0}, "operators_used": ["rank_gap", "zscore", "clamp", "logsigmoid"], "implementation_hint": {"expects": ["cost_a", "cost_b", "logp_a", "logp_b"], "returns": "scalar"}}, "fitness": {"hf_like_score": 8.43294620513916, "validation_objective": 8.43294620513916, "generalization_penalty": 0.0, "generalization_objectives": {"20": 4.009204268455505}, "train_score_mean": 10.47082262992859, "train_loss_mean": 0.5993068927526474, "pair_count": 12902387, "config": {"hf": {"problem": "tsp", "hf_steps": 100, "train_problem_size": 100, "valid_problem_sizes": [20], "train_batch_size": 64, "pomo_size": 64, "learning_rate": 0.0003, "weight_decay": 1e-06, "alpha": 0.05, "device": "cuda", "seed": 1234, "num_validation_episodes": 128, "validation_batch_size": 64, "generalization_penalty_weight": 1.0, "pool_version": "v0"}, "free_loss": {"f1_steps": 100, "f2_steps": 100, "f3_enabled": false}}, "loss_ir": {"name": "Adaptive Rank-Margin Sigmoid Loss", "intuition": "This loss function synthesizes ideas from its parents to create a stable and adaptive preference learning objective. From 'Normalized Rank-Gap Hinge Loss' (Parent A), it inherits the use of a signed, rank-based cost gap (`rank_gap`) and the `zscore` normalization across a batch. This makes the loss robust to the absolute scale of costs and focuses on relative rankings. From 'Adaptive Margin Sigmoid Loss' (Parent B), it inherits the core sigmoid-based structure (`logsigmoid`) for comparing model preference (`logp_a - logp_b`) against a target preference. The new coupling idea is how these inherited concepts are integrated. We create an adaptive margin, `dynamic_margin`, by scaling the z-scored rank gap. This margin is then used to modulate the argument of a `logsigmoid` function. Specifically, we want the model's log-probability difference (`logp_diff`) to be greater than this dynamic margin when `a` is better than `b`. The `logsigmoid` function smoothly penalizes deviations from this target. A second new idea is the use of `clamp` on the z-scored rank gap before scaling, which adds a stability trick to prevent outlier batches from creating excessively large margins, ensuring the loss remains well-behaved. When `cost(a) < cost(b)`, the `dynamic_margin` becomes positive, and the loss encourages `logp_a > logp_b` to make the argument to `logsigmoid` positive, minimizing the loss.", "hyperparams": {"beta": 1.0, "clamp_range": 3.0}, "operators_used": ["rank_gap", "zscore", "clamp", "logsigmoid"]}}, "better_than_baseline": true}
{"generation": 1, "index": 5, "ir": {"name": "Rank-Calibrated Sigmoid Loss", "intuition": "This loss function combines a rank-based margin with a sigmoid-based preference structure, introducing a new coupling mechanism to dynamically adjust the loss's steepness. From the 'Normalized Rank-Gap Hinge Loss' (Parent 1), we inherit the use of a standardized rank gap (`zscore(rank_gap(cost_a, cost_b))`) to create a robust, outlier-resistant signal of cost difference. From the 'Adaptive Margin Sigmoid Loss' (Parent 2), we inherit the core sigmoid loss structure, which frames the problem as a binary classification task where the model's preference (`logp_a - logp_b`) should align with the ground truth. \n\nThe first new coupling idea is using the standardized rank gap to create a dynamic margin (`beta * tanh(cost_gap_zscored)`) within the sigmoid loss. This means the required log-probability difference scales with the relative ranking of costs in the batch. The second new coupling mechanism is an adaptive temperature `gamma`. Instead of a fixed temperature, `gamma` is scaled by the absolute value of the dynamic margin. When the cost difference is significant (large margin), `gamma` increases, making the sigmoid function steeper and demanding a more confident prediction from the model. When the cost difference is small (small margin), `gamma` decreases, allowing for more ambiguity. The final loss is `logsigmoid` applied to the scaled and margin-adjusted log-probability difference. When `cost(a) < cost(b)`, the `rank_gap` is negative, making the `dynamic_margin` negative. The loss then encourages `logp_a - logp_b` to be positive, correctly preferring `a`.", "pseudocode": "1. Compute the log-probability difference: logp_diff = logp_a - logp_b.\n2. Compute the signed, batch-relative rank gap: cost_gap_signed = rank_gap(cost_a, cost_b).\n3. Standardize the rank gap across the batch: cost_gap_zscored = zscore(cost_gap_signed).\n4. Create a bounded dynamic margin from the standardized rank gap (inherited from Parent 1): dynamic_margin = beta * tanh(cost_gap_zscored).\n5. Create an adaptive temperature based on the margin (new coupling idea): adaptive_gamma = 1.0 + gamma * abs(dynamic_margin).\n6. The core argument for the sigmoid is the difference between the model's preference and the margin. We want `logp_diff` to be greater than `dynamic_margin`. The expression `logp_diff - dynamic_margin` captures this. The sign is handled by the `dynamic_margin` itself (e.g., if a is better, margin is negative, so we want `logp_diff` to be positive).\n7. Scale the argument by the adaptive temperature and compute the final loss using `logsigmoid` (structure inspired by Parent 2): loss = -logsigmoid(adaptive_gamma * (logp_diff - dynamic_margin)).", "hyperparams": {"beta": 1.0, "gamma": 0.5}, "operators_used": ["rank_gap", "zscore", "tanh", "logsigmoid"], "implementation_hint": {"expects": ["cost_a", "cost_b", "logp_a", "logp_b"], "returns": "scalar"}}, "fitness": {"hf_like_score": 8.429159164428711, "validation_objective": 8.429159164428711, "generalization_penalty": 0.0, "generalization_objectives": {"20": 3.9786012172698975}, "train_score_mean": 10.462283096313477, "train_loss_mean": 0.6015135997533798, "pair_count": 12902390, "config": {"hf": {"problem": "tsp", "hf_steps": 100, "train_problem_size": 100, "valid_problem_sizes": [20], "train_batch_size": 64, "pomo_size": 64, "learning_rate": 0.0003, "weight_decay": 1e-06, "alpha": 0.05, "device": "cuda", "seed": 1234, "num_validation_episodes": 128, "validation_batch_size": 64, "generalization_penalty_weight": 1.0, "pool_version": "v0"}, "free_loss": {"f1_steps": 100, "f2_steps": 100, "f3_enabled": false}}, "loss_ir": {"name": "Rank-Calibrated Sigmoid Loss", "intuition": "This loss function combines a rank-based margin with a sigmoid-based preference structure, introducing a new coupling mechanism to dynamically adjust the loss's steepness. From the 'Normalized Rank-Gap Hinge Loss' (Parent 1), we inherit the use of a standardized rank gap (`zscore(rank_gap(cost_a, cost_b))`) to create a robust, outlier-resistant signal of cost difference. From the 'Adaptive Margin Sigmoid Loss' (Parent 2), we inherit the core sigmoid loss structure, which frames the problem as a binary classification task where the model's preference (`logp_a - logp_b`) should align with the ground truth. \n\nThe first new coupling idea is using the standardized rank gap to create a dynamic margin (`beta * tanh(cost_gap_zscored)`) within the sigmoid loss. This means the required log-probability difference scales with the relative ranking of costs in the batch. The second new coupling mechanism is an adaptive temperature `gamma`. Instead of a fixed temperature, `gamma` is scaled by the absolute value of the dynamic margin. When the cost difference is significant (large margin), `gamma` increases, making the sigmoid function steeper and demanding a more confident prediction from the model. When the cost difference is small (small margin), `gamma` decreases, allowing for more ambiguity. The final loss is `logsigmoid` applied to the scaled and margin-adjusted log-probability difference. When `cost(a) < cost(b)`, the `rank_gap` is negative, making the `dynamic_margin` negative. The loss then encourages `logp_a - logp_b` to be positive, correctly preferring `a`.", "hyperparams": {"beta": 1.0, "gamma": 0.5}, "operators_used": ["rank_gap", "zscore", "tanh", "logsigmoid"]}}, "better_than_baseline": true}
{"generation": 1, "index": 6, "ir": {"name": "Z-Scored Rank-Gap Sigmoid Loss", "intuition": "This loss function synthesizes a rank-based margin with a sigmoid preference structure. From Normalized Rank-Gap Hinge Loss (Parent 1), we inherit the use of a standardized rank gap, `zscore(rank_gap(cost_a, cost_b))`, to create a robust signal of relative cost difference that is insensitive to the absolute scale of costs. From Adaptive Margin Sigmoid Loss (Parent 2), we inherit the core sigmoid loss structure, which frames the problem as learning the probability that `a` is preferred over `b`. The primary new coupling idea is to use the z-scored rank gap directly as a dynamic margin inside the sigmoid function. Specifically, we multiply the log-probability difference `(logp_a - logp_b)` by a scaling factor `beta` and compare it to this rank-based margin. This creates an objective `beta * (logp_a - logp_b) - zscore(rank_gap(cost_a, cost_b))`. When `cost(a) < cost(b)`, the rank gap is positive, and the loss encourages `logp_a` to be greater than `logp_b`. The `logsigmoid` function then smoothly penalizes deviations from this desired state. A secondary coupling idea is the clamping of the z-scored rank gap before it is used as a margin, which prevents extremely rare cost differences (outliers) from creating an unstable or exploding loss signal, ensuring numerical stability.", "pseudocode": "1. Compute the log-probability difference: logp_diff = logp_a - logp_b.\n2. Compute the signed rank gap based on costs: rank_gap_signed = rank_gap(cost_a, cost_b).\n3. Standardize the rank gap across the batch to get a z-score: rank_gap_zscored = zscore(rank_gap_signed).\n4. (New Stability Trick) Clamp the z-scored rank gap to prevent extreme values from dominating the loss: clamped_rank_gap = clamp(rank_gap_zscored, min=-clamp_val, max=clamp_val).\n5. (New Coupling) Form the argument for the sigmoid loss by combining the scaled log-probability difference and the clamped rank gap. The rank gap acts as a dynamic, batch-normalized margin. The target is for `beta * logp_diff` to be greater than `clamped_rank_gap`: loss_argument = beta * logp_diff - clamped_rank_gap.\n6. Compute the final loss using a numerically stable logistic sigmoid loss formulation: loss = -logsigmoid(loss_argument).", "hyperparams": {"beta": 1.0, "clamp_val": 5.0}, "operators_used": ["rank_gap", "zscore", "clamp", "logsigmoid"], "implementation_hint": {"expects": ["cost_a", "cost_b", "logp_a", "logp_b"], "returns": "scalar"}}, "fitness": {"hf_like_score": 8.495620250701904, "validation_objective": 8.495620250701904, "generalization_penalty": 0.0, "generalization_objectives": {"20": 3.9996750354766846}, "train_score_mean": 10.52146237373352, "train_loss_mean": 0.5991892874240875, "pair_count": 12902386, "config": {"hf": {"problem": "tsp", "hf_steps": 100, "train_problem_size": 100, "valid_problem_sizes": [20], "train_batch_size": 64, "pomo_size": 64, "learning_rate": 0.0003, "weight_decay": 1e-06, "alpha": 0.05, "device": "cuda", "seed": 1234, "num_validation_episodes": 128, "validation_batch_size": 64, "generalization_penalty_weight": 1.0, "pool_version": "v0"}, "free_loss": {"f1_steps": 100, "f2_steps": 100, "f3_enabled": false}}, "loss_ir": {"name": "Z-Scored Rank-Gap Sigmoid Loss", "intuition": "This loss function synthesizes a rank-based margin with a sigmoid preference structure. From Normalized Rank-Gap Hinge Loss (Parent 1), we inherit the use of a standardized rank gap, `zscore(rank_gap(cost_a, cost_b))`, to create a robust signal of relative cost difference that is insensitive to the absolute scale of costs. From Adaptive Margin Sigmoid Loss (Parent 2), we inherit the core sigmoid loss structure, which frames the problem as learning the probability that `a` is preferred over `b`. The primary new coupling idea is to use the z-scored rank gap directly as a dynamic margin inside the sigmoid function. Specifically, we multiply the log-probability difference `(logp_a - logp_b)` by a scaling factor `beta` and compare it to this rank-based margin. This creates an objective `beta * (logp_a - logp_b) - zscore(rank_gap(cost_a, cost_b))`. When `cost(a) < cost(b)`, the rank gap is positive, and the loss encourages `logp_a` to be greater than `logp_b`. The `logsigmoid` function then smoothly penalizes deviations from this desired state. A secondary coupling idea is the clamping of the z-scored rank gap before it is used as a margin, which prevents extremely rare cost differences (outliers) from creating an unstable or exploding loss signal, ensuring numerical stability.", "hyperparams": {"beta": 1.0, "clamp_val": 5.0}, "operators_used": ["rank_gap", "zscore", "clamp", "logsigmoid"]}}, "better_than_baseline": false}
{"generation": 1, "index": 7, "ir": {"name": "Rank-Gated Adaptive Sigmoid Loss", "intuition": "This loss function combines a dynamic margin with a rank-based gating mechanism. The goal is to create a loss that is sensitive to both the absolute magnitude of cost differences and their relative ranking within a batch.\n\nFrom 'Normalized Rank-Gap Hinge Loss' (Parent 0), we inherit the use of a standardized, rank-based cost gap (`rank_gap` followed by `zscore`). This captures the relative importance of a preference pair within the context of the current batch, making the loss robust to the overall scale of costs.\n\nFrom 'Adaptive Margin Sigmoid Loss' (Parent 1), we inherit the core structure of an adaptive margin based on a normalized cost signal, specifically using `tanh` to create a bounded margin and `softplus` to compute the final smooth, non-negative loss. This ensures the required preference strength scales with the cost difference.\n\nAs a new coupling idea, we introduce a 'rank-gating' mechanism. The adaptive margin from Parent 1 (based on `tanh(cost_a - cost_b)`) is multiplicatively gated by the standardized rank gap from Parent 0. This means the margin is not only proportional to the cost difference but is also amplified or dampened based on its batch-wise rank. A pair with a high rank-gap (i.e., a very clear preference) will have its margin amplified, demanding a stronger signal from the model. Conversely, a pair with a low rank-gap will see its margin shrink, making the loss more tolerant of model uncertainty. This coupling ensures the model focuses its learning on the most unambiguously ranked pairs in each batch. We also use `logsigmoid` for the final loss calculation, which is a common and stable choice for binary classification-style preference tasks.", "pseudocode": "1. Compute the log-probability difference: logp_diff = logp_a - logp_b.\n2. Compute the signed cost difference: cost_diff = cost_a - cost_b.\n3. Compute the signed, normalized cost rank gap across the batch: cost_rank_gap = rank_gap(cost_a, cost_b).\n4. Standardize the rank gap across the batch: cost_rank_gap_zscored = zscore(cost_rank_gap).\n5. Create a base adaptive margin from the cost difference: base_margin = margin_scale * tanh(cost_diff / temp).\n6. Create a rank-based gate. We use softplus to ensure the gate is non-negative and scales with the rank gap's importance: rank_gate = softplus(cost_rank_gap_zscored).\n7. Couple the base margin and the rank gate to create the final dynamic margin: dynamic_margin = rank_gate * base_margin.\n8. Compute the final loss. The loss encourages the log-probability difference to align with the dynamic margin. We use logsigmoid for stability. The term inside should be positive when the model is correct, so we want sign(cost_diff) * (logp_diff - dynamic_margin) to be large. The loss is the negative of this term passed through logsigmoid.\n9. loss = -logsigmoid(sign(cost_diff) * (logp_diff - dynamic_margin)).", "hyperparams": {"margin_scale": 1.0, "temp": 10.0}, "operators_used": ["rank_gap", "zscore", "tanh", "softplus", "logsigmoid"], "implementation_hint": {"expects": ["cost_a", "cost_b", "logp_a", "logp_b"], "returns": "scalar"}}, "fitness": {"hf_like_score": 8.452736377716064, "validation_objective": 8.452736377716064, "generalization_penalty": 0.0, "generalization_objectives": {"20": 4.00263249874115}, "train_score_mean": 10.481736507415771, "train_loss_mean": 0.6015222579240799, "pair_count": 12902391, "config": {"hf": {"problem": "tsp", "hf_steps": 100, "train_problem_size": 100, "valid_problem_sizes": [20], "train_batch_size": 64, "pomo_size": 64, "learning_rate": 0.0003, "weight_decay": 1e-06, "alpha": 0.05, "device": "cuda", "seed": 1234, "num_validation_episodes": 128, "validation_batch_size": 64, "generalization_penalty_weight": 1.0, "pool_version": "v0"}, "free_loss": {"f1_steps": 100, "f2_steps": 100, "f3_enabled": false}}, "loss_ir": {"name": "Rank-Gated Adaptive Sigmoid Loss", "intuition": "This loss function combines a dynamic margin with a rank-based gating mechanism. The goal is to create a loss that is sensitive to both the absolute magnitude of cost differences and their relative ranking within a batch.\n\nFrom 'Normalized Rank-Gap Hinge Loss' (Parent 0), we inherit the use of a standardized, rank-based cost gap (`rank_gap` followed by `zscore`). This captures the relative importance of a preference pair within the context of the current batch, making the loss robust to the overall scale of costs.\n\nFrom 'Adaptive Margin Sigmoid Loss' (Parent 1), we inherit the core structure of an adaptive margin based on a normalized cost signal, specifically using `tanh` to create a bounded margin and `softplus` to compute the final smooth, non-negative loss. This ensures the required preference strength scales with the cost difference.\n\nAs a new coupling idea, we introduce a 'rank-gating' mechanism. The adaptive margin from Parent 1 (based on `tanh(cost_a - cost_b)`) is multiplicatively gated by the standardized rank gap from Parent 0. This means the margin is not only proportional to the cost difference but is also amplified or dampened based on its batch-wise rank. A pair with a high rank-gap (i.e., a very clear preference) will have its margin amplified, demanding a stronger signal from the model. Conversely, a pair with a low rank-gap will see its margin shrink, making the loss more tolerant of model uncertainty. This coupling ensures the model focuses its learning on the most unambiguously ranked pairs in each batch. We also use `logsigmoid` for the final loss calculation, which is a common and stable choice for binary classification-style preference tasks.", "hyperparams": {"margin_scale": 1.0, "temp": 10.0}, "operators_used": ["rank_gap", "zscore", "tanh", "softplus", "logsigmoid"]}}, "better_than_baseline": false}
{"generation": 1, "index": 8, "ir": {"name": "Rank-Gapped Sigmoid Loss with Temperature Scaling", "intuition": "This loss function synthesizes ideas from its parents to create a stable and adaptive preference learning objective. From 'Normalized Rank-Gap Hinge Loss', it inherits the use of `rank_gap` to measure the difference between costs, which provides robustness to the scale and distribution of cost values. From 'Adaptive Margin Sigmoid Loss', it inherits the core sigmoid-based loss structure, which frames preference learning as a logistic regression problem where the goal is to predict the correct preference direction. \n\nThe child introduces two new coupling ideas. First, it uses the rank gap directly within the sigmoid function, scaled by a temperature parameter `temp`, to dynamically adjust the steepness of the loss curve. When the rank gap is large, the sigmoid becomes steeper, demanding a confident prediction from the model. When the rank gap is small, the sigmoid is gentler, tolerating more ambiguity. Second, it introduces a fixed `margin` hyperparameter that shifts the decision boundary, ensuring that even for correctly ordered pairs, a certain level of preference strength is encouraged. The `logsigmoid` function is used for numerical stability. When `cost(a) < cost(b)`, the rank gap is negative, and the loss encourages `logp_a - logp_b` to be positive, thus preferring solution `a`.", "pseudocode": "1. Compute the log-probability difference: logp_diff = logp_a - logp_b.\n2. Compute the signed rank gap between costs: cost_rank_gap = rank_gap(cost_a, cost_b). This will be negative if 'a' is better than 'b'.\n3. Scale the log-probability difference by the rank gap and a temperature parameter. The sign of the rank gap determines the expected direction of the log-probability difference: scaled_term = cost_rank_gap * (logp_diff / temp).\n4. Introduce a fixed margin to the scaled term. The loss should be low when the scaled term is large and positive: loss_argument = scaled_term - margin.\n5. Apply the `logsigmoid` function to compute the final loss. The negative sign ensures the loss is a positive value to be minimized: loss = -logsigmoid(loss_argument).", "hyperparams": {"temp": 1.0, "margin": 0.1}, "operators_used": ["rank_gap", "logsigmoid"], "implementation_hint": {"expects": ["cost_a", "cost_b", "logp_a", "logp_b"], "returns": "scalar"}}, "fitness": {"hf_like_score": 8.47964096069336, "validation_objective": 8.47964096069336, "generalization_penalty": 0.0, "generalization_objectives": {"20": 4.0243343114852905}, "train_score_mean": 10.498562469482422, "train_loss_mean": 0.5979754465818405, "pair_count": 12902391, "config": {"hf": {"problem": "tsp", "hf_steps": 100, "train_problem_size": 100, "valid_problem_sizes": [20], "train_batch_size": 64, "pomo_size": 64, "learning_rate": 0.0003, "weight_decay": 1e-06, "alpha": 0.05, "device": "cuda", "seed": 1234, "num_validation_episodes": 128, "validation_batch_size": 64, "generalization_penalty_weight": 1.0, "pool_version": "v0"}, "free_loss": {"f1_steps": 100, "f2_steps": 100, "f3_enabled": false}}, "loss_ir": {"name": "Rank-Gapped Sigmoid Loss with Temperature Scaling", "intuition": "This loss function synthesizes ideas from its parents to create a stable and adaptive preference learning objective. From 'Normalized Rank-Gap Hinge Loss', it inherits the use of `rank_gap` to measure the difference between costs, which provides robustness to the scale and distribution of cost values. From 'Adaptive Margin Sigmoid Loss', it inherits the core sigmoid-based loss structure, which frames preference learning as a logistic regression problem where the goal is to predict the correct preference direction. \n\nThe child introduces two new coupling ideas. First, it uses the rank gap directly within the sigmoid function, scaled by a temperature parameter `temp`, to dynamically adjust the steepness of the loss curve. When the rank gap is large, the sigmoid becomes steeper, demanding a confident prediction from the model. When the rank gap is small, the sigmoid is gentler, tolerating more ambiguity. Second, it introduces a fixed `margin` hyperparameter that shifts the decision boundary, ensuring that even for correctly ordered pairs, a certain level of preference strength is encouraged. The `logsigmoid` function is used for numerical stability. When `cost(a) < cost(b)`, the rank gap is negative, and the loss encourages `logp_a - logp_b` to be positive, thus preferring solution `a`.", "hyperparams": {"temp": 1.0, "margin": 0.1}, "operators_used": ["rank_gap", "logsigmoid"]}}, "better_than_baseline": false}
{"generation": 1, "index": 9, "ir": {"name": "Rank-Calibrated Sigmoid Loss", "intuition": "This loss function synthesizes a rank-aware hinge structure with a sigmoid-based preference term. From 'Normalized Rank-Gap Hinge Loss', we inherit the use of a rank-based cost signal (`rank_gap`) to create a dynamic margin, which makes the loss robust to the absolute scale of costs. From 'Adaptive Margin Sigmoid Loss', we inherit the core sigmoid loss structure, which frames the problem as learning the probability that `a` is preferred over `b`. The key new coupling idea is to use the rank-based dynamic margin to directly scale the argument of the `logsigmoid` function. This dynamically adjusts the steepness of the sigmoid curve based on the rank separation of the costs; a larger rank gap demands a stronger, more confident preference from the model. The loss is then defined as the negative log-likelihood of this calibrated preference. When cost(a) < cost(b), the rank gap is negative, and the loss encourages `logp_a - logp_b` to be positive, thus favoring `a`.", "pseudocode": "1. Compute the log-probability difference: logp_diff = logp_a - logp_b.\n2. Compute the signed rank gap of the costs: rank_gap_signed = rank_gap(cost_a, cost_b).\n3. Create a dynamic margin by scaling the rank gap: dynamic_margin = beta * rank_gap_signed.\n4. Calibrate the log-probability difference with the dynamic margin. The sign of the rank gap determines the expected preference direction: calibrated_preference = -dynamic_margin + logp_diff.\n5. Compute the final loss using logsigmoid. This loss is the negative log-likelihood of the model's preference matching the rank-based ground truth. A `clamp` is used on the input to logsigmoid to prevent numerical overflow with extreme log-probability differences: loss = -logsigmoid(clamp(calibrated_preference, min=-10, max=10)).", "hyperparams": {"beta": 0.5}, "operators_used": ["rank_gap", "logsigmoid", "clamp"], "implementation_hint": {"expects": ["cost_a", "cost_b", "logp_a", "logp_b"], "returns": "scalar"}}, "fitness": {"hf_like_score": 8.452676773071289, "validation_objective": 8.452676773071289, "generalization_penalty": 0.0, "generalization_objectives": {"20": 3.9907920360565186}, "train_score_mean": 10.452649927139282, "train_loss_mean": 0.6012520724534989, "pair_count": 12902391, "config": {"hf": {"problem": "tsp", "hf_steps": 100, "train_problem_size": 100, "valid_problem_sizes": [20], "train_batch_size": 64, "pomo_size": 64, "learning_rate": 0.0003, "weight_decay": 1e-06, "alpha": 0.05, "device": "cuda", "seed": 1234, "num_validation_episodes": 128, "validation_batch_size": 64, "generalization_penalty_weight": 1.0, "pool_version": "v0"}, "free_loss": {"f1_steps": 100, "f2_steps": 100, "f3_enabled": false}}, "loss_ir": {"name": "Rank-Calibrated Sigmoid Loss", "intuition": "This loss function synthesizes a rank-aware hinge structure with a sigmoid-based preference term. From 'Normalized Rank-Gap Hinge Loss', we inherit the use of a rank-based cost signal (`rank_gap`) to create a dynamic margin, which makes the loss robust to the absolute scale of costs. From 'Adaptive Margin Sigmoid Loss', we inherit the core sigmoid loss structure, which frames the problem as learning the probability that `a` is preferred over `b`. The key new coupling idea is to use the rank-based dynamic margin to directly scale the argument of the `logsigmoid` function. This dynamically adjusts the steepness of the sigmoid curve based on the rank separation of the costs; a larger rank gap demands a stronger, more confident preference from the model. The loss is then defined as the negative log-likelihood of this calibrated preference. When cost(a) < cost(b), the rank gap is negative, and the loss encourages `logp_a - logp_b` to be positive, thus favoring `a`.", "hyperparams": {"beta": 0.5}, "operators_used": ["rank_gap", "logsigmoid", "clamp"]}}, "better_than_baseline": false}
{"generation": 2, "index": 0, "ir": {"name": "Adaptive Hinge-Sigmoid Hybrid Loss", "intuition": "This loss function creates a hybrid structure that combines the hinge-like margin from the 'Normalized Rank-Gap Hinge Loss' (Parent 1) with the probabilistic sigmoid framework from the 'Rank-Calibrated Sigmoid Loss' (Parent 2). The goal is to create a loss that is robust like a hinge loss for clear-cut cases but provides smoother, probabilistic gradients for ambiguous cases.\n\nFrom Parent 1, we inherit the core idea of a dynamic margin based on the standardized rank gap: `dynamic_margin = beta * tanh(zscore(rank_gap(cost_a, cost_b)))`. This ensures the required preference strength scales with the relative cost difference in the batch.\n\nFrom Parent 2, we inherit the use of a sigmoid-based loss (`logsigmoid`) and the concept of an adaptive temperature, which we repurpose. The `logsigmoid` function provides a probabilistic interpretation, framing the preference as a classification problem.\n\nWe introduce two new coupling ideas. First, we create a hybrid loss that switches between a hinge-like term and a sigmoid term. The `dynamic_margin` is used to define a 'hinge region'. If the model's preference (`logp_diff`) is already well-aligned and outside this region (i.e., `logp_diff` is much larger than `dynamic_margin`), we apply a small, stable hinge-like penalty using `softplus`. If the preference is inside the hinge region or misaligned, we use a `logsigmoid` term to provide stronger, more informative gradients.\n\nSecond, we introduce a new adaptive temperature, `adaptive_tau`, which is inversely proportional to the standardized cost gap. When the cost difference is large (large `cost_gap_zscored`), `tau` becomes small, making the sigmoid function steeper and demanding a confident prediction. When the cost difference is small, `tau` increases, softening the sigmoid and allowing for more model uncertainty. This is a stability trick that prevents the model from being overly penalized for small preference errors on pairs with very similar costs.\n\nWhen `cost(a) < cost(b)`, `dynamic_margin` is negative. The loss encourages `logp_a - logp_b` to be positive and larger than this margin, correctly preferring `a`.", "pseudocode": "1. Compute the log-probability difference: logp_diff = logp_a - logp_b.\n2. Compute the signed, batch-relative rank gap: cost_gap_signed = rank_gap(cost_a, cost_b).\n3. Standardize the rank gap across the batch: cost_gap_zscored = zscore(cost_gap_signed).\n4. Create a bounded dynamic margin (inherited from Parent 1): dynamic_margin = beta * tanh(cost_gap_zscored).\n5. Create an adaptive temperature `tau` inversely related to the cost gap (new coupling idea): adaptive_tau = 1.0 / (1.0 + abs(cost_gap_zscored)).\n6. Define the core preference alignment signal: alignment = logp_diff - dynamic_margin. We want this to be positive.\n7. Compute a hinge-like component for well-aligned preferences using `softplus` (inspired by Parent 1): hinge_loss = softplus(-alignment).\n8. Compute a sigmoid component for misaligned or ambiguous preferences using `logsigmoid` (inspired by Parent 2), scaled by the adaptive temperature: sigmoid_loss = -logsigmoid(alignment / adaptive_tau).\n9. Combine the two components into a hybrid loss (new coupling idea): loss = hinge_loss + sigmoid_loss.", "hyperparams": {"beta": 1.5}, "operators_used": ["rank_gap", "zscore", "tanh", "softplus", "logsigmoid"], "implementation_hint": {"expects": ["cost_a", "cost_b", "logp_a", "logp_b"], "returns": "scalar"}}, "fitness": {"hf_like_score": 8.488098621368408, "validation_objective": 8.488098621368408, "generalization_penalty": 0.0, "generalization_objectives": {"20": 3.999178171157837}, "train_score_mean": 10.5384202003479, "train_loss_mean": 0.6004257041215897, "pair_count": 12902386, "config": {"hf": {"problem": "tsp", "hf_steps": 100, "train_problem_size": 100, "valid_problem_sizes": [20], "train_batch_size": 64, "pomo_size": 64, "learning_rate": 0.0003, "weight_decay": 1e-06, "alpha": 0.05, "device": "cuda", "seed": 1234, "num_validation_episodes": 128, "validation_batch_size": 64, "generalization_penalty_weight": 1.0, "pool_version": "v0"}, "free_loss": {"f1_steps": 100, "f2_steps": 100, "f3_enabled": false}}, "loss_ir": {"name": "Adaptive Hinge-Sigmoid Hybrid Loss", "intuition": "This loss function creates a hybrid structure that combines the hinge-like margin from the 'Normalized Rank-Gap Hinge Loss' (Parent 1) with the probabilistic sigmoid framework from the 'Rank-Calibrated Sigmoid Loss' (Parent 2). The goal is to create a loss that is robust like a hinge loss for clear-cut cases but provides smoother, probabilistic gradients for ambiguous cases.\n\nFrom Parent 1, we inherit the core idea of a dynamic margin based on the standardized rank gap: `dynamic_margin = beta * tanh(zscore(rank_gap(cost_a, cost_b)))`. This ensures the required preference strength scales with the relative cost difference in the batch.\n\nFrom Parent 2, we inherit the use of a sigmoid-based loss (`logsigmoid`) and the concept of an adaptive temperature, which we repurpose. The `logsigmoid` function provides a probabilistic interpretation, framing the preference as a classification problem.\n\nWe introduce two new coupling ideas. First, we create a hybrid loss that switches between a hinge-like term and a sigmoid term. The `dynamic_margin` is used to define a 'hinge region'. If the model's preference (`logp_diff`) is already well-aligned and outside this region (i.e., `logp_diff` is much larger than `dynamic_margin`), we apply a small, stable hinge-like penalty using `softplus`. If the preference is inside the hinge region or misaligned, we use a `logsigmoid` term to provide stronger, more informative gradients.\n\nSecond, we introduce a new adaptive temperature, `adaptive_tau`, which is inversely proportional to the standardized cost gap. When the cost difference is large (large `cost_gap_zscored`), `tau` becomes small, making the sigmoid function steeper and demanding a confident prediction. When the cost difference is small, `tau` increases, softening the sigmoid and allowing for more model uncertainty. This is a stability trick that prevents the model from being overly penalized for small preference errors on pairs with very similar costs.\n\nWhen `cost(a) < cost(b)`, `dynamic_margin` is negative. The loss encourages `logp_a - logp_b` to be positive and larger than this margin, correctly preferring `a`.", "hyperparams": {"beta": 1.5}, "operators_used": ["rank_gap", "zscore", "tanh", "softplus", "logsigmoid"]}}, "better_than_baseline": false}
{"generation": 2, "index": 1, "ir": {"name": "Adaptive Rank-Calibrated Hinge Loss", "intuition": "This loss function creates a hybrid between a hinge-loss structure and a sigmoid-based preference loss, incorporating an adaptive temperature mechanism. From Parent 1 (Normalized Rank-Gap Hinge Loss), we inherit the core `softplus` hinge structure, which provides a smooth, non-negative penalty when the model's preference does not meet a certain margin. From Parent 2 (Rank-Calibrated Sigmoid Loss), we inherit the concept of an adaptive temperature that scales the loss based on the magnitude of the cost difference.\n\nThe first new coupling idea is to use the rank-based dynamic margin, not just as a threshold, but also as a scaling factor for the loss itself. The core term inside the `softplus` is `dynamic_margin - logp_diff`, inherited from Parent 1. The second new coupling idea is to modulate this term with an adaptive temperature, `adaptive_gamma`, which is derived from the standardized rank gap similar to Parent 2. This `adaptive_gamma` scales the entire argument of the `softplus` function. When the cost difference is large (large `cost_gap_zscored`), `adaptive_gamma` increases, making the hinge loss steeper and penalizing misaligned preferences more severely. When the cost difference is small, the penalty is gentler, allowing for more model uncertainty. This creates a loss that is both a hinge loss in structure and dynamically calibrated in magnitude.\n\nWhen `cost(a) < cost(b)`, `cost_gap_zscored` is negative, making `dynamic_margin` negative. The loss `softplus(adaptive_gamma * (dynamic_margin - logp_diff))` encourages `logp_diff` to be positive and larger than the negative margin, correctly preferring `a`.", "pseudocode": "1. Compute the log-probability difference: logp_diff = logp_a - logp_b.\n2. Compute the signed, batch-relative rank gap: cost_gap_signed = rank_gap(cost_a, cost_b).\n3. Standardize the rank gap across the batch: cost_gap_zscored = zscore(cost_gap_signed).\n4. Create a bounded dynamic margin from the standardized rank gap (inherited from Parent 1): dynamic_margin = beta * tanh(cost_gap_zscored).\n5. Create an adaptive temperature based on the rank gap magnitude (inspired by Parent 2, new coupling): adaptive_gamma = 1.0 + gamma * abs(cost_gap_zscored).\n6. Compute the hinge-like term. The misalignment is measured by `dynamic_margin - logp_diff` (from Parent 1).\n7. Scale the misalignment by the adaptive temperature and compute the final loss using `softplus` for a smooth hinge effect (structure from Parent 1): loss = softplus(adaptive_gamma * (dynamic_margin - logp_diff)).", "hyperparams": {"beta": 1.0, "gamma": 0.5}, "operators_used": ["rank_gap", "zscore", "tanh", "softplus"], "implementation_hint": {"expects": ["cost_a", "cost_b", "logp_a", "logp_b"], "returns": "scalar"}}, "fitness": {"hf_like_score": 8.497498512268066, "validation_objective": 8.497498512268066, "generalization_penalty": 0.0, "generalization_objectives": {"20": 3.9994356632232666}, "train_score_mean": 10.49219476699829, "train_loss_mean": 0.5991048556566239, "pair_count": 12902387, "config": {"hf": {"problem": "tsp", "hf_steps": 100, "train_problem_size": 100, "valid_problem_sizes": [20], "train_batch_size": 64, "pomo_size": 64, "learning_rate": 0.0003, "weight_decay": 1e-06, "alpha": 0.05, "device": "cuda", "seed": 1234, "num_validation_episodes": 128, "validation_batch_size": 64, "generalization_penalty_weight": 1.0, "pool_version": "v0"}, "free_loss": {"f1_steps": 100, "f2_steps": 100, "f3_enabled": false}}, "loss_ir": {"name": "Adaptive Rank-Calibrated Hinge Loss", "intuition": "This loss function creates a hybrid between a hinge-loss structure and a sigmoid-based preference loss, incorporating an adaptive temperature mechanism. From Parent 1 (Normalized Rank-Gap Hinge Loss), we inherit the core `softplus` hinge structure, which provides a smooth, non-negative penalty when the model's preference does not meet a certain margin. From Parent 2 (Rank-Calibrated Sigmoid Loss), we inherit the concept of an adaptive temperature that scales the loss based on the magnitude of the cost difference.\n\nThe first new coupling idea is to use the rank-based dynamic margin, not just as a threshold, but also as a scaling factor for the loss itself. The core term inside the `softplus` is `dynamic_margin - logp_diff`, inherited from Parent 1. The second new coupling idea is to modulate this term with an adaptive temperature, `adaptive_gamma`, which is derived from the standardized rank gap similar to Parent 2. This `adaptive_gamma` scales the entire argument of the `softplus` function. When the cost difference is large (large `cost_gap_zscored`), `adaptive_gamma` increases, making the hinge loss steeper and penalizing misaligned preferences more severely. When the cost difference is small, the penalty is gentler, allowing for more model uncertainty. This creates a loss that is both a hinge loss in structure and dynamically calibrated in magnitude.\n\nWhen `cost(a) < cost(b)`, `cost_gap_zscored` is negative, making `dynamic_margin` negative. The loss `softplus(adaptive_gamma * (dynamic_margin - logp_diff))` encourages `logp_diff` to be positive and larger than the negative margin, correctly preferring `a`.", "hyperparams": {"beta": 1.0, "gamma": 0.5}, "operators_used": ["rank_gap", "zscore", "tanh", "softplus"]}}, "better_than_baseline": false}
{"generation": 2, "index": 2, "ir": {"name": "Adaptive Sigmoid-Hinge Loss", "intuition": "This loss function creates a hybrid between a hinge loss and a sigmoid loss, dynamically blending them based on the magnitude of the cost difference. From the 'Normalized Rank-Gap Hinge Loss' (Parent 1), we inherit the core structure of a smooth hinge loss using `softplus` and the idea of a dynamic margin derived from a standardized rank gap. From the 'Rank-Calibrated Sigmoid Loss' (Parent 2), we inherit the use of a `logsigmoid` function to frame preference learning as a classification problem. \n\nThe first new coupling idea is a dynamic blending weight, `alpha`. This weight is computed using `sigmoid` on the standardized cost gap. When the cost difference is large (large `|cost_gap_zscored|`), `alpha` approaches 1, and the loss behaves more like the `logsigmoid` loss from Parent 2, demanding a confident preference. When the cost difference is small (near zero), `alpha` approaches 0.5, giving more weight to the `softplus` hinge component from Parent 1, which is more forgiving for small margins. The second new coupling idea is applying this blend to both a `logsigmoid` term and a `softplus` term. This creates a loss that is robust to small cost differences (like a hinge loss) but also has the strong gradient signal of a sigmoid loss for large, clear-cut preferences. When `cost(a) < cost(b)`, the `dynamic_margin` is negative. The `logsigmoid` term encourages `logp_diff` to be positive, while the `softplus` term penalizes `logp_diff` if it is less than the negative margin (i.e., if it is not sufficiently positive). Both components work together to prefer `a`.", "pseudocode": "1. Compute the log-probability difference: logp_diff = logp_a - logp_b.\n2. Compute the signed, batch-relative rank gap: cost_gap_signed = rank_gap(cost_a, cost_b).\n3. Standardize the rank gap across the batch: cost_gap_zscored = zscore(cost_gap_signed).\n4. Create a bounded dynamic margin (inherited from Parent 1): dynamic_margin = beta * tanh(cost_gap_zscored).\n5. Compute a dynamic blending weight based on the cost gap magnitude (new coupling idea): alpha = sigmoid(gamma * cost_gap_zscored).\n6. Calculate the sigmoid-based loss component (inspired by Parent 2): sigmoid_loss = -logsigmoid(logp_diff - dynamic_margin).\n7. Calculate the hinge-based loss component (inspired by Parent 1): hinge_loss = softplus(dynamic_margin - logp_diff).\n8. Combine the two components using the dynamic weight `alpha` (new coupling idea): loss = alpha * sigmoid_loss + (1 - alpha) * hinge_loss.", "hyperparams": {"beta": 1.0, "gamma": 1.5}, "operators_used": ["rank_gap", "zscore", "tanh", "sigmoid", "logsigmoid", "softplus"], "implementation_hint": {"expects": ["cost_a", "cost_b", "logp_a", "logp_b"], "returns": "scalar"}}, "fitness": {"hf_like_score": 8.458857536315918, "validation_objective": 8.458857536315918, "generalization_penalty": 0.0, "generalization_objectives": {"20": 3.987090587615967}, "train_score_mean": 10.516194496154785, "train_loss_mean": 0.5987449085712433, "pair_count": 12902386, "config": {"hf": {"problem": "tsp", "hf_steps": 100, "train_problem_size": 100, "valid_problem_sizes": [20], "train_batch_size": 64, "pomo_size": 64, "learning_rate": 0.0003, "weight_decay": 1e-06, "alpha": 0.05, "device": "cuda", "seed": 1234, "num_validation_episodes": 128, "validation_batch_size": 64, "generalization_penalty_weight": 1.0, "pool_version": "v0"}, "free_loss": {"f1_steps": 100, "f2_steps": 100, "f3_enabled": false}}, "loss_ir": {"name": "Adaptive Sigmoid-Hinge Loss", "intuition": "This loss function creates a hybrid between a hinge loss and a sigmoid loss, dynamically blending them based on the magnitude of the cost difference. From the 'Normalized Rank-Gap Hinge Loss' (Parent 1), we inherit the core structure of a smooth hinge loss using `softplus` and the idea of a dynamic margin derived from a standardized rank gap. From the 'Rank-Calibrated Sigmoid Loss' (Parent 2), we inherit the use of a `logsigmoid` function to frame preference learning as a classification problem. \n\nThe first new coupling idea is a dynamic blending weight, `alpha`. This weight is computed using `sigmoid` on the standardized cost gap. When the cost difference is large (large `|cost_gap_zscored|`), `alpha` approaches 1, and the loss behaves more like the `logsigmoid` loss from Parent 2, demanding a confident preference. When the cost difference is small (near zero), `alpha` approaches 0.5, giving more weight to the `softplus` hinge component from Parent 1, which is more forgiving for small margins. The second new coupling idea is applying this blend to both a `logsigmoid` term and a `softplus` term. This creates a loss that is robust to small cost differences (like a hinge loss) but also has the strong gradient signal of a sigmoid loss for large, clear-cut preferences. When `cost(a) < cost(b)`, the `dynamic_margin` is negative. The `logsigmoid` term encourages `logp_diff` to be positive, while the `softplus` term penalizes `logp_diff` if it is less than the negative margin (i.e., if it is not sufficiently positive). Both components work together to prefer `a`.", "hyperparams": {"beta": 1.0, "gamma": 1.5}, "operators_used": ["rank_gap", "zscore", "tanh", "sigmoid", "logsigmoid", "softplus"]}}, "better_than_baseline": false}
{"generation": 2, "index": 3, "ir": {"name": "Adaptive Temperature Hinge-Sigmoid Loss", "intuition": "This loss function synthesizes a hinge-like margin objective with a probabilistic sigmoid framework, introducing an adaptive temperature to modulate the loss steepness. From Parent 0 (Normalized Rank-Gap Hinge Loss), we inherit the core structure of comparing the model's preference (`logp_a - logp_b`) against a dynamic margin. We also inherit the use of `zscore(rank_gap(cost_a, cost_b))` to create a robust, batch-normalized signal of cost difference. From Parent 1 (Rank-Calibrated Sigmoid Loss), we inherit the use of a `logsigmoid` function to frame the problem probabilistically and the idea of an adaptive temperature. \n\nThe first new coupling idea is how the margin and preference are combined. Instead of `margin - logp_diff` inside a `softplus` (from Parent 0), or `logp_diff - margin` inside a `logsigmoid` (from Parent 1), we use a new formulation: `logp_diff - beta * tanh(cost_gap_zscored)`. This term represents the 'excess preference'how much the model's preference exceeds the required dynamic margin. The second new coupling idea is an adaptive temperature, `adaptive_gamma`, which is a function of the standardized rank gap itself, rather than the margin. The temperature is scaled by `1 + gamma * abs(cost_gap_zscored)`. This makes the loss more sensitive (steeper sigmoid) when the cost difference is large and less sensitive when the costs are similar, directly linking the loss curvature to the magnitude of the cost rank difference. The final loss applies a `logsigmoid` to this temperature-scaled 'excess preference' term. When `cost(a) < cost(b)`, the `cost_gap_zscored` is negative, making the `tanh` term negative. The loss then encourages `logp_a - logp_b` to be positive, correctly preferring `a`.", "pseudocode": "1. Compute the log-probability difference: logp_diff = logp_a - logp_b.\n2. Compute the signed, batch-relative rank gap: cost_gap_signed = rank_gap(cost_a, cost_b).\n3. Standardize the rank gap across the batch (inherited from Parent 0 and 1): cost_gap_zscored = zscore(cost_gap_signed).\n4. Create a bounded dynamic margin using the standardized rank gap: dynamic_margin = beta * tanh(cost_gap_zscored).\n5. Define the 'excess preference' term, which is the difference between the model's preference and the required margin (structure inherited from Parent 0): excess_preference = logp_diff - dynamic_margin.\n6. Create an adaptive temperature based on the standardized rank gap (new coupling idea): adaptive_gamma = 1.0 + gamma * abs(cost_gap_zscored).\n7. Scale the excess preference by the adaptive temperature and compute the final loss using `logsigmoid` (framework inherited from Parent 1): loss = -logsigmoid(adaptive_gamma * excess_preference).", "hyperparams": {"beta": 1.0, "gamma": 0.5}, "operators_used": ["rank_gap", "zscore", "tanh", "logsigmoid"], "implementation_hint": {"expects": ["cost_a", "cost_b", "logp_a", "logp_b"], "returns": "scalar"}}, "fitness": {"hf_like_score": 8.487099647521973, "validation_objective": 8.487099647521973, "generalization_penalty": 0.0, "generalization_objectives": {"20": 3.9836032390594482}, "train_score_mean": 10.521904096603393, "train_loss_mean": 0.6002114796638489, "pair_count": 12902391, "config": {"hf": {"problem": "tsp", "hf_steps": 100, "train_problem_size": 100, "valid_problem_sizes": [20], "train_batch_size": 64, "pomo_size": 64, "learning_rate": 0.0003, "weight_decay": 1e-06, "alpha": 0.05, "device": "cuda", "seed": 1234, "num_validation_episodes": 128, "validation_batch_size": 64, "generalization_penalty_weight": 1.0, "pool_version": "v0"}, "free_loss": {"f1_steps": 100, "f2_steps": 100, "f3_enabled": false}}, "loss_ir": {"name": "Adaptive Temperature Hinge-Sigmoid Loss", "intuition": "This loss function synthesizes a hinge-like margin objective with a probabilistic sigmoid framework, introducing an adaptive temperature to modulate the loss steepness. From Parent 0 (Normalized Rank-Gap Hinge Loss), we inherit the core structure of comparing the model's preference (`logp_a - logp_b`) against a dynamic margin. We also inherit the use of `zscore(rank_gap(cost_a, cost_b))` to create a robust, batch-normalized signal of cost difference. From Parent 1 (Rank-Calibrated Sigmoid Loss), we inherit the use of a `logsigmoid` function to frame the problem probabilistically and the idea of an adaptive temperature. \n\nThe first new coupling idea is how the margin and preference are combined. Instead of `margin - logp_diff` inside a `softplus` (from Parent 0), or `logp_diff - margin` inside a `logsigmoid` (from Parent 1), we use a new formulation: `logp_diff - beta * tanh(cost_gap_zscored)`. This term represents the 'excess preference'how much the model's preference exceeds the required dynamic margin. The second new coupling idea is an adaptive temperature, `adaptive_gamma`, which is a function of the standardized rank gap itself, rather than the margin. The temperature is scaled by `1 + gamma * abs(cost_gap_zscored)`. This makes the loss more sensitive (steeper sigmoid) when the cost difference is large and less sensitive when the costs are similar, directly linking the loss curvature to the magnitude of the cost rank difference. The final loss applies a `logsigmoid` to this temperature-scaled 'excess preference' term. When `cost(a) < cost(b)`, the `cost_gap_zscored` is negative, making the `tanh` term negative. The loss then encourages `logp_a - logp_b` to be positive, correctly preferring `a`.", "hyperparams": {"beta": 1.0, "gamma": 0.5}, "operators_used": ["rank_gap", "zscore", "tanh", "logsigmoid"]}}, "better_than_baseline": false}
{"generation": 2, "index": 4, "ir": {"name": "Adaptive Sigmoid-Hinge Loss", "intuition": "This loss function creates a hybrid between a hinge-loss and a sigmoid-loss structure, adapting its behavior based on the magnitude of the cost difference. From the 'Normalized Rank-Gap Hinge Loss' (Parent 1), we inherit the core structure of using a dynamic margin inside a smooth hinge-like function (`softplus`). The dynamic margin itself is also inspired by Parent 1, derived from the standardized rank gap of the costs. From the 'Rank-Calibrated Sigmoid Loss' (Parent 2), we inherit the idea of using the log-probability difference as a measure of the model's preference (`logp_a - logp_b`) and the concept of an adaptive temperature. \n\nThe first new coupling idea is to create a dynamic 'hinge point' using the `tanh` of the standardized cost gap. This margin, `dynamic_margin`, determines the target log-probability difference the model should achieve. The second new coupling idea is an adaptive temperature, `adaptive_gamma`, which is used to scale the argument of the `softplus` function. This temperature is inversely related to the magnitude of the cost difference; when costs are very different (large `|cost_gap_zscored|`), the temperature is low, resulting in a sharp, hinge-like penalty. When costs are similar (small `|cost_gap_zscored|`), the temperature is high, creating a softer, more sigmoid-like penalty that is less punishing of small errors. This allows the loss to focus on clear mis-rankings while being lenient on ambiguous pairs. \n\nWhen `cost(a) < cost(b)`, `cost_gap_zscored` is negative, making `dynamic_margin` negative. The loss `softplus(adaptive_gamma * (dynamic_margin - logp_diff))` will be minimized when `logp_diff` is a large positive number, correctly encouraging the model to prefer `a` over `b`.", "pseudocode": "1. Compute the log-probability difference: logp_diff = logp_a - logp_b.\n2. Compute the signed, normalized cost gap: cost_gap_signed = rank_gap(cost_a, cost_b).\n3. Standardize the cost gap across the batch: cost_gap_zscored = zscore(cost_gap_signed).\n4. Create a bounded dynamic margin (inherited from Parent 1): dynamic_margin = beta * tanh(cost_gap_zscored).\n5. Create an adaptive inverse temperature scaled by the cost gap magnitude (new coupling idea): adaptive_gamma = 1.0 + exp(-alpha * abs(cost_gap_zscored)).\n6. Formulate the hinge-like argument: The loss is based on the difference between the target margin and the model's preference, `dynamic_margin - logp_diff` (structure from Parent 1).\n7. Scale the argument by the adaptive temperature and compute the final loss using `softplus` for a smooth, non-negative penalty: loss = softplus(adaptive_gamma * (dynamic_margin - logp_diff)).", "hyperparams": {"beta": 1.0, "alpha": 0.5}, "operators_used": ["rank_gap", "zscore", "tanh", "exp", "softplus"], "implementation_hint": {"expects": ["cost_a", "cost_b", "logp_a", "logp_b"], "returns": "scalar"}}, "fitness": {"hf_like_score": 17.054643630981445, "validation_objective": 17.054643630981445, "generalization_penalty": 0.0, "generalization_objectives": {"20": 4.444254159927368}, "train_score_mean": 24.69853874206543, "train_loss_mean": 0.9186591571569442, "pair_count": 12902391, "config": {"hf": {"problem": "tsp", "hf_steps": 100, "train_problem_size": 100, "valid_problem_sizes": [20], "train_batch_size": 64, "pomo_size": 64, "learning_rate": 0.0003, "weight_decay": 1e-06, "alpha": 0.05, "device": "cuda", "seed": 1234, "num_validation_episodes": 128, "validation_batch_size": 64, "generalization_penalty_weight": 1.0, "pool_version": "v0"}, "free_loss": {"f1_steps": 100, "f2_steps": 100, "f3_enabled": false}}, "loss_ir": {"name": "Adaptive Sigmoid-Hinge Loss", "intuition": "This loss function creates a hybrid between a hinge-loss and a sigmoid-loss structure, adapting its behavior based on the magnitude of the cost difference. From the 'Normalized Rank-Gap Hinge Loss' (Parent 1), we inherit the core structure of using a dynamic margin inside a smooth hinge-like function (`softplus`). The dynamic margin itself is also inspired by Parent 1, derived from the standardized rank gap of the costs. From the 'Rank-Calibrated Sigmoid Loss' (Parent 2), we inherit the idea of using the log-probability difference as a measure of the model's preference (`logp_a - logp_b`) and the concept of an adaptive temperature. \n\nThe first new coupling idea is to create a dynamic 'hinge point' using the `tanh` of the standardized cost gap. This margin, `dynamic_margin`, determines the target log-probability difference the model should achieve. The second new coupling idea is an adaptive temperature, `adaptive_gamma`, which is used to scale the argument of the `softplus` function. This temperature is inversely related to the magnitude of the cost difference; when costs are very different (large `|cost_gap_zscored|`), the temperature is low, resulting in a sharp, hinge-like penalty. When costs are similar (small `|cost_gap_zscored|`), the temperature is high, creating a softer, more sigmoid-like penalty that is less punishing of small errors. This allows the loss to focus on clear mis-rankings while being lenient on ambiguous pairs. \n\nWhen `cost(a) < cost(b)`, `cost_gap_zscored` is negative, making `dynamic_margin` negative. The loss `softplus(adaptive_gamma * (dynamic_margin - logp_diff))` will be minimized when `logp_diff` is a large positive number, correctly encouraging the model to prefer `a` over `b`.", "hyperparams": {"beta": 1.0, "alpha": 0.5}, "operators_used": ["rank_gap", "zscore", "tanh", "exp", "softplus"]}}, "better_than_baseline": false}
{"generation": 2, "index": 5, "ir": {"name": "Adaptive Sigmoid-Hinge Loss", "intuition": "This loss function creates a hybrid between a hinge loss and a sigmoid loss, dynamically interpolating between them based on the magnitude of the cost difference. From the 'Normalized Rank-Gap Hinge Loss' (Parent 1), we inherit the use of a smooth hinge-like structure via `softplus` and a dynamic margin derived from a standardized rank gap. From the 'Rank-Calibrated Sigmoid Loss' (Parent 2), we inherit the core sigmoid-based preference loss structure (`logsigmoid`) which is effective for binary preference tasks. \n\nThe first new coupling idea is an adaptive weight, `w`, which is calculated using the sigmoid of the absolute standardized cost gap. When the cost difference is large, `w` approaches 1, and the loss behaves more like the `logsigmoid` loss from Parent 2, demanding a confident and correct preference. When the cost difference is small, `w` approaches 0.5, and the loss behavior shifts towards the `softplus` (hinge) loss from Parent 1, which simply penalizes violations of a small margin without forcing a strong preference. This creates a stable interpolation between the two parent structures. The second new coupling is the use of this same weight `w` to scale the dynamic margin itself. This ensures that for small cost differences, the required margin for the hinge component is also small, preventing the model from being penalized for uncertainty when the ground truth is ambiguous. When `cost(a) < cost(b)`, the `dynamic_margin` is negative. The `logsigmoid` term encourages `logp_diff` to be positive, while the `softplus` term penalizes `logp_diff` for not being sufficiently positive to overcome the margin.", "pseudocode": "1. Compute the log-probability difference: logp_diff = logp_a - logp_b.\n2. Compute the signed, batch-relative rank gap: cost_gap_signed = rank_gap(cost_a, cost_b).\n3. Standardize the rank gap across the batch: cost_gap_zscored = zscore(cost_gap_signed).\n4. Create a bounded dynamic margin (idea from Parent 1): dynamic_margin_base = beta * tanh(cost_gap_zscored).\n5. Create an adaptive weight `w` based on the magnitude of the cost gap (new coupling idea): w = sigmoid(gamma * abs(cost_gap_zscored)). This weight is in [0.5, 1.0].\n6. Scale the margin using the adaptive weight (new coupling idea): dynamic_margin = w * dynamic_margin_base.\n7. Compute a sigmoid-based loss component (idea from Parent 2): loss_sigmoid = -logsigmoid(logp_diff - dynamic_margin).\n8. Compute a hinge-based loss component (idea from Parent 1): loss_hinge = softplus(dynamic_margin - logp_diff).\n9. Combine the two components using the adaptive weight `w`: loss = (w * loss_sigmoid) + ((1 - w) * loss_hinge).", "hyperparams": {"beta": 1.0, "gamma": 1.0}, "operators_used": ["rank_gap", "zscore", "tanh", "sigmoid", "logsigmoid", "softplus"], "implementation_hint": {"expects": ["cost_a", "cost_b", "logp_a", "logp_b"], "returns": "scalar"}}, "fitness": {"hf_like_score": 8.448531150817871, "validation_objective": 8.448531150817871, "generalization_penalty": 0.0, "generalization_objectives": {"20": 3.98567795753479}, "train_score_mean": 10.492136831283569, "train_loss_mean": 0.6027596485614777, "pair_count": 12902386, "config": {"hf": {"problem": "tsp", "hf_steps": 100, "train_problem_size": 100, "valid_problem_sizes": [20], "train_batch_size": 64, "pomo_size": 64, "learning_rate": 0.0003, "weight_decay": 1e-06, "alpha": 0.05, "device": "cuda", "seed": 1234, "num_validation_episodes": 128, "validation_batch_size": 64, "generalization_penalty_weight": 1.0, "pool_version": "v0"}, "free_loss": {"f1_steps": 100, "f2_steps": 100, "f3_enabled": false}}, "loss_ir": {"name": "Adaptive Sigmoid-Hinge Loss", "intuition": "This loss function creates a hybrid between a hinge loss and a sigmoid loss, dynamically interpolating between them based on the magnitude of the cost difference. From the 'Normalized Rank-Gap Hinge Loss' (Parent 1), we inherit the use of a smooth hinge-like structure via `softplus` and a dynamic margin derived from a standardized rank gap. From the 'Rank-Calibrated Sigmoid Loss' (Parent 2), we inherit the core sigmoid-based preference loss structure (`logsigmoid`) which is effective for binary preference tasks. \n\nThe first new coupling idea is an adaptive weight, `w`, which is calculated using the sigmoid of the absolute standardized cost gap. When the cost difference is large, `w` approaches 1, and the loss behaves more like the `logsigmoid` loss from Parent 2, demanding a confident and correct preference. When the cost difference is small, `w` approaches 0.5, and the loss behavior shifts towards the `softplus` (hinge) loss from Parent 1, which simply penalizes violations of a small margin without forcing a strong preference. This creates a stable interpolation between the two parent structures. The second new coupling is the use of this same weight `w` to scale the dynamic margin itself. This ensures that for small cost differences, the required margin for the hinge component is also small, preventing the model from being penalized for uncertainty when the ground truth is ambiguous. When `cost(a) < cost(b)`, the `dynamic_margin` is negative. The `logsigmoid` term encourages `logp_diff` to be positive, while the `softplus` term penalizes `logp_diff` for not being sufficiently positive to overcome the margin.", "hyperparams": {"beta": 1.0, "gamma": 1.0}, "operators_used": ["rank_gap", "zscore", "tanh", "sigmoid", "logsigmoid", "softplus"]}}, "better_than_baseline": false}
{"generation": 2, "index": 6, "ir": {"name": "Adaptive Rank-Gap Attenuated Hinge Loss", "intuition": "This loss function creates a hybrid between a hinge-loss structure and a sigmoid-based preference loss, with a new coupling mechanism that dynamically attenuates the loss based on the model's confidence. From the 'Normalized Rank-Gap Hinge Loss' (Parent 1), we inherit the core structure of using a dynamic margin within a smooth hinge loss (`softplus(margin - logp_diff)`). This margin is derived from the standardized rank gap (`zscore(rank_gap(cost_a, cost_b))`), making the required log-probability difference proportional to the cost difference's relative standing in the batch. From the 'Rank-Calibrated Sigmoid Loss' (Parent 2), we inherit the idea of using the log-probability difference itself to modulate the loss, but we re-purpose it as a new coupling mechanism. Instead of using it inside a sigmoid, we use it to create a 'confidence attenuator'. This attenuator, `sigmoid(gamma * abs(logp_diff))`, scales the hinge loss down when the model is very confident (large `abs(logp_diff)`) and scales it up when the model is uncertain (small `abs(logp_diff)`). This prevents the model from being overly penalized for small alignment errors on predictions it is already confident about, focusing the training signal on uncertain pairs. When `cost(a) < cost(b)`, `rank_gap` is negative, making `dynamic_margin` negative. The `softplus` term then encourages `logp_a - logp_b` to be positive, correctly preferring `a`.", "pseudocode": "1. Compute the log-probability difference: logp_diff = logp_a - logp_b.\n2. Compute the signed, batch-relative rank gap: cost_gap_signed = rank_gap(cost_a, cost_b).\n3. Standardize the rank gap across the batch: cost_gap_zscored = zscore(cost_gap_signed).\n4. Create a bounded dynamic margin from the standardized rank gap (inherited from Parent 1): dynamic_margin = beta * tanh(cost_gap_zscored).\n5. Compute the core hinge-like term. This is the primary penalty for misaligned preferences (inherited from Parent 1): hinge_term = softplus(dynamic_margin - logp_diff).\n6. Create a confidence attenuator (new coupling idea, inspired by Parent 2's use of logp_diff). This term approaches 1 for uncertain predictions and 0 for very confident ones: confidence_attenuator = 1.0 - sigmoid(gamma * abs(logp_diff)).\n7. Calculate the final loss by multiplying the hinge term by the confidence attenuator: loss = confidence_attenuator * hinge_term.", "hyperparams": {"beta": 1.0, "gamma": 2.0}, "operators_used": ["rank_gap", "zscore", "tanh", "softplus", "sigmoid"], "implementation_hint": {"expects": ["cost_a", "cost_b", "logp_a", "logp_b"], "returns": "scalar"}}, "fitness": {"hf_like_score": 8.44678258895874, "validation_objective": 8.44678258895874, "generalization_penalty": 0.0, "generalization_objectives": {"20": 4.036804676055908}, "train_score_mean": 10.508783178329468, "train_loss_mean": 0.6030446135997772, "pair_count": 12902386, "config": {"hf": {"problem": "tsp", "hf_steps": 100, "train_problem_size": 100, "valid_problem_sizes": [20], "train_batch_size": 64, "pomo_size": 64, "learning_rate": 0.0003, "weight_decay": 1e-06, "alpha": 0.05, "device": "cuda", "seed": 1234, "num_validation_episodes": 128, "validation_batch_size": 64, "generalization_penalty_weight": 1.0, "pool_version": "v0"}, "free_loss": {"f1_steps": 100, "f2_steps": 100, "f3_enabled": false}}, "loss_ir": {"name": "Adaptive Rank-Gap Attenuated Hinge Loss", "intuition": "This loss function creates a hybrid between a hinge-loss structure and a sigmoid-based preference loss, with a new coupling mechanism that dynamically attenuates the loss based on the model's confidence. From the 'Normalized Rank-Gap Hinge Loss' (Parent 1), we inherit the core structure of using a dynamic margin within a smooth hinge loss (`softplus(margin - logp_diff)`). This margin is derived from the standardized rank gap (`zscore(rank_gap(cost_a, cost_b))`), making the required log-probability difference proportional to the cost difference's relative standing in the batch. From the 'Rank-Calibrated Sigmoid Loss' (Parent 2), we inherit the idea of using the log-probability difference itself to modulate the loss, but we re-purpose it as a new coupling mechanism. Instead of using it inside a sigmoid, we use it to create a 'confidence attenuator'. This attenuator, `sigmoid(gamma * abs(logp_diff))`, scales the hinge loss down when the model is very confident (large `abs(logp_diff)`) and scales it up when the model is uncertain (small `abs(logp_diff)`). This prevents the model from being overly penalized for small alignment errors on predictions it is already confident about, focusing the training signal on uncertain pairs. When `cost(a) < cost(b)`, `rank_gap` is negative, making `dynamic_margin` negative. The `softplus` term then encourages `logp_a - logp_b` to be positive, correctly preferring `a`.", "hyperparams": {"beta": 1.0, "gamma": 2.0}, "operators_used": ["rank_gap", "zscore", "tanh", "softplus", "sigmoid"]}}, "better_than_baseline": true}
{"generation": 2, "index": 7, "ir": {"name": "Adaptive Hinge-Sigmoid Hybrid Loss", "intuition": "This loss function creates a hybrid structure by combining a hinge-like term with a sigmoid-based preference term, adaptively weighting their contributions. From the 'Normalized Rank-Gap Hinge Loss' (Parent 0), we inherit the core structure of a smooth hinge loss `softplus(margin - logp_diff)` and the use of a bounded, dynamic margin derived from the standardized rank gap (`beta * tanh(zscore(rank_gap(...)))`). From the 'Rank-Calibrated Sigmoid Loss' (Parent 1), we inherit the sigmoid-based preference loss `logsigmoid(logp_diff - margin)`, which frames the problem as correctly classifying the preference based on the margin.\n\nThe first new coupling idea is to combine these two loss components into a single expression. The second, and more crucial, coupling idea is an adaptive weight `alpha` that balances the two components. This weight is derived from the magnitude of the dynamic margin itself. When the cost difference is large (large `|dynamic_margin|`), `alpha` approaches 1, emphasizing the hinge-like term which strongly penalizes any deviation from the target preference margin. When the cost difference is small (small `|dynamic_margin|`), `alpha` approaches 0, giving more weight to the softer sigmoid term, which allows for more model uncertainty. This creates a loss that is strict for clear preferences and lenient for ambiguous ones.\n\nWhen `cost(a) < cost(b)`, `rank_gap` is negative, making `dynamic_margin` negative. The loss then encourages `logp_a - logp_b` to be positive (greater than the negative margin), correctly preferring `a`.", "pseudocode": "1. Compute the log-probability difference: logp_diff = logp_a - logp_b.\n2. Compute the signed, batch-relative rank gap: cost_gap_signed = rank_gap(cost_a, cost_b).\n3. Standardize the rank gap across the batch: cost_gap_zscored = zscore(cost_gap_signed).\n4. Create a bounded dynamic margin (inherited from both parents): dynamic_margin = beta * tanh(cost_gap_zscored).\n5. Define the hinge loss component (from Parent 0): hinge_term = softplus(dynamic_margin - logp_diff).\n6. Define the sigmoid loss component (from Parent 1): sigmoid_term = -logsigmoid(logp_diff - dynamic_margin).\n7. Create an adaptive weight based on the margin's magnitude (new coupling idea): adaptive_alpha = sigmoid(gamma * abs(dynamic_margin)).\n8. Combine the two loss components using the adaptive weight: loss = (adaptive_alpha * hinge_term) + ((1 - adaptive_alpha) * sigmoid_term).", "hyperparams": {"beta": 1.0, "gamma": 2.0}, "operators_used": ["rank_gap", "zscore", "tanh", "softplus", "logsigmoid", "sigmoid"], "implementation_hint": {"expects": ["cost_a", "cost_b", "logp_a", "logp_b"], "returns": "scalar"}}, "fitness": {"hf_like_score": 8.416484355926514, "validation_objective": 8.416484355926514, "generalization_penalty": 0.0, "generalization_objectives": {"20": 3.983156442642212}, "train_score_mean": 10.492187185287476, "train_loss_mean": 0.6021484953165054, "pair_count": 12902386, "config": {"hf": {"problem": "tsp", "hf_steps": 100, "train_problem_size": 100, "valid_problem_sizes": [20], "train_batch_size": 64, "pomo_size": 64, "learning_rate": 0.0003, "weight_decay": 1e-06, "alpha": 0.05, "device": "cuda", "seed": 1234, "num_validation_episodes": 128, "validation_batch_size": 64, "generalization_penalty_weight": 1.0, "pool_version": "v0"}, "free_loss": {"f1_steps": 100, "f2_steps": 100, "f3_enabled": false}}, "loss_ir": {"name": "Adaptive Hinge-Sigmoid Hybrid Loss", "intuition": "This loss function creates a hybrid structure by combining a hinge-like term with a sigmoid-based preference term, adaptively weighting their contributions. From the 'Normalized Rank-Gap Hinge Loss' (Parent 0), we inherit the core structure of a smooth hinge loss `softplus(margin - logp_diff)` and the use of a bounded, dynamic margin derived from the standardized rank gap (`beta * tanh(zscore(rank_gap(...)))`). From the 'Rank-Calibrated Sigmoid Loss' (Parent 1), we inherit the sigmoid-based preference loss `logsigmoid(logp_diff - margin)`, which frames the problem as correctly classifying the preference based on the margin.\n\nThe first new coupling idea is to combine these two loss components into a single expression. The second, and more crucial, coupling idea is an adaptive weight `alpha` that balances the two components. This weight is derived from the magnitude of the dynamic margin itself. When the cost difference is large (large `|dynamic_margin|`), `alpha` approaches 1, emphasizing the hinge-like term which strongly penalizes any deviation from the target preference margin. When the cost difference is small (small `|dynamic_margin|`), `alpha` approaches 0, giving more weight to the softer sigmoid term, which allows for more model uncertainty. This creates a loss that is strict for clear preferences and lenient for ambiguous ones.\n\nWhen `cost(a) < cost(b)`, `rank_gap` is negative, making `dynamic_margin` negative. The loss then encourages `logp_a - logp_b` to be positive (greater than the negative margin), correctly preferring `a`.", "hyperparams": {"beta": 1.0, "gamma": 2.0}, "operators_used": ["rank_gap", "zscore", "tanh", "softplus", "logsigmoid", "sigmoid"]}}, "better_than_baseline": true}
{"generation": 2, "index": 8, "ir": {"name": "Adaptive Hinge-Sigmoid Hybrid Loss", "intuition": "This loss function creates a hybrid structure that combines the hinge-like margin mechanism of the 'Normalized Rank-Gap Hinge Loss' (Parent 0) with the probabilistic sigmoid framework of the 'Rank-Calibrated Sigmoid Loss' (Parent 1).\n\nFrom Parent 0, we inherit the core idea of a dynamic margin based on a standardized rank gap (`dynamic_margin = beta * tanh(zscore(rank_gap(cost_a, cost_b)))`) and the use of `softplus` to create a smooth, non-negative hinge loss. This ensures the loss only penalizes the model when its preference alignment is insufficient.\n\nFrom Parent 1, we inherit the adaptive temperature mechanism (`adaptive_gamma`), which scales the loss based on the magnitude of the cost difference. This makes the loss more sensitive when the cost difference is large and more forgiving when it is small.\n\nAs a new coupling idea, we embed the hinge loss term *inside* a sigmoid function. Instead of a simple hinge loss `softplus(margin - logp_diff)`, we compute a 'preference error' term, `preference_error = softplus(dynamic_margin - logp_diff)`. This error is zero if the model's preference `logp_diff` correctly exceeds the `dynamic_margin`. We then use this non-negative error as the argument to a sigmoid-like expression, scaled by the adaptive temperature. The final loss is `log(1 + adaptive_gamma * preference_error)`. This can be seen as a smooth version of `log(1+x)`, which behaves like `x` for small `x` and `log(x)` for large `x`, providing a stable gradient. This structure combines the 'zero loss for correct preference' property of a hinge loss with the calibrated scaling of a sigmoid-based loss.\n\nWhen `cost(a) < cost(b)`, the `dynamic_margin` is negative. The loss encourages `logp_diff` to be positive and larger than this margin to make the `preference_error` zero, thus correctly preferring `a`.", "pseudocode": "1. Compute the log-probability difference: logp_diff = logp_a - logp_b.\n2. Compute the signed, batch-relative rank gap: cost_gap_signed = rank_gap(cost_a, cost_b).\n3. Standardize the rank gap across the batch: cost_gap_zscored = zscore(cost_gap_signed).\n4. Create a bounded dynamic margin from the standardized rank gap (inherited from Parent 0): dynamic_margin = beta * tanh(cost_gap_zscored).\n5. Create an adaptive temperature based on the margin magnitude (inherited from Parent 1): adaptive_gamma = 1.0 + gamma * abs(dynamic_margin).\n6. Compute the preference error using a smooth hinge-like term (structure from Parent 0): preference_error = softplus(dynamic_margin - logp_diff).\n7. Couple the preference error with the adaptive temperature in a stable, logarithmic form (new coupling idea): loss = log(1.0 + adaptive_gamma * preference_error).", "hyperparams": {"beta": 1.0, "gamma": 0.5}, "operators_used": ["rank_gap", "zscore", "tanh", "softplus", "log"], "implementation_hint": {"expects": ["cost_a", "cost_b", "logp_a", "logp_b"], "returns": "scalar"}}, "fitness": {"hf_like_score": 8.375181198120117, "validation_objective": 8.375181198120117, "generalization_penalty": 0.0, "generalization_objectives": {"20": 4.033289194107056}, "train_score_mean": 10.466032495498657, "train_loss_mean": 0.6049032986164093, "pair_count": 12902386, "config": {"hf": {"problem": "tsp", "hf_steps": 100, "train_problem_size": 100, "valid_problem_sizes": [20], "train_batch_size": 64, "pomo_size": 64, "learning_rate": 0.0003, "weight_decay": 1e-06, "alpha": 0.05, "device": "cuda", "seed": 1234, "num_validation_episodes": 128, "validation_batch_size": 64, "generalization_penalty_weight": 1.0, "pool_version": "v0"}, "free_loss": {"f1_steps": 100, "f2_steps": 100, "f3_enabled": false}}, "loss_ir": {"name": "Adaptive Hinge-Sigmoid Hybrid Loss", "intuition": "This loss function creates a hybrid structure that combines the hinge-like margin mechanism of the 'Normalized Rank-Gap Hinge Loss' (Parent 0) with the probabilistic sigmoid framework of the 'Rank-Calibrated Sigmoid Loss' (Parent 1).\n\nFrom Parent 0, we inherit the core idea of a dynamic margin based on a standardized rank gap (`dynamic_margin = beta * tanh(zscore(rank_gap(cost_a, cost_b)))`) and the use of `softplus` to create a smooth, non-negative hinge loss. This ensures the loss only penalizes the model when its preference alignment is insufficient.\n\nFrom Parent 1, we inherit the adaptive temperature mechanism (`adaptive_gamma`), which scales the loss based on the magnitude of the cost difference. This makes the loss more sensitive when the cost difference is large and more forgiving when it is small.\n\nAs a new coupling idea, we embed the hinge loss term *inside* a sigmoid function. Instead of a simple hinge loss `softplus(margin - logp_diff)`, we compute a 'preference error' term, `preference_error = softplus(dynamic_margin - logp_diff)`. This error is zero if the model's preference `logp_diff` correctly exceeds the `dynamic_margin`. We then use this non-negative error as the argument to a sigmoid-like expression, scaled by the adaptive temperature. The final loss is `log(1 + adaptive_gamma * preference_error)`. This can be seen as a smooth version of `log(1+x)`, which behaves like `x` for small `x` and `log(x)` for large `x`, providing a stable gradient. This structure combines the 'zero loss for correct preference' property of a hinge loss with the calibrated scaling of a sigmoid-based loss.\n\nWhen `cost(a) < cost(b)`, the `dynamic_margin` is negative. The loss encourages `logp_diff` to be positive and larger than this margin to make the `preference_error` zero, thus correctly preferring `a`.", "hyperparams": {"beta": 1.0, "gamma": 0.5}, "operators_used": ["rank_gap", "zscore", "tanh", "softplus", "log"]}}, "better_than_baseline": true}
{"generation": 2, "index": 9, "ir": {"name": "Adaptive Temperature Rank-Hinge Loss", "intuition": "This loss function synergizes a dynamic hinge margin with an adaptive temperature mechanism. From 'Normalized Rank-Gap Hinge Loss' (Parent 1), we inherit the core structure of using a smooth hinge loss (`softplus`) with a dynamic margin derived from the standardized rank gap of costs (`beta * tanh(zscore(rank_gap(...)))`). This ensures the required preference gap scales with the relative cost difference in a batch. From 'Rank-Calibrated Sigmoid Loss' (Parent 2), we inherit the concept of an adaptive temperature, which modulates the loss's steepness based on the magnitude of the cost difference. \n\nThe first new coupling idea is to inject this adaptive temperature directly into the `softplus` hinge loss. We compute an `adaptive_temp` that increases as the cost difference becomes more significant. The argument of the `softplus` function (`margin - logp_diff`) is then multiplied by this temperature. This makes the loss penalty much steeper for mispredictions on pairs with large cost differences, while being more lenient for pairs with similar costs. The second new idea is a stability trick: we `clamp` the standardized cost gap before applying `tanh`. This prevents extremely rare z-scores from causing the `tanh` function to saturate too quickly, maintaining a smoother gradient landscape for the margin calculation. \n\nWhen `cost(a) < cost(b)`, the `rank_gap` and thus the `dynamic_margin` are negative. The loss term `softplus(adaptive_temp * (dynamic_margin - logp_diff))` encourages `logp_diff` to be positive and larger than the margin, correctly preferring `a` over `b`.", "pseudocode": "1. Compute the log-probability difference: logp_diff = logp_a - logp_b.\n2. Compute the signed, batch-relative rank gap: cost_gap_signed = rank_gap(cost_a, cost_b).\n3. Standardize the rank gap across the batch: cost_gap_zscored = zscore(cost_gap_signed).\n4. Apply a stability clamp to the standardized gap (new stability trick): clamped_gap = clamp(cost_gap_zscored, min=-3.0, max=3.0).\n5. Create a bounded dynamic margin from the clamped gap (inherited from Parent 1): dynamic_margin = beta * tanh(clamped_gap).\n6. Create an adaptive temperature based on the margin's magnitude (inspired by Parent 2): adaptive_temp = 1.0 + gamma * abs(dynamic_margin).\n7. Compute the hinge-like term: The argument is the difference between the target margin and the model's logp difference. We want logp_diff > dynamic_margin.\n8. Scale this term by the adaptive temperature and apply softplus for a smooth, non-negative loss (new coupling idea): loss = softplus(adaptive_temp * (dynamic_margin - logp_diff)).", "hyperparams": {"beta": 1.0, "gamma": 0.5}, "operators_used": ["rank_gap", "zscore", "clamp", "tanh", "softplus"], "implementation_hint": {"expects": ["cost_a", "cost_b", "logp_a", "logp_b"], "returns": "scalar"}}, "fitness": {"hf_like_score": 8.43416976928711, "validation_objective": 8.43416976928711, "generalization_penalty": 0.0, "generalization_objectives": {"20": 4.002745270729065}, "train_score_mean": 10.476910314559937, "train_loss_mean": 0.599139741063118, "pair_count": 12902391, "config": {"hf": {"problem": "tsp", "hf_steps": 100, "train_problem_size": 100, "valid_problem_sizes": [20], "train_batch_size": 64, "pomo_size": 64, "learning_rate": 0.0003, "weight_decay": 1e-06, "alpha": 0.05, "device": "cuda", "seed": 1234, "num_validation_episodes": 128, "validation_batch_size": 64, "generalization_penalty_weight": 1.0, "pool_version": "v0"}, "free_loss": {"f1_steps": 100, "f2_steps": 100, "f3_enabled": false}}, "loss_ir": {"name": "Adaptive Temperature Rank-Hinge Loss", "intuition": "This loss function synergizes a dynamic hinge margin with an adaptive temperature mechanism. From 'Normalized Rank-Gap Hinge Loss' (Parent 1), we inherit the core structure of using a smooth hinge loss (`softplus`) with a dynamic margin derived from the standardized rank gap of costs (`beta * tanh(zscore(rank_gap(...)))`). This ensures the required preference gap scales with the relative cost difference in a batch. From 'Rank-Calibrated Sigmoid Loss' (Parent 2), we inherit the concept of an adaptive temperature, which modulates the loss's steepness based on the magnitude of the cost difference. \n\nThe first new coupling idea is to inject this adaptive temperature directly into the `softplus` hinge loss. We compute an `adaptive_temp` that increases as the cost difference becomes more significant. The argument of the `softplus` function (`margin - logp_diff`) is then multiplied by this temperature. This makes the loss penalty much steeper for mispredictions on pairs with large cost differences, while being more lenient for pairs with similar costs. The second new idea is a stability trick: we `clamp` the standardized cost gap before applying `tanh`. This prevents extremely rare z-scores from causing the `tanh` function to saturate too quickly, maintaining a smoother gradient landscape for the margin calculation. \n\nWhen `cost(a) < cost(b)`, the `rank_gap` and thus the `dynamic_margin` are negative. The loss term `softplus(adaptive_temp * (dynamic_margin - logp_diff))` encourages `logp_diff` to be positive and larger than the margin, correctly preferring `a` over `b`.", "hyperparams": {"beta": 1.0, "gamma": 0.5}, "operators_used": ["rank_gap", "zscore", "clamp", "tanh", "softplus"]}}, "better_than_baseline": true}
{"generation": 3, "index": 0, "ir": {"name": "Adaptive Rank-Weighted Sigmoid Loss", "intuition": "This loss function creates a novel hybrid by framing the preference learning problem as a classification task, while dynamically weighting the importance of each preference pair based on its rank within the batch. The core of the loss is a sigmoid-based term, but its margin and weighting are derived from rank-based signals.\n\nFrom Parent 0 (`Adaptive Hinge-Sigmoid Hybrid Loss` - first version), we inherit the idea of using a standardized rank gap (`zscore(rank_gap(...))`) to create a dynamic signal. However, instead of using it as a margin, we use it to compute an adaptive weight.\n\nFrom Parent 1 (`Adaptive Hinge-Sigmoid Hybrid Loss` - second version), we inherit the core structure of a sigmoid-based loss, `logsigmoid(logp_diff - margin)`, which treats preference learning as a binary classification problem where the model must correctly classify which of the pair (`a` or `b`) is preferred.\n\nTwo new coupling ideas are introduced. First, the margin is not dynamic but a fixed hyperparameter `beta`, simplifying the sigmoid target. The second, and more significant, coupling idea is an adaptive weight, `adaptive_weight`, derived from the standardized rank gap. We apply `softplus` to the z-scored rank gap to create a non-negative weight that is small for easy-to-rank pairs (large cost difference) and large for hard-to-rank pairs (small cost difference). This is then scaled by `gamma`. This weighting scheme focuses the training on the most ambiguous or 'contested' pairs in the batch, where the model is most likely to be uncertain. The final loss is a weighted version of the sigmoid loss.\n\nWhen `cost(a) < cost(b)`, `rank_gap` is negative, making `zscore(rank_gap)` negative. `softplus` maps this to a small positive value, resulting in a small `adaptive_weight`. The loss encourages `logp_a - logp_b` to be greater than `beta`, correctly preferring `a`. The small weight indicates this is an 'easy' pair that requires less training focus.", "pseudocode": "1. Compute the log-probability difference: logp_diff = logp_a - logp_b.\n2. Compute the signed, batch-relative rank gap: cost_gap_signed = rank_gap(cost_a, cost_b).\n3. Standardize the rank gap across the batch (inherited from Parent 0): cost_gap_zscored = zscore(cost_gap_signed).\n4. Create an adaptive, non-negative weight from the standardized rank gap (new coupling idea): adaptive_weight = gamma * softplus(-cost_gap_zscored).\n5. Define a fixed preference margin: margin = beta.\n6. Compute the core sigmoid-based loss term (structure inherited from Parent 1): sigmoid_loss = -logsigmoid(logp_diff - margin).\n7. Apply the adaptive weight to the sigmoid loss: loss = adaptive_weight * sigmoid_loss.", "hyperparams": {"beta": 0.1, "gamma": 1.0}, "operators_used": ["rank_gap", "zscore", "softplus", "logsigmoid"], "implementation_hint": {"expects": ["cost_a", "cost_b", "logp_a", "logp_b"], "returns": "scalar"}}, "fitness": {"hf_like_score": 8.453609943389893, "validation_objective": 8.453609943389893, "generalization_penalty": 0.0, "generalization_objectives": {"20": 3.9965028762817383}, "train_score_mean": 10.480559825897217, "train_loss_mean": 0.6002099925279617, "pair_count": 12902389, "config": {"hf": {"problem": "tsp", "hf_steps": 100, "train_problem_size": 100, "valid_problem_sizes": [20], "train_batch_size": 64, "pomo_size": 64, "learning_rate": 0.0003, "weight_decay": 1e-06, "alpha": 0.05, "device": "cuda", "seed": 1234, "num_validation_episodes": 128, "validation_batch_size": 64, "generalization_penalty_weight": 1.0, "pool_version": "v0"}, "free_loss": {"f1_steps": 100, "f2_steps": 100, "f3_enabled": false}}, "loss_ir": {"name": "Adaptive Rank-Weighted Sigmoid Loss", "intuition": "This loss function creates a novel hybrid by framing the preference learning problem as a classification task, while dynamically weighting the importance of each preference pair based on its rank within the batch. The core of the loss is a sigmoid-based term, but its margin and weighting are derived from rank-based signals.\n\nFrom Parent 0 (`Adaptive Hinge-Sigmoid Hybrid Loss` - first version), we inherit the idea of using a standardized rank gap (`zscore(rank_gap(...))`) to create a dynamic signal. However, instead of using it as a margin, we use it to compute an adaptive weight.\n\nFrom Parent 1 (`Adaptive Hinge-Sigmoid Hybrid Loss` - second version), we inherit the core structure of a sigmoid-based loss, `logsigmoid(logp_diff - margin)`, which treats preference learning as a binary classification problem where the model must correctly classify which of the pair (`a` or `b`) is preferred.\n\nTwo new coupling ideas are introduced. First, the margin is not dynamic but a fixed hyperparameter `beta`, simplifying the sigmoid target. The second, and more significant, coupling idea is an adaptive weight, `adaptive_weight`, derived from the standardized rank gap. We apply `softplus` to the z-scored rank gap to create a non-negative weight that is small for easy-to-rank pairs (large cost difference) and large for hard-to-rank pairs (small cost difference). This is then scaled by `gamma`. This weighting scheme focuses the training on the most ambiguous or 'contested' pairs in the batch, where the model is most likely to be uncertain. The final loss is a weighted version of the sigmoid loss.\n\nWhen `cost(a) < cost(b)`, `rank_gap` is negative, making `zscore(rank_gap)` negative. `softplus` maps this to a small positive value, resulting in a small `adaptive_weight`. The loss encourages `logp_a - logp_b` to be greater than `beta`, correctly preferring `a`. The small weight indicates this is an 'easy' pair that requires less training focus.", "hyperparams": {"beta": 0.1, "gamma": 1.0}, "operators_used": ["rank_gap", "zscore", "softplus", "logsigmoid"]}}, "better_than_baseline": false}
{"generation": 3, "index": 1, "ir": {"name": "Rank-Weighted DPO-Hinge Loss", "intuition": "This loss function creates a hybrid that merges the rank-based margin from the Adaptive Hinge-Sigmoid Hybrid Loss (Parent 0) with the probabilistic preference modeling of DPO, which is implicitly present in the sigmoid term of Parent 1. The goal is to create a loss that behaves like DPO for clear preferences but transitions to a hinge-like behavior for ambiguous or incorrectly classified pairs.\n\nFrom Parent 0 (and Parent 1), we inherit the use of a dynamic, bounded margin based on the standardized rank gap: `dynamic_margin = beta * tanh(zscore(rank_gap(...)))`. This ensures the required preference strength scales with the relative cost difference in the batch.\n\nFrom Parent 1, we inherit the core structure of a sigmoid-based preference loss, `logsigmoid(logp_diff - margin)`, which is the fundamental building block of DPO-style losses. This frames the problem as maximizing the log-likelihood of the preferred completion.\n\nThe first new coupling idea is to introduce an adaptive temperature `adaptive_gamma` (inspired by Parent 0's adaptive scaling) that modulates the sharpness of the sigmoid function. This temperature is inversely related to the magnitude of the cost difference, making the loss more decisive for large cost gaps and more forgiving for small ones.\n\nThe second new coupling idea is to add a hinge-like penalty term, `hinge_penalty`, which is only activated when the model's preference is incorrect (i.e., `logp_diff` has the wrong sign relative to `dynamic_margin`). This penalty is computed using `softplus` on the mis-predicted preference difference, `softplus(-logp_diff * sign(dynamic_margin))`. This term, inherited from the hinge-loss structure of Parent 0, adds a robust penalty for clear misclassifications without affecting correctly classified pairs, preventing the loss from becoming too small when the model is right.\n\nWhen `cost(a) < cost(b)`, `dynamic_margin` is negative. The `dpo_term` encourages `logp_diff` to be positive. If `logp_diff` is incorrectly negative, the `hinge_penalty` becomes `softplus(logp_diff)`, adding a significant penalty and pushing the model to prefer `a`.", "pseudocode": "1. Compute the log-probability difference: logp_diff = logp_a - logp_b.\n2. Compute the signed, batch-relative rank gap: cost_gap_signed = rank_gap(cost_a, cost_b).\n3. Standardize the rank gap across the batch: cost_gap_zscored = zscore(cost_gap_signed).\n4. Create a bounded dynamic margin from the standardized rank gap (inherited from Parents 0 & 1): dynamic_margin = beta * tanh(cost_gap_zscored).\n5. Create an adaptive temperature based on the margin's magnitude (new coupling idea, inspired by Parent 0's adaptive gamma): adaptive_gamma = 1.0 + gamma / (1.0 + abs(dynamic_margin)).\n6. Compute the primary DPO-style loss term using the adaptive temperature (structure from Parent 1): dpo_term = -logsigmoid(adaptive_gamma * (logp_diff - dynamic_margin)).\n7. Compute a hinge penalty for incorrect preference direction (new coupling idea, inspired by Parent 0's hinge structure): hinge_penalty = softplus(-logp_diff * sign(dynamic_margin)).\n8. Combine the DPO term with the hinge penalty: loss = dpo_term + hinge_penalty.", "hyperparams": {"beta": 1.5, "gamma": 1.0}, "operators_used": ["rank_gap", "zscore", "tanh", "logsigmoid", "softplus"], "implementation_hint": {"expects": ["cost_a", "cost_b", "logp_a", "logp_b"], "returns": "scalar"}}, "fitness": {"hf_like_score": 8.461098670959473, "validation_objective": 8.461098670959473, "generalization_penalty": 0.0, "generalization_objectives": {"20": 3.9931020736694336}, "train_score_mean": 10.500639591217041, "train_loss_mean": 0.6005970323085785, "pair_count": 12902392, "config": {"hf": {"problem": "tsp", "hf_steps": 100, "train_problem_size": 100, "valid_problem_sizes": [20], "train_batch_size": 64, "pomo_size": 64, "learning_rate": 0.0003, "weight_decay": 1e-06, "alpha": 0.05, "device": "cuda", "seed": 1234, "num_validation_episodes": 128, "validation_batch_size": 64, "generalization_penalty_weight": 1.0, "pool_version": "v0"}, "free_loss": {"f1_steps": 100, "f2_steps": 100, "f3_enabled": false}}, "loss_ir": {"name": "Rank-Weighted DPO-Hinge Loss", "intuition": "This loss function creates a hybrid that merges the rank-based margin from the Adaptive Hinge-Sigmoid Hybrid Loss (Parent 0) with the probabilistic preference modeling of DPO, which is implicitly present in the sigmoid term of Parent 1. The goal is to create a loss that behaves like DPO for clear preferences but transitions to a hinge-like behavior for ambiguous or incorrectly classified pairs.\n\nFrom Parent 0 (and Parent 1), we inherit the use of a dynamic, bounded margin based on the standardized rank gap: `dynamic_margin = beta * tanh(zscore(rank_gap(...)))`. This ensures the required preference strength scales with the relative cost difference in the batch.\n\nFrom Parent 1, we inherit the core structure of a sigmoid-based preference loss, `logsigmoid(logp_diff - margin)`, which is the fundamental building block of DPO-style losses. This frames the problem as maximizing the log-likelihood of the preferred completion.\n\nThe first new coupling idea is to introduce an adaptive temperature `adaptive_gamma` (inspired by Parent 0's adaptive scaling) that modulates the sharpness of the sigmoid function. This temperature is inversely related to the magnitude of the cost difference, making the loss more decisive for large cost gaps and more forgiving for small ones.\n\nThe second new coupling idea is to add a hinge-like penalty term, `hinge_penalty`, which is only activated when the model's preference is incorrect (i.e., `logp_diff` has the wrong sign relative to `dynamic_margin`). This penalty is computed using `softplus` on the mis-predicted preference difference, `softplus(-logp_diff * sign(dynamic_margin))`. This term, inherited from the hinge-loss structure of Parent 0, adds a robust penalty for clear misclassifications without affecting correctly classified pairs, preventing the loss from becoming too small when the model is right.\n\nWhen `cost(a) < cost(b)`, `dynamic_margin` is negative. The `dpo_term` encourages `logp_diff` to be positive. If `logp_diff` is incorrectly negative, the `hinge_penalty` becomes `softplus(logp_diff)`, adding a significant penalty and pushing the model to prefer `a`.", "hyperparams": {"beta": 1.5, "gamma": 1.0}, "operators_used": ["rank_gap", "zscore", "tanh", "logsigmoid", "softplus"]}}, "better_than_baseline": false}
{"generation": 3, "index": 2, "ir": {"name": "Adaptive Rank-Modulated Hinge-Sigmoid Loss", "intuition": "This loss function constructs a hybrid that is sensitive to both the magnitude of the model's preference error and the rank-ordering of the costs within a batch. It dynamically shifts between a hinge-like behavior for clear preferences and a sigmoid-like behavior for ambiguous ones.\n\nFrom Parent 0, we inherit the core structure of a smooth hinge loss, `softplus(margin - logp_diff)`, which penalizes the model only when its preference `logp_diff` fails to exceed a target margin. We also inherit the idea of using `tanh` on a standardized signal to create a bounded, well-behaved term.\n\nFrom Parent 1, we inherit the use of a sigmoid function to create an adaptive weight, `adaptive_alpha`. This allows the loss to dynamically balance different components based on the input signals.\n\nWe introduce two new coupling ideas. First, instead of a static margin, we create a `dynamic_margin` that is a function of the log-probability difference itself: `dynamic_margin = beta * tanh(logp_diff)`. This links the target preference directly to the model's current output, creating a self-adjusting target. Second, the adaptive weight `adaptive_alpha` is now modulated by the standardized rank gap of the costs, `adaptive_alpha = sigmoid(gamma * zscore(rank_gap(cost_a, cost_b)))`. This means the loss's structure changes based on the relative ranking of the cost pair in the batch. For pairs with a large cost separation (high rank gap), `adaptive_alpha` approaches 1, emphasizing the hinge term for a stricter penalty. For pairs with a small cost separation (low rank gap), `adaptive_alpha` approaches 0.5, creating a more balanced, softer penalty.\n\nThe final loss combines these elements. When `cost(a) < cost(b)`, the rank gap is negative, making `adaptive_alpha` less than 0.5. The loss encourages `logp_a > logp_b` to make `dynamic_margin` positive and `logp_diff` larger than it, pushing the `softplus` argument towards negative infinity and the loss to zero.", "pseudocode": "1. Compute the log-probability difference: logp_diff = logp_a - logp_b.\n2. Compute the signed, batch-relative rank gap: cost_gap_signed = rank_gap(cost_a, cost_b).\n3. Standardize the rank gap across the batch: cost_gap_zscored = zscore(cost_gap_signed).\n4. Create an adaptive weight based on the standardized rank gap (inspired by Parent 1, but with a new signal): adaptive_alpha = sigmoid(gamma * cost_gap_zscored).\n5. Create a dynamic margin based on the model's own preference (new coupling idea): dynamic_margin = beta * tanh(logp_diff).\n6. Define the hinge loss component using the dynamic margin (structure from Parent 0): hinge_term = softplus(dynamic_margin - logp_diff).\n7. Define a complementary sigmoid-like component: sigmoid_term = softplus(-dynamic_margin - logp_diff).\n8. Combine the two components using the rank-adaptive weight: loss = (adaptive_alpha * hinge_term) + ((1 - adaptive_alpha) * sigmoid_term).", "hyperparams": {"beta": 1.0, "gamma": 2.0}, "operators_used": ["rank_gap", "zscore", "sigmoid", "tanh", "softplus"], "implementation_hint": {"expects": ["cost_a", "cost_b", "logp_a", "logp_b"], "returns": "scalar"}}, "fitness": {"hf_like_score": 8.485513687133789, "validation_objective": 8.485513687133789, "generalization_penalty": 0.0, "generalization_objectives": {"20": 4.021083354949951}, "train_score_mean": 10.495521183013915, "train_loss_mean": 0.6007979482412338, "pair_count": 12902391, "config": {"hf": {"problem": "tsp", "hf_steps": 100, "train_problem_size": 100, "valid_problem_sizes": [20], "train_batch_size": 64, "pomo_size": 64, "learning_rate": 0.0003, "weight_decay": 1e-06, "alpha": 0.05, "device": "cuda", "seed": 1234, "num_validation_episodes": 128, "validation_batch_size": 64, "generalization_penalty_weight": 1.0, "pool_version": "v0"}, "free_loss": {"f1_steps": 100, "f2_steps": 100, "f3_enabled": false}}, "loss_ir": {"name": "Adaptive Rank-Modulated Hinge-Sigmoid Loss", "intuition": "This loss function constructs a hybrid that is sensitive to both the magnitude of the model's preference error and the rank-ordering of the costs within a batch. It dynamically shifts between a hinge-like behavior for clear preferences and a sigmoid-like behavior for ambiguous ones.\n\nFrom Parent 0, we inherit the core structure of a smooth hinge loss, `softplus(margin - logp_diff)`, which penalizes the model only when its preference `logp_diff` fails to exceed a target margin. We also inherit the idea of using `tanh` on a standardized signal to create a bounded, well-behaved term.\n\nFrom Parent 1, we inherit the use of a sigmoid function to create an adaptive weight, `adaptive_alpha`. This allows the loss to dynamically balance different components based on the input signals.\n\nWe introduce two new coupling ideas. First, instead of a static margin, we create a `dynamic_margin` that is a function of the log-probability difference itself: `dynamic_margin = beta * tanh(logp_diff)`. This links the target preference directly to the model's current output, creating a self-adjusting target. Second, the adaptive weight `adaptive_alpha` is now modulated by the standardized rank gap of the costs, `adaptive_alpha = sigmoid(gamma * zscore(rank_gap(cost_a, cost_b)))`. This means the loss's structure changes based on the relative ranking of the cost pair in the batch. For pairs with a large cost separation (high rank gap), `adaptive_alpha` approaches 1, emphasizing the hinge term for a stricter penalty. For pairs with a small cost separation (low rank gap), `adaptive_alpha` approaches 0.5, creating a more balanced, softer penalty.\n\nThe final loss combines these elements. When `cost(a) < cost(b)`, the rank gap is negative, making `adaptive_alpha` less than 0.5. The loss encourages `logp_a > logp_b` to make `dynamic_margin` positive and `logp_diff` larger than it, pushing the `softplus` argument towards negative infinity and the loss to zero.", "hyperparams": {"beta": 1.0, "gamma": 2.0}, "operators_used": ["rank_gap", "zscore", "sigmoid", "tanh", "softplus"]}}, "better_than_baseline": false}
{"generation": 3, "index": 3, "ir": {"name": "Adaptive Margin-Weighted Sigmoid Hinge Loss", "intuition": "This loss function creates a novel hybrid by framing the preference learning problem as a weighted combination of two distinct signals, both modulated by a dynamic margin.\n\nFrom Parent 0, we inherit the core idea of a dynamic margin derived from a standardized rank gap (`dynamic_margin = beta * tanh(zscore(rank_gap(...)))`). We also inherit the `softplus` operator to create a smooth, one-sided hinge loss that penalizes incorrect preference margins.\n\nFrom Parent 1, we inherit the use of a `sigmoid` function to create an adaptive weight. However, instead of using it to mix two different loss types, we use it in a new way.\n\nOur first new coupling idea is to construct two complementary loss terms around the `dynamic_margin`. The first term, `hinge_term = softplus(dynamic_margin - logp_diff)`, is a standard hinge loss that penalizes the model only when `logp_diff` fails to meet the `dynamic_margin`. The second term, `sigmoid_term = -logsigmoid(logp_diff)`, is a standard sigmoid preference loss that always encourages `logp_diff` to be positive. The second new coupling idea is to use the `dynamic_margin` itself to create an adaptive weight that balances these two terms. The weight is `adaptive_alpha = sigmoid(gamma * dynamic_margin)`. When the cost difference is large and positive (`cost(a) >> cost(b)`), `dynamic_margin` is large and positive, so `adaptive_alpha` approaches 1. This places full weight on the `hinge_term`, strongly enforcing a large negative `logp_diff`. When the cost difference is large and negative (`cost(a) << cost(b)`), `dynamic_margin` is large and negative, so `adaptive_alpha` approaches 0. This places full weight on the `sigmoid_term`, strongly encouraging a large positive `logp_diff`. For ambiguous pairs where `cost(a)  cost(b)`, `dynamic_margin` is near zero, and `adaptive_alpha` is near 0.5, creating a balanced mixture of the two objectives.\n\nWhen `cost(a) < cost(b)`, `dynamic_margin` is negative, making `adaptive_alpha` small. The loss is dominated by `(1 - adaptive_alpha) * -logsigmoid(logp_diff)`, which encourages `logp_diff` to be positive, correctly preferring `a`.", "pseudocode": "1. Compute the log-probability difference: logp_diff = logp_a - logp_b.\n2. Compute the signed, batch-relative rank gap: cost_gap_signed = rank_gap(cost_a, cost_b).\n3. Standardize the rank gap across the batch: cost_gap_zscored = zscore(cost_gap_signed).\n4. Create a bounded dynamic margin from the standardized rank gap (inherited from Parent 0): dynamic_margin = beta * tanh(cost_gap_zscored).\n5. Define a hinge-like loss component that penalizes insufficient preference margins: hinge_term = softplus(dynamic_margin - logp_diff).\n6. Define a standard sigmoid preference loss component: sigmoid_term = -logsigmoid(logp_diff).\n7. Create an adaptive weight based on the signed dynamic margin (new coupling idea, structure from Parent 1): adaptive_alpha = sigmoid(gamma * dynamic_margin).\n8. Combine the two loss components using the adaptive weight (new coupling idea): loss = (adaptive_alpha * hinge_term) + ((1 - adaptive_alpha) * sigmoid_term).", "hyperparams": {"beta": 1.5, "gamma": 1.0}, "operators_used": ["rank_gap", "zscore", "tanh", "softplus", "logsigmoid", "sigmoid"], "implementation_hint": {"expects": ["cost_a", "cost_b", "logp_a", "logp_b"], "returns": "scalar"}}, "fitness": {"hf_like_score": 8.466405868530273, "validation_objective": 8.466405868530273, "generalization_penalty": 0.0, "generalization_objectives": {"20": 3.965281844139099}, "train_score_mean": 10.491985759735108, "train_loss_mean": 0.6003055310249329, "pair_count": 12902384, "config": {"hf": {"problem": "tsp", "hf_steps": 100, "train_problem_size": 100, "valid_problem_sizes": [20], "train_batch_size": 64, "pomo_size": 64, "learning_rate": 0.0003, "weight_decay": 1e-06, "alpha": 0.05, "device": "cuda", "seed": 1234, "num_validation_episodes": 128, "validation_batch_size": 64, "generalization_penalty_weight": 1.0, "pool_version": "v0"}, "free_loss": {"f1_steps": 100, "f2_steps": 100, "f3_enabled": false}}, "loss_ir": {"name": "Adaptive Margin-Weighted Sigmoid Hinge Loss", "intuition": "This loss function creates a novel hybrid by framing the preference learning problem as a weighted combination of two distinct signals, both modulated by a dynamic margin.\n\nFrom Parent 0, we inherit the core idea of a dynamic margin derived from a standardized rank gap (`dynamic_margin = beta * tanh(zscore(rank_gap(...)))`). We also inherit the `softplus` operator to create a smooth, one-sided hinge loss that penalizes incorrect preference margins.\n\nFrom Parent 1, we inherit the use of a `sigmoid` function to create an adaptive weight. However, instead of using it to mix two different loss types, we use it in a new way.\n\nOur first new coupling idea is to construct two complementary loss terms around the `dynamic_margin`. The first term, `hinge_term = softplus(dynamic_margin - logp_diff)`, is a standard hinge loss that penalizes the model only when `logp_diff` fails to meet the `dynamic_margin`. The second term, `sigmoid_term = -logsigmoid(logp_diff)`, is a standard sigmoid preference loss that always encourages `logp_diff` to be positive. The second new coupling idea is to use the `dynamic_margin` itself to create an adaptive weight that balances these two terms. The weight is `adaptive_alpha = sigmoid(gamma * dynamic_margin)`. When the cost difference is large and positive (`cost(a) >> cost(b)`), `dynamic_margin` is large and positive, so `adaptive_alpha` approaches 1. This places full weight on the `hinge_term`, strongly enforcing a large negative `logp_diff`. When the cost difference is large and negative (`cost(a) << cost(b)`), `dynamic_margin` is large and negative, so `adaptive_alpha` approaches 0. This places full weight on the `sigmoid_term`, strongly encouraging a large positive `logp_diff`. For ambiguous pairs where `cost(a)  cost(b)`, `dynamic_margin` is near zero, and `adaptive_alpha` is near 0.5, creating a balanced mixture of the two objectives.\n\nWhen `cost(a) < cost(b)`, `dynamic_margin` is negative, making `adaptive_alpha` small. The loss is dominated by `(1 - adaptive_alpha) * -logsigmoid(logp_diff)`, which encourages `logp_diff` to be positive, correctly preferring `a`.", "hyperparams": {"beta": 1.5, "gamma": 1.0}, "operators_used": ["rank_gap", "zscore", "tanh", "softplus", "logsigmoid", "sigmoid"]}}, "better_than_baseline": false}
{"generation": 3, "index": 4, "ir": {"name": "Rank-Adaptive Sigmoid Hinge Loss", "intuition": "This loss function creates a hybrid that uses a dynamic, rank-based margin to modulate a sigmoid preference term, and then applies a smooth hinge function to the result. The goal is to create a loss that only penalizes the model when its preference confidence is below an adaptively set threshold.\n\nFrom Parent 0, we inherit the core idea of a dynamic margin derived from a standardized rank gap (`dynamic_margin = beta * tanh(zscore(rank_gap(...)))`). This margin is negative when `cost(a) < cost(b)` and positive otherwise.\n\nFrom Parent 1, we inherit the structure of an adaptive weight (`adaptive_alpha = sigmoid(gamma * abs(dynamic_margin))`) that scales based on the magnitude of the cost difference. This makes the loss more sensitive to large, obvious cost differences.\n\nThe first new coupling idea is to use the `dynamic_margin` to shift the input of a `logsigmoid` function, calculating a 'preference score' as `logsigmoid(logp_diff - dynamic_margin)`. This score approaches 0 as the model's preference `logp_diff` correctly exceeds the target margin.\n\nThe second new coupling idea is to apply a `softplus` hinge function to the *negated* preference score, weighted by the `adaptive_alpha`. The loss is `softplus(adaptive_alpha - (-preference_score))`. This creates a 'confidence-based hinge'. The `adaptive_alpha` acts as a required confidence level. If the preference score is good enough (close to 0), `-preference_score` is also close to 0, and the loss becomes `softplus(adaptive_alpha - 0) > 0`. However, if the preference is strongly correct, `preference_score` becomes a large positive number, making `-preference_score` a large negative number, and the loss `softplus(adaptive_alpha + large_negative)` goes to zero. This means the model is only penalized if its preference certainty is below the threshold set by `adaptive_alpha`.\n\nWhen `cost(a) < cost(b)`, `dynamic_margin` is negative. The loss encourages `logp_diff` to be positive and larger than this margin, making the `preference_score` large and positive, which in turn drives the overall loss to zero.", "pseudocode": "1. Compute the log-probability difference: logp_diff = logp_a - logp_b.\n2. Compute the signed, batch-relative rank gap: cost_gap_signed = rank_gap(cost_a, cost_b).\n3. Standardize the rank gap across the batch: cost_gap_zscored = zscore(cost_gap_signed).\n4. Create a bounded dynamic margin from the standardized rank gap (inherited from Parent 0): dynamic_margin = beta * tanh(cost_gap_zscored).\n5. Create an adaptive weight based on the margin's magnitude (inherited from Parent 1): adaptive_alpha = sigmoid(gamma * abs(dynamic_margin)). This weight acts as a dynamic 'confidence margin'.\n6. Compute a preference score using a margin-adjusted sigmoid (new coupling idea 1): preference_score = -logsigmoid(logp_diff - dynamic_margin).\n7. Apply a smooth hinge function where the adaptive weight sets the margin for the preference score (new coupling idea 2): loss = softplus(adaptive_alpha - preference_score).", "hyperparams": {"beta": 1.5, "gamma": 2.0}, "operators_used": ["rank_gap", "zscore", "tanh", "sigmoid", "logsigmoid", "softplus"], "implementation_hint": {"expects": ["cost_a", "cost_b", "logp_a", "logp_b"], "returns": "scalar"}}, "fitness": {"hf_like_score": 8.491313934326172, "validation_objective": 8.491313934326172, "generalization_penalty": 0.0, "generalization_objectives": {"20": 4.011975288391113}, "train_score_mean": 10.521658897399902, "train_loss_mean": 0.6001426208019257, "pair_count": 12902389, "config": {"hf": {"problem": "tsp", "hf_steps": 100, "train_problem_size": 100, "valid_problem_sizes": [20], "train_batch_size": 64, "pomo_size": 64, "learning_rate": 0.0003, "weight_decay": 1e-06, "alpha": 0.05, "device": "cuda", "seed": 1234, "num_validation_episodes": 128, "validation_batch_size": 64, "generalization_penalty_weight": 1.0, "pool_version": "v0"}, "free_loss": {"f1_steps": 100, "f2_steps": 100, "f3_enabled": false}}, "loss_ir": {"name": "Rank-Adaptive Sigmoid Hinge Loss", "intuition": "This loss function creates a hybrid that uses a dynamic, rank-based margin to modulate a sigmoid preference term, and then applies a smooth hinge function to the result. The goal is to create a loss that only penalizes the model when its preference confidence is below an adaptively set threshold.\n\nFrom Parent 0, we inherit the core idea of a dynamic margin derived from a standardized rank gap (`dynamic_margin = beta * tanh(zscore(rank_gap(...)))`). This margin is negative when `cost(a) < cost(b)` and positive otherwise.\n\nFrom Parent 1, we inherit the structure of an adaptive weight (`adaptive_alpha = sigmoid(gamma * abs(dynamic_margin))`) that scales based on the magnitude of the cost difference. This makes the loss more sensitive to large, obvious cost differences.\n\nThe first new coupling idea is to use the `dynamic_margin` to shift the input of a `logsigmoid` function, calculating a 'preference score' as `logsigmoid(logp_diff - dynamic_margin)`. This score approaches 0 as the model's preference `logp_diff` correctly exceeds the target margin.\n\nThe second new coupling idea is to apply a `softplus` hinge function to the *negated* preference score, weighted by the `adaptive_alpha`. The loss is `softplus(adaptive_alpha - (-preference_score))`. This creates a 'confidence-based hinge'. The `adaptive_alpha` acts as a required confidence level. If the preference score is good enough (close to 0), `-preference_score` is also close to 0, and the loss becomes `softplus(adaptive_alpha - 0) > 0`. However, if the preference is strongly correct, `preference_score` becomes a large positive number, making `-preference_score` a large negative number, and the loss `softplus(adaptive_alpha + large_negative)` goes to zero. This means the model is only penalized if its preference certainty is below the threshold set by `adaptive_alpha`.\n\nWhen `cost(a) < cost(b)`, `dynamic_margin` is negative. The loss encourages `logp_diff` to be positive and larger than this margin, making the `preference_score` large and positive, which in turn drives the overall loss to zero.", "hyperparams": {"beta": 1.5, "gamma": 2.0}, "operators_used": ["rank_gap", "zscore", "tanh", "sigmoid", "logsigmoid", "softplus"]}}, "better_than_baseline": false}
{"generation": 3, "index": 5, "ir": {"name": "Softplus-Weighted Sigmoid Loss with Dynamic Margin", "intuition": "This loss function constructs a novel hybrid by using a smooth hinge-like term to dynamically weight a probabilistic sigmoid loss.\n\nFrom Parent 0, we inherit the core idea of using a smooth, non-negative hinge-like term, `softplus(margin - logp_diff)`, to represent a 'preference error'. This term is zero when the model's preference `logp_diff` correctly exceeds a target margin, and positive otherwise. We also inherit the `dynamic_margin` itself, which is based on a standardized rank gap: `beta * tanh(zscore(rank_gap(...)))`.\n\nFrom Parent 1, we inherit the fundamental structure of a sigmoid-based preference loss, specifically `-logsigmoid(logp_diff - margin)`, which frames the task as correctly classifying the preference direction relative to the margin.\n\nThe first new coupling idea is to use the `preference_error` term from Parent 0 not as a loss itself, but as an *adaptive weight* for the sigmoid loss from Parent 1. The resulting structure is `preference_error * -logsigmoid(...)`. This means that if the model's preference is already correct and sufficient (i.e., `preference_error` is near zero), the loss is squashed to zero, regardless of the sigmoid term's value. The loss penalty only 'activates' when the model fails to meet the margin requirement.\n\nThe second new coupling idea is to introduce a stabilizing `gamma` hyperparameter inside the `logsigmoid` term. Instead of `-logsigmoid(logp_diff - dynamic_margin)`, we use `-logsigmoid(gamma * (logp_diff - dynamic_margin))`. This `gamma` acts as an inverse temperature, controlling the steepness of the sigmoid function. A higher `gamma` makes the loss more sensitive to small deviations from the margin, while a lower `gamma` creates a smoother, more forgiving loss landscape.\n\nWhen `cost(a) < cost(b)`, the `dynamic_margin` is negative. The loss encourages `logp_diff` to be positive and larger than this margin. If `logp_diff` is sufficiently large, `preference_error` becomes zero, and the total loss is zero. If not, the loss penalizes the model, pushing `logp_diff` higher.", "pseudocode": "1. Compute the log-probability difference: logp_diff = logp_a - logp_b.\n2. Compute the signed, batch-relative rank gap: cost_gap_signed = rank_gap(cost_a, cost_b).\n3. Standardize the rank gap across the batch: cost_gap_zscored = zscore(cost_gap_signed).\n4. Create a bounded dynamic margin from the standardized rank gap (inherited from Parent 0 and 1): dynamic_margin = beta * tanh(cost_gap_zscored).\n5. Compute a smooth 'preference error' that acts as an adaptive weight (structure from Parent 0, used as a new coupling mechanism): preference_error_weight = softplus(dynamic_margin - logp_diff).\n6. Define the core sigmoid loss term, scaled by a new temperature parameter `gamma` (structure from Parent 1, modified with a new coupling idea): sigmoid_term = -logsigmoid(gamma * (logp_diff - dynamic_margin)).\n7. Combine the weight and the sigmoid term to form the final loss: loss = preference_error_weight * sigmoid_term.", "hyperparams": {"beta": 1.0, "gamma": 1.0}, "operators_used": ["rank_gap", "zscore", "tanh", "softplus", "logsigmoid"], "implementation_hint": {"expects": ["cost_a", "cost_b", "logp_a", "logp_b"], "returns": "scalar"}}, "fitness": {"hf_like_score": 8.409343719482422, "validation_objective": 8.409343719482422, "generalization_penalty": 0.0, "generalization_objectives": {"20": 3.9998934268951416}, "train_score_mean": 10.491048879623413, "train_loss_mean": 0.6031722170114517, "pair_count": 12902396, "config": {"hf": {"problem": "tsp", "hf_steps": 100, "train_problem_size": 100, "valid_problem_sizes": [20], "train_batch_size": 64, "pomo_size": 64, "learning_rate": 0.0003, "weight_decay": 1e-06, "alpha": 0.05, "device": "cuda", "seed": 1234, "num_validation_episodes": 128, "validation_batch_size": 64, "generalization_penalty_weight": 1.0, "pool_version": "v0"}, "free_loss": {"f1_steps": 100, "f2_steps": 100, "f3_enabled": false}}, "loss_ir": {"name": "Softplus-Weighted Sigmoid Loss with Dynamic Margin", "intuition": "This loss function constructs a novel hybrid by using a smooth hinge-like term to dynamically weight a probabilistic sigmoid loss.\n\nFrom Parent 0, we inherit the core idea of using a smooth, non-negative hinge-like term, `softplus(margin - logp_diff)`, to represent a 'preference error'. This term is zero when the model's preference `logp_diff` correctly exceeds a target margin, and positive otherwise. We also inherit the `dynamic_margin` itself, which is based on a standardized rank gap: `beta * tanh(zscore(rank_gap(...)))`.\n\nFrom Parent 1, we inherit the fundamental structure of a sigmoid-based preference loss, specifically `-logsigmoid(logp_diff - margin)`, which frames the task as correctly classifying the preference direction relative to the margin.\n\nThe first new coupling idea is to use the `preference_error` term from Parent 0 not as a loss itself, but as an *adaptive weight* for the sigmoid loss from Parent 1. The resulting structure is `preference_error * -logsigmoid(...)`. This means that if the model's preference is already correct and sufficient (i.e., `preference_error` is near zero), the loss is squashed to zero, regardless of the sigmoid term's value. The loss penalty only 'activates' when the model fails to meet the margin requirement.\n\nThe second new coupling idea is to introduce a stabilizing `gamma` hyperparameter inside the `logsigmoid` term. Instead of `-logsigmoid(logp_diff - dynamic_margin)`, we use `-logsigmoid(gamma * (logp_diff - dynamic_margin))`. This `gamma` acts as an inverse temperature, controlling the steepness of the sigmoid function. A higher `gamma` makes the loss more sensitive to small deviations from the margin, while a lower `gamma` creates a smoother, more forgiving loss landscape.\n\nWhen `cost(a) < cost(b)`, the `dynamic_margin` is negative. The loss encourages `logp_diff` to be positive and larger than this margin. If `logp_diff` is sufficiently large, `preference_error` becomes zero, and the total loss is zero. If not, the loss penalizes the model, pushing `logp_diff` higher.", "hyperparams": {"beta": 1.0, "gamma": 1.0}, "operators_used": ["rank_gap", "zscore", "tanh", "softplus", "logsigmoid"]}}, "better_than_baseline": true}
{"generation": 3, "index": 6, "ir": {"name": "Rank-Weighted Dual-Margin Sigmoid Loss", "intuition": "This loss function creates a hybrid that uses a dynamic margin inside a sigmoid framework, with a novel weighting scheme. The core structure is inspired by sigmoid-based losses, but it incorporates two distinct margins to handle different aspects of preference learning.\n\nFrom Parent 0 (Adaptive Hinge-Sigmoid Hybrid Loss, first version), we inherit the idea of a `dynamic_margin` derived from the standardized rank gap (`beta * tanh(zscore(rank_gap(...)))`). This margin captures the relative importance of a preference pair within the current batch.\n\nFrom Parent 1 (Adaptive Hinge-Sigmoid Hybrid Loss, second version), we inherit the use of a sigmoid function to frame the loss as a probabilistic classification problem (`logsigmoid(...)`). This provides a smooth, bounded loss signal.\n\nWe introduce two new coupling ideas:\n1. **Dual-Margin Mechanism**: Instead of a single margin, we use two. The `dynamic_margin` from Parent 0 is used to define the preference target, similar to how it's used in both parents. We introduce a *second*, fixed margin `m` which acts as a 'confidence threshold'. The loss is constructed as `logsigmoid(logp_diff - dynamic_margin - m)`. The model is thus encouraged not only to meet the `dynamic_margin` but to exceed it by a fixed confidence `m`.\n2. **Rank-Based Adaptive Weighting**: We introduce a weight `w` that scales the loss based on the magnitude of the `dynamic_margin`. This is a new take on the adaptive scaling seen in both parents. Instead of using `abs(dynamic_margin)` directly, we use `w = 1.0 + gamma * abs(dynamic_margin)`. This weight amplifies the loss for pairs with larger cost differences (more important pairs), making the model prioritize learning clear preferences, while down-weighting ambiguous pairs.\n\nWhen `cost(a) < cost(b)`, `rank_gap` is negative, making `dynamic_margin` negative. The loss then encourages `logp_diff = logp_a - logp_b` to be positive and larger than `dynamic_margin + m`, correctly preferring `a`.", "pseudocode": "1. Compute the log-probability difference: logp_diff = logp_a - logp_b.\n2. Compute the signed, batch-relative rank gap: cost_gap_signed = rank_gap(cost_a, cost_b).\n3. Standardize the rank gap across the batch: cost_gap_zscored = zscore(cost_gap_signed).\n4. Create a bounded dynamic margin from the standardized rank gap (idea inherited from Parent 0): dynamic_margin = beta * tanh(cost_gap_zscored).\n5. Create an adaptive weight based on the margin's magnitude (new coupling idea): adaptive_weight = 1.0 + gamma * abs(dynamic_margin).\n6. Compute the loss argument using a dual-margin structure (new coupling idea): loss_argument = logp_diff - dynamic_margin - m.\n7. Compute the final weighted sigmoid loss (structure inspired by Parent 1): loss = -adaptive_weight * logsigmoid(loss_argument).", "hyperparams": {"beta": 1.0, "gamma": 0.5, "m": 0.1}, "operators_used": ["rank_gap", "zscore", "tanh", "logsigmoid"], "implementation_hint": {"expects": ["cost_a", "cost_b", "logp_a", "logp_b"], "returns": "scalar"}}, "fitness": {"hf_like_score": 8.458976745605469, "validation_objective": 8.458976745605469, "generalization_penalty": 0.0, "generalization_objectives": {"20": 4.011566877365112}, "train_score_mean": 10.48274694442749, "train_loss_mean": 0.599788333773613, "pair_count": 12902386, "config": {"hf": {"problem": "tsp", "hf_steps": 100, "train_problem_size": 100, "valid_problem_sizes": [20], "train_batch_size": 64, "pomo_size": 64, "learning_rate": 0.0003, "weight_decay": 1e-06, "alpha": 0.05, "device": "cuda", "seed": 1234, "num_validation_episodes": 128, "validation_batch_size": 64, "generalization_penalty_weight": 1.0, "pool_version": "v0"}, "free_loss": {"f1_steps": 100, "f2_steps": 100, "f3_enabled": false}}, "loss_ir": {"name": "Rank-Weighted Dual-Margin Sigmoid Loss", "intuition": "This loss function creates a hybrid that uses a dynamic margin inside a sigmoid framework, with a novel weighting scheme. The core structure is inspired by sigmoid-based losses, but it incorporates two distinct margins to handle different aspects of preference learning.\n\nFrom Parent 0 (Adaptive Hinge-Sigmoid Hybrid Loss, first version), we inherit the idea of a `dynamic_margin` derived from the standardized rank gap (`beta * tanh(zscore(rank_gap(...)))`). This margin captures the relative importance of a preference pair within the current batch.\n\nFrom Parent 1 (Adaptive Hinge-Sigmoid Hybrid Loss, second version), we inherit the use of a sigmoid function to frame the loss as a probabilistic classification problem (`logsigmoid(...)`). This provides a smooth, bounded loss signal.\n\nWe introduce two new coupling ideas:\n1. **Dual-Margin Mechanism**: Instead of a single margin, we use two. The `dynamic_margin` from Parent 0 is used to define the preference target, similar to how it's used in both parents. We introduce a *second*, fixed margin `m` which acts as a 'confidence threshold'. The loss is constructed as `logsigmoid(logp_diff - dynamic_margin - m)`. The model is thus encouraged not only to meet the `dynamic_margin` but to exceed it by a fixed confidence `m`.\n2. **Rank-Based Adaptive Weighting**: We introduce a weight `w` that scales the loss based on the magnitude of the `dynamic_margin`. This is a new take on the adaptive scaling seen in both parents. Instead of using `abs(dynamic_margin)` directly, we use `w = 1.0 + gamma * abs(dynamic_margin)`. This weight amplifies the loss for pairs with larger cost differences (more important pairs), making the model prioritize learning clear preferences, while down-weighting ambiguous pairs.\n\nWhen `cost(a) < cost(b)`, `rank_gap` is negative, making `dynamic_margin` negative. The loss then encourages `logp_diff = logp_a - logp_b` to be positive and larger than `dynamic_margin + m`, correctly preferring `a`.", "hyperparams": {"beta": 1.0, "gamma": 0.5, "m": 0.1}, "operators_used": ["rank_gap", "zscore", "tanh", "logsigmoid"]}}, "better_than_baseline": false}
{"generation": 3, "index": 7, "ir": {"name": "Self-Correcting Rank-Gap Loss", "intuition": "This loss function creates a self-correcting mechanism by blending a primary hinge-based loss with a secondary sigmoid-based regularizer, where the strength of the regularizer is determined by the model's own performance.\n\nFrom Parent 0, we inherit the core structure of a smooth hinge loss based on a dynamic margin: `softplus(dynamic_margin - logp_diff)`. The `dynamic_margin` itself is also inherited, being a `tanh`-bounded, z-scored rank gap: `beta * tanh(zscore(rank_gap(cost_a, cost_b)))`. This provides a robust primary signal that enforces a preference margin.\n\nFrom Parent 1, we inherit the use of a sigmoid-based term, `logsigmoid`, to frame preference learning as a classification problem. Specifically, we use `-logsigmoid(logp_diff - dynamic_margin)` as a secondary, softer loss component.\n\nThe new coupling idea is a 'self-correction weight' (`correction_weight`). This weight is computed from the primary hinge loss term itself. When the model's preference `logp_diff` correctly satisfies the margin (i.e., `logp_diff > dynamic_margin`), the hinge term `primary_loss` becomes close to zero. The `correction_weight = tanh(gamma * primary_loss)` also becomes near zero, effectively turning off the secondary sigmoid loss. However, when the model makes a mistake and the `primary_loss` is large, the `correction_weight` approaches 1. This activates the secondary sigmoid loss, adding a supplementary gradient signal to help 'correct' the model's prediction. This creates a loss that focuses its full power only on difficult examples where the model is failing.\n\nWhen `cost(a) < cost(b)`, the `rank_gap` and `dynamic_margin` are negative. The loss encourages `logp_diff` to be positive and larger than this negative margin, correctly preferring `a`.", "pseudocode": "1. Compute the log-probability difference: logp_diff = logp_a - logp_b.\n2. Compute the signed, batch-relative rank gap: cost_gap_signed = rank_gap(cost_a, cost_b).\n3. Standardize the rank gap across the batch: cost_gap_zscored = zscore(cost_gap_signed).\n4. Create a bounded dynamic margin from the standardized rank gap (inherited from Parent 0): dynamic_margin = beta * tanh(cost_gap_zscored).\n5. Compute the primary loss component using a smooth hinge structure (inherited from Parent 0): primary_loss = softplus(dynamic_margin - logp_diff).\n6. Compute the secondary loss component using a sigmoid structure (inspired by Parent 1): secondary_loss = -logsigmoid(logp_diff - dynamic_margin).\n7. Create a self-correction weight based on the magnitude of the primary loss (new coupling idea): correction_weight = tanh(gamma * primary_loss).\n8. Combine the primary loss with the weighted secondary loss: loss = primary_loss + (correction_weight * secondary_loss).", "hyperparams": {"beta": 1.0, "gamma": 0.5}, "operators_used": ["rank_gap", "zscore", "tanh", "softplus", "logsigmoid"], "implementation_hint": {"expects": ["cost_a", "cost_b", "logp_a", "logp_b"], "returns": "scalar"}}, "fitness": {"hf_like_score": 8.462099075317383, "validation_objective": 8.462099075317383, "generalization_penalty": 0.0, "generalization_objectives": {"20": 4.000938892364502}, "train_score_mean": 10.500088567733764, "train_loss_mean": 0.6010218966007232, "pair_count": 12902391, "config": {"hf": {"problem": "tsp", "hf_steps": 100, "train_problem_size": 100, "valid_problem_sizes": [20], "train_batch_size": 64, "pomo_size": 64, "learning_rate": 0.0003, "weight_decay": 1e-06, "alpha": 0.05, "device": "cuda", "seed": 1234, "num_validation_episodes": 128, "validation_batch_size": 64, "generalization_penalty_weight": 1.0, "pool_version": "v0"}, "free_loss": {"f1_steps": 100, "f2_steps": 100, "f3_enabled": false}}, "loss_ir": {"name": "Self-Correcting Rank-Gap Loss", "intuition": "This loss function creates a self-correcting mechanism by blending a primary hinge-based loss with a secondary sigmoid-based regularizer, where the strength of the regularizer is determined by the model's own performance.\n\nFrom Parent 0, we inherit the core structure of a smooth hinge loss based on a dynamic margin: `softplus(dynamic_margin - logp_diff)`. The `dynamic_margin` itself is also inherited, being a `tanh`-bounded, z-scored rank gap: `beta * tanh(zscore(rank_gap(cost_a, cost_b)))`. This provides a robust primary signal that enforces a preference margin.\n\nFrom Parent 1, we inherit the use of a sigmoid-based term, `logsigmoid`, to frame preference learning as a classification problem. Specifically, we use `-logsigmoid(logp_diff - dynamic_margin)` as a secondary, softer loss component.\n\nThe new coupling idea is a 'self-correction weight' (`correction_weight`). This weight is computed from the primary hinge loss term itself. When the model's preference `logp_diff` correctly satisfies the margin (i.e., `logp_diff > dynamic_margin`), the hinge term `primary_loss` becomes close to zero. The `correction_weight = tanh(gamma * primary_loss)` also becomes near zero, effectively turning off the secondary sigmoid loss. However, when the model makes a mistake and the `primary_loss` is large, the `correction_weight` approaches 1. This activates the secondary sigmoid loss, adding a supplementary gradient signal to help 'correct' the model's prediction. This creates a loss that focuses its full power only on difficult examples where the model is failing.\n\nWhen `cost(a) < cost(b)`, the `rank_gap` and `dynamic_margin` are negative. The loss encourages `logp_diff` to be positive and larger than this negative margin, correctly preferring `a`.", "hyperparams": {"beta": 1.0, "gamma": 0.5}, "operators_used": ["rank_gap", "zscore", "tanh", "softplus", "logsigmoid"]}}, "better_than_baseline": false}
{"generation": 3, "index": 8, "ir": {"name": "Adaptively Weighted Hinge-Sigmoid Loss with Rank-Gap Scaling", "intuition": "This loss function creates a sophisticated hybrid by combining the adaptive weighting scheme of Parent 1 with the stable logarithmic error structure of Parent 0, while introducing a new coupling mechanism that directly scales the preference error by the rank gap.\n\nFrom Parent 1, we inherit the core idea of an adaptive weight (`adaptive_alpha`) that balances two different loss philosophies. This weight is derived from the magnitude of the preference margin, making the loss stricter for clear-cut preferences and more lenient for ambiguous ones.\n\nFrom Parent 0, we inherit the structure of a stable, non-negative `preference_error` term, `softplus(margin - logp_diff)`, which is zero when the model's preference is correct. We also adopt its final logarithmic form, `log(1 + error_term)`, which provides stable gradients.\n\nWe introduce two new coupling ideas. First, instead of combining two separate loss terms (hinge and sigmoid), we use the adaptive weight to modulate a single, unified error term. The loss is structured as `log(1 + adaptive_alpha * preference_error)`. This simplifies the formulation while retaining the adaptive behavior. Second, and more importantly, we introduce a new scaling factor, `rank_gap_scale`, derived from the non-standardized rank gap. This factor directly multiplies the final loss value. The intuition is that pairs with larger cost differences (and thus larger rank gaps) should contribute more significantly to the overall loss, adding another layer of calibration.\n\nWhen `cost(a) < cost(b)`, `dynamic_margin` is negative. The loss encourages `logp_diff` to be positive and greater than this margin to drive `preference_error` to zero, correctly preferring `a`.", "pseudocode": "1. Compute the log-probability difference: logp_diff = logp_a - logp_b.\n2. Compute the signed, batch-relative rank gap: cost_gap_signed = rank_gap(cost_a, cost_b).\n3. Standardize the rank gap across the batch: cost_gap_zscored = zscore(cost_gap_signed).\n4. Create a bounded dynamic margin from the standardized rank gap (inherited structure): dynamic_margin = beta * tanh(cost_gap_zscored).\n5. Compute the preference error using a smooth hinge-like term (inherited from Parent 0): preference_error = softplus(dynamic_margin - logp_diff).\n6. Create an adaptive weight based on the margin's magnitude (inherited from Parent 1): adaptive_alpha = sigmoid(gamma * abs(dynamic_margin)).\n7. Create a rank gap scaling factor (new coupling idea): rank_gap_scale = 1.0 + softplus(abs(cost_gap_signed)).\n8. Combine the preference error and adaptive weight in a stable logarithmic form (structure from Parent 0, coupling from Parent 1): weighted_error = log(1.0 + adaptive_alpha * preference_error).\n9. Apply the rank gap scaling to the final loss (new coupling idea): loss = rank_gap_scale * weighted_error.", "hyperparams": {"beta": 1.0, "gamma": 1.5}, "operators_used": ["rank_gap", "zscore", "tanh", "softplus", "sigmoid", "log"], "implementation_hint": {"expects": ["cost_a", "cost_b", "logp_a", "logp_b"], "returns": "scalar"}}, "fitness": {"hf_like_score": 8.394684314727783, "validation_objective": 8.394684314727783, "generalization_penalty": 0.0, "generalization_objectives": {"20": 3.9919164180755615}, "train_score_mean": 10.460932054519652, "train_loss_mean": 0.6024963009357452, "pair_count": 12902387, "config": {"hf": {"problem": "tsp", "hf_steps": 100, "train_problem_size": 100, "valid_problem_sizes": [20], "train_batch_size": 64, "pomo_size": 64, "learning_rate": 0.0003, "weight_decay": 1e-06, "alpha": 0.05, "device": "cuda", "seed": 1234, "num_validation_episodes": 128, "validation_batch_size": 64, "generalization_penalty_weight": 1.0, "pool_version": "v0"}, "free_loss": {"f1_steps": 100, "f2_steps": 100, "f3_enabled": false}}, "loss_ir": {"name": "Adaptively Weighted Hinge-Sigmoid Loss with Rank-Gap Scaling", "intuition": "This loss function creates a sophisticated hybrid by combining the adaptive weighting scheme of Parent 1 with the stable logarithmic error structure of Parent 0, while introducing a new coupling mechanism that directly scales the preference error by the rank gap.\n\nFrom Parent 1, we inherit the core idea of an adaptive weight (`adaptive_alpha`) that balances two different loss philosophies. This weight is derived from the magnitude of the preference margin, making the loss stricter for clear-cut preferences and more lenient for ambiguous ones.\n\nFrom Parent 0, we inherit the structure of a stable, non-negative `preference_error` term, `softplus(margin - logp_diff)`, which is zero when the model's preference is correct. We also adopt its final logarithmic form, `log(1 + error_term)`, which provides stable gradients.\n\nWe introduce two new coupling ideas. First, instead of combining two separate loss terms (hinge and sigmoid), we use the adaptive weight to modulate a single, unified error term. The loss is structured as `log(1 + adaptive_alpha * preference_error)`. This simplifies the formulation while retaining the adaptive behavior. Second, and more importantly, we introduce a new scaling factor, `rank_gap_scale`, derived from the non-standardized rank gap. This factor directly multiplies the final loss value. The intuition is that pairs with larger cost differences (and thus larger rank gaps) should contribute more significantly to the overall loss, adding another layer of calibration.\n\nWhen `cost(a) < cost(b)`, `dynamic_margin` is negative. The loss encourages `logp_diff` to be positive and greater than this margin to drive `preference_error` to zero, correctly preferring `a`.", "hyperparams": {"beta": 1.0, "gamma": 1.5}, "operators_used": ["rank_gap", "zscore", "tanh", "softplus", "sigmoid", "log"]}}, "better_than_baseline": true}
{"generation": 3, "index": 9, "ir": {"name": "Rank-Weighted DPO-Hinge Fusion", "intuition": "This loss function fuses the probabilistic framework of Direct Preference Optimization (DPO) with the margin-based structure of a hinge loss, using a novel rank-based weighting scheme to balance their contributions.\n\nFrom Parent 0 (Adaptive Hinge-Sigmoid Hybrid Loss v1), we inherit the core idea of using a smooth, one-sided hinge loss via `softplus`. This ensures that the model is only penalized when its preference `logp_diff` fails to meet a target margin, enforcing a clear separation for correctly ordered pairs.\n\nFrom Parent 1 (Adaptive Hinge-Sigmoid Hybrid Loss v2), we inherit the concept of creating a dynamic margin using `beta * tanh(zscore(rank_gap(...)))`. This makes the target preference margin adaptive to the relative cost difference within the batch.\n\nWe introduce two new coupling ideas. First, we structure the core loss in a DPO-like fashion: `logsigmoid(beta * (logp_diff - logr_diff))`, where `logr_diff` is the log-ratio of reference model probabilities (or 0 if not used). This frames the problem as maximizing the log-likelihood of the preferred completion. The `beta` hyperparameter now acts as the DPO temperature.\n\nSecond, and more importantly, we create a dynamic fusion weight, `alpha`, to blend this DPO term with a pure hinge loss. This weight is derived from the magnitude of the standardized rank gap: `alpha = sigmoid(gamma * zscore(rank_gap(...)))`. When the cost difference is large (large positive `zscore`), `alpha` approaches 1, emphasizing the DPO term which is effective for clear preferences. When the cost difference is small or inverted (small or negative `zscore`), `alpha` approaches 0.5 or less, giving more influence to the hinge term, `softplus(margin - logp_diff)`, which robustly enforces a minimum preference gap even for ambiguous or mislabeled pairs. This creates a loss that is probabilistically grounded for clear cases and robustly margin-enforcing for ambiguous ones.\n\nWhen `cost(a) < cost(b)`, `rank_gap` is negative. This makes `alpha` small, prioritizing the hinge term which encourages `logp_a > logp_b`. It also makes the argument to the `logsigmoid` in the DPO term negative, which similarly encourages `logp_a > logp_b` to minimize the loss.", "pseudocode": "1. Compute the log-probability difference: logp_diff = logp_a - logp_b.\n2. Compute the signed, batch-relative rank gap: cost_gap_signed = rank_gap(cost_a, cost_b).\n3. Standardize the rank gap across the batch: cost_gap_zscored = zscore(cost_gap_signed).\n4. Create a dynamic margin from the standardized rank gap (idea inherited from Parent 1): dynamic_margin = margin_scale * tanh(cost_gap_zscored).\n5. Define the DPO-like probabilistic loss component: dpo_term = -logsigmoid(beta * logp_diff). Note: Assumes reference logp ratio is 0.\n6. Define the hinge loss component (structure inherited from Parent 0): hinge_term = softplus(dynamic_margin - logp_diff).\n7. Create a dynamic fusion weight based on the standardized rank gap (new coupling idea): alpha = sigmoid(gamma * cost_gap_zscored).\n8. Combine the two loss components using the dynamic weight: loss = (alpha * dpo_term) + ((1.0 - alpha) * hinge_term).", "hyperparams": {"beta": 0.1, "gamma": 1.0, "margin_scale": 0.5}, "operators_used": ["rank_gap", "zscore", "tanh", "logsigmoid", "softplus", "sigmoid"], "implementation_hint": {"expects": ["cost_a", "cost_b", "logp_a", "logp_b"], "returns": "scalar"}}, "fitness": {"hf_like_score": 8.42029857635498, "validation_objective": 8.42029857635498, "generalization_penalty": 0.0, "generalization_objectives": {"20": 4.014054656028748}, "train_score_mean": 10.462532644271851, "train_loss_mean": 0.6036952000856399, "pair_count": 12902391, "config": {"hf": {"problem": "tsp", "hf_steps": 100, "train_problem_size": 100, "valid_problem_sizes": [20], "train_batch_size": 64, "pomo_size": 64, "learning_rate": 0.0003, "weight_decay": 1e-06, "alpha": 0.05, "device": "cuda", "seed": 1234, "num_validation_episodes": 128, "validation_batch_size": 64, "generalization_penalty_weight": 1.0, "pool_version": "v0"}, "free_loss": {"f1_steps": 100, "f2_steps": 100, "f3_enabled": false}}, "loss_ir": {"name": "Rank-Weighted DPO-Hinge Fusion", "intuition": "This loss function fuses the probabilistic framework of Direct Preference Optimization (DPO) with the margin-based structure of a hinge loss, using a novel rank-based weighting scheme to balance their contributions.\n\nFrom Parent 0 (Adaptive Hinge-Sigmoid Hybrid Loss v1), we inherit the core idea of using a smooth, one-sided hinge loss via `softplus`. This ensures that the model is only penalized when its preference `logp_diff` fails to meet a target margin, enforcing a clear separation for correctly ordered pairs.\n\nFrom Parent 1 (Adaptive Hinge-Sigmoid Hybrid Loss v2), we inherit the concept of creating a dynamic margin using `beta * tanh(zscore(rank_gap(...)))`. This makes the target preference margin adaptive to the relative cost difference within the batch.\n\nWe introduce two new coupling ideas. First, we structure the core loss in a DPO-like fashion: `logsigmoid(beta * (logp_diff - logr_diff))`, where `logr_diff` is the log-ratio of reference model probabilities (or 0 if not used). This frames the problem as maximizing the log-likelihood of the preferred completion. The `beta` hyperparameter now acts as the DPO temperature.\n\nSecond, and more importantly, we create a dynamic fusion weight, `alpha`, to blend this DPO term with a pure hinge loss. This weight is derived from the magnitude of the standardized rank gap: `alpha = sigmoid(gamma * zscore(rank_gap(...)))`. When the cost difference is large (large positive `zscore`), `alpha` approaches 1, emphasizing the DPO term which is effective for clear preferences. When the cost difference is small or inverted (small or negative `zscore`), `alpha` approaches 0.5 or less, giving more influence to the hinge term, `softplus(margin - logp_diff)`, which robustly enforces a minimum preference gap even for ambiguous or mislabeled pairs. This creates a loss that is probabilistically grounded for clear cases and robustly margin-enforcing for ambiguous ones.\n\nWhen `cost(a) < cost(b)`, `rank_gap` is negative. This makes `alpha` small, prioritizing the hinge term which encourages `logp_a > logp_b`. It also makes the argument to the `logsigmoid` in the DPO term negative, which similarly encourages `logp_a > logp_b` to minimize the loss.", "hyperparams": {"beta": 0.1, "gamma": 1.0, "margin_scale": 0.5}, "operators_used": ["rank_gap", "zscore", "tanh", "logsigmoid", "softplus", "sigmoid"]}}, "better_than_baseline": true}
{"generation": 4, "index": 0, "ir": {"name": "Rank-Calibrated Dynamic Hinge Loss", "intuition": "This loss function creates a hybrid that uses a dynamic, rank-based margin for its primary hinge structure, and then calibrates the penalty using an adaptive, sigmoid-based weight. The goal is to combine a robust margin definition with a smooth, adaptive penalty.\n\nFrom Parent 0 ('Adaptive Hinge-Sigmoid Hybrid Loss'), we inherit the core concept of a `dynamic_margin` derived from the standardized rank gap (`beta * tanh(zscore(rank_gap(...)))`). This ensures the required preference margin adapts to the relative cost difference within the batch. We also inherit the use of `softplus` to create a smooth, non-negative hinge error term: `softplus(dynamic_margin - logp_diff)`.\n\nFrom Parent 1 ('Adaptively Weighted Hinge-Sigmoid Loss with Rank-Gap Scaling'), we inherit the idea of an `adaptive_alpha` weight. This weight, calculated using `sigmoid(gamma * abs(dynamic_margin))`, modulates the loss based on the confidence of the preference signal (i.e., the magnitude of the margin). It makes the loss more forgiving for ambiguous pairs (small margin) and stricter for clear-cut pairs (large margin).\n\nAs a new coupling idea, we use this `adaptive_alpha` weight to directly scale the hinge error *before* it enters the final logarithmic stabilization term. The structure becomes `log(1.0 + adaptive_alpha * preference_error)`. This elegantly combines the adaptive weighting from Parent 1 with the stable hinge error from Parent 0, creating a single, unified loss term where the weight directly controls the magnitude of the error signal. This is different from Parent 1, which used the weight to blend two different losses, and from Parent 0, which used a different adaptive scaling (`adaptive_gamma`).\n\nWhen `cost(a) < cost(b)`, the `dynamic_margin` is negative. The loss encourages `logp_diff` to be positive and larger than this margin to make the `preference_error` zero, thus correctly preferring `a`.", "pseudocode": "1. Compute the log-probability difference: logp_diff = logp_a - logp_b.\n2. Compute the signed, batch-relative rank gap: cost_gap_signed = rank_gap(cost_a, cost_b).\n3. Standardize the rank gap across the batch: cost_gap_zscored = zscore(cost_gap_signed).\n4. Create a bounded dynamic margin from the standardized rank gap (inherited from Parent 0): dynamic_margin = beta * tanh(cost_gap_zscored).\n5. Compute the preference error using a smooth hinge-like term (inherited from Parent 0): preference_error = softplus(dynamic_margin - logp_diff).\n6. Create an adaptive weight based on the margin's magnitude (inherited from Parent 1): adaptive_alpha = sigmoid(gamma * abs(dynamic_margin)).\n7. Couple the adaptive weight with the preference error in a stable logarithmic form (new coupling idea): loss = log(1.0 + adaptive_alpha * preference_error).", "hyperparams": {"beta": 1.0, "gamma": 1.0}, "operators_used": ["rank_gap", "zscore", "tanh", "softplus", "sigmoid", "log"], "implementation_hint": {"expects": ["cost_a", "cost_b", "logp_a", "logp_b"], "returns": "scalar"}}, "fitness": {"hf_like_score": 8.467353820800781, "validation_objective": 8.467353820800781, "generalization_penalty": 0.0, "generalization_objectives": {"20": 3.994964599609375}, "train_score_mean": 10.491167249679565, "train_loss_mean": 0.5992400628328324, "pair_count": 12902386, "config": {"hf": {"problem": "tsp", "hf_steps": 100, "train_problem_size": 100, "valid_problem_sizes": [20], "train_batch_size": 64, "pomo_size": 64, "learning_rate": 0.0003, "weight_decay": 1e-06, "alpha": 0.05, "device": "cuda", "seed": 1234, "num_validation_episodes": 128, "validation_batch_size": 64, "generalization_penalty_weight": 1.0, "pool_version": "v0"}, "free_loss": {"f1_steps": 100, "f2_steps": 100, "f3_enabled": false}}, "loss_ir": {"name": "Rank-Calibrated Dynamic Hinge Loss", "intuition": "This loss function creates a hybrid that uses a dynamic, rank-based margin for its primary hinge structure, and then calibrates the penalty using an adaptive, sigmoid-based weight. The goal is to combine a robust margin definition with a smooth, adaptive penalty.\n\nFrom Parent 0 ('Adaptive Hinge-Sigmoid Hybrid Loss'), we inherit the core concept of a `dynamic_margin` derived from the standardized rank gap (`beta * tanh(zscore(rank_gap(...)))`). This ensures the required preference margin adapts to the relative cost difference within the batch. We also inherit the use of `softplus` to create a smooth, non-negative hinge error term: `softplus(dynamic_margin - logp_diff)`.\n\nFrom Parent 1 ('Adaptively Weighted Hinge-Sigmoid Loss with Rank-Gap Scaling'), we inherit the idea of an `adaptive_alpha` weight. This weight, calculated using `sigmoid(gamma * abs(dynamic_margin))`, modulates the loss based on the confidence of the preference signal (i.e., the magnitude of the margin). It makes the loss more forgiving for ambiguous pairs (small margin) and stricter for clear-cut pairs (large margin).\n\nAs a new coupling idea, we use this `adaptive_alpha` weight to directly scale the hinge error *before* it enters the final logarithmic stabilization term. The structure becomes `log(1.0 + adaptive_alpha * preference_error)`. This elegantly combines the adaptive weighting from Parent 1 with the stable hinge error from Parent 0, creating a single, unified loss term where the weight directly controls the magnitude of the error signal. This is different from Parent 1, which used the weight to blend two different losses, and from Parent 0, which used a different adaptive scaling (`adaptive_gamma`).\n\nWhen `cost(a) < cost(b)`, the `dynamic_margin` is negative. The loss encourages `logp_diff` to be positive and larger than this margin to make the `preference_error` zero, thus correctly preferring `a`.", "hyperparams": {"beta": 1.0, "gamma": 1.0}, "operators_used": ["rank_gap", "zscore", "tanh", "softplus", "sigmoid", "log"]}}, "better_than_baseline": false}
{"generation": 4, "index": 1, "ir": {"name": "Adaptive Rank-Gap Attenuated Hinge Loss", "intuition": "This loss function creates a hybrid that modulates a core hinge loss structure with two adaptive mechanisms, one inherited from each parent, and introduces a new coupling for stability and dynamic behavior.\n\nFrom Parent 0 ('Adaptive Hinge-Sigmoid Hybrid Loss'), we inherit the core structure of a smooth hinge loss based on a dynamic margin: `softplus(dynamic_margin - logp_diff)`. This provides a baseline preference error that is zero when the model's preference `logp_diff` sufficiently exceeds the `dynamic_margin` derived from the rank-gap.\n\nFrom Parent 1 ('Adaptively Weighted Hinge-Sigmoid Loss with Rank-Gap Scaling'), we inherit the idea of an adaptive weight, `adaptive_alpha = sigmoid(gamma * abs(dynamic_margin))`. This weight scales the loss based on the confidence of the preference signal (i.e., the magnitude of the margin), making the loss more influential for clear-cut cases and less so for ambiguous ones.\n\nWe introduce two new coupling ideas. First, instead of using the adaptive weight to mix two losses or to scale the final loss, we use it to directly *attenuate* the `logp_diff` term *inside* the hinge loss. The core term becomes `softplus(dynamic_margin - adaptive_alpha * logp_diff)`. This means that for high-confidence pairs (large `adaptive_alpha`), the model's log-probability difference is fully considered. For low-confidence pairs (small `adaptive_alpha`), the effective `logp_diff` is down-weighted, making it easier for the model to satisfy the margin condition and avoid large penalties on ambiguous pairs. Second, we introduce a `rank_gap_scale` factor, similar to Parent 1, but apply it *inside* the final `log` wrapper, `log(1.0 + rank_gap_scale * preference_error)`. This ensures that pairs with larger cost differences contribute more to the error signal, but in a logarithmically-dampened way, preventing instability.\n\nWhen `cost(a) < cost(b)`, `dynamic_margin` is negative. The loss encourages `logp_diff` to be positive. The `adaptive_alpha` will be close to 1 if the rank gap is large, requiring the full `logp_diff` to overcome the margin. If the rank gap is small, `adaptive_alpha` is smaller, relaxing the requirement on `logp_diff`.", "pseudocode": "1. Compute the log-probability difference: logp_diff = logp_a - logp_b.\n2. Compute the signed, batch-relative rank gap: cost_gap_signed = rank_gap(cost_a, cost_b).\n3. Standardize the rank gap across the batch: cost_gap_zscored = zscore(cost_gap_signed).\n4. Create a bounded dynamic margin from the standardized rank gap (inherited from Parent 0): dynamic_margin = beta * tanh(cost_gap_zscored).\n5. Create an adaptive weight based on the margin's magnitude (inherited from Parent 1): adaptive_alpha = sigmoid(gamma * abs(dynamic_margin)).\n6. Compute the preference error by coupling the adaptive weight directly with the logp_diff inside a smooth hinge structure (new coupling idea): preference_error = softplus(dynamic_margin - adaptive_alpha * logp_diff).\n7. Create a rank gap scaling factor (inspired by Parent 1): rank_gap_scale = 1.0 + relu(abs(cost_gap_signed)).\n8. Compute the final loss by combining the preference error and the scaling factor within a stable logarithmic form (new coupling idea): loss = log(1.0 + rank_gap_scale * preference_error).", "hyperparams": {"beta": 1.0, "gamma": 2.0}, "operators_used": ["rank_gap", "zscore", "tanh", "sigmoid", "softplus", "relu", "log"], "implementation_hint": {"expects": ["cost_a", "cost_b", "logp_a", "logp_b"], "returns": "scalar"}}, "fitness": {"hf_like_score": 8.501826286315918, "validation_objective": 8.501826286315918, "generalization_penalty": 0.0, "generalization_objectives": {"20": 4.000634431838989}, "train_score_mean": 10.53780873298645, "train_loss_mean": 0.600624104142189, "pair_count": 12902389, "config": {"hf": {"problem": "tsp", "hf_steps": 100, "train_problem_size": 100, "valid_problem_sizes": [20], "train_batch_size": 64, "pomo_size": 64, "learning_rate": 0.0003, "weight_decay": 1e-06, "alpha": 0.05, "device": "cuda", "seed": 1234, "num_validation_episodes": 128, "validation_batch_size": 64, "generalization_penalty_weight": 1.0, "pool_version": "v0"}, "free_loss": {"f1_steps": 100, "f2_steps": 100, "f3_enabled": false}}, "loss_ir": {"name": "Adaptive Rank-Gap Attenuated Hinge Loss", "intuition": "This loss function creates a hybrid that modulates a core hinge loss structure with two adaptive mechanisms, one inherited from each parent, and introduces a new coupling for stability and dynamic behavior.\n\nFrom Parent 0 ('Adaptive Hinge-Sigmoid Hybrid Loss'), we inherit the core structure of a smooth hinge loss based on a dynamic margin: `softplus(dynamic_margin - logp_diff)`. This provides a baseline preference error that is zero when the model's preference `logp_diff` sufficiently exceeds the `dynamic_margin` derived from the rank-gap.\n\nFrom Parent 1 ('Adaptively Weighted Hinge-Sigmoid Loss with Rank-Gap Scaling'), we inherit the idea of an adaptive weight, `adaptive_alpha = sigmoid(gamma * abs(dynamic_margin))`. This weight scales the loss based on the confidence of the preference signal (i.e., the magnitude of the margin), making the loss more influential for clear-cut cases and less so for ambiguous ones.\n\nWe introduce two new coupling ideas. First, instead of using the adaptive weight to mix two losses or to scale the final loss, we use it to directly *attenuate* the `logp_diff` term *inside* the hinge loss. The core term becomes `softplus(dynamic_margin - adaptive_alpha * logp_diff)`. This means that for high-confidence pairs (large `adaptive_alpha`), the model's log-probability difference is fully considered. For low-confidence pairs (small `adaptive_alpha`), the effective `logp_diff` is down-weighted, making it easier for the model to satisfy the margin condition and avoid large penalties on ambiguous pairs. Second, we introduce a `rank_gap_scale` factor, similar to Parent 1, but apply it *inside* the final `log` wrapper, `log(1.0 + rank_gap_scale * preference_error)`. This ensures that pairs with larger cost differences contribute more to the error signal, but in a logarithmically-dampened way, preventing instability.\n\nWhen `cost(a) < cost(b)`, `dynamic_margin` is negative. The loss encourages `logp_diff` to be positive. The `adaptive_alpha` will be close to 1 if the rank gap is large, requiring the full `logp_diff` to overcome the margin. If the rank gap is small, `adaptive_alpha` is smaller, relaxing the requirement on `logp_diff`.", "hyperparams": {"beta": 1.0, "gamma": 2.0}, "operators_used": ["rank_gap", "zscore", "tanh", "sigmoid", "softplus", "relu", "log"]}}, "better_than_baseline": false}
{"generation": 4, "index": 2, "ir": {"name": "Rank-Calibrated Log-Sigmoid with Dynamic Hinge Regularization", "intuition": "This loss function combines the probabilistic sigmoid framework with a dynamic, rank-based hinge regularization term, introducing a new coupling mechanism that adaptively balances these two components.\n\nFrom Parent 0 ('Adaptive Hinge-Sigmoid Hybrid Loss'), we inherit the core idea of a dynamic margin (`dynamic_margin = beta * tanh(zscore(rank_gap(...)))`). This margin adapts to the relative cost difference within a batch, providing a more nuanced target than a fixed margin.\n\nFrom Parent 1 ('Adaptively Weighted Hinge-Sigmoid Loss'), we inherit the use of a `sigmoid` function to create an adaptive weight (`adaptive_alpha`). This weight smoothly transitions between different loss behaviors based on the magnitude of the preference signal.\n\nWe introduce two new coupling ideas. First, instead of using the dynamic margin *inside* the main loss term, we use it as a separate regularization component. The primary loss is a standard log-sigmoid term (`logsigmoid(-logp_diff)`), which is a common and stable preference loss. The regularization term is a smooth hinge loss (`softplus(dynamic_margin - logp_diff)`) that penalizes the model only if its preference `logp_diff` does not exceed the `dynamic_margin`.\n\nSecond, the `adaptive_alpha` (inspired by Parent 1) is used to dynamically balance these two terms. It is computed from the magnitude of the `dynamic_margin` itself: `adaptive_alpha = sigmoid(gamma * abs(dynamic_margin))`. When the rank gap is small (margin is small, `adaptive_alpha` is near 0.5), the loss is a balanced mix of the probabilistic sigmoid term and the hinge regularization. When the rank gap is large (margin is large, `adaptive_alpha` is near 1.0), the loss puts more weight on the hinge regularization, enforcing a stricter margin requirement. This creates a hybrid loss that is probabilistic at its core but is regularized by a rank-aware margin, with the strength of the regularization adapting to the clarity of the preference signal.\n\nWhen `cost(a) < cost(b)`, `logp_diff` is encouraged to be positive by the `logsigmoid` term, and the `dynamic_margin` becomes negative, pushing the hinge term towards zero.", "pseudocode": "1. Compute the log-probability difference: logp_diff = logp_a - logp_b.\n2. Compute the signed, batch-relative rank gap: cost_gap_signed = rank_gap(cost_a, cost_b).\n3. Standardize the rank gap across the batch: cost_gap_zscored = zscore(cost_gap_signed).\n4. Create a bounded dynamic margin from the standardized rank gap (inherited from Parent 0): dynamic_margin = beta * tanh(cost_gap_zscored).\n5. Compute the primary probabilistic loss term: loss_sigmoid = logsigmoid(-logp_diff).\n6. Compute the hinge-based regularization term (structure from Parent 0): loss_hinge = softplus(dynamic_margin - logp_diff).\n7. Create an adaptive weight based on the margin's magnitude (inspired by Parent 1, new coupling): adaptive_alpha = sigmoid(gamma * abs(dynamic_margin)).\n8. Combine the two loss components using the adaptive weight (new coupling idea): loss = (1.0 - adaptive_alpha) * loss_sigmoid + adaptive_alpha * loss_hinge.", "hyperparams": {"beta": 1.5, "gamma": 1.0}, "operators_used": ["rank_gap", "zscore", "tanh", "logsigmoid", "softplus", "sigmoid"], "implementation_hint": {"expects": ["cost_a", "cost_b", "logp_a", "logp_b"], "returns": "scalar"}}, "fitness": {"hf_like_score": 8.478754997253418, "validation_objective": 8.478754997253418, "generalization_penalty": 0.0, "generalization_objectives": {"20": 4.007348418235779}, "train_score_mean": 10.488528871536255, "train_loss_mean": 0.5996091413497925, "pair_count": 12902391, "config": {"hf": {"problem": "tsp", "hf_steps": 100, "train_problem_size": 100, "valid_problem_sizes": [20], "train_batch_size": 64, "pomo_size": 64, "learning_rate": 0.0003, "weight_decay": 1e-06, "alpha": 0.05, "device": "cuda", "seed": 1234, "num_validation_episodes": 128, "validation_batch_size": 64, "generalization_penalty_weight": 1.0, "pool_version": "v0"}, "free_loss": {"f1_steps": 100, "f2_steps": 100, "f3_enabled": false}}, "loss_ir": {"name": "Rank-Calibrated Log-Sigmoid with Dynamic Hinge Regularization", "intuition": "This loss function combines the probabilistic sigmoid framework with a dynamic, rank-based hinge regularization term, introducing a new coupling mechanism that adaptively balances these two components.\n\nFrom Parent 0 ('Adaptive Hinge-Sigmoid Hybrid Loss'), we inherit the core idea of a dynamic margin (`dynamic_margin = beta * tanh(zscore(rank_gap(...)))`). This margin adapts to the relative cost difference within a batch, providing a more nuanced target than a fixed margin.\n\nFrom Parent 1 ('Adaptively Weighted Hinge-Sigmoid Loss'), we inherit the use of a `sigmoid` function to create an adaptive weight (`adaptive_alpha`). This weight smoothly transitions between different loss behaviors based on the magnitude of the preference signal.\n\nWe introduce two new coupling ideas. First, instead of using the dynamic margin *inside* the main loss term, we use it as a separate regularization component. The primary loss is a standard log-sigmoid term (`logsigmoid(-logp_diff)`), which is a common and stable preference loss. The regularization term is a smooth hinge loss (`softplus(dynamic_margin - logp_diff)`) that penalizes the model only if its preference `logp_diff` does not exceed the `dynamic_margin`.\n\nSecond, the `adaptive_alpha` (inspired by Parent 1) is used to dynamically balance these two terms. It is computed from the magnitude of the `dynamic_margin` itself: `adaptive_alpha = sigmoid(gamma * abs(dynamic_margin))`. When the rank gap is small (margin is small, `adaptive_alpha` is near 0.5), the loss is a balanced mix of the probabilistic sigmoid term and the hinge regularization. When the rank gap is large (margin is large, `adaptive_alpha` is near 1.0), the loss puts more weight on the hinge regularization, enforcing a stricter margin requirement. This creates a hybrid loss that is probabilistic at its core but is regularized by a rank-aware margin, with the strength of the regularization adapting to the clarity of the preference signal.\n\nWhen `cost(a) < cost(b)`, `logp_diff` is encouraged to be positive by the `logsigmoid` term, and the `dynamic_margin` becomes negative, pushing the hinge term towards zero.", "hyperparams": {"beta": 1.5, "gamma": 1.0}, "operators_used": ["rank_gap", "zscore", "tanh", "logsigmoid", "softplus", "sigmoid"]}}, "better_than_baseline": false}
{"generation": 4, "index": 3, "ir": {"name": "Rank-Modulated Sigmoid-Hinge Loss", "intuition": "This loss function creates a hybrid that modulates a core hinge-loss structure with a sigmoid-based weighting scheme, introducing a novel coupling mechanism that directly incorporates the rank gap into the loss calculation.\n\nFrom Parent 0 ('Adaptive Hinge-Sigmoid Hybrid Loss'), we inherit the core structure of a preference error calculated as `softplus(margin - logp_diff)`. This ensures the loss is zero when the model's preference `logp_diff` correctly exceeds a dynamic margin, providing a stable, non-negative error signal. We also inherit the use of `tanh` on a z-scored rank gap to create this bounded `dynamic_margin`.\n\nFrom Parent 1 ('Adaptively Weighted Hinge-Sigmoid Loss with Rank-Gap Scaling'), we inherit the idea of using an adaptive weight, calculated via a `sigmoid` function, to modulate the loss. This weight makes the loss more or less sensitive depending on the confidence of the preference signal.\n\nWe introduce two new coupling ideas. First, instead of using the margin's magnitude to create the adaptive weight (as in Parent 1), we use the standardized rank gap itself. The weight is `sigmoid(gamma * cost_gap_zscored)`. This directly ties the loss's importance to the relative ranking of the cost pair within the batch. For a correctly ordered pair `cost(a) < cost(b)`, `cost_gap_zscored` is negative, making the weight less than 0.5, which down-weights the hinge error. For an incorrectly ordered pair, the weight is greater than 0.5, amplifying the penalty. Second, we combine the adaptive weight and the hinge error in a novel multiplicative structure: `loss = sigmoid_weight * preference_error`. This is simpler than the logarithmic forms in the parents and creates a direct, weighted penalty.\n\nWhen `cost(a) < cost(b)`, `dynamic_margin` is negative and `sigmoid_weight` is less than 0.5. The loss encourages `logp_diff` to be positive and greater than the margin to drive `preference_error` to zero, correctly preferring `a`.", "pseudocode": "1. Compute the log-probability difference: logp_diff = logp_a - logp_b.\n2. Compute the signed, batch-relative rank gap: cost_gap_signed = rank_gap(cost_a, cost_b).\n3. Standardize the rank gap across the batch: cost_gap_zscored = zscore(cost_gap_signed).\n4. Create a bounded dynamic margin from the standardized rank gap (inherited from Parent 0): dynamic_margin = beta * tanh(cost_gap_zscored).\n5. Compute the preference error using a smooth hinge-like term (inherited from Parent 0): preference_error = softplus(dynamic_margin - logp_diff).\n6. Create an adaptive weight directly from the standardized rank gap (new coupling idea, inspired by Parent 1's weighting): sigmoid_weight = sigmoid(gamma * cost_gap_zscored).\n7. Compute the final loss by multiplicatively coupling the adaptive weight and the preference error (new coupling idea): loss = sigmoid_weight * preference_error.", "hyperparams": {"beta": 1.0, "gamma": 1.0}, "operators_used": ["rank_gap", "zscore", "tanh", "softplus", "sigmoid"], "implementation_hint": {"expects": ["cost_a", "cost_b", "logp_a", "logp_b"], "returns": "scalar"}}, "fitness": {"hf_like_score": 8.49167537689209, "validation_objective": 8.49167537689209, "generalization_penalty": 0.0, "generalization_objectives": {"20": 4.0018510818481445}, "train_score_mean": 10.477497730255127, "train_loss_mean": 0.5983544272184372, "pair_count": 12902386, "config": {"hf": {"problem": "tsp", "hf_steps": 100, "train_problem_size": 100, "valid_problem_sizes": [20], "train_batch_size": 64, "pomo_size": 64, "learning_rate": 0.0003, "weight_decay": 1e-06, "alpha": 0.05, "device": "cuda", "seed": 1234, "num_validation_episodes": 128, "validation_batch_size": 64, "generalization_penalty_weight": 1.0, "pool_version": "v0"}, "free_loss": {"f1_steps": 100, "f2_steps": 100, "f3_enabled": false}}, "loss_ir": {"name": "Rank-Modulated Sigmoid-Hinge Loss", "intuition": "This loss function creates a hybrid that modulates a core hinge-loss structure with a sigmoid-based weighting scheme, introducing a novel coupling mechanism that directly incorporates the rank gap into the loss calculation.\n\nFrom Parent 0 ('Adaptive Hinge-Sigmoid Hybrid Loss'), we inherit the core structure of a preference error calculated as `softplus(margin - logp_diff)`. This ensures the loss is zero when the model's preference `logp_diff` correctly exceeds a dynamic margin, providing a stable, non-negative error signal. We also inherit the use of `tanh` on a z-scored rank gap to create this bounded `dynamic_margin`.\n\nFrom Parent 1 ('Adaptively Weighted Hinge-Sigmoid Loss with Rank-Gap Scaling'), we inherit the idea of using an adaptive weight, calculated via a `sigmoid` function, to modulate the loss. This weight makes the loss more or less sensitive depending on the confidence of the preference signal.\n\nWe introduce two new coupling ideas. First, instead of using the margin's magnitude to create the adaptive weight (as in Parent 1), we use the standardized rank gap itself. The weight is `sigmoid(gamma * cost_gap_zscored)`. This directly ties the loss's importance to the relative ranking of the cost pair within the batch. For a correctly ordered pair `cost(a) < cost(b)`, `cost_gap_zscored` is negative, making the weight less than 0.5, which down-weights the hinge error. For an incorrectly ordered pair, the weight is greater than 0.5, amplifying the penalty. Second, we combine the adaptive weight and the hinge error in a novel multiplicative structure: `loss = sigmoid_weight * preference_error`. This is simpler than the logarithmic forms in the parents and creates a direct, weighted penalty.\n\nWhen `cost(a) < cost(b)`, `dynamic_margin` is negative and `sigmoid_weight` is less than 0.5. The loss encourages `logp_diff` to be positive and greater than the margin to drive `preference_error` to zero, correctly preferring `a`.", "hyperparams": {"beta": 1.0, "gamma": 1.0}, "operators_used": ["rank_gap", "zscore", "tanh", "softplus", "sigmoid"]}}, "better_than_baseline": false}
{"generation": 4, "index": 4, "ir": {"name": "Rank-Modulated Adaptive Sigmoid Loss", "intuition": "This loss function creates a hybrid that combines a dynamic margin with an adaptive temperature, modulated by the raw rank gap. The goal is to create a loss that is sensitive to both the relative ranking of costs within a batch and the absolute magnitude of their difference.\n\nFrom Parent 0 ('Adaptive Hinge-Sigmoid Hybrid Loss'), we inherit the core idea of a dynamic margin (`dynamic_margin`) based on a standardized rank gap. This `dynamic_margin = beta * tanh(zscore(rank_gap(...)))` term allows the loss target to adapt based on how separated two costs are relative to other pairs in the batch.\n\nFrom Parent 1 ('Adaptively Weighted Hinge-Sigmoid Loss with Rank-Gap Scaling'), we inherit the concept of an adaptive scaling factor, but we re-purpose it. Instead of a sigmoid-based weight, we use a temperature-like term, `adaptive_gamma`, which is a function of the dynamic margin's magnitude. This makes the loss function's slope steeper for pairs with a clear rank separation.\n\nAs a new coupling idea, we modulate the final loss with a `rank_gap_scale`. This scale is derived from the *un-normalized* signed rank gap using `softplus`. This ensures that pairs with a larger absolute cost difference contribute more to the total loss, adding a signal that is lost during z-score normalization. The final loss is a product of this scale and a sigmoid-like term: `rank_gap_scale * logsigmoid(-adaptive_gamma * (logp_diff - dynamic_margin))`. This structure combines the rank-relative margin from Parent 0, an adaptive temperature inspired by Parent 1, and a new raw-gap scaling mechanism.\n\nWhen `cost(a) < cost(b)`, `dynamic_margin` is negative. The loss encourages `logp_diff` to be positive, pushing the argument of `logsigmoid` towards negative infinity and the loss towards zero, thus correctly preferring `a`.", "pseudocode": "1. Compute the log-probability difference: logp_diff = logp_a - logp_b.\n2. Compute the signed, batch-relative rank gap: cost_gap_signed = rank_gap(cost_a, cost_b).\n3. Standardize the rank gap across the batch: cost_gap_zscored = zscore(cost_gap_signed).\n4. Create a bounded dynamic margin from the standardized rank gap (inherited from Parent 0): dynamic_margin = beta * tanh(cost_gap_zscored).\n5. Create an adaptive temperature based on the margin's magnitude (inspired by Parent 1): adaptive_gamma = 1.0 + gamma * softplus(abs(dynamic_margin)).\n6. Create a rank gap scaling factor from the un-normalized gap (new coupling idea): rank_gap_scale = 1.0 + softplus(abs(cost_gap_signed)).\n7. Compute the core preference loss using a sigmoid structure, incorporating the dynamic margin and adaptive temperature: preference_loss = logsigmoid(-adaptive_gamma * (logp_diff - dynamic_margin)).\n8. Apply the rank gap scaling to the final loss (new coupling idea): loss = rank_gap_scale * preference_loss.", "hyperparams": {"beta": 1.5, "gamma": 1.0}, "operators_used": ["rank_gap", "zscore", "tanh", "softplus", "logsigmoid"], "implementation_hint": {"expects": ["cost_a", "cost_b", "logp_a", "logp_b"], "returns": "scalar"}}, "fitness": {"hf_like_score": 8.474315166473389, "validation_objective": 8.474315166473389, "generalization_penalty": 0.0, "generalization_objectives": {"20": 4.053882122039795}, "train_score_mean": 10.471376028060913, "train_loss_mean": 0.5989128583669663, "pair_count": 12902389, "config": {"hf": {"problem": "tsp", "hf_steps": 100, "train_problem_size": 100, "valid_problem_sizes": [20], "train_batch_size": 64, "pomo_size": 64, "learning_rate": 0.0003, "weight_decay": 1e-06, "alpha": 0.05, "device": "cuda", "seed": 1234, "num_validation_episodes": 128, "validation_batch_size": 64, "generalization_penalty_weight": 1.0, "pool_version": "v0"}, "free_loss": {"f1_steps": 100, "f2_steps": 100, "f3_enabled": false}}, "loss_ir": {"name": "Rank-Modulated Adaptive Sigmoid Loss", "intuition": "This loss function creates a hybrid that combines a dynamic margin with an adaptive temperature, modulated by the raw rank gap. The goal is to create a loss that is sensitive to both the relative ranking of costs within a batch and the absolute magnitude of their difference.\n\nFrom Parent 0 ('Adaptive Hinge-Sigmoid Hybrid Loss'), we inherit the core idea of a dynamic margin (`dynamic_margin`) based on a standardized rank gap. This `dynamic_margin = beta * tanh(zscore(rank_gap(...)))` term allows the loss target to adapt based on how separated two costs are relative to other pairs in the batch.\n\nFrom Parent 1 ('Adaptively Weighted Hinge-Sigmoid Loss with Rank-Gap Scaling'), we inherit the concept of an adaptive scaling factor, but we re-purpose it. Instead of a sigmoid-based weight, we use a temperature-like term, `adaptive_gamma`, which is a function of the dynamic margin's magnitude. This makes the loss function's slope steeper for pairs with a clear rank separation.\n\nAs a new coupling idea, we modulate the final loss with a `rank_gap_scale`. This scale is derived from the *un-normalized* signed rank gap using `softplus`. This ensures that pairs with a larger absolute cost difference contribute more to the total loss, adding a signal that is lost during z-score normalization. The final loss is a product of this scale and a sigmoid-like term: `rank_gap_scale * logsigmoid(-adaptive_gamma * (logp_diff - dynamic_margin))`. This structure combines the rank-relative margin from Parent 0, an adaptive temperature inspired by Parent 1, and a new raw-gap scaling mechanism.\n\nWhen `cost(a) < cost(b)`, `dynamic_margin` is negative. The loss encourages `logp_diff` to be positive, pushing the argument of `logsigmoid` towards negative infinity and the loss towards zero, thus correctly preferring `a`.", "hyperparams": {"beta": 1.5, "gamma": 1.0}, "operators_used": ["rank_gap", "zscore", "tanh", "softplus", "logsigmoid"]}}, "better_than_baseline": false}
{"generation": 4, "index": 5, "ir": {"name": "Adaptive Rank-Modulated Hinge Loss", "intuition": "This loss function creates a hybrid that modulates a core hinge-loss structure with adaptive scaling factors derived from both the rank gap and the cost difference itself. The goal is to create a loss that is both sensitive to relative ranking within a batch and calibrated by the absolute magnitude of the cost difference.\n\nFrom Parent 0 (Adaptive Hinge-Sigmoid Hybrid Loss), we inherit the core structure of a smooth, one-sided hinge loss: `softplus(margin - logp_diff)`. This ensures that the loss is zero when the model's preference `logp_diff` correctly exceeds a target margin, providing a clear optimization target. We also inherit the use of a dynamic margin based on the standardized rank gap: `dynamic_margin = beta * tanh(zscore(rank_gap(...)))`, which makes the preference target sensitive to the relative ordering of costs within the batch.\n\nFrom Parent 1 (Adaptively Weighted Hinge-Sigmoid Loss), we inherit the idea of using the `sigmoid` function to create an adaptive weight. However, instead of basing it on the margin, we base it on the raw cost difference to make the loss more sensitive to pairs with large absolute cost differences.\n\nAs a new coupling idea, we introduce a 'rank modulation factor'. This factor, `rank_modulation = softplus(abs(cost_gap_signed))`, is computed from the *un-normalized* rank gap. We use this factor to directly scale the `dynamic_margin` itself. This means that pairs with a larger rank separation in the batch will have a more demanding margin, pushing the model to be more confident in its preferences for clear-cut rankings. The final loss is a product of the adaptive weight (from Parent 1's idea) and the rank-modulated hinge loss (from Parent 0's structure, with our new coupling). This creates a loss that is high when a large cost difference is not met with a sufficiently large preference margin, especially if that pair also has a high rank gap in the batch.\n\nWhen `cost(a) < cost(b)`, `cost_gap_signed` is negative, making `dynamic_margin` negative. The loss encourages `logp_diff` to be positive and larger than this margin to drive the `softplus` term to zero, correctly preferring `a`.", "pseudocode": "1. Compute the log-probability difference: logp_diff = logp_a - logp_b.\n2. Compute the signed, batch-relative rank gap: cost_gap_signed = rank_gap(cost_a, cost_b).\n3. Standardize the rank gap across the batch: cost_gap_zscored = zscore(cost_gap_signed).\n4. Create a base dynamic margin from the standardized rank gap (inherited from Parent 0): base_margin = beta * tanh(cost_gap_zscored).\n5. Create a rank modulation factor from the un-normalized rank gap (new coupling idea): rank_modulation = softplus(abs(cost_gap_signed)).\n6. Couple the base margin and rank modulation to create a new, scaled margin: modulated_margin = rank_modulation * base_margin.\n7. Compute the core hinge error using the modulated margin (structure from Parent 0): hinge_error = softplus(modulated_margin - logp_diff).\n8. Compute an adaptive weight based on the absolute cost difference (idea inherited from Parent 1): adaptive_weight = sigmoid(gamma * abs(cost_a - cost_b)).\n9. Compute the final loss by combining the adaptive weight and the hinge error (new coupling idea): loss = adaptive_weight * hinge_error.", "hyperparams": {"beta": 1.0, "gamma": 0.1}, "operators_used": ["rank_gap", "zscore", "tanh", "softplus", "sigmoid"], "implementation_hint": {"expects": ["cost_a", "cost_b", "logp_a", "logp_b"], "returns": "scalar"}}, "fitness": {"hf_like_score": 8.448457717895508, "validation_objective": 8.448457717895508, "generalization_penalty": 0.0, "generalization_objectives": {"20": 4.023670434951782}, "train_score_mean": 10.46930417060852, "train_loss_mean": 0.6024290585517883, "pair_count": 12902389, "config": {"hf": {"problem": "tsp", "hf_steps": 100, "train_problem_size": 100, "valid_problem_sizes": [20], "train_batch_size": 64, "pomo_size": 64, "learning_rate": 0.0003, "weight_decay": 1e-06, "alpha": 0.05, "device": "cuda", "seed": 1234, "num_validation_episodes": 128, "validation_batch_size": 64, "generalization_penalty_weight": 1.0, "pool_version": "v0"}, "free_loss": {"f1_steps": 100, "f2_steps": 100, "f3_enabled": false}}, "loss_ir": {"name": "Adaptive Rank-Modulated Hinge Loss", "intuition": "This loss function creates a hybrid that modulates a core hinge-loss structure with adaptive scaling factors derived from both the rank gap and the cost difference itself. The goal is to create a loss that is both sensitive to relative ranking within a batch and calibrated by the absolute magnitude of the cost difference.\n\nFrom Parent 0 (Adaptive Hinge-Sigmoid Hybrid Loss), we inherit the core structure of a smooth, one-sided hinge loss: `softplus(margin - logp_diff)`. This ensures that the loss is zero when the model's preference `logp_diff` correctly exceeds a target margin, providing a clear optimization target. We also inherit the use of a dynamic margin based on the standardized rank gap: `dynamic_margin = beta * tanh(zscore(rank_gap(...)))`, which makes the preference target sensitive to the relative ordering of costs within the batch.\n\nFrom Parent 1 (Adaptively Weighted Hinge-Sigmoid Loss), we inherit the idea of using the `sigmoid` function to create an adaptive weight. However, instead of basing it on the margin, we base it on the raw cost difference to make the loss more sensitive to pairs with large absolute cost differences.\n\nAs a new coupling idea, we introduce a 'rank modulation factor'. This factor, `rank_modulation = softplus(abs(cost_gap_signed))`, is computed from the *un-normalized* rank gap. We use this factor to directly scale the `dynamic_margin` itself. This means that pairs with a larger rank separation in the batch will have a more demanding margin, pushing the model to be more confident in its preferences for clear-cut rankings. The final loss is a product of the adaptive weight (from Parent 1's idea) and the rank-modulated hinge loss (from Parent 0's structure, with our new coupling). This creates a loss that is high when a large cost difference is not met with a sufficiently large preference margin, especially if that pair also has a high rank gap in the batch.\n\nWhen `cost(a) < cost(b)`, `cost_gap_signed` is negative, making `dynamic_margin` negative. The loss encourages `logp_diff` to be positive and larger than this margin to drive the `softplus` term to zero, correctly preferring `a`.", "hyperparams": {"beta": 1.0, "gamma": 0.1}, "operators_used": ["rank_gap", "zscore", "tanh", "softplus", "sigmoid"]}}, "better_than_baseline": false}
{"generation": 4, "index": 6, "ir": {"name": "Adaptive Temperature Hinge Loss with Rank-Gap Scaling", "intuition": "This loss function creates a hybrid that inherits the core adaptive temperature mechanism from the 'Adaptive Hinge-Sigmoid Hybrid Loss' (Parent 0) and the rank-gap based loss scaling from the 'Adaptively Weighted Hinge-Sigmoid Loss' (Parent 1), while introducing a simplified and stable hinge structure.\n\nFrom Parent 0, we inherit the concept of an `adaptive_gamma` temperature, which dynamically scales the loss based on the magnitude of a preference margin. This makes the loss more sensitive to pairs with a clear preference signal (large margin) and less sensitive to ambiguous pairs (small margin).\n\nFrom Parent 1, we inherit the idea of a `rank_gap_scale`. This factor multiplies the final loss value, ensuring that training examples with a larger absolute difference in cost ranks contribute more significantly to the total loss. This directly prioritizes learning on the most mis-ranked pairs.\n\nWe combine these ideas in a clean and direct way. We use a standard hinge loss structure, `softplus(margin - logp_diff)`, which is a stable and common pattern. The margin itself (`dynamic_margin`) is a bounded value derived from the standardized rank gap, an idea present in both parents. The `adaptive_gamma` (from Parent 0) is then used to scale this hinge loss term *before* it is finally weighted by the `rank_gap_scale` (from Parent 1).\n\nAs a new coupling idea, we simplify the final loss structure. Instead of embedding the error term inside a `log(1+x)` or using a sigmoid-based weight, we apply the adaptive temperature and rank-gap scaling directly to the `softplus` output. The structure `rank_gap_scale * adaptive_gamma * softplus(margin - logp_diff)` is a direct and interpretable combination of the inherited components, where each part has a clear role: the `softplus` term computes the base preference error, `adaptive_gamma` modulates its intensity based on margin confidence, and `rank_gap_scale` prioritizes it based on rank disparity.\n\nWhen `cost(a) < cost(b)`, the `dynamic_margin` is negative. The loss encourages `logp_diff` to be positive and greater than this margin to drive the `softplus` term to zero, correctly preferring `a`.", "pseudocode": "1. Compute the log-probability difference: logp_diff = logp_a - logp_b.\n2. Compute the signed, batch-relative rank gap: cost_gap_signed = rank_gap(cost_a, cost_b).\n3. Standardize the rank gap across the batch: cost_gap_zscored = zscore(cost_gap_signed).\n4. Create a bounded dynamic margin from the standardized rank gap (inherited from both parents): dynamic_margin = beta * tanh(cost_gap_zscored).\n5. Create an adaptive temperature based on the margin magnitude (inherited from Parent 0): adaptive_gamma = 1.0 + gamma * abs(dynamic_margin).\n6. Create a rank gap scaling factor based on the non-standardized rank gap (inherited from Parent 1): rank_gap_scale = 1.0 + softplus(abs(cost_gap_signed) - 1.0).\n7. Compute the base preference error using a smooth hinge loss: preference_error = softplus(dynamic_margin - logp_diff).\n8. Apply the adaptive temperature and rank-gap scaling to the error (new coupling structure): loss = rank_gap_scale * adaptive_gamma * preference_error.", "hyperparams": {"beta": 1.0, "gamma": 0.5}, "operators_used": ["rank_gap", "zscore", "tanh", "softplus"], "implementation_hint": {"expects": ["cost_a", "cost_b", "logp_a", "logp_b"], "returns": "scalar"}}, "fitness": {"hf_like_score": 8.529486179351807, "validation_objective": 8.529486179351807, "generalization_penalty": 0.0, "generalization_objectives": {"20": 4.012919664382935}, "train_score_mean": 10.495053987503052, "train_loss_mean": 0.5984243935346604, "pair_count": 12902392, "config": {"hf": {"problem": "tsp", "hf_steps": 100, "train_problem_size": 100, "valid_problem_sizes": [20], "train_batch_size": 64, "pomo_size": 64, "learning_rate": 0.0003, "weight_decay": 1e-06, "alpha": 0.05, "device": "cuda", "seed": 1234, "num_validation_episodes": 128, "validation_batch_size": 64, "generalization_penalty_weight": 1.0, "pool_version": "v0"}, "free_loss": {"f1_steps": 100, "f2_steps": 100, "f3_enabled": false}}, "loss_ir": {"name": "Adaptive Temperature Hinge Loss with Rank-Gap Scaling", "intuition": "This loss function creates a hybrid that inherits the core adaptive temperature mechanism from the 'Adaptive Hinge-Sigmoid Hybrid Loss' (Parent 0) and the rank-gap based loss scaling from the 'Adaptively Weighted Hinge-Sigmoid Loss' (Parent 1), while introducing a simplified and stable hinge structure.\n\nFrom Parent 0, we inherit the concept of an `adaptive_gamma` temperature, which dynamically scales the loss based on the magnitude of a preference margin. This makes the loss more sensitive to pairs with a clear preference signal (large margin) and less sensitive to ambiguous pairs (small margin).\n\nFrom Parent 1, we inherit the idea of a `rank_gap_scale`. This factor multiplies the final loss value, ensuring that training examples with a larger absolute difference in cost ranks contribute more significantly to the total loss. This directly prioritizes learning on the most mis-ranked pairs.\n\nWe combine these ideas in a clean and direct way. We use a standard hinge loss structure, `softplus(margin - logp_diff)`, which is a stable and common pattern. The margin itself (`dynamic_margin`) is a bounded value derived from the standardized rank gap, an idea present in both parents. The `adaptive_gamma` (from Parent 0) is then used to scale this hinge loss term *before* it is finally weighted by the `rank_gap_scale` (from Parent 1).\n\nAs a new coupling idea, we simplify the final loss structure. Instead of embedding the error term inside a `log(1+x)` or using a sigmoid-based weight, we apply the adaptive temperature and rank-gap scaling directly to the `softplus` output. The structure `rank_gap_scale * adaptive_gamma * softplus(margin - logp_diff)` is a direct and interpretable combination of the inherited components, where each part has a clear role: the `softplus` term computes the base preference error, `adaptive_gamma` modulates its intensity based on margin confidence, and `rank_gap_scale` prioritizes it based on rank disparity.\n\nWhen `cost(a) < cost(b)`, the `dynamic_margin` is negative. The loss encourages `logp_diff` to be positive and greater than this margin to drive the `softplus` term to zero, correctly preferring `a`.", "hyperparams": {"beta": 1.0, "gamma": 0.5}, "operators_used": ["rank_gap", "zscore", "tanh", "softplus"]}}, "better_than_baseline": false}
{"generation": 4, "index": 7, "ir": {"name": "Rank-Aware Adaptive Hinge-Sigmoid Loss", "intuition": "This loss function creates a hybrid that inherits the core structures of both parents while introducing new coupling mechanisms to enhance stability and adaptivity. The goal is to combine a dynamic margin with an adaptive temperature, but in a new, more integrated way.\n\nFrom Parent 0 (Adaptive Hinge-Sigmoid Hybrid Loss), we inherit the concept of a `dynamic_margin` based on the standardized rank gap (`beta * tanh(zscore(rank_gap(...)))`) and the use of `softplus` to create a smooth, non-negative hinge-like `preference_error` which is zero when the model's preference is correct. This ensures the loss only penalizes incorrect or insufficient preferences.\n\nFrom Parent 1 (Adaptively Weighted Hinge-Sigmoid Loss), we inherit the idea of using `sigmoid(gamma * abs(margin))` as an adaptive scaling factor. However, instead of using it as a weight between two loss terms, we use it as an adaptive temperature that directly scales the `logp_diff` inside the hinge term.\n\nWe introduce two new coupling ideas. First, we create an `adaptive_temperature` from the `sigmoid` term inherited from Parent 1. This temperature directly modulates the model's preference signal: `adaptive_temp * logp_diff`. This coupling makes the loss more sensitive to the model's preference when the required margin is large (i.e., when `abs(dynamic_margin)` is high). Second, we introduce a `stability_offset` using `relu` on the raw rank gap. This offset is added to the final loss, ensuring that pairs with a very large and correct preference gap (large negative rank gap) still contribute a small, non-zero gradient, preventing the model from becoming overly confident and static. This is a stability trick to maintain learning dynamics.\n\nWhen `cost(a) < cost(b)`, `dynamic_margin` is negative. The loss encourages `logp_diff` to be positive, and the `adaptive_temperature` amplifies this requirement, pushing the model to have a stronger preference for `a` to make the `preference_error` zero.", "pseudocode": "1. Compute the log-probability difference: logp_diff = logp_a - logp_b.\n2. Compute the signed, batch-relative rank gap: cost_gap_signed = rank_gap(cost_a, cost_b).\n3. Standardize the rank gap across the batch: cost_gap_zscored = zscore(cost_gap_signed).\n4. Create a bounded dynamic margin from the standardized rank gap (inherited from Parent 0): dynamic_margin = beta * tanh(cost_gap_zscored).\n5. Create an adaptive temperature based on the margin's magnitude (idea from Parent 1): adaptive_temp = 1.0 + sigmoid(gamma * abs(dynamic_margin)).\n6. Compute the preference error using a smooth hinge-like term, now with the adaptive temperature modulating the logp_diff (new coupling idea): preference_error = softplus(dynamic_margin - adaptive_temp * logp_diff).\n7. Compute a stability offset based on the raw rank gap (new stability trick): stability_offset = alpha * relu(-cost_gap_signed).\n8. The final loss is the sum of the preference error and the stability offset: loss = preference_error + stability_offset.", "hyperparams": {"beta": 1.0, "gamma": 1.5, "alpha": 0.01}, "operators_used": ["rank_gap", "zscore", "tanh", "sigmoid", "softplus", "relu"], "implementation_hint": {"expects": ["cost_a", "cost_b", "logp_a", "logp_b"], "returns": "scalar"}}, "fitness": {"hf_like_score": 8.655203819274902, "validation_objective": 8.655203819274902, "generalization_penalty": 0.0, "generalization_objectives": {"20": 4.016823649406433}, "train_score_mean": 10.518208532333373, "train_loss_mean": 0.673859423995018, "pair_count": 12902390, "config": {"hf": {"problem": "tsp", "hf_steps": 100, "train_problem_size": 100, "valid_problem_sizes": [20], "train_batch_size": 64, "pomo_size": 64, "learning_rate": 0.0003, "weight_decay": 1e-06, "alpha": 0.05, "device": "cuda", "seed": 1234, "num_validation_episodes": 128, "validation_batch_size": 64, "generalization_penalty_weight": 1.0, "pool_version": "v0"}, "free_loss": {"f1_steps": 100, "f2_steps": 100, "f3_enabled": false}}, "loss_ir": {"name": "Rank-Aware Adaptive Hinge-Sigmoid Loss", "intuition": "This loss function creates a hybrid that inherits the core structures of both parents while introducing new coupling mechanisms to enhance stability and adaptivity. The goal is to combine a dynamic margin with an adaptive temperature, but in a new, more integrated way.\n\nFrom Parent 0 (Adaptive Hinge-Sigmoid Hybrid Loss), we inherit the concept of a `dynamic_margin` based on the standardized rank gap (`beta * tanh(zscore(rank_gap(...)))`) and the use of `softplus` to create a smooth, non-negative hinge-like `preference_error` which is zero when the model's preference is correct. This ensures the loss only penalizes incorrect or insufficient preferences.\n\nFrom Parent 1 (Adaptively Weighted Hinge-Sigmoid Loss), we inherit the idea of using `sigmoid(gamma * abs(margin))` as an adaptive scaling factor. However, instead of using it as a weight between two loss terms, we use it as an adaptive temperature that directly scales the `logp_diff` inside the hinge term.\n\nWe introduce two new coupling ideas. First, we create an `adaptive_temperature` from the `sigmoid` term inherited from Parent 1. This temperature directly modulates the model's preference signal: `adaptive_temp * logp_diff`. This coupling makes the loss more sensitive to the model's preference when the required margin is large (i.e., when `abs(dynamic_margin)` is high). Second, we introduce a `stability_offset` using `relu` on the raw rank gap. This offset is added to the final loss, ensuring that pairs with a very large and correct preference gap (large negative rank gap) still contribute a small, non-zero gradient, preventing the model from becoming overly confident and static. This is a stability trick to maintain learning dynamics.\n\nWhen `cost(a) < cost(b)`, `dynamic_margin` is negative. The loss encourages `logp_diff` to be positive, and the `adaptive_temperature` amplifies this requirement, pushing the model to have a stronger preference for `a` to make the `preference_error` zero.", "hyperparams": {"beta": 1.0, "gamma": 1.5, "alpha": 0.01}, "operators_used": ["rank_gap", "zscore", "tanh", "sigmoid", "softplus", "relu"]}}, "better_than_baseline": false}
{"generation": 4, "index": 8, "ir": {"name": "Rank-Scaled Adaptive Hinge-Sigmoid Loss", "intuition": "This child loss function synthesizes ideas from its parents to create a robust, adaptive hinge-style loss. It aims to combine the dynamic margin concept with an adaptive scaling mechanism, while introducing a new coupling that links the preference error directly to the rank gap.\n\nFrom Parent 0 ('Adaptive Hinge-Sigmoid Hybrid Loss'), we inherit the core structure of a smooth hinge loss using a dynamic margin. This includes computing a `dynamic_margin` from a standardized rank gap (`beta * tanh(zscore(rank_gap(...)))`) and using `softplus` to create a non-negative `preference_error = softplus(dynamic_margin - logp_diff)`. This ensures the loss is zero when the model's preference correctly exceeds the required margin.\n\nFrom Parent 1 ('Adaptively Weighted Hinge-Sigmoid Loss with Rank-Gap Scaling'), we inherit the idea of applying an adaptive, sigmoid-based weight to the loss. We compute an `adaptive_weight = sigmoid(gamma * abs(dynamic_margin))`, which modulates the loss based on the certainty of the preference (a larger margin implies a clearer preference and thus a higher weight).\n\nAs a new coupling idea, we introduce a `rank_gap_scale` that directly multiplies the `preference_error` *before* it is weighted. This scale is computed using `softplus(abs(cost_gap_signed))`, where `cost_gap_signed` is the raw, unnormalized rank gap. This new coupling ensures that pairs with larger cost differences (and thus larger rank gaps) not only contribute more to the loss but also require a proportionally larger preference margin to be satisfied. The final loss is `adaptive_weight * rank_gap_scale * preference_error`. This creates a tight link between the magnitude of the cost difference and the penalty incurred.\n\nWhen `cost(a) < cost(b)`, `dynamic_margin` is negative. The loss encourages `logp_diff` to be positive and greater than this margin to drive `preference_error` to zero, correctly preferring `a`.", "pseudocode": "1. Compute the log-probability difference: logp_diff = logp_a - logp_b.\n2. Compute the signed, batch-relative rank gap: cost_gap_signed = rank_gap(cost_a, cost_b).\n3. Standardize the rank gap across the batch: cost_gap_zscored = zscore(cost_gap_signed).\n4. Create a bounded dynamic margin from the standardized rank gap (inherited from Parent 0): dynamic_margin = beta * tanh(cost_gap_zscored).\n5. Compute the base preference error using a smooth hinge-like term (inherited from Parent 0): preference_error = softplus(dynamic_margin - logp_diff).\n6. Create an adaptive weight based on the margin's magnitude (inherited from Parent 1): adaptive_weight = sigmoid(gamma * abs(dynamic_margin)).\n7. Create a rank gap scaling factor based on the raw rank gap (new coupling idea): rank_gap_scale = softplus(abs(cost_gap_signed)).\n8. Couple the rank gap scale directly with the preference error, then apply the adaptive weight (new coupling idea): loss = adaptive_weight * rank_gap_scale * preference_error.", "hyperparams": {"beta": 1.0, "gamma": 1.0}, "operators_used": ["rank_gap", "zscore", "tanh", "softplus", "sigmoid"], "implementation_hint": {"expects": ["cost_a", "cost_b", "logp_a", "logp_b"], "returns": "scalar"}}, "fitness": {"hf_like_score": 8.477949142456055, "validation_objective": 8.477949142456055, "generalization_penalty": 0.0, "generalization_objectives": {"20": 4.000861644744873}, "train_score_mean": 10.47693769454956, "train_loss_mean": 0.5994228601455689, "pair_count": 12902388, "config": {"hf": {"problem": "tsp", "hf_steps": 100, "train_problem_size": 100, "valid_problem_sizes": [20], "train_batch_size": 64, "pomo_size": 64, "learning_rate": 0.0003, "weight_decay": 1e-06, "alpha": 0.05, "device": "cuda", "seed": 1234, "num_validation_episodes": 128, "validation_batch_size": 64, "generalization_penalty_weight": 1.0, "pool_version": "v0"}, "free_loss": {"f1_steps": 100, "f2_steps": 100, "f3_enabled": false}}, "loss_ir": {"name": "Rank-Scaled Adaptive Hinge-Sigmoid Loss", "intuition": "This child loss function synthesizes ideas from its parents to create a robust, adaptive hinge-style loss. It aims to combine the dynamic margin concept with an adaptive scaling mechanism, while introducing a new coupling that links the preference error directly to the rank gap.\n\nFrom Parent 0 ('Adaptive Hinge-Sigmoid Hybrid Loss'), we inherit the core structure of a smooth hinge loss using a dynamic margin. This includes computing a `dynamic_margin` from a standardized rank gap (`beta * tanh(zscore(rank_gap(...)))`) and using `softplus` to create a non-negative `preference_error = softplus(dynamic_margin - logp_diff)`. This ensures the loss is zero when the model's preference correctly exceeds the required margin.\n\nFrom Parent 1 ('Adaptively Weighted Hinge-Sigmoid Loss with Rank-Gap Scaling'), we inherit the idea of applying an adaptive, sigmoid-based weight to the loss. We compute an `adaptive_weight = sigmoid(gamma * abs(dynamic_margin))`, which modulates the loss based on the certainty of the preference (a larger margin implies a clearer preference and thus a higher weight).\n\nAs a new coupling idea, we introduce a `rank_gap_scale` that directly multiplies the `preference_error` *before* it is weighted. This scale is computed using `softplus(abs(cost_gap_signed))`, where `cost_gap_signed` is the raw, unnormalized rank gap. This new coupling ensures that pairs with larger cost differences (and thus larger rank gaps) not only contribute more to the loss but also require a proportionally larger preference margin to be satisfied. The final loss is `adaptive_weight * rank_gap_scale * preference_error`. This creates a tight link between the magnitude of the cost difference and the penalty incurred.\n\nWhen `cost(a) < cost(b)`, `dynamic_margin` is negative. The loss encourages `logp_diff` to be positive and greater than this margin to drive `preference_error` to zero, correctly preferring `a`.", "hyperparams": {"beta": 1.0, "gamma": 1.0}, "operators_used": ["rank_gap", "zscore", "tanh", "softplus", "sigmoid"]}}, "better_than_baseline": false}
{"generation": 4, "index": 9, "ir": {"name": "Rank-Gap Scaled Adaptive Sigmoid Loss", "intuition": "This loss function creates a hybrid that uses an adaptive margin and error structure from Parent 0, while incorporating an adaptive scaling mechanism inspired by Parent 1, and introducing a new coupling that directly modulates the sigmoid's steepness.\n\nFrom Parent 0 ('Adaptive Hinge-Sigmoid Hybrid Loss'), we inherit the core concept of a `dynamic_margin` based on a standardized rank gap (`beta * tanh(zscore(rank_gap(...)))`). We also inherit the idea of an `adaptive_gamma` that scales the loss based on the margin's magnitude, making the loss more sensitive to larger, more certain cost differences.\n\nFrom Parent 1 ('Adaptively Weighted Hinge-Sigmoid Loss with Rank-Gap Scaling'), we inherit the idea of using the raw, un-standardized rank gap to apply a final scaling to the loss (`rank_gap_scale`). This ensures that pairs with larger absolute cost differences have a proportionally larger impact on the total loss, adding a layer of cost-aware importance weighting.\n\nAs a new coupling idea, we merge these components into a single, unified sigmoid-based structure. Instead of using a hinge-like `softplus` term, we directly use the difference `(dynamic_margin - logp_diff)` as the input to a `logsigmoid` function. The `adaptive_gamma` (from Parent 0) is used as a temperature parameter *inside* the `logsigmoid`, controlling its steepness: `logsigmoid(adaptive_gamma * (dynamic_margin - logp_diff))`. This directly couples the margin-based error signal with the adaptive temperature. The final loss is then scaled by the `rank_gap_scale` (from Parent 1), ensuring the overall contribution is proportional to the cost difference magnitude.\n\nWhen `cost(a) < cost(b)`, the `dynamic_margin` is negative. The loss encourages `logp_diff` to be positive to minimize the argument to `logsigmoid`, thus correctly preferring `a`.", "pseudocode": "1. Compute the log-probability difference: logp_diff = logp_a - logp_b.\n2. Compute the signed, batch-relative rank gap: cost_gap_signed = rank_gap(cost_a, cost_b).\n3. Standardize the rank gap across the batch: cost_gap_zscored = zscore(cost_gap_signed).\n4. Create a bounded dynamic margin from the standardized rank gap (inherited from Parent 0): dynamic_margin = beta * tanh(cost_gap_zscored).\n5. Create an adaptive temperature based on the margin magnitude (inherited from Parent 0): adaptive_gamma = 1.0 + gamma * abs(dynamic_margin).\n6. Create a rank gap scaling factor from the un-standardized rank gap (inherited from Parent 1): rank_gap_scale = 1.0 + softplus(abs(cost_gap_signed) - 1.0).\n7. Compute the core preference error using a sigmoid structure where the adaptive temperature controls the steepness (new coupling idea): sigmoid_error = -logsigmoid(adaptive_gamma * (dynamic_margin - logp_diff)).\n8. Apply the rank gap scaling to the final loss: loss = rank_gap_scale * sigmoid_error.", "hyperparams": {"beta": 1.5, "gamma": 1.0}, "operators_used": ["rank_gap", "zscore", "tanh", "softplus", "logsigmoid"], "implementation_hint": {"expects": ["cost_a", "cost_b", "logp_a", "logp_b"], "returns": "scalar"}}, "fitness": {"hf_like_score": 8.546077728271484, "validation_objective": 8.546077728271484, "generalization_penalty": 0.0, "generalization_objectives": {"20": 3.971068024635315}, "train_score_mean": 10.557883586883545, "train_loss_mean": 0.6017942613363266, "pair_count": 12902392, "config": {"hf": {"problem": "tsp", "hf_steps": 100, "train_problem_size": 100, "valid_problem_sizes": [20], "train_batch_size": 64, "pomo_size": 64, "learning_rate": 0.0003, "weight_decay": 1e-06, "alpha": 0.05, "device": "cuda", "seed": 1234, "num_validation_episodes": 128, "validation_batch_size": 64, "generalization_penalty_weight": 1.0, "pool_version": "v0"}, "free_loss": {"f1_steps": 100, "f2_steps": 100, "f3_enabled": false}}, "loss_ir": {"name": "Rank-Gap Scaled Adaptive Sigmoid Loss", "intuition": "This loss function creates a hybrid that uses an adaptive margin and error structure from Parent 0, while incorporating an adaptive scaling mechanism inspired by Parent 1, and introducing a new coupling that directly modulates the sigmoid's steepness.\n\nFrom Parent 0 ('Adaptive Hinge-Sigmoid Hybrid Loss'), we inherit the core concept of a `dynamic_margin` based on a standardized rank gap (`beta * tanh(zscore(rank_gap(...)))`). We also inherit the idea of an `adaptive_gamma` that scales the loss based on the margin's magnitude, making the loss more sensitive to larger, more certain cost differences.\n\nFrom Parent 1 ('Adaptively Weighted Hinge-Sigmoid Loss with Rank-Gap Scaling'), we inherit the idea of using the raw, un-standardized rank gap to apply a final scaling to the loss (`rank_gap_scale`). This ensures that pairs with larger absolute cost differences have a proportionally larger impact on the total loss, adding a layer of cost-aware importance weighting.\n\nAs a new coupling idea, we merge these components into a single, unified sigmoid-based structure. Instead of using a hinge-like `softplus` term, we directly use the difference `(dynamic_margin - logp_diff)` as the input to a `logsigmoid` function. The `adaptive_gamma` (from Parent 0) is used as a temperature parameter *inside* the `logsigmoid`, controlling its steepness: `logsigmoid(adaptive_gamma * (dynamic_margin - logp_diff))`. This directly couples the margin-based error signal with the adaptive temperature. The final loss is then scaled by the `rank_gap_scale` (from Parent 1), ensuring the overall contribution is proportional to the cost difference magnitude.\n\nWhen `cost(a) < cost(b)`, the `dynamic_margin` is negative. The loss encourages `logp_diff` to be positive to minimize the argument to `logsigmoid`, thus correctly preferring `a`.", "hyperparams": {"beta": 1.5, "gamma": 1.0}, "operators_used": ["rank_gap", "zscore", "tanh", "softplus", "logsigmoid"]}}, "better_than_baseline": false}
{"generation": 5, "index": 0, "ir": {"name": "Rank-Gated Sigmoid Loss with Adaptive Temperature", "intuition": "This loss function creates a hybrid that gates the application of a sigmoid-style loss based on a rank-derived margin, and then adaptively scales the penalty based on the magnitude of that margin.\n\nFrom Parent 0 ('Adaptive Hinge-Sigmoid Hybrid Loss'), we inherit the core concept of a `dynamic_margin` derived from the standardized rank gap (`beta * tanh(zscore(rank_gap(...)))`). This provides a robust, bounded target for the model's preference score. We also inherit the `adaptive_gamma` mechanism, which scales the loss penalty based on the magnitude of this margin, making the loss more sensitive to clear-cut cases.\n\nFrom Parent 1 ('Adaptively Weighted Hinge-Sigmoid Loss with Rank-Gap Scaling'), we inherit the use of a `sigmoid` function to create an adaptive weight. However, instead of using it to blend two losses, we use it as a 'gate' for the primary loss term.\n\nAs a new coupling idea, we introduce a 'rank-gated' structure. We compute a `gating_value = sigmoid(gamma * (logp_diff - dynamic_margin))`. This value approaches 1 when the model's preference `logp_diff` correctly exceeds the `dynamic_margin`, and approaches 0 otherwise. The main loss term is then calculated as `(1 - gating_value)`. This creates a smooth penalty that is close to zero for correct preferences and close to one for incorrect preferences. A second new idea is to scale this gated penalty directly by the `adaptive_gamma` term inherited from Parent 0. The final loss is `adaptive_gamma * (1 - gating_value)`. This structure ensures that pairs with a larger, more certain rank-gap (and thus a larger `dynamic_margin` and `adaptive_gamma`) are penalized more heavily if the model fails to meet the preference target.\n\nWhen `cost(a) < cost(b)`, `dynamic_margin` is negative. The loss encourages `logp_diff` to be positive and larger than this margin, which pushes `gating_value` towards 1 and the overall loss towards 0, correctly preferring `a`.", "pseudocode": "1. Compute the log-probability difference: logp_diff = logp_a - logp_b.\n2. Compute the signed, batch-relative rank gap: cost_gap_signed = rank_gap(cost_a, cost_b).\n3. Standardize the rank gap across the batch: cost_gap_zscored = zscore(cost_gap_signed).\n4. Create a bounded dynamic margin from the standardized rank gap (inherited from Parent 0): dynamic_margin = beta * tanh(cost_gap_zscored).\n5. Create an adaptive temperature based on the margin magnitude (inherited from Parent 0): adaptive_gamma = 1.0 + gamma * abs(dynamic_margin).\n6. Compute a sigmoid-based gating value that measures how well the model's preference meets the margin (new coupling idea, inspired by Parent 1's use of sigmoid): gating_value = sigmoid(gamma * (logp_diff - dynamic_margin)).\n7. The core loss is the inverse of the gating value, representing the error: error_term = 1.0 - gating_value.\n8. Scale the error by the adaptive temperature to penalize high-certainty errors more (new coupling idea): loss = adaptive_gamma * error_term.", "hyperparams": {"beta": 1.5, "gamma": 1.0}, "operators_used": ["rank_gap", "zscore", "tanh", "sigmoid"], "implementation_hint": {"expects": ["cost_a", "cost_b", "logp_a", "logp_b"], "returns": "scalar"}}, "fitness": {"hf_like_score": 8.461941242218018, "validation_objective": 8.461941242218018, "generalization_penalty": 0.0, "generalization_objectives": {"20": 4.007627487182617}, "train_score_mean": 10.483429613113403, "train_loss_mean": 0.6011939781904221, "pair_count": 12902387, "config": {"hf": {"problem": "tsp", "hf_steps": 100, "train_problem_size": 100, "valid_problem_sizes": [20], "train_batch_size": 64, "pomo_size": 64, "learning_rate": 0.0003, "weight_decay": 1e-06, "alpha": 0.05, "device": "cuda", "seed": 1234, "num_validation_episodes": 128, "validation_batch_size": 64, "generalization_penalty_weight": 1.0, "pool_version": "v0"}, "free_loss": {"f1_steps": 100, "f2_steps": 100, "f3_enabled": false}}, "loss_ir": {"name": "Rank-Gated Sigmoid Loss with Adaptive Temperature", "intuition": "This loss function creates a hybrid that gates the application of a sigmoid-style loss based on a rank-derived margin, and then adaptively scales the penalty based on the magnitude of that margin.\n\nFrom Parent 0 ('Adaptive Hinge-Sigmoid Hybrid Loss'), we inherit the core concept of a `dynamic_margin` derived from the standardized rank gap (`beta * tanh(zscore(rank_gap(...)))`). This provides a robust, bounded target for the model's preference score. We also inherit the `adaptive_gamma` mechanism, which scales the loss penalty based on the magnitude of this margin, making the loss more sensitive to clear-cut cases.\n\nFrom Parent 1 ('Adaptively Weighted Hinge-Sigmoid Loss with Rank-Gap Scaling'), we inherit the use of a `sigmoid` function to create an adaptive weight. However, instead of using it to blend two losses, we use it as a 'gate' for the primary loss term.\n\nAs a new coupling idea, we introduce a 'rank-gated' structure. We compute a `gating_value = sigmoid(gamma * (logp_diff - dynamic_margin))`. This value approaches 1 when the model's preference `logp_diff` correctly exceeds the `dynamic_margin`, and approaches 0 otherwise. The main loss term is then calculated as `(1 - gating_value)`. This creates a smooth penalty that is close to zero for correct preferences and close to one for incorrect preferences. A second new idea is to scale this gated penalty directly by the `adaptive_gamma` term inherited from Parent 0. The final loss is `adaptive_gamma * (1 - gating_value)`. This structure ensures that pairs with a larger, more certain rank-gap (and thus a larger `dynamic_margin` and `adaptive_gamma`) are penalized more heavily if the model fails to meet the preference target.\n\nWhen `cost(a) < cost(b)`, `dynamic_margin` is negative. The loss encourages `logp_diff` to be positive and larger than this margin, which pushes `gating_value` towards 1 and the overall loss towards 0, correctly preferring `a`.", "hyperparams": {"beta": 1.5, "gamma": 1.0}, "operators_used": ["rank_gap", "zscore", "tanh", "sigmoid"]}}, "better_than_baseline": false}
{"generation": 5, "index": 1, "ir": {"name": "Adaptive Rank-Calibrated Hinge Loss", "intuition": "This child loss function creates a hybrid that uses a rank-calibrated margin within a stable, adaptive hinge framework. The goal is to make the loss sensitive to both the relative ranking of costs within a batch and the absolute magnitude of the cost differences.\n\nFrom Parent 0 (Adaptive Hinge-Sigmoid Hybrid Loss), we inherit the core structure of a smooth, one-sided hinge loss: `softplus(margin - logp_diff)`. This ensures that the loss is zero when the model's preference `logp_diff` correctly exceeds the required margin, providing a clear optimization target.\n\nFrom Parent 1 (Adaptively Weighted Hinge-Sigmoid Loss), we inherit the adaptive weighting mechanism, `adaptive_alpha = sigmoid(gamma * abs(dynamic_margin))`. This weight dynamically adjusts the loss's influence, making it stronger for pairs with a clear preference signal (large margin) and weaker for ambiguous pairs (small margin).\n\nWe introduce two new coupling ideas. First, we create a 'dual-calibrated margin' by combining the rank-based signal from the parents with a signal based on the raw cost difference. The `dynamic_margin` is now a weighted sum of a rank-based term (`tanh(zscore(rank_gap))`) and a cost-difference-based term (`tanh(cost_a - cost_b)`). This allows the margin to be sensitive to both batch-wide ranking and the specific cost gap of the pair. Second, we introduce a new stability trick by applying a `clamp` operator to the final loss. The `adaptive_alpha` can approach 1, and the `softplus` term can grow large, so clamping the final loss to a maximum value `max_loss` prevents extreme gradient spikes from unstable pairs, improving overall training stability.\n\nWhen `cost(a) < cost(b)`, both terms contributing to `dynamic_margin` become negative. The loss encourages `logp_diff` to be positive and larger than this negative margin to drive the `softplus` term to zero, thus correctly preferring `a`.", "pseudocode": "1. Compute core differences: logp_diff = logp_a - logp_b and cost_diff = cost_a - cost_b.\n2. Compute the signed, batch-relative rank gap: cost_gap_signed = rank_gap(cost_a, cost_b).\n3. Standardize the rank gap across the batch: cost_gap_zscored = zscore(cost_gap_signed).\n4. Create the first part of the margin from the rank gap (structure from parents): rank_margin_term = tanh(cost_gap_zscored).\n5. Create the second part of the margin from the raw cost difference (new coupling idea): cost_margin_term = tanh(cost_diff).\n6. Combine the two terms into a dual-calibrated dynamic margin (new coupling idea): dynamic_margin = beta * (w_rank * rank_margin_term + (1 - w_rank) * cost_margin_term).\n7. Create an adaptive weight based on the margin's magnitude (inherited from Parent 1): adaptive_alpha = sigmoid(gamma * abs(dynamic_margin)).\n8. Compute the base preference error using a smooth hinge structure (inherited from Parent 0): preference_error = softplus(dynamic_margin - logp_diff).\n9. Modulate the error with the adaptive weight: weighted_error = adaptive_alpha * preference_error.\n10. Apply a stability clamp to the final loss (new stability trick): loss = clamp(weighted_error, 0, max_loss).", "hyperparams": {"beta": 1.5, "gamma": 1.0, "w_rank": 0.5, "max_loss": 5.0}, "operators_used": ["rank_gap", "zscore", "tanh", "sigmoid", "softplus", "clamp"], "implementation_hint": {"expects": ["cost_a", "cost_b", "logp_a", "logp_b"], "returns": "scalar"}}, "fitness": {"hf_like_score": 8.424729347229004, "validation_objective": 8.424729347229004, "generalization_penalty": 0.0, "generalization_objectives": {"20": 4.012831091880798}, "train_score_mean": 10.478371896743774, "train_loss_mean": 0.6021029776334763, "pair_count": 12902389, "config": {"hf": {"problem": "tsp", "hf_steps": 100, "train_problem_size": 100, "valid_problem_sizes": [20], "train_batch_size": 64, "pomo_size": 64, "learning_rate": 0.0003, "weight_decay": 1e-06, "alpha": 0.05, "device": "cuda", "seed": 1234, "num_validation_episodes": 128, "validation_batch_size": 64, "generalization_penalty_weight": 1.0, "pool_version": "v0"}, "free_loss": {"f1_steps": 100, "f2_steps": 100, "f3_enabled": false}}, "loss_ir": {"name": "Adaptive Rank-Calibrated Hinge Loss", "intuition": "This child loss function creates a hybrid that uses a rank-calibrated margin within a stable, adaptive hinge framework. The goal is to make the loss sensitive to both the relative ranking of costs within a batch and the absolute magnitude of the cost differences.\n\nFrom Parent 0 (Adaptive Hinge-Sigmoid Hybrid Loss), we inherit the core structure of a smooth, one-sided hinge loss: `softplus(margin - logp_diff)`. This ensures that the loss is zero when the model's preference `logp_diff` correctly exceeds the required margin, providing a clear optimization target.\n\nFrom Parent 1 (Adaptively Weighted Hinge-Sigmoid Loss), we inherit the adaptive weighting mechanism, `adaptive_alpha = sigmoid(gamma * abs(dynamic_margin))`. This weight dynamically adjusts the loss's influence, making it stronger for pairs with a clear preference signal (large margin) and weaker for ambiguous pairs (small margin).\n\nWe introduce two new coupling ideas. First, we create a 'dual-calibrated margin' by combining the rank-based signal from the parents with a signal based on the raw cost difference. The `dynamic_margin` is now a weighted sum of a rank-based term (`tanh(zscore(rank_gap))`) and a cost-difference-based term (`tanh(cost_a - cost_b)`). This allows the margin to be sensitive to both batch-wide ranking and the specific cost gap of the pair. Second, we introduce a new stability trick by applying a `clamp` operator to the final loss. The `adaptive_alpha` can approach 1, and the `softplus` term can grow large, so clamping the final loss to a maximum value `max_loss` prevents extreme gradient spikes from unstable pairs, improving overall training stability.\n\nWhen `cost(a) < cost(b)`, both terms contributing to `dynamic_margin` become negative. The loss encourages `logp_diff` to be positive and larger than this negative margin to drive the `softplus` term to zero, thus correctly preferring `a`.", "hyperparams": {"beta": 1.5, "gamma": 1.0, "w_rank": 0.5, "max_loss": 5.0}, "operators_used": ["rank_gap", "zscore", "tanh", "sigmoid", "softplus", "clamp"]}}, "better_than_baseline": true}
{"generation": 5, "index": 2, "ir": {"name": "Rank-Calibrated Sigmoid-Hinge with Adaptive Temperature", "intuition": "This loss function creates a dynamic hybrid that balances a hinge-like objective with a probabilistic sigmoid framework, calibrated by rank information.\n\nFrom Parent 0 ('Adaptive Hinge-Sigmoid Hybrid Loss'), we inherit the core concept of a `dynamic_margin` that is derived from the standardized rank gap of costs (`beta * tanh(zscore(rank_gap(...)))`). This makes the required preference margin adaptive to the relative ranking of the cost pair within the batch, penalizing mis-rankings more harshly.\n\nFrom Parent 1 ('Adaptively Weighted Hinge-Sigmoid Loss with Rank-Gap Scaling'), we inherit the idea of using a `sigmoid` function to create an adaptive weight (`adaptive_weight = sigmoid(gamma * abs(dynamic_margin))`). This weight modulates the loss, making it more sensitive to pairs with a clear rank separation and more lenient for ambiguous pairs.\n\nWe introduce two new coupling ideas. First, we combine the hinge and sigmoid concepts in a novel way. The loss is a weighted sum: `loss = (1 - adaptive_weight) * hinge_term + adaptive_weight * sigmoid_term`. This creates a smooth interpolation between a margin-based loss (dominant for small rank gaps) and a probabilistic log-sigmoid loss (dominant for large rank gaps). Second, we introduce an `adaptive_temperature` that scales the argument of the log-sigmoid term. This temperature is derived from the non-standardized rank gap, making the probabilistic component more sensitive to pairs with large absolute cost differences. This ensures that even if the standardized rank gap is small, a large absolute cost difference still results in a strong preference signal.\n\nWhen `cost(a) < cost(b)`, the `dynamic_margin` is negative. The loss encourages `logp_diff` to be positive. The `hinge_term` pushes `logp_diff` to be greater than the margin, while the `sigmoid_term` pushes `logp_diff` to be large and positive, thus correctly preferring `a`.", "pseudocode": "1. Compute the log-probability difference: logp_diff = logp_a - logp_b.\n2. Compute the signed, batch-relative rank gap: cost_gap_signed = rank_gap(cost_a, cost_b).\n3. Standardize the rank gap across the batch: cost_gap_zscored = zscore(cost_gap_signed).\n4. Create a bounded dynamic margin from the standardized rank gap (inherited from Parent 0): dynamic_margin = beta * tanh(cost_gap_zscored).\n5. Create an adaptive interpolation weight based on the margin's magnitude (inherited from Parent 1): adaptive_weight = sigmoid(gamma * abs(dynamic_margin)).\n6. Compute a hinge-like loss component: hinge_term = softplus(dynamic_margin - logp_diff).\n7. Create an adaptive temperature from the non-standardized rank gap (new coupling idea): adaptive_temperature = 1.0 + tau * softplus(abs(cost_gap_signed)).\n8. Compute a sigmoid-like loss component, scaled by the adaptive temperature: sigmoid_term = softplus(-logp_diff * adaptive_temperature).\n9. Combine the hinge and sigmoid components using the adaptive weight (new coupling idea): loss = (1.0 - adaptive_weight) * hinge_term + adaptive_weight * sigmoid_term.", "hyperparams": {"beta": 1.0, "gamma": 1.0, "tau": 0.1}, "operators_used": ["rank_gap", "zscore", "tanh", "sigmoid", "softplus"], "implementation_hint": {"expects": ["cost_a", "cost_b", "logp_a", "logp_b"], "returns": "scalar"}}, "fitness": {"hf_like_score": 8.473191261291504, "validation_objective": 8.473191261291504, "generalization_penalty": 0.0, "generalization_objectives": {"20": 4.0104851722717285}, "train_score_mean": 10.528286361694336, "train_loss_mean": 0.6015978682041169, "pair_count": 12902378, "config": {"hf": {"problem": "tsp", "hf_steps": 100, "train_problem_size": 100, "valid_problem_sizes": [20], "train_batch_size": 64, "pomo_size": 64, "learning_rate": 0.0003, "weight_decay": 1e-06, "alpha": 0.05, "device": "cuda", "seed": 1234, "num_validation_episodes": 128, "validation_batch_size": 64, "generalization_penalty_weight": 1.0, "pool_version": "v0"}, "free_loss": {"f1_steps": 100, "f2_steps": 100, "f3_enabled": false}}, "loss_ir": {"name": "Rank-Calibrated Sigmoid-Hinge with Adaptive Temperature", "intuition": "This loss function creates a dynamic hybrid that balances a hinge-like objective with a probabilistic sigmoid framework, calibrated by rank information.\n\nFrom Parent 0 ('Adaptive Hinge-Sigmoid Hybrid Loss'), we inherit the core concept of a `dynamic_margin` that is derived from the standardized rank gap of costs (`beta * tanh(zscore(rank_gap(...)))`). This makes the required preference margin adaptive to the relative ranking of the cost pair within the batch, penalizing mis-rankings more harshly.\n\nFrom Parent 1 ('Adaptively Weighted Hinge-Sigmoid Loss with Rank-Gap Scaling'), we inherit the idea of using a `sigmoid` function to create an adaptive weight (`adaptive_weight = sigmoid(gamma * abs(dynamic_margin))`). This weight modulates the loss, making it more sensitive to pairs with a clear rank separation and more lenient for ambiguous pairs.\n\nWe introduce two new coupling ideas. First, we combine the hinge and sigmoid concepts in a novel way. The loss is a weighted sum: `loss = (1 - adaptive_weight) * hinge_term + adaptive_weight * sigmoid_term`. This creates a smooth interpolation between a margin-based loss (dominant for small rank gaps) and a probabilistic log-sigmoid loss (dominant for large rank gaps). Second, we introduce an `adaptive_temperature` that scales the argument of the log-sigmoid term. This temperature is derived from the non-standardized rank gap, making the probabilistic component more sensitive to pairs with large absolute cost differences. This ensures that even if the standardized rank gap is small, a large absolute cost difference still results in a strong preference signal.\n\nWhen `cost(a) < cost(b)`, the `dynamic_margin` is negative. The loss encourages `logp_diff` to be positive. The `hinge_term` pushes `logp_diff` to be greater than the margin, while the `sigmoid_term` pushes `logp_diff` to be large and positive, thus correctly preferring `a`.", "hyperparams": {"beta": 1.0, "gamma": 1.0, "tau": 0.1}, "operators_used": ["rank_gap", "zscore", "tanh", "sigmoid", "softplus"]}}, "better_than_baseline": false}
{"generation": 5, "index": 3, "ir": {"name": "Rank-Modulated Sigmoid-Hinge Loss", "intuition": "This loss function creates a hybrid that modulates a stable hinge-like error term using both a probabilistic weight and a raw rank-gap scale. It aims to combine the stability of a smooth hinge loss with multiple layers of adaptive scaling.\n\nFrom Parent 0 ('Adaptive Hinge-Sigmoid Hybrid Loss'), we inherit the core structure of computing a `preference_error` using `softplus(dynamic_margin - logp_diff)`. This provides a smooth, non-negative error that is zero when the model's preference `logp_diff` correctly exceeds the `dynamic_margin`. We also inherit the use of `tanh(zscore(rank_gap(...)))` to create this margin, ensuring it is bounded and relative to the batch distribution.\n\nFrom Parent 1 ('Adaptively Weighted Hinge-Sigmoid Loss with Rank-Gap Scaling'), we inherit the idea of an `adaptive_alpha` weight, calculated using `sigmoid(gamma * abs(dynamic_margin))`. This weight makes the loss more sensitive to pairs with a clear preference margin (large `abs(dynamic_margin)`) and less sensitive to ambiguous pairs.\n\nAs a new coupling idea, we introduce a direct modulation of the `logp_diff` term itself. Instead of a simple `logp_a - logp_b`, we compute a `scaled_logp_diff = (logp_a - logp_b) / (1 + abs(cost_gap_signed))`. This new term normalizes the log-probability difference by the magnitude of the raw cost gap. The intuition is to make the loss more forgiving of small logp differences when the cost gap is large (as the preference should be obvious) and stricter when the cost gap is small (requiring a clearer signal from the model). This scaled term is then used within the inherited hinge structure: `preference_error = softplus(dynamic_margin - scaled_logp_diff)`.\n\nFinally, we combine these ideas: the `preference_error` (built with the new scaled `logp_diff`) is multiplied by the `adaptive_alpha` weight from Parent 1. The final loss `loss = adaptive_alpha * preference_error` is a simple, direct application of the adaptive weight to the novel preference error term, avoiding the extra `log(1+x)` structure for a more direct gradient signal.\n\nWhen `cost(a) < cost(b)`, the `dynamic_margin` is negative. The loss encourages `scaled_logp_diff` to be positive and larger than this margin, driving the `preference_error` to zero.", "pseudocode": "1. Compute the log-probability difference: logp_diff = logp_a - logp_b.\n2. Compute the signed, batch-relative rank gap: cost_gap_signed = rank_gap(cost_a, cost_b).\n3. Standardize the rank gap across the batch: cost_gap_zscored = zscore(cost_gap_signed).\n4. Create a bounded dynamic margin from the standardized rank gap (inherited from Parent 0): dynamic_margin = beta * tanh(cost_gap_zscored).\n5. Create an adaptive weight based on the margin's magnitude (inherited from Parent 1): adaptive_alpha = sigmoid(gamma * abs(dynamic_margin)).\n6. Scale the log-probability difference by the raw cost gap magnitude (new coupling idea): scaled_logp_diff = logp_diff / (1.0 + abs(cost_gap_signed)).\n7. Compute the preference error using the scaled logp_diff and a smooth hinge structure (inherited from Parent 0, modified with new coupling): preference_error = softplus(dynamic_margin - scaled_logp_diff).\n8. Compute the final loss by applying the adaptive weight to the preference error: loss = adaptive_alpha * preference_error.", "hyperparams": {"beta": 1.0, "gamma": 1.0}, "operators_used": ["rank_gap", "zscore", "tanh", "sigmoid", "softplus"], "implementation_hint": {"expects": ["cost_a", "cost_b", "logp_a", "logp_b"], "returns": "scalar"}}, "fitness": {"hf_like_score": 8.418383121490479, "validation_objective": 8.418383121490479, "generalization_penalty": 0.0, "generalization_objectives": {"20": 4.07118034362793}, "train_score_mean": 10.517462768554687, "train_loss_mean": 0.6025020080804825, "pair_count": 12902386, "config": {"hf": {"problem": "tsp", "hf_steps": 100, "train_problem_size": 100, "valid_problem_sizes": [20], "train_batch_size": 64, "pomo_size": 64, "learning_rate": 0.0003, "weight_decay": 1e-06, "alpha": 0.05, "device": "cuda", "seed": 1234, "num_validation_episodes": 128, "validation_batch_size": 64, "generalization_penalty_weight": 1.0, "pool_version": "v0"}, "free_loss": {"f1_steps": 100, "f2_steps": 100, "f3_enabled": false}}, "loss_ir": {"name": "Rank-Modulated Sigmoid-Hinge Loss", "intuition": "This loss function creates a hybrid that modulates a stable hinge-like error term using both a probabilistic weight and a raw rank-gap scale. It aims to combine the stability of a smooth hinge loss with multiple layers of adaptive scaling.\n\nFrom Parent 0 ('Adaptive Hinge-Sigmoid Hybrid Loss'), we inherit the core structure of computing a `preference_error` using `softplus(dynamic_margin - logp_diff)`. This provides a smooth, non-negative error that is zero when the model's preference `logp_diff` correctly exceeds the `dynamic_margin`. We also inherit the use of `tanh(zscore(rank_gap(...)))` to create this margin, ensuring it is bounded and relative to the batch distribution.\n\nFrom Parent 1 ('Adaptively Weighted Hinge-Sigmoid Loss with Rank-Gap Scaling'), we inherit the idea of an `adaptive_alpha` weight, calculated using `sigmoid(gamma * abs(dynamic_margin))`. This weight makes the loss more sensitive to pairs with a clear preference margin (large `abs(dynamic_margin)`) and less sensitive to ambiguous pairs.\n\nAs a new coupling idea, we introduce a direct modulation of the `logp_diff` term itself. Instead of a simple `logp_a - logp_b`, we compute a `scaled_logp_diff = (logp_a - logp_b) / (1 + abs(cost_gap_signed))`. This new term normalizes the log-probability difference by the magnitude of the raw cost gap. The intuition is to make the loss more forgiving of small logp differences when the cost gap is large (as the preference should be obvious) and stricter when the cost gap is small (requiring a clearer signal from the model). This scaled term is then used within the inherited hinge structure: `preference_error = softplus(dynamic_margin - scaled_logp_diff)`.\n\nFinally, we combine these ideas: the `preference_error` (built with the new scaled `logp_diff`) is multiplied by the `adaptive_alpha` weight from Parent 1. The final loss `loss = adaptive_alpha * preference_error` is a simple, direct application of the adaptive weight to the novel preference error term, avoiding the extra `log(1+x)` structure for a more direct gradient signal.\n\nWhen `cost(a) < cost(b)`, the `dynamic_margin` is negative. The loss encourages `scaled_logp_diff` to be positive and larger than this margin, driving the `preference_error` to zero.", "hyperparams": {"beta": 1.0, "gamma": 1.0}, "operators_used": ["rank_gap", "zscore", "tanh", "sigmoid", "softplus"]}}, "better_than_baseline": true}
{"generation": 5, "index": 4, "ir": {"name": "Adaptive Rank-Calibrated Hinge Loss", "intuition": "This loss function creates a hybrid by taking the core hinge-like structure from one parent and the adaptive temperature/scaling idea from the other, coupling them with a new normalization scheme for the preference signal.\n\nFrom Parent 0 ('Adaptive Hinge-Sigmoid Hybrid Loss'), we inherit the fundamental structure of a dynamic margin hinge loss: `softplus(dynamic_margin - logp_diff)`. This provides a clean, zero-loss region when the model's preference `logp_diff` correctly exceeds the target margin, which is dynamically set by the rank gap.\n\nFrom Parent 1 ('Adaptively Weighted Hinge-Sigmoid Loss with Rank-Gap Scaling'), we inherit the idea of an adaptive weight or scaling factor, `adaptive_alpha`, derived from the magnitude of the preference margin. This makes the loss more sensitive to pairs with a clear cost difference (large margin) and less sensitive to ambiguous pairs (small margin).\n\nAs a new coupling idea, we introduce a `zscore` normalization to the log-probability difference (`logp_diff`) itself. Instead of using the raw `logp_diff`, we use a standardized version, `logp_diff_zscored`. This stabilizes training by preventing extreme log-probability differences from dominating the loss calculation, making the hinge comparison against the `dynamic_margin` more consistent across a batch. The final loss is a product of the adaptive weight and the hinge term, `adaptive_alpha * softplus(dynamic_margin - logp_diff_zscored)`, directly combining the inherited ideas with the new stabilization trick.\n\nWhen `cost(a) < cost(b)`, the `dynamic_margin` is negative. The loss encourages `logp_diff_zscored` to be positive and greater than this margin to make the `softplus` term zero, thus correctly preferring `a`.", "pseudocode": "1. Compute the log-probability difference: logp_diff = logp_a - logp_b.\n2. Compute the signed, batch-relative rank gap: cost_gap_signed = rank_gap(cost_a, cost_b).\n3. Standardize the rank gap across the batch: cost_gap_zscored = zscore(cost_gap_signed).\n4. Create a bounded dynamic margin from the standardized rank gap (inherited from Parent 0): dynamic_margin = beta * tanh(cost_gap_zscored).\n5. Create an adaptive weight based on the margin's magnitude (inherited from Parent 1): adaptive_alpha = sigmoid(gamma * abs(dynamic_margin)).\n6. Standardize the log-probability difference across the batch (new coupling idea): logp_diff_zscored = zscore(logp_diff).\n7. Compute the final loss by combining the adaptive weight with a hinge term that uses the standardized log-probability difference (structure from Parent 0): loss = adaptive_alpha * softplus(dynamic_margin - logp_diff_zscored).", "hyperparams": {"beta": 1.0, "gamma": 1.25}, "operators_used": ["rank_gap", "zscore", "tanh", "sigmoid", "softplus"], "implementation_hint": {"expects": ["cost_a", "cost_b", "logp_a", "logp_b"], "returns": "scalar"}}, "fitness": {"hf_like_score": 8.44991159439087, "validation_objective": 8.44991159439087, "generalization_penalty": 0.0, "generalization_objectives": {"20": 4.035057544708252}, "train_score_mean": 10.46654543876648, "train_loss_mean": 0.602916721701622, "pair_count": 12902387, "config": {"hf": {"problem": "tsp", "hf_steps": 100, "train_problem_size": 100, "valid_problem_sizes": [20], "train_batch_size": 64, "pomo_size": 64, "learning_rate": 0.0003, "weight_decay": 1e-06, "alpha": 0.05, "device": "cuda", "seed": 1234, "num_validation_episodes": 128, "validation_batch_size": 64, "generalization_penalty_weight": 1.0, "pool_version": "v0"}, "free_loss": {"f1_steps": 100, "f2_steps": 100, "f3_enabled": false}}, "loss_ir": {"name": "Adaptive Rank-Calibrated Hinge Loss", "intuition": "This loss function creates a hybrid by taking the core hinge-like structure from one parent and the adaptive temperature/scaling idea from the other, coupling them with a new normalization scheme for the preference signal.\n\nFrom Parent 0 ('Adaptive Hinge-Sigmoid Hybrid Loss'), we inherit the fundamental structure of a dynamic margin hinge loss: `softplus(dynamic_margin - logp_diff)`. This provides a clean, zero-loss region when the model's preference `logp_diff` correctly exceeds the target margin, which is dynamically set by the rank gap.\n\nFrom Parent 1 ('Adaptively Weighted Hinge-Sigmoid Loss with Rank-Gap Scaling'), we inherit the idea of an adaptive weight or scaling factor, `adaptive_alpha`, derived from the magnitude of the preference margin. This makes the loss more sensitive to pairs with a clear cost difference (large margin) and less sensitive to ambiguous pairs (small margin).\n\nAs a new coupling idea, we introduce a `zscore` normalization to the log-probability difference (`logp_diff`) itself. Instead of using the raw `logp_diff`, we use a standardized version, `logp_diff_zscored`. This stabilizes training by preventing extreme log-probability differences from dominating the loss calculation, making the hinge comparison against the `dynamic_margin` more consistent across a batch. The final loss is a product of the adaptive weight and the hinge term, `adaptive_alpha * softplus(dynamic_margin - logp_diff_zscored)`, directly combining the inherited ideas with the new stabilization trick.\n\nWhen `cost(a) < cost(b)`, the `dynamic_margin` is negative. The loss encourages `logp_diff_zscored` to be positive and greater than this margin to make the `softplus` term zero, thus correctly preferring `a`.", "hyperparams": {"beta": 1.0, "gamma": 1.25}, "operators_used": ["rank_gap", "zscore", "tanh", "sigmoid", "softplus"]}}, "better_than_baseline": false}
{"generation": 5, "index": 5, "ir": {"name": "Rank-Modulated Sigmoid-Hinge Loss", "intuition": "This child loss creates a hybrid that modulates a stable hinge-like error term using both probabilistic and rank-based signals, borrowing from both parents and introducing a new coupling mechanism.\n\nFrom Parent 0 (Adaptive Hinge-Sigmoid Hybrid Loss), we inherit the core structure of a stable, smooth hinge loss. Specifically, we use a `preference_error = softplus(dynamic_margin - logp_diff)` term, which penalizes the model only when its preference `logp_diff` fails to exceed a `dynamic_margin`. We also inherit the `dynamic_margin` itself, which is computed as `beta * tanh(zscore(rank_gap(...)))`, providing a bounded, rank-sensitive target for the model's preference.\n\nFrom Parent 1 (Adaptively Weighted Hinge-Sigmoid Loss), we inherit the idea of using a `sigmoid`-based adaptive weight to modulate the loss. However, instead of basing the weight on the margin as in the parent, we base it on the model's own confidence, `logp_diff`.\n\nAs a new coupling idea, we introduce a 'rank-gap scaling' factor, `rank_gap_scale`, similar to the one in Parent 1, but we apply it *inside* the loss computation rather than as a final multiplier. The final loss is `rank_gap_scale * sigmoid(gamma * preference_error)`. This structure has several benefits: the `sigmoid` squashes the error into a (0, 1) range, preventing extreme loss values and providing stable gradients. The `rank_gap_scale` then ensures that pairs with larger cost differences (and thus larger rank gaps) contribute more significantly to the final loss, effectively prioritizing the most important preference pairs. This combines the stability of a sigmoid with the calibration of rank-gap scaling.\n\nWhen `cost(a) < cost(b)`, the `dynamic_margin` is negative. The loss encourages `logp_diff` to be positive and greater than this margin to drive the `preference_error` to zero, which in turn minimizes the overall loss.", "pseudocode": "1. Compute the log-probability difference: logp_diff = logp_a - logp_b.\n2. Compute the signed, batch-relative rank gap: cost_gap_signed = rank_gap(cost_a, cost_b).\n3. Standardize the rank gap across the batch: cost_gap_zscored = zscore(cost_gap_signed).\n4. Create a bounded dynamic margin from the standardized rank gap (inherited from Parent 0): dynamic_margin = beta * tanh(cost_gap_zscored).\n5. Compute the preference error using a smooth hinge-like term (inherited from Parent 0): preference_error = softplus(dynamic_margin - logp_diff).\n6. Create a rank gap scaling factor based on the raw rank gap (inspired by Parent 1, new coupling): rank_gap_scale = 1.0 + softplus(abs(cost_gap_signed)).\n7. Calculate the final loss by applying the scaling factor to a sigmoid-transformed error (new coupling idea combining sigmoid from Parent 1 and hinge from Parent 0): loss = rank_gap_scale * sigmoid(gamma * preference_error).", "hyperparams": {"beta": 1.0, "gamma": 1.0}, "operators_used": ["rank_gap", "zscore", "tanh", "softplus", "sigmoid"], "implementation_hint": {"expects": ["cost_a", "cost_b", "logp_a", "logp_b"], "returns": "scalar"}}, "fitness": {"hf_like_score": 8.402042388916016, "validation_objective": 8.402042388916016, "generalization_penalty": 0.0, "generalization_objectives": {"20": 4.001859188079834}, "train_score_mean": 10.483344297409058, "train_loss_mean": 0.6032283544540405, "pair_count": 12902390, "config": {"hf": {"problem": "tsp", "hf_steps": 100, "train_problem_size": 100, "valid_problem_sizes": [20], "train_batch_size": 64, "pomo_size": 64, "learning_rate": 0.0003, "weight_decay": 1e-06, "alpha": 0.05, "device": "cuda", "seed": 1234, "num_validation_episodes": 128, "validation_batch_size": 64, "generalization_penalty_weight": 1.0, "pool_version": "v0"}, "free_loss": {"f1_steps": 100, "f2_steps": 100, "f3_enabled": false}}, "loss_ir": {"name": "Rank-Modulated Sigmoid-Hinge Loss", "intuition": "This child loss creates a hybrid that modulates a stable hinge-like error term using both probabilistic and rank-based signals, borrowing from both parents and introducing a new coupling mechanism.\n\nFrom Parent 0 (Adaptive Hinge-Sigmoid Hybrid Loss), we inherit the core structure of a stable, smooth hinge loss. Specifically, we use a `preference_error = softplus(dynamic_margin - logp_diff)` term, which penalizes the model only when its preference `logp_diff` fails to exceed a `dynamic_margin`. We also inherit the `dynamic_margin` itself, which is computed as `beta * tanh(zscore(rank_gap(...)))`, providing a bounded, rank-sensitive target for the model's preference.\n\nFrom Parent 1 (Adaptively Weighted Hinge-Sigmoid Loss), we inherit the idea of using a `sigmoid`-based adaptive weight to modulate the loss. However, instead of basing the weight on the margin as in the parent, we base it on the model's own confidence, `logp_diff`.\n\nAs a new coupling idea, we introduce a 'rank-gap scaling' factor, `rank_gap_scale`, similar to the one in Parent 1, but we apply it *inside* the loss computation rather than as a final multiplier. The final loss is `rank_gap_scale * sigmoid(gamma * preference_error)`. This structure has several benefits: the `sigmoid` squashes the error into a (0, 1) range, preventing extreme loss values and providing stable gradients. The `rank_gap_scale` then ensures that pairs with larger cost differences (and thus larger rank gaps) contribute more significantly to the final loss, effectively prioritizing the most important preference pairs. This combines the stability of a sigmoid with the calibration of rank-gap scaling.\n\nWhen `cost(a) < cost(b)`, the `dynamic_margin` is negative. The loss encourages `logp_diff` to be positive and greater than this margin to drive the `preference_error` to zero, which in turn minimizes the overall loss.", "hyperparams": {"beta": 1.0, "gamma": 1.0}, "operators_used": ["rank_gap", "zscore", "tanh", "softplus", "sigmoid"]}}, "better_than_baseline": true}
{"generation": 5, "index": 6, "ir": {"name": "Rank-Gated Sigmoid-Hinge Loss", "intuition": "This loss function creates a hybrid that gates between a sigmoid-like probabilistic loss and a hinge-like margin loss, using the rank of the cost difference as the gating mechanism. It aims to apply a stricter margin-based penalty for clear-cut preference pairs (large rank gaps) and a softer probabilistic penalty for ambiguous pairs (small rank gaps).\n\nFrom Parent 0 ('Adaptive Hinge-Sigmoid Hybrid Loss'), we inherit the core idea of using a dynamic margin based on the standardized rank gap (`dynamic_margin = beta * tanh(zscore(rank_gap(...)))`). We also inherit its use of `softplus` to form a smooth, non-negative hinge-like preference error: `hinge_error = softplus(dynamic_margin - logp_diff)`.\n\nFrom Parent 1 ('Adaptively Weighted Hinge-Sigmoid Loss with Rank-Gap Scaling'), we inherit the idea of using `sigmoid` to create an adaptive weight. However, instead of using it to blend two loss terms, we use it as a gating function.\n\nAs a new coupling idea, we introduce a 'rank gate' (`rank_gate = sigmoid(gamma * cost_gap_zscored)`). This gate smoothly transitions between two loss regimes based on the standardized rank gap. For pairs with large positive rank gaps (clear win for `a`), the gate approaches 1, emphasizing the `hinge_error`. For pairs with large negative rank gaps (clear loss for `a`), the gate approaches 0. For ambiguous pairs around a rank gap of zero, the gate is near 0.5. The final loss is a gated combination: `rank_gate * hinge_error + (1 - rank_gate) * sigmoid_error`, where `sigmoid_error` is a simple `logsigmoid(-logp_diff)`. This structure adaptively chooses the most appropriate loss formulation based on the relative importance of the preference pair within the batch.\n\nWhen `cost(a) < cost(b)`, `cost_gap_zscored` is positive. The `rank_gate` will be > 0.5, emphasizing the `hinge_error` term. This term encourages `logp_diff` to be positive and larger than the `dynamic_margin`, correctly preferring `a`. The `sigmoid_error` term also encourages a positive `logp_diff`.", "pseudocode": "1. Compute the log-probability difference: logp_diff = logp_a - logp_b.\n2. Compute the signed, batch-relative rank gap: cost_gap_signed = rank_gap(cost_a, cost_b).\n3. Standardize the rank gap across the batch: cost_gap_zscored = zscore(cost_gap_signed).\n4. Create a bounded dynamic margin from the standardized rank gap (inherited from Parent 0): dynamic_margin = beta * tanh(cost_gap_zscored).\n5. Compute a smooth hinge-like error term (inherited from Parent 0): hinge_error = softplus(dynamic_margin - logp_diff).\n6. Compute a simple sigmoid-based probabilistic error term: sigmoid_error = logsigmoid(-logp_diff).\n7. Create a rank-based gating value using the standardized rank gap (new coupling idea, structure from Parent 1): rank_gate = sigmoid(gamma * cost_gap_zscored).\n8. Combine the hinge and sigmoid errors using the rank gate (new coupling idea): loss = rank_gate * hinge_error + (1.0 - rank_gate) * sigmoid_error.", "hyperparams": {"beta": 1.0, "gamma": 2.0}, "operators_used": ["rank_gap", "zscore", "tanh", "softplus", "logsigmoid", "sigmoid"], "implementation_hint": {"expects": ["cost_a", "cost_b", "logp_a", "logp_b"], "returns": "scalar"}}, "fitness": {"hf_like_score": 8.471590995788574, "validation_objective": 8.471590995788574, "generalization_penalty": 0.0, "generalization_objectives": {"20": 3.993884325027466}, "train_score_mean": 10.518161640167236, "train_loss_mean": 0.6015181601047516, "pair_count": 12902389, "config": {"hf": {"problem": "tsp", "hf_steps": 100, "train_problem_size": 100, "valid_problem_sizes": [20], "train_batch_size": 64, "pomo_size": 64, "learning_rate": 0.0003, "weight_decay": 1e-06, "alpha": 0.05, "device": "cuda", "seed": 1234, "num_validation_episodes": 128, "validation_batch_size": 64, "generalization_penalty_weight": 1.0, "pool_version": "v0"}, "free_loss": {"f1_steps": 100, "f2_steps": 100, "f3_enabled": false}}, "loss_ir": {"name": "Rank-Gated Sigmoid-Hinge Loss", "intuition": "This loss function creates a hybrid that gates between a sigmoid-like probabilistic loss and a hinge-like margin loss, using the rank of the cost difference as the gating mechanism. It aims to apply a stricter margin-based penalty for clear-cut preference pairs (large rank gaps) and a softer probabilistic penalty for ambiguous pairs (small rank gaps).\n\nFrom Parent 0 ('Adaptive Hinge-Sigmoid Hybrid Loss'), we inherit the core idea of using a dynamic margin based on the standardized rank gap (`dynamic_margin = beta * tanh(zscore(rank_gap(...)))`). We also inherit its use of `softplus` to form a smooth, non-negative hinge-like preference error: `hinge_error = softplus(dynamic_margin - logp_diff)`.\n\nFrom Parent 1 ('Adaptively Weighted Hinge-Sigmoid Loss with Rank-Gap Scaling'), we inherit the idea of using `sigmoid` to create an adaptive weight. However, instead of using it to blend two loss terms, we use it as a gating function.\n\nAs a new coupling idea, we introduce a 'rank gate' (`rank_gate = sigmoid(gamma * cost_gap_zscored)`). This gate smoothly transitions between two loss regimes based on the standardized rank gap. For pairs with large positive rank gaps (clear win for `a`), the gate approaches 1, emphasizing the `hinge_error`. For pairs with large negative rank gaps (clear loss for `a`), the gate approaches 0. For ambiguous pairs around a rank gap of zero, the gate is near 0.5. The final loss is a gated combination: `rank_gate * hinge_error + (1 - rank_gate) * sigmoid_error`, where `sigmoid_error` is a simple `logsigmoid(-logp_diff)`. This structure adaptively chooses the most appropriate loss formulation based on the relative importance of the preference pair within the batch.\n\nWhen `cost(a) < cost(b)`, `cost_gap_zscored` is positive. The `rank_gate` will be > 0.5, emphasizing the `hinge_error` term. This term encourages `logp_diff` to be positive and larger than the `dynamic_margin`, correctly preferring `a`. The `sigmoid_error` term also encourages a positive `logp_diff`.", "hyperparams": {"beta": 1.0, "gamma": 2.0}, "operators_used": ["rank_gap", "zscore", "tanh", "softplus", "logsigmoid", "sigmoid"]}}, "better_than_baseline": false}
{"generation": 5, "index": 7, "ir": {"name": "Rank-Calibrated Sigmoid-Hinge with Adaptive Temperature", "intuition": "This loss function creates a hybrid that is sensitive to both the rank and magnitude of cost differences by blending ideas from its parents and introducing a new coupling mechanism.\n\nFrom Parent 0 (Adaptive Hinge-Sigmoid Hybrid Loss), we inherit the core concept of a dynamic, rank-based margin. This is achieved by computing a standardized rank gap (`cost_gap_zscored = zscore(rank_gap(cost_a, cost_b))`) and passing it through a `tanh` function to create a bounded `dynamic_margin`. This ensures the loss target is relative to the batch distribution of preferences.\n\nFrom Parent 1 (Adaptively Weighted Hinge-Sigmoid Loss), we inherit the idea of using an adaptive weight, but we repurpose it. Instead of weighting two loss terms, we use the `sigmoid(gamma * abs(dynamic_margin))` to create an `adaptive_temperature` that modulates the steepness of the loss function. This makes the loss more decisive for pairs with a clear rank separation (large margin) and more gentle for ambiguous pairs (small margin).\n\nAs a new coupling idea, we embed the hinge-like error term directly inside a `logsigmoid` function, which is a common and stable way to implement a logistic loss. The loss is formulated as `-logsigmoid(adaptive_temperature * (logp_diff - dynamic_margin))`. This structure combines the rank-based margin from Parent 0 with a probabilistic interpretation. The `adaptive_temperature` (inspired by Parent 1) acts as a scaling factor, making the sigmoid function steeper or shallower based on the rank-gap's significance. This ensures the model is penalized more severely for failing to meet a clear, high-rank-gap margin.\n\nWhen `cost(a) < cost(b)`, the `dynamic_margin` becomes negative. The loss encourages `logp_diff` to be positive and larger than this margin, pushing the argument of `logsigmoid` to be positive and thus minimizing the loss. This correctly encourages a preference for `a`.", "pseudocode": "1. Compute the log-probability difference: logp_diff = logp_a - logp_b.\n2. Compute the signed, batch-relative rank gap: cost_gap_signed = rank_gap(cost_a, cost_b).\n3. Standardize the rank gap across the batch: cost_gap_zscored = zscore(cost_gap_signed).\n4. Create a bounded dynamic margin from the standardized rank gap (inherited from Parent 0): dynamic_margin = beta * tanh(cost_gap_zscored).\n5. Create an adaptive temperature based on the margin's magnitude (inspired by Parent 1): adaptive_temperature = 1.0 + gamma * sigmoid(abs(dynamic_margin)).\n6. Compute the final loss by coupling the margin, temperature, and log-probability difference within a stable logsigmoid function (new coupling idea): loss = -logsigmoid(adaptive_temperature * (logp_diff - dynamic_margin)).", "hyperparams": {"beta": 1.5, "gamma": 1.0}, "operators_used": ["rank_gap", "zscore", "tanh", "sigmoid", "logsigmoid"], "implementation_hint": {"expects": ["cost_a", "cost_b", "logp_a", "logp_b"], "returns": "scalar"}}, "fitness": {"hf_like_score": 8.46272897720337, "validation_objective": 8.46272897720337, "generalization_penalty": 0.0, "generalization_objectives": {"20": 3.995147943496704}, "train_score_mean": 10.500747518539429, "train_loss_mean": 0.6014169943332672, "pair_count": 12902394, "config": {"hf": {"problem": "tsp", "hf_steps": 100, "train_problem_size": 100, "valid_problem_sizes": [20], "train_batch_size": 64, "pomo_size": 64, "learning_rate": 0.0003, "weight_decay": 1e-06, "alpha": 0.05, "device": "cuda", "seed": 1234, "num_validation_episodes": 128, "validation_batch_size": 64, "generalization_penalty_weight": 1.0, "pool_version": "v0"}, "free_loss": {"f1_steps": 100, "f2_steps": 100, "f3_enabled": false}}, "loss_ir": {"name": "Rank-Calibrated Sigmoid-Hinge with Adaptive Temperature", "intuition": "This loss function creates a hybrid that is sensitive to both the rank and magnitude of cost differences by blending ideas from its parents and introducing a new coupling mechanism.\n\nFrom Parent 0 (Adaptive Hinge-Sigmoid Hybrid Loss), we inherit the core concept of a dynamic, rank-based margin. This is achieved by computing a standardized rank gap (`cost_gap_zscored = zscore(rank_gap(cost_a, cost_b))`) and passing it through a `tanh` function to create a bounded `dynamic_margin`. This ensures the loss target is relative to the batch distribution of preferences.\n\nFrom Parent 1 (Adaptively Weighted Hinge-Sigmoid Loss), we inherit the idea of using an adaptive weight, but we repurpose it. Instead of weighting two loss terms, we use the `sigmoid(gamma * abs(dynamic_margin))` to create an `adaptive_temperature` that modulates the steepness of the loss function. This makes the loss more decisive for pairs with a clear rank separation (large margin) and more gentle for ambiguous pairs (small margin).\n\nAs a new coupling idea, we embed the hinge-like error term directly inside a `logsigmoid` function, which is a common and stable way to implement a logistic loss. The loss is formulated as `-logsigmoid(adaptive_temperature * (logp_diff - dynamic_margin))`. This structure combines the rank-based margin from Parent 0 with a probabilistic interpretation. The `adaptive_temperature` (inspired by Parent 1) acts as a scaling factor, making the sigmoid function steeper or shallower based on the rank-gap's significance. This ensures the model is penalized more severely for failing to meet a clear, high-rank-gap margin.\n\nWhen `cost(a) < cost(b)`, the `dynamic_margin` becomes negative. The loss encourages `logp_diff` to be positive and larger than this margin, pushing the argument of `logsigmoid` to be positive and thus minimizing the loss. This correctly encourages a preference for `a`.", "hyperparams": {"beta": 1.5, "gamma": 1.0}, "operators_used": ["rank_gap", "zscore", "tanh", "sigmoid", "logsigmoid"]}}, "better_than_baseline": false}
{"generation": 5, "index": 8, "ir": {"name": "Rank-Modulated Hinge-Sigmoid Loss", "intuition": "This child loss creates a hybrid that blends the hinge-like error structure from Parent 0 with the adaptive sigmoid-based weighting from Parent 1, introducing a new coupling that directly modulates the hinge margin with the raw rank gap.\n\nFrom Parent 0 (Adaptive Hinge-Sigmoid Hybrid Loss), we inherit the core structure of a smooth, non-negative hinge loss: `preference_error = softplus(margin - logp_diff)`. This ensures the loss is zero when the model's preference `logp_diff` is correctly aligned and sufficiently confident.\n\nFrom Parent 1 (Adaptively Weighted Hinge-Sigmoid Loss with Rank-Gap Scaling), we inherit the adaptive weighting mechanism, `adaptive_alpha = sigmoid(gamma * abs(logp_diff))`. This weight, which is based on the model's own preference confidence, is used to scale the final loss, making it more sensitive to errors on confident (but wrong) predictions.\n\nWe introduce two new coupling ideas. First, we create a composite margin, `composite_margin`, that is a sum of the standardized rank-based margin from Parent 0 and a new term directly proportional to the raw, un-normalized rank gap (`rho * cost_gap_signed`). This makes the margin more sensitive to the absolute difference in ranks, not just the relative ordering. Second, we combine the `preference_error` (from Parent 0's structure) and the `adaptive_alpha` (from Parent 1's idea) into a single, stable logarithmic form: `log(1.0 + adaptive_alpha * preference_error)`. This elegantly fuses the hinge error with the adaptive sigmoid weight.\n\nWhen `cost(a) < cost(b)`, both `cost_gap_signed` and `zscored_gap` are negative, resulting in a negative `composite_margin`. The loss encourages `logp_diff` to be positive and larger than this negative margin to drive `preference_error` to zero, correctly preferring `a`.", "pseudocode": "1. Compute the log-probability difference: logp_diff = logp_a - logp_b.\n2. Compute the signed, batch-relative rank gap: cost_gap_signed = rank_gap(cost_a, cost_b).\n3. Standardize the rank gap across the batch: zscored_gap = zscore(cost_gap_signed).\n4. Compute the standardized part of the margin (inherited from Parent 0): standardized_margin = beta * tanh(zscored_gap).\n5. Create a composite margin by coupling the standardized margin with the raw rank gap (new coupling idea): composite_margin = standardized_margin + rho * cost_gap_signed.\n6. Compute the preference error using a smooth hinge-like term (inherited from Parent 0): preference_error = softplus(composite_margin - logp_diff).\n7. Create an adaptive weight based on the model's preference confidence (inherited from Parent 1): adaptive_alpha = sigmoid(gamma * abs(logp_diff)).\n8. Combine the preference error and the adaptive weight in a stable logarithmic form (new coupling idea): loss = log(1.0 + adaptive_alpha * preference_error).", "hyperparams": {"beta": 1.0, "gamma": 1.0, "rho": 0.1}, "operators_used": ["rank_gap", "zscore", "tanh", "softplus", "sigmoid", "log"], "implementation_hint": {"expects": ["cost_a", "cost_b", "logp_a", "logp_b"], "returns": "scalar"}}, "fitness": {"hf_like_score": 8.521687030792236, "validation_objective": 8.521687030792236, "generalization_penalty": 0.0, "generalization_objectives": {"20": 4.004365563392639}, "train_score_mean": 10.506511516571045, "train_loss_mean": 0.5997651040554046, "pair_count": 12902389, "config": {"hf": {"problem": "tsp", "hf_steps": 100, "train_problem_size": 100, "valid_problem_sizes": [20], "train_batch_size": 64, "pomo_size": 64, "learning_rate": 0.0003, "weight_decay": 1e-06, "alpha": 0.05, "device": "cuda", "seed": 1234, "num_validation_episodes": 128, "validation_batch_size": 64, "generalization_penalty_weight": 1.0, "pool_version": "v0"}, "free_loss": {"f1_steps": 100, "f2_steps": 100, "f3_enabled": false}}, "loss_ir": {"name": "Rank-Modulated Hinge-Sigmoid Loss", "intuition": "This child loss creates a hybrid that blends the hinge-like error structure from Parent 0 with the adaptive sigmoid-based weighting from Parent 1, introducing a new coupling that directly modulates the hinge margin with the raw rank gap.\n\nFrom Parent 0 (Adaptive Hinge-Sigmoid Hybrid Loss), we inherit the core structure of a smooth, non-negative hinge loss: `preference_error = softplus(margin - logp_diff)`. This ensures the loss is zero when the model's preference `logp_diff` is correctly aligned and sufficiently confident.\n\nFrom Parent 1 (Adaptively Weighted Hinge-Sigmoid Loss with Rank-Gap Scaling), we inherit the adaptive weighting mechanism, `adaptive_alpha = sigmoid(gamma * abs(logp_diff))`. This weight, which is based on the model's own preference confidence, is used to scale the final loss, making it more sensitive to errors on confident (but wrong) predictions.\n\nWe introduce two new coupling ideas. First, we create a composite margin, `composite_margin`, that is a sum of the standardized rank-based margin from Parent 0 and a new term directly proportional to the raw, un-normalized rank gap (`rho * cost_gap_signed`). This makes the margin more sensitive to the absolute difference in ranks, not just the relative ordering. Second, we combine the `preference_error` (from Parent 0's structure) and the `adaptive_alpha` (from Parent 1's idea) into a single, stable logarithmic form: `log(1.0 + adaptive_alpha * preference_error)`. This elegantly fuses the hinge error with the adaptive sigmoid weight.\n\nWhen `cost(a) < cost(b)`, both `cost_gap_signed` and `zscored_gap` are negative, resulting in a negative `composite_margin`. The loss encourages `logp_diff` to be positive and larger than this negative margin to drive `preference_error` to zero, correctly preferring `a`.", "hyperparams": {"beta": 1.0, "gamma": 1.0, "rho": 0.1}, "operators_used": ["rank_gap", "zscore", "tanh", "softplus", "sigmoid", "log"]}}, "better_than_baseline": false}
{"generation": 5, "index": 9, "ir": {"name": "Rank-Calibrated Dynamic Hinge Loss", "intuition": "This child loss creates a hybrid that inherits the dynamic margin from both parents and the adaptive temperature/weighting concept, but combines them in a novel way. The core idea is to use two different signalsthe standardized rank gap and the raw cost differenceto separately control the *target margin* and the *loss penalty*, respectively.\n\nFrom both parents, we inherit the use of a dynamic margin based on the standardized, bounded rank gap (`dynamic_margin = beta * tanh(zscore(rank_gap(...)))`). This ensures the preference target is relative to the batch distribution of cost differences, making it robust to outliers.\n\nFrom Parent 0 (Adaptive Hinge-Sigmoid Hybrid Loss), we inherit the adaptive temperature (`adaptive_gamma`) which is based on the magnitude of the dynamic margin. However, we repurpose it.\n\nFrom Parent 1 (Adaptively Weighted Hinge-Sigmoid Loss), we are inspired by its use of an adaptive weight (`adaptive_alpha`) and the final scaling of the loss by a cost-related term (`rank_gap_scale`).\n\nAs a new coupling idea, we create a 'dynamic penalty scale' that is a function of the *un-normalized* cost difference (`cost_diff = cost_a - cost_b`). This scale is computed as `dynamic_penalty = 1.0 + gamma * softplus(cost_diff)`. When `cost_a > cost_b`, this penalty term becomes large, increasing the loss for incorrect preferences on high-stakes pairs. When `cost_a < cost_b`, `cost_diff` is negative, and `softplus` makes the penalty term close to 1.0, applying a baseline penalty. The final loss is a product of this new dynamic penalty and the familiar hinge term `softplus(dynamic_margin - logp_diff)`. This structure cleanly separates the roles: the *rank gap* sets the relative preference target, while the *raw cost difference* sets the magnitude of the penalty for failing to meet that target.\n\nWhen `cost(a) < cost(b)`, `dynamic_margin` is negative, and `cost_diff` is negative. The loss encourages `logp_diff` to be positive and larger than the negative margin to drive the `softplus` term to zero. The `dynamic_penalty` term will be close to 1.0, applying a standard penalty for the mistake.", "pseudocode": "1. Compute the log-probability difference: logp_diff = logp_a - logp_b.\n2. Compute the raw cost difference: cost_diff = cost_a - cost_b.\n3. Compute the signed, batch-relative rank gap: cost_gap_signed = rank_gap(cost_a, cost_b).\n4. Standardize the rank gap across the batch: cost_gap_zscored = zscore(cost_gap_signed).\n5. Create a bounded dynamic margin from the standardized rank gap (inherited from both parents): dynamic_margin = beta * tanh(cost_gap_zscored).\n6. Compute the core hinge error term based on the model's preference vs. the dynamic margin: hinge_error = softplus(dynamic_margin - logp_diff).\n7. Create a new dynamic penalty scale based on the un-normalized cost difference (new coupling idea): dynamic_penalty = 1.0 + gamma * softplus(cost_diff).\n8. Compute the final loss by scaling the hinge error with the dynamic penalty: loss = dynamic_penalty * hinge_error.", "hyperparams": {"beta": 1.5, "gamma": 0.5}, "operators_used": ["rank_gap", "zscore", "tanh", "softplus"], "implementation_hint": {"expects": ["cost_a", "cost_b", "logp_a", "logp_b"], "returns": "scalar"}}, "fitness": {"hf_like_score": 8.472796440124512, "validation_objective": 8.472796440124512, "generalization_penalty": 0.0, "generalization_objectives": {"20": 4.019471049308777}, "train_score_mean": 10.491016750335694, "train_loss_mean": 0.5988376247882843, "pair_count": 12902387, "config": {"hf": {"problem": "tsp", "hf_steps": 100, "train_problem_size": 100, "valid_problem_sizes": [20], "train_batch_size": 64, "pomo_size": 64, "learning_rate": 0.0003, "weight_decay": 1e-06, "alpha": 0.05, "device": "cuda", "seed": 1234, "num_validation_episodes": 128, "validation_batch_size": 64, "generalization_penalty_weight": 1.0, "pool_version": "v0"}, "free_loss": {"f1_steps": 100, "f2_steps": 100, "f3_enabled": false}}, "loss_ir": {"name": "Rank-Calibrated Dynamic Hinge Loss", "intuition": "This child loss creates a hybrid that inherits the dynamic margin from both parents and the adaptive temperature/weighting concept, but combines them in a novel way. The core idea is to use two different signalsthe standardized rank gap and the raw cost differenceto separately control the *target margin* and the *loss penalty*, respectively.\n\nFrom both parents, we inherit the use of a dynamic margin based on the standardized, bounded rank gap (`dynamic_margin = beta * tanh(zscore(rank_gap(...)))`). This ensures the preference target is relative to the batch distribution of cost differences, making it robust to outliers.\n\nFrom Parent 0 (Adaptive Hinge-Sigmoid Hybrid Loss), we inherit the adaptive temperature (`adaptive_gamma`) which is based on the magnitude of the dynamic margin. However, we repurpose it.\n\nFrom Parent 1 (Adaptively Weighted Hinge-Sigmoid Loss), we are inspired by its use of an adaptive weight (`adaptive_alpha`) and the final scaling of the loss by a cost-related term (`rank_gap_scale`).\n\nAs a new coupling idea, we create a 'dynamic penalty scale' that is a function of the *un-normalized* cost difference (`cost_diff = cost_a - cost_b`). This scale is computed as `dynamic_penalty = 1.0 + gamma * softplus(cost_diff)`. When `cost_a > cost_b`, this penalty term becomes large, increasing the loss for incorrect preferences on high-stakes pairs. When `cost_a < cost_b`, `cost_diff` is negative, and `softplus` makes the penalty term close to 1.0, applying a baseline penalty. The final loss is a product of this new dynamic penalty and the familiar hinge term `softplus(dynamic_margin - logp_diff)`. This structure cleanly separates the roles: the *rank gap* sets the relative preference target, while the *raw cost difference* sets the magnitude of the penalty for failing to meet that target.\n\nWhen `cost(a) < cost(b)`, `dynamic_margin` is negative, and `cost_diff` is negative. The loss encourages `logp_diff` to be positive and larger than the negative margin to drive the `softplus` term to zero. The `dynamic_penalty` term will be close to 1.0, applying a standard penalty for the mistake.", "hyperparams": {"beta": 1.5, "gamma": 0.5}, "operators_used": ["rank_gap", "zscore", "tanh", "softplus"]}}, "better_than_baseline": false}
{"generation": 6, "index": 0, "ir": {"name": "Adaptive Margin Sigmoid Loss with Rank-Gap Annealing", "intuition": "This child loss creates a novel structure by combining a dynamic margin with an adaptive temperature inside a single, stable sigmoid framework, and introduces a new annealing mechanism to control the loss's sharpness.\n\nFrom Parent 0 (Adaptive Hinge-Sigmoid Hybrid Loss), we inherit the core concept of a `dynamic_margin` derived from the standardized rank gap (`beta * tanh(zscore(rank_gap(...)))`). This allows the required preference margin to adapt based on the relative ranking of costs within a batch.\n\nFrom Parent 1 (Adaptively Weighted Hinge-Sigmoid Loss), we inherit the idea of an adaptive scaling factor, here called `adaptive_gamma`, which is a function of the margin's magnitude. This makes the loss more sensitive to pairs with a clear cost separation (large margin) and less sensitive to ambiguous pairs (small margin).\n\nWe introduce two new coupling ideas. First, instead of using a hinge-like structure (`softplus(margin - logp_diff)`), we directly embed the `dynamic_margin` and `adaptive_gamma` into a single, unified sigmoid expression: `logsigmoid(adaptive_gamma * (logp_diff - dynamic_margin))`. This elegantly combines the margin from Parent 0 and the adaptive scaling from Parent 1 into a classic probabilistic preference loss formulation. The loss is the negative of this value, which penalizes the model when the log-probability difference `logp_diff` fails to exceed the `dynamic_margin`. Second, we introduce a `rank_gap_annealing` factor. This factor, derived from the non-standardized rank gap, acts as a temperature term *inside* the `tanh` function that creates the margin. For pairs with a small rank gap, it sharpens the `tanh` curve, making the margin more sensitive to small changes in the z-scored rank gap. For pairs with a large rank gap, it flattens the `tanh` curve, creating a more saturated and stable margin. This annealing helps stabilize the margin calculation and focuses the loss's precision on finely ranked pairs.\n\nWhen `cost(a) < cost(b)`, `rank_gap` is negative, making `dynamic_margin` negative. The loss encourages `logp_a - logp_b` to be positive and greater than this negative margin, correctly preferring `a`.", "pseudocode": "1. Compute the log-probability difference: logp_diff = logp_a - logp_b.\n2. Compute the signed, batch-relative rank gap: cost_gap_signed = rank_gap(cost_a, cost_b).\n3. Standardize the rank gap across the batch: cost_gap_zscored = zscore(cost_gap_signed).\n4. Create a rank-gap annealing factor to control margin sharpness (new coupling idea): rank_gap_annealing = 1.0 / (1.0 + 0.1 * softplus(abs(cost_gap_signed))).\n5. Create a bounded dynamic margin, annealed by the rank gap (inherited from Parent 0, modified with new idea): dynamic_margin = beta * tanh(rank_gap_annealing * cost_gap_zscored).\n6. Create an adaptive temperature based on the margin's magnitude (inherited from Parent 1): adaptive_gamma = 1.0 + gamma * sigmoid(abs(dynamic_margin)).\n7. Combine the log-probability difference, dynamic margin, and adaptive temperature into a single probabilistic term (new coupling idea): preference_likelihood = logsigmoid(adaptive_gamma * (logp_diff - dynamic_margin)).\n8. The final loss is the negative of this log-likelihood: loss = -preference_likelihood.", "hyperparams": {"beta": 1.5, "gamma": 1.0}, "operators_used": ["rank_gap", "zscore", "softplus", "tanh", "sigmoid", "logsigmoid"], "implementation_hint": {"expects": ["cost_a", "cost_b", "logp_a", "logp_b"], "returns": "scalar"}}, "fitness": {"hf_like_score": 8.423751831054688, "validation_objective": 8.423751831054688, "generalization_penalty": 0.0, "generalization_objectives": {"20": 4.007841348648071}, "train_score_mean": 10.472060375213623, "train_loss_mean": 0.6022524791955948, "pair_count": 12902385, "config": {"hf": {"problem": "tsp", "hf_steps": 100, "train_problem_size": 100, "valid_problem_sizes": [20], "train_batch_size": 64, "pomo_size": 64, "learning_rate": 0.0003, "weight_decay": 1e-06, "alpha": 0.05, "device": "cuda", "seed": 1234, "num_validation_episodes": 128, "validation_batch_size": 64, "generalization_penalty_weight": 1.0, "pool_version": "v0"}, "free_loss": {"f1_steps": 100, "f2_steps": 100, "f3_enabled": false}}, "loss_ir": {"name": "Adaptive Margin Sigmoid Loss with Rank-Gap Annealing", "intuition": "This child loss creates a novel structure by combining a dynamic margin with an adaptive temperature inside a single, stable sigmoid framework, and introduces a new annealing mechanism to control the loss's sharpness.\n\nFrom Parent 0 (Adaptive Hinge-Sigmoid Hybrid Loss), we inherit the core concept of a `dynamic_margin` derived from the standardized rank gap (`beta * tanh(zscore(rank_gap(...)))`). This allows the required preference margin to adapt based on the relative ranking of costs within a batch.\n\nFrom Parent 1 (Adaptively Weighted Hinge-Sigmoid Loss), we inherit the idea of an adaptive scaling factor, here called `adaptive_gamma`, which is a function of the margin's magnitude. This makes the loss more sensitive to pairs with a clear cost separation (large margin) and less sensitive to ambiguous pairs (small margin).\n\nWe introduce two new coupling ideas. First, instead of using a hinge-like structure (`softplus(margin - logp_diff)`), we directly embed the `dynamic_margin` and `adaptive_gamma` into a single, unified sigmoid expression: `logsigmoid(adaptive_gamma * (logp_diff - dynamic_margin))`. This elegantly combines the margin from Parent 0 and the adaptive scaling from Parent 1 into a classic probabilistic preference loss formulation. The loss is the negative of this value, which penalizes the model when the log-probability difference `logp_diff` fails to exceed the `dynamic_margin`. Second, we introduce a `rank_gap_annealing` factor. This factor, derived from the non-standardized rank gap, acts as a temperature term *inside* the `tanh` function that creates the margin. For pairs with a small rank gap, it sharpens the `tanh` curve, making the margin more sensitive to small changes in the z-scored rank gap. For pairs with a large rank gap, it flattens the `tanh` curve, creating a more saturated and stable margin. This annealing helps stabilize the margin calculation and focuses the loss's precision on finely ranked pairs.\n\nWhen `cost(a) < cost(b)`, `rank_gap` is negative, making `dynamic_margin` negative. The loss encourages `logp_a - logp_b` to be positive and greater than this negative margin, correctly preferring `a`.", "hyperparams": {"beta": 1.5, "gamma": 1.0}, "operators_used": ["rank_gap", "zscore", "softplus", "tanh", "sigmoid", "logsigmoid"]}}, "better_than_baseline": true}
{"generation": 6, "index": 1, "ir": {"name": "Rank-Modulated Adaptive Sigmoid-Hinge Loss", "intuition": "This loss function creates a hybrid that modulates a stable hinge-loss error with an adaptive sigmoid-based weight, introducing a new coupling mechanism where the rank gap directly influences the hinge margin itself.\n\nFrom Parent 0 (Adaptive Hinge-Sigmoid Hybrid Loss), we inherit the core structure of a stable, logarithmic preference error: `log(1.0 + scaled_error)`. This structure, derived from `softplus(margin - logp_diff)`, ensures the loss is zero when the model's preference is correct and provides stable gradients.\n\nFrom Parent 1 (Adaptively Weighted Hinge-Sigmoid Loss with Rank-Gap Scaling), we inherit the idea of an adaptive weight, `adaptive_alpha`, calculated using `sigmoid(gamma * abs(margin))`. This weight scales the loss contribution, making it more sensitive to pairs with a clear preference margin.\n\nWe introduce two new coupling ideas. First, we directly incorporate the standardized rank gap into the margin calculation itself, creating a `rank_modulated_margin = beta * tanh(cost_gap_zscored) + cost_gap_zscored`. This makes the margin more responsive to the magnitude of the rank difference, combining the bounded `tanh` term with an unbounded linear term. Second, we use the adaptive weight `adaptive_alpha` to modulate the preference error *before* it enters the final logarithmic function, as in `log(1.0 + adaptive_alpha * preference_error)`. This creates a unified loss where the weight directly controls the magnitude of the error signal, rather than combining two separate loss terms.\n\nWhen `cost(a) < cost(b)`, the `cost_gap_zscored` is negative, making the `rank_modulated_margin` also negative. The loss encourages `logp_diff` to be positive and larger than this negative margin to drive the `preference_error` to zero, correctly preferring `a`.", "pseudocode": "1. Compute the log-probability difference: logp_diff = logp_a - logp_b.\n2. Compute the signed, batch-relative rank gap: cost_gap_signed = rank_gap(cost_a, cost_b).\n3. Standardize the rank gap across the batch: cost_gap_zscored = zscore(cost_gap_signed).\n4. Create a rank-modulated margin by coupling the standardized rank gap with a bounded term (new coupling idea): rank_modulated_margin = beta * tanh(cost_gap_zscored) + cost_gap_zscored.\n5. Create an adaptive weight based on the margin's magnitude (inherited from Parent 1): adaptive_alpha = sigmoid(gamma * abs(rank_modulated_margin)).\n6. Compute the preference error using a smooth hinge-like term (structure inherited from Parent 0): preference_error = softplus(rank_modulated_margin - logp_diff).\n7. Couple the preference error with the adaptive weight and apply a stable logarithmic function (structure from Parent 0, coupling from Parent 1): loss = log(1.0 + adaptive_alpha * preference_error).", "hyperparams": {"beta": 1.0, "gamma": 1.0}, "operators_used": ["rank_gap", "zscore", "tanh", "sigmoid", "softplus", "log"], "implementation_hint": {"expects": ["cost_a", "cost_b", "logp_a", "logp_b"], "returns": "scalar"}}, "fitness": {"hf_like_score": 8.465382099151611, "validation_objective": 8.465382099151611, "generalization_penalty": 0.0, "generalization_objectives": {"20": 4.00524377822876}, "train_score_mean": 10.510867586135864, "train_loss_mean": 0.6003744715452194, "pair_count": 12902392, "config": {"hf": {"problem": "tsp", "hf_steps": 100, "train_problem_size": 100, "valid_problem_sizes": [20], "train_batch_size": 64, "pomo_size": 64, "learning_rate": 0.0003, "weight_decay": 1e-06, "alpha": 0.05, "device": "cuda", "seed": 1234, "num_validation_episodes": 128, "validation_batch_size": 64, "generalization_penalty_weight": 1.0, "pool_version": "v0"}, "free_loss": {"f1_steps": 100, "f2_steps": 100, "f3_enabled": false}}, "loss_ir": {"name": "Rank-Modulated Adaptive Sigmoid-Hinge Loss", "intuition": "This loss function creates a hybrid that modulates a stable hinge-loss error with an adaptive sigmoid-based weight, introducing a new coupling mechanism where the rank gap directly influences the hinge margin itself.\n\nFrom Parent 0 (Adaptive Hinge-Sigmoid Hybrid Loss), we inherit the core structure of a stable, logarithmic preference error: `log(1.0 + scaled_error)`. This structure, derived from `softplus(margin - logp_diff)`, ensures the loss is zero when the model's preference is correct and provides stable gradients.\n\nFrom Parent 1 (Adaptively Weighted Hinge-Sigmoid Loss with Rank-Gap Scaling), we inherit the idea of an adaptive weight, `adaptive_alpha`, calculated using `sigmoid(gamma * abs(margin))`. This weight scales the loss contribution, making it more sensitive to pairs with a clear preference margin.\n\nWe introduce two new coupling ideas. First, we directly incorporate the standardized rank gap into the margin calculation itself, creating a `rank_modulated_margin = beta * tanh(cost_gap_zscored) + cost_gap_zscored`. This makes the margin more responsive to the magnitude of the rank difference, combining the bounded `tanh` term with an unbounded linear term. Second, we use the adaptive weight `adaptive_alpha` to modulate the preference error *before* it enters the final logarithmic function, as in `log(1.0 + adaptive_alpha * preference_error)`. This creates a unified loss where the weight directly controls the magnitude of the error signal, rather than combining two separate loss terms.\n\nWhen `cost(a) < cost(b)`, the `cost_gap_zscored` is negative, making the `rank_modulated_margin` also negative. The loss encourages `logp_diff` to be positive and larger than this negative margin to drive the `preference_error` to zero, correctly preferring `a`.", "hyperparams": {"beta": 1.0, "gamma": 1.0}, "operators_used": ["rank_gap", "zscore", "tanh", "sigmoid", "softplus", "log"]}}, "better_than_baseline": false}
{"generation": 6, "index": 2, "ir": {"name": "Rank-Calibrated Log-Sigmoid with Adaptive Hinge Margin", "intuition": "This loss function creates a hybrid that blends the probabilistic log-sigmoid framework with an adaptive hinge-like margin, introducing a new coupling mechanism for stability and calibration.\n\nFrom Parent 0 ('Adaptive Hinge-Sigmoid Hybrid Loss'), we inherit the core idea of a dynamic margin that adapts based on the rank of the costs. Specifically, we use a `dynamic_margin` derived from the `tanh` of the `zscore` of the rank gap. This makes the required preference margin larger for pairs with a more significant cost difference in the batch context.\n\nFrom Parent 1 ('Adaptively Weighted Hinge-Sigmoid Loss with Rank-Gap Scaling'), we inherit the concept of adaptive weighting. However, instead of using the weight to mix two losses, we use `adaptive_alpha` (a `sigmoid` of the margin magnitude) to directly scale the entire log-sigmoid term. This makes the loss more influential for pairs with a clear preference signal.\n\nAs a new coupling idea, we embed the dynamic margin directly into a classic log-sigmoid loss structure. The core of the loss is `logsigmoid(logp_diff - dynamic_margin)`. This is a well-understood and stable probabilistic loss. By subtracting the `dynamic_margin`, we are effectively telling the model that `logp_a` must not just be greater than `logp_b`, but greater by at least the margin amount. This combines the probabilistic interpretation of a sigmoid loss with the explicit margin requirement of a hinge loss. The `adaptive_alpha` then weights this entire term. A second new idea is using `softplus(abs(cost_gap_signed))` as an additional loss scaling factor, ensuring that pairs with larger absolute cost differences contribute more, similar to the idea in Parent 1 but implemented more directly.\n\nWhen `cost(a) < cost(b)`, `dynamic_margin` is negative. The loss encourages `logp_diff` to be positive, pushing `logp_diff - dynamic_margin` to be a large positive number, which maximizes `logsigmoid` towards zero.", "pseudocode": "1. Compute the log-probability difference: logp_diff = logp_a - logp_b.\n2. Compute the signed, batch-relative rank gap: cost_gap_signed = rank_gap(cost_a, cost_b).\n3. Standardize the rank gap across the batch: cost_gap_zscored = zscore(cost_gap_signed).\n4. Create a bounded dynamic margin from the standardized rank gap (inherited from Parent 0): dynamic_margin = beta * tanh(cost_gap_zscored).\n5. Create an adaptive weight based on the margin's magnitude (inherited from Parent 1): adaptive_alpha = sigmoid(gamma * abs(dynamic_margin)).\n6. Compute the core preference loss by embedding the dynamic margin into a log-sigmoid function (new coupling idea): preference_term = -logsigmoid(logp_diff - dynamic_margin).\n7. Create a cost-based scaling factor (new stability/coupling idea): cost_scale = softplus(abs(cost_gap_signed)).\n8. Compute the final loss by combining the adaptive weight and cost scaling: loss = cost_scale * adaptive_alpha * preference_term.", "hyperparams": {"beta": 1.5, "gamma": 1.0}, "operators_used": ["rank_gap", "zscore", "tanh", "sigmoid", "logsigmoid", "softplus"], "implementation_hint": {"expects": ["cost_a", "cost_b", "logp_a", "logp_b"], "returns": "scalar"}}, "fitness": {"hf_like_score": 8.447440147399902, "validation_objective": 8.447440147399902, "generalization_penalty": 0.0, "generalization_objectives": {"20": 3.999967575073242}, "train_score_mean": 10.486463136672974, "train_loss_mean": 0.6007978469133377, "pair_count": 12902385, "config": {"hf": {"problem": "tsp", "hf_steps": 100, "train_problem_size": 100, "valid_problem_sizes": [20], "train_batch_size": 64, "pomo_size": 64, "learning_rate": 0.0003, "weight_decay": 1e-06, "alpha": 0.05, "device": "cuda", "seed": 1234, "num_validation_episodes": 128, "validation_batch_size": 64, "generalization_penalty_weight": 1.0, "pool_version": "v0"}, "free_loss": {"f1_steps": 100, "f2_steps": 100, "f3_enabled": false}}, "loss_ir": {"name": "Rank-Calibrated Log-Sigmoid with Adaptive Hinge Margin", "intuition": "This loss function creates a hybrid that blends the probabilistic log-sigmoid framework with an adaptive hinge-like margin, introducing a new coupling mechanism for stability and calibration.\n\nFrom Parent 0 ('Adaptive Hinge-Sigmoid Hybrid Loss'), we inherit the core idea of a dynamic margin that adapts based on the rank of the costs. Specifically, we use a `dynamic_margin` derived from the `tanh` of the `zscore` of the rank gap. This makes the required preference margin larger for pairs with a more significant cost difference in the batch context.\n\nFrom Parent 1 ('Adaptively Weighted Hinge-Sigmoid Loss with Rank-Gap Scaling'), we inherit the concept of adaptive weighting. However, instead of using the weight to mix two losses, we use `adaptive_alpha` (a `sigmoid` of the margin magnitude) to directly scale the entire log-sigmoid term. This makes the loss more influential for pairs with a clear preference signal.\n\nAs a new coupling idea, we embed the dynamic margin directly into a classic log-sigmoid loss structure. The core of the loss is `logsigmoid(logp_diff - dynamic_margin)`. This is a well-understood and stable probabilistic loss. By subtracting the `dynamic_margin`, we are effectively telling the model that `logp_a` must not just be greater than `logp_b`, but greater by at least the margin amount. This combines the probabilistic interpretation of a sigmoid loss with the explicit margin requirement of a hinge loss. The `adaptive_alpha` then weights this entire term. A second new idea is using `softplus(abs(cost_gap_signed))` as an additional loss scaling factor, ensuring that pairs with larger absolute cost differences contribute more, similar to the idea in Parent 1 but implemented more directly.\n\nWhen `cost(a) < cost(b)`, `dynamic_margin` is negative. The loss encourages `logp_diff` to be positive, pushing `logp_diff - dynamic_margin` to be a large positive number, which maximizes `logsigmoid` towards zero.", "hyperparams": {"beta": 1.5, "gamma": 1.0}, "operators_used": ["rank_gap", "zscore", "tanh", "sigmoid", "logsigmoid", "softplus"]}}, "better_than_baseline": false}
{"generation": 6, "index": 3, "ir": {"name": "Rank-Modulated Adaptive Sigmoid Loss", "intuition": "This loss function synthesizes the adaptive temperature from 'Adaptive Hinge-Sigmoid Hybrid Loss' (Parent 0) with the adaptive weighting from 'Adaptively Weighted Hinge-Sigmoid Loss with Rank-Gap Scaling' (Parent 1) into a single, unified sigmoid framework.\n\nFrom Parent 0, we inherit the idea of an `adaptive_gamma` that scales the loss based on the magnitude of the cost difference. This makes the loss more sensitive to large cost differences and more forgiving for smaller ones.\n\nFrom Parent 1, we inherit the `adaptive_alpha` weight, which is a sigmoid function of the margin's magnitude. This weight intelligently modulates the loss, making it stricter for clear-cut preferences.\n\nAs a new coupling idea, we move away from the hinge-like `softplus(margin - logp_diff)` structure and instead use a pure sigmoid loss form: `logsigmoid(argument)`. The core novelty lies in how we construct the `argument`. We create a `base_error` term, `dynamic_margin - logp_diff`, which captures the preference misalignment. This error is then simultaneously scaled by both inherited adaptive terms: `argument = adaptive_gamma * adaptive_alpha * base_error`. This creates a powerful interaction where the loss is most sensitive when both the cost difference is large (high `adaptive_gamma`) and the preference margin is clear (high `adaptive_alpha`). The use of `logsigmoid` provides a stable, probabilistic interpretation, framing the problem as correctly classifying the preference direction.\n\nWhen `cost(a) < cost(b)`, the `dynamic_margin` is negative. The `base_error` becomes `dynamic_margin - logp_diff`. The loss encourages `logp_diff` to be positive, making the argument to `logsigmoid` negative, which in turn minimizes the loss (since `logsigmoid(-inf) = -inf`, and we are maximizing this value or minimizing its negative). This correctly incentivizes preferring `a`.", "pseudocode": "1. Compute the log-probability difference: logp_diff = logp_a - logp_b.\n2. Compute the signed, batch-relative rank gap: cost_gap_signed = rank_gap(cost_a, cost_b).\n3. Standardize the rank gap across the batch: cost_gap_zscored = zscore(cost_gap_signed).\n4. Create a bounded dynamic margin from the standardized rank gap (structure from both parents): dynamic_margin = beta * tanh(cost_gap_zscored).\n5. Create an adaptive temperature based on margin magnitude (inherited from Parent 0): adaptive_gamma = 1.0 + gamma * abs(dynamic_margin).\n6. Create an adaptive weight based on margin magnitude (inherited from Parent 1): adaptive_alpha = sigmoid(alpha * abs(dynamic_margin)).\n7. Define the base preference error term: base_error = dynamic_margin - logp_diff.\n8. Construct the final sigmoid argument by coupling the adaptive terms with the error (new coupling idea): argument = adaptive_gamma * adaptive_alpha * base_error.\n9. Compute the final loss using a stable sigmoid formulation: loss = -logsigmoid(-argument).", "hyperparams": {"beta": 1.0, "gamma": 0.5, "alpha": 1.5}, "operators_used": ["rank_gap", "zscore", "tanh", "sigmoid", "logsigmoid"], "implementation_hint": {"expects": ["cost_a", "cost_b", "logp_a", "logp_b"], "returns": "scalar"}}, "fitness": {"hf_like_score": 23.24793243408203, "validation_objective": 23.24793243408203, "generalization_penalty": 0.0, "generalization_objectives": {"20": 5.206668138504028}, "train_score_mean": 25.720730543136597, "train_loss_mean": 2.1489821577072146, "pair_count": 12902391, "config": {"hf": {"problem": "tsp", "hf_steps": 100, "train_problem_size": 100, "valid_problem_sizes": [20], "train_batch_size": 64, "pomo_size": 64, "learning_rate": 0.0003, "weight_decay": 1e-06, "alpha": 0.05, "device": "cuda", "seed": 1234, "num_validation_episodes": 128, "validation_batch_size": 64, "generalization_penalty_weight": 1.0, "pool_version": "v0"}, "free_loss": {"f1_steps": 100, "f2_steps": 100, "f3_enabled": false}}, "loss_ir": {"name": "Rank-Modulated Adaptive Sigmoid Loss", "intuition": "This loss function synthesizes the adaptive temperature from 'Adaptive Hinge-Sigmoid Hybrid Loss' (Parent 0) with the adaptive weighting from 'Adaptively Weighted Hinge-Sigmoid Loss with Rank-Gap Scaling' (Parent 1) into a single, unified sigmoid framework.\n\nFrom Parent 0, we inherit the idea of an `adaptive_gamma` that scales the loss based on the magnitude of the cost difference. This makes the loss more sensitive to large cost differences and more forgiving for smaller ones.\n\nFrom Parent 1, we inherit the `adaptive_alpha` weight, which is a sigmoid function of the margin's magnitude. This weight intelligently modulates the loss, making it stricter for clear-cut preferences.\n\nAs a new coupling idea, we move away from the hinge-like `softplus(margin - logp_diff)` structure and instead use a pure sigmoid loss form: `logsigmoid(argument)`. The core novelty lies in how we construct the `argument`. We create a `base_error` term, `dynamic_margin - logp_diff`, which captures the preference misalignment. This error is then simultaneously scaled by both inherited adaptive terms: `argument = adaptive_gamma * adaptive_alpha * base_error`. This creates a powerful interaction where the loss is most sensitive when both the cost difference is large (high `adaptive_gamma`) and the preference margin is clear (high `adaptive_alpha`). The use of `logsigmoid` provides a stable, probabilistic interpretation, framing the problem as correctly classifying the preference direction.\n\nWhen `cost(a) < cost(b)`, the `dynamic_margin` is negative. The `base_error` becomes `dynamic_margin - logp_diff`. The loss encourages `logp_diff` to be positive, making the argument to `logsigmoid` negative, which in turn minimizes the loss (since `logsigmoid(-inf) = -inf`, and we are maximizing this value or minimizing its negative). This correctly incentivizes preferring `a`.", "hyperparams": {"beta": 1.0, "gamma": 0.5, "alpha": 1.5}, "operators_used": ["rank_gap", "zscore", "tanh", "sigmoid", "logsigmoid"]}}, "better_than_baseline": false}
{"generation": 6, "index": 4, "ir": {"name": "Rank-Aware Adaptive Hinge-Logmoid Loss", "intuition": "This loss function creates a novel hybrid by combining a dynamic margin from the first parent with an adaptive temperature from the second, and introduces a new coupling mechanism where the preference error is modulated by a logarithmic function before being scaled.\n\nFrom Parent 0 (Adaptive Hinge-Sigmoid Hybrid Loss), we inherit the core concept of a `dynamic_margin` that is computed from a standardized rank gap (`zscore(rank_gap(...))`). This provides a per-pair, batch-relative target for the model's preference score.\n\nFrom Parent 1 (Adaptively Weighted Hinge-Sigmoid Loss), we inherit the idea of an adaptive scaling factor, here called `adaptive_gamma`. This factor is derived from the magnitude of the dynamic margin (`abs(dynamic_margin)`), making the loss more sensitive to pairs with a clearer preference signal as determined by the batch context.\n\nAs a new coupling idea, we introduce a 'logmoid' error term. Instead of using the raw preference error `softplus(dynamic_margin - logp_diff)` directly, we pass it through a `log(1 + x)` function. This creates a `log_error = log(1.0 + softplus(dynamic_margin - logp_diff))`. This new term has the property of being zero when the preference is correct, but it dampens the influence of very large errors, promoting stability. The final loss is then the product of this log-error and the adaptive gamma (`adaptive_gamma * log_error`). This structure ensures that the loss remains sensitive (due to `adaptive_gamma`) while preventing extremely large errors on outlier pairs from dominating the gradient.\n\nWhen `cost(a) < cost(b)`, `dynamic_margin` is negative. The loss encourages `logp_diff` to be positive and larger than this margin to make the `softplus` term (and thus the `log_error`) zero, correctly preferring `a`.", "pseudocode": "1. Compute the log-probability difference: logp_diff = logp_a - logp_b.\n2. Compute the signed, batch-relative rank gap: cost_gap_signed = rank_gap(cost_a, cost_b).\n3. Standardize the rank gap across the batch: cost_gap_zscored = zscore(cost_gap_signed).\n4. Create a bounded dynamic margin from the standardized rank gap (inherited from Parent 0): dynamic_margin = beta * tanh(cost_gap_zscored).\n5. Compute a preliminary preference error using a smooth hinge-like term: hinge_error = softplus(dynamic_margin - logp_diff).\n6. Create a 'logmoid' error term by passing the hinge error through a logarithmic function (new coupling idea): log_error = log(1.0 + hinge_error).\n7. Create an adaptive temperature based on the margin's magnitude (inspired by Parent 1's adaptive weighting): adaptive_gamma = 1.0 + gamma * abs(dynamic_margin).\n8. Compute the final loss by scaling the log-error with the adaptive temperature (new coupling idea): loss = adaptive_gamma * log_error.", "hyperparams": {"beta": 1.0, "gamma": 0.5}, "operators_used": ["rank_gap", "zscore", "tanh", "softplus", "log"], "implementation_hint": {"expects": ["cost_a", "cost_b", "logp_a", "logp_b"], "returns": "scalar"}}, "fitness": {"hf_like_score": 8.445663928985596, "validation_objective": 8.445663928985596, "generalization_penalty": 0.0, "generalization_objectives": {"20": 4.030268907546997}, "train_score_mean": 10.455341205596923, "train_loss_mean": 0.6022029441595077, "pair_count": 12902392, "config": {"hf": {"problem": "tsp", "hf_steps": 100, "train_problem_size": 100, "valid_problem_sizes": [20], "train_batch_size": 64, "pomo_size": 64, "learning_rate": 0.0003, "weight_decay": 1e-06, "alpha": 0.05, "device": "cuda", "seed": 1234, "num_validation_episodes": 128, "validation_batch_size": 64, "generalization_penalty_weight": 1.0, "pool_version": "v0"}, "free_loss": {"f1_steps": 100, "f2_steps": 100, "f3_enabled": false}}, "loss_ir": {"name": "Rank-Aware Adaptive Hinge-Logmoid Loss", "intuition": "This loss function creates a novel hybrid by combining a dynamic margin from the first parent with an adaptive temperature from the second, and introduces a new coupling mechanism where the preference error is modulated by a logarithmic function before being scaled.\n\nFrom Parent 0 (Adaptive Hinge-Sigmoid Hybrid Loss), we inherit the core concept of a `dynamic_margin` that is computed from a standardized rank gap (`zscore(rank_gap(...))`). This provides a per-pair, batch-relative target for the model's preference score.\n\nFrom Parent 1 (Adaptively Weighted Hinge-Sigmoid Loss), we inherit the idea of an adaptive scaling factor, here called `adaptive_gamma`. This factor is derived from the magnitude of the dynamic margin (`abs(dynamic_margin)`), making the loss more sensitive to pairs with a clearer preference signal as determined by the batch context.\n\nAs a new coupling idea, we introduce a 'logmoid' error term. Instead of using the raw preference error `softplus(dynamic_margin - logp_diff)` directly, we pass it through a `log(1 + x)` function. This creates a `log_error = log(1.0 + softplus(dynamic_margin - logp_diff))`. This new term has the property of being zero when the preference is correct, but it dampens the influence of very large errors, promoting stability. The final loss is then the product of this log-error and the adaptive gamma (`adaptive_gamma * log_error`). This structure ensures that the loss remains sensitive (due to `adaptive_gamma`) while preventing extremely large errors on outlier pairs from dominating the gradient.\n\nWhen `cost(a) < cost(b)`, `dynamic_margin` is negative. The loss encourages `logp_diff` to be positive and larger than this margin to make the `softplus` term (and thus the `log_error`) zero, correctly preferring `a`.", "hyperparams": {"beta": 1.0, "gamma": 0.5}, "operators_used": ["rank_gap", "zscore", "tanh", "softplus", "log"]}}, "better_than_baseline": true}
{"generation": 6, "index": 5, "ir": {"name": "Rank-Calibrated Dynamic Hinge Loss", "intuition": "This loss function creates a hybrid that inherits the dynamic margin from one parent and an adaptive scaling mechanism from the other, while introducing a new coupling that directly modulates the model's preference signal.\n\nFrom Parent 0 ('Adaptive Hinge-Sigmoid Hybrid Loss'), we inherit the core idea of a `dynamic_margin` derived from the standardized rank gap (`beta * tanh(zscore(rank_gap(...)))`). This creates a target preference margin that adapts to the relative cost difference within the batch.\n\nFrom Parent 1 ('Adaptively Weighted Hinge-Sigmoid Loss with Rank-Gap Scaling'), we inherit the concept of an `adaptive_weight` (called `adaptive_alpha` in the parent) that is a sigmoid function of the margin's magnitude. This makes the loss more sensitive to pairs with a clear cost separation.\n\nWe introduce two new coupling ideas. First, instead of using the adaptive weight to scale the final loss, we use it to directly modulate the model's preference signal (`logp_diff`). The modulated preference is `modulated_logp_diff = adaptive_weight * logp_diff`. This forces the model to express a stronger preference signal for pairs where the cost difference is clearer. Second, we introduce a `stability_offset` using `softplus` on the raw, unstandardized rank gap. This offset is added to the hinge term, ensuring the loss remains non-zero even for perfectly classified pairs, preventing gradient saturation and encouraging continuous improvement. The final loss is a smooth hinge loss (`softplus`) comparing the `dynamic_margin` to the `modulated_logp_diff`, with the added stability offset.\n\nWhen `cost(a) < cost(b)`, the `dynamic_margin` is negative. The loss encourages a positive `logp_diff`, which makes `modulated_logp_diff` positive, thereby pushing the argument of `softplus` towards zero and minimizing the loss.", "pseudocode": "1. Compute the log-probability difference: logp_diff = logp_a - logp_b.\n2. Compute the signed, batch-relative rank gap: cost_gap_signed = rank_gap(cost_a, cost_b).\n3. Standardize the rank gap across the batch: cost_gap_zscored = zscore(cost_gap_signed).\n4. Create a bounded dynamic margin from the standardized rank gap (inherited from Parent 0): dynamic_margin = beta * tanh(cost_gap_zscored).\n5. Create an adaptive weight based on the margin's magnitude (inherited from Parent 1): adaptive_weight = sigmoid(gamma * abs(dynamic_margin)).\n6. Modulate the model's preference signal using the adaptive weight (new coupling idea): modulated_logp_diff = adaptive_weight * logp_diff.\n7. Compute a stability offset based on the raw rank gap magnitude (new coupling idea): stability_offset = softplus(abs(cost_gap_signed) - 1.0).\n8. Compute the core hinge loss using the dynamic margin and modulated preference: hinge_term = softplus(dynamic_margin - modulated_logp_diff).\n9. Combine the hinge term and the stability offset for the final loss: loss = hinge_term + stability_offset.", "hyperparams": {"beta": 1.5, "gamma": 1.0}, "operators_used": ["rank_gap", "zscore", "tanh", "sigmoid", "softplus"], "implementation_hint": {"expects": ["cost_a", "cost_b", "logp_a", "logp_b"], "returns": "scalar"}}, "fitness": {"hf_like_score": 8.448623180389404, "validation_objective": 8.448623180389404, "generalization_penalty": 0.0, "generalization_objectives": {"20": 4.034071207046509}, "train_score_mean": 10.452947187423707, "train_loss_mean": 0.5997407728433609, "pair_count": 12902387, "config": {"hf": {"problem": "tsp", "hf_steps": 100, "train_problem_size": 100, "valid_problem_sizes": [20], "train_batch_size": 64, "pomo_size": 64, "learning_rate": 0.0003, "weight_decay": 1e-06, "alpha": 0.05, "device": "cuda", "seed": 1234, "num_validation_episodes": 128, "validation_batch_size": 64, "generalization_penalty_weight": 1.0, "pool_version": "v0"}, "free_loss": {"f1_steps": 100, "f2_steps": 100, "f3_enabled": false}}, "loss_ir": {"name": "Rank-Calibrated Dynamic Hinge Loss", "intuition": "This loss function creates a hybrid that inherits the dynamic margin from one parent and an adaptive scaling mechanism from the other, while introducing a new coupling that directly modulates the model's preference signal.\n\nFrom Parent 0 ('Adaptive Hinge-Sigmoid Hybrid Loss'), we inherit the core idea of a `dynamic_margin` derived from the standardized rank gap (`beta * tanh(zscore(rank_gap(...)))`). This creates a target preference margin that adapts to the relative cost difference within the batch.\n\nFrom Parent 1 ('Adaptively Weighted Hinge-Sigmoid Loss with Rank-Gap Scaling'), we inherit the concept of an `adaptive_weight` (called `adaptive_alpha` in the parent) that is a sigmoid function of the margin's magnitude. This makes the loss more sensitive to pairs with a clear cost separation.\n\nWe introduce two new coupling ideas. First, instead of using the adaptive weight to scale the final loss, we use it to directly modulate the model's preference signal (`logp_diff`). The modulated preference is `modulated_logp_diff = adaptive_weight * logp_diff`. This forces the model to express a stronger preference signal for pairs where the cost difference is clearer. Second, we introduce a `stability_offset` using `softplus` on the raw, unstandardized rank gap. This offset is added to the hinge term, ensuring the loss remains non-zero even for perfectly classified pairs, preventing gradient saturation and encouraging continuous improvement. The final loss is a smooth hinge loss (`softplus`) comparing the `dynamic_margin` to the `modulated_logp_diff`, with the added stability offset.\n\nWhen `cost(a) < cost(b)`, the `dynamic_margin` is negative. The loss encourages a positive `logp_diff`, which makes `modulated_logp_diff` positive, thereby pushing the argument of `softplus` towards zero and minimizing the loss.", "hyperparams": {"beta": 1.5, "gamma": 1.0}, "operators_used": ["rank_gap", "zscore", "tanh", "sigmoid", "softplus"]}}, "better_than_baseline": false}
{"generation": 6, "index": 6, "ir": {"name": "Rank-Aware Adaptive Sigmoid Loss", "intuition": "This loss function creates a hybrid that uses a dynamic, rank-based margin to modulate a sigmoid loss, and introduces an adaptive temperature that is sensitive to both the margin and the model's own confidence.\n\nFrom Parent 0 ('Adaptive Hinge-Sigmoid Hybrid Loss'), we inherit the core concept of a `dynamic_margin` derived from the standardized, batch-relative rank gap. This ensures the loss target is not fixed but adapts to the relative cost difference within a batch.\n\nFrom Parent 1 ('Adaptively Weighted Hinge-Sigmoid Loss with Rank-Gap Scaling'), we inherit the idea of using `sigmoid` to create an adaptive weight or scale, specifically `sigmoid(gamma * abs(dynamic_margin))`. This makes the loss more sensitive to pairs with a clear preference ordering.\n\nWe introduce two new coupling ideas. First, instead of a hinge-loss `softplus` structure, we directly embed the `dynamic_margin` into a classic sigmoid loss formulation: `logsigmoid(logp_diff - dynamic_margin)`. This directly frames the problem as classifying whether the model's preference `logp_diff` exceeds the rank-aware margin. Second, we introduce a novel adaptive temperature, `adaptive_temp`, which is a function of both the `dynamic_margin` (problem difficulty) and the model's current preference `logp_diff` (model confidence). This temperature, `softplus(abs(dynamic_margin) - abs(logp_diff))`, increases when the model's preference is weaker than the required margin, making the loss gradient steeper and encouraging a faster correction. It decreases when the model is already confident, preventing over-correction.\n\nWhen `cost(a) < cost(b)`, the `dynamic_margin` is negative. The loss encourages `logp_diff` to be positive, thus making `logp_diff - dynamic_margin` a large positive value and minimizing the `logsigmoid` loss, correctly preferring `a`.", "pseudocode": "1. Compute the log-probability difference: logp_diff = logp_a - logp_b.\n2. Compute the signed, batch-relative rank gap: cost_gap_signed = rank_gap(cost_a, cost_b).\n3. Standardize the rank gap across the batch: cost_gap_zscored = zscore(cost_gap_signed).\n4. Create a bounded dynamic margin from the standardized rank gap (inherited from Parent 0): dynamic_margin = beta * tanh(cost_gap_zscored).\n5. Create an adaptive scaling factor based on the margin's magnitude (inspired by Parent 1): adaptive_scale = sigmoid(gamma * abs(dynamic_margin)).\n6. Create a novel adaptive temperature based on the difference between margin magnitude and model confidence (new coupling idea): adaptive_temp = 1.0 + softplus(abs(dynamic_margin) - abs(logp_diff)).\n7. Compute the core sigmoid loss term, using the dynamic margin directly (new coupling idea): sigmoid_term = logsigmoid(logp_diff - dynamic_margin).\n8. Combine the components into the final loss: loss = -1.0 * adaptive_scale * adaptive_temp * sigmoid_term.", "hyperparams": {"beta": 1.0, "gamma": 1.0}, "operators_used": ["rank_gap", "zscore", "tanh", "sigmoid", "softplus", "logsigmoid"], "implementation_hint": {"expects": ["cost_a", "cost_b", "logp_a", "logp_b"], "returns": "scalar"}}, "fitness": {"hf_like_score": 8.483580589294434, "validation_objective": 8.483580589294434, "generalization_penalty": 0.0, "generalization_objectives": {"20": 3.981849431991577}, "train_score_mean": 10.512809534072876, "train_loss_mean": 0.600330845117569, "pair_count": 12902388, "config": {"hf": {"problem": "tsp", "hf_steps": 100, "train_problem_size": 100, "valid_problem_sizes": [20], "train_batch_size": 64, "pomo_size": 64, "learning_rate": 0.0003, "weight_decay": 1e-06, "alpha": 0.05, "device": "cuda", "seed": 1234, "num_validation_episodes": 128, "validation_batch_size": 64, "generalization_penalty_weight": 1.0, "pool_version": "v0"}, "free_loss": {"f1_steps": 100, "f2_steps": 100, "f3_enabled": false}}, "loss_ir": {"name": "Rank-Aware Adaptive Sigmoid Loss", "intuition": "This loss function creates a hybrid that uses a dynamic, rank-based margin to modulate a sigmoid loss, and introduces an adaptive temperature that is sensitive to both the margin and the model's own confidence.\n\nFrom Parent 0 ('Adaptive Hinge-Sigmoid Hybrid Loss'), we inherit the core concept of a `dynamic_margin` derived from the standardized, batch-relative rank gap. This ensures the loss target is not fixed but adapts to the relative cost difference within a batch.\n\nFrom Parent 1 ('Adaptively Weighted Hinge-Sigmoid Loss with Rank-Gap Scaling'), we inherit the idea of using `sigmoid` to create an adaptive weight or scale, specifically `sigmoid(gamma * abs(dynamic_margin))`. This makes the loss more sensitive to pairs with a clear preference ordering.\n\nWe introduce two new coupling ideas. First, instead of a hinge-loss `softplus` structure, we directly embed the `dynamic_margin` into a classic sigmoid loss formulation: `logsigmoid(logp_diff - dynamic_margin)`. This directly frames the problem as classifying whether the model's preference `logp_diff` exceeds the rank-aware margin. Second, we introduce a novel adaptive temperature, `adaptive_temp`, which is a function of both the `dynamic_margin` (problem difficulty) and the model's current preference `logp_diff` (model confidence). This temperature, `softplus(abs(dynamic_margin) - abs(logp_diff))`, increases when the model's preference is weaker than the required margin, making the loss gradient steeper and encouraging a faster correction. It decreases when the model is already confident, preventing over-correction.\n\nWhen `cost(a) < cost(b)`, the `dynamic_margin` is negative. The loss encourages `logp_diff` to be positive, thus making `logp_diff - dynamic_margin` a large positive value and minimizing the `logsigmoid` loss, correctly preferring `a`.", "hyperparams": {"beta": 1.0, "gamma": 1.0}, "operators_used": ["rank_gap", "zscore", "tanh", "sigmoid", "softplus", "logsigmoid"]}}, "better_than_baseline": false}
{"generation": 6, "index": 7, "ir": {"name": "Rank-Calibrated Dynamic Hinge Loss", "intuition": "This loss function creates a hybrid that focuses on a stable, margin-based error signal, calibrated by both rank and cost information.\n\nFrom Parent 0 ('Adaptive Hinge-Sigmoid Hybrid Loss'), we inherit the core structure of a dynamic, bounded margin based on a standardized rank gap (`dynamic_margin = beta * tanh(zscore(rank_gap(...)))`). We also inherit its use of `softplus` to create a smooth, non-negative hinge-like error term (`preference_error = softplus(dynamic_margin - logp_diff)`), which is zero when the model's preference correctly exceeds the margin.\n\nFrom Parent 1 ('Adaptively Weighted Hinge-Sigmoid Loss with Rank-Gap Scaling'), we inherit the idea of scaling the final loss value. However, instead of using the non-standardized rank gap directly, we use a sigmoid-based adaptive weight (`adaptive_weight = sigmoid(gamma * abs(dynamic_margin))`) derived from the margin's magnitude. This makes the loss contribution stronger for pairs with a clear preference signal (large margin) and weaker for ambiguous pairs (small margin).\n\nAs a new coupling idea, we introduce a 'stability factor' that modulates the `preference_error` before it is scaled by the adaptive weight. This factor, `stability_factor = 1.0 - tanh(abs(logp_diff))`, reduces the error contribution from pairs where the model's preference is extremely confident (large `abs(logp_diff)`). This prevents single outlier predictions from dominating the gradient, improving training stability. The final loss is `adaptive_weight * stability_factor * preference_error`.\n\nWhen `cost(a) < cost(b)`, `dynamic_margin` becomes negative. The loss encourages `logp_diff` to be positive and larger than this margin to drive `preference_error` to zero, thus correctly preferring `a`.", "pseudocode": "1. Compute the log-probability difference: logp_diff = logp_a - logp_b.\n2. Compute the signed, batch-relative rank gap: cost_gap_signed = rank_gap(cost_a, cost_b).\n3. Standardize the rank gap across the batch: cost_gap_zscored = zscore(cost_gap_signed).\n4. Create a bounded dynamic margin from the standardized rank gap (inherited from Parent 0): dynamic_margin = beta * tanh(cost_gap_zscored).\n5. Compute the core preference error using a smooth hinge-like term (inherited from Parent 0): preference_error = softplus(dynamic_margin - logp_diff).\n6. Create an adaptive weight based on the margin's magnitude (inherited from Parent 1): adaptive_weight = sigmoid(gamma * abs(dynamic_margin)).\n7. Introduce a new stability factor to down-weight overly confident predictions (new coupling idea): stability_factor = 1.0 - tanh(abs(logp_diff)).\n8. Compute the final loss by coupling the preference error, stability factor, and adaptive weight: loss = adaptive_weight * stability_factor * preference_error.", "hyperparams": {"beta": 1.0, "gamma": 1.0}, "operators_used": ["rank_gap", "zscore", "tanh", "softplus", "sigmoid"], "implementation_hint": {"expects": ["cost_a", "cost_b", "logp_a", "logp_b"], "returns": "scalar"}}, "fitness": {"hf_like_score": 8.457887172698975, "validation_objective": 8.457887172698975, "generalization_penalty": 0.0, "generalization_objectives": {"20": 3.989327073097229}, "train_score_mean": 10.502836856842041, "train_loss_mean": 0.6004270219802856, "pair_count": 12902385, "config": {"hf": {"problem": "tsp", "hf_steps": 100, "train_problem_size": 100, "valid_problem_sizes": [20], "train_batch_size": 64, "pomo_size": 64, "learning_rate": 0.0003, "weight_decay": 1e-06, "alpha": 0.05, "device": "cuda", "seed": 1234, "num_validation_episodes": 128, "validation_batch_size": 64, "generalization_penalty_weight": 1.0, "pool_version": "v0"}, "free_loss": {"f1_steps": 100, "f2_steps": 100, "f3_enabled": false}}, "loss_ir": {"name": "Rank-Calibrated Dynamic Hinge Loss", "intuition": "This loss function creates a hybrid that focuses on a stable, margin-based error signal, calibrated by both rank and cost information.\n\nFrom Parent 0 ('Adaptive Hinge-Sigmoid Hybrid Loss'), we inherit the core structure of a dynamic, bounded margin based on a standardized rank gap (`dynamic_margin = beta * tanh(zscore(rank_gap(...)))`). We also inherit its use of `softplus` to create a smooth, non-negative hinge-like error term (`preference_error = softplus(dynamic_margin - logp_diff)`), which is zero when the model's preference correctly exceeds the margin.\n\nFrom Parent 1 ('Adaptively Weighted Hinge-Sigmoid Loss with Rank-Gap Scaling'), we inherit the idea of scaling the final loss value. However, instead of using the non-standardized rank gap directly, we use a sigmoid-based adaptive weight (`adaptive_weight = sigmoid(gamma * abs(dynamic_margin))`) derived from the margin's magnitude. This makes the loss contribution stronger for pairs with a clear preference signal (large margin) and weaker for ambiguous pairs (small margin).\n\nAs a new coupling idea, we introduce a 'stability factor' that modulates the `preference_error` before it is scaled by the adaptive weight. This factor, `stability_factor = 1.0 - tanh(abs(logp_diff))`, reduces the error contribution from pairs where the model's preference is extremely confident (large `abs(logp_diff)`). This prevents single outlier predictions from dominating the gradient, improving training stability. The final loss is `adaptive_weight * stability_factor * preference_error`.\n\nWhen `cost(a) < cost(b)`, `dynamic_margin` becomes negative. The loss encourages `logp_diff` to be positive and larger than this margin to drive `preference_error` to zero, thus correctly preferring `a`.", "hyperparams": {"beta": 1.0, "gamma": 1.0}, "operators_used": ["rank_gap", "zscore", "tanh", "softplus", "sigmoid"]}}, "better_than_baseline": false}
{"generation": 6, "index": 8, "ir": {"name": "Rank-Gated Adaptive Hinge Loss", "intuition": "This loss function creates a hybrid that selectively applies a penalty based on the significance of the cost difference, borrowing ideas from both parents and introducing a gating mechanism.\n\nFrom Parent 0 ('Adaptive Hinge-Sigmoid Hybrid Loss'), we inherit the core concept of a `preference_error` calculated using a smooth hinge loss: `softplus(dynamic_margin - logp_diff)`. This term elegantly becomes zero when the model's preference `logp_diff` correctly exceeds the desired `dynamic_margin`. We also inherit the `dynamic_margin` itself, which is based on a `tanh`-squashed, z-scored rank gap.\n\nFrom Parent 1 ('Adaptively Weighted Hinge-Sigmoid Loss'), we inherit the idea of using the raw, unstandardized rank gap to modulate the final loss. While Parent 1 used it as a continuous scaling factor, we adapt this idea into a discrete gate.\n\nAs a new coupling idea, we introduce a 'significance gate'. We compute a `gate` using `sigmoid(gamma * (abs(cost_gap_signed) - tau))`. This gate only 'opens' (approaches 1) when the absolute rank gap `abs(cost_gap_signed)` is significantly larger than a threshold `tau`. The loss is then calculated as `gate * preference_error`. This means the loss is only applied to pairs with a meaningful cost difference, effectively ignoring noisy or ambiguous pairs where the ranks are very close. This gating mechanism combines the rank-gap sensitivity from Parent 1 with the hinge-error structure from Parent 0.\n\nAdditionally, we introduce a second new idea: a scheduled margin. The `beta` hyperparameter, which controls the scale of the `dynamic_margin`, is not fixed but is annealed over training (e.g., from a small value to a larger one). This is a conceptual addition, represented by `beta_scheduled` in the pseudocode.\n\nWhen `cost(a) < cost(b)`, `cost_gap_signed` is negative, making `dynamic_margin` negative. The loss encourages `logp_diff` to be positive and larger than this margin to make `preference_error` zero, correctly preferring `a`.", "pseudocode": "1. Assume beta is scheduled (e.g., annealed from 0.1 to 1.0 over training): beta_scheduled.\n2. Compute the log-probability difference: logp_diff = logp_a - logp_b.\n3. Compute the signed, batch-relative rank gap: cost_gap_signed = rank_gap(cost_a, cost_b).\n4. Standardize the rank gap across the batch: cost_gap_zscored = zscore(cost_gap_signed).\n5. Create a bounded dynamic margin using the scheduled beta (structure inherited from Parent 0, with new scheduling idea): dynamic_margin = beta_scheduled * tanh(cost_gap_zscored).\n6. Compute the preference error using a smooth hinge-like term (inherited from Parent 0): preference_error = softplus(dynamic_margin - logp_diff).\n7. Compute a significance gate based on the raw rank gap (new coupling idea, inspired by Parent 1's rank-gap scaling): gate = sigmoid(gamma * (abs(cost_gap_signed) - tau)).\n8. Apply the gate to the preference error to compute the final loss: loss = gate * preference_error.", "hyperparams": {"beta_scheduled": "Annealed, e.g., from 0.1 to 1.0", "gamma": 2.0, "tau": 0.5}, "operators_used": ["rank_gap", "zscore", "tanh", "softplus", "sigmoid"], "implementation_hint": {"expects": ["cost_a", "cost_b", "logp_a", "logp_b"], "returns": "scalar"}}, "fitness": {"hf_like_score": 8.414087295532227, "validation_objective": 8.414087295532227, "generalization_penalty": 0.0, "generalization_objectives": {"20": 4.002763032913208}, "train_score_mean": 10.469329280853271, "train_loss_mean": 0.6036651134490967, "pair_count": 12902389, "config": {"hf": {"problem": "tsp", "hf_steps": 100, "train_problem_size": 100, "valid_problem_sizes": [20], "train_batch_size": 64, "pomo_size": 64, "learning_rate": 0.0003, "weight_decay": 1e-06, "alpha": 0.05, "device": "cuda", "seed": 1234, "num_validation_episodes": 128, "validation_batch_size": 64, "generalization_penalty_weight": 1.0, "pool_version": "v0"}, "free_loss": {"f1_steps": 100, "f2_steps": 100, "f3_enabled": false}}, "loss_ir": {"name": "Rank-Gated Adaptive Hinge Loss", "intuition": "This loss function creates a hybrid that selectively applies a penalty based on the significance of the cost difference, borrowing ideas from both parents and introducing a gating mechanism.\n\nFrom Parent 0 ('Adaptive Hinge-Sigmoid Hybrid Loss'), we inherit the core concept of a `preference_error` calculated using a smooth hinge loss: `softplus(dynamic_margin - logp_diff)`. This term elegantly becomes zero when the model's preference `logp_diff` correctly exceeds the desired `dynamic_margin`. We also inherit the `dynamic_margin` itself, which is based on a `tanh`-squashed, z-scored rank gap.\n\nFrom Parent 1 ('Adaptively Weighted Hinge-Sigmoid Loss'), we inherit the idea of using the raw, unstandardized rank gap to modulate the final loss. While Parent 1 used it as a continuous scaling factor, we adapt this idea into a discrete gate.\n\nAs a new coupling idea, we introduce a 'significance gate'. We compute a `gate` using `sigmoid(gamma * (abs(cost_gap_signed) - tau))`. This gate only 'opens' (approaches 1) when the absolute rank gap `abs(cost_gap_signed)` is significantly larger than a threshold `tau`. The loss is then calculated as `gate * preference_error`. This means the loss is only applied to pairs with a meaningful cost difference, effectively ignoring noisy or ambiguous pairs where the ranks are very close. This gating mechanism combines the rank-gap sensitivity from Parent 1 with the hinge-error structure from Parent 0.\n\nAdditionally, we introduce a second new idea: a scheduled margin. The `beta` hyperparameter, which controls the scale of the `dynamic_margin`, is not fixed but is annealed over training (e.g., from a small value to a larger one). This is a conceptual addition, represented by `beta_scheduled` in the pseudocode.\n\nWhen `cost(a) < cost(b)`, `cost_gap_signed` is negative, making `dynamic_margin` negative. The loss encourages `logp_diff` to be positive and larger than this margin to make `preference_error` zero, correctly preferring `a`.", "hyperparams": {"beta_scheduled": "Annealed, e.g., from 0.1 to 1.0", "gamma": 2.0, "tau": 0.5}, "operators_used": ["rank_gap", "zscore", "tanh", "softplus", "sigmoid"]}}, "better_than_baseline": true}
{"generation": 6, "index": 9, "ir": {"name": "Adaptive Rank-Calibrated Hinge-Log Loss", "intuition": "This loss function creates a hybrid that stabilizes preference learning by combining a dynamic margin with an adaptive, rank-calibrated error signal. The goal is to make the loss sensitive to the magnitude of both the model's confidence and the ground-truth cost gap, while maintaining numerical stability.\n\nFrom Parent 0 (Adaptive Hinge-Sigmoid Hybrid Loss), we inherit the core concept of a `dynamic_margin` based on a standardized rank gap (`beta * tanh(zscore(rank_gap(...)))`). This provides a bounded, relative target for the model's preference score, making the learning target adaptive to the batch statistics.\n\nFrom Parent 1 (Adaptively Weighted Hinge-Sigmoid Loss with Rank-Gap Scaling), we inherit the idea of scaling the loss based on the raw cost difference, embodied here by the `rank_gap_scale`. This ensures that pairs with larger, more significant cost differences contribute more to the overall loss, focusing the model on correcting severe mis-rankings.\n\nWe introduce a new coupling mechanism that combines these inherited ideas in a novel way. Instead of using a simple hinge loss like `softplus(margin - logp_diff)`, we compute a 'calibrated preference error' defined as `softplus(dynamic_margin - logp_diff)`. This error term is then multiplied by an `adaptive_gamma` factor, which is itself a function of the `dynamic_margin`'s magnitude, similar to the adaptive temperature in Parent 0. The key novelty is how we use this combined term: `final_error = adaptive_gamma * calibrated_preference_error`. This product is then passed through a stable logarithmic function, `log(1 + final_error)`. This structure makes the loss penalty grow more aggressively (via `adaptive_gamma`) for pairs with a larger standardized rank gap, while the `softplus` ensures the loss is zero for correctly ordered pairs. Finally, the entire expression is scaled by the `rank_gap_scale` inherited from Parent 1, further emphasizing pairs with large absolute cost differences.\n\nWhen `cost(a) < cost(b)`, the `dynamic_margin` is negative. The loss encourages `logp_diff` to be positive and greater than this margin to drive the `calibrated_preference_error` to zero, thus correctly preferring `a`.", "pseudocode": "1. Compute the log-probability difference: logp_diff = logp_a - logp_b.\n2. Compute the signed, batch-relative rank gap: cost_gap_signed = rank_gap(cost_a, cost_b).\n3. Standardize the rank gap across the batch: cost_gap_zscored = zscore(cost_gap_signed).\n4. Create a bounded dynamic margin from the standardized rank gap (inherited from Parent 0): dynamic_margin = beta * tanh(cost_gap_zscored).\n5. Create an adaptive temperature based on the margin's magnitude (new coupling idea, inspired by Parent 0): adaptive_gamma = 1.0 + gamma * abs(dynamic_margin).\n6. Compute a calibrated preference error using a smooth hinge-like term (structure from Parent 0): calibrated_preference_error = softplus(dynamic_margin - logp_diff).\n7. Create a scaling factor from the raw rank gap (inherited from Parent 1): rank_gap_scale = 1.0 + softplus(abs(cost_gap_signed) - 1.0).\n8. Combine the adaptive temperature and preference error into a single term (new coupling idea): final_error = adaptive_gamma * calibrated_preference_error.\n9. Compute the base loss using a stable logarithmic function: base_loss = log(1.0 + final_error).\n10. Apply the rank gap scaling to get the final loss: loss = rank_gap_scale * base_loss.", "hyperparams": {"beta": 1.5, "gamma": 1.0}, "operators_used": ["rank_gap", "zscore", "tanh", "softplus", "log"], "implementation_hint": {"expects": ["cost_a", "cost_b", "logp_a", "logp_b"], "returns": "scalar"}}, "fitness": {"hf_like_score": 8.419316291809082, "validation_objective": 8.419316291809082, "generalization_penalty": 0.0, "generalization_objectives": {"20": 4.018252491950989}, "train_score_mean": 10.499323682785034, "train_loss_mean": 0.6004150718450546, "pair_count": 12902388, "config": {"hf": {"problem": "tsp", "hf_steps": 100, "train_problem_size": 100, "valid_problem_sizes": [20], "train_batch_size": 64, "pomo_size": 64, "learning_rate": 0.0003, "weight_decay": 1e-06, "alpha": 0.05, "device": "cuda", "seed": 1234, "num_validation_episodes": 128, "validation_batch_size": 64, "generalization_penalty_weight": 1.0, "pool_version": "v0"}, "free_loss": {"f1_steps": 100, "f2_steps": 100, "f3_enabled": false}}, "loss_ir": {"name": "Adaptive Rank-Calibrated Hinge-Log Loss", "intuition": "This loss function creates a hybrid that stabilizes preference learning by combining a dynamic margin with an adaptive, rank-calibrated error signal. The goal is to make the loss sensitive to the magnitude of both the model's confidence and the ground-truth cost gap, while maintaining numerical stability.\n\nFrom Parent 0 (Adaptive Hinge-Sigmoid Hybrid Loss), we inherit the core concept of a `dynamic_margin` based on a standardized rank gap (`beta * tanh(zscore(rank_gap(...)))`). This provides a bounded, relative target for the model's preference score, making the learning target adaptive to the batch statistics.\n\nFrom Parent 1 (Adaptively Weighted Hinge-Sigmoid Loss with Rank-Gap Scaling), we inherit the idea of scaling the loss based on the raw cost difference, embodied here by the `rank_gap_scale`. This ensures that pairs with larger, more significant cost differences contribute more to the overall loss, focusing the model on correcting severe mis-rankings.\n\nWe introduce a new coupling mechanism that combines these inherited ideas in a novel way. Instead of using a simple hinge loss like `softplus(margin - logp_diff)`, we compute a 'calibrated preference error' defined as `softplus(dynamic_margin - logp_diff)`. This error term is then multiplied by an `adaptive_gamma` factor, which is itself a function of the `dynamic_margin`'s magnitude, similar to the adaptive temperature in Parent 0. The key novelty is how we use this combined term: `final_error = adaptive_gamma * calibrated_preference_error`. This product is then passed through a stable logarithmic function, `log(1 + final_error)`. This structure makes the loss penalty grow more aggressively (via `adaptive_gamma`) for pairs with a larger standardized rank gap, while the `softplus` ensures the loss is zero for correctly ordered pairs. Finally, the entire expression is scaled by the `rank_gap_scale` inherited from Parent 1, further emphasizing pairs with large absolute cost differences.\n\nWhen `cost(a) < cost(b)`, the `dynamic_margin` is negative. The loss encourages `logp_diff` to be positive and greater than this margin to drive the `calibrated_preference_error` to zero, thus correctly preferring `a`.", "hyperparams": {"beta": 1.5, "gamma": 1.0}, "operators_used": ["rank_gap", "zscore", "tanh", "softplus", "log"]}}, "better_than_baseline": true}
{"generation": 7, "index": 0, "ir": {"name": "Rank-Calibrated Dynamic Hinge Loss", "intuition": "This loss function creates a robust hybrid by combining a dynamic margin with an adaptive error scaling mechanism, introducing a new coupling that directly modulates the error based on rank-gap confidence.\n\nFrom Parent 0 (Adaptive Hinge-Sigmoid Hybrid Loss), we inherit the core concept of a `dynamic_margin` derived from a standardized rank gap using `beta * tanh(zscore(rank_gap(...)))`. This creates a bounded, relative target for the model's preference score that is sensitive to the batch-wise distribution of cost differences.\n\nFrom Parent 1 (Adaptively Weighted Hinge-Sigmoid Loss), we inherit the idea of an `adaptive_alpha` weight, which is a sigmoid-based factor that responds to the magnitude of the preference margin. This makes the loss more or less strict depending on how clear the cost difference is.\n\nWe introduce two new coupling ideas. First, instead of using the adaptive weight `adaptive_alpha` to combine two loss terms or to scale the final loss, we use it to directly scale the *hinge error itself* before it is logged. The core error term becomes `adaptive_alpha * softplus(dynamic_margin - logp_diff)`. This means the penalty for mis-prediction is dynamically adjusted based on the margin's magnitude. Second, we introduce a `confidence_penalty_factor` based on the non-standardized rank gap. This factor, `1.0 + gamma * relu(1.0 - abs(cost_gap_signed))`, increases the loss for pairs with very small cost differences (i.e., low confidence in the ranking), forcing the model to pay more attention to ambiguous cases. The final loss is a stable logarithmic function of this scaled and penalized error.\n\nWhen `cost(a) < cost(b)`, the `dynamic_margin` is negative. The loss encourages `logp_diff` to be positive and larger than this margin to make the `softplus` term zero, correctly preferring `a`.", "pseudocode": "1. Compute the log-probability difference: logp_diff = logp_a - logp_b.\n2. Compute the signed, batch-relative rank gap: cost_gap_signed = rank_gap(cost_a, cost_b).\n3. Standardize the rank gap across the batch: cost_gap_zscored = zscore(cost_gap_signed).\n4. Create a bounded dynamic margin from the standardized rank gap (inherited from Parent 0): dynamic_margin = beta * tanh(cost_gap_zscored).\n5. Compute the raw hinge error: hinge_error = softplus(dynamic_margin - logp_diff).\n6. Create an adaptive weight based on the margin's magnitude (inherited from Parent 1): adaptive_alpha = sigmoid(alpha_scale * abs(dynamic_margin)).\n7. Create a confidence penalty for small rank gaps (new coupling idea): confidence_penalty_factor = 1.0 + gamma * relu(1.0 - abs(cost_gap_signed)).\n8. Scale the hinge error by the adaptive weight (new coupling idea): scaled_error = adaptive_alpha * hinge_error.\n9. Apply the confidence penalty and compute the final stable loss: loss = confidence_penalty_factor * log(1.0 + scaled_error).", "hyperparams": {"beta": 1.0, "alpha_scale": 1.5, "gamma": 0.25}, "operators_used": ["rank_gap", "zscore", "tanh", "softplus", "sigmoid", "relu", "log"], "implementation_hint": {"expects": ["cost_a", "cost_b", "logp_a", "logp_b"], "returns": "scalar"}}, "fitness": {"hf_like_score": 8.445843696594238, "validation_objective": 8.445843696594238, "generalization_penalty": 0.0, "generalization_objectives": {"20": 4.028648614883423}, "train_score_mean": 10.472429542541503, "train_loss_mean": 0.6002667498588562, "pair_count": 12902387, "config": {"hf": {"problem": "tsp", "hf_steps": 100, "train_problem_size": 100, "valid_problem_sizes": [20], "train_batch_size": 64, "pomo_size": 64, "learning_rate": 0.0003, "weight_decay": 1e-06, "alpha": 0.05, "device": "cuda", "seed": 1234, "num_validation_episodes": 128, "validation_batch_size": 64, "generalization_penalty_weight": 1.0, "pool_version": "v0"}, "free_loss": {"f1_steps": 100, "f2_steps": 100, "f3_enabled": false}}, "loss_ir": {"name": "Rank-Calibrated Dynamic Hinge Loss", "intuition": "This loss function creates a robust hybrid by combining a dynamic margin with an adaptive error scaling mechanism, introducing a new coupling that directly modulates the error based on rank-gap confidence.\n\nFrom Parent 0 (Adaptive Hinge-Sigmoid Hybrid Loss), we inherit the core concept of a `dynamic_margin` derived from a standardized rank gap using `beta * tanh(zscore(rank_gap(...)))`. This creates a bounded, relative target for the model's preference score that is sensitive to the batch-wise distribution of cost differences.\n\nFrom Parent 1 (Adaptively Weighted Hinge-Sigmoid Loss), we inherit the idea of an `adaptive_alpha` weight, which is a sigmoid-based factor that responds to the magnitude of the preference margin. This makes the loss more or less strict depending on how clear the cost difference is.\n\nWe introduce two new coupling ideas. First, instead of using the adaptive weight `adaptive_alpha` to combine two loss terms or to scale the final loss, we use it to directly scale the *hinge error itself* before it is logged. The core error term becomes `adaptive_alpha * softplus(dynamic_margin - logp_diff)`. This means the penalty for mis-prediction is dynamically adjusted based on the margin's magnitude. Second, we introduce a `confidence_penalty_factor` based on the non-standardized rank gap. This factor, `1.0 + gamma * relu(1.0 - abs(cost_gap_signed))`, increases the loss for pairs with very small cost differences (i.e., low confidence in the ranking), forcing the model to pay more attention to ambiguous cases. The final loss is a stable logarithmic function of this scaled and penalized error.\n\nWhen `cost(a) < cost(b)`, the `dynamic_margin` is negative. The loss encourages `logp_diff` to be positive and larger than this margin to make the `softplus` term zero, correctly preferring `a`.", "hyperparams": {"beta": 1.0, "alpha_scale": 1.5, "gamma": 0.25}, "operators_used": ["rank_gap", "zscore", "tanh", "softplus", "sigmoid", "relu", "log"]}}, "better_than_baseline": true}
{"generation": 7, "index": 1, "ir": {"name": "Rank-Calibrated Dynamic Hinge Loss", "intuition": "This loss function creates a hybrid that inherits the dynamic margin from Parent 0 and the adaptive temperature scaling from Parent 1, while introducing a new coupling mechanism that makes the loss penalty proportional to the raw cost difference.\n\nFrom Parent 0 (Adaptive Hinge-Sigmoid Hybrid Loss), we inherit the core concept of a `dynamic_margin` calculated as `beta * tanh(zscore(rank_gap(cost_a, cost_b)))`. This margin dynamically adapts based on the relative ranking of the cost pair within the batch, providing a flexible target for the model's preference score.\n\nFrom Parent 1 (Adaptively Weighted Hinge-Sigmoid Loss with Rank-Gap Scaling), we inherit the idea of an `adaptive_weight` (here called `adaptive_gamma`) that is a function of the margin's magnitude, `sigmoid(gamma * abs(dynamic_margin))`. This makes the loss more sensitive to pairs with a clearer preference signal (larger margin) and more lenient for ambiguous pairs.\n\nWe introduce two new coupling ideas. First, we compute a core `preference_error` using the classic hinge-loss structure `softplus(dynamic_margin - logp_diff)`, which penalizes the model only when its preference `logp_diff` is insufficient to overcome the `dynamic_margin`. Second, we introduce a `cost_scale` factor, `softplus(cost_b - cost_a)`. This new term directly scales the final loss, ensuring that pairs with a larger absolute difference in cost contribute more significantly to the gradient. The final loss is `cost_scale * adaptive_gamma * preference_error`, which elegantly combines the dynamic margin, adaptive weighting, and cost-proportional penalty into a single, stable expression.\n\nWhen `cost(a) < cost(b)`, the `dynamic_margin` is negative, `cost_scale` is positive, and the loss encourages `logp_diff` to be positive and larger than the margin to drive `preference_error` to zero, correctly preferring `a`.", "pseudocode": "1. Compute the log-probability difference: logp_diff = logp_a - logp_b.\n2. Compute the signed, batch-relative rank gap: cost_gap_signed = rank_gap(cost_a, cost_b).\n3. Standardize the rank gap across the batch: cost_gap_zscored = zscore(cost_gap_signed).\n4. Create a bounded dynamic margin from the standardized rank gap (inherited from Parent 0): dynamic_margin = beta * tanh(cost_gap_zscored).\n5. Create an adaptive weight based on the margin's magnitude (inherited from Parent 1): adaptive_gamma = sigmoid(gamma * abs(dynamic_margin)).\n6. Compute the preference error using a smooth hinge term: preference_error = softplus(dynamic_margin - logp_diff).\n7. Create a cost-proportional scaling factor (new coupling idea): cost_scale = softplus(cost_b - cost_a).\n8. Combine the terms, scaling the hinge error by both the adaptive weight and the cost difference (new coupling idea): loss = cost_scale * adaptive_gamma * preference_error.", "hyperparams": {"beta": 1.0, "gamma": 1.0}, "operators_used": ["rank_gap", "zscore", "tanh", "sigmoid", "softplus"], "implementation_hint": {"expects": ["cost_a", "cost_b", "logp_a", "logp_b"], "returns": "scalar"}}, "fitness": {"hf_like_score": 8.483667850494385, "validation_objective": 8.483667850494385, "generalization_penalty": 0.0, "generalization_objectives": {"20": 4.000878572463989}, "train_score_mean": 10.476347732543946, "train_loss_mean": 0.598932433128357, "pair_count": 12902387, "config": {"hf": {"problem": "tsp", "hf_steps": 100, "train_problem_size": 100, "valid_problem_sizes": [20], "train_batch_size": 64, "pomo_size": 64, "learning_rate": 0.0003, "weight_decay": 1e-06, "alpha": 0.05, "device": "cuda", "seed": 1234, "num_validation_episodes": 128, "validation_batch_size": 64, "generalization_penalty_weight": 1.0, "pool_version": "v0"}, "free_loss": {"f1_steps": 100, "f2_steps": 100, "f3_enabled": false}}, "loss_ir": {"name": "Rank-Calibrated Dynamic Hinge Loss", "intuition": "This loss function creates a hybrid that inherits the dynamic margin from Parent 0 and the adaptive temperature scaling from Parent 1, while introducing a new coupling mechanism that makes the loss penalty proportional to the raw cost difference.\n\nFrom Parent 0 (Adaptive Hinge-Sigmoid Hybrid Loss), we inherit the core concept of a `dynamic_margin` calculated as `beta * tanh(zscore(rank_gap(cost_a, cost_b)))`. This margin dynamically adapts based on the relative ranking of the cost pair within the batch, providing a flexible target for the model's preference score.\n\nFrom Parent 1 (Adaptively Weighted Hinge-Sigmoid Loss with Rank-Gap Scaling), we inherit the idea of an `adaptive_weight` (here called `adaptive_gamma`) that is a function of the margin's magnitude, `sigmoid(gamma * abs(dynamic_margin))`. This makes the loss more sensitive to pairs with a clearer preference signal (larger margin) and more lenient for ambiguous pairs.\n\nWe introduce two new coupling ideas. First, we compute a core `preference_error` using the classic hinge-loss structure `softplus(dynamic_margin - logp_diff)`, which penalizes the model only when its preference `logp_diff` is insufficient to overcome the `dynamic_margin`. Second, we introduce a `cost_scale` factor, `softplus(cost_b - cost_a)`. This new term directly scales the final loss, ensuring that pairs with a larger absolute difference in cost contribute more significantly to the gradient. The final loss is `cost_scale * adaptive_gamma * preference_error`, which elegantly combines the dynamic margin, adaptive weighting, and cost-proportional penalty into a single, stable expression.\n\nWhen `cost(a) < cost(b)`, the `dynamic_margin` is negative, `cost_scale` is positive, and the loss encourages `logp_diff` to be positive and larger than the margin to drive `preference_error` to zero, correctly preferring `a`.", "hyperparams": {"beta": 1.0, "gamma": 1.0}, "operators_used": ["rank_gap", "zscore", "tanh", "sigmoid", "softplus"]}}, "better_than_baseline": false}
{"generation": 7, "index": 2, "ir": {"name": "Progressive Margin Hinge Loss with Adaptive Regularization", "intuition": "This loss function constructs a hybrid that focuses on progressively refining the model's preference margin, borrowing stability and adaptive mechanisms from both parents while introducing a new regularization term.\n\nFrom Parent 0 ('Adaptive Hinge-Sigmoid Hybrid Loss'), we inherit the core structure of a smooth hinge loss based on a dynamic margin: `softplus(dynamic_margin - logp_diff)`. This ensures the loss is zero when the model's preference `logp_diff` correctly exceeds a target margin defined by the rank gap. We also inherit the `dynamic_margin` itself, which is a bounded, standardized measure of the cost difference: `beta * tanh(zscore(rank_gap(...)))`.\n\nFrom Parent 1 ('Adaptively Weighted Hinge-Sigmoid Loss with Rank-Gap Scaling'), we inherit the idea of an adaptive weight, `adaptive_alpha = sigmoid(gamma * abs(dynamic_margin))`, which modulates the loss based on the confidence of the preference (i.e., the magnitude of the margin). This makes the loss more sensitive to clear-cut cases.\n\nWe introduce two new coupling ideas. First, instead of using the adaptive weight to scale the entire loss, we use it to create an 'adaptive regularization' term. The final loss is `hinge_error + adaptive_alpha * regularization_term`. The `hinge_error` is the primary driver for learning the preference order. The second new idea is the `regularization_term` itself, `softplus(-logp_diff)`. This term gently penalizes the model for being overly confident in the wrong direction (`logp_diff` is very negative) and is inspired by the standard sigmoid loss `logsigmoid(-logp_diff)`. The `adaptive_alpha` ensures this regularization is strongest for pairs with large, clear cost differences, preventing the model from making large errors on obvious examples.\n\nWhen `cost(a) < cost(b)`, `dynamic_margin` is negative. The `hinge_error` encourages `logp_diff` to be positive and larger than this margin. The `regularization_term` also encourages `logp_diff` to be positive. The combined effect is a strong push to prefer `a` over `b`.", "pseudocode": "1. Compute the log-probability difference: logp_diff = logp_a - logp_b.\n2. Compute the signed, batch-relative rank gap: cost_gap_signed = rank_gap(cost_a, cost_b).\n3. Standardize the rank gap across the batch: cost_gap_zscored = zscore(cost_gap_signed).\n4. Create a bounded dynamic margin from the standardized rank gap (inherited from Parent 0): dynamic_margin = beta * tanh(cost_gap_zscored).\n5. Compute the primary preference error using a smooth hinge loss (inherited from Parent 0): hinge_error = softplus(dynamic_margin - logp_diff).\n6. Create an adaptive weight based on the margin's magnitude (inherited from Parent 1): adaptive_alpha = sigmoid(gamma * abs(dynamic_margin)).\n7. Define a stable regularization term that penalizes incorrect preference direction (new coupling idea): regularization_term = softplus(-logp_diff).\n8. Combine the primary hinge error with the adaptively weighted regularization term (new coupling idea): loss = hinge_error + adaptive_alpha * regularization_term.", "hyperparams": {"beta": 1.0, "gamma": 1.0}, "operators_used": ["rank_gap", "zscore", "tanh", "softplus", "sigmoid"], "implementation_hint": {"expects": ["cost_a", "cost_b", "logp_a", "logp_b"], "returns": "scalar"}}, "fitness": {"hf_like_score": 8.4793701171875, "validation_objective": 8.4793701171875, "generalization_penalty": 0.0, "generalization_objectives": {"20": 3.987684726715088}, "train_score_mean": 10.47684471130371, "train_loss_mean": 0.5985438603162766, "pair_count": 12902388, "config": {"hf": {"problem": "tsp", "hf_steps": 100, "train_problem_size": 100, "valid_problem_sizes": [20], "train_batch_size": 64, "pomo_size": 64, "learning_rate": 0.0003, "weight_decay": 1e-06, "alpha": 0.05, "device": "cuda", "seed": 1234, "num_validation_episodes": 128, "validation_batch_size": 64, "generalization_penalty_weight": 1.0, "pool_version": "v0"}, "free_loss": {"f1_steps": 100, "f2_steps": 100, "f3_enabled": false}}, "loss_ir": {"name": "Progressive Margin Hinge Loss with Adaptive Regularization", "intuition": "This loss function constructs a hybrid that focuses on progressively refining the model's preference margin, borrowing stability and adaptive mechanisms from both parents while introducing a new regularization term.\n\nFrom Parent 0 ('Adaptive Hinge-Sigmoid Hybrid Loss'), we inherit the core structure of a smooth hinge loss based on a dynamic margin: `softplus(dynamic_margin - logp_diff)`. This ensures the loss is zero when the model's preference `logp_diff` correctly exceeds a target margin defined by the rank gap. We also inherit the `dynamic_margin` itself, which is a bounded, standardized measure of the cost difference: `beta * tanh(zscore(rank_gap(...)))`.\n\nFrom Parent 1 ('Adaptively Weighted Hinge-Sigmoid Loss with Rank-Gap Scaling'), we inherit the idea of an adaptive weight, `adaptive_alpha = sigmoid(gamma * abs(dynamic_margin))`, which modulates the loss based on the confidence of the preference (i.e., the magnitude of the margin). This makes the loss more sensitive to clear-cut cases.\n\nWe introduce two new coupling ideas. First, instead of using the adaptive weight to scale the entire loss, we use it to create an 'adaptive regularization' term. The final loss is `hinge_error + adaptive_alpha * regularization_term`. The `hinge_error` is the primary driver for learning the preference order. The second new idea is the `regularization_term` itself, `softplus(-logp_diff)`. This term gently penalizes the model for being overly confident in the wrong direction (`logp_diff` is very negative) and is inspired by the standard sigmoid loss `logsigmoid(-logp_diff)`. The `adaptive_alpha` ensures this regularization is strongest for pairs with large, clear cost differences, preventing the model from making large errors on obvious examples.\n\nWhen `cost(a) < cost(b)`, `dynamic_margin` is negative. The `hinge_error` encourages `logp_diff` to be positive and larger than this margin. The `regularization_term` also encourages `logp_diff` to be positive. The combined effect is a strong push to prefer `a` over `b`.", "hyperparams": {"beta": 1.0, "gamma": 1.0}, "operators_used": ["rank_gap", "zscore", "tanh", "softplus", "sigmoid"]}}, "better_than_baseline": false}
{"generation": 7, "index": 3, "ir": {"name": "Rank-Calibrated Dynamic Hinge Loss", "intuition": "This loss function creates a hybrid that uses a dynamic, rank-based margin for preference comparison, and introduces a novel calibration mechanism to adjust the loss severity based on both the rank gap and the model's current confidence.\n\nFrom Parent 0 (Adaptive Hinge-Sigmoid Hybrid Loss), we inherit the core concept of a `dynamic_margin` derived from the standardized rank gap (`beta * tanh(zscore(rank_gap(...)))`). We also inherit the fundamental hinge-loss structure where the model is penalized only if its preference `logp_diff` fails to exceed this margin, using `softplus(dynamic_margin - logp_diff)` to compute a non-negative `preference_error`.\n\nFrom Parent 1 (Adaptively Weighted Hinge-Sigmoid Loss), we inherit the idea of an `adaptive_weight` (`sigmoid(gamma * abs(dynamic_margin))`) that modulates the loss based on the magnitude of the required margin. This makes the loss more sensitive to pairs with clear cost separation.\n\nAs a new coupling idea, we introduce a 'confidence penalty factor'. This factor, `confidence_penalty = exp(-abs(logp_diff))`, becomes smaller as the model's preference (the absolute difference in log-probabilities) becomes stronger, regardless of whether the preference is correct or not. We use this factor to scale the adaptive weight from Parent 1, creating a `calibrated_weight = confidence_penalty * adaptive_weight`. This calibrated weight is then used to scale the preference error from Parent 0. The final loss is `calibrated_weight * preference_error`. This coupling ensures that when the model is very confident (large `abs(logp_diff)`), the loss is attenuated, preventing overly large gradients from confident-but-wrong predictions and stabilizing training. Conversely, it applies a stronger penalty when the model is uncertain.\n\nWhen `cost(a) < cost(b)`, `dynamic_margin` is negative. The loss encourages `logp_diff` to be positive and larger than this margin to make `preference_error` zero, correctly preferring `a`.", "pseudocode": "1. Compute the log-probability difference: logp_diff = logp_a - logp_b.\n2. Compute the signed, batch-relative rank gap: cost_gap_signed = rank_gap(cost_a, cost_b).\n3. Standardize the rank gap across the batch: cost_gap_zscored = zscore(cost_gap_signed).\n4. Create a bounded dynamic margin from the standardized rank gap (inherited from Parent 0): dynamic_margin = beta * tanh(cost_gap_zscored).\n5. Compute the preference error using a smooth hinge-like term (inherited from Parent 0): preference_error = softplus(dynamic_margin - logp_diff).\n6. Create an adaptive weight based on the margin's magnitude (inherited from Parent 1): adaptive_weight = sigmoid(gamma * abs(dynamic_margin)).\n7. Introduce a new coupling idea: a confidence penalty factor that decreases as model confidence increases: confidence_penalty = exp(-abs(logp_diff)).\n8. Couple the adaptive weight and confidence penalty to create a calibrated weight: calibrated_weight = confidence_penalty * adaptive_weight.\n9. Compute the final loss by scaling the preference error with the calibrated weight: loss = calibrated_weight * preference_error.", "hyperparams": {"beta": 1.0, "gamma": 1.0}, "operators_used": ["rank_gap", "zscore", "tanh", "softplus", "sigmoid", "exp"], "implementation_hint": {"expects": ["cost_a", "cost_b", "logp_a", "logp_b"], "returns": "scalar"}}, "fitness": {"hf_like_score": 8.43929672241211, "validation_objective": 8.43929672241211, "generalization_penalty": 0.0, "generalization_objectives": {"20": 4.028515338897705}, "train_score_mean": 10.460430355072022, "train_loss_mean": 0.6004841834306717, "pair_count": 12902389, "config": {"hf": {"problem": "tsp", "hf_steps": 100, "train_problem_size": 100, "valid_problem_sizes": [20], "train_batch_size": 64, "pomo_size": 64, "learning_rate": 0.0003, "weight_decay": 1e-06, "alpha": 0.05, "device": "cuda", "seed": 1234, "num_validation_episodes": 128, "validation_batch_size": 64, "generalization_penalty_weight": 1.0, "pool_version": "v0"}, "free_loss": {"f1_steps": 100, "f2_steps": 100, "f3_enabled": false}}, "loss_ir": {"name": "Rank-Calibrated Dynamic Hinge Loss", "intuition": "This loss function creates a hybrid that uses a dynamic, rank-based margin for preference comparison, and introduces a novel calibration mechanism to adjust the loss severity based on both the rank gap and the model's current confidence.\n\nFrom Parent 0 (Adaptive Hinge-Sigmoid Hybrid Loss), we inherit the core concept of a `dynamic_margin` derived from the standardized rank gap (`beta * tanh(zscore(rank_gap(...)))`). We also inherit the fundamental hinge-loss structure where the model is penalized only if its preference `logp_diff` fails to exceed this margin, using `softplus(dynamic_margin - logp_diff)` to compute a non-negative `preference_error`.\n\nFrom Parent 1 (Adaptively Weighted Hinge-Sigmoid Loss), we inherit the idea of an `adaptive_weight` (`sigmoid(gamma * abs(dynamic_margin))`) that modulates the loss based on the magnitude of the required margin. This makes the loss more sensitive to pairs with clear cost separation.\n\nAs a new coupling idea, we introduce a 'confidence penalty factor'. This factor, `confidence_penalty = exp(-abs(logp_diff))`, becomes smaller as the model's preference (the absolute difference in log-probabilities) becomes stronger, regardless of whether the preference is correct or not. We use this factor to scale the adaptive weight from Parent 1, creating a `calibrated_weight = confidence_penalty * adaptive_weight`. This calibrated weight is then used to scale the preference error from Parent 0. The final loss is `calibrated_weight * preference_error`. This coupling ensures that when the model is very confident (large `abs(logp_diff)`), the loss is attenuated, preventing overly large gradients from confident-but-wrong predictions and stabilizing training. Conversely, it applies a stronger penalty when the model is uncertain.\n\nWhen `cost(a) < cost(b)`, `dynamic_margin` is negative. The loss encourages `logp_diff` to be positive and larger than this margin to make `preference_error` zero, correctly preferring `a`.", "hyperparams": {"beta": 1.0, "gamma": 1.0}, "operators_used": ["rank_gap", "zscore", "tanh", "softplus", "sigmoid", "exp"]}}, "better_than_baseline": true}
{"generation": 7, "index": 4, "ir": {"name": "Rank-Calibrated Dynamic Margin Loss", "intuition": "This loss function creates a novel structure by combining a dynamic margin from Parent 0 with an adaptive scaling mechanism inspired by Parent 1, coupled with a new stability and calibration technique.\n\nFrom Parent 0 (Adaptive Hinge-Sigmoid Hybrid Loss), we inherit the core concept of a `dynamic_margin` that is a function of the standardized rank gap. This allows the required preference margin to adapt based on the relative ranking of costs within a batch.\n\nFrom Parent 1 (Adaptively Weighted Hinge-Sigmoid Loss), we inherit the idea of using an adaptive scaling factor, but we modify its source. Instead of being based on the margin, our adaptive scaling (`adaptive_gamma`) is based on the model's own confidence, `logp_diff`, making the loss more sensitive to confident but incorrect predictions.\n\nAs a new coupling idea, we introduce a `rank_gap_scale`. This term, derived directly from the unnormalized rank gap, multiplies the margin itself. This ensures that pairs with a larger absolute difference in cost ranks have a proportionally larger margin to overcome, directly embedding cost-rank importance into the preference target. A second new idea is the final loss structure, `softplus(scaled_dynamic_margin - logp_diff) * adaptive_gamma`. This combines the scaled margin with the adaptive gamma, creating a loss that is zero for correctly classified pairs but is dynamically amplified for incorrect pairs, especially when the model is confidently wrong.\n\nWhen `cost(a) < cost(b)`, both `rank_gap_signed` and `dynamic_margin` are negative. The `rank_gap_scale` is positive. The `scaled_dynamic_margin` is thus also negative. The loss encourages `logp_diff` to be positive and larger than this negative margin to drive the `softplus` term to zero, thereby correctly preferring `a`.", "pseudocode": "1. Compute the log-probability difference: logp_diff = logp_a - logp_b.\n2. Compute the signed, batch-relative rank gap: cost_gap_signed = rank_gap(cost_a, cost_b).\n3. Standardize the rank gap across the batch: cost_gap_zscored = zscore(cost_gap_signed).\n4. Create a base dynamic margin from the standardized rank gap (inherited from Parent 0): dynamic_margin = beta * tanh(cost_gap_zscored).\n5. Create a rank-gap scaling factor from the unnormalized rank gap (new coupling idea): rank_gap_scale = 1.0 + softplus(abs(cost_gap_signed)).\n6. Scale the margin by the rank-gap scale to create a more sensitive preference target (new coupling idea): scaled_dynamic_margin = rank_gap_scale * dynamic_margin.\n7. Create an adaptive gamma based on the model's confidence in its preference (inspired by Parent 1): adaptive_gamma = 1.0 + gamma * sigmoid(abs(logp_diff)).\n8. Compute the final loss by combining the scaled margin and adaptive gamma within a stable `softplus` structure: loss = softplus(scaled_dynamic_margin - logp_diff) * adaptive_gamma.", "hyperparams": {"beta": 1.0, "gamma": 1.0}, "operators_used": ["rank_gap", "zscore", "tanh", "softplus", "sigmoid"], "implementation_hint": {"expects": ["cost_a", "cost_b", "logp_a", "logp_b"], "returns": "scalar"}}, "fitness": {"hf_like_score": 8.453155040740967, "validation_objective": 8.453155040740967, "generalization_penalty": 0.0, "generalization_objectives": {"20": 4.024904489517212}, "train_score_mean": 10.476648530960084, "train_loss_mean": 0.5992891120910645, "pair_count": 12902390, "config": {"hf": {"problem": "tsp", "hf_steps": 100, "train_problem_size": 100, "valid_problem_sizes": [20], "train_batch_size": 64, "pomo_size": 64, "learning_rate": 0.0003, "weight_decay": 1e-06, "alpha": 0.05, "device": "cuda", "seed": 1234, "num_validation_episodes": 128, "validation_batch_size": 64, "generalization_penalty_weight": 1.0, "pool_version": "v0"}, "free_loss": {"f1_steps": 100, "f2_steps": 100, "f3_enabled": false}}, "loss_ir": {"name": "Rank-Calibrated Dynamic Margin Loss", "intuition": "This loss function creates a novel structure by combining a dynamic margin from Parent 0 with an adaptive scaling mechanism inspired by Parent 1, coupled with a new stability and calibration technique.\n\nFrom Parent 0 (Adaptive Hinge-Sigmoid Hybrid Loss), we inherit the core concept of a `dynamic_margin` that is a function of the standardized rank gap. This allows the required preference margin to adapt based on the relative ranking of costs within a batch.\n\nFrom Parent 1 (Adaptively Weighted Hinge-Sigmoid Loss), we inherit the idea of using an adaptive scaling factor, but we modify its source. Instead of being based on the margin, our adaptive scaling (`adaptive_gamma`) is based on the model's own confidence, `logp_diff`, making the loss more sensitive to confident but incorrect predictions.\n\nAs a new coupling idea, we introduce a `rank_gap_scale`. This term, derived directly from the unnormalized rank gap, multiplies the margin itself. This ensures that pairs with a larger absolute difference in cost ranks have a proportionally larger margin to overcome, directly embedding cost-rank importance into the preference target. A second new idea is the final loss structure, `softplus(scaled_dynamic_margin - logp_diff) * adaptive_gamma`. This combines the scaled margin with the adaptive gamma, creating a loss that is zero for correctly classified pairs but is dynamically amplified for incorrect pairs, especially when the model is confidently wrong.\n\nWhen `cost(a) < cost(b)`, both `rank_gap_signed` and `dynamic_margin` are negative. The `rank_gap_scale` is positive. The `scaled_dynamic_margin` is thus also negative. The loss encourages `logp_diff` to be positive and larger than this negative margin to drive the `softplus` term to zero, thereby correctly preferring `a`.", "hyperparams": {"beta": 1.0, "gamma": 1.0}, "operators_used": ["rank_gap", "zscore", "tanh", "softplus", "sigmoid"]}}, "better_than_baseline": false}
{"generation": 7, "index": 5, "ir": {"name": "Rank-Calibrated Dynamic Margin Loss", "intuition": "This child loss creates a novel structure by combining a dynamic margin with an adaptive, rank-calibrated weighting scheme, introducing a new coupling mechanism to modulate the loss signal.\n\nFrom Parent 0 (`Adaptive Hinge-Sigmoid Hybrid Loss`), we inherit the core concept of a `dynamic_margin` derived from a standardized rank gap (`beta * tanh(zscore(rank_gap(...)))`). This creates a target preference margin that adapts to the relative cost ranking within a batch.\n\nFrom Parent 1 (`Adaptively Weighted Hinge-Sigmoid Loss with Rank-Gap Scaling`), we inherit the idea of using an `adaptive_alpha` weight, which is a sigmoid function of the margin's magnitude. This makes the loss more sensitive to pairs with a clear preference signal (large margin) and more lenient for ambiguous pairs (small margin).\n\nWe introduce two new coupling ideas. First, instead of using the adaptive weight to combine two loss terms or scale a final loss value, we use it to *dynamically adjust the temperature* of a `logsigmoid` function. The core loss term is `logsigmoid(adaptive_alpha * (logp_diff - dynamic_margin))`. This elegantly couples the margin-based weighting from Parent 1 with the preference alignment task from Parent 0. Second, we introduce a `rank_gap_scale` factor, similar to Parent 1, but apply it as a multiplier *inside* the `logsigmoid`. This directly scales the preference signal by the raw rank gap, ensuring that pairs with larger cost differences contribute more strongly to the gradient signal before the non-linearity is applied, adding another layer of calibration.\n\nWhen `cost(a) < cost(b)`, `dynamic_margin` is negative, and `rank_gap_scale` is positive. The loss encourages `logp_diff` to be positive, correctly preferring `a` over `b`.", "pseudocode": "1. Compute the log-probability difference: logp_diff = logp_a - logp_b.\n2. Compute the signed, batch-relative rank gap: cost_gap_signed = rank_gap(cost_a, cost_b).\n3. Standardize the rank gap across the batch: cost_gap_zscored = zscore(cost_gap_signed).\n4. Create a bounded dynamic margin from the standardized rank gap (inherited from Parent 0): dynamic_margin = beta * tanh(cost_gap_zscored).\n5. Create an adaptive alpha weight based on the margin's magnitude (inherited from Parent 1): adaptive_alpha = sigmoid(gamma * abs(dynamic_margin)).\n6. Create a rank gap scaling factor (new coupling idea, inspired by Parent 1): rank_gap_scale = 1.0 + softplus(cost_gap_signed).\n7. Compute the scaled preference difference, where the rank gap directly modulates the signal (new coupling idea): scaled_diff = rank_gap_scale * (logp_diff - dynamic_margin).\n8. Compute the final loss by using the adaptive alpha as a dynamic temperature inside a logsigmoid function (new coupling idea): loss = -logsigmoid(adaptive_alpha * scaled_diff).", "hyperparams": {"beta": 1.5, "gamma": 1.0}, "operators_used": ["rank_gap", "zscore", "tanh", "sigmoid", "softplus", "logsigmoid"], "implementation_hint": {"expects": ["cost_a", "cost_b", "logp_a", "logp_b"], "returns": "scalar"}}, "fitness": {"hf_like_score": 8.420725345611572, "validation_objective": 8.420725345611572, "generalization_penalty": 0.0, "generalization_objectives": {"20": 3.9912867546081543}, "train_score_mean": 10.49334231376648, "train_loss_mean": 0.6002590352296829, "pair_count": 12902390, "config": {"hf": {"problem": "tsp", "hf_steps": 100, "train_problem_size": 100, "valid_problem_sizes": [20], "train_batch_size": 64, "pomo_size": 64, "learning_rate": 0.0003, "weight_decay": 1e-06, "alpha": 0.05, "device": "cuda", "seed": 1234, "num_validation_episodes": 128, "validation_batch_size": 64, "generalization_penalty_weight": 1.0, "pool_version": "v0"}, "free_loss": {"f1_steps": 100, "f2_steps": 100, "f3_enabled": false}}, "loss_ir": {"name": "Rank-Calibrated Dynamic Margin Loss", "intuition": "This child loss creates a novel structure by combining a dynamic margin with an adaptive, rank-calibrated weighting scheme, introducing a new coupling mechanism to modulate the loss signal.\n\nFrom Parent 0 (`Adaptive Hinge-Sigmoid Hybrid Loss`), we inherit the core concept of a `dynamic_margin` derived from a standardized rank gap (`beta * tanh(zscore(rank_gap(...)))`). This creates a target preference margin that adapts to the relative cost ranking within a batch.\n\nFrom Parent 1 (`Adaptively Weighted Hinge-Sigmoid Loss with Rank-Gap Scaling`), we inherit the idea of using an `adaptive_alpha` weight, which is a sigmoid function of the margin's magnitude. This makes the loss more sensitive to pairs with a clear preference signal (large margin) and more lenient for ambiguous pairs (small margin).\n\nWe introduce two new coupling ideas. First, instead of using the adaptive weight to combine two loss terms or scale a final loss value, we use it to *dynamically adjust the temperature* of a `logsigmoid` function. The core loss term is `logsigmoid(adaptive_alpha * (logp_diff - dynamic_margin))`. This elegantly couples the margin-based weighting from Parent 1 with the preference alignment task from Parent 0. Second, we introduce a `rank_gap_scale` factor, similar to Parent 1, but apply it as a multiplier *inside* the `logsigmoid`. This directly scales the preference signal by the raw rank gap, ensuring that pairs with larger cost differences contribute more strongly to the gradient signal before the non-linearity is applied, adding another layer of calibration.\n\nWhen `cost(a) < cost(b)`, `dynamic_margin` is negative, and `rank_gap_scale` is positive. The loss encourages `logp_diff` to be positive, correctly preferring `a` over `b`.", "hyperparams": {"beta": 1.5, "gamma": 1.0}, "operators_used": ["rank_gap", "zscore", "tanh", "sigmoid", "softplus", "logsigmoid"]}}, "better_than_baseline": true}
{"generation": 7, "index": 6, "ir": {"name": "Rank-Gated Sigmoid Hinge Loss", "intuition": "This loss function creates a hybrid that selectively applies a hinge-like penalty based on the significance of the rank difference between two responses. It smoothly transitions between a lenient sigmoid-based loss for small rank gaps and a stricter hinge-based loss for large rank gaps.\n\nFrom Parent 0 ('Adaptive Hinge-Sigmoid Hybrid Loss'), we inherit the core structure of a preference error based on a smooth hinge mechanism: `preference_error = softplus(dynamic_margin - logp_diff)`. This term is zero when the model's preference `logp_diff` correctly exceeds the `dynamic_margin`, ensuring no penalty for correct preferences.\n\nFrom Parent 1 ('Adaptively Weighted Hinge-Sigmoid Loss with Rank-Gap Scaling'), we inherit the idea of using an adaptive weight, `adaptive_alpha`, derived from the preference margin's magnitude via a `sigmoid` function. This makes the loss more sensitive to clear-cut preferences.\n\nWe introduce two new coupling ideas. First, we create a 'rank gate' (`rank_gate`) using a `sigmoid` function on the non-standardized rank gap. This gate smoothly transitions from near 0 for small rank differences to near 1 for large rank differences. Second, we use this gate to create a blended error term: `blended_error = (1 - rank_gate) * sigmoid_error + rank_gate * hinge_error`. For small rank gaps, the loss is dominated by a simple `logsigmoid` term (`sigmoid_error`), which is more forgiving. For large, significant rank gaps, the loss is dominated by the `hinge_error` (the inherited `preference_error` from Parent 0, weighted by `adaptive_alpha` from Parent 1). This gating mechanism couples the choice of loss penalty (lenient vs. strict) directly to the perceived importance of the preference, as measured by the rank gap.\n\nWhen `cost(a) < cost(b)`, the `dynamic_margin` is negative and the `rank_gap` is negative. The loss encourages `logp_diff` to be positive. For large cost differences, the `rank_gate` approaches 1, activating the hinge component which pushes `logp_diff` to be greater than the `dynamic_margin` to achieve zero loss.", "pseudocode": "1. Compute the log-probability difference: logp_diff = logp_a - logp_b.\n2. Compute the signed, batch-relative rank gap: cost_gap_signed = rank_gap(cost_a, cost_b).\n3. Standardize the rank gap across the batch: cost_gap_zscored = zscore(cost_gap_signed).\n4. Create a bounded dynamic margin from the standardized rank gap (structure from Parent 0): dynamic_margin = beta * tanh(cost_gap_zscored).\n5. Create an adaptive weight based on the margin's magnitude (inherited from Parent 1): adaptive_alpha = sigmoid(gamma * abs(dynamic_margin)).\n6. Compute the hinge-based error component (structure from Parent 0): hinge_error = adaptive_alpha * softplus(dynamic_margin - logp_diff).\n7. Compute a simple sigmoid-based error component: sigmoid_error = -logsigmoid(logp_diff).\n8. Create a 'rank gate' based on the non-standardized rank gap (new coupling idea): rank_gate = sigmoid(tau * abs(cost_gap_signed)).\n9. Use the rank gate to blend the two error components (new coupling idea): loss = (1.0 - rank_gate) * sigmoid_error + rank_gate * hinge_error.", "hyperparams": {"beta": 1.0, "gamma": 1.5, "tau": 0.5}, "operators_used": ["rank_gap", "zscore", "tanh", "sigmoid", "softplus", "logsigmoid"], "implementation_hint": {"expects": ["cost_a", "cost_b", "logp_a", "logp_b"], "returns": "scalar"}}, "fitness": {"hf_like_score": 8.446029663085938, "validation_objective": 8.446029663085938, "generalization_penalty": 0.0, "generalization_objectives": {"20": 3.9926185607910156}, "train_score_mean": 10.487318353652954, "train_loss_mean": 0.6017898392677307, "pair_count": 12902392, "config": {"hf": {"problem": "tsp", "hf_steps": 100, "train_problem_size": 100, "valid_problem_sizes": [20], "train_batch_size": 64, "pomo_size": 64, "learning_rate": 0.0003, "weight_decay": 1e-06, "alpha": 0.05, "device": "cuda", "seed": 1234, "num_validation_episodes": 128, "validation_batch_size": 64, "generalization_penalty_weight": 1.0, "pool_version": "v0"}, "free_loss": {"f1_steps": 100, "f2_steps": 100, "f3_enabled": false}}, "loss_ir": {"name": "Rank-Gated Sigmoid Hinge Loss", "intuition": "This loss function creates a hybrid that selectively applies a hinge-like penalty based on the significance of the rank difference between two responses. It smoothly transitions between a lenient sigmoid-based loss for small rank gaps and a stricter hinge-based loss for large rank gaps.\n\nFrom Parent 0 ('Adaptive Hinge-Sigmoid Hybrid Loss'), we inherit the core structure of a preference error based on a smooth hinge mechanism: `preference_error = softplus(dynamic_margin - logp_diff)`. This term is zero when the model's preference `logp_diff` correctly exceeds the `dynamic_margin`, ensuring no penalty for correct preferences.\n\nFrom Parent 1 ('Adaptively Weighted Hinge-Sigmoid Loss with Rank-Gap Scaling'), we inherit the idea of using an adaptive weight, `adaptive_alpha`, derived from the preference margin's magnitude via a `sigmoid` function. This makes the loss more sensitive to clear-cut preferences.\n\nWe introduce two new coupling ideas. First, we create a 'rank gate' (`rank_gate`) using a `sigmoid` function on the non-standardized rank gap. This gate smoothly transitions from near 0 for small rank differences to near 1 for large rank differences. Second, we use this gate to create a blended error term: `blended_error = (1 - rank_gate) * sigmoid_error + rank_gate * hinge_error`. For small rank gaps, the loss is dominated by a simple `logsigmoid` term (`sigmoid_error`), which is more forgiving. For large, significant rank gaps, the loss is dominated by the `hinge_error` (the inherited `preference_error` from Parent 0, weighted by `adaptive_alpha` from Parent 1). This gating mechanism couples the choice of loss penalty (lenient vs. strict) directly to the perceived importance of the preference, as measured by the rank gap.\n\nWhen `cost(a) < cost(b)`, the `dynamic_margin` is negative and the `rank_gap` is negative. The loss encourages `logp_diff` to be positive. For large cost differences, the `rank_gate` approaches 1, activating the hinge component which pushes `logp_diff` to be greater than the `dynamic_margin` to achieve zero loss.", "hyperparams": {"beta": 1.0, "gamma": 1.5, "tau": 0.5}, "operators_used": ["rank_gap", "zscore", "tanh", "sigmoid", "softplus", "logsigmoid"]}}, "better_than_baseline": true}
{"generation": 7, "index": 7, "ir": {"name": "Adaptive Rank-Modulated Hinge Loss", "intuition": "This loss function creates a robust hybrid by combining a dynamic margin with an adaptive error scaling, and introduces a new coupling mechanism that modulates the loss based on the un-normalized rank gap.\n\nFrom Parent 0 (Adaptive Hinge-Sigmoid Hybrid Loss), we inherit the core idea of a dynamic margin that adapts based on the batch-wide rank distribution. This is achieved by computing a `dynamic_margin = beta * tanh(zscore(rank_gap(...)))`. We also inherit the use of `softplus` to create a smooth, non-negative hinge-like `preference_error` term: `softplus(dynamic_margin - logp_diff)`.\n\nFrom Parent 1 (Adaptively Weighted Hinge-Sigmoid Loss), we inherit the concept of an adaptive weight, `adaptive_alpha`, which is a sigmoid function of the margin's magnitude. This makes the loss more sensitive to clear-cut preferences and more lenient for ambiguous ones.\n\nAs a new coupling idea, we introduce a `rank_gap_modulator`. This term is derived from the non-standardized, absolute rank gap and is used to directly scale the `preference_error` *before* it is passed to the final logarithmic function. The intuition is that pairs with a larger rank difference are more significant and should thus amplify the underlying preference error. The final loss structure becomes `log(1.0 + rank_gap_modulator * preference_error)`, where the adaptive weight `adaptive_alpha` is applied to the final result. This creates a tight coupling where the raw rank difference directly influences the magnitude of the error signal itself, which is then weighted by its perceived importance.\n\nWhen `cost(a) < cost(b)`, the `dynamic_margin` is negative. The loss encourages `logp_diff` to be positive and larger than this margin to make `preference_error` zero, thus correctly preferring `a`.", "pseudocode": "1. Compute the log-probability difference: logp_diff = logp_a - logp_b.\n2. Compute the signed, batch-relative rank gap: cost_gap_signed = rank_gap(cost_a, cost_b).\n3. Standardize the rank gap across the batch: cost_gap_zscored = zscore(cost_gap_signed).\n4. Create a bounded dynamic margin from the standardized rank gap (inherited from Parent 0): dynamic_margin = beta * tanh(cost_gap_zscored).\n5. Compute the preference error using a smooth hinge-like term (inherited from Parent 0): preference_error = softplus(dynamic_margin - logp_diff).\n6. Create an adaptive weight based on the margin's magnitude (inherited from Parent 1): adaptive_alpha = sigmoid(gamma * abs(dynamic_margin)).\n7. Create a rank gap modulator from the raw rank gap (new coupling idea): rank_gap_modulator = 1.0 + softplus(abs(cost_gap_signed)).\n8. Modulate the preference error with the rank gap modulator and compute the base loss using a stable log form: base_loss = log(1.0 + rank_gap_modulator * preference_error).\n9. Apply the adaptive weight to the final loss: loss = adaptive_alpha * base_loss.", "hyperparams": {"beta": 1.0, "gamma": 1.2}, "operators_used": ["rank_gap", "zscore", "tanh", "softplus", "sigmoid", "log"], "implementation_hint": {"expects": ["cost_a", "cost_b", "logp_a", "logp_b"], "returns": "scalar"}}, "fitness": {"hf_like_score": 8.446176052093506, "validation_objective": 8.446176052093506, "generalization_penalty": 0.0, "generalization_objectives": {"20": 4.006303548812866}, "train_score_mean": 10.472378520965576, "train_loss_mean": 0.6023607814311981, "pair_count": 12902389, "config": {"hf": {"problem": "tsp", "hf_steps": 100, "train_problem_size": 100, "valid_problem_sizes": [20], "train_batch_size": 64, "pomo_size": 64, "learning_rate": 0.0003, "weight_decay": 1e-06, "alpha": 0.05, "device": "cuda", "seed": 1234, "num_validation_episodes": 128, "validation_batch_size": 64, "generalization_penalty_weight": 1.0, "pool_version": "v0"}, "free_loss": {"f1_steps": 100, "f2_steps": 100, "f3_enabled": false}}, "loss_ir": {"name": "Adaptive Rank-Modulated Hinge Loss", "intuition": "This loss function creates a robust hybrid by combining a dynamic margin with an adaptive error scaling, and introduces a new coupling mechanism that modulates the loss based on the un-normalized rank gap.\n\nFrom Parent 0 (Adaptive Hinge-Sigmoid Hybrid Loss), we inherit the core idea of a dynamic margin that adapts based on the batch-wide rank distribution. This is achieved by computing a `dynamic_margin = beta * tanh(zscore(rank_gap(...)))`. We also inherit the use of `softplus` to create a smooth, non-negative hinge-like `preference_error` term: `softplus(dynamic_margin - logp_diff)`.\n\nFrom Parent 1 (Adaptively Weighted Hinge-Sigmoid Loss), we inherit the concept of an adaptive weight, `adaptive_alpha`, which is a sigmoid function of the margin's magnitude. This makes the loss more sensitive to clear-cut preferences and more lenient for ambiguous ones.\n\nAs a new coupling idea, we introduce a `rank_gap_modulator`. This term is derived from the non-standardized, absolute rank gap and is used to directly scale the `preference_error` *before* it is passed to the final logarithmic function. The intuition is that pairs with a larger rank difference are more significant and should thus amplify the underlying preference error. The final loss structure becomes `log(1.0 + rank_gap_modulator * preference_error)`, where the adaptive weight `adaptive_alpha` is applied to the final result. This creates a tight coupling where the raw rank difference directly influences the magnitude of the error signal itself, which is then weighted by its perceived importance.\n\nWhen `cost(a) < cost(b)`, the `dynamic_margin` is negative. The loss encourages `logp_diff` to be positive and larger than this margin to make `preference_error` zero, thus correctly preferring `a`.", "hyperparams": {"beta": 1.0, "gamma": 1.2}, "operators_used": ["rank_gap", "zscore", "tanh", "softplus", "sigmoid", "log"]}}, "better_than_baseline": true}
{"generation": 7, "index": 8, "ir": {"name": "Rank-Calibrated Dynamic Hinge Loss", "intuition": "This loss function creates a robust hinge-based preference loss by combining a dynamic margin with an adaptive temperature, and introduces a new coupling mechanism where the preference error is directly modulated by the raw rank gap.\n\nFrom Parent 0 (Adaptive Hinge-Sigmoid Hybrid Loss), we inherit the core concept of a `dynamic_margin` derived from a standardized rank gap (`beta * tanh(zscore(rank_gap(...)))`). This creates a margin that is sensitive to the relative ranking of a pair within the batch. We also inherit its use of `softplus` to form a non-negative, smooth hinge loss term.\n\nFrom Parent 1 (Adaptively Weighted Hinge-Sigmoid Loss with Rank-Gap Scaling), we inherit the idea of an adaptive scaling factor, but we apply it as an `adaptive_temperature` inside the loss calculation, similar to Parent 0's `adaptive_gamma`. This temperature, `1.0 + gamma * sigmoid(abs(dynamic_margin))`, makes the loss more sensitive to pairs with a clear preference margin (large `abs(dynamic_margin)`).\n\nAs a new coupling idea, we directly scale the `preference_error` by a factor derived from the raw, un-normalized rank gap (`rank_gap_scale = softplus(abs(cost_gap_signed))`). The intuition is to make the loss penalty proportional to the magnitude of the cost difference, ensuring that pairs with larger cost discrepancies contribute more significantly to the final loss. This `rank_gap_scale` modulates the error *before* the final logarithmic stabilization, coupling the raw cost signal with the preference error term more tightly than in the parents.\n\nWhen `cost(a) < cost(b)`, the `dynamic_margin` becomes negative. The loss encourages `logp_diff` to be positive and larger than this margin to drive the `preference_error` to zero, thus correctly preferring `a`.", "pseudocode": "1. Compute the log-probability difference: logp_diff = logp_a - logp_b.\n2. Compute the signed, batch-relative rank gap: cost_gap_signed = rank_gap(cost_a, cost_b).\n3. Standardize the rank gap across the batch: cost_gap_zscored = zscore(cost_gap_signed).\n4. Create a bounded dynamic margin from the standardized rank gap (inherited from Parent 0): dynamic_margin = beta * tanh(cost_gap_zscored).\n5. Compute a smooth hinge-like preference error (structure from Parent 0): preference_error = softplus(dynamic_margin - logp_diff).\n6. Create a scaling factor from the raw rank gap (new coupling idea): rank_gap_scale = softplus(abs(cost_gap_signed)).\n7. Modulate the preference error with the rank gap scale (new coupling idea): scaled_error = rank_gap_scale * preference_error.\n8. Create an adaptive temperature based on the margin's magnitude (inspired by Parent 1's weighting): adaptive_temperature = 1.0 + gamma * sigmoid(abs(dynamic_margin)).\n9. Combine the scaled error and adaptive temperature in a stable logarithmic form (structure from parents): loss = log(1.0 + adaptive_temperature * scaled_error).", "hyperparams": {"beta": 1.0, "gamma": 1.0}, "operators_used": ["rank_gap", "zscore", "tanh", "softplus", "sigmoid", "log"], "implementation_hint": {"expects": ["cost_a", "cost_b", "logp_a", "logp_b"], "returns": "scalar"}}, "fitness": {"hf_like_score": 8.39125394821167, "validation_objective": 8.39125394821167, "generalization_penalty": 0.0, "generalization_objectives": {"20": 4.0301079750061035}, "train_score_mean": 10.482086448669433, "train_loss_mean": 0.6041832911968231, "pair_count": 12902389, "config": {"hf": {"problem": "tsp", "hf_steps": 100, "train_problem_size": 100, "valid_problem_sizes": [20], "train_batch_size": 64, "pomo_size": 64, "learning_rate": 0.0003, "weight_decay": 1e-06, "alpha": 0.05, "device": "cuda", "seed": 1234, "num_validation_episodes": 128, "validation_batch_size": 64, "generalization_penalty_weight": 1.0, "pool_version": "v0"}, "free_loss": {"f1_steps": 100, "f2_steps": 100, "f3_enabled": false}}, "loss_ir": {"name": "Rank-Calibrated Dynamic Hinge Loss", "intuition": "This loss function creates a robust hinge-based preference loss by combining a dynamic margin with an adaptive temperature, and introduces a new coupling mechanism where the preference error is directly modulated by the raw rank gap.\n\nFrom Parent 0 (Adaptive Hinge-Sigmoid Hybrid Loss), we inherit the core concept of a `dynamic_margin` derived from a standardized rank gap (`beta * tanh(zscore(rank_gap(...)))`). This creates a margin that is sensitive to the relative ranking of a pair within the batch. We also inherit its use of `softplus` to form a non-negative, smooth hinge loss term.\n\nFrom Parent 1 (Adaptively Weighted Hinge-Sigmoid Loss with Rank-Gap Scaling), we inherit the idea of an adaptive scaling factor, but we apply it as an `adaptive_temperature` inside the loss calculation, similar to Parent 0's `adaptive_gamma`. This temperature, `1.0 + gamma * sigmoid(abs(dynamic_margin))`, makes the loss more sensitive to pairs with a clear preference margin (large `abs(dynamic_margin)`).\n\nAs a new coupling idea, we directly scale the `preference_error` by a factor derived from the raw, un-normalized rank gap (`rank_gap_scale = softplus(abs(cost_gap_signed))`). The intuition is to make the loss penalty proportional to the magnitude of the cost difference, ensuring that pairs with larger cost discrepancies contribute more significantly to the final loss. This `rank_gap_scale` modulates the error *before* the final logarithmic stabilization, coupling the raw cost signal with the preference error term more tightly than in the parents.\n\nWhen `cost(a) < cost(b)`, the `dynamic_margin` becomes negative. The loss encourages `logp_diff` to be positive and larger than this margin to drive the `preference_error` to zero, thus correctly preferring `a`.", "hyperparams": {"beta": 1.0, "gamma": 1.0}, "operators_used": ["rank_gap", "zscore", "tanh", "softplus", "sigmoid", "log"]}}, "better_than_baseline": true}
{"generation": 7, "index": 9, "ir": {"name": "Rank-Modulated Adaptive Sigmoid-Hinge Loss", "intuition": "This loss function creates a hybrid that modulates a core hinge-like error term with an adaptive sigmoid weight, introducing a new coupling mechanism where the rank gap directly influences the hinge margin.\n\nFrom 'Adaptive Hinge-Sigmoid Hybrid Loss' (Parent 0), we inherit the use of a smooth hinge loss structure, `softplus(margin - logp_diff)`, which ensures the loss is zero when the model's preference `logp_diff` correctly exceeds a target margin. We also adopt its stable final form `log(1 + error_term)`.\n\nFrom 'Adaptively Weighted Hinge-Sigmoid Loss with Rank-Gap Scaling' (Parent 1), we inherit the concept of an adaptive weight, `adaptive_alpha = sigmoid(gamma * abs(margin))`, that modulates the loss based on the confidence of the preference target. This makes the loss more sensitive to clear-cut cases.\n\nWe introduce two new coupling ideas. First, instead of using a standardized rank gap for the margin, we create a 'rank-modulated margin' where the raw, unnormalized rank gap `cost_gap_signed` directly scales the base margin `beta`. This makes the required preference `logp_diff` proportionally larger for pairs with greater cost separation, adding a layer of direct cost-sensitivity. Second, we use the adaptive weight `adaptive_alpha` to directly scale the hinge error *inside* the final logarithm, creating a unified structure `log(1.0 + adaptive_alpha * preference_error)`. This elegantly combines the adaptive weighting from Parent 1 with the stable hinge error from Parent 0.\n\nWhen `cost(a) < cost(b)`, `cost_gap_signed` is negative, making the `rank_modulated_margin` negative. The loss encourages `logp_diff` to be positive and larger than this margin to drive `preference_error` to zero, correctly preferring `a`.", "pseudocode": "1. Compute the log-probability difference: logp_diff = logp_a - logp_b.\n2. Compute the signed, batch-relative rank gap: cost_gap_signed = rank_gap(cost_a, cost_b).\n3. Create a rank-modulated margin where the rank gap directly scales the base margin (new coupling idea): rank_modulated_margin = beta * cost_gap_signed.\n4. Compute the preference error using a smooth hinge structure (inherited from Parent 0): preference_error = softplus(rank_modulated_margin - logp_diff).\n5. Create an adaptive weight based on the margin's magnitude (inherited from Parent 1): adaptive_alpha = sigmoid(gamma * abs(rank_modulated_margin)).\n6. Combine the preference error and adaptive weight in a stable logarithmic form (new coupling idea, structure from Parent 0): loss = log(1.0 + adaptive_alpha * preference_error).", "hyperparams": {"beta": 0.1, "gamma": 1.0}, "operators_used": ["rank_gap", "softplus", "sigmoid", "log"], "implementation_hint": {"expects": ["cost_a", "cost_b", "logp_a", "logp_b"], "returns": "scalar"}}, "fitness": {"hf_like_score": 8.466797828674316, "validation_objective": 8.466797828674316, "generalization_penalty": 0.0, "generalization_objectives": {"20": 3.987104654312134}, "train_score_mean": 10.501026849746705, "train_loss_mean": 0.5983182621002198, "pair_count": 12902387, "config": {"hf": {"problem": "tsp", "hf_steps": 100, "train_problem_size": 100, "valid_problem_sizes": [20], "train_batch_size": 64, "pomo_size": 64, "learning_rate": 0.0003, "weight_decay": 1e-06, "alpha": 0.05, "device": "cuda", "seed": 1234, "num_validation_episodes": 128, "validation_batch_size": 64, "generalization_penalty_weight": 1.0, "pool_version": "v0"}, "free_loss": {"f1_steps": 100, "f2_steps": 100, "f3_enabled": false}}, "loss_ir": {"name": "Rank-Modulated Adaptive Sigmoid-Hinge Loss", "intuition": "This loss function creates a hybrid that modulates a core hinge-like error term with an adaptive sigmoid weight, introducing a new coupling mechanism where the rank gap directly influences the hinge margin.\n\nFrom 'Adaptive Hinge-Sigmoid Hybrid Loss' (Parent 0), we inherit the use of a smooth hinge loss structure, `softplus(margin - logp_diff)`, which ensures the loss is zero when the model's preference `logp_diff` correctly exceeds a target margin. We also adopt its stable final form `log(1 + error_term)`.\n\nFrom 'Adaptively Weighted Hinge-Sigmoid Loss with Rank-Gap Scaling' (Parent 1), we inherit the concept of an adaptive weight, `adaptive_alpha = sigmoid(gamma * abs(margin))`, that modulates the loss based on the confidence of the preference target. This makes the loss more sensitive to clear-cut cases.\n\nWe introduce two new coupling ideas. First, instead of using a standardized rank gap for the margin, we create a 'rank-modulated margin' where the raw, unnormalized rank gap `cost_gap_signed` directly scales the base margin `beta`. This makes the required preference `logp_diff` proportionally larger for pairs with greater cost separation, adding a layer of direct cost-sensitivity. Second, we use the adaptive weight `adaptive_alpha` to directly scale the hinge error *inside* the final logarithm, creating a unified structure `log(1.0 + adaptive_alpha * preference_error)`. This elegantly combines the adaptive weighting from Parent 1 with the stable hinge error from Parent 0.\n\nWhen `cost(a) < cost(b)`, `cost_gap_signed` is negative, making the `rank_modulated_margin` negative. The loss encourages `logp_diff` to be positive and larger than this margin to drive `preference_error` to zero, correctly preferring `a`.", "hyperparams": {"beta": 0.1, "gamma": 1.0}, "operators_used": ["rank_gap", "softplus", "sigmoid", "log"]}}, "better_than_baseline": false}
{"generation": 8, "index": 0, "ir": {"name": "Rank-Modulated Adaptive Sigmoid Loss", "intuition": "This loss function creates a hybrid sigmoid-hinge structure by modulating the target preference margin with the raw rank gap, offering a balance between batch-relative ranking and absolute cost magnitude.\n\nFrom Parent 0 (Adaptive Hinge-Sigmoid Hybrid Loss), we inherit the core idea of using a dynamic, bounded margin based on a standardized rank gap (`dynamic_margin = beta * tanh(zscore(rank_gap(...)))`). This ensures the loss is sensitive to the relative ranking of a pair within a batch.\n\nFrom Parent 1 (Rank-Calibrated Dynamic Hinge Loss), we inherit the idea of directly using the raw rank gap magnitude to scale a component of the loss. However, instead of scaling the final error, we use it to modulate the margin itself.\n\nAs a new coupling idea, we introduce a `rank_modulated_margin`. This margin is the sum of the batch-relative `dynamic_margin` and a term derived from the raw, un-normalized rank gap (`cost_gap_signed`). This couples the batch-relative signal (from the z-scored gap) with the absolute signal (from the raw gap) directly within the margin definition. This new margin is then used inside a `logsigmoid` function, `logsigmoid(logp_diff - rank_modulated_margin)`. This is a departure from the parents' `softplus`-based hinge structures, opting for a probabilistic interpretation where the model's preference `logp_diff` is expected to exceed the `rank_modulated_margin`.\n\nA second new idea is the introduction of a `stability_clamp` on the raw rank gap before it's used in the margin. This prevents extremely large cost differences from dominating the margin and causing numerical instability, ensuring the loss remains well-behaved.\n\nWhen `cost(a) < cost(b)`, `cost_gap_signed` is negative, making the `rank_modulated_margin` negative. The loss then encourages `logp_diff` to be positive to maximize `logsigmoid(logp_diff - rank_modulated_margin)`, correctly preferring `a`.", "pseudocode": "1. Compute the log-probability difference: logp_diff = logp_a - logp_b.\n2. Compute the signed, batch-relative rank gap: cost_gap_signed = rank_gap(cost_a, cost_b).\n3. Standardize the rank gap across the batch: cost_gap_zscored = zscore(cost_gap_signed).\n4. Create a bounded, batch-relative margin component (inherited from Parent 0): dynamic_margin = beta * tanh(cost_gap_zscored).\n5. Introduce a stability clamp on the raw rank gap (new stability idea): clamped_cost_gap = clamp(cost_gap_signed, min=-10.0, max=10.0).\n6. Create a rank-modulated margin by coupling the relative and absolute rank signals (new coupling idea): rank_modulated_margin = dynamic_margin + gamma * clamped_cost_gap.\n7. Compute the final loss using a stable sigmoid formulation, where the model's preference is compared against the new modulated margin: loss = -logsigmoid(logp_diff - rank_modulated_margin).", "hyperparams": {"beta": 0.5, "gamma": 0.1}, "operators_used": ["rank_gap", "zscore", "tanh", "clamp", "logsigmoid"], "implementation_hint": {"expects": ["cost_a", "cost_b", "logp_a", "logp_b"], "returns": "scalar"}}, "fitness": {"hf_like_score": 8.469352722167969, "validation_objective": 8.469352722167969, "generalization_penalty": 0.0, "generalization_objectives": {"20": 3.9830487966537476}, "train_score_mean": 10.473353881835937, "train_loss_mean": 0.6011806958913803, "pair_count": 12902388, "config": {"hf": {"problem": "tsp", "hf_steps": 100, "train_problem_size": 100, "valid_problem_sizes": [20], "train_batch_size": 64, "pomo_size": 64, "learning_rate": 0.0003, "weight_decay": 1e-06, "alpha": 0.05, "device": "cuda", "seed": 1234, "num_validation_episodes": 128, "validation_batch_size": 64, "generalization_penalty_weight": 1.0, "pool_version": "v0"}, "free_loss": {"f1_steps": 100, "f2_steps": 100, "f3_enabled": false}}, "loss_ir": {"name": "Rank-Modulated Adaptive Sigmoid Loss", "intuition": "This loss function creates a hybrid sigmoid-hinge structure by modulating the target preference margin with the raw rank gap, offering a balance between batch-relative ranking and absolute cost magnitude.\n\nFrom Parent 0 (Adaptive Hinge-Sigmoid Hybrid Loss), we inherit the core idea of using a dynamic, bounded margin based on a standardized rank gap (`dynamic_margin = beta * tanh(zscore(rank_gap(...)))`). This ensures the loss is sensitive to the relative ranking of a pair within a batch.\n\nFrom Parent 1 (Rank-Calibrated Dynamic Hinge Loss), we inherit the idea of directly using the raw rank gap magnitude to scale a component of the loss. However, instead of scaling the final error, we use it to modulate the margin itself.\n\nAs a new coupling idea, we introduce a `rank_modulated_margin`. This margin is the sum of the batch-relative `dynamic_margin` and a term derived from the raw, un-normalized rank gap (`cost_gap_signed`). This couples the batch-relative signal (from the z-scored gap) with the absolute signal (from the raw gap) directly within the margin definition. This new margin is then used inside a `logsigmoid` function, `logsigmoid(logp_diff - rank_modulated_margin)`. This is a departure from the parents' `softplus`-based hinge structures, opting for a probabilistic interpretation where the model's preference `logp_diff` is expected to exceed the `rank_modulated_margin`.\n\nA second new idea is the introduction of a `stability_clamp` on the raw rank gap before it's used in the margin. This prevents extremely large cost differences from dominating the margin and causing numerical instability, ensuring the loss remains well-behaved.\n\nWhen `cost(a) < cost(b)`, `cost_gap_signed` is negative, making the `rank_modulated_margin` negative. The loss then encourages `logp_diff` to be positive to maximize `logsigmoid(logp_diff - rank_modulated_margin)`, correctly preferring `a`.", "hyperparams": {"beta": 0.5, "gamma": 0.1}, "operators_used": ["rank_gap", "zscore", "tanh", "clamp", "logsigmoid"]}}, "better_than_baseline": false}
{"generation": 8, "index": 1, "ir": {"name": "Rank-Modulated Adaptive Sigmoid-Hinge Loss", "intuition": "This loss function creates a hybrid preference loss that combines a dynamic margin with a probabilistic sigmoid structure, and introduces a new coupling mechanism where the preference signal is directly modulated by the raw cost difference.\n\nFrom Parent 0 (Adaptive Hinge-Sigmoid Hybrid Loss), we inherit the core idea of an `adaptive_gamma` that scales the loss based on the magnitude of the preference margin. This makes the loss more responsive to pairs with a clear, standardized rank difference.\n\nFrom Parent 1 (Rank-Calibrated Dynamic Hinge Loss), we inherit the use of a `dynamic_margin` derived from the standardized rank gap (`beta * tanh(zscore(rank_gap(...)))`). This provides a target for the model's preference that is sensitive to the relative ranking of a pair within its batch. We also inherit its use of `softplus` to create a smooth, non-negative hinge-like term.\n\nAs a new coupling idea, we introduce a `rank_modulated_logp_diff`. Instead of comparing `logp_diff` directly to the margin, we first scale it by a factor derived from the raw, un-normalized cost difference: `softplus(abs(cost_gap_signed))`. This modulation amplifies the model's preference signal for pairs with a large cost difference, effectively telling the model to be more confident when the ground truth difference is large. This `rank_modulated_logp_diff` is then used within the hinge-like term `softplus(dynamic_margin - rank_modulated_logp_diff)`. This structure couples the raw cost signal directly with the model's preference signal before the margin comparison, creating a more fine-grained target.\n\nWhen `cost(a) < cost(b)`, `cost_gap_signed` is negative, making `dynamic_margin` negative. The loss encourages `logp_a > logp_b`. The `rank_modulated_logp_diff` will be a positively scaled version of `logp_diff`, pushing the model to produce an even larger `logp_diff` to overcome the margin and drive the loss to zero.", "pseudocode": "1. Compute the log-probability difference: logp_diff = logp_a - logp_b.\n2. Compute the signed, batch-relative rank gap: cost_gap_signed = rank_gap(cost_a, cost_b).\n3. Standardize the rank gap across the batch: cost_gap_zscored = zscore(cost_gap_signed).\n4. Create a bounded dynamic margin from the standardized rank gap (inherited from Parent 1): dynamic_margin = beta * tanh(cost_gap_zscored).\n5. Create a scaling factor from the raw rank gap (new coupling idea): rank_scale = softplus(abs(cost_gap_signed)).\n6. Modulate the logp_diff with the rank scale (new coupling idea): rank_modulated_logp_diff = rank_scale * logp_diff.\n7. Compute the preference error using a smooth hinge-like term with the modulated preference signal: preference_error = softplus(dynamic_margin - rank_modulated_logp_diff).\n8. Create an adaptive temperature based on the margin's magnitude (inherited from Parent 0): adaptive_gamma = 1.0 + gamma * abs(dynamic_margin).\n9. Combine the preference error and adaptive temperature in a stable logarithmic form (structure from parents): loss = log(1.0 + adaptive_gamma * preference_error).", "hyperparams": {"beta": 1.0, "gamma": 0.5}, "operators_used": ["rank_gap", "zscore", "tanh", "softplus", "log"], "implementation_hint": {"expects": ["cost_a", "cost_b", "logp_a", "logp_b"], "returns": "scalar"}}, "fitness": {"hf_like_score": 8.448985576629639, "validation_objective": 8.448985576629639, "generalization_penalty": 0.0, "generalization_objectives": {"20": 3.979970693588257}, "train_score_mean": 10.487573757171631, "train_loss_mean": 0.6010009121894836, "pair_count": 12902388, "config": {"hf": {"problem": "tsp", "hf_steps": 100, "train_problem_size": 100, "valid_problem_sizes": [20], "train_batch_size": 64, "pomo_size": 64, "learning_rate": 0.0003, "weight_decay": 1e-06, "alpha": 0.05, "device": "cuda", "seed": 1234, "num_validation_episodes": 128, "validation_batch_size": 64, "generalization_penalty_weight": 1.0, "pool_version": "v0"}, "free_loss": {"f1_steps": 100, "f2_steps": 100, "f3_enabled": false}}, "loss_ir": {"name": "Rank-Modulated Adaptive Sigmoid-Hinge Loss", "intuition": "This loss function creates a hybrid preference loss that combines a dynamic margin with a probabilistic sigmoid structure, and introduces a new coupling mechanism where the preference signal is directly modulated by the raw cost difference.\n\nFrom Parent 0 (Adaptive Hinge-Sigmoid Hybrid Loss), we inherit the core idea of an `adaptive_gamma` that scales the loss based on the magnitude of the preference margin. This makes the loss more responsive to pairs with a clear, standardized rank difference.\n\nFrom Parent 1 (Rank-Calibrated Dynamic Hinge Loss), we inherit the use of a `dynamic_margin` derived from the standardized rank gap (`beta * tanh(zscore(rank_gap(...)))`). This provides a target for the model's preference that is sensitive to the relative ranking of a pair within its batch. We also inherit its use of `softplus` to create a smooth, non-negative hinge-like term.\n\nAs a new coupling idea, we introduce a `rank_modulated_logp_diff`. Instead of comparing `logp_diff` directly to the margin, we first scale it by a factor derived from the raw, un-normalized cost difference: `softplus(abs(cost_gap_signed))`. This modulation amplifies the model's preference signal for pairs with a large cost difference, effectively telling the model to be more confident when the ground truth difference is large. This `rank_modulated_logp_diff` is then used within the hinge-like term `softplus(dynamic_margin - rank_modulated_logp_diff)`. This structure couples the raw cost signal directly with the model's preference signal before the margin comparison, creating a more fine-grained target.\n\nWhen `cost(a) < cost(b)`, `cost_gap_signed` is negative, making `dynamic_margin` negative. The loss encourages `logp_a > logp_b`. The `rank_modulated_logp_diff` will be a positively scaled version of `logp_diff`, pushing the model to produce an even larger `logp_diff` to overcome the margin and drive the loss to zero.", "hyperparams": {"beta": 1.0, "gamma": 0.5}, "operators_used": ["rank_gap", "zscore", "tanh", "softplus", "log"]}}, "better_than_baseline": false}
{"generation": 8, "index": 2, "ir": {"name": "Adaptive Dual-Margin Hinge Loss", "intuition": "This loss function constructs a robust preference signal by combining a dynamic, rank-based margin with a secondary, cost-magnitude-based margin, and couples this with an adaptive temperature.\n\nFrom Parent 0 (Adaptive Hinge-Sigmoid Hybrid Loss), we inherit the core mechanism of a `dynamic_margin` derived from the standardized rank gap (`beta * tanh(zscore(rank_gap(...)))`). This provides a margin sensitive to the relative ranking of a pair within the batch. We also inherit the use of `softplus` to create a smooth, non-negative hinge loss.\n\nFrom Parent 1 (Rank-Calibrated Dynamic Hinge Loss), we inherit the idea of using the raw cost difference to modulate the loss. However, instead of using it as a multiplicative scale on the error, we use it to create a secondary, 'magnitude-aware' margin. This is a new coupling idea.\n\nSpecifically, we introduce a `magnitude_margin` using `alpha * tanh(cost_a - cost_b)`. This margin is directly proportional to the magnitude of the cost difference, providing a stronger preference signal for pairs with a large cost gap. The two margins are then combined into a `fused_margin = dynamic_margin + magnitude_margin`. This `fused_margin` now reflects both the relative rank of the pair in the batch and the absolute difference in their costs.\n\nAs a second new idea, we introduce an adaptive temperature that responds to the `fused_margin`'s magnitude. This temperature, `adaptive_temp = 1.0 + gamma * sigmoid(abs(fused_margin))`, increases the loss's sensitivity for pairs where the combined margin indicates a clear preference, making the optimization more focused. The final loss is `log(1.0 + adaptive_temp * preference_error)`, where `preference_error = softplus(fused_margin - logp_diff)`.\n\nWhen `cost(a) < cost(b)`, both `dynamic_margin` and `magnitude_margin` are negative, making `fused_margin` strongly negative. The loss encourages `logp_diff` to be positive and larger than this negative margin to drive the `preference_error` to zero, thus correctly preferring `a`.", "pseudocode": "1. Compute the log-probability difference: logp_diff = logp_a - logp_b.\n2. Compute the signed, batch-relative rank gap: cost_gap_signed = rank_gap(cost_a, cost_b).\n3. Standardize the rank gap across the batch: cost_gap_zscored = zscore(cost_gap_signed).\n4. Create a rank-based dynamic margin (inherited from Parent 0): dynamic_margin = beta * tanh(cost_gap_zscored).\n5. Create a magnitude-aware margin from the raw cost difference (new coupling idea, inspired by Parent 1's use of raw cost): magnitude_margin = alpha * tanh(cost_a - cost_b).\n6. Combine the two margins into a single fused margin (new coupling idea): fused_margin = dynamic_margin + magnitude_margin.\n7. Compute an adaptive temperature based on the fused margin's magnitude (new coupling idea): adaptive_temp = 1.0 + gamma * sigmoid(abs(fused_margin)).\n8. Compute the preference error using a smooth hinge with the fused margin (structure from Parent 0): preference_error = softplus(fused_margin - logp_diff).\n9. Combine the components into a stable final loss (structure from parents): loss = log(1.0 + adaptive_temp * preference_error).", "hyperparams": {"beta": 1.0, "alpha": 0.5, "gamma": 1.0}, "operators_used": ["rank_gap", "zscore", "tanh", "sigmoid", "softplus", "log"], "implementation_hint": {"expects": ["cost_a", "cost_b", "logp_a", "logp_b"], "returns": "scalar"}}, "fitness": {"hf_like_score": 17.056471824645996, "validation_objective": 17.056471824645996, "generalization_penalty": 0.0, "generalization_objectives": {"20": 4.412067174911499}, "train_score_mean": 24.669329051971435, "train_loss_mean": 0.919778608083725, "pair_count": 12902391, "config": {"hf": {"problem": "tsp", "hf_steps": 100, "train_problem_size": 100, "valid_problem_sizes": [20], "train_batch_size": 64, "pomo_size": 64, "learning_rate": 0.0003, "weight_decay": 1e-06, "alpha": 0.05, "device": "cuda", "seed": 1234, "num_validation_episodes": 128, "validation_batch_size": 64, "generalization_penalty_weight": 1.0, "pool_version": "v0"}, "free_loss": {"f1_steps": 100, "f2_steps": 100, "f3_enabled": false}}, "loss_ir": {"name": "Adaptive Dual-Margin Hinge Loss", "intuition": "This loss function constructs a robust preference signal by combining a dynamic, rank-based margin with a secondary, cost-magnitude-based margin, and couples this with an adaptive temperature.\n\nFrom Parent 0 (Adaptive Hinge-Sigmoid Hybrid Loss), we inherit the core mechanism of a `dynamic_margin` derived from the standardized rank gap (`beta * tanh(zscore(rank_gap(...)))`). This provides a margin sensitive to the relative ranking of a pair within the batch. We also inherit the use of `softplus` to create a smooth, non-negative hinge loss.\n\nFrom Parent 1 (Rank-Calibrated Dynamic Hinge Loss), we inherit the idea of using the raw cost difference to modulate the loss. However, instead of using it as a multiplicative scale on the error, we use it to create a secondary, 'magnitude-aware' margin. This is a new coupling idea.\n\nSpecifically, we introduce a `magnitude_margin` using `alpha * tanh(cost_a - cost_b)`. This margin is directly proportional to the magnitude of the cost difference, providing a stronger preference signal for pairs with a large cost gap. The two margins are then combined into a `fused_margin = dynamic_margin + magnitude_margin`. This `fused_margin` now reflects both the relative rank of the pair in the batch and the absolute difference in their costs.\n\nAs a second new idea, we introduce an adaptive temperature that responds to the `fused_margin`'s magnitude. This temperature, `adaptive_temp = 1.0 + gamma * sigmoid(abs(fused_margin))`, increases the loss's sensitivity for pairs where the combined margin indicates a clear preference, making the optimization more focused. The final loss is `log(1.0 + adaptive_temp * preference_error)`, where `preference_error = softplus(fused_margin - logp_diff)`.\n\nWhen `cost(a) < cost(b)`, both `dynamic_margin` and `magnitude_margin` are negative, making `fused_margin` strongly negative. The loss encourages `logp_diff` to be positive and larger than this negative margin to drive the `preference_error` to zero, thus correctly preferring `a`.", "hyperparams": {"beta": 1.0, "alpha": 0.5, "gamma": 1.0}, "operators_used": ["rank_gap", "zscore", "tanh", "sigmoid", "softplus", "log"]}}, "better_than_baseline": false}
{"generation": 8, "index": 3, "ir": {"name": "Adaptive Margin & Temperature Hinge with Rank-Modulated Error", "intuition": "This loss function creates a sophisticated hinge-based preference loss by combining a dynamic margin, an adaptive temperature, and a novel error modulation scheme.\n\nFrom Parent 0 (Adaptive Hinge-Sigmoid Hybrid Loss), we inherit the core idea of a `dynamic_margin` derived from a standardized rank gap (`beta * tanh(zscore(rank_gap(...)))`). This provides a margin that is sensitive to the relative ranking of a pair within the batch. We also inherit its use of `softplus` to create a smooth, non-negative hinge-like `preference_error` term.\n\nFrom Parent 1 (Rank-Calibrated Dynamic Hinge Loss), we inherit the concept of directly scaling the preference error. However, instead of using the raw rank gap, we use a bounded, sigmoid-transformed version of the z-scored rank gap. This provides a more stable and controlled modulation.\n\nAs a new coupling idea, we introduce two distinct mechanisms. First, we create an `error_modulator` from the standardized rank gap (`sigmoid(abs(cost_gap_zscored))`). This modulator scales the `preference_error`, making the loss penalty more sensitive to pairs with a clearer relative ranking within the batch, while keeping the scaling factor bounded between 0 and 1. Second, we introduce a new adaptive temperature (`adaptive_temp`) that is also based on the z-scored rank gap, but uses a `softplus` activation. This temperature `1.0 + gamma * softplus(abs(cost_gap_zscored))` amplifies the final loss for pairs with a large rank separation, creating a synergistic effect with the error modulator.\n\nWhen `cost(a) < cost(b)`, `cost_gap_signed` is negative, making `dynamic_margin` negative. The loss encourages `logp_diff` to be positive and larger than this margin to drive `preference_error` to zero, thus correctly preferring `a`.", "pseudocode": "1. Compute the log-probability difference: logp_diff = logp_a - logp_b.\n2. Compute the signed, batch-relative rank gap: cost_gap_signed = rank_gap(cost_a, cost_b).\n3. Standardize the rank gap across the batch: cost_gap_zscored = zscore(cost_gap_signed).\n4. Create a bounded dynamic margin from the standardized rank gap (inherited from Parent 0): dynamic_margin = beta * tanh(cost_gap_zscored).\n5. Compute the smooth hinge-like preference error (structure from Parent 0): preference_error = softplus(dynamic_margin - logp_diff).\n6. Create a bounded error modulator from the standardized rank gap (new coupling idea 1): error_modulator = sigmoid(abs(cost_gap_zscored)).\n7. Modulate the preference error (inspired by Parent 1, but with new modulator): modulated_error = error_modulator * preference_error.\n8. Create an unbounded adaptive temperature from the standardized rank gap (new coupling idea 2): adaptive_temp = 1.0 + gamma * softplus(abs(cost_gap_zscored)).\n9. Combine the modulated error and adaptive temperature in a stable logarithmic form: loss = log(1.0 + adaptive_temp * modulated_error).", "hyperparams": {"beta": 1.0, "gamma": 0.5}, "operators_used": ["rank_gap", "zscore", "tanh", "softplus", "sigmoid", "log"], "implementation_hint": {"expects": ["cost_a", "cost_b", "logp_a", "logp_b"], "returns": "scalar"}}, "fitness": {"hf_like_score": 8.519197463989258, "validation_objective": 8.519197463989258, "generalization_penalty": 0.0, "generalization_objectives": {"20": 3.994112491607666}, "train_score_mean": 10.504894046783447, "train_loss_mean": 0.5997029578685761, "pair_count": 12902391, "config": {"hf": {"problem": "tsp", "hf_steps": 100, "train_problem_size": 100, "valid_problem_sizes": [20], "train_batch_size": 64, "pomo_size": 64, "learning_rate": 0.0003, "weight_decay": 1e-06, "alpha": 0.05, "device": "cuda", "seed": 1234, "num_validation_episodes": 128, "validation_batch_size": 64, "generalization_penalty_weight": 1.0, "pool_version": "v0"}, "free_loss": {"f1_steps": 100, "f2_steps": 100, "f3_enabled": false}}, "loss_ir": {"name": "Adaptive Margin & Temperature Hinge with Rank-Modulated Error", "intuition": "This loss function creates a sophisticated hinge-based preference loss by combining a dynamic margin, an adaptive temperature, and a novel error modulation scheme.\n\nFrom Parent 0 (Adaptive Hinge-Sigmoid Hybrid Loss), we inherit the core idea of a `dynamic_margin` derived from a standardized rank gap (`beta * tanh(zscore(rank_gap(...)))`). This provides a margin that is sensitive to the relative ranking of a pair within the batch. We also inherit its use of `softplus` to create a smooth, non-negative hinge-like `preference_error` term.\n\nFrom Parent 1 (Rank-Calibrated Dynamic Hinge Loss), we inherit the concept of directly scaling the preference error. However, instead of using the raw rank gap, we use a bounded, sigmoid-transformed version of the z-scored rank gap. This provides a more stable and controlled modulation.\n\nAs a new coupling idea, we introduce two distinct mechanisms. First, we create an `error_modulator` from the standardized rank gap (`sigmoid(abs(cost_gap_zscored))`). This modulator scales the `preference_error`, making the loss penalty more sensitive to pairs with a clearer relative ranking within the batch, while keeping the scaling factor bounded between 0 and 1. Second, we introduce a new adaptive temperature (`adaptive_temp`) that is also based on the z-scored rank gap, but uses a `softplus` activation. This temperature `1.0 + gamma * softplus(abs(cost_gap_zscored))` amplifies the final loss for pairs with a large rank separation, creating a synergistic effect with the error modulator.\n\nWhen `cost(a) < cost(b)`, `cost_gap_signed` is negative, making `dynamic_margin` negative. The loss encourages `logp_diff` to be positive and larger than this margin to drive `preference_error` to zero, thus correctly preferring `a`.", "hyperparams": {"beta": 1.0, "gamma": 0.5}, "operators_used": ["rank_gap", "zscore", "tanh", "softplus", "sigmoid", "log"]}}, "better_than_baseline": false}
{"generation": 8, "index": 4, "ir": {"name": "Rank-Modulated Hinge-Sigmoid Loss", "intuition": "This loss function creates a hybrid preference model by combining a dynamic margin with an adaptive temperature, and introduces a new coupling mechanism that directly modulates the preference error by the raw rank gap.\n\nFrom Parent 0 (Adaptive Hinge-Sigmoid Hybrid Loss), we inherit the core concept of a `dynamic_margin` derived from a standardized rank gap (`beta * tanh(zscore(rank_gap(...)))`). This creates a margin that is sensitive to the relative ranking of a pair within the batch. We also inherit its use of `softplus` to form a non-negative, smooth hinge loss term (`preference_error`).\n\nFrom Parent 1 (Rank-Calibrated Dynamic Hinge Loss), we inherit the idea of an adaptive scaling factor, which we use as an `adaptive_temperature` inside the loss calculation. This temperature, `1.0 + gamma * sigmoid(abs(dynamic_margin))`, makes the loss more sensitive to pairs with a clear preference margin (large `abs(dynamic_margin)`).\n\nAs a new coupling idea, we directly scale the `preference_error` by a factor derived from the raw, un-normalized rank gap (`rank_gap_scale = softplus(abs(cost_gap_signed))`). The intuition is to make the loss penalty proportional to the magnitude of the cost difference, ensuring that pairs with larger cost discrepancies contribute more significantly to the final loss. This `rank_gap_scale` modulates the error *before* the final logarithmic stabilization, coupling the raw cost signal with the preference error term more tightly than in the parents.\n\nWhen `cost(a) < cost(b)`, the `dynamic_margin` becomes negative. The loss encourages `logp_diff` to be positive and larger than this margin to drive the `preference_error` to zero, thus correctly preferring `a`.", "pseudocode": "1. Compute the log-probability difference: logp_diff = logp_a - logp_b.\n2. Compute the signed, batch-relative rank gap: cost_gap_signed = rank_gap(cost_a, cost_b).\n3. Standardize the rank gap across the batch: cost_gap_zscored = zscore(cost_gap_signed).\n4. Create a bounded dynamic margin from the standardized rank gap (inherited from Parent 0): dynamic_margin = beta * tanh(cost_gap_zscored).\n5. Compute a smooth hinge-like preference error (structure from Parent 0): preference_error = softplus(dynamic_margin - logp_diff).\n6. Create a scaling factor from the raw rank gap (new coupling idea): rank_gap_scale = softplus(abs(cost_gap_signed)).\n7. Modulate the preference error with the rank gap scale (new coupling idea): scaled_error = rank_gap_scale * preference_error.\n8. Create an adaptive temperature based on the margin's magnitude (inspired by Parent 1): adaptive_temperature = 1.0 + gamma * sigmoid(abs(dynamic_margin)).\n9. Combine the scaled error and adaptive temperature in a stable logarithmic form (structure from parents): loss = log(1.0 + adaptive_temperature * scaled_error).", "hyperparams": {"beta": 1.0, "gamma": 1.0}, "operators_used": ["rank_gap", "zscore", "tanh", "softplus", "sigmoid", "log"], "implementation_hint": {"expects": ["cost_a", "cost_b", "logp_a", "logp_b"], "returns": "scalar"}}, "fitness": {"hf_like_score": 8.490773677825928, "validation_objective": 8.490773677825928, "generalization_penalty": 0.0, "generalization_objectives": {"20": 3.988558769226074}, "train_score_mean": 10.525263872146606, "train_loss_mean": 0.6020399338006973, "pair_count": 12902384, "config": {"hf": {"problem": "tsp", "hf_steps": 100, "train_problem_size": 100, "valid_problem_sizes": [20], "train_batch_size": 64, "pomo_size": 64, "learning_rate": 0.0003, "weight_decay": 1e-06, "alpha": 0.05, "device": "cuda", "seed": 1234, "num_validation_episodes": 128, "validation_batch_size": 64, "generalization_penalty_weight": 1.0, "pool_version": "v0"}, "free_loss": {"f1_steps": 100, "f2_steps": 100, "f3_enabled": false}}, "loss_ir": {"name": "Rank-Modulated Hinge-Sigmoid Loss", "intuition": "This loss function creates a hybrid preference model by combining a dynamic margin with an adaptive temperature, and introduces a new coupling mechanism that directly modulates the preference error by the raw rank gap.\n\nFrom Parent 0 (Adaptive Hinge-Sigmoid Hybrid Loss), we inherit the core concept of a `dynamic_margin` derived from a standardized rank gap (`beta * tanh(zscore(rank_gap(...)))`). This creates a margin that is sensitive to the relative ranking of a pair within the batch. We also inherit its use of `softplus` to form a non-negative, smooth hinge loss term (`preference_error`).\n\nFrom Parent 1 (Rank-Calibrated Dynamic Hinge Loss), we inherit the idea of an adaptive scaling factor, which we use as an `adaptive_temperature` inside the loss calculation. This temperature, `1.0 + gamma * sigmoid(abs(dynamic_margin))`, makes the loss more sensitive to pairs with a clear preference margin (large `abs(dynamic_margin)`).\n\nAs a new coupling idea, we directly scale the `preference_error` by a factor derived from the raw, un-normalized rank gap (`rank_gap_scale = softplus(abs(cost_gap_signed))`). The intuition is to make the loss penalty proportional to the magnitude of the cost difference, ensuring that pairs with larger cost discrepancies contribute more significantly to the final loss. This `rank_gap_scale` modulates the error *before* the final logarithmic stabilization, coupling the raw cost signal with the preference error term more tightly than in the parents.\n\nWhen `cost(a) < cost(b)`, the `dynamic_margin` becomes negative. The loss encourages `logp_diff` to be positive and larger than this margin to drive the `preference_error` to zero, thus correctly preferring `a`.", "hyperparams": {"beta": 1.0, "gamma": 1.0}, "operators_used": ["rank_gap", "zscore", "tanh", "softplus", "sigmoid", "log"]}}, "better_than_baseline": false}
{"generation": 8, "index": 5, "ir": {"name": "Rank-Modulated Adaptive Sigmoid Loss", "intuition": "This loss function creates a hybrid probabilistic loss that adaptively scales its sensitivity based on both the rank and raw magnitude of cost differences.\n\nFrom Parent 0 (Adaptive Hinge-Sigmoid Hybrid Loss), we inherit the core mechanism of an `adaptive_gamma` that scales the loss based on the magnitude of a dynamic margin. This makes the loss more sensitive when the preference signal derived from rank is strong.\n\nFrom Parent 1 (Rank-Calibrated Dynamic Hinge Loss), we inherit the idea of directly using the raw rank gap to modulate the loss. Instead of scaling the error *outside* a log, we use it to create a `rank_gap_scale` that directly influences the argument of a `logsigmoid` function.\n\nAs a new coupling idea, we combine these two concepts within a single probabilistic framework. The core of the loss is a `logsigmoid` term, but its argument is a product of three key quantities: the log-probability difference (`logp_diff`), the adaptive gamma from Parent 0 (`adaptive_gamma`), and a new `rank_gap_scale` derived from Parent 1's idea. The `rank_gap_scale` is `softplus(rank_gap(cost_a, cost_b))`, ensuring it is positive and reflects the signed magnitude of the rank difference. This three-way coupling means the loss is sensitive not only to the model's preference (`logp_diff`) but is also dynamically amplified by both the standardized rank margin (`adaptive_gamma`) and the raw rank gap (`rank_gap_scale`). A second new idea is a stability term, `alpha * relu(-logp_diff)`, which adds a small penalty if the model strongly prefers the worse candidate, preventing extreme negative log-probabilities and improving stability.\n\nWhen `cost(a) < cost(b)`, `rank_gap` is negative, making `rank_gap_scale` a small positive number. The loss encourages `logp_diff` to be positive to maximize the `logsigmoid` term. When `cost(b) < cost(a)`, `rank_gap` is positive, making `rank_gap_scale` large and positive. The loss then strongly encourages `logp_diff` to be negative.", "pseudocode": "1. Compute the log-probability difference: logp_diff = logp_a - logp_b.\n2. Compute the signed, batch-relative rank gap: cost_gap_signed = rank_gap(cost_a, cost_b).\n3. Standardize the rank gap across the batch: cost_gap_zscored = zscore(cost_gap_signed).\n4. Create a bounded dynamic margin from the standardized rank gap: dynamic_margin = beta * tanh(cost_gap_zscored).\n5. Create an adaptive temperature based on the margin magnitude (inherited from Parent 0): adaptive_gamma = 1.0 + gamma * abs(dynamic_margin).\n6. Create a scaling factor from the raw rank gap (inspired by Parent 1): rank_gap_scale = softplus(cost_gap_signed).\n7. Couple the log-probability difference with both adaptive scaling factors (new coupling idea): scaled_logp_diff = adaptive_gamma * rank_gap_scale * logp_diff.\n8. Compute the primary loss using a logsigmoid function: primary_loss = -logsigmoid(scaled_logp_diff).\n9. Add a stability term to penalize strong incorrect preferences (new stability trick): stability_penalty = alpha * relu(-logp_diff * sign(cost_gap_signed)).\n10. The final loss is the sum of the primary loss and the stability penalty: loss = primary_loss + stability_penalty.", "hyperparams": {"beta": 1.0, "gamma": 0.5, "alpha": 0.01}, "operators_used": ["rank_gap", "zscore", "tanh", "softplus", "logsigmoid", "relu"], "implementation_hint": {"expects": ["cost_a", "cost_b", "logp_a", "logp_b"], "returns": "scalar"}}, "fitness": {"hf_like_score": 8.685826301574707, "validation_objective": 8.685826301574707, "generalization_penalty": 0.0, "generalization_objectives": {"20": 4.0323967933654785}, "train_score_mean": 10.484593439102174, "train_loss_mean": 0.6739026916027069, "pair_count": 12902395, "config": {"hf": {"problem": "tsp", "hf_steps": 100, "train_problem_size": 100, "valid_problem_sizes": [20], "train_batch_size": 64, "pomo_size": 64, "learning_rate": 0.0003, "weight_decay": 1e-06, "alpha": 0.05, "device": "cuda", "seed": 1234, "num_validation_episodes": 128, "validation_batch_size": 64, "generalization_penalty_weight": 1.0, "pool_version": "v0"}, "free_loss": {"f1_steps": 100, "f2_steps": 100, "f3_enabled": false}}, "loss_ir": {"name": "Rank-Modulated Adaptive Sigmoid Loss", "intuition": "This loss function creates a hybrid probabilistic loss that adaptively scales its sensitivity based on both the rank and raw magnitude of cost differences.\n\nFrom Parent 0 (Adaptive Hinge-Sigmoid Hybrid Loss), we inherit the core mechanism of an `adaptive_gamma` that scales the loss based on the magnitude of a dynamic margin. This makes the loss more sensitive when the preference signal derived from rank is strong.\n\nFrom Parent 1 (Rank-Calibrated Dynamic Hinge Loss), we inherit the idea of directly using the raw rank gap to modulate the loss. Instead of scaling the error *outside* a log, we use it to create a `rank_gap_scale` that directly influences the argument of a `logsigmoid` function.\n\nAs a new coupling idea, we combine these two concepts within a single probabilistic framework. The core of the loss is a `logsigmoid` term, but its argument is a product of three key quantities: the log-probability difference (`logp_diff`), the adaptive gamma from Parent 0 (`adaptive_gamma`), and a new `rank_gap_scale` derived from Parent 1's idea. The `rank_gap_scale` is `softplus(rank_gap(cost_a, cost_b))`, ensuring it is positive and reflects the signed magnitude of the rank difference. This three-way coupling means the loss is sensitive not only to the model's preference (`logp_diff`) but is also dynamically amplified by both the standardized rank margin (`adaptive_gamma`) and the raw rank gap (`rank_gap_scale`). A second new idea is a stability term, `alpha * relu(-logp_diff)`, which adds a small penalty if the model strongly prefers the worse candidate, preventing extreme negative log-probabilities and improving stability.\n\nWhen `cost(a) < cost(b)`, `rank_gap` is negative, making `rank_gap_scale` a small positive number. The loss encourages `logp_diff` to be positive to maximize the `logsigmoid` term. When `cost(b) < cost(a)`, `rank_gap` is positive, making `rank_gap_scale` large and positive. The loss then strongly encourages `logp_diff` to be negative.", "hyperparams": {"beta": 1.0, "gamma": 0.5, "alpha": 0.01}, "operators_used": ["rank_gap", "zscore", "tanh", "softplus", "logsigmoid", "relu"]}}, "better_than_baseline": false}
{"generation": 8, "index": 6, "ir": {"name": "Rank-Modulated Adaptive Sigmoid Loss", "intuition": "This loss function combines a dynamic margin with an adaptive temperature inside a probabilistic sigmoid framework, introducing a new coupling mechanism that directly modulates the preference signal with the raw rank gap.\n\nFrom Parent 0 (Adaptive Hinge-Sigmoid Hybrid Loss), we inherit the core mechanism of a `dynamic_margin` derived from a standardized rank gap (`beta * tanh(zscore(rank_gap(...)))`). This sets a target for the log-probability difference that is sensitive to the relative ranking of a pair within the batch.\n\nFrom Parent 1 (Rank-Calibrated Dynamic Hinge Loss), we inherit the idea of using the raw, un-normalized rank gap to scale the loss. However, instead of applying it as an external weight, we use it to directly modulate the preference signal *inside* the sigmoid.\n\nAs a new coupling idea, we introduce a `rank_modulated_logp_diff`. This is computed by `logp_diff * softplus(abs(cost_gap_signed))`. This step couples the model's preference signal (`logp_diff`) with the magnitude of the cost difference. The intuition is to amplify the model's confidence (or lack thereof) for pairs with a large cost discrepancy, making the loss more responsive to significant preference errors. The final loss is a `logsigmoid` of the difference between this modulated preference signal and the dynamic margin, scaled by an adaptive temperature. This structure elegantly combines the rank-based margin (Parent 0), rank-based scaling (Parent 1), and a stable probabilistic loss.\n\nWhen `cost(a) < cost(b)`, `dynamic_margin` is negative. The loss encourages `logp_diff` to be positive, which is then amplified by the rank modulation, making it easier to satisfy the condition `rank_modulated_logp_diff > dynamic_margin` and minimize the loss, thus correctly preferring `a`.", "pseudocode": "1. Compute the log-probability difference: logp_diff = logp_a - logp_b.\n2. Compute the signed, batch-relative rank gap: cost_gap_signed = rank_gap(cost_a, cost_b).\n3. Standardize the rank gap across the batch: cost_gap_zscored = zscore(cost_gap_signed).\n4. Create a bounded dynamic margin from the standardized rank gap (inherited from Parent 0): dynamic_margin = beta * tanh(cost_gap_zscored).\n5. Create a rank-based modulation factor from the raw rank gap (inspired by Parent 1): rank_modulator = softplus(abs(cost_gap_signed)).\n6. Modulate the log-probability difference with the rank factor (new coupling idea): rank_modulated_logp_diff = logp_diff * rank_modulator.\n7. Create an adaptive temperature based on the margin's magnitude (inspired by Parent 0): adaptive_gamma = 1.0 + gamma * abs(dynamic_margin).\n8. Compute the final loss using a scaled sigmoid framework: loss = -logsigmoid(adaptive_gamma * (rank_modulated_logp_diff - dynamic_margin)).", "hyperparams": {"beta": 1.0, "gamma": 0.5}, "operators_used": ["rank_gap", "zscore", "tanh", "softplus", "logsigmoid"], "implementation_hint": {"expects": ["cost_a", "cost_b", "logp_a", "logp_b"], "returns": "scalar"}}, "fitness": {"hf_like_score": 8.442161560058594, "validation_objective": 8.442161560058594, "generalization_penalty": 0.0, "generalization_objectives": {"20": 3.9932663440704346}, "train_score_mean": 10.455535469055176, "train_loss_mean": 0.6033686542510986, "pair_count": 12902388, "config": {"hf": {"problem": "tsp", "hf_steps": 100, "train_problem_size": 100, "valid_problem_sizes": [20], "train_batch_size": 64, "pomo_size": 64, "learning_rate": 0.0003, "weight_decay": 1e-06, "alpha": 0.05, "device": "cuda", "seed": 1234, "num_validation_episodes": 128, "validation_batch_size": 64, "generalization_penalty_weight": 1.0, "pool_version": "v0"}, "free_loss": {"f1_steps": 100, "f2_steps": 100, "f3_enabled": false}}, "loss_ir": {"name": "Rank-Modulated Adaptive Sigmoid Loss", "intuition": "This loss function combines a dynamic margin with an adaptive temperature inside a probabilistic sigmoid framework, introducing a new coupling mechanism that directly modulates the preference signal with the raw rank gap.\n\nFrom Parent 0 (Adaptive Hinge-Sigmoid Hybrid Loss), we inherit the core mechanism of a `dynamic_margin` derived from a standardized rank gap (`beta * tanh(zscore(rank_gap(...)))`). This sets a target for the log-probability difference that is sensitive to the relative ranking of a pair within the batch.\n\nFrom Parent 1 (Rank-Calibrated Dynamic Hinge Loss), we inherit the idea of using the raw, un-normalized rank gap to scale the loss. However, instead of applying it as an external weight, we use it to directly modulate the preference signal *inside* the sigmoid.\n\nAs a new coupling idea, we introduce a `rank_modulated_logp_diff`. This is computed by `logp_diff * softplus(abs(cost_gap_signed))`. This step couples the model's preference signal (`logp_diff`) with the magnitude of the cost difference. The intuition is to amplify the model's confidence (or lack thereof) for pairs with a large cost discrepancy, making the loss more responsive to significant preference errors. The final loss is a `logsigmoid` of the difference between this modulated preference signal and the dynamic margin, scaled by an adaptive temperature. This structure elegantly combines the rank-based margin (Parent 0), rank-based scaling (Parent 1), and a stable probabilistic loss.\n\nWhen `cost(a) < cost(b)`, `dynamic_margin` is negative. The loss encourages `logp_diff` to be positive, which is then amplified by the rank modulation, making it easier to satisfy the condition `rank_modulated_logp_diff > dynamic_margin` and minimize the loss, thus correctly preferring `a`.", "hyperparams": {"beta": 1.0, "gamma": 0.5}, "operators_used": ["rank_gap", "zscore", "tanh", "softplus", "logsigmoid"]}}, "better_than_baseline": true}
{"generation": 8, "index": 7, "ir": {"name": "Adaptive Temperature Hinge Loss with Rank-Gap Modulation", "intuition": "This loss function creates a dynamic hinge loss by combining a margin sensitive to batch-wise rank with an adaptive temperature, and introduces a new coupling mechanism that directly modulates the model's log-probability difference.\n\nFrom Parent 0 (Adaptive Hinge-Sigmoid Hybrid Loss), we inherit the core idea of a `dynamic_margin` derived from a standardized rank gap (`beta * tanh(zscore(rank_gap(...)))`). This creates a target margin that adapts to the relative cost difference of a pair within its batch. We also inherit the use of `softplus` to create a smooth, non-negative hinge loss.\n\nFrom Parent 1 (Rank-Calibrated Dynamic Hinge Loss), we inherit the concept of an `adaptive_temperature` that scales the loss based on the magnitude of the dynamic margin (`1.0 + gamma * sigmoid(abs(dynamic_margin))`). This makes the loss more sensitive to pairs with a clear, well-separated rank.\n\nAs a new coupling idea, we introduce a `rank_gap_modulation` factor that directly scales the model's log-probability difference (`logp_diff`). This modulation, `(1.0 + alpha * softplus(cost_gap_signed))`, is applied *before* the log-probability difference is compared to the margin. The intuition is to amplify the model's perceived preference for the better response (`a`) when the cost difference is large and positive, effectively demanding a stronger signal from the model for easier-to-rank pairs. This couples the raw cost signal directly with the model's output signal within the hinge comparison.\n\nA second new idea is a stability trick where the final hinge error is clamped to a maximum value `max_error` before being scaled by the adaptive temperature. This prevents extremely misaligned predictions on a single pair from dominating the batch gradient.\n\nWhen `cost(a) < cost(b)`, `cost_gap_signed` is positive. The loss encourages the modulated `logp_diff` to be greater than the `dynamic_margin` to achieve zero loss, thus correctly preferring `a`.", "pseudocode": "1. Compute the log-probability difference: logp_diff = logp_a - logp_b.\n2. Compute the signed, batch-relative rank gap: cost_gap_signed = rank_gap(cost_a, cost_b).\n3. Standardize the rank gap across the batch: cost_gap_zscored = zscore(cost_gap_signed).\n4. Create a bounded dynamic margin from the standardized rank gap (inherited from Parent 0): dynamic_margin = beta * tanh(cost_gap_zscored).\n5. Create a modulation factor from the signed rank gap (new coupling idea): rank_gap_modulation = 1.0 + alpha * softplus(cost_gap_signed).\n6. Modulate the log-probability difference: modulated_logp_diff = rank_gap_modulation * logp_diff.\n7. Compute the core hinge error: hinge_error = softplus(dynamic_margin - modulated_logp_diff).\n8. Clamp the error for stability (new stability trick): clamped_error = clamp(hinge_error, min=0, max=max_error).\n9. Create an adaptive temperature based on the margin's magnitude (inspired by Parent 1): adaptive_temperature = 1.0 + gamma * sigmoid(abs(dynamic_margin)).\n10. Compute the final loss by scaling the clamped error with the adaptive temperature: loss = adaptive_temperature * clamped_error.", "hyperparams": {"beta": 1.0, "gamma": 0.5, "alpha": 0.1, "max_error": 10.0}, "operators_used": ["rank_gap", "zscore", "tanh", "softplus", "clamp", "sigmoid"], "implementation_hint": {"expects": ["cost_a", "cost_b", "logp_a", "logp_b"], "returns": "scalar"}}, "fitness": {"hf_like_score": 8.402971744537354, "validation_objective": 8.402971744537354, "generalization_penalty": 0.0, "generalization_objectives": {"20": 3.9807212352752686}, "train_score_mean": 10.82150884628296, "train_loss_mean": 0.5488855385780335, "pair_count": 12902394, "config": {"hf": {"problem": "tsp", "hf_steps": 100, "train_problem_size": 100, "valid_problem_sizes": [20], "train_batch_size": 64, "pomo_size": 64, "learning_rate": 0.0003, "weight_decay": 1e-06, "alpha": 0.05, "device": "cuda", "seed": 1234, "num_validation_episodes": 128, "validation_batch_size": 64, "generalization_penalty_weight": 1.0, "pool_version": "v0"}, "free_loss": {"f1_steps": 100, "f2_steps": 100, "f3_enabled": false}}, "loss_ir": {"name": "Adaptive Temperature Hinge Loss with Rank-Gap Modulation", "intuition": "This loss function creates a dynamic hinge loss by combining a margin sensitive to batch-wise rank with an adaptive temperature, and introduces a new coupling mechanism that directly modulates the model's log-probability difference.\n\nFrom Parent 0 (Adaptive Hinge-Sigmoid Hybrid Loss), we inherit the core idea of a `dynamic_margin` derived from a standardized rank gap (`beta * tanh(zscore(rank_gap(...)))`). This creates a target margin that adapts to the relative cost difference of a pair within its batch. We also inherit the use of `softplus` to create a smooth, non-negative hinge loss.\n\nFrom Parent 1 (Rank-Calibrated Dynamic Hinge Loss), we inherit the concept of an `adaptive_temperature` that scales the loss based on the magnitude of the dynamic margin (`1.0 + gamma * sigmoid(abs(dynamic_margin))`). This makes the loss more sensitive to pairs with a clear, well-separated rank.\n\nAs a new coupling idea, we introduce a `rank_gap_modulation` factor that directly scales the model's log-probability difference (`logp_diff`). This modulation, `(1.0 + alpha * softplus(cost_gap_signed))`, is applied *before* the log-probability difference is compared to the margin. The intuition is to amplify the model's perceived preference for the better response (`a`) when the cost difference is large and positive, effectively demanding a stronger signal from the model for easier-to-rank pairs. This couples the raw cost signal directly with the model's output signal within the hinge comparison.\n\nA second new idea is a stability trick where the final hinge error is clamped to a maximum value `max_error` before being scaled by the adaptive temperature. This prevents extremely misaligned predictions on a single pair from dominating the batch gradient.\n\nWhen `cost(a) < cost(b)`, `cost_gap_signed` is positive. The loss encourages the modulated `logp_diff` to be greater than the `dynamic_margin` to achieve zero loss, thus correctly preferring `a`.", "hyperparams": {"beta": 1.0, "gamma": 0.5, "alpha": 0.1, "max_error": 10.0}, "operators_used": ["rank_gap", "zscore", "tanh", "softplus", "clamp", "sigmoid"]}}, "better_than_baseline": true}
{"generation": 8, "index": 8, "ir": {"name": "Rank-Modulated Sigmoid-Hinge Loss", "intuition": "This loss function combines the adaptive margin and hinge-like error structure of one parent with a sigmoid-based adaptive temperature from the other, introducing a new coupling mechanism that directly modulates the preference error by the raw rank gap.\n\nFrom Parent 0 (Adaptive Hinge-Sigmoid Hybrid Loss), we inherit the core idea of a `dynamic_margin` derived from a standardized rank gap (`beta * tanh(zscore(rank_gap(...)))`). We also adopt its `softplus`-based hinge structure to compute a non-negative `preference_error` (`softplus(dynamic_margin - logp_diff)`), which is zero when the model's preference is correct.\n\nFrom Parent 1 (Rank-Calibrated Dynamic Hinge Loss), we inherit the concept of an `adaptive_temperature` that scales the loss. Specifically, we use its sigmoid-based formulation (`1.0 + gamma * sigmoid(abs(dynamic_margin))`) to create a smooth, bounded temperature that increases sensitivity for pairs with a clearer preference signal.\n\nAs a new coupling idea, we introduce a `rank_gap_modulator`. This is a non-negative term, `softplus(cost_gap_signed)`, that directly scales the `preference_error`. Unlike the parents, which use `abs(cost_gap_signed)` or don't use the raw gap for scaling, this modulator uses the *signed* rank gap. The `softplus` function ensures it's always positive but makes the scaling factor much larger when `cost_gap_signed` is positive (i.e., when `cost(a) > cost(b)` and the model should prefer `b`). This directly ties the magnitude of the penalty to the raw rank difference, significantly increasing the loss for incorrectly ranked pairs with large rank gaps.\n\nWhen `cost(a) < cost(b)`, `cost_gap_signed` is negative. The `dynamic_margin` is also negative, encouraging `logp_diff` to be positive. The `rank_gap_modulator` is a small positive value close to zero, so any `preference_error` is down-weighted. Conversely, when `cost(a) > cost(b)`, `cost_gap_signed` is positive, making the modulator large and heavily penalizing any failure to prefer `b`.", "pseudocode": "1. Compute the log-probability difference: logp_diff = logp_a - logp_b.\n2. Compute the signed, batch-relative rank gap: cost_gap_signed = rank_gap(cost_a, cost_b).\n3. Standardize the rank gap across the batch: cost_gap_zscored = zscore(cost_gap_signed).\n4. Create a bounded dynamic margin from the standardized rank gap (inherited from Parent 0): dynamic_margin = beta * tanh(cost_gap_zscored).\n5. Compute a smooth, non-negative preference error (structure from Parent 0): preference_error = softplus(dynamic_margin - logp_diff).\n6. Create an adaptive temperature based on the margin's magnitude (inherited from Parent 1): adaptive_temperature = 1.0 + gamma * sigmoid(abs(dynamic_margin)).\n7. Create a modulator from the signed rank gap (new coupling idea): rank_gap_modulator = softplus(cost_gap_signed).\n8. Modulate the preference error with the rank gap modulator (new coupling idea): modulated_error = rank_gap_modulator * preference_error.\n9. Combine the modulated error and adaptive temperature in a stable logarithmic form (structure from parents): loss = log(1.0 + adaptive_temperature * modulated_error).", "hyperparams": {"beta": 1.0, "gamma": 0.5}, "operators_used": ["rank_gap", "zscore", "tanh", "softplus", "sigmoid", "log"], "implementation_hint": {"expects": ["cost_a", "cost_b", "logp_a", "logp_b"], "returns": "scalar"}}, "fitness": {"hf_like_score": 8.467233180999756, "validation_objective": 8.467233180999756, "generalization_penalty": 0.0, "generalization_objectives": {"20": 4.044079303741455}, "train_score_mean": 10.523358116149902, "train_loss_mean": 0.599608747959137, "pair_count": 12902394, "config": {"hf": {"problem": "tsp", "hf_steps": 100, "train_problem_size": 100, "valid_problem_sizes": [20], "train_batch_size": 64, "pomo_size": 64, "learning_rate": 0.0003, "weight_decay": 1e-06, "alpha": 0.05, "device": "cuda", "seed": 1234, "num_validation_episodes": 128, "validation_batch_size": 64, "generalization_penalty_weight": 1.0, "pool_version": "v0"}, "free_loss": {"f1_steps": 100, "f2_steps": 100, "f3_enabled": false}}, "loss_ir": {"name": "Rank-Modulated Sigmoid-Hinge Loss", "intuition": "This loss function combines the adaptive margin and hinge-like error structure of one parent with a sigmoid-based adaptive temperature from the other, introducing a new coupling mechanism that directly modulates the preference error by the raw rank gap.\n\nFrom Parent 0 (Adaptive Hinge-Sigmoid Hybrid Loss), we inherit the core idea of a `dynamic_margin` derived from a standardized rank gap (`beta * tanh(zscore(rank_gap(...)))`). We also adopt its `softplus`-based hinge structure to compute a non-negative `preference_error` (`softplus(dynamic_margin - logp_diff)`), which is zero when the model's preference is correct.\n\nFrom Parent 1 (Rank-Calibrated Dynamic Hinge Loss), we inherit the concept of an `adaptive_temperature` that scales the loss. Specifically, we use its sigmoid-based formulation (`1.0 + gamma * sigmoid(abs(dynamic_margin))`) to create a smooth, bounded temperature that increases sensitivity for pairs with a clearer preference signal.\n\nAs a new coupling idea, we introduce a `rank_gap_modulator`. This is a non-negative term, `softplus(cost_gap_signed)`, that directly scales the `preference_error`. Unlike the parents, which use `abs(cost_gap_signed)` or don't use the raw gap for scaling, this modulator uses the *signed* rank gap. The `softplus` function ensures it's always positive but makes the scaling factor much larger when `cost_gap_signed` is positive (i.e., when `cost(a) > cost(b)` and the model should prefer `b`). This directly ties the magnitude of the penalty to the raw rank difference, significantly increasing the loss for incorrectly ranked pairs with large rank gaps.\n\nWhen `cost(a) < cost(b)`, `cost_gap_signed` is negative. The `dynamic_margin` is also negative, encouraging `logp_diff` to be positive. The `rank_gap_modulator` is a small positive value close to zero, so any `preference_error` is down-weighted. Conversely, when `cost(a) > cost(b)`, `cost_gap_signed` is positive, making the modulator large and heavily penalizing any failure to prefer `b`.", "hyperparams": {"beta": 1.0, "gamma": 0.5}, "operators_used": ["rank_gap", "zscore", "tanh", "softplus", "sigmoid", "log"]}}, "better_than_baseline": false}
{"generation": 8, "index": 9, "ir": {"name": "Rank-Modulated Dual-Margin Loss", "intuition": "This loss function creates a preference signal by combining a dynamic, rank-based margin with a direct, cost-based scaling factor, and introduces a novel dual-margin coupling mechanism.\n\nFrom Parent 0 (Adaptive Hinge-Sigmoid Hybrid Loss), we inherit the core mechanism of a `dynamic_margin` derived from a standardized rank gap (`beta * tanh(zscore(rank_gap(...)))`). This provides a target for the log-probability difference that is sensitive to a pair's relative rank within a batch.\n\nFrom Parent 1 (Rank-Calibrated Dynamic Hinge Loss), we inherit the idea of directly using the raw cost difference to modulate the loss magnitude. We adapt this by creating a `rank_gap_scale` (`softplus(abs(cost_gap_signed))`) that directly weights the final loss term, making pairs with larger cost differences contribute more significantly.\n\nAs a new coupling idea, we introduce a **dual-margin structure**. We compute two hinge-like terms. The first, `preference_error`, is the standard hinge loss `softplus(dynamic_margin - logp_diff)`. The second, `reverse_preference_error`, is `softplus(-dynamic_margin - logp_diff)`. The total error is the sum of these two. This symmetric structure penalizes the model for both failing to meet the target margin (`preference_error`) and for being too far on the *wrong* side of the preference (`reverse_preference_error`), providing a more robust error signal than a single-sided hinge. The final loss is `rank_gap_scale * (preference_error + reverse_preference_error)`, effectively coupling the raw cost magnitude (from Parent 1's idea) with a symmetric, rank-based preference error (built upon Parent 0's idea).\n\nWhen `cost(a) < cost(b)`, `dynamic_margin` is negative. The loss encourages `logp_diff` to be positive. The `preference_error` term pushes `logp_diff` to be greater than the negative `dynamic_margin`, while the `reverse_preference_error` term (with a positive `-dynamic_margin`) strongly penalizes `logp_diff` if it is negative, thus encouraging a clear preference for `a`.", "pseudocode": "1. Compute the log-probability difference: logp_diff = logp_a - logp_b.\n2. Compute the signed, batch-relative rank gap: cost_gap_signed = rank_gap(cost_a, cost_b).\n3. Standardize the rank gap across the batch: cost_gap_zscored = zscore(cost_gap_signed).\n4. Create a bounded dynamic margin from the standardized rank gap (inherited from Parent 0): dynamic_margin = beta * tanh(cost_gap_zscored).\n5. Compute the primary preference error using a smooth hinge: preference_error = softplus(dynamic_margin - logp_diff).\n6. Compute a reverse preference error to penalize being on the wrong side (new coupling idea): reverse_preference_error = softplus(-dynamic_margin - logp_diff).\n7. Sum the two error terms to create a symmetric penalty (new coupling idea): total_error = preference_error + reverse_preference_error.\n8. Create a scaling factor from the raw rank gap magnitude (idea from Parent 1): rank_gap_scale = softplus(abs(cost_gap_signed)).\n9. Modulate the total error with the rank gap scale: loss = rank_gap_scale * total_error.", "hyperparams": {"beta": 1.0}, "operators_used": ["rank_gap", "zscore", "tanh", "softplus"], "implementation_hint": {"expects": ["cost_a", "cost_b", "logp_a", "logp_b"], "returns": "scalar"}}, "fitness": {"hf_like_score": 8.456775188446045, "validation_objective": 8.456775188446045, "generalization_penalty": 0.0, "generalization_objectives": {"20": 3.9957261085510254}, "train_score_mean": 10.514750299453736, "train_loss_mean": 0.5987137937545777, "pair_count": 12902383, "config": {"hf": {"problem": "tsp", "hf_steps": 100, "train_problem_size": 100, "valid_problem_sizes": [20], "train_batch_size": 64, "pomo_size": 64, "learning_rate": 0.0003, "weight_decay": 1e-06, "alpha": 0.05, "device": "cuda", "seed": 1234, "num_validation_episodes": 128, "validation_batch_size": 64, "generalization_penalty_weight": 1.0, "pool_version": "v0"}, "free_loss": {"f1_steps": 100, "f2_steps": 100, "f3_enabled": false}}, "loss_ir": {"name": "Rank-Modulated Dual-Margin Loss", "intuition": "This loss function creates a preference signal by combining a dynamic, rank-based margin with a direct, cost-based scaling factor, and introduces a novel dual-margin coupling mechanism.\n\nFrom Parent 0 (Adaptive Hinge-Sigmoid Hybrid Loss), we inherit the core mechanism of a `dynamic_margin` derived from a standardized rank gap (`beta * tanh(zscore(rank_gap(...)))`). This provides a target for the log-probability difference that is sensitive to a pair's relative rank within a batch.\n\nFrom Parent 1 (Rank-Calibrated Dynamic Hinge Loss), we inherit the idea of directly using the raw cost difference to modulate the loss magnitude. We adapt this by creating a `rank_gap_scale` (`softplus(abs(cost_gap_signed))`) that directly weights the final loss term, making pairs with larger cost differences contribute more significantly.\n\nAs a new coupling idea, we introduce a **dual-margin structure**. We compute two hinge-like terms. The first, `preference_error`, is the standard hinge loss `softplus(dynamic_margin - logp_diff)`. The second, `reverse_preference_error`, is `softplus(-dynamic_margin - logp_diff)`. The total error is the sum of these two. This symmetric structure penalizes the model for both failing to meet the target margin (`preference_error`) and for being too far on the *wrong* side of the preference (`reverse_preference_error`), providing a more robust error signal than a single-sided hinge. The final loss is `rank_gap_scale * (preference_error + reverse_preference_error)`, effectively coupling the raw cost magnitude (from Parent 1's idea) with a symmetric, rank-based preference error (built upon Parent 0's idea).\n\nWhen `cost(a) < cost(b)`, `dynamic_margin` is negative. The loss encourages `logp_diff` to be positive. The `preference_error` term pushes `logp_diff` to be greater than the negative `dynamic_margin`, while the `reverse_preference_error` term (with a positive `-dynamic_margin`) strongly penalizes `logp_diff` if it is negative, thus encouraging a clear preference for `a`.", "hyperparams": {"beta": 1.0}, "operators_used": ["rank_gap", "zscore", "tanh", "softplus"]}}, "better_than_baseline": false}
{"generation": 9, "index": 0, "ir": {"name": "Adaptive Rank-Modulated Hinge Loss", "intuition": "This loss function creates a dynamic hinge loss by combining adaptive margin and temperature mechanisms from its parents, and introduces a new coupling idea where the preference error is directly modulated by the raw rank gap.\n\nFrom Parent 0 (Adaptive Hinge-Sigmoid Hybrid Loss), we inherit the core concept of a `dynamic_margin` derived from a standardized rank gap (`beta * tanh(zscore(rank_gap(...)))`). This creates a target margin that is sensitive to the relative ranking of a pair within the batch. We also inherit its use of `softplus` to form a non-negative, smooth hinge loss term, which we call `preference_error`.\n\nFrom Parent 1 (Rank-Calibrated Dynamic Hinge Loss), we inherit the idea of an `adaptive_temperature` that scales the loss based on the magnitude of the dynamic margin (`1.0 + gamma * sigmoid(abs(dynamic_margin))`). This makes the loss more sensitive to pairs with a clear preference signal (large `abs(dynamic_margin)`).\n\nAs a new coupling idea, we directly scale the `preference_error` by a factor derived from the raw, un-normalized rank gap (`rank_gap_scale = softplus(abs(cost_gap_signed))`). The intuition is to make the loss penalty proportional to the magnitude of the cost difference, ensuring that pairs with larger cost discrepancies contribute more significantly to the final loss. This `rank_gap_scale` modulates the error *before* the final logarithmic stabilization, coupling the raw cost signal with the preference error term more tightly than in the parents.\n\nWhen `cost(a) < cost(b)`, the `dynamic_margin` becomes negative. The loss encourages `logp_diff` to be positive and larger than this margin to drive the `preference_error` to zero, thus correctly preferring `a`.", "pseudocode": "1. Compute the log-probability difference: logp_diff = logp_a - logp_b.\n2. Compute the signed, batch-relative rank gap: cost_gap_signed = rank_gap(cost_a, cost_b).\n3. Standardize the rank gap across the batch: cost_gap_zscored = zscore(cost_gap_signed).\n4. Create a bounded dynamic margin from the standardized rank gap (inherited from Parent 0): dynamic_margin = beta * tanh(cost_gap_zscored).\n5. Compute a smooth hinge-like preference error (structure from Parent 0): preference_error = softplus(dynamic_margin - logp_diff).\n6. Create a scaling factor from the raw rank gap (new coupling idea): rank_gap_scale = softplus(abs(cost_gap_signed)).\n7. Modulate the preference error with the rank gap scale (new coupling idea): scaled_error = rank_gap_scale * preference_error.\n8. Create an adaptive temperature based on the margin's magnitude (inherited from Parent 1): adaptive_temperature = 1.0 + gamma * sigmoid(abs(dynamic_margin)).\n9. Combine the scaled error and adaptive temperature in a stable logarithmic form (structure from parents): loss = log(1.0 + adaptive_temperature * scaled_error).", "hyperparams": {"beta": 1.0, "gamma": 1.0}, "operators_used": ["rank_gap", "zscore", "tanh", "softplus", "sigmoid", "log"], "implementation_hint": {"expects": ["cost_a", "cost_b", "logp_a", "logp_b"], "returns": "scalar"}}, "fitness": {"hf_like_score": 8.45470142364502, "validation_objective": 8.45470142364502, "generalization_penalty": 0.0, "generalization_objectives": {"20": 4.0151143074035645}, "train_score_mean": 10.496652803421021, "train_loss_mean": 0.5990154057741165, "pair_count": 12902388, "config": {"hf": {"problem": "tsp", "hf_steps": 100, "train_problem_size": 100, "valid_problem_sizes": [20], "train_batch_size": 64, "pomo_size": 64, "learning_rate": 0.0003, "weight_decay": 1e-06, "alpha": 0.05, "device": "cuda", "seed": 1234, "num_validation_episodes": 128, "validation_batch_size": 64, "generalization_penalty_weight": 1.0, "pool_version": "v0"}, "free_loss": {"f1_steps": 100, "f2_steps": 100, "f3_enabled": false}}, "loss_ir": {"name": "Adaptive Rank-Modulated Hinge Loss", "intuition": "This loss function creates a dynamic hinge loss by combining adaptive margin and temperature mechanisms from its parents, and introduces a new coupling idea where the preference error is directly modulated by the raw rank gap.\n\nFrom Parent 0 (Adaptive Hinge-Sigmoid Hybrid Loss), we inherit the core concept of a `dynamic_margin` derived from a standardized rank gap (`beta * tanh(zscore(rank_gap(...)))`). This creates a target margin that is sensitive to the relative ranking of a pair within the batch. We also inherit its use of `softplus` to form a non-negative, smooth hinge loss term, which we call `preference_error`.\n\nFrom Parent 1 (Rank-Calibrated Dynamic Hinge Loss), we inherit the idea of an `adaptive_temperature` that scales the loss based on the magnitude of the dynamic margin (`1.0 + gamma * sigmoid(abs(dynamic_margin))`). This makes the loss more sensitive to pairs with a clear preference signal (large `abs(dynamic_margin)`).\n\nAs a new coupling idea, we directly scale the `preference_error` by a factor derived from the raw, un-normalized rank gap (`rank_gap_scale = softplus(abs(cost_gap_signed))`). The intuition is to make the loss penalty proportional to the magnitude of the cost difference, ensuring that pairs with larger cost discrepancies contribute more significantly to the final loss. This `rank_gap_scale` modulates the error *before* the final logarithmic stabilization, coupling the raw cost signal with the preference error term more tightly than in the parents.\n\nWhen `cost(a) < cost(b)`, the `dynamic_margin` becomes negative. The loss encourages `logp_diff` to be positive and larger than this margin to drive the `preference_error` to zero, thus correctly preferring `a`.", "hyperparams": {"beta": 1.0, "gamma": 1.0}, "operators_used": ["rank_gap", "zscore", "tanh", "softplus", "sigmoid", "log"]}}, "better_than_baseline": false}
{"generation": 9, "index": 1, "ir": {"name": "Rank-Modulated Dual-Margin Hinge Loss", "intuition": "This loss function constructs a robust preference learning signal by combining a dynamic, rank-based margin with a novel secondary margin and a rank-based scaling mechanism.\n\nFrom Parent 0 (Adaptive Hinge-Sigmoid Hybrid Loss), we inherit the core idea of a `dynamic_margin` that is sensitive to the relative ranking of a pair within a batch. This is calculated as `beta * tanh(zscore(rank_gap(...)))`, providing a bounded and standardized preference target.\n\nFrom Parent 1 (Rank-Calibrated Dynamic Hinge Loss), we inherit the concept of directly using the raw rank gap to modulate the loss magnitude. We use this idea to create a `rank_gap_scale` from the raw `cost_gap_signed`, ensuring that pairs with larger cost differences contribute more significantly to the final loss.\n\nWe introduce two new coupling ideas:\n1. **Dual-Margin Structure**: Instead of a single margin, we introduce a secondary, static margin `alpha`. The preference error is calculated as `softplus(dynamic_margin + alpha - logp_diff)`. This `alpha` acts as a fixed 'buffer', ensuring that the model must not only meet the dynamic margin but exceed it by a constant amount. This provides a more robust separation between preferred and dispreferred candidates.\n2. **Error Modulation before Temperature Scaling**: We modulate the `preference_error` with the `rank_gap_scale` *before* applying the final adaptive temperature. This creates a `scaled_error` that is sensitive to both the relative rank (`dynamic_margin`) and the absolute cost difference (`rank_gap_scale`).\n\nThe final loss is constructed using an adaptive temperature (inspired by both parents) and a stable logarithmic form, `log(1 + adaptive_temperature * scaled_error)`, which prevents exploding gradients while maintaining sensitivity.\n\nWhen `cost(a) < cost(b)`, the `dynamic_margin` is negative. The loss encourages `logp_diff` to be positive and greater than `dynamic_margin + alpha` to drive the `preference_error` to zero, thus correctly preferring `a` with a robust margin.", "pseudocode": "1. Compute the log-probability difference: logp_diff = logp_a - logp_b.\n2. Compute the signed, batch-relative rank gap: cost_gap_signed = rank_gap(cost_a, cost_b).\n3. Standardize the rank gap across the batch: cost_gap_zscored = zscore(cost_gap_signed).\n4. Create a bounded dynamic margin from the standardized rank gap (inherited from Parent 0): dynamic_margin = beta * tanh(cost_gap_zscored).\n5. Create a scaling factor from the raw rank gap (inspired by Parent 1): rank_gap_scale = softplus(abs(cost_gap_signed)).\n6. Compute a preference error using a dual-margin structure (new coupling idea): preference_error = softplus(dynamic_margin + alpha - logp_diff).\n7. Modulate the preference error with the rank gap scale (new coupling idea): scaled_error = rank_gap_scale * preference_error.\n8. Create an adaptive temperature based on the margin's magnitude (inspired by both parents): adaptive_temperature = 1.0 + gamma * sigmoid(abs(dynamic_margin)).\n9. Combine the scaled error and adaptive temperature in a stable logarithmic form: loss = log(1.0 + adaptive_temperature * scaled_error).", "hyperparams": {"beta": 1.0, "gamma": 1.0, "alpha": 0.1}, "operators_used": ["rank_gap", "zscore", "tanh", "softplus", "sigmoid", "log"], "implementation_hint": {"expects": ["cost_a", "cost_b", "logp_a", "logp_b"], "returns": "scalar"}}, "fitness": {"hf_like_score": 8.383600234985352, "validation_objective": 8.383600234985352, "generalization_penalty": 0.0, "generalization_objectives": {"20": 3.970643997192383}, "train_score_mean": 10.828165254592896, "train_loss_mean": 0.54294069647789, "pair_count": 12902391, "config": {"hf": {"problem": "tsp", "hf_steps": 100, "train_problem_size": 100, "valid_problem_sizes": [20], "train_batch_size": 64, "pomo_size": 64, "learning_rate": 0.0003, "weight_decay": 1e-06, "alpha": 0.05, "device": "cuda", "seed": 1234, "num_validation_episodes": 128, "validation_batch_size": 64, "generalization_penalty_weight": 1.0, "pool_version": "v0"}, "free_loss": {"f1_steps": 100, "f2_steps": 100, "f3_enabled": false}}, "loss_ir": {"name": "Rank-Modulated Dual-Margin Hinge Loss", "intuition": "This loss function constructs a robust preference learning signal by combining a dynamic, rank-based margin with a novel secondary margin and a rank-based scaling mechanism.\n\nFrom Parent 0 (Adaptive Hinge-Sigmoid Hybrid Loss), we inherit the core idea of a `dynamic_margin` that is sensitive to the relative ranking of a pair within a batch. This is calculated as `beta * tanh(zscore(rank_gap(...)))`, providing a bounded and standardized preference target.\n\nFrom Parent 1 (Rank-Calibrated Dynamic Hinge Loss), we inherit the concept of directly using the raw rank gap to modulate the loss magnitude. We use this idea to create a `rank_gap_scale` from the raw `cost_gap_signed`, ensuring that pairs with larger cost differences contribute more significantly to the final loss.\n\nWe introduce two new coupling ideas:\n1. **Dual-Margin Structure**: Instead of a single margin, we introduce a secondary, static margin `alpha`. The preference error is calculated as `softplus(dynamic_margin + alpha - logp_diff)`. This `alpha` acts as a fixed 'buffer', ensuring that the model must not only meet the dynamic margin but exceed it by a constant amount. This provides a more robust separation between preferred and dispreferred candidates.\n2. **Error Modulation before Temperature Scaling**: We modulate the `preference_error` with the `rank_gap_scale` *before* applying the final adaptive temperature. This creates a `scaled_error` that is sensitive to both the relative rank (`dynamic_margin`) and the absolute cost difference (`rank_gap_scale`).\n\nThe final loss is constructed using an adaptive temperature (inspired by both parents) and a stable logarithmic form, `log(1 + adaptive_temperature * scaled_error)`, which prevents exploding gradients while maintaining sensitivity.\n\nWhen `cost(a) < cost(b)`, the `dynamic_margin` is negative. The loss encourages `logp_diff` to be positive and greater than `dynamic_margin + alpha` to drive the `preference_error` to zero, thus correctly preferring `a` with a robust margin.", "hyperparams": {"beta": 1.0, "gamma": 1.0, "alpha": 0.1}, "operators_used": ["rank_gap", "zscore", "tanh", "softplus", "sigmoid", "log"]}}, "better_than_baseline": true}
{"generation": 9, "index": 2, "ir": {"name": "Rank-Modulated Dual-Margin Hinge Loss", "intuition": "This loss function constructs a dual-margin hinge loss where the margin's characteristics are adaptively controlled by both the relative rank and the absolute cost difference of a pair. It combines a dynamic, rank-based margin with a direct, cost-based modulation.\n\nFrom Parent 0 (Adaptive Hinge-Sigmoid Hybrid Loss), we inherit the core idea of a `dynamic_margin` derived from the standardized rank gap (`beta * tanh(zscore(rank_gap(...)))`). This creates a target preference margin that is sensitive to the relative ranking of a pair within the batch.\n\nFrom Parent 1 (Rank-Calibrated Dynamic Hinge Loss), we inherit the idea of directly using the raw cost signal to modulate the loss. Specifically, we adapt its `rank_gap_scale` concept to create a second, 'raw' margin component.\n\nAs a new coupling idea, we introduce a **dual-margin mechanism**. The final margin is a weighted sum of the rank-based `dynamic_margin` and a new `raw_cost_margin` derived directly from the cost difference (`gamma * tanh(cost_a - cost_b)`). This `raw_cost_margin` provides a baseline preference signal based on the absolute cost difference, which is then refined by the batch-relative `dynamic_margin`. The final margin, `combined_margin`, thus reflects both the local (batch-rank) and global (absolute cost) importance of the preference. The loss is then a standard smooth hinge loss (`softplus`) applied to this combined margin against the model's log-probability difference.\n\nWhen `cost(a) < cost(b)`, both `dynamic_margin` and `raw_cost_margin` become negative. The `combined_margin` is therefore negative, and the loss encourages `logp_diff` (logp_a - logp_b) to be positive and larger than this combined margin to drive the loss to zero, thus correctly preferring `a`.", "pseudocode": "1. Compute the log-probability difference: logp_diff = logp_a - logp_b.\n2. Compute the signed, batch-relative rank gap: cost_gap_signed = rank_gap(cost_a, cost_b).\n3. Standardize the rank gap across the batch: cost_gap_zscored = zscore(cost_gap_signed).\n4. Create a rank-based dynamic margin from the standardized rank gap (inherited from Parent 0): dynamic_margin = beta * tanh(cost_gap_zscored).\n5. Create a raw cost-based margin directly from the cost difference (inspired by Parent 1's cost-scaling): raw_cost_margin = gamma * tanh(cost_a - cost_b).\n6. Combine the two margins into a single target margin (new coupling idea): combined_margin = dynamic_margin + raw_cost_margin.\n7. Compute the final loss using a smooth hinge-like structure against the combined margin: loss = softplus(combined_margin - logp_diff).", "hyperparams": {"beta": 1.0, "gamma": 0.5}, "operators_used": ["rank_gap", "zscore", "tanh", "softplus"], "implementation_hint": {"expects": ["cost_a", "cost_b", "logp_a", "logp_b"], "returns": "scalar"}}, "fitness": {"hf_like_score": 8.477487564086914, "validation_objective": 8.477487564086914, "generalization_penalty": 0.0, "generalization_objectives": {"20": 4.019715785980225}, "train_score_mean": 10.50021559715271, "train_loss_mean": 0.6008014410734177, "pair_count": 12902385, "config": {"hf": {"problem": "tsp", "hf_steps": 100, "train_problem_size": 100, "valid_problem_sizes": [20], "train_batch_size": 64, "pomo_size": 64, "learning_rate": 0.0003, "weight_decay": 1e-06, "alpha": 0.05, "device": "cuda", "seed": 1234, "num_validation_episodes": 128, "validation_batch_size": 64, "generalization_penalty_weight": 1.0, "pool_version": "v0"}, "free_loss": {"f1_steps": 100, "f2_steps": 100, "f3_enabled": false}}, "loss_ir": {"name": "Rank-Modulated Dual-Margin Hinge Loss", "intuition": "This loss function constructs a dual-margin hinge loss where the margin's characteristics are adaptively controlled by both the relative rank and the absolute cost difference of a pair. It combines a dynamic, rank-based margin with a direct, cost-based modulation.\n\nFrom Parent 0 (Adaptive Hinge-Sigmoid Hybrid Loss), we inherit the core idea of a `dynamic_margin` derived from the standardized rank gap (`beta * tanh(zscore(rank_gap(...)))`). This creates a target preference margin that is sensitive to the relative ranking of a pair within the batch.\n\nFrom Parent 1 (Rank-Calibrated Dynamic Hinge Loss), we inherit the idea of directly using the raw cost signal to modulate the loss. Specifically, we adapt its `rank_gap_scale` concept to create a second, 'raw' margin component.\n\nAs a new coupling idea, we introduce a **dual-margin mechanism**. The final margin is a weighted sum of the rank-based `dynamic_margin` and a new `raw_cost_margin` derived directly from the cost difference (`gamma * tanh(cost_a - cost_b)`). This `raw_cost_margin` provides a baseline preference signal based on the absolute cost difference, which is then refined by the batch-relative `dynamic_margin`. The final margin, `combined_margin`, thus reflects both the local (batch-rank) and global (absolute cost) importance of the preference. The loss is then a standard smooth hinge loss (`softplus`) applied to this combined margin against the model's log-probability difference.\n\nWhen `cost(a) < cost(b)`, both `dynamic_margin` and `raw_cost_margin` become negative. The `combined_margin` is therefore negative, and the loss encourages `logp_diff` (logp_a - logp_b) to be positive and larger than this combined margin to drive the loss to zero, thus correctly preferring `a`.", "hyperparams": {"beta": 1.0, "gamma": 0.5}, "operators_used": ["rank_gap", "zscore", "tanh", "softplus"]}}, "better_than_baseline": false}
{"generation": 9, "index": 3, "ir": {"name": "Adaptive Margin Sigmoid Loss with Rank-Gap Modulation", "intuition": "This loss function creates a hybrid loss that integrates a dynamic margin within a probabilistic sigmoid framework, further modulated by the raw rank-gap signal.\n\nFrom Parent 0 (Adaptive Hinge-Sigmoid Hybrid Loss), we inherit the core idea of creating a `dynamic_margin` based on a standardized rank gap: `beta * tanh(zscore(rank_gap(...)))`. This makes the required preference margin sensitive to the relative ranking of a pair within a batch.\n\nFrom Parent 1 (Rank-Calibrated Dynamic Hinge Loss), we inherit the structural idea of using the raw, un-normalized rank gap as a modulating factor. However, instead of scaling the error, we use it to directly adjust the log-probability difference itself, making the model's perceived preference stronger for pairs with large cost differences.\n\nAs a new coupling idea, we combine these inherited concepts into a single, cohesive sigmoid loss. The `dynamic_margin` is not used in a hinge loss, but rather subtracted directly from a modulated log-probability difference inside a `logsigmoid` function. Specifically, we first create a `rank_gap_scale` from the raw rank gap using `softplus`. We then use this to modulate the log-probability difference: `modulated_logp_diff = rank_gap_scale * logp_diff`. The final loss is `-logsigmoid(modulated_logp_diff - dynamic_margin)`. This formulation smoothly transitions from a probabilistic loss (like DPO) to one that enforces a margin, where the margin itself is dynamic and the preference signal is amplified by the magnitude of the cost difference.\n\nWhen `cost(a) < cost(b)`, `dynamic_margin` is negative. The loss encourages `logp_a > logp_b`. The `rank_gap_scale` is positive, amplifying this effect. The loss pushes `modulated_logp_diff` to be much larger than the negative `dynamic_margin`, correctly preferring `a`.", "pseudocode": "1. Compute the log-probability difference: logp_diff = logp_a - logp_b.\n2. Compute the signed, batch-relative rank gap: cost_gap_signed = rank_gap(cost_a, cost_b).\n3. Standardize the rank gap across the batch: cost_gap_zscored = zscore(cost_gap_signed).\n4. Create a bounded dynamic margin from the standardized rank gap (inherited from Parent 0): dynamic_margin = beta * tanh(cost_gap_zscored).\n5. Create a scaling factor from the raw rank gap magnitude (inspired by Parent 1): rank_gap_scale = softplus(gamma * abs(cost_gap_signed)).\n6. Modulate the log-probability difference using the rank gap scale (new coupling idea): modulated_logp_diff = rank_gap_scale * logp_diff.\n7. Combine the modulated preference and dynamic margin within a sigmoid loss framework (new coupling idea): loss = -logsigmoid(modulated_logp_diff - dynamic_margin).", "hyperparams": {"beta": 1.0, "gamma": 0.1}, "operators_used": ["rank_gap", "zscore", "tanh", "softplus", "logsigmoid"], "implementation_hint": {"expects": ["cost_a", "cost_b", "logp_a", "logp_b"], "returns": "scalar"}}, "fitness": {"hf_like_score": 8.399177551269531, "validation_objective": 8.399177551269531, "generalization_penalty": 0.0, "generalization_objectives": {"20": 4.03782320022583}, "train_score_mean": 10.456263580322265, "train_loss_mean": 0.6048736494779586, "pair_count": 12902385, "config": {"hf": {"problem": "tsp", "hf_steps": 100, "train_problem_size": 100, "valid_problem_sizes": [20], "train_batch_size": 64, "pomo_size": 64, "learning_rate": 0.0003, "weight_decay": 1e-06, "alpha": 0.05, "device": "cuda", "seed": 1234, "num_validation_episodes": 128, "validation_batch_size": 64, "generalization_penalty_weight": 1.0, "pool_version": "v0"}, "free_loss": {"f1_steps": 100, "f2_steps": 100, "f3_enabled": false}}, "loss_ir": {"name": "Adaptive Margin Sigmoid Loss with Rank-Gap Modulation", "intuition": "This loss function creates a hybrid loss that integrates a dynamic margin within a probabilistic sigmoid framework, further modulated by the raw rank-gap signal.\n\nFrom Parent 0 (Adaptive Hinge-Sigmoid Hybrid Loss), we inherit the core idea of creating a `dynamic_margin` based on a standardized rank gap: `beta * tanh(zscore(rank_gap(...)))`. This makes the required preference margin sensitive to the relative ranking of a pair within a batch.\n\nFrom Parent 1 (Rank-Calibrated Dynamic Hinge Loss), we inherit the structural idea of using the raw, un-normalized rank gap as a modulating factor. However, instead of scaling the error, we use it to directly adjust the log-probability difference itself, making the model's perceived preference stronger for pairs with large cost differences.\n\nAs a new coupling idea, we combine these inherited concepts into a single, cohesive sigmoid loss. The `dynamic_margin` is not used in a hinge loss, but rather subtracted directly from a modulated log-probability difference inside a `logsigmoid` function. Specifically, we first create a `rank_gap_scale` from the raw rank gap using `softplus`. We then use this to modulate the log-probability difference: `modulated_logp_diff = rank_gap_scale * logp_diff`. The final loss is `-logsigmoid(modulated_logp_diff - dynamic_margin)`. This formulation smoothly transitions from a probabilistic loss (like DPO) to one that enforces a margin, where the margin itself is dynamic and the preference signal is amplified by the magnitude of the cost difference.\n\nWhen `cost(a) < cost(b)`, `dynamic_margin` is negative. The loss encourages `logp_a > logp_b`. The `rank_gap_scale` is positive, amplifying this effect. The loss pushes `modulated_logp_diff` to be much larger than the negative `dynamic_margin`, correctly preferring `a`.", "hyperparams": {"beta": 1.0, "gamma": 0.1}, "operators_used": ["rank_gap", "zscore", "tanh", "softplus", "logsigmoid"]}}, "better_than_baseline": true}
{"generation": 9, "index": 4, "ir": {"name": "Adaptive Rank-Modulated Hinge Loss", "intuition": "This loss function creates a dynamic and stable preference loss by combining a rank-based margin with adaptive scaling, and introduces a new coupling mechanism that directly modulates the preference error by the raw rank difference.\n\nFrom Parent 0 (Adaptive Hinge-Sigmoid Hybrid Loss), we inherit the core mechanism of a dynamic, bounded margin (`dynamic_margin = beta * tanh(zscore(rank_gap(...)))`). This margin adapts to the relative ranking of a pair within a batch, providing a flexible target for the model's preference score. We also inherit the use of `softplus` to create a smooth, non-negative hinge-like error term.\n\nFrom Parent 1 (Rank-Calibrated Dynamic Hinge Loss), we inherit the idea of scaling the preference error by a factor derived from the raw cost difference. However, instead of using `softplus`, we use a simple `abs(cost_gap_signed)` to create a `rank_gap_scale`, making the scaling more direct and linear with the magnitude of the rank difference.\n\nAs new coupling ideas, we introduce two modifications. First, we apply the `rank_gap_scale` directly to the `logp_diff` *inside* the hinge calculation (`softplus(dynamic_margin - rank_gap_scale * logp_diff)`). This forces the model's log-probability difference to be scaled up or down based on the rank gap, essentially requiring a stronger preference signal for pairs with a larger rank difference. Second, we introduce a new adaptive temperature `adaptive_temp = 1.0 + gamma * sigmoid(cost_gap_zscored)`, which uses the *signed* standardized rank gap. This makes the loss more sensitive for correctly ordered pairs (positive `cost_gap_zscored`) and less sensitive for incorrectly ordered ones, providing a form of curriculum learning within the batch.\n\nWhen `cost(a) < cost(b)`, `cost_gap_signed` is negative. The loss encourages `logp_diff` to be positive. The negative `dynamic_margin` and the scaled `logp_diff` work together to drive the argument of `softplus` to be negative, minimizing the loss and correctly preferring `a`.", "pseudocode": "1. Compute the log-probability difference: logp_diff = logp_a - logp_b.\n2. Compute the signed, batch-relative rank gap: cost_gap_signed = rank_gap(cost_a, cost_b).\n3. Standardize the rank gap across the batch: cost_gap_zscored = zscore(cost_gap_signed).\n4. Create a bounded dynamic margin from the standardized rank gap (inherited from Parent 0): dynamic_margin = beta * tanh(cost_gap_zscored).\n5. Create a scaling factor from the raw rank gap magnitude (inspired by Parent 1): rank_gap_scale = 1.0 + abs(cost_gap_signed).\n6. Compute the preference error by coupling the rank scale with the logp_diff inside a smooth hinge term (new coupling idea): preference_error = softplus(dynamic_margin - rank_gap_scale * logp_diff).\n7. Create an adaptive temperature from the signed standardized rank gap (new coupling idea): adaptive_temp = 1.0 + gamma * sigmoid(cost_gap_zscored).\n8. Combine the preference error and adaptive temperature in a stable logarithmic form: loss = log(1.0 + adaptive_temp * preference_error).", "hyperparams": {"beta": 1.0, "gamma": 0.5}, "operators_used": ["rank_gap", "zscore", "tanh", "softplus", "sigmoid", "log"], "implementation_hint": {"expects": ["cost_a", "cost_b", "logp_a", "logp_b"], "returns": "scalar"}}, "fitness": {"hf_like_score": 8.47011661529541, "validation_objective": 8.47011661529541, "generalization_penalty": 0.0, "generalization_objectives": {"20": 4.006249666213989}, "train_score_mean": 10.47272120475769, "train_loss_mean": 0.6017997199296952, "pair_count": 12902389, "config": {"hf": {"problem": "tsp", "hf_steps": 100, "train_problem_size": 100, "valid_problem_sizes": [20], "train_batch_size": 64, "pomo_size": 64, "learning_rate": 0.0003, "weight_decay": 1e-06, "alpha": 0.05, "device": "cuda", "seed": 1234, "num_validation_episodes": 128, "validation_batch_size": 64, "generalization_penalty_weight": 1.0, "pool_version": "v0"}, "free_loss": {"f1_steps": 100, "f2_steps": 100, "f3_enabled": false}}, "loss_ir": {"name": "Adaptive Rank-Modulated Hinge Loss", "intuition": "This loss function creates a dynamic and stable preference loss by combining a rank-based margin with adaptive scaling, and introduces a new coupling mechanism that directly modulates the preference error by the raw rank difference.\n\nFrom Parent 0 (Adaptive Hinge-Sigmoid Hybrid Loss), we inherit the core mechanism of a dynamic, bounded margin (`dynamic_margin = beta * tanh(zscore(rank_gap(...)))`). This margin adapts to the relative ranking of a pair within a batch, providing a flexible target for the model's preference score. We also inherit the use of `softplus` to create a smooth, non-negative hinge-like error term.\n\nFrom Parent 1 (Rank-Calibrated Dynamic Hinge Loss), we inherit the idea of scaling the preference error by a factor derived from the raw cost difference. However, instead of using `softplus`, we use a simple `abs(cost_gap_signed)` to create a `rank_gap_scale`, making the scaling more direct and linear with the magnitude of the rank difference.\n\nAs new coupling ideas, we introduce two modifications. First, we apply the `rank_gap_scale` directly to the `logp_diff` *inside* the hinge calculation (`softplus(dynamic_margin - rank_gap_scale * logp_diff)`). This forces the model's log-probability difference to be scaled up or down based on the rank gap, essentially requiring a stronger preference signal for pairs with a larger rank difference. Second, we introduce a new adaptive temperature `adaptive_temp = 1.0 + gamma * sigmoid(cost_gap_zscored)`, which uses the *signed* standardized rank gap. This makes the loss more sensitive for correctly ordered pairs (positive `cost_gap_zscored`) and less sensitive for incorrectly ordered ones, providing a form of curriculum learning within the batch.\n\nWhen `cost(a) < cost(b)`, `cost_gap_signed` is negative. The loss encourages `logp_diff` to be positive. The negative `dynamic_margin` and the scaled `logp_diff` work together to drive the argument of `softplus` to be negative, minimizing the loss and correctly preferring `a`.", "hyperparams": {"beta": 1.0, "gamma": 0.5}, "operators_used": ["rank_gap", "zscore", "tanh", "softplus", "sigmoid", "log"]}}, "better_than_baseline": false}
{"generation": 9, "index": 5, "ir": {"name": "Rank-Modulated Hinge-LogSigmoid Loss", "intuition": "This child loss creates a hybrid preference model by blending a dynamic hinge mechanism with a probabilistic logsigmoid framework, and introduces a new coupling mechanism that uses the rank gap to modulate the hinge margin itself.\n\nFrom Parent 0 (Adaptive Hinge-Sigmoid Hybrid Loss), we inherit the core structure of a smooth hinge loss, `softplus(margin - logp_diff)`, which penalizes the model only when its preference `logp_diff` is insufficient. We also inherit its use of a bounded, standardized rank gap `tanh(zscore(cost_gap_signed))` as a key signal.\n\nFrom Parent 1 (Rank-Calibrated Dynamic Hinge Loss), we inherit the idea of using the raw, un-normalized rank gap (`cost_gap_signed`) as a direct scaling factor to emphasize pairs with larger cost differences. Instead of scaling the final loss, we use it to modulate the margin.\n\nAs a new coupling idea, we create a 'rank-modulated margin'. The margin is not just a function of the standardized rank gap but is dynamically scaled by the raw rank gap itself: `modulated_margin = beta * tanh(zscore(cost_gap_signed)) * (1 + gamma * abs(cost_gap_signed))`. This makes the required preference margin much larger for pairs with significant cost differences, forcing the model to be more decisive on easy-to-rank pairs, while keeping the margin small for pairs with similar costs.\n\nAs a second new idea, we embed the resulting hinge error term `softplus(modulated_margin - logp_diff)` inside a `logsigmoid` function. The final loss is `-logsigmoid(hinge_error)`. This transforms the non-negative hinge error into a probabilistic loss. When the hinge error is zero (correct preference), `logsigmoid(0)` is `log(0.5)`, a small constant loss. As the error grows, the loss increases towards infinity, creating a strong gradient signal for incorrect preferences. This combines the 'zero-error for correctness' property of a hinge loss with the probabilistic interpretation of a sigmoid-based loss.\n\nWhen `cost(a) < cost(b)`, `cost_gap_signed` is negative, making `modulated_margin` negative. The loss encourages `logp_diff` to be positive and larger than this margin to drive `hinge_error` to zero, thus correctly preferring `a`.", "pseudocode": "1. Compute the log-probability difference: logp_diff = logp_a - logp_b.\n2. Compute the signed, batch-relative rank gap: cost_gap_signed = rank_gap(cost_a, cost_b).\n3. Standardize the rank gap across the batch: cost_gap_zscored = zscore(cost_gap_signed).\n4. Create a bounded base margin from the standardized rank gap (inspired by Parent 0): base_margin_signal = tanh(cost_gap_zscored).\n5. Create a rank-modulated margin by scaling the base signal with the raw rank gap (new coupling idea, inspired by Parent 1's scaling): modulated_margin = beta * base_margin_signal * (1.0 + gamma * abs(cost_gap_signed)).\n6. Compute the smooth hinge preference error using the modulated margin (structure from Parent 0): hinge_error = softplus(modulated_margin - logp_diff).\n7. Compute the final loss by embedding the hinge error within a logsigmoid function (new coupling idea): loss = -logsigmoid(hinge_error).", "hyperparams": {"beta": 1.0, "gamma": 0.1}, "operators_used": ["rank_gap", "zscore", "tanh", "softplus", "logsigmoid"], "implementation_hint": {"expects": ["cost_a", "cost_b", "logp_a", "logp_b"], "returns": "scalar"}}, "fitness": {"hf_like_score": 8.489141464233398, "validation_objective": 8.489141464233398, "generalization_penalty": 0.0, "generalization_objectives": {"20": 4.0068676471710205}, "train_score_mean": 10.500226068496705, "train_loss_mean": 0.5994988775253296, "pair_count": 12902388, "config": {"hf": {"problem": "tsp", "hf_steps": 100, "train_problem_size": 100, "valid_problem_sizes": [20], "train_batch_size": 64, "pomo_size": 64, "learning_rate": 0.0003, "weight_decay": 1e-06, "alpha": 0.05, "device": "cuda", "seed": 1234, "num_validation_episodes": 128, "validation_batch_size": 64, "generalization_penalty_weight": 1.0, "pool_version": "v0"}, "free_loss": {"f1_steps": 100, "f2_steps": 100, "f3_enabled": false}}, "loss_ir": {"name": "Rank-Modulated Hinge-LogSigmoid Loss", "intuition": "This child loss creates a hybrid preference model by blending a dynamic hinge mechanism with a probabilistic logsigmoid framework, and introduces a new coupling mechanism that uses the rank gap to modulate the hinge margin itself.\n\nFrom Parent 0 (Adaptive Hinge-Sigmoid Hybrid Loss), we inherit the core structure of a smooth hinge loss, `softplus(margin - logp_diff)`, which penalizes the model only when its preference `logp_diff` is insufficient. We also inherit its use of a bounded, standardized rank gap `tanh(zscore(cost_gap_signed))` as a key signal.\n\nFrom Parent 1 (Rank-Calibrated Dynamic Hinge Loss), we inherit the idea of using the raw, un-normalized rank gap (`cost_gap_signed`) as a direct scaling factor to emphasize pairs with larger cost differences. Instead of scaling the final loss, we use it to modulate the margin.\n\nAs a new coupling idea, we create a 'rank-modulated margin'. The margin is not just a function of the standardized rank gap but is dynamically scaled by the raw rank gap itself: `modulated_margin = beta * tanh(zscore(cost_gap_signed)) * (1 + gamma * abs(cost_gap_signed))`. This makes the required preference margin much larger for pairs with significant cost differences, forcing the model to be more decisive on easy-to-rank pairs, while keeping the margin small for pairs with similar costs.\n\nAs a second new idea, we embed the resulting hinge error term `softplus(modulated_margin - logp_diff)` inside a `logsigmoid` function. The final loss is `-logsigmoid(hinge_error)`. This transforms the non-negative hinge error into a probabilistic loss. When the hinge error is zero (correct preference), `logsigmoid(0)` is `log(0.5)`, a small constant loss. As the error grows, the loss increases towards infinity, creating a strong gradient signal for incorrect preferences. This combines the 'zero-error for correctness' property of a hinge loss with the probabilistic interpretation of a sigmoid-based loss.\n\nWhen `cost(a) < cost(b)`, `cost_gap_signed` is negative, making `modulated_margin` negative. The loss encourages `logp_diff` to be positive and larger than this margin to drive `hinge_error` to zero, thus correctly preferring `a`.", "hyperparams": {"beta": 1.0, "gamma": 0.1}, "operators_used": ["rank_gap", "zscore", "tanh", "softplus", "logsigmoid"]}}, "better_than_baseline": false}
{"generation": 9, "index": 6, "ir": {"name": "Rank-Modulated Hinge-Sigmoid Loss", "intuition": "This child loss function creates a hybrid preference learning objective by blending the dynamic margin concept from one parent with an adaptive temperature from the other, and introduces a new coupling mechanism that uses the rank gap to directly modulate the preference error before final scaling.\n\nFrom Parent 0 (Adaptive Hinge-Sigmoid Hybrid Loss), we inherit the core structure of a smooth hinge loss based on a dynamic, rank-sensitive margin. Specifically, we use `preference_error = softplus(dynamic_margin - logp_diff)` where `dynamic_margin = beta * tanh(zscore(rank_gap(...)))`. This provides a non-negative error signal that is only active when the model's preference `logp_diff` does not sufficiently exceed the target margin.\n\nFrom Parent 1 (Rank-Calibrated Dynamic Hinge Loss), we inherit the idea of an adaptive temperature, `adaptive_temperature = 1.0 + gamma * sigmoid(abs(dynamic_margin))`. This makes the loss more responsive to pairs with a clearer preference signal (i.e., a larger standardized rank gap), adding a layer of calibration to the loss magnitude.\n\nAs a new coupling idea, we introduce a 'rank-gap modulation' factor. We directly modulate the `preference_error` by `relu(1.0 + cost_gap_signed)`. The intuition is to amplify the error for pairs where `cost(a) < cost(b)` (i.e., `cost_gap_signed` is negative) and dampen it for pairs where `cost(a) > cost(b)` (`cost_gap_signed` is positive). The `relu` ensures this modulation factor is non-negative. This couples the raw directional information of the cost gap with the preference error, effectively creating an asymmetric penalty. This modulated error is then combined with the adaptive temperature in a stable logarithmic form.\n\nWhen `cost(a) < cost(b)`, `cost_gap_signed` is negative. The loss encourages `logp_diff` to be positive and larger than the negative `dynamic_margin` to zero out the `preference_error`. The modulation factor `relu(1.0 + cost_gap_signed)` is less than 1, slightly reducing the penalty, reflecting that this is the 'correct' preference direction to learn.", "pseudocode": "1. Compute the log-probability difference: logp_diff = logp_a - logp_b.\n2. Compute the signed, batch-relative rank gap: cost_gap_signed = rank_gap(cost_a, cost_b).\n3. Standardize the rank gap across the batch: cost_gap_zscored = zscore(cost_gap_signed).\n4. Create a bounded dynamic margin from the standardized rank gap (inherited from Parent 0): dynamic_margin = beta * tanh(cost_gap_zscored).\n5. Compute a smooth hinge-like preference error (structure from Parent 0): preference_error = softplus(dynamic_margin - logp_diff).\n6. Create a rank-based modulation factor (new coupling idea): rank_modulation = relu(1.0 + cost_gap_signed).\n7. Apply the modulation to the preference error (new coupling idea): modulated_error = rank_modulation * preference_error.\n8. Create an adaptive temperature based on the margin's magnitude (inherited from Parent 1): adaptive_temperature = 1.0 + gamma * sigmoid(abs(dynamic_margin)).\n9. Combine the modulated error and adaptive temperature in a stable logarithmic form (structure from both parents): loss = log(1.0 + adaptive_temperature * modulated_error).", "hyperparams": {"beta": 1.0, "gamma": 0.75}, "operators_used": ["rank_gap", "zscore", "tanh", "softplus", "relu", "sigmoid", "log"], "implementation_hint": {"expects": ["cost_a", "cost_b", "logp_a", "logp_b"], "returns": "scalar"}}, "fitness": {"hf_like_score": 8.470609664916992, "validation_objective": 8.470609664916992, "generalization_penalty": 0.0, "generalization_objectives": {"20": 3.992692708969116}, "train_score_mean": 10.49932469367981, "train_loss_mean": 0.6008035588264465, "pair_count": 12902390, "config": {"hf": {"problem": "tsp", "hf_steps": 100, "train_problem_size": 100, "valid_problem_sizes": [20], "train_batch_size": 64, "pomo_size": 64, "learning_rate": 0.0003, "weight_decay": 1e-06, "alpha": 0.05, "device": "cuda", "seed": 1234, "num_validation_episodes": 128, "validation_batch_size": 64, "generalization_penalty_weight": 1.0, "pool_version": "v0"}, "free_loss": {"f1_steps": 100, "f2_steps": 100, "f3_enabled": false}}, "loss_ir": {"name": "Rank-Modulated Hinge-Sigmoid Loss", "intuition": "This child loss function creates a hybrid preference learning objective by blending the dynamic margin concept from one parent with an adaptive temperature from the other, and introduces a new coupling mechanism that uses the rank gap to directly modulate the preference error before final scaling.\n\nFrom Parent 0 (Adaptive Hinge-Sigmoid Hybrid Loss), we inherit the core structure of a smooth hinge loss based on a dynamic, rank-sensitive margin. Specifically, we use `preference_error = softplus(dynamic_margin - logp_diff)` where `dynamic_margin = beta * tanh(zscore(rank_gap(...)))`. This provides a non-negative error signal that is only active when the model's preference `logp_diff` does not sufficiently exceed the target margin.\n\nFrom Parent 1 (Rank-Calibrated Dynamic Hinge Loss), we inherit the idea of an adaptive temperature, `adaptive_temperature = 1.0 + gamma * sigmoid(abs(dynamic_margin))`. This makes the loss more responsive to pairs with a clearer preference signal (i.e., a larger standardized rank gap), adding a layer of calibration to the loss magnitude.\n\nAs a new coupling idea, we introduce a 'rank-gap modulation' factor. We directly modulate the `preference_error` by `relu(1.0 + cost_gap_signed)`. The intuition is to amplify the error for pairs where `cost(a) < cost(b)` (i.e., `cost_gap_signed` is negative) and dampen it for pairs where `cost(a) > cost(b)` (`cost_gap_signed` is positive). The `relu` ensures this modulation factor is non-negative. This couples the raw directional information of the cost gap with the preference error, effectively creating an asymmetric penalty. This modulated error is then combined with the adaptive temperature in a stable logarithmic form.\n\nWhen `cost(a) < cost(b)`, `cost_gap_signed` is negative. The loss encourages `logp_diff` to be positive and larger than the negative `dynamic_margin` to zero out the `preference_error`. The modulation factor `relu(1.0 + cost_gap_signed)` is less than 1, slightly reducing the penalty, reflecting that this is the 'correct' preference direction to learn.", "hyperparams": {"beta": 1.0, "gamma": 0.75}, "operators_used": ["rank_gap", "zscore", "tanh", "softplus", "relu", "sigmoid", "log"]}}, "better_than_baseline": false}
{"generation": 9, "index": 7, "ir": {"name": "Rank-Modulated Hinge-Sigmoid Loss", "intuition": "This loss function creates a hybrid preference model by combining a dynamic hinge mechanism with a rank-modulated error term, introducing a new coupling that directly links the raw cost signal to the preference error.\n\nFrom Parent 0 (Adaptive Hinge-Sigmoid Hybrid Loss), we inherit the core structure of a dynamic, rank-based margin. This `dynamic_margin = beta * tanh(zscore(rank_gap(...)))` creates a target for the log-probability difference that is sensitive to the relative ranking of a pair within a batch. We also inherit the use of `softplus` to form a smooth, non-negative hinge-like preference error: `preference_error = softplus(dynamic_margin - logp_diff)`.\n\nFrom Parent 1 (Rank-Calibrated Dynamic Hinge Loss), we inherit the idea of directly modulating the loss based on the magnitude of the cost difference. However, instead of using a separate scaling factor, we integrate this idea differently.\n\nAs a new coupling idea, we introduce a 'rank-modulated temperature' (`rank_temp`) that scales the argument *inside* the `softplus` function. This temperature, `rank_temp = exp(gamma * clamp(zscore(abs(cost_gap_signed)), min=-3, max=3))`, is derived from the standardized absolute rank gap. This makes the hinge boundary 'sharper' for pairs with a large rank difference and 'softer' for pairs with a small rank difference. By applying this scaling inside the `softplus`, we directly influence the gradient of the preference error itself, coupling the rank signal more tightly with the core preference calculation. A second new idea is using a `logsigmoid` function on the final scaled error (`logsigmoid(scaled_error)`), which provides a stable, probabilistically-grounded loss that is naturally bounded and avoids the `log(1+x)` structure of the parents.\n\nWhen `cost(a) < cost(b)`, `dynamic_margin` is negative. The loss encourages `logp_diff` to be positive and larger than this margin to drive `preference_error` to zero, correctly preferring `a`.", "pseudocode": "1. Compute the log-probability difference: logp_diff = logp_a - logp_b.\n2. Compute the signed, batch-relative rank gap: cost_gap_signed = rank_gap(cost_a, cost_b).\n3. Standardize the signed rank gap across the batch: cost_gap_zscored = zscore(cost_gap_signed).\n4. Create a bounded dynamic margin from the standardized rank gap (inherited from Parent 0): dynamic_margin = beta * tanh(cost_gap_zscored).\n5. Create a rank-modulated temperature from the standardized absolute rank gap (new coupling idea): rank_temp = exp(gamma * clamp(zscore(abs(cost_gap_signed)), min=-3, max=3)).\n6. Compute the preference error using a smooth hinge-like term, scaled by the rank temperature (new coupling idea): preference_error = softplus(rank_temp * (dynamic_margin - logp_diff)).\n7. Calculate the final loss using a stable logsigmoid function (new structural idea): loss = -logsigmoid(-preference_error).", "hyperparams": {"beta": 1.0, "gamma": 0.25}, "operators_used": ["rank_gap", "zscore", "tanh", "exp", "clamp", "softplus", "logsigmoid"], "implementation_hint": {"expects": ["cost_a", "cost_b", "logp_a", "logp_b"], "returns": "scalar"}}, "fitness": {"hf_like_score": 8.444458961486816, "validation_objective": 8.444458961486816, "generalization_penalty": 0.0, "generalization_objectives": {"20": 4.033019065856934}, "train_score_mean": 10.47455062866211, "train_loss_mean": 0.5994716215133667, "pair_count": 12902388, "config": {"hf": {"problem": "tsp", "hf_steps": 100, "train_problem_size": 100, "valid_problem_sizes": [20], "train_batch_size": 64, "pomo_size": 64, "learning_rate": 0.0003, "weight_decay": 1e-06, "alpha": 0.05, "device": "cuda", "seed": 1234, "num_validation_episodes": 128, "validation_batch_size": 64, "generalization_penalty_weight": 1.0, "pool_version": "v0"}, "free_loss": {"f1_steps": 100, "f2_steps": 100, "f3_enabled": false}}, "loss_ir": {"name": "Rank-Modulated Hinge-Sigmoid Loss", "intuition": "This loss function creates a hybrid preference model by combining a dynamic hinge mechanism with a rank-modulated error term, introducing a new coupling that directly links the raw cost signal to the preference error.\n\nFrom Parent 0 (Adaptive Hinge-Sigmoid Hybrid Loss), we inherit the core structure of a dynamic, rank-based margin. This `dynamic_margin = beta * tanh(zscore(rank_gap(...)))` creates a target for the log-probability difference that is sensitive to the relative ranking of a pair within a batch. We also inherit the use of `softplus` to form a smooth, non-negative hinge-like preference error: `preference_error = softplus(dynamic_margin - logp_diff)`.\n\nFrom Parent 1 (Rank-Calibrated Dynamic Hinge Loss), we inherit the idea of directly modulating the loss based on the magnitude of the cost difference. However, instead of using a separate scaling factor, we integrate this idea differently.\n\nAs a new coupling idea, we introduce a 'rank-modulated temperature' (`rank_temp`) that scales the argument *inside* the `softplus` function. This temperature, `rank_temp = exp(gamma * clamp(zscore(abs(cost_gap_signed)), min=-3, max=3))`, is derived from the standardized absolute rank gap. This makes the hinge boundary 'sharper' for pairs with a large rank difference and 'softer' for pairs with a small rank difference. By applying this scaling inside the `softplus`, we directly influence the gradient of the preference error itself, coupling the rank signal more tightly with the core preference calculation. A second new idea is using a `logsigmoid` function on the final scaled error (`logsigmoid(scaled_error)`), which provides a stable, probabilistically-grounded loss that is naturally bounded and avoids the `log(1+x)` structure of the parents.\n\nWhen `cost(a) < cost(b)`, `dynamic_margin` is negative. The loss encourages `logp_diff` to be positive and larger than this margin to drive `preference_error` to zero, correctly preferring `a`.", "hyperparams": {"beta": 1.0, "gamma": 0.25}, "operators_used": ["rank_gap", "zscore", "tanh", "exp", "clamp", "softplus", "logsigmoid"]}}, "better_than_baseline": true}
{"generation": 9, "index": 8, "ir": {"name": "Adaptive Margin-Gated Sigmoid Loss", "intuition": "This loss function creates a novel hybrid that gates a sigmoid loss with a dynamic, rank-based margin. It aims to combine the probabilistic interpretation of a sigmoid loss with the explicit margin enforcement of a hinge loss, while introducing a new gating mechanism.\n\nFrom Parent 0 (Adaptive Hinge-Sigmoid Hybrid Loss), we inherit the core idea of a `dynamic_margin` derived from a standardized rank gap: `beta * tanh(zscore(rank_gap(...)))`. This ensures the loss is sensitive to the relative rank of a pair within the batch.\n\nFrom Parent 1 (Rank-Calibrated Dynamic Hinge Loss), we inherit the structural use of a sigmoid function to scale a term, but instead of using it for temperature, we use it as the core loss component, similar to traditional DPO-style losses. We compute a sigmoid loss on the preference difference: `logsigmoid(-logp_diff)`.\n\nAs a new coupling idea, we introduce a 'margin gate'. This gate, `gate = sigmoid(gamma * (logp_diff - dynamic_margin))`, measures how well the model's preference `logp_diff` satisfies the `dynamic_margin`. The gate value approaches 1 when the margin is satisfied and 0 when it is violated. The final loss is the product of this gate and the standard sigmoid loss: `gate * logsigmoid(-logp_diff)`. This gating mechanism has two effects: (1) It smoothly attenuates the loss to zero as the model's preference `logp_diff` comfortably exceeds the required `dynamic_margin`, effectively combining the 'zero-loss for correct preference' property of a hinge loss with the probabilistic nature of a sigmoid loss. (2) It focuses the gradient on pairs where the preference is close to or violates the margin, making training more efficient.\n\nWhen `cost(a) < cost(b)`, `dynamic_margin` is negative. The loss encourages `logp_diff` to be positive. As `logp_diff` becomes larger than the negative `dynamic_margin`, the `gate` approaches 1, allowing the full `logsigmoid(-logp_diff)` loss to be applied, pushing `logp_diff` even higher. If `logp_diff` is much smaller than the margin (i.e., wrong preference), the gate approaches 0, reducing the loss contribution and preventing excessively large gradients from unstable pairs.", "pseudocode": "1. Compute the log-probability difference: logp_diff = logp_a - logp_b.\n2. Compute the signed, batch-relative rank gap: cost_gap_signed = rank_gap(cost_a, cost_b).\n3. Standardize the rank gap across the batch: cost_gap_zscored = zscore(cost_gap_signed).\n4. Create a bounded dynamic margin from the standardized rank gap (inherited from Parent 0): dynamic_margin = beta * tanh(cost_gap_zscored).\n5. Compute the core sigmoid-based preference loss (structure inspired by Parent 1): base_loss = logsigmoid(-logp_diff).\n6. Compute the margin satisfaction gate (new coupling idea): gate = sigmoid(gamma * (logp_diff - dynamic_margin)).\n7. Apply the gate to the base loss to get the final loss (new coupling idea): loss = gate * base_loss.", "hyperparams": {"beta": 1.0, "gamma": 2.0}, "operators_used": ["rank_gap", "zscore", "tanh", "logsigmoid", "sigmoid"], "implementation_hint": {"expects": ["cost_a", "cost_b", "logp_a", "logp_b"], "returns": "scalar"}}, "fitness": {"hf_like_score": 8.462986946105957, "validation_objective": 8.462986946105957, "generalization_penalty": 0.0, "generalization_objectives": {"20": 3.9830102920532227}, "train_score_mean": 10.495475549697876, "train_loss_mean": 0.6008468741178512, "pair_count": 12902386, "config": {"hf": {"problem": "tsp", "hf_steps": 100, "train_problem_size": 100, "valid_problem_sizes": [20], "train_batch_size": 64, "pomo_size": 64, "learning_rate": 0.0003, "weight_decay": 1e-06, "alpha": 0.05, "device": "cuda", "seed": 1234, "num_validation_episodes": 128, "validation_batch_size": 64, "generalization_penalty_weight": 1.0, "pool_version": "v0"}, "free_loss": {"f1_steps": 100, "f2_steps": 100, "f3_enabled": false}}, "loss_ir": {"name": "Adaptive Margin-Gated Sigmoid Loss", "intuition": "This loss function creates a novel hybrid that gates a sigmoid loss with a dynamic, rank-based margin. It aims to combine the probabilistic interpretation of a sigmoid loss with the explicit margin enforcement of a hinge loss, while introducing a new gating mechanism.\n\nFrom Parent 0 (Adaptive Hinge-Sigmoid Hybrid Loss), we inherit the core idea of a `dynamic_margin` derived from a standardized rank gap: `beta * tanh(zscore(rank_gap(...)))`. This ensures the loss is sensitive to the relative rank of a pair within the batch.\n\nFrom Parent 1 (Rank-Calibrated Dynamic Hinge Loss), we inherit the structural use of a sigmoid function to scale a term, but instead of using it for temperature, we use it as the core loss component, similar to traditional DPO-style losses. We compute a sigmoid loss on the preference difference: `logsigmoid(-logp_diff)`.\n\nAs a new coupling idea, we introduce a 'margin gate'. This gate, `gate = sigmoid(gamma * (logp_diff - dynamic_margin))`, measures how well the model's preference `logp_diff` satisfies the `dynamic_margin`. The gate value approaches 1 when the margin is satisfied and 0 when it is violated. The final loss is the product of this gate and the standard sigmoid loss: `gate * logsigmoid(-logp_diff)`. This gating mechanism has two effects: (1) It smoothly attenuates the loss to zero as the model's preference `logp_diff` comfortably exceeds the required `dynamic_margin`, effectively combining the 'zero-loss for correct preference' property of a hinge loss with the probabilistic nature of a sigmoid loss. (2) It focuses the gradient on pairs where the preference is close to or violates the margin, making training more efficient.\n\nWhen `cost(a) < cost(b)`, `dynamic_margin` is negative. The loss encourages `logp_diff` to be positive. As `logp_diff` becomes larger than the negative `dynamic_margin`, the `gate` approaches 1, allowing the full `logsigmoid(-logp_diff)` loss to be applied, pushing `logp_diff` even higher. If `logp_diff` is much smaller than the margin (i.e., wrong preference), the gate approaches 0, reducing the loss contribution and preventing excessively large gradients from unstable pairs.", "hyperparams": {"beta": 1.0, "gamma": 2.0}, "operators_used": ["rank_gap", "zscore", "tanh", "logsigmoid", "sigmoid"]}}, "better_than_baseline": false}
{"generation": 9, "index": 9, "ir": {"name": "Rank-Modulated Adaptive Sigmoid Loss", "intuition": "This loss function creates a hybrid sigmoid-hinge structure by modulating a probabilistic preference term with a rank-based error signal, introducing a new coupling mechanism for stability and adaptivity.\n\nFrom Parent 0 (Adaptive Hinge-Sigmoid Hybrid Loss), we inherit the core idea of an `adaptive_gamma` that scales the loss based on the magnitude of the preference margin. This makes the loss more sensitive to pairs with a clear cost ranking. We also inherit its use of a stable logarithmic form `log(1 + ...)` for the final loss calculation.\n\nFrom Parent 1 (Rank-Calibrated Dynamic Hinge Loss), we inherit the concept of a `dynamic_margin` derived from the standardized rank gap (`beta * tanh(zscore(rank_gap(...)))`). This margin represents the desired preference separation based on the pair's relative cost ranking within the batch.\n\nAs a new coupling idea, we introduce a 'rank-modulated preference term'. Instead of using the log-probability difference `logp_diff` directly, we first calculate a smooth, hinge-like preference error: `preference_error = softplus(dynamic_margin - logp_diff)`. This error is zero if the model's preference `logp_diff` correctly exceeds the `dynamic_margin`. We then use this non-negative error as the argument to a `logsigmoid` function. The core term becomes `logsigmoid(-adaptive_gamma * preference_error)`. This elegantly combines the hinge property (zero loss for correct preferences) with the probabilistic interpretation of a sigmoid loss. The `adaptive_gamma` scales the 'steepness' of the sigmoid based on the margin's importance, while the `preference_error` ensures the loss only activates when the model's preference is insufficient.\n\nWhen `cost(a) < cost(b)`, the `dynamic_margin` is negative. The loss encourages `logp_diff` to be positive and larger than this margin to drive `preference_error` to zero. When `preference_error` is zero, `logsigmoid(0)` is approximately -0.693, but since we are minimizing the negative of this, the loss for correctly classified pairs approaches a stable, non-zero minimum, while incorrectly classified pairs incur a larger penalty.\n\nThe final loss is the negative of this term, ensuring we are minimizing a positive value.", "pseudocode": "1. Compute the log-probability difference: logp_diff = logp_a - logp_b.\n2. Compute the signed, batch-relative rank gap: cost_gap_signed = rank_gap(cost_a, cost_b).\n3. Standardize the rank gap across the batch: cost_gap_zscored = zscore(cost_gap_signed).\n4. Create a bounded dynamic margin from the standardized rank gap (inherited from Parent 1): dynamic_margin = beta * tanh(cost_gap_zscored).\n5. Create an adaptive temperature based on the margin magnitude (inherited from Parent 0): adaptive_gamma = 1.0 + gamma * abs(dynamic_margin).\n6. Compute a smooth, hinge-like preference error, which is zero if the preference is sufficient (new coupling idea): preference_error = softplus(dynamic_margin - logp_diff).\n7. Apply the adaptive temperature to the preference error within a stable logsigmoid function (new coupling idea): loss_term = logsigmoid(-adaptive_gamma * preference_error).\n8. The final loss is the negative of this term to ensure we minimize a positive value: loss = -loss_term.", "hyperparams": {"beta": 1.0, "gamma": 0.5}, "operators_used": ["rank_gap", "zscore", "tanh", "softplus", "logsigmoid"], "implementation_hint": {"expects": ["cost_a", "cost_b", "logp_a", "logp_b"], "returns": "scalar"}}, "fitness": {"hf_like_score": 8.469285011291504, "validation_objective": 8.469285011291504, "generalization_penalty": 0.0, "generalization_objectives": {"20": 4.007490396499634}, "train_score_mean": 10.533109979629517, "train_loss_mean": 0.6011025166511536, "pair_count": 12902390, "config": {"hf": {"problem": "tsp", "hf_steps": 100, "train_problem_size": 100, "valid_problem_sizes": [20], "train_batch_size": 64, "pomo_size": 64, "learning_rate": 0.0003, "weight_decay": 1e-06, "alpha": 0.05, "device": "cuda", "seed": 1234, "num_validation_episodes": 128, "validation_batch_size": 64, "generalization_penalty_weight": 1.0, "pool_version": "v0"}, "free_loss": {"f1_steps": 100, "f2_steps": 100, "f3_enabled": false}}, "loss_ir": {"name": "Rank-Modulated Adaptive Sigmoid Loss", "intuition": "This loss function creates a hybrid sigmoid-hinge structure by modulating a probabilistic preference term with a rank-based error signal, introducing a new coupling mechanism for stability and adaptivity.\n\nFrom Parent 0 (Adaptive Hinge-Sigmoid Hybrid Loss), we inherit the core idea of an `adaptive_gamma` that scales the loss based on the magnitude of the preference margin. This makes the loss more sensitive to pairs with a clear cost ranking. We also inherit its use of a stable logarithmic form `log(1 + ...)` for the final loss calculation.\n\nFrom Parent 1 (Rank-Calibrated Dynamic Hinge Loss), we inherit the concept of a `dynamic_margin` derived from the standardized rank gap (`beta * tanh(zscore(rank_gap(...)))`). This margin represents the desired preference separation based on the pair's relative cost ranking within the batch.\n\nAs a new coupling idea, we introduce a 'rank-modulated preference term'. Instead of using the log-probability difference `logp_diff` directly, we first calculate a smooth, hinge-like preference error: `preference_error = softplus(dynamic_margin - logp_diff)`. This error is zero if the model's preference `logp_diff` correctly exceeds the `dynamic_margin`. We then use this non-negative error as the argument to a `logsigmoid` function. The core term becomes `logsigmoid(-adaptive_gamma * preference_error)`. This elegantly combines the hinge property (zero loss for correct preferences) with the probabilistic interpretation of a sigmoid loss. The `adaptive_gamma` scales the 'steepness' of the sigmoid based on the margin's importance, while the `preference_error` ensures the loss only activates when the model's preference is insufficient.\n\nWhen `cost(a) < cost(b)`, the `dynamic_margin` is negative. The loss encourages `logp_diff` to be positive and larger than this margin to drive `preference_error` to zero. When `preference_error` is zero, `logsigmoid(0)` is approximately -0.693, but since we are minimizing the negative of this, the loss for correctly classified pairs approaches a stable, non-zero minimum, while incorrectly classified pairs incur a larger penalty.\n\nThe final loss is the negative of this term, ensuring we are minimizing a positive value.", "hyperparams": {"beta": 1.0, "gamma": 0.5}, "operators_used": ["rank_gap", "zscore", "tanh", "softplus", "logsigmoid"]}}, "better_than_baseline": false}
