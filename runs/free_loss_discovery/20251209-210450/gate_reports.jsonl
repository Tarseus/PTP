{"generation": 0, "index": 0, "ir": {"name": "AdaptiveMarginHingeLoss", "intuition": "This loss function, named Adaptive Margin Hinge Loss, is designed for stable and robust preference learning in combinatorial optimization. Its core idea is to combine a classic hinge loss structure with a dynamically adjusted margin that adapts to the magnitude of the cost difference. When the cost difference is large, the margin should be larger, demanding a more confident preference from the model. Conversely, when the cost difference is small (solutions are of similar quality), the margin shrinks, allowing the model more leeway and preventing over-penalization for minor preference disagreements. This adaptability is achieved by scaling the log-probability difference with a sigmoid-transformed rank gap of the costs. The sigmoid function squashes the cost difference into a (0, 1) range, acting as a stable, adaptive scaling factor. The final softplus ensures the loss is always non-negative and smooth, providing a reliable gradient for backpropagation.", "pseudocode": "def AdaptiveMarginHingeLoss(cost_a, cost_b, logp_a, logp_b, beta, margin):\n    # For a pair (a, b) where cost(a) < cost(b) is preferred.\n    # We want logp(a) > logp(b).\n\n    # 1. Calculate the difference in log probabilities. We want this to be positive.\n    logit_diff = logp_a - logp_b\n\n    # 2. Calculate the normalized rank gap of costs. This will serve as an adaptive weight.\n    # rank_gap ensures the sign is correct (positive if a is better).\n    # sigmoid squashes it to (0, 1) for stable scaling.\n    cost_gap = rank_gap(cost_b, cost_a)  # cost_b > cost_a -> positive gap\n    adaptive_scale = sigmoid(beta * cost_gap)\n\n    # 3. Formulate the core loss term. We penalize when the logit_diff is not sufficiently positive.\n    # The 'target' for logit_diff is a margin scaled by how much better solution 'a' is.\n    # The expression inside softplus is: margin * adaptive_scale - logit_diff\n    # This means we want: logit_diff > margin * adaptive_scale\n    loss_argument = margin * adaptive_scale - logit_diff\n\n    # 4. Apply softplus to create a smooth, non-negative hinge-like loss.\n    # softplus(x) = log(1 + exp(x)). It's a smooth version of relu(x).\n    # If logit_diff is much larger than the target margin, loss_argument is very negative, and softplus approaches 0.\n    # If logit_diff is smaller than the target margin, loss_argument is positive, and loss is > 0.\n    loss = softplus(loss_argument)\n\n    return loss", "hyperparams": {"beta": 1.0, "margin": 0.5}, "operators_used": ["rank_gap", "sigmoid", "softplus"], "implementation_hint": {"expects": ["cost_a", "cost_b", "logp_a", "logp_b"], "returns": "A scalar loss value (mean of losses over the batch)."}}, "static_ok": false, "static_reason": "implementation_hint.returns must be 'scalar'."}
{"generation": 0, "index": 1, "ir": {"name": "Rank-Aware Sigmoid Hinge Loss (RASH Loss)", "intuition": "This loss function combines the ideas of hinge loss and logistic loss, but with a dynamically scaled margin and target. The core idea is to make the learning signal proportional to the 'rank gap' between the costs of two solutions, while ensuring stability. It uses `tanh` to map the normalized cost difference to a target preference strength in [-1, 1]. A better solution (cost_a < cost_b) will have a target close to +1, while a worse one will have a target close to -1. The `softplus` function then acts like a smoothed hinge/logistic loss, penalizing the model if its predicted preference (`logit_diff`) doesn't align with this target. The `beta` hyperparameter controls the steepness of the `tanh` curve, determining how sensitive the target is to small cost differences. The `alpha` hyperparameter scales the overall loss magnitude.", "pseudocode": "def RASH_Loss(cost_a, cost_b, logit_a, logit_b, alpha, beta):\n  # 1. Calculate the normalized rank gap of costs.\n  # rank_gap ensures that the difference is based on their relative ordering, not absolute values.\n  # Using tanh squashes the gap into a stable [-1, 1] range, acting as a preference target.\n  cost_target = tanh(beta * rank_gap(cost_a, cost_b))\n\n  # 2. Calculate the difference in model's log probabilities (logits).\n  logit_diff = logit_a - logit_b\n\n  # 3. Compute the core loss using softplus.\n  # The structure is softplus(-y * f(x)), similar to logistic loss, where y is the target and f(x) is the prediction.\n  # Here, y is `cost_target` and f(x) is `logit_diff`.\n  # If cost_a < cost_b, cost_target is negative. The loss becomes softplus(-(-ve) * logit_diff) = softplus(logit_diff).\n  # This penalizes negative logit_diff (i.e., logit_a < logit_b), pushing logit_a to be larger.\n  # If cost_a > cost_b, cost_target is positive. The loss becomes softplus(-(ve) * logit_diff) = softplus(-logit_diff).\n  # This penalizes positive logit_diff (i.e., logit_a > logit_b), pushing logit_a to be smaller.\n  loss = softplus(-cost_target * logit_diff)\n\n  # 4. Scale the final loss.\n  return alpha * loss", "hyperparams": {"alpha": 1.0, "beta": 5.0}, "operators_used": ["rank_gap", "tanh", "softplus"], "implementation_hint": {"expects": ["pair_costs_a", "pair_costs_b", "pair_logits_a", "pair_logits_b"], "returns": "scalar_loss (averaged over the batch)"}}, "static_ok": false, "static_reason": "implementation_hint.returns must be 'scalar'."}
{"generation": 0, "index": 2, "ir": {"name": "SigmoidAttenuatedRankGapLoss", "intuition": "This loss function uses the rank gap of costs to define a target preference margin, which is then attenuated by a sigmoid function. The core idea is to create a 'soft hinge' loss. When the model's preference (logit difference) aligns with the cost-based preference and exceeds a dynamic, cost-difference-dependent margin, the loss approaches zero. When it misaligns, the loss increases, pushing the model to correct its preference. The sigmoid attenuation ensures that the influence of the cost difference is bounded and smooth, preventing extreme cost gaps from creating excessively large loss values or gradients, thus enhancing numerical stability. The loss is asymmetric: correctly predicting a large-gap preference is rewarded more (lower loss) than correctly predicting a small-gap one, encouraging the model to focus on significant improvements.", "pseudocode": "def SigmoidAttenuatedRankGapLoss(cost_a, cost_b, logp_a, logp_b, beta, tau):\n  # Determine which solution is preferred based on cost\n  if cost_a < cost_b:\n    # a is preferred (winner), b is unpreferred (loser)\n    cost_w, cost_l = cost_a, cost_b\n    logp_w, logp_l = logp_a, logp_b\n  else:\n    # b is preferred (winner), a is unpreferred (loser)\n    cost_w, cost_l = cost_b, cost_a\n    logp_w, logp_l = logp_b, logp_a\n\n  # Calculate the difference in log probabilities\n  logit_diff = logp_w - logp_l\n\n  # Calculate a normalized, bounded margin based on the rank gap of costs\n  # The rank_gap is positive since cost_l > cost_w\n  # The sigmoid scales the gap to a (0, 1) range, which is then scaled by beta\n  target_margin = beta * sigmoid(rank_gap(cost_l, cost_w) / tau)\n\n  # The loss is the softplus of the margin minus the logit difference.\n  # This is a smooth version of max(0, margin - logit_diff).\n  # If logit_diff > margin (correct preference), loss is small.\n  # If logit_diff < margin (incorrect or insufficient preference), loss is large.\n  loss = softplus(target_margin - logit_diff)\n\n  return loss", "hyperparams": {"beta": {"value": 5.0, "description": "Scales the maximum target margin. A higher beta creates a stronger push for the model to separate the logits of preferred and unpreferred solutions."}, "tau": {"value": 1.0, "description": "Temperature parameter for the sigmoid function. It controls the sensitivity to the cost gap. A smaller tau makes the margin transition from 0 to beta more sharply as the cost gap increases."}}, "operators_used": ["rank_gap", "sigmoid", "softplus"], "implementation_hint": {"expects": ["cost_a", "cost_b", "logp_a", "logp_b"], "returns": "A single scalar loss value for the pair, which is differentiable."}}, "static_ok": false, "static_reason": "implementation_hint.returns must be 'scalar'."}
{"generation": 0, "index": 3, "ir": {"name": "AdaptiveMarginHingeLoss", "intuition": "The core idea is to create a 'hinge-like' loss where the margin is not fixed, but dynamically adapts to the magnitude of the cost difference between two solutions. When the cost difference is large, the model should be more confident in its preference (i.e., have a larger logit difference). When the cost difference is small, a smaller logit difference is acceptable. We use `tanh` to map the normalized cost difference to a bounded margin in [-1, 1], preventing extreme cost gaps from creating excessively large loss targets. The `softplus` function is then used to create a smooth, one-sided penalty, similar to a hinge loss, which only penalizes the model when its logit difference `logp(a) - logp(b)` is not sufficiently aligned with the preference implied by the costs. The `beta` hyperparameter controls the sensitivity of the loss to the logit difference.", "pseudocode": "def AdaptiveMarginHingeLoss(cost_a, cost_b, logp_a, logp_b, alpha, beta):\n    # For a batch of pairs, normalize the cost difference to prevent scale issues.\n    # Using rank_gap is robust to outliers.\n    cost_diff_normalized = rank_gap(cost_a, cost_b)\n\n    # Create a dynamic margin based on the normalized cost difference.\n    # tanh maps the difference to a bounded range [-1, 1].\n    # alpha scales the influence of the cost difference on the margin.\n    # If cost_a < cost_b, cost_diff_normalized is negative, so margin is negative.\n    adaptive_margin = tanh(alpha * cost_diff_normalized)\n\n    # Calculate the log probability difference.\n    log_prob_diff = logp_a - logp_b\n\n    # The loss penalizes when the log_prob_diff doesn't 'exceed' the adaptive_margin.\n    # If cost_a < cost_b (preferred), margin is negative. We want log_prob_diff to be positive.\n    # The term inside softplus becomes `margin - log_prob_diff`.\n    # If log_prob_diff > margin, the argument is negative, softplus is near zero (low loss).\n    # If log_prob_diff < margin, the argument is positive, softplus is > 0 (high loss).\n    # beta scales the overall loss penalty.\n    loss = softplus(adaptive_margin - log_prob_diff) * beta\n\n    return loss", "hyperparams": {"alpha": 5.0, "beta": 1.0}, "operators_used": ["rank_gap", "tanh", "softplus"], "implementation_hint": {"expects": ["pair_cost_a", "pair_cost_b", "pair_logp_a", "pair_logp_b"], "returns": "A scalar loss value (mean over the batch)."}}, "static_ok": false, "static_reason": "implementation_hint.returns must be 'scalar'."}
{"generation": 1, "index": 0, "ir": {"name": "AdaptiveMarginHingeLoss", "intuition": "This loss function, named 'Adaptive Margin Hinge Loss', is designed for stable preference learning in combinatorial optimization. Its core idea is to create a dynamic, cost-difference-aware margin that the model's log-probability difference must overcome. The hinge loss structure (ReLU) penalizes the model only when its preference contradicts the ground truth cost, i.e., when the better solution 'a' (cost_a < cost_b) is assigned a lower log-probability than 'b'. The margin itself is a non-linear, bounded function of the cost difference, constructed using `tanh` and `softplus`. This ensures that a larger cost difference demands a stronger preference signal from the model, but this demand saturates gracefully to prevent extreme cost gaps from causing exploding gradients. The `tanh` function squashes the log-probability difference, making the loss robust to models that are overly confident or unconfident. The overall structure is smooth (using ReLU's subgradient), numerically stable, and consistent with the preference learning objective.", "pseudocode": "def AdaptiveMarginHingeLoss(cost_a, cost_b, logp_a, logp_b, beta, tau, gamma):\n    # Ensure cost_a is the better solution\n    if cost_a > cost_b:\n        cost_a, cost_b = cost_b, cost_a\n        logp_a, logp_b = logp_b, logp_a\n\n    # Calculate log probability difference\n    logp_diff = logp_a - logp_b\n\n    # Calculate normalized cost difference. softplus(cost_b - cost_a) makes it non-negative and smooth.\n    # Dividing by tau controls the sensitivity to cost differences.\n    delta_cost_norm = softplus(cost_b - cost_a) / tau\n\n    # Create an adaptive margin that is a bounded, monotonic function of the cost difference.\n    # The margin grows with delta_cost_norm but is capped by gamma.\n    margin = gamma * tanh(delta_cost_norm)\n\n    # The core loss logic: Penalize only when the model's preference is wrong (logp_diff < 0)\n    # or when the correct preference is not confident enough to overcome the adaptive margin.\n    # The term to be rectified is (margin - beta * tanh(logp_diff)).\n    # tanh(logp_diff) squashes the logit difference, preventing extreme values from dominating the loss.\n    # beta scales the influence of the model's confidence.\n    loss = relu(margin - beta * tanh(logp_diff))\n\n    return loss", "hyperparams": {"beta": 1.0, "tau": 10.0, "gamma": 2.0}, "operators_used": ["softplus", "tanh", "relu"], "implementation_hint": {"expects": ["cost_w", "cost_l", "logp_w", "logp_l"], "returns": "scalar (mean loss over the batch)"}}, "static_ok": false, "static_reason": "implementation_hint.returns must be 'scalar'."}
{"generation": 1, "index": 1, "ir": {"name": "AdaptiveMarginHingeLoss", "intuition": "This loss function, named Adaptive Margin Hinge Loss (AM-Hinge), is designed for stable preference learning in combinatorial optimization. Its core idea is to create an adaptive margin based on the normalized cost difference between two solutions. The loss penalizes the model only when its preference (indicated by logit difference) contradicts the ground-truth cost preference by more than this adaptive margin. Specifically, it uses a clamped `rank_gap` to create a normalized, bounded cost difference `\u0394_cost_norm`. This value then acts as a dynamic margin `m(\u0394_cost)` in a hinge-loss-like structure: `relu(m(\u0394_cost) - logit_diff)`. This design ensures that: (1) Larger cost differences demand a stronger alignment from the model's logits, creating a more significant push for correction. (2) The `relu` function provides a 'zone of indifference' where the model is not penalized if its preference already aligns with the cost, encouraging stability. (3) The `tanh` function used in `rank_gap` and the `clamp` operation on the margin prevent extreme cost values from causing numerical instability, ensuring the loss remains bounded and well-behaved.", "pseudocode": "def AdaptiveMarginHingeLoss(cost_a, cost_b, logp_a, logp_b, beta, tau):\n    # Assume cost_a < cost_b, so (a) is the preferred solution\n    # logit_diff should be positive to reflect this preference\n    logit_diff = logp_a - logp_b\n\n    # 1. Calculate a normalized, bounded cost difference using rank_gap.\n    # rank_gap(x, y) = tanh(beta * (x - y)) results in a value in (-1, 1).\n    # For cost_a < cost_b, this will be negative. We take the absolute value.\n    cost_gap = abs(rank_gap(cost_a, cost_b, beta=beta))\n\n    # 2. Create an adaptive margin from the cost gap.\n    # The margin is scaled by tau and is bounded between [0, tau].\n    # This margin is larger for larger cost differences.\n    adaptive_margin = clamp(cost_gap * tau, min=0.0, max=tau)\n\n    # 3. Apply a hinge-loss structure.\n    # The loss is incurred only if the model's preference (logit_diff)\n    # is smaller than the required adaptive margin. \n    # This penalizes incorrect or insufficiently confident correct preferences.\n    loss = relu(adaptive_margin - logit_diff)\n\n    return loss", "hyperparams": {"beta": 0.1, "tau": 5.0}, "operators_used": ["rank_gap", "clamp", "relu"], "implementation_hint": {"expects": ["cost_w", "cost_l", "logp_w", "logp_l"], "returns": "scalar loss value (non-negative)."}}, "static_ok": false, "static_reason": "implementation_hint.returns must be 'scalar'."}
{"generation": 1, "index": 2, "ir": {"name": "AdaptiveMarginHingeLoss", "intuition": "This loss function, inspired by hinge loss, introduces an adaptive margin that depends on the normalized cost difference between two solutions. The core idea is that when one solution is significantly better than another (large cost difference), the model should be penalized more heavily if it fails to prefer the better solution by a corresponding large margin. We use `tanh` to map the cost difference to a bounded range [-1, 1], creating a normalized 'cost gap'. This gap then dynamically sets the margin for a hinge-like loss. The `softplus` function is used instead of a hard `max(0, ...)` to ensure the loss is smooth and always differentiable. A temperature parameter `tau` controls the steepness of the cost-to-margin mapping, and a `beta` parameter scales the overall influence of the log-probability difference.", "pseudocode": "def AdaptiveMarginHingeLoss(cost_a, cost_b, logp_a, logp_b, tau, beta):\n  # Calculate the difference in log probabilities\n  log_prob_diff = logp_a - logp_b\n\n  # Calculate the normalized cost difference using tanh. This maps any cost diff to [-1, 1].\n  # We use rank_gap to handle potential scaling issues in raw costs.\n  cost_gap = rank_gap(cost_a, cost_b)\n  normalized_margin_target = tanh(cost_gap / tau)\n\n  # The core loss term. We want log_prob_diff to be greater than the adaptive margin.\n  # The loss is `softplus(margin - beta * log_prob_diff)`.\n  # If cost_a < cost_b, margin_target is negative, so we want log_prob_diff to be positive.\n  # The loss term becomes softplus(-normalized_margin_target - beta * log_prob_diff).\n  # This penalizes the model if beta * log_prob_diff is not larger than -normalized_margin_target.\n  loss_argument = -normalized_margin_target - beta * log_prob_diff\n  loss = softplus(loss_argument)\n\n  return loss", "hyperparams": {"tau": {"description": "Temperature for scaling the cost difference. Smaller tau makes the tanh transition sharper, meaning small cost differences create larger margins.", "default": 1.0}, "beta": {"description": "Scaling factor for the log probability difference, similar to the beta in DPO. Controls the strength of the policy update.", "default": 1.0}}, "operators_used": ["rank_gap", "tanh", "softplus"], "implementation_hint": {"expects": ["cost_a", "cost_b", "logp_a", "logp_b"], "returns": "A scalar loss value (mean over the batch)."}}, "static_ok": false, "static_reason": "implementation_hint.returns must be 'scalar'."}
{"generation": 1, "index": 3, "ir": {"name": "AdaptiveMarginHingeLoss", "intuition": "This loss function, named Adaptive Margin Hinge Loss, is designed for robust preference learning in combinatorial optimization. Its core idea is to combine a classic hinge loss structure with an adaptive margin that is sensitive to the magnitude of the cost difference between two solutions. The hinge loss (relu) ensures that when the model's preference aligns correctly with the ground-truth cost (i.e., the better solution has a higher log-probability), and this alignment is sufficiently confident (exceeds the margin), the loss becomes zero, preventing the model from becoming overconfident on 'easy' pairs. The key innovation is the adaptive margin, calculated as `tanh(scale * \u0394cost)`. By using the hyperbolic tangent (tanh) function, the margin is naturally bounded between 0 and 1. This means that for very small cost differences, the margin is small, requiring only a slight preference from the model. As the cost difference grows, the margin increases, demanding a more confident prediction from the model, but it saturates at 1, preventing the margin from becoming excessively large and causing numerical instability or overly harsh penalties for pairs with huge cost disparities. This saturation property is crucial for stability. The `softplus` function is used as a smooth, non-negative final wrapper, which behaves like a ReLu but is differentiable everywhere, ensuring smooth gradients for optimization.", "pseudocode": "def AdaptiveMarginHingeLoss(cost_a, cost_b, logp_a, logp_b, scale):\n  # Calculate the difference in log-probabilities (model's preference)\n  logit_diff = logp_a - logp_b\n\n  # Calculate the signed, normalized cost difference. We use tanh to bound it.\n  # This serves as our adaptive margin. When cost_a < cost_b, cost_diff_signed is negative.\n  cost_diff_signed = cost_b - cost_a\n  adaptive_margin = tanh(scale * cost_diff_signed)\n\n  # The core hinge loss logic: loss = max(0, margin - logit_diff)\n  # For a preferred pair (a wins, cost_a < cost_b), cost_diff_signed is positive.\n  # We want logit_diff (logp_a - logp_b) to be greater than the positive margin.\n  # The loss term is thus: adaptive_margin - logit_diff.\n  # We use softplus as a smooth version of relu(x), i.e., softplus(x) \u2248 max(0, x).\n  loss = softplus(adaptive_margin - logit_diff)\n\n  return loss", "hyperparams": {"scale": 1.0}, "operators_used": ["tanh", "softplus"], "implementation_hint": {"expects": ["cost_a", "cost_b", "logp_a", "logp_b"], "returns": "A scalar loss value, typically the mean of the losses for all pairs in the batch."}}, "static_ok": false, "static_reason": "implementation_hint.returns must be 'scalar'."}
{"generation": 1, "index": 4, "ir": {"name": "AdaptiveMarginHingeLoss", "intuition": "This loss function, named Adaptive Margin Hinge Loss (AMHL), is inspired by the hinge loss used in Support Vector Machines (SVMs), but with a crucial modification for preference learning. The core idea is to treat the preference `cost(a) < cost(b)` as a classification problem where the model's log probability difference `logp(a) - logp(b)` should be positive. A standard hinge loss would be `max(0, margin - (logp(a) - logp(b)))`. AMHL makes the margin adaptive: the margin is not a fixed constant, but a function of the cost difference `cost(b) - cost(a)`. When the cost difference is large, the model is required to produce a large log probability difference (a large margin) to achieve zero loss. When the cost difference is small, the required margin is also small, allowing the model to be uncertain about nearly-tied solutions. This prevents the model from over-confidently distinguishing between solutions with negligible cost differences. The use of `tanh` to scale the cost difference ensures that the margin is bounded, preventing extreme cost differences from causing an infinitely large loss, thus ensuring numerical stability.", "pseudocode": "def amhl_loss(cost_a, cost_b, logp_a, logp_b, beta, tau):\n  # Ensure cost_b is the higher cost (worse solution)\n  if cost_a > cost_b:\n    cost_a, cost_b = cost_b, cost_a\n    logp_a, logp_b = logp_b, logp_a\n  \n  # logit_diff should be positive if model prefers the better solution 'a'\n  logit_diff = logp_a - logp_b\n  \n  # cost_diff is non-negative, representing the magnitude of preference\n  cost_diff = cost_b - cost_a\n  \n  # Calculate an adaptive margin based on the cost difference.\n  # tanh scales the cost_diff to a bounded range [0, 1].\n  # beta controls the maximum size of the margin.\n  # tau controls the sensitivity to small cost differences.\n  adaptive_margin = beta * tanh(cost_diff / tau)\n  \n  # Apply a hinge-like loss. The loss is positive only when the model's preference\n  # (logit_diff) is smaller than the required adaptive margin.\n  # softplus is a smooth approximation of relu(x) = max(0, x).\n  loss = softplus(adaptive_margin - logit_diff)\n  \n  return loss", "hyperparams": {"beta": {"description": "Controls the maximum margin size. A larger beta enforces a stronger separation for clearly distinct pairs.", "default": 5.0}, "tau": {"description": "Temperature parameter that controls the sensitivity of the margin to the cost difference. A smaller tau makes the margin quickly saturate to beta even for small cost differences.", "default": 1.0}}, "operators_used": ["tanh", "softplus"], "implementation_hint": {"expects": ["cost_w", "cost_l", "logp_w", "logp_l"], "returns": "A single non-negative scalar representing the loss for the pair."}}, "static_ok": false, "static_reason": "implementation_hint.returns must be 'scalar'."}
{"generation": 1, "index": 5, "ir": {"name": "AdaptiveMarginHingeLoss", "intuition": "This loss function is designed to be robust and adaptive. It uses a hinge-like structure, meaning no penalty is applied if the model's preference (logit difference) already aligns with the cost-based preference by a sufficient margin. The key innovation is that this 'sufficient margin' is not a fixed hyperparameter but is dynamically scaled by the normalized cost difference. When the cost difference between two solutions is large, the loss function demands a larger logit difference to be satisfied, pushing the model to be more decisive. Conversely, for solutions with very similar costs, it requires only a small logit difference, preventing the model from over-optimizing on negligible cost variations. The use of `tanh` on the logit difference and `clamp` on the cost difference ensures that extreme input values do not cause numerical instability, and the entire loss is bounded. The `softplus` function is used to create a smooth version of the hinge loss (ReLU), ensuring good gradient flow.", "pseudocode": "def AdaptiveMarginHingeLoss(cost_a, cost_b, logp_a, logp_b, beta, tau):\n    # Preference direction: 1 if a is better, -1 if b is better\n    pref_direction = sign(cost_b - cost_a)\n\n    # Calculate the log probability difference\n    logit_diff = logp_a - logp_b\n\n    # Calculate the normalized cost difference and clamp it to avoid extreme values\n    # rank_gap normalizes the difference, e.g., to [-1, 1]\n    normalized_cost_gap = rank_gap(cost_a, cost_b)\n    clamped_cost_gap = clamp(abs(normalized_cost_gap), 0.0, 1.0)\n\n    # The adaptive margin is proportional to how much better one solution is\n    adaptive_margin = tau * clamped_cost_gap\n\n    # The core argument for the loss function\n    # We want logit_diff * pref_direction to be greater than the adaptive_margin\n    x = adaptive_margin - pref_direction * tanh(logit_diff)\n\n    # Apply a scaled softplus function to create a smooth, non-negative hinge loss\n    # The beta parameter controls the steepness of the loss curve\n    loss = (1/beta) * softplus(beta * x)\n\n    return loss", "hyperparams": {"beta": {"description": "An inverse temperature parameter that controls the steepness of the softplus function, approximating the hinge loss. A larger beta makes it closer to a hard ReLU, while a smaller beta makes it smoother. Default: 1.0", "value": 1.0}, "tau": {"description": "A scaling factor for the adaptive margin. It controls the maximum required margin when the cost difference is significant. Default: 0.5", "value": 0.5}}, "operators_used": ["rank_gap", "clamp", "tanh", "softplus"], "implementation_hint": {"expects": ["pair_cost_a", "pair_cost_b", "pair_logp_a", "pair_logp_b"], "returns": "A scalar representing the mean loss over the batch."}}, "static_ok": false, "static_reason": "implementation_hint.returns must be 'scalar'."}
{"generation": 1, "index": 6, "ir": {"name": "AdaptiveMarginHingeLoss", "intuition": "This loss function, named Adaptive Margin Hinge Loss (AM-Hinge), is designed for stable preference learning in combinatorial optimization. Its core idea is to create an 'adaptive margin' that scales with the magnitude of the cost difference between two solutions. Unlike a fixed-margin hinge loss, this makes the learning signal proportional to how much better one solution is than another. Specifically, it uses a hyperbolic tangent (tanh) function to squash the normalized cost difference (\u0394_cost) into a bounded range, creating a dynamic margin. The loss is then calculated as a hinge-like term (using softplus for smoothness) on the difference between the model's log-probability difference (\u0394_logp) and this adaptive margin. This design ensures that: (1) When a better solution has a lower probability, the penalty is larger if the cost improvement is significant. (2) When a better solution already has a higher probability, the loss quickly drops to near zero, preventing overconfident penalization. (3) The use of tanh and softplus guarantees numerical stability against extreme cost or logit differences, satisfying all constraints.", "pseudocode": "def adaptive_margin_hinge_loss(cost_a, cost_b, logp_a, logp_b, alpha, beta, tau):\n    # Assume cost_a < cost_b without loss of generality (preferred pair)\n    delta_cost = cost_b - cost_a  # This is positive\n    delta_logp = logp_a - logp_b\n\n    # 1. Normalize the cost difference to prevent scale sensitivity\n    # rank_gap is a robust way to get a normalized difference in [0, 1]\n    normalized_delta_cost = rank_gap(cost_a, cost_b)\n\n    # 2. Create a bounded, adaptive margin using tanh\n    # The margin grows with the cost difference but is capped by 'alpha'\n    adaptive_margin = alpha * tanh(normalized_delta_cost / tau)\n\n    # 3. Formulate the core loss term\n    # We want delta_logp to be greater than the adaptive_margin.\n    # The loss is incurred when delta_logp < adaptive_margin.\n    # We use a temperature 'beta' to scale the learning signal.\n    loss_argument = beta * (adaptive_margin - delta_logp)\n\n    # 4. Apply a smooth hinge loss (softplus)\n    # softplus(x) = log(1 + exp(x)). It's a smooth version of relu(x).\n    # This ensures the loss is non-negative and smooth.\n    loss = softplus(loss_argument)\n\n    return loss", "hyperparams": {"alpha": {"value": 2.0, "description": "Maximum margin value. Controls the upper bound of the desired log-probability separation."}, "beta": {"value": 1.0, "description": "Inverse temperature for scaling the loss. Higher beta makes the loss steeper."}, "tau": {"value": 0.1, "description": "Temperature for tanh saturation. Smaller tau makes the margin reach 'alpha' faster with smaller cost differences."}}, "operators_used": ["rank_gap", "tanh", "softplus"], "implementation_hint": {"expects": ["cost_w", "cost_l", "logp_w", "logp_l"], "returns": "scalar_loss"}}, "static_ok": false, "static_reason": "implementation_hint.returns must be 'scalar'."}
{"generation": 1, "index": 7, "ir": {"name": "AdaptiveMarginHingeLoss", "intuition": "This loss function, named Adaptive Margin Hinge Loss, is designed to dynamically adjust the learning signal based on the magnitude of the cost difference between two solutions. It combines the robustness of a hinge loss with a dynamically scaled margin. The core idea is that when the cost difference is large, the model should be penalized more severely for getting the preference wrong, and rewarded more for getting it right. We use `tanh` to map the cost difference into a bounded, non-linear margin, preventing extreme cost gaps from causing excessively large gradients. The `softplus` function is used as a smooth approximation of the hinge loss (`relu`), ensuring the loss is always non-negative and has smooth gradients, which is beneficial for stable training. This structure encourages the model's log probability difference to align with the cost difference, with a stronger push for pairs that have a more significant quality gap.", "pseudocode": "def AdaptiveMarginHingeLoss(cost_a, cost_b, logp_a, logp_b, alpha, beta):\n    # Calculate the difference in log probabilities\n    logit_diff = logp_a - logp_b\n\n    # Calculate the normalized, bounded cost difference as the adaptive margin\n    # rank_gap ensures the sign is correct (positive if a is better)\n    cost_gap = rank_gap(cost_b, cost_a)  # cost_b - cost_a\n    adaptive_margin = alpha * tanh(beta * cost_gap)\n\n    # The core hinge-like structure.\n    # We want logit_diff to be greater than adaptive_margin.\n    # So, the penalty is for when adaptive_margin - logit_diff > 0.\n    loss_argument = adaptive_margin - logit_diff\n\n    # Use softplus for a smooth, non-negative loss.\n    # softplus(x) = log(1 + exp(x)). It's a smooth version of relu(x).\n    loss = softplus(loss_argument)\n\n    return loss", "hyperparams": {"alpha": {"value": 2.0, "description": "Maximum margin magnitude. Controls the overall strength of the preference signal."}, "beta": {"value": 0.1, "description": "Scaling factor for the cost difference. Controls the sensitivity of the margin to the cost gap. A smaller beta makes the margin adapt more slowly."}}, "operators_used": ["rank_gap", "tanh", "softplus"], "implementation_hint": {"expects": ["cost_a", "cost_b", "logp_a", "logp_b"], "returns": "A scalar loss value, typically the mean of the loss over the batch."}}, "static_ok": false, "static_reason": "implementation_hint.returns must be 'scalar'."}
{"generation": 2, "index": 0, "ir": {"name": "AdaptiveMarginHingeLoss", "intuition": "This loss function, named 'Adaptive Margin Hinge Loss', is designed for stable preference learning in combinatorial optimization. Its core idea is to create an adaptive margin based on the normalized cost difference between two solutions. Unlike a fixed-margin hinge loss, this margin dynamically scales with how much better one solution is than the other. When the better solution (lower cost) already has a sufficiently higher log probability than the worse one (exceeding the adaptive margin), the loss becomes zero, indicating no need for further optimization on this pair. Otherwise, the loss is proportional to how much the log probability difference falls short of this target margin. The use of `tanh` on the cost difference and `softplus` on the overall loss ensures numerical stability by bounding the influence of extreme cost gaps and preventing negative loss values, respectively. This approach focuses the model's learning on 'difficult' pairs where the current policy preference contradicts the ground-truth cost preference, while gracefully ignoring pairs that are already correctly ordered.", "pseudocode": "def AdaptiveMarginHingeLoss(cost_a, cost_b, logp_a, logp_b, beta, tau):\n    # Ensure cost_w is the better solution (winner)\n    if cost_a < cost_b:\n        cost_w, cost_l = cost_a, cost_b\n        logp_w, logp_l = logp_a, logp_b\n    else:\n        cost_w, cost_l = cost_b, cost_a\n        logp_w, logp_l = logp_b, logp_a\n\n    # Calculate normalized cost difference and scale it with beta\n    # tanh squashes the difference to (-1, 1), making it stable for extreme cost gaps.\n    # We use (cost_l - cost_w) so the result is positive.\n    # tau acts as a temperature, controlling the sensitivity to the cost difference.\n    cost_diff_normalized = tanh((cost_l - cost_w) / tau)\n\n    # The adaptive margin is proportional to the normalized cost difference.\n    # A larger cost gap demands a larger log probability separation.\n    adaptive_margin = beta * cost_diff_normalized\n\n    # Calculate the log probability difference.\n    logp_diff = logp_w - logp_l\n\n    # The core hinge-like loss mechanism.\n    # We want logp_diff to be greater than the adaptive_margin.\n    # softplus(x) = log(1 + exp(x)), a smooth version of relu(x).\n    # The loss is softplus(margin - logp_diff), which is non-zero only when logp_diff < margin.\n    loss = softplus(adaptive_margin - logp_diff)\n\n    return loss", "hyperparams": {"beta": {"description": "A scaling factor that determines the maximum target margin. It controls how strongly the cost difference influences the desired log probability separation. A larger beta means a larger cost difference will demand a much larger log probability difference.", "default_value": 5.0}, "tau": {"description": "A temperature-like parameter for normalizing the cost difference. It controls the sensitivity of the `tanh` function. A smaller tau makes the function steeper, meaning the margin saturates to `beta` even for small cost differences. A larger tau makes the margin increase more gradually.", "default_value": 1.0}}, "operators_used": ["tanh", "softplus"], "implementation_hint": {"expects": ["cost_w", "cost_l", "logp_w", "logp_l"], "returns": "A scalar loss value, typically the mean of the losses calculated for each pair in the batch."}}, "static_ok": false, "static_reason": "implementation_hint.returns must be 'scalar'."}
{"generation": 2, "index": 1, "ir": {"name": "SigmoidAttenuatedRankGapLoss", "intuition": "This loss function, inspired by contrastive learning principles, aims to align the model's preference (logit difference) with the ground-truth cost difference. The core idea is to create a 'target' preference margin based on the rank-normalized cost difference (\u0394cost_norm). The loss is then the softplus of the discrepancy between this target and the model's current logit difference. A key innovation is the use of a sigmoid function to attenuate the influence of very large logit differences. This prevents the loss from becoming overly aggressive when the model is already very confident (correctly or incorrectly), ensuring smoother gradients and improved numerical stability. The loss penalizes the model when its logit difference doesn't sufficiently reflect the normalized cost gap, pushing it to favor better solutions more strongly, but in a controlled, non-explosive manner.", "pseudocode": "def SigmoidAttenuatedRankGapLoss(cost_a, cost_b, logp_a, logp_b, beta, margin, scale):\n  # 1. Calculate the rank-normalized cost difference.\n  # This maps the cost difference to a [-1, 1] range, focusing on the relative ranking.\n  delta_cost_norm = rank_gap(cost_a, cost_b)\n\n  # 2. Calculate the model's log probability difference.\n  logit_diff = logp_a - logp_b\n\n  # 3. Create a dynamic target margin based on the normalized cost difference.\n  # If cost_a < cost_b, target > 0, pushing logp(a) to be higher than logp(b).\n  # The margin hyperparameter adds a fixed desired separation.\n  target = scale * delta_cost_norm + margin * sign(delta_cost_norm)\n\n  # 4. Calculate the discrepancy between the model's preference and the target.\n  # We want logit_diff to be close to or greater than the target.\n  discrepancy = target - logit_diff\n\n  # 5. Attenuate the discrepancy using a sigmoid function of the logit difference.\n  # When |logit_diff| is large, the model is confident. The sigmoid weight approaches 1,\n  # but its gradient w.r.t. logit_diff approaches 0, preventing extreme updates.\n  # This acts as a soft gradient clipper.\n  attenuation_weight = sigmoid(abs(logit_diff))\n\n  # 6. Compute the final loss using softplus for smoothness and non-negativity.\n  # The loss is a weighted version of the discrepancy, ensuring that the model is penalized\n  # when its preference does not align with the cost-based target.\n  loss = softplus(beta * attenuation_weight * discrepancy)\n\n  return loss", "hyperparams": {"beta": 1.0, "margin": 0.1, "scale": 5.0}, "operators_used": ["rank_gap", "softplus", "sigmoid"], "implementation_hint": {"expects": ["pair_cost_a", "pair_cost_b", "pair_logp_a", "pair_logp_b"], "returns": "A scalar loss value, averaged over the batch."}}, "static_ok": false, "static_reason": "implementation_hint.returns must be 'scalar'."}
{"generation": 2, "index": 2, "ir": {"name": "AdaptiveSigmoidRankLoss", "intuition": "This loss function, named 'Adaptive Sigmoid Rank Loss', is designed to address the challenges of varying cost scales and outlier sensitivity in combinatorial optimization. The core idea is to transform the absolute cost difference into a stable, normalized rank-based gap. This gap then dynamically modulates the margin of a sigmoid-based preference loss. When the cost difference is significant (as determined by its rank within a batch), the loss function imposes a stronger penalty for incorrect preference ordering. Conversely, for negligible cost differences, the penalty is softened, preventing the model from overfitting to noisy or insignificant preferences. The use of `tanh` on the log-probability difference ensures that extreme logit values do not cause numerical instability, effectively bounding the internal activation before it enters the final loss calculation.", "pseudocode": "def AdaptiveSigmoidRankLoss(cost_a, cost_b, logp_a, logp_b, beta, tau):\n    # 1. Calculate the rank-based cost difference. This is more robust to outliers than the raw difference.\n    # rank_gap is assumed to be a batch-wise operation that returns a normalized value, e.g., in [-1, 1].\n    cost_gap = rank_gap(cost_a, cost_b) # Expected to be positive if cost_a < cost_b\n\n    # 2. Calculate the difference in log probabilities.\n    logp_diff = logp_a - logp_b\n\n    # 3. Create an adaptive margin based on the significance of the cost difference.\n    # The margin is larger for more significant cost gaps.\n    adaptive_margin = softplus(cost_gap)\n\n    # 4. Stabilize the log probability difference using tanh to prevent explosion.\n    # This bounds the influence of extreme logits.\n    stable_logp_diff = tanh(logp_diff / tau)\n\n    # 5. Combine the components. The loss aims to make (stable_logp_diff * beta) > adaptive_margin.\n    # We use a logistic loss (logsigmoid) formulation for this comparison.\n    # The argument to logsigmoid is negative because we want to maximize it, which is equivalent to minimizing its negative.\n    loss = -logsigmoid(beta * stable_logp_diff - adaptive_margin)\n\n    return loss", "hyperparams": {"beta": {"value": 5.0, "description": "Scales the influence of the log-probability difference. A higher beta makes the loss more sensitive to the model's preference."}, "tau": {"value": 2.0, "description": "Temperature parameter for tanh stabilization. A higher tau allows for a wider range of logit differences before saturation."}}, "operators_used": ["rank_gap", "softplus", "tanh", "logsigmoid"], "implementation_hint": {"expects": ["cost_a", "cost_b", "logp_a", "logp_b"], "returns": "A scalar loss value, typically the mean of the loss over the batch."}}, "static_ok": false, "static_reason": "implementation_hint.returns must be 'scalar'."}
{"generation": 2, "index": 3, "ir": {"name": "AdaptiveMarginHingeLoss", "intuition": "This loss function, named Adaptive Margin Hinge Loss, is designed for preference learning in combinatorial optimization. Its core idea is to create a dynamic, cost-sensitive margin that the model's preference (logit difference) must overcome. Unlike a fixed-margin hinge loss, this margin adapts to the magnitude of the cost difference between two solutions. When the cost difference is large, the model is strongly penalized for disagreeing with the ground truth preference. Conversely, when the cost difference is small (i.e., the solutions are of similar quality), the loss provides a smaller penalty, allowing the model more flexibility and preventing it from overfitting to noisy or insignificant preferences. The use of `tanh` on the cost gap and `softplus` on the overall expression ensures the loss is smooth, bounded, and numerically stable, preventing issues with extreme input values.", "pseudocode": "1. Calculate the cost difference: delta_cost = cost_b - cost_a.\n2. Normalize and bound the cost difference into a [-1, 1] range to represent the preference strength: cost_gap = tanh(delta_cost / scale).\n3. Calculate the model's preference score: logit_diff = logp_a - logp_b.\n4. Formulate the core loss term. The term 'cost_gap - logit_diff' represents the alignment error. It's positive if the model's preference contradicts the cost-based preference (e.g., cost_a < cost_b but logp_a < logp_b). We want to penalize this misalignment.\n5. Apply the softplus function to create a smooth hinge-like loss. softplus(x) = log(1 + exp(x)), which is a smooth approximation of relu(x). This penalizes only when the argument is positive.\n6. The final loss is softplus(alpha * (cost_gap - logit_diff)). The hyperparameter 'alpha' controls the steepness of the penalty.", "hyperparams": {"alpha": {"default": 5.0, "description": "Steepness parameter. A higher value makes the hinge penalty sharper, more closely resembling a standard hinge loss."}, "scale": {"default": 10.0, "description": "Cost scaling factor. It normalizes the cost difference before it's passed to tanh. This value should be tuned based on the typical range of cost differences in the problem."}}, "operators_used": ["tanh", "softplus"], "implementation_hint": {"expects": ["cost_a", "cost_b", "logp_a", "logp_b"], "returns": "A scalar loss value, typically the mean of the loss over the batch."}}, "static_ok": false, "static_reason": "implementation_hint.returns must be 'scalar'."}
{"generation": 2, "index": 4, "ir": {"name": "AdaptiveMarginHingeLoss", "intuition": "This loss function, named Adaptive Margin Hinge Loss, is designed to be a more robust and adaptive alternative to traditional preference losses like the hinge loss or logistic loss. Its core idea is to create a dynamic 'margin' based on the normalized cost difference between two solutions. A larger cost difference (one solution is clearly better) demands a larger log probability difference, while a smaller cost difference allows for a smaller, or even zero, log probability difference. This prevents the model from being overly punished for minor or noisy cost variations, focusing its learning capacity on significant preference signals. The use of `tanh` on the log probability difference and `softplus` on the cost difference ensures that extreme values are compressed into a stable range, preventing numerical instability (NaN/Inf). The `relu` function implements the hinge mechanism, meaning no loss is incurred if the model's preference already satisfies the adaptive margin.", "pseudocode": "def AdaptiveMarginHingeLoss(cost_a, cost_b, logp_a, logp_b, beta, tau):\n    # 1. Calculate the log probability difference.\n    log_prob_diff = logp_a - logp_b\n\n    # 2. Compute a stable, normalized cost difference. softplus ensures non-negativity and smoothness.\n    # We use softplus on the raw difference instead of a simple max(0, diff) to maintain gradients everywhere.\n    cost_diff = cost_b - cost_a\n    normalized_cost_gap = softplus(cost_diff / tau)\n\n    # 3. The adaptive margin is proportional to the normalized cost gap.\n    # This is the target log probability difference the model should achieve.\n    adaptive_margin = beta * normalized_cost_gap\n\n    # 4. Use tanh to bound the log_prob_diff, making the loss robust to extreme logit predictions.\n    # This prevents the loss from becoming excessively large when the model is very wrong.\n    bounded_log_prob_diff = tanh(log_prob_diff)\n\n    # 5. The core hinge loss structure: loss = max(0, margin - model_preference).\n    # If the model's preference (bounded_log_prob_diff) exceeds the required margin, the loss is zero.\n    # Otherwise, the loss is the deficit.\n    loss = relu(adaptive_margin - bounded_log_prob_diff)\n\n    return loss", "hyperparams": {"beta": 1.0, "tau": 10.0}, "operators_used": ["softplus", "tanh", "relu"], "implementation_hint": {"expects": ["cost_a", "cost_b", "logp_a", "logp_b"], "returns": "A scalar tensor representing the mean loss over the batch."}}, "static_ok": false, "static_reason": "implementation_hint.returns must be 'scalar'."}
{"generation": 2, "index": 5, "ir": {"name": "AdaptiveMarginHingeLoss", "intuition": "This loss function, named Adaptive Margin Hinge Loss, is designed to be a more robust and adaptive alternative to standard hinge or logistic losses for preference learning. Its core idea is to create a dynamic 'margin' that depends on the magnitude of the true cost difference between two solutions. When the cost difference is large, the model is penalized more heavily for having the wrong preference, demanding a larger log-probability gap. When the cost difference is small (i.e., the solutions are of similar quality), the loss is more forgiving, preventing the model from overfitting to noisy or insignificant preferences. This is achieved by using the hyperbolic tangent (tanh) of the normalized cost difference to create a bounded, non-linear margin. The loss is then formulated as a softplus (a smooth version of ReLU) of the 'preference violation', which ensures it is always non-negative, smooth, and punishes incorrect preferences (logit_diff < margin) while yielding zero loss for correctly and confidently ranked pairs (logit_diff > margin). The use of softplus and tanh guarantees numerical stability against extreme input values.", "pseudocode": "def AdaptiveMarginHingeLoss(cost_a, cost_b, logp_a, logp_b, tau, beta):\n  # 1. Calculate the signed log probability difference.\n  # This represents the model's current preference strength.\n  logit_diff = logp_a - logp_b\n\n  # 2. Calculate the normalized cost difference.\n  # The rank_gap operator returns a signed, normalized value in [-1, 1].\n  # rank_gap(a,b) is positive if a is better (cost_a < cost_b).\n  cost_gap = rank_gap(cost_b, cost_a) # Note the order: better solution first\n\n  # 3. Create an adaptive margin using tanh.\n  # The margin is a value in [0, beta] that grows with the cost difference.\n  # It's the target logit difference the model should achieve.\n  # tanh makes the margin gracefully saturate for large cost gaps.\n  adaptive_margin = beta * tanh(cost_gap / tau)\n\n  # 4. Calculate the preference violation.\n  # We want logit_diff to be greater than adaptive_margin.\n  # A negative value here means the preference is wrong or not strong enough.\n  violation = adaptive_margin - logit_diff\n\n  # 5. Apply softplus to create a smooth, non-negative loss.\n  # softplus(x) = log(1 + exp(x)). It's a smooth approximation of ReLU(x).\n  # This penalizes violations (when violation > 0) and gives ~0 loss otherwise.\n  loss = softplus(violation)\n\n  return loss", "hyperparams": {"tau": {"default": 0.1, "comment": "Temperature for scaling the cost gap. Smaller tau makes the margin more sensitive to small cost differences."}, "beta": {"default": 5.0, "comment": "Maximum margin value. This controls the maximum separation pressure applied to the logits."}}, "operators_used": ["rank_gap", "tanh", "softplus"], "implementation_hint": {"expects": ["pair_cost_a", "pair_cost_b", "pair_logp_a", "pair_logp_b"], "returns": "scalar_loss"}}, "static_ok": false, "static_reason": "implementation_hint.returns must be 'scalar'."}
{"generation": 2, "index": 6, "ir": {"name": "AdaptiveMarginHingeLoss", "intuition": "This loss function, named 'Adaptive Margin Hinge Loss', is designed for stable preference learning in combinatorial optimization. Its core idea is to create a dynamic, cost-sensitive margin that the model's preference (logit difference) must overcome. The margin is not a fixed value but adapts based on the normalized rank difference between the costs of two solutions. This ensures that pairs with a large, meaningful difference in quality are given a proportionally larger 'push' during training, while pairs with negligible cost differences have a smaller, less impactful margin, preventing the model from overfitting to noise. The `tanh` function is used to squash the logit difference, preventing extreme logit values from causing instability. The `softplus` function, a smooth version of ReLU, creates a one-sided hinge-like loss, only penalizing the model when its preference contradicts the ground-truth cost ordering, while also ensuring the loss surface is smooth for stable gradient-based optimization.", "pseudocode": "def AdaptiveMarginHingeLoss(cost_a, cost_b, logit_a, logit_b, alpha, beta):\n  # 1. Calculate the signed rank gap based on costs. This is robust to cost distribution.\n  # Positive if a is better (cost_a < cost_b).\n  signed_gap = rank_gap(cost_b, cost_a)  # Swapped to get positive sign for cost_a < cost_b\n\n  # 2. Create an adaptive margin. The margin grows with the significance of the cost difference.\n  # The sigmoid function scales the gap to a (0, 1) range, which is then scaled by alpha.\n  adaptive_margin = alpha * sigmoid(signed_gap)\n\n  # 3. Calculate the difference in model's log-probabilities (logits).\n  logit_diff = logit_a - logit_b\n\n  # 4. Squash the logit difference using tanh for numerical stability.\n  # This bounds the influence of extreme logit predictions.\n  stable_logit_diff = beta * tanh(logit_diff / beta)\n\n  # 5. Combine into a hinge-like loss. The loss is positive only when the model's preference\n  # (stable_logit_diff) is smaller than the required adaptive margin.\n  # softplus(x) is a smooth approximation of max(0, x).\n  loss = softplus(adaptive_margin - stable_logit_diff)\n\n  return loss", "hyperparams": {"alpha": {"default": 5.0, "description": "Scales the adaptive margin. A larger alpha creates a stronger separation target for pairs with significant cost differences."}, "beta": {"default": 10.0, "description": "Controls the saturation point of the tanh function on the logit difference. A larger beta makes the tanh approximation closer to a linear function for a wider range of inputs, while a smaller beta squashes the logits more aggressively, increasing robustness to outlier logits."}}, "operators_used": ["rank_gap", "sigmoid", "tanh", "softplus"], "implementation_hint": {"expects": ["pair_cost_a", "pair_cost_b", "pair_logit_a", "pair_logit_b"], "returns": "A scalar loss value, typically the mean of the loss over the batch."}}, "static_ok": false, "static_reason": "implementation_hint.returns must be 'scalar'."}
{"generation": 2, "index": 7, "ir": {"name": "AdaptiveMarginHingeLoss", "intuition": "This loss function, named 'Adaptive Margin Hinge Loss', is designed for stable preference learning in combinatorial optimization. Its core idea is to create a dynamic margin that adapts to the magnitude of the cost difference between two solutions. Unlike a fixed-margin hinge loss, this prevents the model from being overly penalized for small, insignificant cost differences, while still strongly enforcing preferences for large, meaningful cost gaps. It uses `tanh` to non-linearly scale the cost difference, creating a margin that saturates at both ends. This prevents extreme cost differences from causing exploding gradients. The final loss is a `softplus` (a smooth version of ReLU) applied to the difference between the logit difference and this adaptive margin, ensuring the loss is always non-negative, smooth, and punishes the model only when its preference (`logit_diff`) doesn't align with the ground truth cost-based preference, considering the adaptive margin.", "pseudocode": "def adaptive_margin_hinge_loss(cost_a, cost_b, logp_a, logp_b, beta, tau, margin_max):\n    # 1. Calculate cost difference. Assume cost_a is preferred (smaller).\n    delta_cost = cost_b - cost_a\n\n    # 2. Create an adaptive margin based on the cost difference.\n    # tanh scales the cost difference into a [-1, 1] range.\n    # The margin adapts from 0 to `margin_max` based on the significance of delta_cost.\n    # `tau` controls the sensitivity of the margin to the cost difference.\n    adaptive_margin = margin_max * tanh(delta_cost / tau)\n\n    # 3. Calculate the log probability difference (logit difference).\n    # We want logp(a) to be greater than logp(b).\n    logit_diff = logp_a - logp_b\n\n    # 4. Calculate the core hinge-like term.\n    # The loss is triggered if logit_diff is not greater than the adaptive_margin.\n    # We want to maximize logit_diff, so we penalize `adaptive_margin - logit_diff`.\n    hinge_term = adaptive_margin - logit_diff\n\n    # 5. Apply softplus for a smooth, non-negative loss, and scale it.\n    # softplus(x) = log(1 + exp(x)). It's a smooth approximation of ReLU(x).\n    loss = beta * softplus(hinge_term)\n\n    return loss", "hyperparams": {"beta": {"default": 1.0, "description": "A scaling factor for the overall loss magnitude."}, "tau": {"default": 10.0, "description": "Temperature parameter to control the steepness of the tanh function, affecting how quickly the adaptive margin saturates. A smaller tau means the margin saturates with smaller cost differences."}, "margin_max": {"default": 5.0, "description": "The maximum possible margin, achieved when the cost difference is very large. This caps the target for the logit difference."}}, "operators_used": ["tanh", "softplus"], "implementation_hint": {"expects": ["preferred_cost", "dispreferred_cost", "preferred_logp", "dispreferred_logp"], "returns": "A single scalar representing the loss for the pair."}}, "static_ok": false, "static_reason": "implementation_hint.returns must be 'scalar'."}
