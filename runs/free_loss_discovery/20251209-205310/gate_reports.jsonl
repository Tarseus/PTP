{"generation": 0, "index": 0, "ir": {"name": "AdaptiveMarginHingeLoss", "intuition": "This loss function, inspired by hinge loss, introduces an adaptive margin that is proportional to the normalized cost difference between two solutions. The core idea is that when the cost difference between the preferred solution (a) and the non-preferred solution (b) is large, the model should be more confident in its preference (i.e., logp(a) should be significantly larger than logp(b)). We use `tanh` on the normalized cost difference to create a bounded, non-linear margin `m` between 0 and `alpha`. This prevents extreme cost differences from creating excessively large margins and destabilizing training. The `softplus` function is used as a smooth, non-negative approximation of the hinge loss `max(0, x)`, ensuring the loss is always differentiable and well-behaved. The loss penalizes the model only when its preference `(logp(a) - logp(b))` is not 'better' than the adaptive margin `m` requires.", "pseudocode": "def AdaptiveMarginHingeLoss(cost_a, cost_b, logp_a, logp_b, alpha, scale):\n    # Assume cost_a < cost_b (a is preferred)\n    logit_diff = logp_a - logp_b\n\n    # 1. Calculate normalized cost difference. We use log to compress the range.\n    #    Adding 1e-6 for stability if costs can be zero.\n    log_cost_a = log(cost_a + 1e-6)\n    log_cost_b = log(cost_b + 1e-6)\n    cost_gap = log_cost_b - log_cost_a # Positive value\n\n    # 2. Create a bounded, adaptive margin using tanh.\n    #    The margin 'm' will be in the range [0, alpha).\n    adaptive_margin = alpha * tanh(scale * cost_gap)\n\n    # 3. Compute the hinge loss using softplus for smoothness.\n    #    The target is for logit_diff to be greater than the adaptive_margin.\n    #    So, we penalize (adaptive_margin - logit_diff) when it's positive.\n    loss = softplus(adaptive_margin - logit_diff)\n\n    return loss", "hyperparams": {"alpha": 1.0, "scale": 0.5}, "operators_used": ["log", "tanh", "softplus"], "implementation_hint": {"expects": ["{'cost_w': 'Scalar cost of the winning/preferred solution in a pair.', 'cost_l': 'Scalar cost of the losing/non-preferred solution in a pair.', 'logp_w': 'Scalar log probability of the winning solution.', 'logp_l': 'Scalar log probability of the losing solution.'}"], "returns": "A single scalar loss value for the pair."}}, "static_ok": false, "static_reason": "implementation_hint.returns must be 'scalar'."}
{"generation": 0, "index": 1, "ir": {"name": "AdaptiveMarginHingeLoss", "intuition": "This loss function, named Adaptive Margin Hinge Loss (AM-Hinge), is inspired by the hinge loss used in Support Vector Machines (SVMs). The core idea is to enforce a margin between the log-probabilities of two solutions, where the size of this margin is dynamically adapted based on the normalized difference in their costs. Specifically, if solution 'a' is better than 'b' (cost(a) < cost(b)), the loss penalizes the model only if the log-probability of 'a' is not sufficiently higher than that of 'b'. The required 'sufficiency' (the margin) is proportional to how much better 'a' is than 'b'. This proportionality is controlled by a temperature-like parameter `tau`. By using `tanh` to squash the cost difference and `softplus` (a smooth version of ReLU) for the hinge mechanism, the loss remains numerically stable and avoids gradient explosion even with extreme cost or logit differences.", "pseudocode": "def adaptive_margin_hinge_loss(cost_a, cost_b, logp_a, logp_b, tau, beta):\n    # Ensure a is the better solution (winner)\n    if cost_a > cost_b:\n        cost_a, cost_b = cost_b, cost_a\n        logp_a, logp_b = logp_b, logp_a\n\n    # Calculate log probability difference\n    logp_diff = logp_a - logp_b\n\n    # Calculate normalized cost difference and scale it\n    # rank_gap ensures a non-negative difference. tanh squashes it to [-1, 1] for stability.\n    # The negative sign aligns it with the preference: a larger positive gap means 'a' is much better.\n    normalized_cost_gap = tanh(rank_gap(cost_b, cost_a))\n\n    # The adaptive margin: larger cost gap demands a larger logp difference.\n    # tau controls the sensitivity of the margin to the cost gap.\n    margin = normalized_cost_gap / tau\n\n    # Hinge loss mechanism: penalize if logp_diff is less than the required margin.\n    # We use softplus for a smooth, non-negative loss.\n    # beta scales the overall loss magnitude.\n    loss = beta * softplus(margin - logp_diff)\n\n    return loss", "hyperparams": {"tau": {"default": 0.1, "description": "Temperature parameter. A smaller tau increases the margin's sensitivity to the cost difference, enforcing a stricter preference ordering. A larger tau softens the margin."}, "beta": {"default": 1.0, "description": "A scaling factor for the overall loss magnitude, similar to the DPO beta. It balances the preference learning objective against other potential objectives (e.g., policy likelihood)."}}, "operators_used": ["rank_gap", "tanh", "softplus"], "implementation_hint": {"expects": ["{'cost_w': 'Tensor of costs for the winning solutions in a batch.', 'cost_l': 'Tensor of costs for the losing solutions in a batch.', 'logp_w': 'Tensor of log probabilities for the winning solutions.', 'logp_l': 'Tensor of log probabilities for the losing solutions.'}"], "returns": "A scalar representing the mean loss over the batch."}}, "static_ok": false, "static_reason": "implementation_hint.returns must be 'scalar'."}
{"generation": 0, "index": 2, "ir": {"name": "AdaptiveMarginHingeLoss", "intuition": "This loss function, named Adaptive Margin Hinge Loss, is designed to dynamically adjust the learning signal based on the magnitude of the cost difference between two solutions. It combines a classic hinge loss structure with a margin that adapts to the cost gap. When the cost difference is small, the margin is also small, allowing the model to focus on subtle preferences. When the cost difference is large, the margin increases, creating a stronger push for the model to align its preferences with the ground truth costs. The `tanh` function is used to scale both the log probability difference and the cost difference, which bounds their influence and prevents extreme values from causing numerical instability. This ensures that the loss remains stable and provides a smooth, well-behaved gradient even with large variations in costs or model logits.", "pseudocode": "def AdaptiveMarginHingeLoss(cost_a, cost_b, logp_a, logp_b, alpha, beta, tau):\n    # 1. Calculate the difference in log probabilities\n    log_prob_diff = logp_a - logp_b\n\n    # 2. Calculate the normalized cost difference. Use rank_gap for robust cost comparison.\n    # The sign indicates preference: positive if b is worse than a.\n    cost_diff = rank_gap(cost_b, cost_a) # cost_b - cost_a\n\n    # 3. Create an adaptive margin that grows with the cost difference but is bounded.\n    # The margin is positive when cost_a < cost_b.\n    # tau controls the sensitivity of the margin to the cost difference.\n    adaptive_margin = tanh(cost_diff / tau)\n\n    # 4. Scale the log probability difference to control its influence.\n    # This term is positive if the model prefers a over b.\n    scaled_log_prob_term = tanh(alpha * log_prob_diff)\n\n    # 5. Combine them in a hinge-like structure.\n    # The loss is incurred when the model's preference (scaled_log_prob_term)\n    # is less than the desired preference margin (adaptive_margin).\n    # The 'beta' hyperparameter scales the overall magnitude of the loss.\n    # The softplus function creates a smooth version of the hinge loss (max(0, x)).\n    loss = beta * softplus(adaptive_margin - scaled_log_prob_term)\n\n    return loss", "hyperparams": {"alpha": 1.0, "beta": 2.0, "tau": 10.0}, "operators_used": ["rank_gap", "tanh", "softplus"], "implementation_hint": {"expects": ["{'cost_a': \"Tensor of costs for solution 'a' in a batch of pairs.\", 'cost_b': \"Tensor of costs for solution 'b' in a batch of pairs.\", 'logp_a': \"Tensor of log probabilities for solution 'a' from the model.\", 'logp_b': \"Tensor of log probabilities for solution 'b' from the model.\"}"], "returns": "A scalar loss value, typically the mean over the batch."}}, "static_ok": false, "static_reason": "implementation_hint.returns must be 'scalar'."}
{"generation": 0, "index": 3, "ir": {"name": "AdaptiveMarginHingeLoss", "intuition": "This loss function, inspired by hinge loss, introduces an adaptive margin that is proportional to the normalized cost difference between two solutions. The core idea is that when the better solution 'a' (cost(a) < cost(b)) is significantly better (large cost gap), the model should be penalized more heavily for not preferring it strongly enough. The margin is dynamically set by `tanh(beta * normalized_cost_gap)`, which bounds the margin between 0 and 1, preventing extreme cost differences from causing instability. The `softplus` function is used instead of a hard `max(0, ...)` to ensure the loss is smooth and has a non-zero gradient everywhere, facilitating stable backpropagation. The loss penalizes the model when the log probability of the better solution is not sufficiently higher than the worse one, with 'sufficiently' defined by this adaptive margin.", "pseudocode": "def AdaptiveMarginHingeLoss(cost_a, cost_b, logp_a, logp_b, beta, tau):\n    # Ensure a is the better solution for consistent calculation\n    if cost_a > cost_b:\n        cost_a, cost_b = cost_b, cost_a\n        logp_a, logp_b = logp_b, logp_a\n\n    # Calculate log probability difference (logit difference)\n    logit_diff = logp_a - logp_b\n\n    # Calculate normalized cost difference using rank_gap for robustness\n    cost_gap = rank_gap(cost_b, cost_a)  # cost_b > cost_a, so gap is positive\n\n    # Create an adaptive margin based on the cost gap, scaled and bounded by tanh\n    # beta controls the steepness of the margin's response to the cost gap\n    adaptive_margin = tanh(beta * cost_gap)\n\n    # The core loss logic: penalize if the logit_diff is not greater than the adaptive margin.\n    # The loss is `margin - logit_diff`, but only when this value is positive.\n    # We use softplus for a smooth, non-negative loss.\n    # The argument to softplus is scaled by tau to control the loss magnitude.\n    loss = softplus(tau * (adaptive_margin - logit_diff))\n\n    return loss", "hyperparams": {"beta": 5.0, "tau": 1.0}, "operators_used": ["softplus", "tanh", "rank_gap"], "implementation_hint": {"expects": ["{'cost_a': \"Scalar cost of solution 'a'\", 'cost_b': \"Scalar cost of solution 'b'\", 'logp_a': \"Scalar log probability of solution 'a'\", 'logp_b': \"Scalar log probability of solution 'b'\"}"], "returns": "scalar_loss"}}, "static_ok": false, "static_reason": "implementation_hint.returns must be 'scalar'."}
{"generation": 1, "index": 0, "ir": {"name": "AdaptiveMarginHingeLoss", "intuition": "This loss function, named 'Adaptive Margin Hinge Loss', is designed for robust preference learning in combinatorial optimization. Its core idea is to combine the principles of a hinge loss with an adaptive margin that is sensitive to the magnitude of the cost difference. When the preferred solution (a) is significantly better than the other (b), the loss function imposes a larger margin, pushing the model more aggressively to favor 'a'. Conversely, when the cost difference is small, the margin shrinks, preventing the model from over-optimizing on noisy or insignificant preferences. The use of 'tanh' to scale the cost difference and 'softplus' (a smooth version of ReLU) ensures that the loss is smooth, numerically stable, and bounded, avoiding issues with extreme cost or logit differences.", "pseudocode": "def AdaptiveMarginHingeLoss(cost_a, cost_b, logp_a, logp_b, alpha, beta, tau):\n  # cost_a is preferred over cost_b, so cost_a < cost_b\n  # Ensure cost_diff is non-negative and normalized to a sensible range\n  cost_diff = relu(cost_b - cost_a)\n  \n  # Create an adaptive margin based on the cost difference.\n  # tanh non-linearly maps the cost difference to a [0, 1) range.\n  # The margin grows with the cost difference but saturates.\n  adaptive_margin = alpha * tanh(cost_diff / tau)\n  \n  # Calculate the log probability difference (logit difference).\n  # We want logp_a to be greater than logp_b.\n  logp_diff = logp_a - logp_b\n  \n  # The core hinge-like structure.\n  # We want logp_diff to be larger than the adaptive_margin.\n  # The loss is positive only when this condition is violated.\n  # softplus(x) = log(1 + exp(x)), a smooth version of max(0, x).\n  # beta scales the overall magnitude of the loss.\n  loss = softplus(adaptive_margin - logp_diff) * beta\n  \n  return loss", "hyperparams": {"alpha": 1.0, "beta": 1.0, "tau": 10.0}, "operators_used": ["relu", "tanh", "softplus"], "implementation_hint": {"expects": ["cost_a", "cost_b", "logp_a", "logp_b"], "returns": "scalar_loss"}}, "static_ok": false, "static_reason": "implementation_hint.returns must be 'scalar'."}
{"generation": 1, "index": 1, "ir": {"name": "AdaptiveMarginHingeLoss", "intuition": "This loss function is inspired by the hinge loss (or margin loss) commonly used in SVMs, but with two key adaptations for preference learning in optimization. First, the 'margin' is not a fixed hyperparameter but is dynamically determined by the normalized difference in cost between the two solutions. A larger cost difference demands a larger log probability difference to satisfy the preference. Second, it uses `softplus` instead of a hard `max(0, x)` hinge to ensure the loss is smooth everywhere, which is beneficial for gradient-based optimization. The `tanh` function is used to squash the cost difference into a bounded range [-1, 1], preventing extreme cost gaps from creating excessively large margins and ensuring numerical stability. The overall effect is a 'soft' and 'adaptive' hinge loss that penalizes the model only when its preference (`logit_diff`) doesn't align with the ground-truth cost preference, with the penalty magnitude gracefully scaling with the severity of the mis-ranking.", "pseudocode": "def AdaptiveMarginHingeLoss(cost_a, cost_b, logp_a, logp_b, tau, beta):\n    # 1. Normalize the cost difference to a bounded range [-1, 1]\n    # The rank_gap operator (y_pref) is 1 if cost_a < cost_b, -1 otherwise.\n    y_pref = rank_gap(cost_a, cost_b)\n    cost_diff_abs = abs(cost_a - cost_b)\n    # Use tanh to squash the scaled cost difference, ensuring stability.\n    adaptive_margin = tanh(cost_diff_abs / tau)\n\n    # 2. Calculate the log probability difference\n    logit_diff = logp_a - logp_b\n\n    # 3. Combine them in a hinge-like structure.\n    # The term inside softplus is `margin - y_pref * logit_diff`.\n    # If preference is correct (y_pref and logit_diff have the same sign), the term is small or negative.\n    # If preference is incorrect, the term is positive and large, leading to a penalty.\n    # beta scales the overall loss.\n    loss_value = beta * softplus(adaptive_margin - y_pref * logit_diff)\n\n    return loss_value", "hyperparams": {"tau": {"default": 10.0, "description": "Temperature parameter to scale the cost difference before tanh. A smaller tau makes the margin more sensitive to small cost differences."}, "beta": {"default": 1.0, "description": "Overall scaling factor for the loss magnitude."}}, "operators_used": ["rank_gap", "tanh", "softplus"], "implementation_hint": {"expects": ["{'cost_a': \"Tensor of costs for solution 'a' in each pair.\", 'cost_b': \"Tensor of costs for solution 'b' in each pair.\", 'logp_a': \"Tensor of log probabilities for solution 'a' in each pair.\", 'logp_b': \"Tensor of log probabilities for solution 'b' in each pair.\"}"], "returns": "A scalar loss value, typically the mean of losses over the batch."}}, "static_ok": false, "static_reason": "implementation_hint.returns must be 'scalar'."}
{"generation": 1, "index": 2, "ir": {"name": "Sigmoid-Scaled Rank-Gap Loss", "intuition": "This loss function aims to dynamically adjust the learning signal based on the magnitude of the cost difference. The core idea is to use a sigmoid function to scale the 'rank gap' between costs. When the cost difference is small, the sigmoid output is close to 0.5, creating a soft margin and preventing over-penalization for minor or noisy cost variations. As the cost difference becomes significant, the sigmoid output approaches 0 or 1, applying a stronger, almost linear penalty (similar to a hinge loss) to the logit difference. This makes the loss robust to noise in small cost gaps while being decisive for large, clear-cut preferences. The final softplus ensures the loss is non-negative and smooth, and clamping the logit difference prevents extreme gradients from destabilizing training.", "pseudocode": "def loss(cost_a, cost_b, logp_a, logp_b, beta, margin):\n    # 1. Determine the preferred (w) and dispreferred (l) solutions based on cost.\n    # We want logp(w) to be higher than logp(l).\n    if cost_a < cost_b:\n        cost_w, cost_l = cost_a, cost_b\n        logp_w, logp_l = logp_a, logp_b\n    else:\n        cost_w, cost_l = cost_b, cost_a\n        logp_w, logp_l = logp_b, logp_a\n\n    # 2. Calculate the normalized rank gap of costs.\n    # This value is between -1 and 1.\n    cost_gap = rank_gap(cost_l, cost_w) # Positive, as cost_l > cost_w\n\n    # 3. Calculate the log probability difference. We want this to be positive.\n    logit_diff = logp_w - logp_l\n    \n    # 4. Clamp the logit difference to prevent extreme values from causing instability.\n    clamped_logit_diff = clamp(logit_diff, min=-10.0, max=10.0)\n\n    # 5. Create a dynamic margin/weight based on the cost gap, scaled by beta.\n    # The sigmoid output ranges from 0.5 (for zero gap) to 1.0 (for large gaps).\n    # This means the target separation 'M' is adaptive.\n    target_separation_weight = sigmoid(beta * cost_gap)\n\n    # 6. The core loss term. We want 'clamped_logit_diff' to be greater than 'margin'.\n    # The 'target_separation_weight' scales the importance of this violation.\n    # The structure is softplus(M - x), which penalizes x < M.\n    # Here, M is our adaptive margin.\n    loss_value = softplus(margin - target_separation_weight * clamped_logit_diff)\n    \n    return loss_value", "hyperparams": {"beta": 10.0, "margin": 0.1}, "operators_used": ["rank_gap", "sigmoid", "clamp", "softplus"], "implementation_hint": {"expects": ["{'pair_cost_a': \"Tensor of costs for solution 'a' in each pair.\", 'pair_cost_b': \"Tensor of costs for solution 'b' in each pair.\", 'pair_logp_a': \"Tensor of log probabilities for solution 'a' in each pair.\", 'pair_logp_b': \"Tensor of log probabilities for solution 'b' in each pair.\"}"], "returns": "A scalar loss value, typically the mean of the losses for all pairs in the batch."}}, "static_ok": false, "static_reason": "implementation_hint.returns must be 'scalar'."}
{"generation": 1, "index": 3, "ir": {"name": "SigmoidAttenuatedMarginLoss", "intuition": "This loss function, inspired by hinge loss, introduces a margin that the model's preference (logit difference) for the better solution should exceed. The key innovation is that this margin is not fixed; it's dynamically scaled by the normalized cost difference. When the cost difference is large, the model is pushed harder to favor the better solution. Conversely, when solutions are of similar quality, the target margin is small, preventing the model from over-optimizing on negligible differences. The entire term is then passed through a softplus function to ensure non-negativity and smoothness, which is more numerically stable than a hard ReLU margin. A sigmoid function is used to attenuate the influence of the logit difference, preventing extreme logit values from causing instability and ensuring the gradient remains bounded.", "pseudocode": "def SigmoidAttenuatedMarginLoss(cost_a, cost_b, logit_a, logit_b, beta, tau):\n    # Assume cost_a < cost_b without loss of generality\n    cost_diff = cost_b - cost_a # Always positive\n    logit_diff = logit_a - logit_b\n\n    # Normalize cost difference to [0, 1] using tanh, creating a dynamic margin\n    # This prevents extreme cost differences from creating an unbounded target\n    dynamic_margin = tanh(cost_diff / tau)\n\n    # Attenuate the logit difference using a scaled sigmoid\n    # This bounds the influence of logits, preventing instability from extreme values\n    attenuated_logit_diff = sigmoid(logit_diff)\n\n    # Formulate the core loss term. The model is penalized if the\n    # sigmoid-of-logit-difference does not exceed the dynamic margin.\n    # We use (margin - value) structure. A preference for b (logit_a < logit_b)\n    # means attenuated_logit_diff < 0.5, increasing the loss.\n    # A strong preference for a (logit_a >> logit_b) means attenuated_logit_diff -> 1,\n    # which helps satisfy the margin and reduces the loss.\n    loss_argument = dynamic_margin - attenuated_logit_diff\n\n    # Apply a scaled softplus for a smooth, non-negative, and stable loss\n    loss = beta * softplus(loss_argument)\n\n    return loss", "hyperparams": {"beta": {"description": "A scaling factor for the overall loss magnitude.", "default_value": 1.0}, "tau": {"description": "A temperature parameter to control the sensitivity of the dynamic margin to the cost difference. Smaller tau makes the margin saturate faster.", "default_value": 10.0}}, "operators_used": ["tanh", "sigmoid", "softplus"], "implementation_hint": {"expects": ["{'cost_w': 'Scalar cost of the winning solution (lower is better).', 'cost_l': 'Scalar cost of the losing solution.', 'logit_w': 'Log probability (or logit) of the winning solution from the model.', 'logit_l': 'Log probability (or logit) of the losing solution from the model.'}"], "returns": "A single scalar loss value for the pair."}}, "static_ok": false, "static_reason": "implementation_hint.returns must be 'scalar'."}
{"generation": 1, "index": 4, "ir": {"name": "SigmoidRankDisagreementLoss", "intuition": "This loss function measures the disagreement between the model's preference and the ground-truth cost ranking, scaled by the magnitude of this disagreement. The core idea is to create a 'target' preference margin based on the normalized cost difference, and then penalize the deviation of the model's log-probability difference from this target. We use `tanh` to map both the cost difference and log-probability difference into a bounded [-1, 1] range, preventing extreme values from causing instability. The final loss is a softplus of the squared difference, which is a smooth, non-negative, and convex-like penalty that is zero only when the model's preference perfectly matches the target preference.", "pseudocode": "1. For a batch of solution pairs, calculate the cost difference `delta_cost = cost_a - cost_b`.\n2. Normalize `delta_cost` across the batch using z-score to get `norm_delta_cost`. This makes the loss scale-invariant to the absolute cost values.\n3. Compute a target preference `target_pref = tanh(beta * norm_delta_cost)`. This maps the normalized cost difference to a target value in [-1, 1]. A large cost difference results in a target close to -1 or 1.\n4. Similarly, compute the model's preference `model_pref = tanh(logit_diff)`. This bounds the model's output, ensuring stability.\n5. The core of the loss is the squared difference between the model's preference and the target: `disagreement = (model_pref - target_pref)^2`.\n6. Apply a `softplus` function to the scaled disagreement: `loss = softplus(alpha * disagreement - margin)`. The `softplus` ensures the loss is non-negative and smooth. The `margin` allows for a 'tolerance zone' where small disagreements are not penalized, and `alpha` scales the penalty's steepness.", "hyperparams": {"alpha": 2.0, "beta": 1.0, "margin": 0.05}, "operators_used": ["zscore", "tanh", "softplus"], "implementation_hint": {"expects": ["{'pair_cost_a': \"Tensor of costs for solution 'a' in each pair (shape: [batch_size])\", 'pair_cost_b': \"Tensor of costs for solution 'b' in each pair (shape: [batch_size])\", 'logit_diff': 'Tensor of logp(a) - logp(b) for each pair (shape: [batch_size])'}"], "returns": "A single scalar loss value, averaged over the batch."}}, "static_ok": false, "static_reason": "implementation_hint.returns must be 'scalar'."}
{"generation": 1, "index": 5, "ir": {"name": "AdaptiveMarginHingeLoss", "intuition": "This loss function, named 'Adaptive Margin Hinge Loss', is designed for stable preference learning in combinatorial optimization. Its core idea is to combine a classic hinge loss structure with an adaptive margin that is sensitive to the magnitude of the cost difference. When the cost difference is small, the margin is also small, allowing the model to learn from subtle preferences. When the cost difference is large, the margin increases, creating a stronger push for the model to align its preferences with the ground truth costs. The hinge loss (implemented with `softplus`) provides a clear target: if the model's preference `logp(a) - logp(b)` already aligns with the cost preference `cost(a) < cost(b)` by a sufficient margin, the loss becomes zero (or near-zero), preventing over-optimization on 'easy' pairs. The use of `tanh` on the cost difference `delta_cost` and `clamp` on the logit difference ensures that extreme input values are squashed into a bounded range, guaranteeing numerical stability and preventing gradient explosion.", "pseudocode": "def AdaptiveMarginHingeLoss(cost_a, cost_b, logp_a, logp_b, alpha, beta, tau):\n  # 1. Calculate the normalized cost difference in a bounded range [-1, 1]\n  # The sign indicates which solution is better. tau controls sensitivity.\n  # If cost_a < cost_b, normalized_delta_cost is negative.\n  normalized_delta_cost = tanh((cost_a - cost_b) / tau)\n\n  # 2. Calculate the log probability difference (logit difference)\n  # Clamp to prevent extreme values from causing instability.\n  logit_diff = clamp(logp_a - logp_b, min=-10.0, max=10.0)\n\n  # 3. Define an adaptive margin. The margin is larger for pairs with a larger cost difference.\n  # The margin is always non-negative.\n  # alpha controls the base margin, beta scales the adaptive component.\n  adaptive_margin = alpha + beta * abs(normalized_delta_cost)\n\n  # 4. Construct the core hinge loss term.\n  # We want logit_diff to be positive if cost_a < cost_b (i.e., normalized_delta_cost < 0).\n  # The argument to softplus should be positive when the model is wrong.\n  # Model is wrong if: normalized_delta_cost < 0 AND logit_diff is not positive enough.\n  # This is captured by: margin - sign(cost_b - cost_a) * logit_diff\n  # which is equivalent to: margin + normalized_delta_cost_sign * logit_diff\n  # We use normalized_delta_cost directly as a continuous version of the sign.\n  # The loss is softplus(margin + normalized_delta_cost * logit_diff)\n  # However, a more standard hinge formulation is softplus(margin - y * f(x)), where y is {-1, 1}.\n  # Here, y = -sign(cost_a - cost_b), f(x) = logit_diff.\n  # So, loss = softplus(adaptive_margin + sign(cost_a - cost_b) * logit_diff)\n  # Using normalized_delta_cost as a soft sign gives:\n  loss_argument = adaptive_margin + (normalized_delta_cost / abs(normalized_delta_cost)) * logit_diff if normalized_delta_cost != 0 else adaptive_margin\n  # A simpler, more robust formulation: The target is for -logit_diff to align with normalized_delta_cost.\n  # We want logit_diff to be large and positive when normalized_delta_cost is large and negative.\n  # The term to minimize is (logit_diff + adaptive_margin * sign(delta_cost))^2 or similar.\n  # Let's use a simpler hinge: loss = softplus(margin - y * logit_diff), where y is the target sign.\n  # Target y = -sign(cost_a - cost_b). Let's use normalized_delta_cost as a soft target value.\n  # We want logit_diff to be close to some positive value when cost_a < cost_b.\n  # Let's define the error as how much logit_diff deviates from the desired direction, plus a margin.\n  # Loss = softplus(adaptive_margin - (-sign(cost_a - cost_b)) * logit_diff)\n  # Let's use the stable continuous version: softplus(adaptive_margin + normalized_delta_cost * logit_diff)\n  loss = softplus(normalized_delta_cost * logit_diff + adaptive_margin)\n\n  return loss", "hyperparams": {"alpha": 0.1, "beta": 2.0, "tau": 1.0}, "operators_used": ["tanh", "clamp", "softplus"], "implementation_hint": {"expects": ["{'cost_a': 'Tensor, scalar cost for solution a', 'cost_b': 'Tensor, scalar cost for solution b', 'logp_a': 'Tensor, log probability of solution a', 'logp_b': 'Tensor, log probability of solution b'}"], "returns": "scalar, the computed loss value for the pair"}}, "static_ok": false, "static_reason": "implementation_hint.returns must be 'scalar'."}
{"generation": 1, "index": 6, "ir": {"name": "AdaptiveMarginHingeLoss", "intuition": "This loss function, named 'Adaptive Margin Hinge Loss', is inspired by the hinge loss used in SVMs but with a crucial modification for preference learning in combinatorial optimization. The core idea is to create a dynamic 'margin' that adapts to the quality difference between two solutions. When the cost difference is large, the model is pushed more strongly to respect this preference. When the cost difference is small, the loss becomes more lenient, allowing the model to explore solutions that are close in quality. This is achieved by multiplying the log probability difference with a normalized cost difference. We use `tanh` to scale the cost difference into a [-1, 1] range, which acts as a stable, adaptive margin. The `softplus` function is then used as a smooth version of the hinge loss (ReLU), ensuring the loss is always non-negative, smooth, and penalizes incorrect preferences (`logp(a) < logp(b)` when `cost(a) < cost(b)`). This design avoids issues with extreme cost differences and provides a smooth, stable gradient for backpropagation.", "pseudocode": "def AdaptiveMarginHingeLoss(cost_a, cost_b, logp_a, logp_b, tau):\n    # Calculate the difference in log probabilities. We want logp_a to be greater than logp_b.\n    log_prob_diff = logp_b - logp_a\n\n    # Calculate the cost difference.\n    cost_diff = cost_a - cost_b\n\n    # Scale the cost difference using tanh to create a stable, adaptive margin factor in [-1, 1].\n    # A negative cost_diff (a is better) results in a negative adaptive_margin.\n    adaptive_margin = tanh(cost_diff / tau)\n\n    # The target for the hinge loss is the product of the log probability difference and the adaptive margin.\n    # If preferences align (e.g., cost_a < cost_b and logp_a > logp_b), then cost_diff < 0 and log_prob_diff < 0.\n    # Their product (adaptive_margin * log_prob_diff) will be positive, and softplus of a positive number is close to the number itself, resulting in a small loss.\n    # If preferences misalign (e.g., cost_a < cost_b but logp_a < logp_b), then cost_diff < 0 and log_prob_diff > 0.\n    # Their product will be negative. softplus of a large negative number is close to 0. This is incorrect. The signs must be flipped.\n    # Let's redefine the core argument.\n    # We want to penalize log_prob_diff > 0 when cost_a < cost_b.\n    # The term to penalize is `adaptive_margin * log_prob_diff`.\n    # Let's check signs again. If cost_a < cost_b, adaptive_margin is negative. If logp_a < logp_b, log_prob_diff is positive. Product is negative. softplus(-ve) is ~0. Wrong.\n    # Let's correct the logic. We want to penalize `logp_b - logp_a` when `cost_a < cost_b`.\n    # Let's use `rank_gap` which is `sign(cost_b - cost_a)`. Let's call it `preference_direction`.\n    # `preference_direction` is +1 if a is preferred, -1 if b is preferred.\n    # We want `logp_a - logp_b` to have the same sign as `preference_direction`.\n    # So we want to minimize `loss = softplus(-preference_direction * (logp_a - logp_b))`.\n    # Now, let's introduce the adaptive margin. The magnitude of penalty should scale with `abs(tanh(cost_diff/tau))`.\n    # Let's redefine the core logic.\n\n    # Corrected Pseudocode:\n    cost_diff = cost_a - cost_b  # Negative if 'a' is better\n    log_prob_diff = logp_a - logp_b # Should be positive if 'a' is better\n\n    # `cost_weight` is a bounded measure of how much better 'a' is than 'b'. \n    # It's in [-1, 1]. It's positive if 'b' is better, negative if 'a' is better.\n    cost_weight = tanh(cost_diff / tau)\n\n    # The argument to softplus is `-cost_weight * log_prob_diff`.\n    # Case 1: a is better (cost_a < cost_b). `cost_weight` is negative. We want `log_prob_diff` to be positive.\n    # The product `-cost_weight * log_prob_diff` will be negative. `softplus` of a large negative value is ~0 (low loss). Correct.\n    # Case 2: a is better (cost_a < cost_b), but model prefers b (logp_a < logp_b). `cost_weight` is negative, `log_prob_diff` is negative.\n    # The product `-cost_weight * log_prob_diff` will be positive. `softplus` of a positive value is > 0 (high loss). Correct.\n    # The magnitude of the loss is scaled by both the cost difference and the log_prob difference.\n    loss_argument = -cost_weight * log_prob_diff\n\n    loss = softplus(loss_argument)\n\n    return loss", "hyperparams": {"tau": {"value": 1.0, "description": "Temperature parameter to scale the cost difference before applying tanh. A smaller tau makes the function steeper, causing the weight to saturate to -1 or 1 more quickly. A larger tau results in a softer, more linear weight in the typical cost difference range."}}, "operators_used": ["tanh", "softplus"], "implementation_hint": {"expects": ["{'cost_a': \"Tensor of costs for solution 'a' in each pair.\", 'cost_b': \"Tensor of costs for solution 'b' in each pair.\", 'logp_a': \"Tensor of model's log probabilities for solution 'a'.\", 'logp_b': \"Tensor of model's log probabilities for solution 'b'.\"}"], "returns": "A scalar loss value (the mean of the loss over the batch)."}}, "static_ok": false, "static_reason": "implementation_hint.returns must be 'scalar'."}
{"generation": 1, "index": 7, "ir": {"name": "AdaptiveMarginHingeLoss", "intuition": "This loss function combines the ideas of a margin-based hinge loss with adaptive scaling based on the cost difference. The core idea is that the 'margin' a model should aim for between the log probabilities of two solutions should be proportional to the difference in their quality (cost). A large cost difference warrants a large log probability difference, while a small cost difference only requires a small, or even zero, probability difference. The tanh function is used to create a smooth, bounded 'target margin' from the cost difference, preventing extreme cost gaps from creating excessively large loss signals. The entire expression is wrapped in a softplus function, which acts as a smooth version of ReLU (max(0, x)). This ensures the loss is always non-negative and only penalizes the model when its probability assignment is inconsistent with the cost-based preference (i.e., when logp(a) - logp(b) is smaller than the desired adaptive margin). This design is inherently stable due to the bounded nature of tanh and the smooth, non-exploding behavior of softplus.", "pseudocode": "def AdaptiveMarginHingeLoss(cost_a, cost_b, logp_a, logp_b, beta, tau):\n    # Calculate the difference in log probabilities\n    log_prob_diff = logp_a - logp_b\n\n    # Calculate the normalized cost difference. The sign is flipped (b-a) so that\n    # if cost(a) < cost(b), the diff is positive, indicating 'a' is better.\n    cost_diff = cost_b - cost_a\n\n    # Create a bounded, adaptive margin using tanh. \n    # The margin scales with the cost difference but is capped between -beta and +beta.\n    # tau controls the sensitivity of the margin to the cost difference.\n    adaptive_margin = beta * tanh(cost_diff / tau)\n\n    # The core of the loss: we want log_prob_diff to be at least as large as the adaptive_margin.\n    # We penalize the model if log_prob_diff < adaptive_margin.\n    # This is equivalent to penalizing 'adaptive_margin - log_prob_diff' when it's positive.\n    # softplus(x) is a smooth approximation of max(0, x).\n    loss = softplus(adaptive_margin - log_prob_diff)\n\n    return loss", "hyperparams": {"beta": {"value": 5.0, "description": "Maximum margin factor. Controls the maximum target separation in log-probability space that the loss encourages. A larger beta creates a stronger push for separation when cost differences are significant."}, "tau": {"value": 1.0, "description": "Temperature for scaling the cost difference. Controls the steepness of the tanh function. A smaller tau makes the margin more sensitive to small cost differences, while a larger tau makes the transition smoother."}}, "operators_used": ["tanh", "softplus"], "implementation_hint": {"expects": ["{'cost_a': \"Scalar tensor representing the cost of solution 'a' in a pair. Lower is better.\", 'cost_b': \"Scalar tensor representing the cost of solution 'b' in a pair.\", 'logp_a': \"Scalar tensor representing the model's log probability for solution 'a'.\", 'logp_b': \"Scalar tensor representing the model's log probability for solution 'b'.\"}"], "returns": "A single scalar tensor representing the loss for the pair."}}, "static_ok": false, "static_reason": "implementation_hint.returns must be 'scalar'."}
{"generation": 2, "index": 0, "ir": {"name": "SigmoidRankScaledHingeLoss", "intuition": "This loss function, named Sigmoid Rank-Scaled Hinge Loss, combines the principles of hinge loss with a dynamic, cost-aware scaling mechanism. The core idea is to create a 'desired margin' that is proportional to the rank-based gap in costs between two solutions. Instead of a fixed margin, a larger cost difference demands a larger log probability difference, making the learning signal stronger for clearer preferences. We use `tanh` on the log probability difference to prevent extreme values from dominating the loss, ensuring stability. The `softplus` function is used as a smooth, non-negative hinge-like mechanism. The final loss is near zero when the model's preference `logp(a) - logp(b)` sufficiently exceeds the cost-derived target margin, and it increases smoothly as the model's preference contradicts the ground-truth cost ordering.", "pseudocode": "def SigmoidRankScaledHingeLoss(cost_a, cost_b, logp_a, logp_b, tau, beta, margin):\n    # 1. Calculate the rank-based cost difference, normalized to [0, 1].\n    # rank_gap(a, b) = 1 if a < b, 0.5 if a == b, 0 if a > b.\n    # We map this to a target sign: 1 for a<b, -1 for a>b.\n    target_sign = 2 * rank_gap(cost_a, cost_b) - 1\n\n    # 2. Compute the model's log probability difference.\n    log_prob_diff = logp_a - logp_b\n\n    # 3. Calculate a dynamic, cost-aware margin.\n    # We use a sigmoid function on the raw cost difference to create a stable, bounded scaling factor in [0, 1].\n    cost_diff_raw = cost_b - cost_a\n    cost_scale_factor = sigmoid(cost_diff_raw / tau)\n    dynamic_margin = margin * cost_scale_factor\n\n    # 4. Combine the log probability difference and the dynamic margin.\n    # We use tanh to bound the log_prob_diff, preventing explosions.\n    # The argument to softplus is negative when the model agrees with the preference beyond the margin.\n    argument = dynamic_margin - target_sign * tanh(log_prob_diff)\n\n    # 5. Apply a scaled softplus (smooth hinge loss) to get the final loss.\n    # softplus(x) = log(1 + exp(x)). It's a smooth version of relu(x).\n    loss = beta * softplus(argument)\n\n    return loss", "hyperparams": {"tau": {"value": 10.0, "description": "Temperature for scaling the raw cost difference. A larger tau makes the sigmoid transition smoother, meaning the dynamic margin is less sensitive to small cost differences."}, "beta": {"value": 1.0, "description": "Overall scaling factor for the final loss value. Adjusts the magnitude of the gradients."}, "margin": {"value": 0.5, "description": "Base margin, which is scaled by the cost difference. Represents the maximum desired log probability separation for very distinct costs."}}, "operators_used": ["rank_gap", "sigmoid", "tanh", "softplus"], "implementation_hint": {"expects": ["{'cost_a': \"Scalar tensor representing the cost of solution 'a'.\", 'cost_b': \"Scalar tensor representing the cost of solution 'b'.\", 'logp_a': \"Scalar tensor representing the log probability of solution 'a' from the model.\", 'logp_b': \"Scalar tensor representing the log probability of solution 'b' from the model.\"}"], "returns": "A scalar tensor representing the final loss for the pair (a, b)."}}, "static_ok": false, "static_reason": "implementation_hint.returns must be 'scalar'."}
{"generation": 2, "index": 1, "ir": {"name": "Sigmoid-Attenuated Rank-Gap Loss", "intuition": "This loss function is designed to be robust to outliers in both cost differences and model confidence (logit differences). It uses a normalized rank gap to represent the 'ground truth' preference margin, preventing extreme cost values from dominating the gradient. This margin is then compared to the model's predicted preference (logit difference). The core idea is to use a sigmoid function to attenuate the influence of very large logit differences. This creates a 'saturation' effect: once the model is very confident (either correctly or incorrectly), further increases in confidence have diminishing returns on the loss, preventing exploding gradients and promoting stability. The loss is essentially a softplus-smoothed hinge loss where the margin is dynamically scaled by the cost difference, and the penalty is attenuated by the model's own confidence level.", "pseudocode": "def SigmoidAttenuatedRankGapLoss(cost_a, cost_b, logp_a, logp_b, beta, tau):\n  # 1. Calculate the normalized rank gap as the target preference margin.\n  # This value is in [-1, 1], representing the strength and direction of the true preference.\n  target_margin = rank_gap(cost_a, cost_b)\n\n  # 2. Calculate the model's predicted log preference ratio.\n  logit_diff = logp_a - logp_b\n\n  # 3. Calculate the core term: how much the model's prediction deviates from the target.\n  # We want logit_diff to be positive if cost_a < cost_b (target_margin > 0).\n  # So, the error term is -(target_margin * logit_diff).\n  # A negative value means the model agrees with the preference.\n  error_term = -target_margin * logit_diff\n\n  # 4. Create an attenuation factor based on the model's confidence.\n  # The sigmoid of the absolute logit difference scales from 0.5 to 1.\n  # This factor dampens the loss when the model is already very confident (large |logit_diff|).\n  attenuation_factor = sigmoid(abs(logit_diff) / tau)\n\n  # 5. Combine the error term and attenuation factor, then apply softplus for a smooth, non-negative loss.\n  # The beta hyperparameter scales the overall magnitude of the loss.\n  # softplus(x) is a smooth approximation of relu(x).\n  loss = softplus(beta * attenuation_factor * error_term)\n\n  return loss", "hyperparams": {"beta": 10.0, "tau": 2.0}, "operators_used": ["rank_gap", "sigmoid", "softplus"], "implementation_hint": {"expects": ["{'cost_a': \"Tensor of costs for solution 'a' in each pair (shape: [batch_size])\", 'cost_b': \"Tensor of costs for solution 'b' in each pair (shape: [batch_size])\", 'logp_a': \"Tensor of log probabilities for solution 'a' in each pair (shape: [batch_size])\", 'logp_b': \"Tensor of log probabilities for solution 'b' in each pair (shape: [batch_size])\"}"], "returns": "A scalar loss value, typically the mean of the batch."}}, "static_ok": false, "static_reason": "implementation_hint.returns must be 'scalar'."}
{"generation": 2, "index": 2, "ir": {"name": "AdaptiveMarginHingeLoss", "intuition": "This loss function, named 'Adaptive Margin Hinge Loss', is designed for stable preference learning in combinatorial optimization. Its core idea is to create a dynamic, cost-sensitive margin that the model's preference (logit difference) must overcome. Unlike a fixed-margin hinge loss, this margin adapts to the magnitude of the cost difference between two solutions. When the cost difference is large, the model is strongly pushed to agree with this clear preference. When the cost difference is small (i.e., the solutions are of similar quality), the margin shrinks, giving the model more leeway and preventing it from overfitting to noisy or insignificant preferences. The use of `tanh` to scale the cost difference and `softplus` (a smooth version of ReLU) for the hinge mechanism ensures the loss is smooth, bounded, and numerically stable, avoiding issues with extreme input values.", "pseudocode": "def AdaptiveMarginHingeLoss(cost_a, cost_b, logp_a, logp_b, beta, margin_scale):\n    # Calculate the normalized cost difference, bounded between -1 and 1.\n    # tanh is used for stability and to capture the relative difference.\n    # beta controls the sensitivity to the cost difference.\n    cost_diff = cost_b - cost_a\n    normalized_cost_signal = tanh(beta * cost_diff)\n\n    # Calculate the model's preference strength.\n    logit_diff = logp_a - logp_b\n\n    # The target is for the logit_diff to align with the cost_signal.\n    # We want logit_diff to be positive if cost_a < cost_b.\n    # The loss is incurred when the model's preference is insufficient or wrong.\n    # The term 'margin_scale * normalized_cost_signal' acts as an adaptive margin.\n    # softplus(x) is a smooth approximation of max(0, x).\n    loss = softplus(-logit_diff * margin_scale * normalized_cost_signal)\n\n    return loss", "hyperparams": {"beta": {"description": "Controls the sensitivity to the cost difference. A higher beta makes the tanh function steeper, meaning the loss reacts more sharply to small cost differences. Default: 1.0.", "default": 1.0}, "margin_scale": {"description": "Scales the adaptive margin. It amplifies the target preference signal derived from the cost difference. Default: 5.0.", "default": 5.0}}, "operators_used": ["tanh", "softplus"], "implementation_hint": {"expects": ["{'cost_a': \"Scalar tensor representing the cost of solution 'a' (lower is better).\", 'cost_b': \"Scalar tensor representing the cost of solution 'b'.\", 'logp_a': \"Scalar tensor representing the log probability of solution 'a' from the model.\", 'logp_b': \"Scalar tensor representing the log probability of solution 'b' from the model.\"}"], "returns": "A scalar tensor representing the final loss value for the pair (a, b)."}}, "static_ok": false, "static_reason": "implementation_hint.returns must be 'scalar'."}
{"generation": 2, "index": 3, "ir": {"name": "SigmoidScaledRankGapLoss", "intuition": "This loss function is designed for robust preference learning in combinatorial optimization, inspired by the idea of adaptive margins. The core idea is that the 'margin' a model should aim for between the log-probabilities of a preferred solution (a) and a non-preferred one (b) should be proportional to the magnitude of their cost difference. A large cost difference should demand a large log-probability separation, while a small, possibly noisy, cost difference should only require a small separation. To achieve this, we first compute a normalized 'rank gap' from the costs, which quantifies the preference strength. This gap is then scaled by a sigmoid function, which bounds its influence between (0, 1), preventing extreme cost differences from creating excessively large targets. This scaled gap acts as a dynamic margin. The final loss is a softplus (a smooth version of ReLU) applied to the difference between this dynamic margin and the model's current log-probability difference. This structure robustly penalizes the model only when its preference (logp(a) - logp(b)) is not as strong as dictated by the cost difference, while being numerically stable due to the bounding nature of the sigmoid and the smoothness of softplus.", "pseudocode": "def SigmoidScaledRankGapLoss(cost_a, cost_b, logp_a, logp_b, beta, margin):\n  # Ensure a is the better solution for consistent calculation\n  if cost_a > cost_b:\n    cost_a, cost_b = cost_b, cost_a\n    logp_a, logp_b = logp_b, logp_a\n\n  # 1. Calculate the rank gap based on the cost difference. \n  # rank_gap is a normalized, non-negative value representing preference strength.\n  cost_delta = rank_gap(cost_a, cost_b)\n\n  # 2. Create a dynamic, bounded target separation using sigmoid.\n  # This prevents extreme cost differences from creating unbounded targets.\n  # The target is a value in (0, 1) representing the desired log-probability gap.\n  target_separation = sigmoid(cost_delta - margin)\n\n  # 3. Calculate the model's current log-probability difference.\n  logp_diff = logp_a - logp_b\n\n  # 4. The core loss calculation.\n  # We want logp_diff to be greater than (beta * target_separation).\n  # The loss is incurred when logp_diff is smaller than this target.\n  # softplus(x) = log(1 + exp(x)), a smooth version of relu.\n  loss_value = softplus(beta * target_separation - logp_diff)\n\n  return loss_value", "hyperparams": {"beta": {"value": 5.0, "description": "A scaling factor that controls the steepness of the penalty. Higher beta means a stronger push to meet the target separation."}, "margin": {"value": 0.5, "description": "A shift for the sigmoid activation. It defines the cost_delta midpoint where the target separation is 0.5. It helps ignore very small, potentially noisy cost differences."}}, "operators_used": ["rank_gap", "sigmoid", "softplus"], "implementation_hint": {"expects": ["{'cost_w': 'Scalar cost of the winning/preferred solution in a pair.', 'cost_l': 'Scalar cost of the losing/non-preferred solution in a pair.', 'logp_w': 'Scalar log-probability of the winning solution, from the model.', 'logp_l': 'Scalar log-probability of the losing solution, from the model.'}"], "returns": "A single scalar representing the loss for the pair."}}, "static_ok": false, "static_reason": "implementation_hint.returns must be 'scalar'."}
{"generation": 2, "index": 4, "ir": {"name": "AdaptiveMarginHingeLoss", "intuition": "This loss function, named Adaptive Margin Hinge Loss (AM-Hinge), is inspired by the hinge loss used in SVMs, but with two key innovations for preference learning in combinatorial optimization. First, the 'margin' is not a fixed hyperparameter but is adaptive, determined by the normalized difference in the costs of the two solutions. A larger cost difference between a good and a bad solution demands a larger log probability separation, pushing the model to be more decisive when the quality gap is significant. Second, it uses a softplus function instead of a hard hinge (max(0, x)) to ensure the loss is smooth and differentiable everywhere, facilitating stable backpropagation. The tanh function is used to normalize the cost difference into a [-1, 1] range, preventing extreme cost gaps from creating excessively large margins and ensuring numerical stability.", "pseudocode": "def AdaptiveMarginHingeLoss(cost_a, cost_b, logp_a, logp_b, beta, tau):\n    # For a pair (a, b) where a is the preferred solution (cost_a < cost_b)\n    # We expect logp(a) to be greater than logp(b)\n\n    # 1. Calculate the cost difference and log probability difference.\n    # The sign convention ensures that if the model's preference matches the ground truth (logp_a > logp_b), the value is positive.\n    delta_logp = logp_a - logp_b\n\n    # 2. Compute the adaptive margin based on the normalized cost difference.\n    # The cost difference is normalized by tau, then passed through tanh to bound it between [-1, 1].\n    # This prevents extreme cost differences from causing the margin to explode.\n    # We use (cost_b - cost_a) so a better 'a' results in a positive margin.\n    normalized_cost_gap = tanh((cost_b - cost_a) / tau)\n    adaptive_margin = normalized_cost_gap\n\n    # 3. Calculate the core hinge-like term.\n    # The loss is incurred when the model's preference (delta_logp) is smaller than the required adaptive_margin.\n    # The term (adaptive_margin - delta_logp) is positive when the model is underperforming.\n    hinge_term = adaptive_margin - delta_logp\n\n    # 4. Apply the softplus function for a smooth, non-negative loss.\n    # softplus(x) = log(1 + exp(x)). It approximates relu(x) but is smooth.\n    # 'beta' controls the steepness of the softplus function, acting like an inverse temperature.\n    # A higher beta makes it behave more like a hard hinge.\n    loss = softplus(beta * hinge_term)\n\n    return loss", "hyperparams": {"beta": {"description": "An inverse temperature parameter that controls the sharpness of the softplus function. Higher values make the loss approximate a hard-margin hinge loss more closely. Default: 5.0", "value": 5.0}, "tau": {"description": "A temperature scaling factor for normalizing the cost difference. It should be set to the expected scale of cost differences within a batch or across the dataset to ensure the tanh input is well-distributed. Default: 1.0", "value": 1.0}}, "operators_used": ["tanh", "softplus"], "implementation_hint": {"expects": ["{'cost_a': 'Tensor of costs for the first solution in each pair (preferred).', 'cost_b': 'Tensor of costs for the second solution in each pair (not preferred).', 'logp_a': 'Tensor of log probabilities for the first solution in each pair.', 'logp_b': 'Tensor of log probabilities for the second solution in each pair.'}"], "returns": "A scalar loss value (after reduction, e.g., mean over the batch)."}}, "static_ok": false, "static_reason": "implementation_hint.returns must be 'scalar'."}
{"generation": 2, "index": 5, "ir": {"name": "AdaptiveMarginHingeLoss", "intuition": "This loss function, named 'Adaptive Margin Hinge Loss', is designed to dynamically adjust the preference margin based on the magnitude of the cost difference between two solutions. The core idea is that when two solutions have a very large cost difference, the model should be strongly penalized if it prefers the worse solution. Conversely, when the cost difference is negligible, the model has more freedom, and the penalty for a 'wrong' preference should be small. This is achieved by creating an 'adaptive margin' that is a sigmoidal function of the cost difference. This margin is then used within a hinge-like structure (implemented with `softplus`, a smooth version of ReLU) to create the final loss. The `tanh` function is applied to the log-probability difference to bound it, preventing extreme logit values from causing instability and ensuring the loss remains within a predictable range.", "pseudocode": "1. Calculate the cost difference: delta_cost = cost_b - cost_a. This is positive if 'a' is the preferred solution.\n2. Compute an adaptive margin based on the cost difference. The margin grows with the absolute cost difference but saturates, preventing extreme values: margin = alpha * tanh(beta * delta_cost).\n3. Calculate the log probability difference: logp_diff = logp_a - logp_b.\n4. Apply a bounding function to the log probability difference for stability: bounded_logp_diff = gamma * tanh(logp_diff / gamma).\n5. The core loss is a smooth hinge loss (using softplus) that penalizes the model if the bounded log probability difference does not exceed the adaptive margin: loss = softplus(-bounded_logp_diff + margin).\n6. When cost_a < cost_b, delta_cost > 0, margin > 0. The loss encourages logp_a > logp_b. If logp_a is already much larger than logp_b, the loss is near zero. If logp_a < logp_b, the loss is positive and large.\n7. When cost_a > cost_b, delta_cost < 0, margin < 0. The loss encourages logp_a < logp_b. The logic is symmetric.", "hyperparams": {"alpha": 2.0, "beta": 0.1, "gamma": 5.0}, "operators_used": ["tanh", "softplus"], "implementation_hint": {"expects": ["{'cost_a': \"Scalar cost of solution 'a' for one instance.\", 'cost_b': \"Scalar cost of solution 'b' for the same instance.\", 'logp_a': \"Scalar log-probability of generating solution 'a'.\", 'logp_b': \"Scalar log-probability of generating solution 'b'.\"}"], "returns": "A single scalar value representing the loss for the pair (a, b)."}}, "static_ok": false, "static_reason": "implementation_hint.returns must be 'scalar'."}
{"generation": 2, "index": 6, "ir": {"name": "SigmoidAttenuatedRankLoss", "intuition": "This loss function conceptualizes the learning process as correcting the model's preference errors. The magnitude of the 'error' is defined by the rank difference in costs (\u0394_cost), which is more robust to outliers than the raw cost difference. The loss is activated only when the model's preference (indicated by logit difference) contradicts the ground-truth cost preference. The core idea is to use a sigmoid function to attenuate the loss signal based on the model's confidence. When the model is confidently wrong (large negative logit_diff), the sigmoid's gradient is small, preventing overly aggressive updates that could destabilize training. Conversely, when the model is uncertain or slightly wrong (logit_diff near zero), the gradient is maximal, encouraging efficient learning. The loss is scaled by the rank gap, so larger preference violations are penalized more heavily. This design ensures both robustness to cost scales and stability against extreme logit values.", "pseudocode": "def SigmoidAttenuatedRankLoss(cost_a, cost_b, logit_diff, beta, margin):\n  # cost_a is preferred over cost_b\n  # We expect logit_diff = logp(a) - logp(b) to be positive.\n\n  # 1. Calculate the rank gap in costs. This normalizes the cost difference.\n  # The output is 1 if cost_a < cost_b, -1 if cost_a > cost_b, 0 if equal.\n  # We assume cost_a < cost_b, so rank_gap is 1.\n  cost_rank_gap = rank_gap(cost_a, cost_b) # Should be 1 for preferred pair (a, b)\n\n  # 2. The loss is incurred when the model prefers the wrong solution (logit_diff < 0).\n  # We introduce a margin to create a clear separation boundary.\n  # We want logit_diff > margin.\n  violation = margin - logit_diff\n\n  # 3. Use a sigmoid function to attenuate the loss based on the logit difference.\n  # The argument to sigmoid is -beta * logit_diff.\n  # If logit_diff is very negative (confidently wrong), sigmoid -> 1, but its gradient -> 0, preventing explosion.\n  # If logit_diff is near zero (uncertain), sigmoid -> 0.5, gradient is large, encouraging learning.\n  # If logit_diff is very positive (confidently correct), sigmoid -> 0, loss -> 0.\n  attenuation_factor = sigmoid(-beta * logit_diff)\n\n  # 4. The final loss is the rank gap scaled by the sigmoid-attenuated violation.\n  # We use ReLU to ensure the loss is non-negative and only applied when there's a violation.\n  # The loss is non-zero only if logit_diff < margin.\n  loss = cost_rank_gap * relu(violation) * attenuation_factor\n\n  return loss", "hyperparams": {"beta": 1.0, "margin": 0.0}, "operators_used": ["rank_gap", "sigmoid", "relu"], "implementation_hint": {"expects": ["{'pair_cost_a': 'Tensor of costs for the preferred solutions in pairs.', 'pair_cost_b': 'Tensor of costs for the less preferred solutions in pairs.', 'logit_diff': 'Tensor of logit differences, logp(a) - logp(b).'}"], "returns": "A scalar loss value (after reduction, e.g., mean)."}}, "static_ok": false, "static_reason": "implementation_hint.returns must be 'scalar'."}
{"generation": 2, "index": 7, "ir": {"name": "Adaptive Margin Ranking Loss with Temperature Scaling (AMaRT)", "intuition": "This loss function is a variation of the classic margin ranking loss, but with two key innovations for stability and adaptivity in combinatorial optimization. First, it uses a non-linear, bounded transformation of the cost difference (tanh of the rank-gap) to create an 'adaptive margin'. This margin is small when costs are similar (less certain preference) and approaches a maximum value when costs are very different (strong preference). This prevents extremely large cost differences from dominating the gradient and causing instability. Second, it incorporates a temperature-like scaling factor (tau) on the log-probability difference. This allows us to control the sharpness of the preference signal, similar to its role in DPO or softmax functions. The softplus function is used as the final activation to ensure the loss is always non-negative and smooth, providing a stable, one-sided penalty only when the model's preference contradicts the ground-truth cost ordering.", "pseudocode": "def AMaRT_loss(cost_a, cost_b, logp_a, logp_b, tau, margin_scale):\n  # Assume cost_a < cost_b without loss of generality (preferred > dispreferred)\n  \n  # 1. Calculate the rank-based gap for costs. This is robust to outliers.\n  cost_gap = rank_gap(cost_a, cost_b) # e.g., rank_gap(c1, c2) = (c2 - c1) / (c2 + c1 + eps)\n  \n  # 2. Create a bounded, adaptive margin from the cost gap using tanh.\n  # The margin will be in [0, margin_scale).\n  adaptive_margin = margin_scale * tanh(cost_gap)\n  \n  # 3. Calculate the scaled log probability difference.\n  logp_diff = logp_a - logp_b # We want this to be positive.\n  \n  # 4. Combine the logp_diff and the adaptive margin.\n  # The argument to the loss is negative when the model agrees with the preference (logp_a > logp_b),\n  # and becomes positive when it disagrees.\n  # The adaptive_margin pushes for a larger probability gap for clearer cost differences.\n  loss_argument = - (logp_diff - adaptive_margin) / tau\n  \n  # 5. Apply softplus for a smooth, non-negative, and one-sided loss.\n  # softplus(x) = log(1 + exp(x)). Loss is near zero if loss_argument is very negative.\n  loss = softplus(loss_argument)\n  \n  return loss", "hyperparams": {"tau": {"description": "Temperature parameter to scale the log probability difference. Smaller values lead to sharper preference enforcement.", "default_value": 0.1}, "margin_scale": {"description": "The maximum possible margin, which scales the bounded tanh output. Controls the maximum separation pressure.", "default_value": 1.0}}, "operators_used": ["rank_gap", "tanh", "softplus"], "implementation_hint": {"expects": ["{'cost_w': 'Scalar cost of the preferred solution in a pair.', 'cost_l': 'Scalar cost of the dispreferred solution in a pair.', 'logp_w': 'Scalar log probability of the preferred solution.', 'logp_l': 'Scalar log probability of the dispreferred solution.'}"], "returns": "scalar (the final loss value for the pair)"}}, "static_ok": false, "static_reason": "implementation_hint.returns must be 'scalar'."}
