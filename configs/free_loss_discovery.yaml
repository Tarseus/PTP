seed: 1234

# Number of evolutionary generations (outer loop).
generations: 10

# Population size per generation.
population_size: 8

# Number of elites carried over between generations.
elite_size: 4

# Number of initial LLM-generated candidates.
init_llm: 8

# Multi-fidelity budget (in training steps).
f0_only: false
# High-fidelity training budget for F1.
# Option 1 (legacy): specify the total number of training *steps* directly.
f1_steps: 0
# Option 2: specify epochs and instances per epoch. When both are > 0,
# they are used to derive the total number of training steps as:
#   total_steps = hf_epochs * ceil(hf_instances_per_epoch / train_batch_size)
hf_epochs: 10
hf_instances_per_epoch: 100000
f2_steps: 0    # set >0 to enable F2
f3_enabled: false
max_repair_rounds: 2

# Training / evaluation config for TSP.
# Use TSP20 for both training and evaluation so that each candidate's
# fitness reflects full training behaviour on the canonical small-scale
# benchmark.
problem: tsp
train_problem_size: 100
valid_problem_sizes: [100]
train_batch_size: 64
pomo_size: 100
learning_rate: 0.0003
weight_decay: 0.000001
alpha: 0.05
device: cuda
num_validation_episodes: 10000
validation_batch_size: 64
generalization_penalty_weight: 1.0

# Gate thresholds for dynamic checks.
grad_norm_max: 10.0
loss_soft_min: -5.0
loss_soft_max: 5.0

# Whitelist of allowed operators in free-form losses.
operator_whitelist:
  - logsigmoid
  - softplus
  - sigmoid
  - exp
  - log
  - tanh
  - relu
  - clamp
  - normalize
  - zscore
  - rank_gap

# Paths to prompt templates (relative to repo root).
prompts:
  generation: prompts/free_loss_generation.txt
  crossover: prompts/free_loss_crossover.txt
  mutation: prompts/free_loss_mutation.txt
  repair: prompts/free_loss_repair.txt
  expects_repair: prompts/free_loss_expects_repair.txt

# Output directory pattern for discovery runs.
output_root: runs/free_loss_discovery
