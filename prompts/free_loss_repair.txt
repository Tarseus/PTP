You are an assistant responsible for **error repair** of a preference loss candidate that failed gate checks (static or dynamic), without changing its core innovation.

You will receive a `CANDIDATE_AND_FAILURE_JSON` containing:
- `candidate`: the current lossâ€™s `name`, `intuition`, `pseudocode`, `hyperparams`, `operators_used`, and `code` (Python implementation of the loss).
- `failure_reason`: a structured object describing why gates failed, with fields such as:
  - `stage`: one of `"static_gate"`, `"compile"`, `"dynamic_gate"`, `"preference_gate"`.
  - `code`: an error code like `E_JSON_PARSE`, `E_SCHEMA_MISSING_FIELD`, `E_OPERATOR_VIOLATION`, `E_GRAD_EXPLODE`, `E_LOSS_OUT_OF_RANGE`, `E_RUNTIME_NAN_LOSS`, etc.
  - `message`: a human-readable explanation.
  - `extra`: optional numeric context (e.g., `loss_value`, `grad_norm`).

Your task:
- Based on `failure_reason.code`, `failure_reason.stage`, and `failure_reason.message`, make the **minimal necessary changes** to fix numerical or structural issues, such as:
  - Adding `clamp`, `normalize`, or `zscore` to stabilize intermediate quantities.
  - Reducing the value/range of a hyperparameter (e.g., lowering `alpha` or `scale`).
  - Replacing a non-whitelisted operator with a composition of whitelisted ones, if needed.
- Do **not** change the core loss form (the main way it combines cost/logits and produces a scalar).

Hard constraints:
- Only use the following operators: `logsigmoid`, `softplus`, `sigmoid`, `exp`, `log`, `tanh`, `relu`, `clamp`, `normalize`, `zscore`, `rank_gap`.
- The loss must remain differentiable w.r.t. model parameters.
- When `cost(a) < cost(b)`, the loss should statistically encourage preferring `a` over `b`.
- It must be numerically stable (avoid NaN/Inf).

Implementation notes (to avoid gate failures):
- `log_prob_w` must correspond to the lower-cost solution and `log_prob_l` to the higher-cost solution.
- If you scale or weight `log_prob_w - log_prob_l` or a margin using cost gaps or normalizations, ensure the scale is non-negative (e.g., apply `softplus` or `clamp(min=0)`).
- Only read from `batch` the keys `cost_a`, `cost_b`, `log_prob_w`, `log_prob_l`, and optional `weight`. Do NOT access `batch['ops']` or `batch['torch']`; `ops` and `torch` are globals.

**Output requirements:**
- Output **must** be a single JSON object that is a repaired version of the candidate IR, with fields:
  - `name`
  - `intuition` (English only; briefly explain what you changed and why, ideally referencing the error code)
  - `pseudocode` (English only; updated pseudo-implementation)
  - `hyperparams`
  - `operators_used`
  - `implementation_hint` (with `expects` and `returns`)
  - `code` (Python implementation of the repaired loss function)
- For `implementation_hint`:
  - `expects` **must** be a JSON array of short input names **only**, and **must** be exactly: `["cost_a", "cost_b", "log_prob_w", "log_prob_l"]`.
    Do **not** write descriptions or full sentences here. Do **not** use a single string or an object.
  - `returns` **must** be exactly the string `"scalar"`.
- All text fields must be in **English only**.

Return **only** the JSON object, with no extra explanation or Markdown code fences.

Minimal valid example (structure only, keep your own content):
{"name":"MinimalLogsigmoidLoss","intuition":"Repaired: keep standard signature and return a scalar tensor.","pseudocode":"Compute delta = log_prob_w - log_prob_l; loss = -logsigmoid(delta).","hyperparams":{},"operators_used":["logsigmoid"],"implementation_hint":{"expects":["cost_a","cost_b","log_prob_w","log_prob_l"],"returns":"scalar"},"code":"def generated_loss(batch, model_output, extra):\\n    log_prob_w = batch['log_prob_w']\\n    log_prob_l = batch['log_prob_l']\\n    delta = log_prob_w - log_prob_l\\n    loss = -F.logsigmoid(delta)\\n    return loss.mean()"}

Common mistakes to avoid:
- Do NOT define generated_loss with positional args like generated_loss(cost_a, cost_b, ...).
- Do NOT call bare tanh/sigmoid/zscore; use torch.tanh, torch.sigmoid, F.logsigmoid, or ops.*.
