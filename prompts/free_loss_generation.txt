Task: Propose a new **preference loss** for learning from pairwise preferences in combinatorial optimization (e.g., TSP), in a *free-form* way.

You may freely design the loss structure and are **not** restricted to Bradley–Terry, logistic, DPO, etc. Only three hard constraints:
1. The loss must be differentiable w.r.t. model parameters (subgradients are allowed).
2. For the same instance, if cost(a) < cost(b), the loss should *statistically* encourage the model to prefer a over b (weak preference consistency).
3. It must be numerically stable: extreme Δcost or logits must not produce NaN/Inf.

Setting:
- You consider a pair of solutions (a, b) for the same instance.
- `cost(a)` and `cost(b)` are scalar objective values (lower is better).
- The model produces log probabilities `logp(a)` and `logp(b)`.
- You may only use the following **whitelisted operators** to compose your loss:
  `logsigmoid`, `softplus`, `sigmoid`, `exp`, `log`, `tanh`, `relu`, `clamp`, `normalize`, `zscore`, `rank_gap`.

Design a loss that combines costs and log-probability differences via scaling, margins, normalization, or other *stable* nonlinear transforms so that:
- If `cost(a) < cost(b)` and `logp(a) < logp(b)`, the loss tends to **increase** (pressure to raise `logp(a)`).
- If `cost(a) < cost(b)` and `logp(a) > logp(b)`, the loss tends to **decrease** (reward current preference).
- In extreme cases (very large cost differences or logit differences), the loss remains bounded and finite.

**Output requirements (IMPORTANT):**
- Output **must be a single JSON object**, with fields:
  - `name` (string, English)
  - `intuition` (string, English; short explanation of the idea)
  - `pseudocode` (string, English; high-level pseudo-implementation)
  - `hyperparams` (JSON object of hyperparameters)
  - `operators_used` (array, subset of the whitelist)
  - `implementation_hint` (JSON object with `expects` and `returns`)
- All text (including `intuition` and `pseudocode`) must be in **English only**.

JSON structure example (for format only):
{
  "name": "MyStablePreferenceLoss",
  "intuition": "Explain in English the main design idea and why it is stable.",
  "pseudocode": "Use rank_gap(cost_a, cost_b) to compute a signed cost gap; normalize or zscore it; combine it linearly or non-linearly with logit_diff = logp_a - logp_b; pass through logsigmoid or softplus to get a stable scalar loss.",
  "hyperparams": {
    "alpha": 5.0,
    "scale": 1.0,
    "margin": 0.5
  },
  "operators_used": ["rank_gap", "zscore", "logsigmoid"],
  "implementation_hint": {
    "expects": ["cost_a", "cost_b", "logp_a", "logp_b"],
    "returns": "scalar"
  }
}

Return **only** the JSON object, with no extra explanation or Markdown code fences, and use **English only** in all string fields.

