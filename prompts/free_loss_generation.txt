Task: Propose a new **preference loss** for learning from pairwise preferences in combinatorial optimization (e.g., TSP), in a *free-form* way.

You may freely design the loss structure and are **not** restricted to Bradley–Terry, logistic, DPO, etc. Only three hard constraints:
1. The loss must be differentiable w.r.t. model parameters (subgradients are allowed).
2. For the same instance, if cost(a) < cost(b), the loss should *statistically* encourage the model to prefer a over b (weak preference consistency).
3. It must be numerically stable: extreme Δcost or logits must not produce NaN/Inf.

Setting:
- You consider a pair of solutions (a, b) for the same instance.
- `cost(a)` and `cost(b)` are scalar objective values (lower is better).
- The model produces log probabilities `logp(a)` and `logp(b)`.
- You may only use the following **whitelisted operators** to compose your loss:
  `logsigmoid`, `softplus`, `sigmoid`, `exp`, `log`, `tanh`, `relu`, `clamp`, `normalize`, `zscore`, `rank_gap`.

Design a loss that combines costs and log-probability differences via scaling, margins, normalization, or other *stable* nonlinear transforms so that:
- If `cost(a) < cost(b)` and `logp(a) < logp(b)`, the loss tends to **increase** (pressure to raise `logp(a)`).
- If `cost(a) < cost(b)` and `logp(a) > logp(b)`, the loss tends to **decrease** (reward current preference).
- In extreme cases (very large cost differences or logit differences), the loss remains bounded and finite.

**Output requirements (IMPORTANT):**
- Output **must be a single JSON object**, with fields:
  - `name` (string, English)
  - `intuition` (string, English; short explanation of the idea)
  - `pseudocode` (string, English; high-level description of the idea; not executed)
  - `hyperparams` (JSON object of hyperparameters)
  - `operators_used` (array, subset of the whitelist)
  - `implementation_hint` (JSON object with `expects` and `returns`)
  - `code` (string containing valid Python source code for the loss function)
- For `implementation_hint`:
  - `expects` **must** be a JSON array of short input names **only**, e.g. `["cost_a", "cost_b", "logp_a", "logp_b"]`.
    Do **not** write descriptions or full sentences here. Do **not** use a single string or an object.
  - `returns` **must** be exactly the string `"scalar"`.
- For `code`:
  - It **must** define a function with the exact signature:

    `def generated_loss(batch, model_output, extra):`

  - The function must:
    - Read tensors from `batch`:
      * `cost_a`, `cost_b` (shape [N])
      * `log_prob_w`, `log_prob_l` (shape [N])
      * optional `weight` (shape [N]) which may be None.
    - Optionally read hyperparameters from `hyperparams` and/or `extra` (e.g. `alpha`).
    - Use only PyTorch (`torch`), `torch.nn.functional as F`, and the whitelisted operators listed above.
    - Return a **scalar tensor** representing the mean loss over the batch.
  - Do **not** wrap the code in Markdown fences. Encode it as a normal JSON string with escaped newlines (`\n`).
- All text fields (including `intuition` and `pseudocode`) must be in **English only**.

Return **only** the JSON object, with no extra explanation or Markdown code fences, and use **English only** in all string fields.
