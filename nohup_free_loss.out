[2025-12-17 21:40:45,431] INFO:ptp_discovery.free_loss_eoh: Starting free loss EoH search with config=configs/free_loss_discovery.yaml
[2025-12-17 21:40:48,545] INFO:ptp_discovery.free_loss_eoh: Baseline PO training: steps=15630, train_problem_size=100, pomo_size=100, batch_size=64, device=cuda, init_time=1.949s
[2025-12-17 21:40:50,125] INFO:ptp_discovery.free_loss_eoh: Baseline PO step 1/15630: score=48.226318 (avg=48.226318), loss=0.356016 (avg=0.356016)
[2025-12-17 21:41:24,438] INFO:fitness.ptp_high_fidelity: Evaluating TSP model: problem_size=100, pomo_size=100, episodes=10000, batch_size=64
[2025-12-17 21:45:37,313] INFO:ptp_discovery.free_loss_eoh: Baseline PO step 781/15630: score=8.230547 (avg=8.637331), loss=0.307676 (avg=0.303080)
[2025-12-17 21:50:12,676] INFO:ptp_discovery.free_loss_eoh: Baseline PO step 1562/15630: score=8.043215 (avg=8.392464), loss=0.315015 (avg=0.307076)
[2025-12-17 21:54:46,392] INFO:ptp_discovery.free_loss_eoh: Baseline PO step 2343/15630: score=8.021035 (avg=8.290861), loss=0.315185 (avg=0.309648)
[2025-12-17 21:59:17,500] INFO:ptp_discovery.free_loss_eoh: Baseline PO step 3124/15630: score=8.032766 (avg=8.231873), loss=0.318136 (avg=0.311400)
[2025-12-17 22:03:49,614] INFO:ptp_discovery.free_loss_eoh: Baseline PO step 3905/15630: score=8.052111 (avg=8.192495), loss=0.318630 (avg=0.312676)
[2025-12-17 22:08:20,115] INFO:ptp_discovery.free_loss_eoh: Baseline PO step 4686/15630: score=8.010311 (avg=8.163539), loss=0.320593 (avg=0.313703)
[2025-12-17 22:12:47,707] INFO:ptp_discovery.free_loss_eoh: Baseline PO step 5467/15630: score=7.981519 (avg=8.140789), loss=0.319995 (avg=0.314606)
[2025-12-17 22:17:18,441] INFO:ptp_discovery.free_loss_eoh: Baseline PO step 6248/15630: score=7.948230 (avg=8.122168), loss=0.321129 (avg=0.315423)
[2025-12-17 22:21:50,224] INFO:ptp_discovery.free_loss_eoh: Baseline PO step 7029/15630: score=7.968524 (avg=8.106567), loss=0.323089 (avg=0.316164)
[2025-12-17 22:26:17,921] INFO:ptp_discovery.free_loss_eoh: Baseline PO step 7810/15630: score=7.937525 (avg=8.093092), loss=0.323267 (avg=0.316834)
[2025-12-17 22:30:46,271] INFO:ptp_discovery.free_loss_eoh: Baseline PO step 8591/15630: score=7.947982 (avg=8.081307), loss=0.325709 (avg=0.317426)
[2025-12-17 22:35:19,984] INFO:ptp_discovery.free_loss_eoh: Baseline PO step 9372/15630: score=7.984466 (avg=8.071055), loss=0.324123 (avg=0.317956)
[2025-12-17 22:39:51,840] INFO:ptp_discovery.free_loss_eoh: Baseline PO step 10153/15630: score=7.977964 (avg=8.061766), loss=0.324739 (avg=0.318431)
[2025-12-17 22:44:16,960] INFO:ptp_discovery.free_loss_eoh: Baseline PO step 10934/15630: score=7.923026 (avg=8.053243), loss=0.324900 (avg=0.318861)
[2025-12-17 22:48:47,206] INFO:ptp_discovery.free_loss_eoh: Baseline PO step 11715/15630: score=7.939601 (avg=8.045707), loss=0.324281 (avg=0.319263)
[2025-12-17 22:53:13,353] INFO:ptp_discovery.free_loss_eoh: Baseline PO step 12496/15630: score=7.940905 (avg=8.038693), loss=0.326533 (avg=0.319633)
[2025-12-17 22:58:46,959] INFO:ptp_discovery.free_loss_eoh: Baseline PO step 13277/15630: score=7.914261 (avg=8.032226), loss=0.325397 (avg=0.319970)
[2025-12-17 23:04:02,869] INFO:ptp_discovery.free_loss_eoh: Baseline PO step 14058/15630: score=7.905068 (avg=8.026277), loss=0.326751 (avg=0.320285)
[2025-12-17 23:09:36,834] INFO:ptp_discovery.free_loss_eoh: Baseline PO step 14839/15630: score=7.951564 (avg=8.020861), loss=0.326467 (avg=0.320570)
[2025-12-17 23:14:48,972] INFO:ptp_discovery.free_loss_eoh: Baseline PO step 15620/15630: score=7.893592 (avg=8.015726), loss=0.326018 (avg=0.320848)
[2025-12-17 23:14:52,308] INFO:fitness.ptp_high_fidelity: Evaluating TSP model: problem_size=100, pomo_size=100, episodes=10000, batch_size=64
[2025-12-17 23:15:11,056] INFO:fitness.ptp_high_fidelity: Evaluating TSP model: problem_size=100, pomo_size=100, episodes=10000, batch_size=64
[2025-12-17 23:15:29,906] INFO:ptp_discovery.free_loss_eoh: Baseline PO timing: init=1.949s, train=5643.763s, eval=37.598s
[2025-12-17 23:15:29,907] INFO:ptp_discovery.free_loss_eoh: Baseline po_loss: hf_score=7.900410, validation_objective=7.898616, gen_penalty=0.001794
[2025-12-17 23:15:29,908] INFO:ptp_discovery.free_loss_eoh: Run directory: /data1/gushengda/PTP/runs/free_loss_discovery/20251217-231529
[2025-12-17 23:15:29,909] INFO:ptp_discovery.free_loss_eoh: === Generation 0/9 ===
[2025-12-17 23:15:29,909] INFO:ptp_discovery.free_loss_eoh: Generating initial population with 8 LLM candidates
[2025-12-17 23:15:44,555] INFO:httpx: HTTP Request: POST https://api.bltcy.ai/v1/chat/completions "HTTP/1.1 200 OK"
[2025-12-17 23:15:57,574] INFO:httpx: HTTP Request: POST https://api.bltcy.ai/v1/chat/completions "HTTP/1.1 200 OK"
[2025-12-17 23:16:13,166] INFO:httpx: HTTP Request: POST https://api.bltcy.ai/v1/chat/completions "HTTP/1.1 200 OK"
[2025-12-17 23:16:25,830] INFO:httpx: HTTP Request: POST https://api.bltcy.ai/v1/chat/completions "HTTP/1.1 200 OK"
[2025-12-17 23:16:37,541] INFO:httpx: HTTP Request: POST https://api.bltcy.ai/v1/chat/completions "HTTP/1.1 200 OK"
[2025-12-17 23:16:47,270] INFO:httpx: HTTP Request: POST https://api.bltcy.ai/v1/chat/completions "HTTP/1.1 200 OK"
[2025-12-17 23:17:01,407] INFO:httpx: HTTP Request: POST https://api.bltcy.ai/v1/chat/completions "HTTP/1.1 200 OK"
[2025-12-17 23:17:12,362] INFO:httpx: HTTP Request: POST https://api.bltcy.ai/v1/chat/completions "HTTP/1.1 200 OK"
[2025-12-17 23:17:26,238] INFO:httpx: HTTP Request: POST https://api.bltcy.ai/v1/chat/completions "HTTP/1.1 200 OK"
[2025-12-17 23:17:37,132] INFO:httpx: HTTP Request: POST https://api.bltcy.ai/v1/chat/completions "HTTP/1.1 200 OK"
[2025-12-17 23:17:49,961] INFO:httpx: HTTP Request: POST https://api.bltcy.ai/v1/chat/completions "HTTP/1.1 200 OK"
[2025-12-17 23:18:01,711] INFO:httpx: HTTP Request: POST https://api.bltcy.ai/v1/chat/completions "HTTP/1.1 200 OK"
[2025-12-17 23:18:17,879] INFO:httpx: HTTP Request: POST https://api.bltcy.ai/v1/chat/completions "HTTP/1.1 200 OK"
[2025-12-17 23:18:30,147] INFO:httpx: HTTP Request: POST https://api.bltcy.ai/v1/chat/completions "HTTP/1.1 200 OK"
[2025-12-17 23:18:43,978] INFO:httpx: HTTP Request: POST https://api.bltcy.ai/v1/chat/completions "HTTP/1.1 200 OK"
[2025-12-17 23:18:54,552] INFO:httpx: HTTP Request: POST https://api.bltcy.ai/v1/chat/completions "HTTP/1.1 200 OK"
[2025-12-17 23:18:54,554] INFO:ptp_discovery.free_loss_eoh: Population size for generation 0: 8
[2025-12-17 23:19:05,660] INFO:httpx: HTTP Request: POST https://api.bltcy.ai/v1/chat/completions "HTTP/1.1 200 OK"
[2025-12-17 23:19:05,663] DEBUG:ptp_discovery.free_loss_ir: implementation_hint.expects not array or Mapping; type=<class 'str'>, raw="A batch dictionary with keys 'cost_a', 'cost_b', 'log_prob_w', 'log_prob_l'. 'log_prob_w' and 'log_prob_l' are the log probabilities of the model's preferred and less-preferred completions, respectively. The loss re-orders these based on the ground truth costs."
[2025-12-17 23:19:05,672] WARNING:ptp_discovery.free_loss_eoh: Dynamic gates failed for gen=0, idx=0, name=SigmoidAsymmetricMarginLoss: reason=forward_error: 'logsigmoid', loss_value=None, grad_norm=None; attempting repair (attempt=2/2)
[2025-12-17 23:19:18,522] INFO:httpx: HTTP Request: POST https://api.bltcy.ai/v1/chat/completions "HTTP/1.1 200 OK"
[2025-12-17 23:19:18,524] DEBUG:ptp_discovery.free_loss_ir: implementation_hint.expects not array or Mapping; type=<class 'str'>, raw="A batch dictionary with keys 'cost_a', 'cost_b', 'log_prob_w', 'log_prob_l'. 'log_prob_w' and 'log_prob_l' are log probabilities from the model for its preferred and less-preferred outputs, respectively. An optional 'weight' key can be provided for instance weighting."
[2025-12-17 23:19:18,532] WARNING:ptp_discovery.free_loss_eoh: Dynamic gates failed for gen=0, idx=0, name=SigmoidAsymmetricMarginLoss: reason=forward_error: 'logsigmoid', loss_value=None, grad_norm=None
[2025-12-17 23:19:31,818] INFO:httpx: HTTP Request: POST https://api.bltcy.ai/v1/chat/completions "HTTP/1.1 200 OK"
[2025-12-17 23:19:31,831] WARNING:ptp_discovery.free_loss_eoh: Dynamic gates failed for gen=0, idx=1, name=AdaptiveSigmoidFocalLoss: reason=forward_error: 'logp_a', loss_value=None, grad_norm=None; attempting repair (attempt=2/2)
[2025-12-17 23:19:44,246] INFO:httpx: HTTP Request: POST https://api.bltcy.ai/v1/chat/completions "HTTP/1.1 200 OK"
[2025-12-17 23:19:44,248] DEBUG:ptp_discovery.free_loss_ir: implementation_hint.expects not array or Mapping; type=<class 'str'>, raw='`batch` is a dict with keys `logp_chosen`, `logp_rejected`, `cost_chosen`, `cost_rejected`. Each is a Tensor of shape (batch_size,). `extra` is a dict with an optional `hyperparams` field.'
[2025-12-17 23:19:44,256] WARNING:ptp_discovery.free_loss_eoh: Dynamic gates failed for gen=0, idx=1, name=AdaptiveSigmoidFocalLoss: reason=forward_error: 'logp_chosen', loss_value=None, grad_norm=None
[2025-12-17 23:19:53,467] INFO:httpx: HTTP Request: POST https://api.bltcy.ai/v1/chat/completions "HTTP/1.1 200 OK"
[2025-12-17 23:20:04,167] INFO:httpx: HTTP Request: POST https://api.bltcy.ai/v1/chat/completions "HTTP/1.1 200 OK"
[2025-12-17 23:20:04,179] WARNING:ptp_discovery.free_loss_eoh: Dynamic gates failed for gen=0, idx=2, name=AdaptiveMarginHingeLoss: reason=forward_error: generated_loss() missing 1 required positional argument: 'operators', loss_value=None, grad_norm=None
[2025-12-17 23:20:14,867] INFO:httpx: HTTP Request: POST https://api.bltcy.ai/v1/chat/completions "HTTP/1.1 200 OK"
[2025-12-17 23:20:14,869] DEBUG:ptp_discovery.free_loss_ir: implementation_hint.expects not array or Mapping; type=<class 'str'>, raw='A batch dictionary containing `cost_a`, `cost_b`, `log_prob_w`, `log_prob_l`, and optionally `weight`. Hyperparameters are passed via `extra`.'
[2025-12-17 23:20:25,570] INFO:httpx: HTTP Request: POST https://api.bltcy.ai/v1/chat/completions "HTTP/1.1 200 OK"
[2025-12-17 23:20:25,572] DEBUG:ptp_discovery.free_loss_ir: implementation_hint.expects not array or Mapping; type=<class 'str'>, raw='A dictionary `batch` containing `cost_a`, `cost_b`, `log_prob_w`, `log_prob_l`, and optionally `weight` tensors.'
[2025-12-17 23:20:25,582] WARNING:ptp_discovery.free_loss_eoh: Dynamic gates failed for gen=0, idx=3, name=AdaptiveMarginHingeLoss: reason=forward_error: module 'torch' has no attribute 'softplus', loss_value=None, grad_norm=None
[2025-12-17 23:20:36,219] INFO:httpx: HTTP Request: POST https://api.bltcy.ai/v1/chat/completions "HTTP/1.1 200 OK"
[2025-12-17 23:20:36,229] WARNING:ptp_discovery.free_loss_eoh: Dynamic gates failed for gen=0, idx=4, name=AdaptiveMarginHingeLoss: reason=forward_error: name 'zscore' is not defined, loss_value=None, grad_norm=None; attempting repair (attempt=2/2)
[2025-12-17 23:20:45,931] INFO:httpx: HTTP Request: POST https://api.bltcy.ai/v1/chat/completions "HTTP/1.1 200 OK"
[2025-12-17 23:20:58,881] INFO:httpx: HTTP Request: POST https://api.bltcy.ai/v1/chat/completions "HTTP/1.1 200 OK"
[2025-12-17 23:20:58,894] WARNING:ptp_discovery.free_loss_eoh: Dynamic gates failed for gen=0, idx=5, name=AdaptiveSigmoidHingeLoss: reason=forward_error: 'torch', loss_value=None, grad_norm=None; attempting repair (attempt=2/2)
[2025-12-17 23:21:10,587] INFO:httpx: HTTP Request: POST https://api.bltcy.ai/v1/chat/completions "HTTP/1.1 200 OK"
[2025-12-17 23:21:24,807] INFO:httpx: HTTP Request: POST https://api.bltcy.ai/v1/chat/completions "HTTP/1.1 200 OK"
[2025-12-17 23:21:24,818] WARNING:ptp_discovery.free_loss_eoh: Dynamic gates failed for gen=0, idx=6, name=AdaptiveMarginHingeLoss: reason=forward_error: name 'zscore' is not defined, loss_value=None, grad_norm=None; attempting repair (attempt=2/2)
[2025-12-17 23:21:34,770] INFO:httpx: HTTP Request: POST https://api.bltcy.ai/v1/chat/completions "HTTP/1.1 200 OK"
[2025-12-17 23:21:34,772] DEBUG:ptp_discovery.free_loss_ir: implementation_hint.expects not array or Mapping; type=<class 'str'>, raw="A batch dictionary with keys 'cost_a', 'cost_b', 'log_prob_w', 'log_prob_l'. 'cost_a' and 'log_prob_w' correspond to the preferred solutions, and 'cost_b' and 'log_prob_l' to the dispreferred ones. The dictionary can optionally contain 'weight' for each pair."
[2025-12-17 23:21:34,780] WARNING:ptp_discovery.free_loss_eoh: Dynamic gates failed for gen=0, idx=6, name=AdaptiveMarginHingeLoss: reason=forward_error: name 'clamp' is not defined, loss_value=None, grad_norm=None
[2025-12-17 23:21:45,576] INFO:httpx: HTTP Request: POST https://api.bltcy.ai/v1/chat/completions "HTTP/1.1 200 OK"
[2025-12-17 23:21:45,589] WARNING:ptp_discovery.free_loss_eoh: Dynamic gates failed for gen=0, idx=7, name=AdaptiveMarginHingeLoss: reason=forward_error: name 'abs' is not defined, loss_value=None, grad_norm=None; attempting repair (attempt=2/2)
[2025-12-17 23:21:58,388] INFO:httpx: HTTP Request: POST https://api.bltcy.ai/v1/chat/completions "HTTP/1.1 200 OK"
[2025-12-17 23:21:58,400] WARNING:ptp_discovery.free_loss_eoh: Dynamic gates failed for gen=0, idx=7, name=AdaptiveMarginHingeLoss: reason=forward_error: name 'softplus' is not defined, loss_value=None, grad_norm=None
[2025-12-17 23:21:58,400] INFO:ptp_discovery.free_loss_eoh: Generation 0 summary: static_fail=0, dynamic_fail=6, evaluated=0, new_elites=0
[2025-12-17 23:21:58,400] INFO:ptp_discovery.free_loss_eoh: === Generation 1/9 ===
[2025-12-17 23:21:58,400] INFO:ptp_discovery.free_loss_eoh: Generating population via crossover/mutation: size=8, elite_size=4, available_elites=0
[2025-12-17 23:22:17,036] INFO:httpx: HTTP Request: POST https://api.bltcy.ai/v1/chat/completions "HTTP/1.1 200 OK"
[2025-12-17 23:22:29,611] INFO:httpx: HTTP Request: POST https://api.bltcy.ai/v1/chat/completions "HTTP/1.1 200 OK"
[2025-12-17 23:22:45,294] INFO:httpx: HTTP Request: POST https://api.bltcy.ai/v1/chat/completions "HTTP/1.1 200 OK"
[2025-12-17 23:22:57,822] INFO:httpx: HTTP Request: POST https://api.bltcy.ai/v1/chat/completions "HTTP/1.1 200 OK"
[2025-12-17 23:23:14,569] INFO:httpx: HTTP Request: POST https://api.bltcy.ai/v1/chat/completions "HTTP/1.1 200 OK"
[2025-12-17 23:23:28,167] INFO:httpx: HTTP Request: POST https://api.bltcy.ai/v1/chat/completions "HTTP/1.1 200 OK"
[2025-12-17 23:23:41,370] INFO:httpx: HTTP Request: POST https://api.bltcy.ai/v1/chat/completions "HTTP/1.1 200 OK"
[2025-12-17 23:23:53,166] INFO:httpx: HTTP Request: POST https://api.bltcy.ai/v1/chat/completions "HTTP/1.1 200 OK"
[2025-12-17 23:24:07,028] INFO:httpx: HTTP Request: POST https://api.bltcy.ai/v1/chat/completions "HTTP/1.1 200 OK"
[2025-12-17 23:24:19,216] INFO:httpx: HTTP Request: POST https://api.bltcy.ai/v1/chat/completions "HTTP/1.1 200 OK"
[2025-12-17 23:24:35,493] INFO:httpx: HTTP Request: POST https://api.bltcy.ai/v1/chat/completions "HTTP/1.1 200 OK"
[2025-12-17 23:24:48,297] INFO:httpx: HTTP Request: POST https://api.bltcy.ai/v1/chat/completions "HTTP/1.1 200 OK"
[2025-12-17 23:25:02,374] INFO:httpx: HTTP Request: POST https://api.bltcy.ai/v1/chat/completions "HTTP/1.1 200 OK"
[2025-12-17 23:25:13,265] INFO:httpx: HTTP Request: POST https://api.bltcy.ai/v1/chat/completions "HTTP/1.1 200 OK"
[2025-12-17 23:25:27,120] INFO:httpx: HTTP Request: POST https://api.bltcy.ai/v1/chat/completions "HTTP/1.1 200 OK"
[2025-12-17 23:25:38,336] INFO:httpx: HTTP Request: POST https://api.bltcy.ai/v1/chat/completions "HTTP/1.1 200 OK"
[2025-12-17 23:25:38,338] INFO:ptp_discovery.free_loss_eoh: Population size for generation 1: 8
[2025-12-17 23:25:49,886] INFO:httpx: HTTP Request: POST https://api.bltcy.ai/v1/chat/completions "HTTP/1.1 200 OK"
[2025-12-17 23:26:01,997] INFO:httpx: HTTP Request: POST https://api.bltcy.ai/v1/chat/completions "HTTP/1.1 200 OK"
[2025-12-17 23:26:02,009] WARNING:ptp_discovery.free_loss_eoh: Dynamic gates failed for gen=1, idx=0, name=AsymmetricFocalBradleyTerry: reason=forward_error: 'logp_a', loss_value=None, grad_norm=None
[2025-12-17 23:26:14,214] INFO:httpx: HTTP Request: POST https://api.bltcy.ai/v1/chat/completions "HTTP/1.1 200 OK"
[2025-12-17 23:26:14,217] DEBUG:ptp_discovery.free_loss_ir: implementation_hint.expects not array or Mapping; type=<class 'str'>, raw="A `batch` dictionary with keys 'cost_a', 'cost_b', 'logp_a', 'logp_b' and optional 'weight'. An `extra` dictionary with 'margin_scale' and 'epsilon'."
[2025-12-17 23:26:14,225] WARNING:ptp_discovery.free_loss_eoh: Dynamic gates failed for gen=1, idx=1, name=AdaptiveMarginHingeLoss: reason=forward_error: 'logp_a', loss_value=None, grad_norm=None; attempting repair (attempt=2/2)
[2025-12-17 23:26:27,597] INFO:httpx: HTTP Request: POST https://api.bltcy.ai/v1/chat/completions "HTTP/1.1 200 OK"
[2025-12-17 23:26:40,378] INFO:httpx: HTTP Request: POST https://api.bltcy.ai/v1/chat/completions "HTTP/1.1 200 OK"
[2025-12-17 23:26:40,381] DEBUG:ptp_discovery.free_loss_ir: implementation_hint.expects not array or Mapping; type=<class 'str'>, raw='The `batch` dictionary is expected to contain `log_prob_w`, `log_prob_l`, `cost_a`, and `cost_b` tensors. `log_prob_w` and `log_prob_l` are the log probabilities of the winning and losing solutions in a pair, respectively. `cost_a` and `cost_b` are the costs used to determine the winner/loser.'
[2025-12-17 23:26:40,398] WARNING:ptp_discovery.free_loss_eoh: Dynamic gates failed for gen=1, idx=3, name=AdaptiveMarginHingeLoss: reason=forward_error: 'log_prob_a', loss_value=None, grad_norm=None; attempting repair (attempt=1/2)
[2025-12-17 23:26:53,969] INFO:httpx: HTTP Request: POST https://api.bltcy.ai/v1/chat/completions "HTTP/1.1 200 OK"
[2025-12-17 23:27:05,099] INFO:httpx: HTTP Request: POST https://api.bltcy.ai/v1/chat/completions "HTTP/1.1 200 OK"
[2025-12-17 23:27:05,101] DEBUG:ptp_discovery.free_loss_ir: implementation_hint.expects not array or Mapping; type=<class 'str'>, raw='The `batch` dictionary should contain `cost_a` and `cost_b` tensors. The `model_output` dictionary should contain `logits_a` and `logits_b` tensors. An optional `weight` tensor can be provided in the batch.'
[2025-12-17 23:27:05,111] WARNING:ptp_discovery.free_loss_eoh: Dynamic gates failed for gen=1, idx=3, name=AdaptiveMarginHingeLoss: reason=forward_error: 'logits_a', loss_value=None, grad_norm=None
[2025-12-17 23:27:16,531] INFO:httpx: HTTP Request: POST https://api.bltcy.ai/v1/chat/completions "HTTP/1.1 200 OK"
[2025-12-17 23:27:16,541] WARNING:ptp_discovery.free_loss_eoh: Dynamic gates failed for gen=1, idx=4, name=AdaptiveMarginHingeLoss: reason=forward_error: name 'softplus' is not defined, loss_value=None, grad_norm=None; attempting repair (attempt=2/2)
[2025-12-17 23:27:27,696] INFO:httpx: HTTP Request: POST https://api.bltcy.ai/v1/chat/completions "HTTP/1.1 200 OK"
[2025-12-17 23:27:27,892] WARNING:ptp_discovery.free_loss_eoh: Dynamic gates failed for gen=1, idx=4, name=AdaptiveMarginHingeLoss: reason=forward_error: 'ops', loss_value=None, grad_norm=None
[2025-12-17 23:27:41,394] INFO:httpx: HTTP Request: POST https://api.bltcy.ai/v1/chat/completions "HTTP/1.1 200 OK"
[2025-12-17 23:27:52,249] INFO:httpx: HTTP Request: POST https://api.bltcy.ai/v1/chat/completions "HTTP/1.1 200 OK"
[2025-12-17 23:28:03,271] INFO:httpx: HTTP Request: POST https://api.bltcy.ai/v1/chat/completions "HTTP/1.1 200 OK"
[2025-12-17 23:28:03,280] WARNING:ptp_discovery.free_loss_eoh: Dynamic gates failed for gen=1, idx=7, name=AdaptiveMarginHingeLoss: reason=forward_error: 'log_prob_a', loss_value=None, grad_norm=None; attempting repair (attempt=2/2)
[2025-12-17 23:28:15,107] INFO:httpx: HTTP Request: POST https://api.bltcy.ai/v1/chat/completions "HTTP/1.1 200 OK"
[2025-12-17 23:28:15,122] WARNING:ptp_discovery.free_loss_eoh: Dynamic gates failed for gen=1, idx=7, name=AdaptiveMarginHingeLoss: reason=forward_error: 'logits_a', loss_value=None, grad_norm=None
[2025-12-17 23:29:48,516] INFO:ptp_discovery.free_loss_eoh: Gen 1 cand 2: hf_like_score=22.061812, validation_objective=22.061812, baseline=7.900410, better_than_baseline=False
[2025-12-17 23:29:48,516] INFO:ptp_discovery.free_loss_eoh: Gen 1 cand 5: hf_like_score=8.502699, validation_objective=8.502699, baseline=7.900410, better_than_baseline=False
[2025-12-17 23:29:48,517] INFO:ptp_discovery.free_loss_eoh: Generation 1 summary: static_fail=2, dynamic_fail=4, evaluated=2, new_elites=2
[2025-12-17 23:29:48,517] INFO:ptp_discovery.free_loss_eoh: === Generation 2/9 ===
[2025-12-17 23:29:48,517] INFO:ptp_discovery.free_loss_eoh: Generating population via crossover/mutation: size=8, elite_size=4, available_elites=2
[2025-12-17 23:30:03,749] INFO:httpx: HTTP Request: POST https://api.bltcy.ai/v1/chat/completions "HTTP/1.1 200 OK"
[2025-12-17 23:30:15,065] INFO:httpx: HTTP Request: POST https://api.bltcy.ai/v1/chat/completions "HTTP/1.1 200 OK"
[2025-12-17 23:30:30,304] INFO:httpx: HTTP Request: POST https://api.bltcy.ai/v1/chat/completions "HTTP/1.1 200 OK"
[2025-12-17 23:30:43,171] INFO:httpx: HTTP Request: POST https://api.bltcy.ai/v1/chat/completions "HTTP/1.1 200 OK"
[2025-12-17 23:31:01,680] INFO:httpx: HTTP Request: POST https://api.bltcy.ai/v1/chat/completions "HTTP/1.1 200 OK"
[2025-12-17 23:31:15,041] INFO:httpx: HTTP Request: POST https://api.bltcy.ai/v1/chat/completions "HTTP/1.1 200 OK"
[2025-12-17 23:31:32,501] INFO:httpx: HTTP Request: POST https://api.bltcy.ai/v1/chat/completions "HTTP/1.1 200 OK"
[2025-12-17 23:31:45,112] INFO:httpx: HTTP Request: POST https://api.bltcy.ai/v1/chat/completions "HTTP/1.1 200 OK"
[2025-12-17 23:32:01,729] INFO:httpx: HTTP Request: POST https://api.bltcy.ai/v1/chat/completions "HTTP/1.1 200 OK"
[2025-12-17 23:32:13,922] INFO:httpx: HTTP Request: POST https://api.bltcy.ai/v1/chat/completions "HTTP/1.1 200 OK"
[2025-12-17 23:32:31,861] INFO:httpx: HTTP Request: POST https://api.bltcy.ai/v1/chat/completions "HTTP/1.1 200 OK"
[2025-12-17 23:32:44,095] INFO:httpx: HTTP Request: POST https://api.bltcy.ai/v1/chat/completions "HTTP/1.1 200 OK"
[2025-12-17 23:33:00,383] INFO:httpx: HTTP Request: POST https://api.bltcy.ai/v1/chat/completions "HTTP/1.1 200 OK"
[2025-12-17 23:33:12,118] INFO:httpx: HTTP Request: POST https://api.bltcy.ai/v1/chat/completions "HTTP/1.1 200 OK"
[2025-12-17 23:33:27,601] INFO:httpx: HTTP Request: POST https://api.bltcy.ai/v1/chat/completions "HTTP/1.1 200 OK"
[2025-12-17 23:33:40,306] INFO:httpx: HTTP Request: POST https://api.bltcy.ai/v1/chat/completions "HTTP/1.1 200 OK"
[2025-12-17 23:33:40,309] INFO:ptp_discovery.free_loss_eoh: Population size for generation 2: 8
multiprocessing.pool.RemoteTraceback: 
"""
Traceback (most recent call last):
  File "/data1/anaconda3/envs/rlco/lib/python3.11/multiprocessing/pool.py", line 125, in worker
    result = (True, func(*args, **kwds))
                    ^^^^^^^^^^^^^^^^^^^
  File "/data1/anaconda3/envs/rlco/lib/python3.11/multiprocessing/pool.py", line 48, in mapstar
    return list(map(*args))
           ^^^^^^^^^^^^^^^^
  File "/data1/gushengda/PTP/ptp_discovery/free_loss_eoh_loop.py", line 368, in _worker_evaluate_candidate
    fitness = evaluate_free_loss_candidate(
              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/data1/gushengda/PTP/fitness/free_loss_fidelity.py", line 205, in evaluate_free_loss_candidate
    score, loss, pair_count = _train_one_batch_with_free_loss(
                              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/data1/gushengda/PTP/fitness/free_loss_fidelity.py", line 77, in _train_one_batch_with_free_loss
    selected, prob = model(state)
                     ^^^^^^^^^^^^
  File "/data1/anaconda3/envs/rlco/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1553, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/data1/anaconda3/envs/rlco/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/data1/gushengda/PTP/POMO/TSP/POMO/TSPModel.py", line 68, in forward
    probs = self.decoder(encoded_last_node, ninf_mask=state.ninf_mask)
            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/data1/anaconda3/envs/rlco/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1553, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/data1/anaconda3/envs/rlco/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/data1/gushengda/PTP/POMO/TSP/POMO/TSPModel.py", line 253, in forward
    out = layer(out, ninf_mask)
          ^^^^^^^^^^^^^^^^^^^^^
  File "/data1/anaconda3/envs/rlco/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1553, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/data1/anaconda3/envs/rlco/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/data1/gushengda/PTP/POMO/TSP/POMO/TSPModel.py", line 327, in forward
    out_concat = multi_head_attention(q, self.k, self.v, rank3_ninf_mask=ninf_mask)
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/data1/gushengda/PTP/POMO/TSP/POMO/TSPModel.py", line 411, in multi_head_attention
    out = torch.matmul(weights, v)
          ^^^^^^^^^^^^^^^^^^^^^^^^
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 20.00 MiB. GPU 0 has a total capacity of 23.69 GiB of which 24.19 MiB is free. Process 132648 has 9.58 GiB memory in use. Process 248134 has 8.62 GiB memory in use. Including non-PyTorch memory, this process has 5.46 GiB memory in use. Of the allocated memory 4.72 GiB is allocated by PyTorch, and 281.84 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
"""

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/data1/gushengda/PTP/ptp_discovery/run_free_loss_eoh.py", line 57, in <module>
    main()
  File "/data1/gushengda/PTP/ptp_discovery/run_free_loss_eoh.py", line 53, in main
    run_free_loss_eoh(args.config, **overrides)
  File "/data1/gushengda/PTP/ptp_discovery/free_loss_eoh_loop.py", line 988, in run_free_loss_eoh
    batch_results = pool.map(_worker_evaluate_candidate, jobs_with_devices)
                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/data1/anaconda3/envs/rlco/lib/python3.11/multiprocessing/pool.py", line 367, in map
    return self._map_async(func, iterable, mapstar, chunksize).get()
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/data1/anaconda3/envs/rlco/lib/python3.11/multiprocessing/pool.py", line 774, in get
    raise self._value
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 20.00 MiB. GPU 0 has a total capacity of 23.69 GiB of which 24.19 MiB is free. Process 132648 has 9.58 GiB memory in use. Process 248134 has 8.62 GiB memory in use. Including non-PyTorch memory, this process has 5.46 GiB memory in use. Of the allocated memory 4.72 GiB is allocated by PyTorch, and 281.84 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
